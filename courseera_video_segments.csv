key,course_id,week_nbr,video_id,video_title,timeline_start,timeline_end,segment_nbr,segment_link,segment_txt
cs-410_1_1_1,cs-410,1,1, Natural Language Content Analysis,"00:00:00,008","00:00:04,018",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_1_1,cs-410,1,1, Natural Language Content Analysis,"00:00:00,000","00:00:05,420",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,Hello welcome to CS410 DSO Text Information Systems.
cs-410_1_1_2,cs-410,1,1, Natural Language Content Analysis,"00:00:09,625","00:00:12,226",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,>> This lecture is about Natural Language
cs-410_1_1_2,cs-410,1,1, Natural Language Content Analysis,"00:00:05,420","00:00:10,840",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=5,This is an online course offered by University of Illinois at Urbana-Champaign.
cs-410_1_1_3,cs-410,1,1, Natural Language Content Analysis,"00:00:12,226","00:00:13,732",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,of Content Analysis.
cs-410_1_1_3,cs-410,1,1, Natural Language Content Analysis,"00:00:10,840","00:00:12,750",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,My name is ChengXiang Zhai.
cs-410_1_1_4,cs-410,1,1, Natural Language Content Analysis,"00:00:13,732","00:00:15,569",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"As you see from this picture,"
cs-410_1_1_4,cs-410,1,1, Natural Language Content Analysis,"00:00:12,750","00:00:14,920",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"I also have a nickname, Cheng."
cs-410_1_1_5,cs-410,1,1, Natural Language Content Analysis,"00:00:15,569","00:00:19,540",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,this is really the first step
cs-410_1_1_5,cs-410,1,1, Natural Language Content Analysis,"00:00:14,920","00:00:20,050",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,I'm a professor of Computer Science at the University of Illinois at Urbana-Champaign.
cs-410_1_1_6,cs-410,1,1, Natural Language Content Analysis,"00:00:19,540","00:00:22,060",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,Text data are in natural languages.
cs-410_1_1_6,cs-410,1,1, Natural Language Content Analysis,"00:00:20,050","00:00:22,800",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,I'm the instructor of this course.
cs-410_1_1_7,cs-410,1,1, Natural Language Content Analysis,"00:00:22,060","00:00:26,820",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,So computers have to understand
cs-410_1_1_7,cs-410,1,1, Natural Language Content Analysis,"00:00:22,800","00:00:27,560",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,This first lecture is just an introduction to the course.
cs-410_1_1_8,cs-410,1,1, Natural Language Content Analysis,"00:00:26,820","00:00:29,380",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,in order to make use of the data.
cs-410_1_1_8,cs-410,1,1, Natural Language Content Analysis,"00:00:27,560","00:00:31,000",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,Let's first to start with some motivation.
cs-410_1_1_9,cs-410,1,1, Natural Language Content Analysis,"00:00:29,380","00:00:32,000",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,So that's the topic of this lecture.
cs-410_1_1_9,cs-410,1,1, Natural Language Content Analysis,"00:00:31,000","00:00:38,165",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,The problem we are trying to address in this course is how to harness big text data.
cs-410_1_1_10,cs-410,1,1, Natural Language Content Analysis,"00:00:32,000","00:00:33,910",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,We're going to cover three things.
cs-410_1_1_10,cs-410,1,1, Natural Language Content Analysis,"00:00:38,165","00:00:41,382",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,Text data is all kinds of data in
cs-410_1_1_11,cs-410,1,1, Natural Language Content Analysis,"00:00:33,910","00:00:36,430",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"First, what is natural"
cs-410_1_1_11,cs-410,1,1, Natural Language Content Analysis,"00:00:41,382","00:00:47,100",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,the form of natural languages such as English and Chinese.
cs-410_1_1_12,cs-410,1,1, Natural Language Content Analysis,"00:00:36,430","00:00:41,740",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,which is the main technique for processing
cs-410_1_1_12,cs-410,1,1, Natural Language Content Analysis,"00:00:47,100","00:00:52,200",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,"Now, this kind of data is everywhere and also growing very quickly."
cs-410_1_1_13,cs-410,1,1, Natural Language Content Analysis,"00:00:43,150","00:00:46,420",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,The second is the state of
cs-410_1_1_13,cs-410,1,1, Natural Language Content Analysis,"00:00:52,200","00:00:54,390",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,"For example, you can find all kinds of"
cs-410_1_1_14,cs-410,1,1, Natural Language Content Analysis,"00:00:46,420","00:00:48,350",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,natural language processing.
cs-410_1_1_14,cs-410,1,1, Natural Language Content Analysis,"00:00:54,390","00:00:58,300",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,web pages on the Internet and the number of pages is growing quickly.
cs-410_1_1_15,cs-410,1,1, Natural Language Content Analysis,"00:00:49,540","00:00:53,430",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,Finally we're going to cover the relation
cs-410_1_1_15,cs-410,1,1, Natural Language Content Analysis,"00:00:58,300","00:01:01,995",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,"You can also find the blogs articles, news articles,"
cs-410_1_1_16,cs-410,1,1, Natural Language Content Analysis,"00:00:53,430","00:00:54,900",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,text retrieval.
cs-410_1_1_16,cs-410,1,1, Natural Language Content Analysis,"00:01:01,995","00:01:07,540",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,or Emails and other kind of documents and enterprise environment.
cs-410_1_1_17,cs-410,1,1, Natural Language Content Analysis,"00:00:54,900","00:00:57,280",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,"First, what is NLP?"
cs-410_1_1_17,cs-410,1,1, Natural Language Content Analysis,"00:01:07,540","00:01:12,470",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,"Of course, we will also have a lot of scientific literature in text form."
cs-410_1_1_18,cs-410,1,1, Natural Language Content Analysis,"00:00:57,280","00:01:02,240",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,Well the best way to explain it
cs-410_1_1_18,cs-410,1,1, Natural Language Content Analysis,"00:01:12,470","00:01:16,250",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"And nowadays, social media has been growing quickly."
cs-410_1_1_19,cs-410,1,1, Natural Language Content Analysis,"00:01:02,240","00:01:05,860",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,a text in a foreign language
cs-410_1_1_19,cs-410,1,1, Natural Language Content Analysis,"00:01:16,250","00:01:21,675",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,"So we now see, tweets and other social media data also in the form of text."
cs-410_1_1_20,cs-410,1,1, Natural Language Content Analysis,"00:01:06,980","00:01:10,907",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,Now what do you have to do in
cs-410_1_1_20,cs-410,1,1, Natural Language Content Analysis,"00:01:21,675","00:01:27,709",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,All such text data encode a lot of useful knowledge about the world
cs-410_1_1_21,cs-410,1,1, Natural Language Content Analysis,"00:01:10,907","00:01:13,172",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,This is basically what
cs-410_1_1_21,cs-410,1,1, Natural Language Content Analysis,"00:01:27,709","00:01:35,360",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,because it's in some sense the data reported by human census about the observe the world.
cs-410_1_1_22,cs-410,1,1, Natural Language Content Analysis,"00:01:13,172","00:01:17,580",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,So looking at the simple sentence like
cs-410_1_1_22,cs-410,1,1, Natural Language Content Analysis,"00:01:35,360","00:01:41,115",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,So we can analyzed this kind of data to discover a lot of useful knowledge.
cs-410_1_1_23,cs-410,1,1, Natural Language Content Analysis,"00:01:18,730","00:01:22,250",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,We don't have any problems
cs-410_1_1_23,cs-410,1,1, Natural Language Content Analysis,"00:01:41,115","00:01:47,565",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,Especially the knowledge about the human opinions or preferences.
cs-410_1_1_24,cs-410,1,1, Natural Language Content Analysis,"00:01:22,250","00:01:25,930",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,But imagine what the computer would
cs-410_1_1_24,cs-410,1,1, Natural Language Content Analysis,"00:01:47,565","00:01:51,420",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,So this kind of data is very useful and we can use
cs-410_1_1_25,cs-410,1,1, Natural Language Content Analysis,"00:01:25,930","00:01:27,830",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,"Well in general,"
cs-410_1_1_25,cs-410,1,1, Natural Language Content Analysis,"00:01:51,420","00:01:55,725",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,"computational methods to turn such data into useful knowledge,"
cs-410_1_1_26,cs-410,1,1, Natural Language Content Analysis,"00:01:27,830","00:01:34,310",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"First, it would have to know dog"
cs-410_1_1_26,cs-410,1,1, Natural Language Content Analysis,"00:01:55,725","00:02:00,870",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,which can then be further used in many applications.
cs-410_1_1_27,cs-410,1,1, Natural Language Content Analysis,"00:01:34,310","00:01:38,410",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"So this is called lexical analysis,"
cs-410_1_1_27,cs-410,1,1, Natural Language Content Analysis,"00:02:00,870","00:02:09,470",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,The main techniques for making this happen include the Text Retrieval and Text Mining.
cs-410_1_1_28,cs-410,1,1, Natural Language Content Analysis,"00:01:38,410","00:01:42,230",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,we need to figure out the syntactic
cs-410_1_1_28,cs-410,1,1, Natural Language Content Analysis,"00:02:09,470","00:02:13,030",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,And these are the main techniques that we will cover in this course.
cs-410_1_1_29,cs-410,1,1, Natural Language Content Analysis,"00:01:42,230","00:01:43,930",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,So that's the first step.
cs-410_1_1_29,cs-410,1,1, Natural Language Content Analysis,"00:02:13,030","00:02:16,745",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,"Logically, in order to make use a lot of text data."
cs-410_1_1_30,cs-410,1,1, Natural Language Content Analysis,"00:01:43,930","00:01:48,060",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"After that, we're going to figure"
cs-410_1_1_30,cs-410,1,1, Natural Language Content Analysis,"00:02:16,745","00:02:19,225",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,"We would first do text retrieval,"
cs-410_1_1_31,cs-410,1,1, Natural Language Content Analysis,"00:01:48,060","00:01:50,370",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,"So for example, here it shows that A and"
cs-410_1_1_31,cs-410,1,1, Natural Language Content Analysis,"00:02:19,225","00:02:23,670",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,and that's due to a large set of text data
cs-410_1_1_32,cs-410,1,1, Natural Language Content Analysis,"00:01:50,370","00:01:54,260",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,the dog would go together
cs-410_1_1_32,cs-410,1,1, Natural Language Content Analysis,"00:02:23,670","00:02:28,995",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,"into a smaller but much more relevant set of data,"
cs-410_1_1_33,cs-410,1,1, Natural Language Content Analysis,"00:01:55,730","00:01:59,500",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,And we won't have dog and is to go first.
cs-410_1_1_33,cs-410,1,1, Natural Language Content Analysis,"00:02:28,995","00:02:32,577",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,that we actually need for a particular problem.
cs-410_1_1_34,cs-410,1,1, Natural Language Content Analysis,"00:01:59,500","00:02:02,969",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,And there are some structures
cs-410_1_1_34,cs-410,1,1, Natural Language Content Analysis,"00:02:32,577","00:02:35,790",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,"And this step, is usually implemented by"
cs-410_1_1_35,cs-410,1,1, Natural Language Content Analysis,"00:02:04,470","00:02:09,650",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,But this structure shows what we might
cs-410_1_1_35,cs-410,1,1, Natural Language Content Analysis,"00:02:35,790","00:02:39,300",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,using text retrieval techniques that involve humans in
cs-410_1_1_36,cs-410,1,1, Natural Language Content Analysis,"00:02:09,650","00:02:11,850",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,try to interpret the sentence.
cs-410_1_1_36,cs-410,1,1, Natural Language Content Analysis,"00:02:39,300","00:02:46,665",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,the loop to find and locate the most relevant documents to a particular problem.
cs-410_1_1_37,cs-410,1,1, Natural Language Content Analysis,"00:02:11,850","00:02:13,960",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,"Some words would go together first, and"
cs-410_1_1_37,cs-410,1,1, Natural Language Content Analysis,"00:02:46,665","00:02:49,900",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,"Once we find the relevant documents,"
cs-410_1_1_38,cs-410,1,1, Natural Language Content Analysis,"00:02:13,960","00:02:15,640",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,then they will go together
cs-410_1_1_38,cs-410,1,1, Natural Language Content Analysis,"00:02:49,900","00:02:52,467",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"the next step is to do text mining,"
cs-410_1_1_39,cs-410,1,1, Natural Language Content Analysis,"00:02:16,640","00:02:20,200",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,So here we show we have noun phrases
cs-410_1_1_39,cs-410,1,1, Natural Language Content Analysis,"00:02:52,467","00:02:57,090",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,which is to further analyze the found of relevant documents to
cs-410_1_1_40,cs-410,1,1, Natural Language Content Analysis,"00:02:20,200","00:02:21,500",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,then verbal phrases.
cs-410_1_1_40,cs-410,1,1, Natural Language Content Analysis,"00:02:57,090","00:02:59,785",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,discover useful knowledge to extract
cs-410_1_1_41,cs-410,1,1, Natural Language Content Analysis,"00:02:21,500","00:02:23,670",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,Finally we have a sentence.
cs-410_1_1_41,cs-410,1,1, Natural Language Content Analysis,"00:02:59,785","00:03:03,240",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,"the knowledge that can be directly used in application,"
cs-410_1_1_42,cs-410,1,1, Natural Language Content Analysis,"00:02:23,670","00:02:25,430",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,And you get this structure.
cs-410_1_1_42,cs-410,1,1, Natural Language Content Analysis,"00:03:03,240","00:03:06,408",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,especially in a application such as decision making.
cs-410_1_1_43,cs-410,1,1, Natural Language Content Analysis,"00:02:25,430","00:02:29,400",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,We need to do something called
cs-410_1_1_43,cs-410,1,1, Natural Language Content Analysis,"00:03:06,408","00:03:14,445",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"These two steps corresponding to Text Retrieval and Text Mining Techniques,"
cs-410_1_1_44,cs-410,1,1, Natural Language Content Analysis,"00:02:29,400","00:02:31,610",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,And we may have a parser
cs-410_1_1_44,cs-410,1,1, Natural Language Content Analysis,"00:03:14,445","00:03:20,968",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,that we will cover in this course.
cs-410_1_1_45,cs-410,1,1, Natural Language Content Analysis,"00:02:31,610","00:02:34,880",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,that would automatically
cs-410_1_1_45,cs-410,1,1, Natural Language Content Analysis,"00:03:20,968","00:03:23,880",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,"Based on this picture that I show you,"
cs-410_1_1_46,cs-410,1,1, Natural Language Content Analysis,"00:02:34,880","00:02:38,220",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,At this point you would know
cs-410_1_1_46,cs-410,1,1, Natural Language Content Analysis,"00:03:23,880","00:03:31,110",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,"this course is designed to leverage to corresponding books that I've offered on Coursera,"
cs-410_1_1_47,cs-410,1,1, Natural Language Content Analysis,"00:02:38,220","00:02:40,440",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,still you don't know
cs-410_1_1_47,cs-410,1,1, Natural Language Content Analysis,"00:03:31,110","00:03:34,665",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,which cover Text Retrieval and Text Mining respectively.
cs-410_1_1_48,cs-410,1,1, Natural Language Content Analysis,"00:02:40,440","00:02:44,060",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,So we have to go further
cs-410_1_1_48,cs-410,1,1, Natural Language Content Analysis,"00:03:34,665","00:03:36,420",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"The first book is called,"
cs-410_1_1_49,cs-410,1,1, Natural Language Content Analysis,"00:02:44,060","00:02:47,120",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,In our mind we usually can map
cs-410_1_1_49,cs-410,1,1, Natural Language Content Analysis,"00:03:36,420","00:03:38,515",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,Text Retrieval and Search Engines.
cs-410_1_1_50,cs-410,1,1, Natural Language Content Analysis,"00:02:47,120","00:02:51,330",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,such a sentence to what we already
cs-410_1_1_50,cs-410,1,1, Natural Language Content Analysis,"00:03:38,515","00:03:39,895",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"The second book is called,"
cs-410_1_1_51,cs-410,1,1, Natural Language Content Analysis,"00:02:51,330","00:02:53,970",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,"For example, you might imagine"
cs-410_1_1_51,cs-410,1,1, Natural Language Content Analysis,"00:03:39,895","00:03:41,275",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,the Text Mining and Analytics.
cs-410_1_1_52,cs-410,1,1, Natural Language Content Analysis,"00:02:53,970","00:02:56,800",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,There's a boy and
cs-410_1_1_52,cs-410,1,1, Natural Language Content Analysis,"00:03:41,275","00:03:44,400",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,These two books have
cs-410_1_1_53,cs-410,1,1, Natural Language Content Analysis,"00:02:56,800","00:02:59,860",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,But for a computer would have
cs-410_1_1_53,cs-410,1,1, Natural Language Content Analysis,"00:03:44,400","00:03:48,165",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,comprehensive lecture videos that cover
cs-410_1_1_54,cs-410,1,1, Natural Language Content Analysis,"00:03:00,890","00:03:05,232",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,We'd use a symbol (d1) to denote a dog.
cs-410_1_1_54,cs-410,1,1, Natural Language Content Analysis,"00:03:48,165","00:03:49,980",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,basic concepts and principles and
cs-410_1_1_55,cs-410,1,1, Natural Language Content Analysis,"00:03:05,232","00:03:10,430",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,And (b)1 can denote a boy and
cs-410_1_1_55,cs-410,1,1, Natural Language Content Analysis,"00:03:49,980","00:03:53,915",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,methods for text retrieval and text mining respectively.
cs-410_1_1_56,cs-410,1,1, Natural Language Content Analysis,"00:03:12,650","00:03:15,440",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,Now there is also a chasing
cs-410_1_1_56,cs-410,1,1, Natural Language Content Analysis,"00:03:53,915","00:03:56,405",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,"So, this online course,"
cs-410_1_1_57,cs-410,1,1, Natural Language Content Analysis,"00:03:15,440","00:03:19,130",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,we have a relationship chasing
cs-410_1_1_57,cs-410,1,1, Natural Language Content Analysis,"00:03:56,405","00:04:00,360",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,will leverage all those lecture videos and also
cs-410_1_1_58,cs-410,1,1, Natural Language Content Analysis,"00:03:19,130","00:03:23,909",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,So this is how a computer would obtain
cs-410_1_1_58,cs-410,1,1, Natural Language Content Analysis,"00:04:00,360","00:04:05,455",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,the online quizzes that are provided through the Coursera platform.
cs-410_1_1_59,cs-410,1,1, Natural Language Content Analysis,"00:03:25,920","00:03:31,590",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,Now from this representation we could
cs-410_1_1_59,cs-410,1,1, Natural Language Content Analysis,"00:04:05,455","00:04:07,170",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"But in addition to that,"
cs-410_1_1_60,cs-410,1,1, Natural Language Content Analysis,"00:03:31,590","00:03:35,960",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,and we might indeed naturally think of
cs-410_1_1_60,cs-410,1,1, Natural Language Content Analysis,"00:04:07,170","00:04:15,270",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,we also need to add additional components in order to explore applications because
cs-410_1_1_61,cs-410,1,1, Natural Language Content Analysis,"00:03:35,960","00:03:37,470",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,this is called inference.
cs-410_1_1_61,cs-410,1,1, Natural Language Content Analysis,"00:04:15,270","00:04:19,013",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,those books have covered a general techniques without a necessary discussing in depths
cs-410_1_1_62,cs-410,1,1, Natural Language Content Analysis,"00:03:37,470","00:03:42,490",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"So for example, if you believe"
cs-410_1_1_62,cs-410,1,1, Natural Language Content Analysis,"00:04:19,013","00:04:24,670",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,how those techniques are using applications.
cs-410_1_1_63,cs-410,1,1, Natural Language Content Analysis,"00:03:42,490","00:03:46,180",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,"this person might be scared,"
cs-410_1_1_63,cs-410,1,1, Natural Language Content Analysis,"00:04:24,670","00:04:30,200",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,"So, to make the coverage more complete in CS410,"
cs-410_1_1_64,cs-410,1,1, Natural Language Content Analysis,"00:03:46,180","00:03:50,880",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,you can see computers could also
cs-410_1_1_64,cs-410,1,1, Natural Language Content Analysis,"00:04:30,200","00:04:33,620",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,we were adding two additional components as shown here.
cs-410_1_1_65,cs-410,1,1, Natural Language Content Analysis,"00:03:50,880","00:03:54,080",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,So this is some extra knowledge
cs-410_1_1_65,cs-410,1,1, Natural Language Content Analysis,"00:04:33,620","00:04:37,415",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,That is of course Products and Technology Review.
cs-410_1_1_66,cs-410,1,1, Natural Language Content Analysis,"00:03:54,080","00:03:56,430",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,some understanding of the text.
cs-410_1_1_66,cs-410,1,1, Natural Language Content Analysis,"00:04:37,415","00:04:45,990",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,Both are meant to give the students freedom to choose a particular way to apply some of
cs-410_1_1_67,cs-410,1,1, Natural Language Content Analysis,"00:03:56,430","00:04:02,280",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,You can even go further to understand
cs-410_1_1_67,cs-410,1,1, Natural Language Content Analysis,"00:04:45,990","00:04:50,430",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,the techniques that you have learned in those two books to solve
cs-410_1_1_68,cs-410,1,1, Natural Language Content Analysis,"00:04:02,280","00:04:05,000",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,So this has to do as a use of language.
cs-410_1_1_68,cs-410,1,1, Natural Language Content Analysis,"00:04:50,430","00:04:56,635",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,a real world problem or to further learn about a particular topic.
cs-410_1_1_69,cs-410,1,1, Natural Language Content Analysis,"00:04:05,000","00:04:08,740",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,This is called pragmatic analysis.
cs-410_1_1_69,cs-410,1,1, Natural Language Content Analysis,"00:04:56,635","00:05:01,200",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"In the project, you will have opportunities to apply the knowledge that you have"
cs-410_1_1_70,cs-410,1,1, Natural Language Content Analysis,"00:04:08,740","00:04:13,910",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,In order to understand the speak
cs-410_1_1_70,cs-410,1,1, Natural Language Content Analysis,"00:05:01,200","00:05:08,337",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,learned in those two books to solve a real world problem in an integrated manner.
cs-410_1_1_71,cs-410,1,1, Natural Language Content Analysis,"00:04:13,910","00:04:18,370",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,We say something to
cs-410_1_1_71,cs-410,1,1, Natural Language Content Analysis,"00:05:08,337","00:05:11,070",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"In technical review, you can leverage what you have"
cs-410_1_1_72,cs-410,1,1, Natural Language Content Analysis,"00:04:18,370","00:04:19,440",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,There's some purpose there.
cs-410_1_1_72,cs-410,1,1, Natural Language Content Analysis,"00:05:11,070","00:05:14,960",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,learned to learn even more about either tool kit
cs-410_1_1_73,cs-410,1,1, Natural Language Content Analysis,"00:04:19,440","00:04:22,100",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,And this has to do with
cs-410_1_1_73,cs-410,1,1, Natural Language Content Analysis,"00:05:14,960","00:05:17,760",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,or a particular technology that can be
cs-410_1_1_74,cs-410,1,1, Natural Language Content Analysis,"00:04:22,100","00:04:24,750",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,In this case the person who said
cs-410_1_1_74,cs-410,1,1, Natural Language Content Analysis,"00:05:17,760","00:05:25,355",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,used to extend your knowledge about how to solve a problem.
cs-410_1_1_75,cs-410,1,1, Natural Language Content Analysis,"00:04:24,750","00:04:29,200",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,this sentence might be reminding
cs-410_1_1_75,cs-410,1,1, Natural Language Content Analysis,"00:05:25,355","00:05:31,605",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,The format of the course thus is a mixture of online videos plus
cs-410_1_1_76,cs-410,1,1, Natural Language Content Analysis,"00:04:29,200","00:04:31,410",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,That could be one possible intent.
cs-410_1_1_76,cs-410,1,1, Natural Language Content Analysis,"00:05:31,605","00:05:35,055",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,some high engagement module which mostly consists of
cs-410_1_1_77,cs-410,1,1, Natural Language Content Analysis,"00:04:33,020","00:04:36,500",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,To reach this level of
cs-410_1_1_77,cs-410,1,1, Natural Language Content Analysis,"00:05:35,055","00:05:40,475",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,our interactions through forums as I will explain later.
cs-410_1_1_78,cs-410,1,1, Natural Language Content Analysis,"00:04:36,500","00:04:41,390",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,all of these steps and
cs-410_1_1_78,cs-410,1,1, Natural Language Content Analysis,"00:05:40,475","00:05:45,120",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"And also, we will have a lot of interactions to help you"
cs-410_1_1_79,cs-410,1,1, Natural Language Content Analysis,"00:04:41,390","00:04:46,940",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,these steps in order to completely
cs-410_1_1_79,cs-410,1,1, Natural Language Content Analysis,"00:05:45,120","00:05:50,043",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,finish the course Project and Technology Review.
cs-410_1_1_80,cs-410,1,1, Natural Language Content Analysis,"00:04:46,940","00:04:49,560",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,Yet we humans have no trouble
cs-410_1_1_80,cs-410,1,1, Natural Language Content Analysis,"00:05:50,043","00:05:53,160",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,There are a number of goals that we
cs-410_1_1_81,cs-410,1,1, Natural Language Content Analysis,"00:04:49,560","00:04:51,430",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,we instantly would get everything.
cs-410_1_1_81,cs-410,1,1, Natural Language Content Analysis,"00:05:53,160","00:05:56,280",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,have kept in mind when designing this course and I want to share with
cs-410_1_1_82,cs-410,1,1, Natural Language Content Analysis,"00:04:52,790","00:04:53,760",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,There is a reason for that.
cs-410_1_1_82,cs-410,1,1, Natural Language Content Analysis,"00:05:56,280","00:05:59,100",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,you these goals because they would help you understand
cs-410_1_1_83,cs-410,1,1, Natural Language Content Analysis,"00:04:53,760","00:04:57,430",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,That's because we have a large
cs-410_1_1_83,cs-410,1,1, Natural Language Content Analysis,"00:05:59,100","00:06:02,070",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,why the course has this particular format and
cs-410_1_1_84,cs-410,1,1, Natural Language Content Analysis,"00:04:57,430","00:05:01,890",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,we can use common sense knowledge
cs-410_1_1_84,cs-410,1,1, Natural Language Content Analysis,"00:06:02,070","00:06:10,310",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,why you are asked to do certain components of the tasks.
cs-410_1_1_85,cs-410,1,1, Natural Language Content Analysis,"00:05:01,890","00:05:06,330",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,Computers unfortunately are hard
cs-410_1_1_85,cs-410,1,1, Natural Language Content Analysis,"00:06:10,310","00:06:15,650",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,The first goal is we want to emphasize both theory and practice.
cs-410_1_1_86,cs-410,1,1, Natural Language Content Analysis,"00:05:06,330","00:05:08,430",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,They don't have such a knowledge base.
cs-410_1_1_86,cs-410,1,1, Natural Language Content Analysis,"00:06:15,650","00:06:19,350",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,Theory is obviously very important because the basic concepts and
cs-410_1_1_87,cs-410,1,1, Natural Language Content Analysis,"00:05:08,430","00:05:12,520",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,They are still incapable of doing
cs-410_1_1_87,cs-410,1,1, Natural Language Content Analysis,"00:06:19,350","00:06:26,135",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,general principles covering the theoretical part would be applicable to all applications.
cs-410_1_1_88,cs-410,1,1, Natural Language Content Analysis,"00:05:14,290","00:05:18,430",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,so that makes natural language
cs-410_1_1_88,cs-410,1,1, Natural Language Content Analysis,"00:06:26,135","00:06:29,445",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"That means, they are not just used for"
cs-410_1_1_89,cs-410,1,1, Natural Language Content Analysis,"00:05:18,430","00:05:21,540",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,But the fundamental reason why natural
cs-410_1_1_89,cs-410,1,1, Natural Language Content Analysis,"00:06:29,445","00:06:34,815",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"today's applications but they can be used for solving future problems as well so,"
cs-410_1_1_90,cs-410,1,1, Natural Language Content Analysis,"00:05:21,540","00:05:25,430",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,computers is simply because natural
cs-410_1_1_90,cs-410,1,1, Natural Language Content Analysis,"00:06:34,815","00:06:36,230",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,they are more general.
cs-410_1_1_91,cs-410,1,1, Natural Language Content Analysis,"00:05:25,430","00:05:26,430",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,computers.
cs-410_1_1_91,cs-410,1,1, Natural Language Content Analysis,"00:06:36,230","00:06:41,470",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"Therefore, they have long lasting impact and utility value."
cs-410_1_1_92,cs-410,1,1, Natural Language Content Analysis,"00:05:26,430","00:05:30,960",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,Natural languages are designed for
cs-410_1_1_92,cs-410,1,1, Natural Language Content Analysis,"00:06:41,470","00:06:45,195",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,"And this is achieved by having you to watch all the lecture videos,"
cs-410_1_1_93,cs-410,1,1, Natural Language Content Analysis,"00:05:30,960","00:05:33,480",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,There are other languages designed for
cs-410_1_1_93,cs-410,1,1, Natural Language Content Analysis,"00:06:45,195","00:06:49,080",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,and then to use quizzes and exams to make sure
cs-410_1_1_94,cs-410,1,1, Natural Language Content Analysis,"00:05:33,480","00:05:36,220",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,"For example, programming languages."
cs-410_1_1_94,cs-410,1,1, Natural Language Content Analysis,"00:06:49,080","00:06:53,110",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,"that you have mastered all the basic ideas,"
cs-410_1_1_95,cs-410,1,1, Natural Language Content Analysis,"00:05:36,220","00:05:38,780",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,"Those are harder for us, right?"
cs-410_1_1_95,cs-410,1,1, Natural Language Content Analysis,"00:06:53,110","00:06:58,830",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,basic concepts and general principles and those general methods.
cs-410_1_1_96,cs-410,1,1, Natural Language Content Analysis,"00:05:38,780","00:05:43,690",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,So natural languages is designed to
cs-410_1_1_96,cs-410,1,1, Natural Language Content Analysis,"00:06:58,830","00:07:02,773",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"Practical skills are also very important,"
cs-410_1_1_97,cs-410,1,1, Natural Language Content Analysis,"00:05:43,690","00:05:46,770",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"As a result,"
cs-410_1_1_97,cs-410,1,1, Natural Language Content Analysis,"00:07:02,773","00:07:07,485",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,and because of specific practical skills can be immediately useful for solving
cs-410_1_1_98,cs-410,1,1, Natural Language Content Analysis,"00:05:46,770","00:05:49,540",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,because we assume everyone
cs-410_1_1_98,cs-410,1,1, Natural Language Content Analysis,"00:07:07,485","00:07:10,230",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,the problems today and maybe you will be able to
cs-410_1_1_99,cs-410,1,1, Natural Language Content Analysis,"00:05:49,540","00:05:56,250",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,We also keep a lot of ambiguities because
cs-410_1_1_99,cs-410,1,1, Natural Language Content Analysis,"00:07:10,230","00:07:13,945",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,resolve the problems that you encountered in your job.
cs-410_1_1_100,cs-410,1,1, Natural Language Content Analysis,"00:05:56,250","00:06:02,020",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,know how to decipher an ambiguous word
cs-410_1_1_100,cs-410,1,1, Natural Language Content Analysis,"00:07:13,945","00:07:18,640",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,"And this is achieved by having you to do Programming assignments,"
cs-410_1_1_101,cs-410,1,1, Natural Language Content Analysis,"00:06:02,020","00:06:05,320",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,There's no need to demand different
cs-410_1_1_101,cs-410,1,1, Natural Language Content Analysis,"00:07:18,640","00:07:22,660",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,"where you would be able to use existing tool kits,"
cs-410_1_1_102,cs-410,1,1, Natural Language Content Analysis,"00:06:05,320","00:06:08,820",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,We could overload the same word with
cs-410_1_1_102,cs-410,1,1, Natural Language Content Analysis,"00:07:22,660","00:07:27,090",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,to look into some algorithms in depths and to use some of
cs-410_1_1_103,cs-410,1,1, Natural Language Content Analysis,"00:06:10,460","00:06:14,350",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,Because of these reasons this makes every
cs-410_1_1_103,cs-410,1,1, Natural Language Content Analysis,"00:07:27,090","00:07:33,790",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,the algorithms to have some sense about how they work or how to improve them.
cs-410_1_1_104,cs-410,1,1, Natural Language Content Analysis,"00:06:14,350","00:06:17,520",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"difficult for computers,"
cs-410_1_1_104,cs-410,1,1, Natural Language Content Analysis,"00:07:33,790","00:07:38,235",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,"And finally, of course it's very important to integrate the theory and practice."
cs-410_1_1_105,cs-410,1,1, Natural Language Content Analysis,"00:06:18,780","00:06:22,060",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,And common sense and reasoning is
cs-410_1_1_105,cs-410,1,1, Natural Language Content Analysis,"00:07:38,235","00:07:39,637",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"And this will be done by,"
cs-410_1_1_106,cs-410,1,1, Natural Language Content Analysis,"00:06:23,800","00:06:26,300",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,So let me give you some
cs-410_1_1_106,cs-410,1,1, Natural Language Content Analysis,"00:07:39,637","00:07:43,365",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,having you to work on Course projects.
cs-410_1_1_107,cs-410,1,1, Natural Language Content Analysis,"00:06:27,505","00:06:29,350",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,Consider the word level ambiguity.
cs-410_1_1_107,cs-410,1,1, Natural Language Content Analysis,"00:07:43,365","00:07:48,060",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,"The second goal is to Personalized Learning,"
cs-410_1_1_108,cs-410,1,1, Natural Language Content Analysis,"00:06:30,730","00:06:34,510",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,The same word can have
cs-410_1_1_108,cs-410,1,1, Natural Language Content Analysis,"00:07:48,060","00:07:55,815",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,"because every one of you has a different in need and different in preference,"
cs-410_1_1_109,cs-410,1,1, Natural Language Content Analysis,"00:06:34,510","00:06:36,780",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,For example design can be a noun or
cs-410_1_1_109,cs-410,1,1, Natural Language Content Analysis,"00:07:55,815","00:07:59,970",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,"and you will have a different in schedules as well,"
cs-410_1_1_110,cs-410,1,1, Natural Language Content Analysis,"00:06:39,270","00:06:42,160",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,The word of root may
cs-410_1_1_110,cs-410,1,1, Natural Language Content Analysis,"00:07:59,970","00:08:04,440",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"because many of you are full time workers, perhaps."
cs-410_1_1_111,cs-410,1,1, Natural Language Content Analysis,"00:06:42,160","00:06:45,120",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So square root in math sense or
cs-410_1_1_111,cs-410,1,1, Natural Language Content Analysis,"00:08:04,440","00:08:09,500",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,"And so, Personalized Learning is very important to ensure"
cs-410_1_1_112,cs-410,1,1, Natural Language Content Analysis,"00:06:46,450","00:06:49,464",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,You might be able to think
cs-410_1_1_112,cs-410,1,1, Natural Language Content Analysis,"00:08:09,500","00:08:16,635",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,everyone to receive the best education.
cs-410_1_1_113,cs-410,1,1, Natural Language Content Analysis,"00:06:49,464","00:06:52,609",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,There are also syntactical ambiguities.
cs-410_1_1_113,cs-410,1,1, Natural Language Content Analysis,"00:08:16,635","00:08:18,645",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"So to achieve this goal,"
cs-410_1_1_114,cs-410,1,1, Natural Language Content Analysis,"00:06:52,609","00:06:56,932",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"For example, the main topic of this"
cs-410_1_1_114,cs-410,1,1, Natural Language Content Analysis,"00:08:18,645","00:08:22,285",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,we have intentionally designed
cs-410_1_1_115,cs-410,1,1, Natural Language Content Analysis,"00:06:56,932","00:07:01,480",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,can actually be interpreted in two
cs-410_1_1_115,cs-410,1,1, Natural Language Content Analysis,"00:08:22,285","00:08:26,520",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,"the deadlines and the format of the course to be flexible so that,"
cs-410_1_1_116,cs-410,1,1, Natural Language Content Analysis,"00:07:01,480","00:07:03,900",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,Think for a moment and
cs-410_1_1_116,cs-410,1,1, Natural Language Content Analysis,"00:08:26,520","00:08:30,780",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,you can have a lot of freedom to do self-paced learning.
cs-410_1_1_117,cs-410,1,1, Natural Language Content Analysis,"00:07:03,900","00:07:09,560",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,We usually think of this as
cs-410_1_1_117,cs-410,1,1, Natural Language Content Analysis,"00:08:30,780","00:08:36,345",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,"For example, you can watch over the lecture videos at anytime that you want to."
cs-410_1_1_118,cs-410,1,1, Natural Language Content Analysis,"00:07:09,560","00:07:13,991",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,but you could also think of this as do
cs-410_1_1_118,cs-410,1,1, Natural Language Content Analysis,"00:08:36,345","00:08:38,430",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"Watch it, watch that."
cs-410_1_1_119,cs-410,1,1, Natural Language Content Analysis,"00:07:16,130","00:07:20,440",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,So this is an example
cs-410_1_1_119,cs-410,1,1, Natural Language Content Analysis,"00:08:38,430","00:08:42,010",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,And you can also finish all the quizzes pretty much
cs-410_1_1_120,cs-410,1,1, Natural Language Content Analysis,"00:07:20,440","00:07:23,190",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,What we have different is
cs-410_1_1_120,cs-410,1,1, Natural Language Content Analysis,"00:08:42,010","00:08:47,160",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,"anytime and you can do the assignment,"
cs-410_1_1_121,cs-410,1,1, Natural Language Content Analysis,"00:07:24,510","00:07:27,480",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,applied to the same sequence of words.
cs-410_1_1_121,cs-410,1,1, Natural Language Content Analysis,"00:08:47,160","00:08:53,100",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,program assigns particularly again in a flexible way.
cs-410_1_1_122,cs-410,1,1, Natural Language Content Analysis,"00:07:27,480","00:07:31,730",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,Another common example of an ambiguous
cs-410_1_1_122,cs-410,1,1, Natural Language Content Analysis,"00:08:53,100","00:08:56,690",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,"In addition to self paced learning,"
cs-410_1_1_123,cs-410,1,1, Natural Language Content Analysis,"00:07:31,730","00:07:34,480",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,A man saw a boy with a telescope.
cs-410_1_1_123,cs-410,1,1, Natural Language Content Analysis,"00:08:56,690","00:09:02,541",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,you would also have a choices of topics for project and the technology review.
cs-410_1_1_124,cs-410,1,1, Natural Language Content Analysis,"00:07:34,480","00:07:37,810",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,"Now in this case the question is,"
cs-410_1_1_124,cs-410,1,1, Natural Language Content Analysis,"00:09:02,541","00:09:07,350",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,"And you will be able to work on a topic that's most interesting to you,"
cs-410_1_1_125,cs-410,1,1, Natural Language Content Analysis,"00:07:38,820","00:07:42,700",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,This is called a prepositional
cs-410_1_1_125,cs-410,1,1, Natural Language Content Analysis,"00:09:07,350","00:09:11,960",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,for both course project and technology review.
cs-410_1_1_126,cs-410,1,1, Natural Language Content Analysis,"00:07:42,700","00:07:45,030",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,PP attachment ambiguity.
cs-410_1_1_126,cs-410,1,1, Natural Language Content Analysis,"00:09:11,960","00:09:15,230",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,"Finally, we also have the goal of"
cs-410_1_1_127,cs-410,1,1, Natural Language Content Analysis,"00:07:45,030","00:07:50,000",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,Now we generally don't have a problem with
cs-410_1_1_127,cs-410,1,1, Natural Language Content Analysis,"00:09:15,230","00:09:20,190",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,collaborative learning because this would maximize the efficiency of learning.
cs-410_1_1_128,cs-410,1,1, Natural Language Content Analysis,"00:07:50,000","00:07:54,340",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,background knowledge to help
cs-410_1_1_128,cs-410,1,1, Natural Language Content Analysis,"00:09:20,190","00:09:25,730",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"Our common goal is to help everyone learn maximum amount of knowledge,"
cs-410_1_1_129,cs-410,1,1, Natural Language Content Analysis,"00:07:55,380","00:07:57,961",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,Another example of difficulty
cs-410_1_1_129,cs-410,1,1, Natural Language Content Analysis,"00:09:25,730","00:09:29,895",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,while spending hopefully minimum amount of effort.
cs-410_1_1_130,cs-410,1,1, Natural Language Content Analysis,"00:07:57,961","00:08:03,290",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,So think about the sentence John
cs-410_1_1_130,cs-410,1,1, Natural Language Content Analysis,"00:09:29,895","00:09:34,625",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,"So, this is a common goal that we should all work toward,"
cs-410_1_1_131,cs-410,1,1, Natural Language Content Analysis,"00:08:03,290","00:08:07,632",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,The question here is does
cs-410_1_1_131,cs-410,1,1, Natural Language Content Analysis,"00:09:34,625","00:09:38,135",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,and we should help each other achieve this goal.
cs-410_1_1_132,cs-410,1,1, Natural Language Content Analysis,"00:08:07,632","00:08:10,803",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So again this is something that
cs-410_1_1_132,cs-410,1,1, Natural Language Content Analysis,"00:09:38,135","00:09:44,014",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,"To promote collaborative learning and to facilitate collaborative learning,"
cs-410_1_1_133,cs-410,1,1, Natural Language Content Analysis,"00:08:10,803","00:08:12,540",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,the context to figure out.
cs-410_1_1_133,cs-410,1,1, Natural Language Content Analysis,"00:09:44,014","00:09:50,855",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,"we will use forum-based interactions to enable all of us,"
cs-410_1_1_134,cs-410,1,1, Natural Language Content Analysis,"00:08:12,540","00:08:15,470",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,"Finally, presupposition"
cs-410_1_1_134,cs-410,1,1, Natural Language Content Analysis,"00:09:50,855","00:09:53,465",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,"including many of you from different time zones,"
cs-410_1_1_135,cs-410,1,1, Natural Language Content Analysis,"00:08:15,470","00:08:18,110",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,"Consider the sentence,"
cs-410_1_1_135,cs-410,1,1, Natural Language Content Analysis,"00:09:53,465","00:09:56,975",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,to interact with each other effectively.
cs-410_1_1_136,cs-410,1,1, Natural Language Content Analysis,"00:08:18,110","00:08:20,710",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,Now this obviously implies
cs-410_1_1_136,cs-410,1,1, Natural Language Content Analysis,"00:09:56,975","00:10:05,360",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,"Another way to support a collaborative learning is to have you to work in groups,"
cs-410_1_1_137,cs-410,1,1, Natural Language Content Analysis,"00:08:22,430","00:08:27,000",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,So imagine a computer wants to understand
cs-410_1_1_137,cs-410,1,1, Natural Language Content Analysis,"00:10:05,360","00:10:08,950",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,to finish cost projects and technology reviews.
cs-410_1_1_138,cs-410,1,1, Natural Language Content Analysis,"00:08:27,000","00:08:30,750",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,It would have to use a lot of
cs-410_1_1_138,cs-410,1,1, Natural Language Content Analysis,"00:10:08,950","00:10:11,240",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"So, both technology reviews and"
cs-410_1_1_139,cs-410,1,1, Natural Language Content Analysis,"00:08:30,750","00:08:35,890",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,It also would have to maintain a large
cs-410_1_1_139,cs-410,1,1, Natural Language Content Analysis,"00:10:11,240","00:10:18,085",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,products can be done by working together with others in a group.
cs-410_1_1_140,cs-410,1,1, Natural Language Content Analysis,"00:08:35,890","00:08:41,940",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,words and how they are connected to our
cs-410_1_1_140,cs-410,1,1, Natural Language Content Analysis,"00:10:18,085","00:10:25,495",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,"So, overall the format is as follows: first,"
cs-410_1_1_141,cs-410,1,1, Natural Language Content Analysis,"00:08:41,940","00:08:44,130",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,So this is why it's very difficult.
cs-410_1_1_141,cs-410,1,1, Natural Language Content Analysis,"00:10:25,495","00:10:30,521",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,"you will have to watch the lecture videos of those two MOOCs,"
cs-410_1_1_142,cs-410,1,1, Natural Language Content Analysis,"00:08:45,530","00:08:49,110",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,"So as a result, we are steep not perfect,"
cs-410_1_1_142,cs-410,1,1, Natural Language Content Analysis,"00:10:30,521","00:10:34,120",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,"and this is also shown here on this slide,"
cs-410_1_1_143,cs-410,1,1, Natural Language Content Analysis,"00:08:49,110","00:08:54,240",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,in fact far from perfect in understanding
cs-410_1_1_143,cs-410,1,1, Natural Language Content Analysis,"00:10:34,120","00:10:38,100",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,and then you also need to take quizzes.
cs-410_1_1_144,cs-410,1,1, Natural Language Content Analysis,"00:08:54,240","00:09:00,200",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,So this slide sort of gains a simplified
cs-410_1_1_144,cs-410,1,1, Natural Language Content Analysis,"00:10:38,100","00:10:40,590",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,And these quizzes are given in a weekly manner.
cs-410_1_1_145,cs-410,1,1, Natural Language Content Analysis,"00:09:01,580","00:09:06,640",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,We can do part of speech
cs-410_1_1_145,cs-410,1,1, Natural Language Content Analysis,"00:10:40,590","00:10:43,375",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,"But as I said, the deadlines are actually flexible."
cs-410_1_1_146,cs-410,1,1, Natural Language Content Analysis,"00:09:06,640","00:09:09,610",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,I showed 97% accuracy here.
cs-410_1_1_146,cs-410,1,1, Natural Language Content Analysis,"00:10:43,375","00:10:47,530",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,"So, you should be able to finish quizzes as soon as you"
cs-410_1_1_147,cs-410,1,1, Natural Language Content Analysis,"00:09:09,610","00:09:13,830",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,Now this number is obviously
cs-410_1_1_147,cs-410,1,1, Natural Language Content Analysis,"00:10:47,530","00:10:52,190",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,finish the corresponding lecture videos.
cs-410_1_1_148,cs-410,1,1, Natural Language Content Analysis,"00:09:13,830","00:09:15,680",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,don't take this literally.
cs-410_1_1_148,cs-410,1,1, Natural Language Content Analysis,"00:10:52,190","00:10:55,685",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,These quizzes are two.
cs-410_1_1_149,cs-410,1,1, Natural Language Content Analysis,"00:09:15,680","00:09:18,210",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,This just shows that we
cs-410_1_1_149,cs-410,1,1, Natural Language Content Analysis,"00:10:55,685","00:10:59,085",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,There are also two kinds of quizzes.
cs-410_1_1_150,cs-410,1,1, Natural Language Content Analysis,"00:09:18,210","00:09:20,320",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,But it's still not perfect.
cs-410_1_1_150,cs-410,1,1, Natural Language Content Analysis,"00:10:59,085","00:11:05,810",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,There are practice quizzes that you can use to help you understand some concepts.
cs-410_1_1_151,cs-410,1,1, Natural Language Content Analysis,"00:09:20,320","00:09:23,620",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"In terms of parsing,"
cs-410_1_1_151,cs-410,1,1, Natural Language Content Analysis,"00:11:05,810","00:11:09,040",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,And there are also test quizzes.
cs-410_1_1_152,cs-410,1,1, Natural Language Content Analysis,"00:09:23,620","00:09:27,800",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,That means we can get noun phrase
cs-410_1_1_152,cs-410,1,1, Natural Language Content Analysis,"00:11:09,040","00:11:15,010",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=669,Those are of course to make sure that you have indeed mastered the materials.
cs-410_1_1_153,cs-410,1,1, Natural Language Content Analysis,"00:09:27,800","00:09:31,106",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"or some segment of the sentence, and"
cs-410_1_1_153,cs-410,1,1, Natural Language Content Analysis,"00:11:15,010","00:11:16,480",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,Then there are two exams.
cs-410_1_1_154,cs-410,1,1, Natural Language Content Analysis,"00:09:31,106","00:09:33,439",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,this dude correct them in
cs-410_1_1_154,cs-410,1,1, Natural Language Content Analysis,"00:11:16,480","00:11:23,270",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,Those two exams will be given at the end of each MOOC corresponding.
cs-410_1_1_155,cs-410,1,1, Natural Language Content Analysis,"00:09:34,470","00:09:39,310",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"And in some evaluation results,"
cs-410_1_1_155,cs-410,1,1, Natural Language Content Analysis,"00:11:23,270","00:11:30,010",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,"So, the first exam will be given at the end of the first MOOC on Test Retrieval,"
cs-410_1_1_156,cs-410,1,1, Natural Language Content Analysis,"00:09:39,310","00:09:43,140",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,accuracy in terms of partial
cs-410_1_1_156,cs-410,1,1, Natural Language Content Analysis,"00:11:30,010","00:11:36,620",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,and the second one will be given at the end of the second MOOC on Test Mining.
cs-410_1_1_157,cs-410,1,1, Natural Language Content Analysis,"00:09:43,140","00:09:46,910",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"Again, I have to say these numbers"
cs-410_1_1_157,cs-410,1,1, Natural Language Content Analysis,"00:11:36,620","00:11:40,445",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,"And then, during this period of watching these videos,"
cs-410_1_1_158,cs-410,1,1, Natural Language Content Analysis,"00:09:46,910","00:09:50,300",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,"In some other datasets,"
cs-410_1_1_158,cs-410,1,1, Natural Language Content Analysis,"00:11:40,445","00:11:44,020",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,you will be also working on programming assignments.
cs-410_1_1_159,cs-410,1,1, Natural Language Content Analysis,"00:09:50,300","00:09:54,230",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,Most of the existing work has been
cs-410_1_1_159,cs-410,1,1, Natural Language Content Analysis,"00:11:44,020","00:11:47,130",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,"Then at the end of the semester,"
cs-410_1_1_160,cs-410,1,1, Natural Language Content Analysis,"00:09:54,230","00:09:59,800",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,And so a lot of these numbers are more or
cs-410_1_1_160,cs-410,1,1, Natural Language Content Analysis,"00:11:47,130","00:11:52,735",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,"we'll leave about two weeks time for you to work intensively on the course project,"
cs-410_1_1_161,cs-410,1,1, Natural Language Content Analysis,"00:09:59,800","00:10:02,980",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"Think about social media data,"
cs-410_1_1_161,cs-410,1,1, Natural Language Content Analysis,"00:11:52,735","00:11:57,730",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,although you will be working on the course project throughout the semester.
cs-410_1_1_162,cs-410,1,1, Natural Language Content Analysis,"00:10:05,460","00:10:07,860",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,"In terms of a semantical analysis,"
cs-410_1_1_162,cs-410,1,1, Natural Language Content Analysis,"00:11:57,730","00:12:04,950",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,But most of the time will be in the end of the semester.
cs-410_1_1_163,cs-410,1,1, Natural Language Content Analysis,"00:10:07,860","00:10:13,730",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,we are far from being able to do
cs-410_1_1_163,cs-410,1,1, Natural Language Content Analysis,"00:12:04,950","00:12:12,850",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,"Now, the course project will be down sequentially by following multiple steps."
cs-410_1_1_164,cs-410,1,1, Natural Language Content Analysis,"00:10:13,730","00:10:16,430",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,But we have some techniques
cs-410_1_1_164,cs-410,1,1, Natural Language Content Analysis,"00:12:12,850","00:12:15,235",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,The first is for you to select a topic.
cs-410_1_1_165,cs-410,1,1, Natural Language Content Analysis,"00:10:16,430","00:10:18,880",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,do partial understanding of the sentence.
cs-410_1_1_165,cs-410,1,1, Natural Language Content Analysis,"00:12:15,235","00:12:17,920",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=735,You can select the topic from a list of topics that
cs-410_1_1_166,cs-410,1,1, Natural Language Content Analysis,"00:10:18,880","00:10:22,360",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,So I could mention some of them.
cs-410_1_1_166,cs-410,1,1, Natural Language Content Analysis,"00:12:17,920","00:12:20,740",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,"we provide or you can propose your topic,"
cs-410_1_1_167,cs-410,1,1, Natural Language Content Analysis,"00:10:22,360","00:10:27,190",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,"For example, we have techniques that can"
cs-410_1_1_167,cs-410,1,1, Natural Language Content Analysis,"00:12:20,740","00:12:23,540",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,and then we'll ask you to submit
cs-410_1_1_168,cs-410,1,1, Natural Language Content Analysis,"00:10:27,190","00:10:30,310",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,relations mentioned in text articles.
cs-410_1_1_168,cs-410,1,1, Natural Language Content Analysis,"00:12:23,540","00:12:28,120",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,a short proposal to be more specific about what you want to work on.
cs-410_1_1_169,cs-410,1,1, Natural Language Content Analysis,"00:10:30,310","00:10:34,766",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,"For example,"
cs-410_1_1_169,cs-410,1,1, Natural Language Content Analysis,"00:12:28,120","00:12:30,175",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,"And then, in the middle of the semester,"
cs-410_1_1_170,cs-410,1,1, Natural Language Content Analysis,"00:10:34,766","00:10:38,606",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"locations, organizations, etc in text."
cs-410_1_1_170,cs-410,1,1, Natural Language Content Analysis,"00:12:30,175","00:12:32,980",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=750,you will be asked to provide
cs-410_1_1_171,cs-410,1,1, Natural Language Content Analysis,"00:10:38,606","00:10:40,930",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,So this is called entity extraction.
cs-410_1_1_171,cs-410,1,1, Natural Language Content Analysis,"00:12:32,980","00:12:35,680",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,a short progress report so that we can check
cs-410_1_1_172,cs-410,1,1, Natural Language Content Analysis,"00:10:40,930","00:10:42,950",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,We may be able to recognize the relations.
cs-410_1_1_172,cs-410,1,1, Natural Language Content Analysis,"00:12:35,680","00:12:39,495",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=755,your progress and we can provide help in a timely manner.
cs-410_1_1_173,cs-410,1,1, Natural Language Content Analysis,"00:10:42,950","00:10:46,140",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,"For example,"
cs-410_1_1_173,cs-410,1,1, Natural Language Content Analysis,"00:12:39,495","00:12:41,650",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=759,"And then, at the end of the semester,"
cs-410_1_1_174,cs-410,1,1, Natural Language Content Analysis,"00:10:46,140","00:10:51,340",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,this person met that person or
cs-410_1_1_174,cs-410,1,1, Natural Language Content Analysis,"00:12:41,650","00:12:49,040",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,you will deliver two things: one is a software and its documentation.
cs-410_1_1_175,cs-410,1,1, Natural Language Content Analysis,"00:10:51,340","00:10:54,350",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,Such relations can be extracted by using
cs-410_1_1_175,cs-410,1,1, Natural Language Content Analysis,"00:12:49,040","00:12:55,820",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,You upload that to a public website so that it's available to people.
cs-410_1_1_176,cs-410,1,1, Natural Language Content Analysis,"00:10:54,350","00:10:57,230",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,the computer current
cs-410_1_1_176,cs-410,1,1, Natural Language Content Analysis,"00:12:55,820","00:12:57,920",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,"The second one is a tutorial presentation,"
cs-410_1_1_177,cs-410,1,1, Natural Language Content Analysis,"00:10:57,230","00:11:00,170",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,They're not perfect but
cs-410_1_1_177,cs-410,1,1, Natural Language Content Analysis,"00:12:57,920","00:13:04,385",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=777,a short presentation to explain how your software can be used by people.
cs-410_1_1_178,cs-410,1,1, Natural Language Content Analysis,"00:11:00,170","00:11:02,015",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,Some entities are harder than others.
cs-410_1_1_178,cs-410,1,1, Natural Language Content Analysis,"00:13:04,385","00:13:11,480",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,There is also a technology review component which is mandatory and
cs-410_1_1_179,cs-410,1,1, Natural Language Content Analysis,"00:11:03,040","00:11:05,907",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,We can also do word sense
cs-410_1_1_179,cs-410,1,1, Natural Language Content Analysis,"00:13:11,480","00:13:18,180",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=791,this component will be correlated based on completion.
cs-410_1_1_180,cs-410,1,1, Natural Language Content Analysis,"00:11:05,907","00:11:10,446",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,We have to figure out whether this word in
cs-410_1_1_180,cs-410,1,1, Natural Language Content Analysis,"00:13:18,180","00:13:22,645",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,Everyone is required to finish this component.
cs-410_1_1_181,cs-410,1,1, Natural Language Content Analysis,"00:11:10,446","00:11:15,250",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,in another context the computer could
cs-410_1_1_181,cs-410,1,1, Natural Language Content Analysis,"00:13:22,645","00:13:26,070",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=802,"Now, this component is designed to give you"
cs-410_1_1_182,cs-410,1,1, Natural Language Content Analysis,"00:11:15,250","00:11:18,200",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,"Again, it's not perfect, but"
cs-410_1_1_182,cs-410,1,1, Natural Language Content Analysis,"00:13:26,070","00:13:30,630",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=806,an opportunity to explore further any topic that you're interested in.
cs-410_1_1_183,cs-410,1,1, Natural Language Content Analysis,"00:11:19,530","00:11:21,240",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,"We can also do sentiment analysis,"
cs-410_1_1_183,cs-410,1,1, Natural Language Content Analysis,"00:13:30,630","00:13:35,755",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,"This can be in that examination of a tool kit that you are interested in,"
cs-410_1_1_184,cs-410,1,1, Natural Language Content Analysis,"00:11:21,240","00:11:25,830",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,"meaning, to figure out whether"
cs-410_1_1_184,cs-410,1,1, Natural Language Content Analysis,"00:13:35,755","00:13:39,095",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,"perhaps a tool that you have used in the course project,"
cs-410_1_1_185,cs-410,1,1, Natural Language Content Analysis,"00:11:25,830","00:11:28,940",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,This is especially useful for
cs-410_1_1_185,cs-410,1,1, Natural Language Content Analysis,"00:13:39,095","00:13:43,550",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=819,"or a comparison of multiple tools that can do similar things,"
cs-410_1_1_186,cs-410,1,1, Natural Language Content Analysis,"00:11:30,410","00:11:33,150",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,So these are examples
cs-410_1_1_186,cs-410,1,1, Natural Language Content Analysis,"00:13:43,550","00:13:46,050",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,"and you might have looked into multiple tools,"
cs-410_1_1_187,cs-410,1,1, Natural Language Content Analysis,"00:11:33,150","00:11:37,570",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,And they help us to obtain partial
cs-410_1_1_187,cs-410,1,1, Natural Language Content Analysis,"00:13:46,050","00:13:55,950",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=826,and you have some opportunity to compare them and to figure out which one is the best.
cs-410_1_1_188,cs-410,1,1, Natural Language Content Analysis,"00:11:38,850","00:11:43,410",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,It's not giving us a complete
cs-410_1_1_188,cs-410,1,1, Natural Language Content Analysis,"00:13:55,950","00:13:59,780",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=835,Such a writing would be helpful for others to understand how to
cs-410_1_1_189,cs-410,1,1, Natural Language Content Analysis,"00:11:43,410","00:11:44,380",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=703,this sentence.
cs-410_1_1_189,cs-410,1,1, Natural Language Content Analysis,"00:13:59,780","00:14:04,210",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=839,use a tool kit or which one to choose.
cs-410_1_1_190,cs-410,1,1, Natural Language Content Analysis,"00:11:44,380","00:11:48,150",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,But it would still help us gain
cs-410_1_1_190,cs-410,1,1, Natural Language Content Analysis,"00:14:04,210","00:14:08,360",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,"Of course, those of you who are interested in the methods,"
cs-410_1_1_191,cs-410,1,1, Natural Language Content Analysis,"00:11:48,150","00:11:49,580",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,And these can be useful.
cs-410_1_1_191,cs-410,1,1, Natural Language Content Analysis,"00:14:08,360","00:14:11,870",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=848,you can also go in-depth to examine a certain type
cs-410_1_1_192,cs-410,1,1, Natural Language Content Analysis,"00:11:51,620","00:11:54,730",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,"In terms of inference,"
cs-410_1_1_192,cs-410,1,1, Natural Language Content Analysis,"00:14:11,870","00:14:15,725",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,"of methods and write a brief review of them,"
cs-410_1_1_193,cs-410,1,1, Natural Language Content Analysis,"00:11:54,730","00:12:00,050",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,probably because of the general difficulty
cs-410_1_1_193,cs-410,1,1, Natural Language Content Analysis,"00:14:15,725","00:14:20,210",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,or looking to a cutting edge topic in your research and write
cs-410_1_1_194,cs-410,1,1, Natural Language Content Analysis,"00:12:00,050","00:12:03,390",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=720,This is a general challenge
cs-410_1_1_194,cs-410,1,1, Natural Language Content Analysis,"00:14:20,210","00:14:26,250",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=860,a review of the recent papers about the topic.
cs-410_1_1_195,cs-410,1,1, Natural Language Content Analysis,"00:12:03,390","00:12:07,468",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,Now that's probably also because
cs-410_1_1_195,cs-410,1,1, Natural Language Content Analysis,"00:14:26,250","00:14:29,020",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=866,"Now, we hope that you might be able to align"
cs-410_1_1_196,cs-410,1,1, Natural Language Content Analysis,"00:12:07,468","00:12:10,172",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,representation for
cs-410_1_1_196,cs-410,1,1, Natural Language Content Analysis,"00:14:29,020","00:14:33,980",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=869,the technology review with your course project so that the two will
cs-410_1_1_197,cs-410,1,1, Natural Language Content Analysis,"00:12:10,172","00:12:11,320",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,So this is hard.
cs-410_1_1_197,cs-410,1,1, Natural Language Content Analysis,"00:14:33,980","00:14:38,330",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,be kind of synergistic in that technology review will help
cs-410_1_1_198,cs-410,1,1, Natural Language Content Analysis,"00:12:11,320","00:12:16,540",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,"Yet in some domains perhaps,"
cs-410_1_1_198,cs-410,1,1, Natural Language Content Analysis,"00:14:38,330","00:14:44,040",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=878,"you learn more about the topic related to your course project,"
cs-410_1_1_199,cs-410,1,1, Natural Language Content Analysis,"00:12:16,540","00:12:23,340",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,"restrictions on the word uses, you may be"
cs-410_1_1_199,cs-410,1,1, Natural Language Content Analysis,"00:14:44,040","00:14:48,600",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=884,while the course project will give you also a motivation for doing
cs-410_1_1_200,cs-410,1,1, Natural Language Content Analysis,"00:12:23,340","00:12:28,050",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,But in general we can not
cs-410_1_1_200,cs-410,1,1, Natural Language Content Analysis,"00:14:48,600","00:14:54,745",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=888,an in-depth study of the topic via technology review.
cs-410_1_1_201,cs-410,1,1, Natural Language Content Analysis,"00:12:28,050","00:12:31,650",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,Speech act analysis is also
cs-410_1_1_201,cs-410,1,1, Natural Language Content Analysis,"00:14:54,745","00:15:00,770",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,"Of course, we will try to help you finish all these tasks,"
cs-410_1_1_202,cs-410,1,1, Natural Language Content Analysis,"00:12:31,650","00:12:36,600",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=751,we can only do that analysis for
cs-410_1_1_202,cs-410,1,1, Natural Language Content Analysis,"00:15:00,770","00:15:03,000",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=900,and we'll do our best to help you.
cs-410_1_1_203,cs-410,1,1, Natural Language Content Analysis,"00:12:36,600","00:12:41,193",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,So this roughly gives you some
cs-410_1_1_203,cs-410,1,1, Natural Language Content Analysis,"00:15:03,000","00:15:08,865",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=903,"That means, the TAs and I will interact with you in various ways to help you,"
cs-410_1_1_204,cs-410,1,1, Natural Language Content Analysis,"00:12:41,193","00:12:46,356",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,And then we also talk a little
cs-410_1_1_204,cs-410,1,1, Natural Language Content Analysis,"00:15:08,865","00:15:13,725",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=908,and one way is a synchronous question answering and discussing via forums.
cs-410_1_1_205,cs-410,1,1, Natural Language Content Analysis,"00:12:46,356","00:12:51,780",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,and so we can't even do 100%
cs-410_1_1_205,cs-410,1,1, Natural Language Content Analysis,"00:15:13,725","00:15:16,400",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,And the other way is to have
cs-410_1_1_206,cs-410,1,1, Natural Language Content Analysis,"00:12:51,780","00:12:54,700",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=771,"Now this looks like a simple task, but"
cs-410_1_1_206,cs-410,1,1, Natural Language Content Analysis,"00:15:16,400","00:15:25,610",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=916,synchronous weekly office hours and that will be down by using video teleconferencing.
cs-410_1_1_207,cs-410,1,1, Natural Language Content Analysis,"00:12:54,700","00:12:59,800",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,"think about the example here,"
cs-410_1_1_207,cs-410,1,1, Natural Language Content Analysis,"00:15:25,610","00:15:33,265",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=925,"The grading will be done as follows: 25% of your grade will be based on the quizzes,"
cs-410_1_1_208,cs-410,1,1, Natural Language Content Analysis,"00:12:59,800","00:13:04,840",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=779,have different syntactic categories if you
cs-410_1_1_208,cs-410,1,1, Natural Language Content Analysis,"00:15:33,265","00:15:37,075",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=933,all the quizzes for the two MOOCs;
cs-410_1_1_209,cs-410,1,1, Natural Language Content Analysis,"00:13:04,840","00:13:07,600",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,It's not that easy to figure
cs-410_1_1_209,cs-410,1,1, Natural Language Content Analysis,"00:15:37,075","00:15:39,655",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=937,"30% will be based on the two exams,"
cs-410_1_1_210,cs-410,1,1, Natural Language Content Analysis,"00:13:10,000","00:13:12,900",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=790,It's also hard to do
cs-410_1_1_210,cs-410,1,1, Natural Language Content Analysis,"00:15:39,655","00:15:43,740",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,those two exams will be proctored exams;
cs-410_1_1_211,cs-410,1,1, Natural Language Content Analysis,"00:13:12,900","00:13:16,940",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,"And again, the same sentence"
cs-410_1_1_211,cs-410,1,1, Natural Language Content Analysis,"00:15:43,740","00:15:48,560",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,"25% will be based on the programming assignments,"
cs-410_1_1_212,cs-410,1,1, Natural Language Content Analysis,"00:13:18,010","00:13:23,330",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,This ambiguity can be very hard to
cs-410_1_1_212,cs-410,1,1, Natural Language Content Analysis,"00:15:48,560","00:15:52,115",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,there will be multiple programming assignments throughout the semester.
cs-410_1_1_213,cs-410,1,1, Natural Language Content Analysis,"00:13:23,330","00:13:27,940",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,where you have to use a lot of knowledge
cs-410_1_1_213,cs-410,1,1, Natural Language Content Analysis,"00:15:52,115","00:15:59,270",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=952,The remaining 20% will be based on your course project and that's for
cs-410_1_1_214,cs-410,1,1, Natural Language Content Analysis,"00:13:27,940","00:13:33,310",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,"from the background, in order to figure"
cs-410_1_1_214,cs-410,1,1, Natural Language Content Analysis,"00:15:59,270","00:16:05,965",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=959,"the distribution as follows: 5% is for topic selection,"
cs-410_1_1_215,cs-410,1,1, Natural Language Content Analysis,"00:13:33,310","00:13:37,730",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,So although the sentence looks very
cs-410_1_1_215,cs-410,1,1, Natural Language Content Analysis,"00:16:05,965","00:16:07,760",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=965,"5% is for proposal,"
cs-410_1_1_216,cs-410,1,1, Natural Language Content Analysis,"00:13:37,730","00:13:42,380",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,And in cases when the sentence is
cs-410_1_1_216,cs-410,1,1, Natural Language Content Analysis,"00:16:07,760","00:16:11,280",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,"and another 5% for progress report,"
cs-410_1_1_217,cs-410,1,1, Natural Language Content Analysis,"00:13:42,380","00:13:46,760",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,"five prepositional phrases, and there"
cs-410_1_1_217,cs-410,1,1, Natural Language Content Analysis,"00:16:11,280","00:16:20,080",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,"65% in most of the grade of the course project will be based on software deposit,"
cs-410_1_1_218,cs-410,1,1, Natural Language Content Analysis,"00:13:48,580","00:13:51,650",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,It's also harder to do precise
cs-410_1_1_218,cs-410,1,1, Natural Language Content Analysis,"00:16:20,080","00:16:23,885",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,your deliverable for the project.
cs-410_1_1_219,cs-410,1,1, Natural Language Content Analysis,"00:13:51,650","00:13:53,410",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=831,So here's an example.
cs-410_1_1_219,cs-410,1,1, Natural Language Content Analysis,"00:16:23,885","00:16:28,495",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=983,"And finally, 20% is based on the tutorial presentation."
cs-410_1_1_220,cs-410,1,1, Natural Language Content Analysis,"00:13:53,410","00:14:00,108",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=833,"In the sentence ""John owns a restaurant."""
cs-410_1_1_220,cs-410,1,1, Natural Language Content Analysis,"00:16:28,495","00:16:34,610",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=988,"Now, most of these components in the course project will be graded based on completion."
cs-410_1_1_221,cs-410,1,1, Natural Language Content Analysis,"00:14:00,108","00:14:05,340",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,"The word own,"
cs-410_1_1_221,cs-410,1,1, Natural Language Content Analysis,"00:16:34,610","00:16:38,000",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=994,"Indeed, a lot of tasks you see in programming assignments are graded in"
cs-410_1_1_222,cs-410,1,1, Natural Language Content Analysis,"00:14:05,340","00:14:10,210",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=845,it's very hard to precisely describe
cs-410_1_1_222,cs-410,1,1, Natural Language Content Analysis,"00:16:38,000","00:16:41,710",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=998,the same way and that is because we believe we have
cs-410_1_1_223,cs-410,1,1, Natural Language Content Analysis,"00:14:11,430","00:14:16,467",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,So as a result we have a robust and
cs-410_1_1_223,cs-410,1,1, Natural Language Content Analysis,"00:16:41,710","00:16:46,955",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1001,designed the program assignments and course project in such a way
cs-410_1_1_224,cs-410,1,1, Natural Language Content Analysis,"00:14:16,467","00:14:20,860",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=856,Natural Language Processing techniques
cs-410_1_1_224,cs-410,1,1, Natural Language Content Analysis,"00:16:46,955","00:16:54,405",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1006,that you will be able to learn a lot by simply going through these tasks.
cs-410_1_1_225,cs-410,1,1, Natural Language Content Analysis,"00:14:22,490","00:14:25,640",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,"In a shallow way,"
cs-410_1_1_225,cs-410,1,1, Natural Language Content Analysis,"00:16:54,405","00:16:57,590",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1014,"So, this is the effective way of learning by"
cs-410_1_1_226,cs-410,1,1, Natural Language Content Analysis,"00:14:25,640","00:14:33,600",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,"For example, parts of speech tagging or a"
cs-410_1_1_226,cs-410,1,1, Natural Language Content Analysis,"00:16:57,590","00:17:02,885",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1017,doing and that's why we choose to grade based on completion.
cs-410_1_1_227,cs-410,1,1, Natural Language Content Analysis,"00:14:33,600","00:14:35,520",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,"And those are not deep understanding,"
cs-410_1_1_227,cs-410,1,1, Natural Language Content Analysis,"00:17:02,885","00:17:07,070",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1022,"That also means you have a lot of control over these grades,"
cs-410_1_1_228,cs-410,1,1, Natural Language Content Analysis,"00:14:35,520","00:14:39,419",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,because we're not really understanding
cs-410_1_1_228,cs-410,1,1, Natural Language Content Analysis,"00:17:07,070","00:17:10,100",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1027,because you can ensure that you finish
cs-410_1_1_229,cs-410,1,1, Natural Language Content Analysis,"00:14:41,270","00:14:45,170",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=881,On the other hand of the deep
cs-410_1_1_229,cs-410,1,1, Natural Language Content Analysis,"00:17:10,100","00:17:15,265",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1030,all these tasks and then you will get most of these grades.
cs-410_1_1_230,cs-410,1,1, Natural Language Content Analysis,"00:14:45,170","00:14:50,840",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=885,"up well, meaning that they would"
cs-410_1_1_230,cs-410,1,1, Natural Language Content Analysis,"00:17:15,265","00:17:21,660",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,The only part that will be based on the quality of solution is the tutorial presentation.
cs-410_1_1_231,cs-410,1,1, Natural Language Content Analysis,"00:14:50,840","00:14:54,850",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,And if you don't restrict
cs-410_1_1_231,cs-410,1,1, Natural Language Content Analysis,"00:17:21,660","00:17:24,110",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1041,This is actually not just based on the quality of
cs-410_1_1_232,cs-410,1,1, Natural Language Content Analysis,"00:14:54,850","00:14:59,750",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,"the use of words, then these"
cs-410_1_1_232,cs-410,1,1, Natural Language Content Analysis,"00:17:24,110","00:17:27,020",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1044,your tutorial presenting but rather based on whether
cs-410_1_1_233,cs-410,1,1, Natural Language Content Analysis,"00:14:59,750","00:15:04,310",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,They may work well based on machine
cs-410_1_1_233,cs-410,1,1, Natural Language Content Analysis,"00:17:27,020","00:17:32,930",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1047,you are software actually work as you propose.
cs-410_1_1_234,cs-410,1,1, Natural Language Content Analysis,"00:15:04,310","00:15:08,520",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,that are similar to the training data
cs-410_1_1_234,cs-410,1,1, Natural Language Content Analysis,"00:17:32,930","00:17:36,403",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1052,"So, does it provide all the functions that you propose?"
cs-410_1_1_235,cs-410,1,1, Natural Language Content Analysis,"00:15:08,520","00:15:13,090",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=908,But they generally wouldn't work well on
cs-410_1_1_235,cs-410,1,1, Natural Language Content Analysis,"00:17:36,403","00:17:38,110",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1056,Does it really work?
cs-410_1_1_236,cs-410,1,1, Natural Language Content Analysis,"00:15:13,090","00:15:14,290",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,the training data.
cs-410_1_1_236,cs-410,1,1, Natural Language Content Analysis,"00:17:38,110","00:17:45,415",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1058,"And this will likely affect some of the grade for tutorial presentation,"
cs-410_1_1_237,cs-410,1,1, Natural Language Content Analysis,"00:15:14,290","00:15:19,150",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=914,So this pretty much summarizes the state
cs-410_1_1_237,cs-410,1,1, Natural Language Content Analysis,"00:17:45,415","00:17:50,405",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1065,"meaning that if a software for some reason doesn't really pass our test,"
cs-410_1_1_238,cs-410,1,1, Natural Language Content Analysis,"00:15:19,150","00:15:23,590",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=919,"Of course, within such a short amount"
cs-410_1_1_238,cs-410,1,1, Natural Language Content Analysis,"00:17:50,405","00:17:54,495",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1070,then we would deduct the points from here.
cs-410_1_1_239,cs-410,1,1, Natural Language Content Analysis,"00:15:23,590","00:15:27,120",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,"a complete view of NLP,"
cs-410_1_1_239,cs-410,1,1, Natural Language Content Analysis,"00:17:54,495","00:18:00,945",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1074,"As I said, the technology review is not actually contributing to you grade,"
cs-410_1_1_240,cs-410,1,1, Natural Language Content Analysis,"00:15:27,120","00:15:35,896",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=927,And I'd expect to see multiple courses on
cs-410_1_1_240,cs-410,1,1, Natural Language Content Analysis,"00:18:00,945","00:18:06,890",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1080,but it's a metric component and it will be graded based on completion.
cs-410_1_1_241,cs-410,1,1, Natural Language Content Analysis,"00:15:35,896","00:15:40,960",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=935,But because of its relevance to the topic
cs-410_1_1_241,cs-410,1,1, Natural Language Content Analysis,"00:18:07,050","00:18:12,260",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1087,We also provide up to 5% of
cs-410_1_1_242,cs-410,1,1, Natural Language Content Analysis,"00:15:40,960","00:15:45,410",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=940,you to know the background in case
cs-410_1_1_242,cs-410,1,1, Natural Language Content Analysis,"00:18:12,260","00:18:17,135",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1092,extra credit based on your participation in the forum discussions.
cs-410_1_1_243,cs-410,1,1, Natural Language Content Analysis,"00:15:45,410","00:15:47,340",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=945,So what does that mean for Text Retrieval?
cs-410_1_1_243,cs-410,1,1, Natural Language Content Analysis,"00:18:17,135","00:18:21,680",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1097,And this is to encourage you to help
cs-410_1_1_244,cs-410,1,1, Natural Language Content Analysis,"00:15:48,980","00:15:53,254",244,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,"Well, in Text Retrieval we"
cs-410_1_1_244,cs-410,1,1, Natural Language Content Analysis,"00:18:21,680","00:18:26,715",244,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1101,each other and particularly by answering questions posed by others.
cs-410_1_1_245,cs-410,1,1, Natural Language Content Analysis,"00:15:53,254","00:15:56,470",245,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,It's very hard to restrict
cs-410_1_1_245,cs-410,1,1, Natural Language Content Analysis,"00:18:26,715","00:18:33,890",245,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1106,"We will be able to use log data from the forum to give you credit,"
cs-410_1_1_246,cs-410,1,1, Natural Language Content Analysis,"00:15:56,470","00:16:00,092",246,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=956,And we also are often dealing
cs-410_1_1_246,cs-410,1,1, Natural Language Content Analysis,"00:18:33,890","00:18:41,255",246,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1113,"extra credit, and these extra credit points will be actually added to the regular points."
cs-410_1_1_247,cs-410,1,1, Natural Language Content Analysis,"00:16:00,092","00:16:06,730",247,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=960,So that means The NLP techniques must
cs-410_1_1_247,cs-410,1,1, Natural Language Content Analysis,"00:18:41,255","00:18:43,490",247,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1121,So that will allow you to actually increase
cs-410_1_1_248,cs-410,1,1, Natural Language Content Analysis,"00:16:06,730","00:16:12,060",248,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=966,And that just implies today we can only
cs-410_1_1_248,cs-410,1,1, Natural Language Content Analysis,"00:18:43,490","00:18:49,270",248,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,your grade as will be shown here in more detail.
cs-410_1_1_249,cs-410,1,1, Natural Language Content Analysis,"00:16:12,060","00:16:13,550",249,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=972,text retrieval.
cs-410_1_1_249,cs-410,1,1, Natural Language Content Analysis,"00:18:49,270","00:18:53,375",249,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1129,And this is how we are going to determine your final letter grades.
cs-410_1_1_250,cs-410,1,1, Natural Language Content Analysis,"00:16:13,550","00:16:14,780",250,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=973,"In fact,"
cs-410_1_1_250,cs-410,1,1, Natural Language Content Analysis,"00:18:53,375","00:18:57,980",250,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1133,They are based on the grade points you have collected over
cs-410_1_1_251,cs-410,1,1, Natural Language Content Analysis,"00:16:14,780","00:16:19,070",251,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=974,most search engines today use something
cs-410_1_1_251,cs-410,1,1, Natural Language Content Analysis,"00:18:57,980","00:19:03,100",251,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1137,the semester using this map shown here on the slide.
cs-410_1_1_252,cs-410,1,1, Natural Language Content Analysis,"00:16:20,740","00:16:25,450",252,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,"Now, this is probably the simplest"
cs-410_1_1_252,cs-410,1,1, Natural Language Content Analysis,"00:19:03,100","00:19:09,080",252,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1143,Mostly it's five points for each bracket but not always.
cs-410_1_1_253,cs-410,1,1, Natural Language Content Analysis,"00:16:25,450","00:16:29,250",253,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=985,That is to turn text data
cs-410_1_1_253,cs-410,1,1, Natural Language Content Analysis,"00:19:09,080","00:19:11,825",253,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1149,"And as you can see,"
cs-410_1_1_254,cs-410,1,1, Natural Language Content Analysis,"00:16:29,250","00:16:33,930",254,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=989,"Meaning we'll keep individual words, but"
cs-410_1_1_254,cs-410,1,1, Natural Language Content Analysis,"00:19:11,825","00:19:18,980",254,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1151,"if you have earned 5% extra credit and when adding this extra credit,"
cs-410_1_1_255,cs-410,1,1, Natural Language Content Analysis,"00:16:33,930","00:16:37,660",255,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=993,And we'll keep duplicated
cs-410_1_1_255,cs-410,1,1, Natural Language Content Analysis,"00:19:18,980","00:19:23,540",255,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1158,"will likely help you move your grades up by one bracket,"
cs-410_1_1_256,cs-410,1,1, Natural Language Content Analysis,"00:16:37,660","00:16:39,950",256,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=997,So this is called a bag
cs-410_1_1_256,cs-410,1,1, Natural Language Content Analysis,"00:19:23,540","00:19:28,020",256,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1163,although it won't be more than one bracket.
cs-410_1_1_257,cs-410,1,1, Natural Language Content Analysis,"00:16:39,950","00:16:45,990",257,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=999,"When you represent text in this way,"
cs-410_1_1_257,cs-410,1,1, Natural Language Content Analysis,"00:19:28,020","00:19:33,405",257,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1168,"So, we feel that this fixed mapping would give you complete control over your grade,"
cs-410_1_1_258,cs-410,1,1, Natural Language Content Analysis,"00:16:45,990","00:16:51,020",258,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,That just makes it harder to understand
cs-410_1_1_258,cs-410,1,1, Natural Language Content Analysis,"00:19:33,405","00:19:37,265",258,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1173,so you can monitor your grade over the semester and kind of assess
cs-410_1_1_259,cs-410,1,1, Natural Language Content Analysis,"00:16:51,020","00:16:52,440",259,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1011,because we've lost the order.
cs-410_1_1_259,cs-410,1,1, Natural Language Content Analysis,"00:19:37,265","00:19:42,590",259,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1177,your progress and also to adjust your schedule accordingly.
cs-410_1_1_260,cs-410,1,1, Natural Language Content Analysis,"00:16:53,870","00:16:57,320",260,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,But yet this representation tends
cs-410_1_1_260,cs-410,1,1, Natural Language Content Analysis,"00:19:42,590","00:19:46,845",260,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1182,"So, this is a visualization of your workload."
cs-410_1_1_261,cs-410,1,1, Natural Language Content Analysis,"00:16:57,320","00:16:59,150",261,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1017,most search tasks.
cs-410_1_1_261,cs-410,1,1, Natural Language Content Analysis,"00:19:46,845","00:19:49,190",261,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1186,"Horizontally, we show the timeline,"
cs-410_1_1_262,cs-410,1,1, Natural Language Content Analysis,"00:16:59,150","00:17:03,450",262,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1019,And this was partly because the search
cs-410_1_1_262,cs-410,1,1, Natural Language Content Analysis,"00:19:49,190","00:19:55,350",262,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1189,from the first day of instruction to the last day of instruction.
cs-410_1_1_263,cs-410,1,1, Natural Language Content Analysis,"00:17:03,450","00:17:08,230",263,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1023,If you see matching of some of
cs-410_1_1_263,cs-410,1,1, Natural Language Content Analysis,"00:19:55,350","00:20:01,730",263,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1195,"And then vertically, you can see there are mainly six tasks for you over the semester."
cs-410_1_1_264,cs-410,1,1, Natural Language Content Analysis,"00:17:08,230","00:17:12,560",264,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1028,chances are that that document is about
cs-410_1_1_264,cs-410,1,1, Natural Language Content Analysis,"00:20:01,730","00:20:08,105",264,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1201,"First, you will spend most of your time on watching the lecture videos,"
cs-410_1_1_265,cs-410,1,1, Natural Language Content Analysis,"00:17:13,670","00:17:15,775",265,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1033,"So in comparison of some other tasks, for"
cs-410_1_1_265,cs-410,1,1, Natural Language Content Analysis,"00:20:08,105","00:20:11,810",265,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1208,and this is why we have a thick black line
cs-410_1_1_266,cs-410,1,1, Natural Language Content Analysis,"00:17:15,775","00:17:20,490",266,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,"example, machine translation would require"
cs-410_1_1_266,cs-410,1,1, Natural Language Content Analysis,"00:20:11,810","00:20:14,810",266,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1211,there that shows that most of
cs-410_1_1_267,cs-410,1,1, Natural Language Content Analysis,"00:17:20,490","00:17:22,680",267,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1040,Otherwise the translation would be wrong.
cs-410_1_1_267,cs-410,1,1, Natural Language Content Analysis,"00:20:14,810","00:20:18,605",267,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1214,your effort probably will be spent on watching lecture videos.
cs-410_1_1_268,cs-410,1,1, Natural Language Content Analysis,"00:17:22,680","00:17:25,780",268,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1042,So in comparison such tasks
cs-410_1_1_268,cs-410,1,1, Natural Language Content Analysis,"00:20:18,605","00:20:22,230",268,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1218,"And then, you will be taking 12 quizzes,"
cs-410_1_1_269,cs-410,1,1, Natural Language Content Analysis,"00:17:25,780","00:17:30,670",269,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1045,Such a representation is often sufficient
cs-410_1_1_269,cs-410,1,1, Natural Language Content Analysis,"00:20:22,230","00:20:28,400",269,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1222,"and these will be spreading over most of the semester,"
cs-410_1_1_270,cs-410,1,1, Natural Language Content Analysis,"00:17:30,670","00:17:34,050",270,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1050,"the major search engines today,"
cs-410_1_1_270,cs-410,1,1, Natural Language Content Analysis,"00:20:28,400","00:20:36,665",270,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1228,corresponding to about 12 weeks when you are expected to watch those lecture videos.
cs-410_1_1_271,cs-410,1,1, Natural Language Content Analysis,"00:17:35,770","00:17:40,240",271,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1055,"Of course, I put in parentheses but"
cs-410_1_1_271,cs-410,1,1, Natural Language Content Analysis,"00:20:36,665","00:20:41,900",271,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1236,"Now, we want to emphasize that you should spend most of your time to"
cs-410_1_1_272,cs-410,1,1, Natural Language Content Analysis,"00:17:40,240","00:17:42,750",272,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1060,that are not answered well by
cs-410_1_1_272,cs-410,1,1, Natural Language Content Analysis,"00:20:41,900","00:20:47,636",272,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1241,"watch videos and make sure you understand the materials before you take quizzes,"
cs-410_1_1_273,cs-410,1,1, Natural Language Content Analysis,"00:17:42,750","00:17:48,320",273,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1062,they do require the replantation that
cs-410_1_1_273,cs-410,1,1, Natural Language Content Analysis,"00:20:47,636","00:20:51,455",273,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1247,and this will ensure that you don't leave any holes.
cs-410_1_1_274,cs-410,1,1, Natural Language Content Analysis,"00:17:48,320","00:17:51,900",274,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1068,That would require more natural
cs-410_1_1_274,cs-410,1,1, Natural Language Content Analysis,"00:20:51,455","00:20:57,415",274,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1251,"If you do it the other way and by using quizzes to guide you through the lecture videos,"
cs-410_1_1_275,cs-410,1,1, Natural Language Content Analysis,"00:17:52,950","00:17:56,600",275,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1072,There was another reason why we
cs-410_1_1_275,cs-410,1,1, Natural Language Content Analysis,"00:20:57,415","00:20:59,720",275,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1257,"then you might leave some holes,"
cs-410_1_1_276,cs-410,1,1, Natural Language Content Analysis,"00:17:56,600","00:17:59,100",276,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1076,NLP techniques in modern search engines.
cs-410_1_1_276,cs-410,1,1, Natural Language Content Analysis,"00:20:59,720","00:21:04,075",276,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1259,and that might hurt your performance on exams.
cs-410_1_1_277,cs-410,1,1, Natural Language Content Analysis,"00:17:59,100","00:18:02,460",277,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1079,And that's because some
cs-410_1_1_277,cs-410,1,1, Natural Language Content Analysis,"00:21:04,075","00:21:07,670",277,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1264,There will be two proctored exams that will be
cs-410_1_1_278,cs-410,1,1, Natural Language Content Analysis,"00:18:02,460","00:18:05,400",278,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1082,naturally solved the problem of NLP.
cs-410_1_1_278,cs-410,1,1, Natural Language Content Analysis,"00:21:07,670","00:21:13,190",278,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1267,"given in the middle of the semester and also later in the semester,"
cs-410_1_1_279,cs-410,1,1, Natural Language Content Analysis,"00:18:05,400","00:18:09,240",279,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1085,So one example is word
cs-410_1_1_279,cs-410,1,1, Natural Language Content Analysis,"00:21:13,190","00:21:16,635",279,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1273,at the end of the two box.
cs-410_1_1_280,cs-410,1,1, Natural Language Content Analysis,"00:18:09,240","00:18:11,060",280,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1089,Think about a word like Java.
cs-410_1_1_280,cs-410,1,1, Natural Language Content Analysis,"00:21:16,635","00:21:19,640",280,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1276,"So, this is your third task."
cs-410_1_1_281,cs-410,1,1, Natural Language Content Analysis,"00:18:11,060","00:18:13,900",281,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1091,It could mean coffee or
cs-410_1_1_281,cs-410,1,1, Natural Language Content Analysis,"00:21:19,640","00:21:21,845",281,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1279,"The fourth task is programming assignments,"
cs-410_1_1_282,cs-410,1,1, Natural Language Content Analysis,"00:18:15,090","00:18:18,230",282,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1095,"If you look at the word anome,"
cs-410_1_1_282,cs-410,1,1, Natural Language Content Analysis,"00:21:21,845","00:21:23,600",282,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1281,"and this will be,"
cs-410_1_1_283,cs-410,1,1, Natural Language Content Analysis,"00:18:18,230","00:18:23,050",283,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1098,"when the user uses the word in the query,"
cs-410_1_1_283,cs-410,1,1, Natural Language Content Analysis,"00:21:23,600","00:21:27,690",283,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1283,"again, through the entire semester,"
cs-410_1_1_284,cs-410,1,1, Natural Language Content Analysis,"00:18:23,050","00:18:26,240",284,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1103,"For example, I'm looking for"
cs-410_1_1_284,cs-410,1,1, Natural Language Content Analysis,"00:21:27,690","00:21:33,040",284,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1287,"actually not really all the weeks in the semester,"
cs-410_1_1_285,cs-410,1,1, Natural Language Content Analysis,"00:18:26,240","00:18:31,990",285,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1106,"When I have applet there,"
cs-410_1_1_285,cs-410,1,1, Natural Language Content Analysis,"00:21:33,040","00:21:35,352",285,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1293,"but most of the weeks,"
cs-410_1_1_286,cs-410,1,1, Natural Language Content Analysis,"00:18:31,990","00:18:36,360",286,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1111,And that contest can help us
cs-410_1_1_286,cs-410,1,1, Natural Language Content Analysis,"00:21:35,352","00:21:40,340",286,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1295,"because we don't want you to use up the last two weeks,"
cs-410_1_1_287,cs-410,1,1, Natural Language Content Analysis,"00:18:36,360","00:18:39,690",287,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1116,which Java is referring
cs-410_1_1_287,cs-410,1,1, Natural Language Content Analysis,"00:21:40,340","00:21:43,920",287,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1300,which we reserve for you to work on course project.
cs-410_1_1_288,cs-410,1,1, Natural Language Content Analysis,"00:18:39,690","00:18:43,710",288,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1119,Because those documents would
cs-410_1_1_288,cs-410,1,1, Natural Language Content Analysis,"00:21:43,920","00:21:47,805",288,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1303,And the course project is the fifth component.
cs-410_1_1_289,cs-410,1,1, Natural Language Content Analysis,"00:18:43,710","00:18:48,560",289,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,If Java occurs in that
cs-410_1_1_289,cs-410,1,1, Natural Language Content Analysis,"00:21:47,805","00:21:51,983",289,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1307,"And finally, you need to finish technology review."
cs-410_1_1_290,cs-410,1,1, Natural Language Content Analysis,"00:18:48,560","00:18:52,960",290,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1128,then you would never match applet or
cs-410_1_1_290,cs-410,1,1, Natural Language Content Analysis,"00:21:51,983","00:21:54,410",290,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1311,"Now, although most of the work of"
cs-410_1_1_291,cs-410,1,1, Natural Language Content Analysis,"00:18:52,960","00:18:56,250",291,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1132,So this is the case when
cs-410_1_1_291,cs-410,1,1, Natural Language Content Analysis,"00:21:54,410","00:21:58,485",291,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1314,"the project is expected to be done at the end of the semester,"
cs-410_1_1_292,cs-410,1,1, Natural Language Content Analysis,"00:18:56,250","00:18:58,580",292,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1136,naturally achieve the goal of word.
cs-410_1_1_292,cs-410,1,1, Natural Language Content Analysis,"00:21:58,485","00:22:02,280",292,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1318,you will actually start working on it from the very beginning.
cs-410_1_1_293,cs-410,1,1, Natural Language Content Analysis,"00:19:01,530","00:19:05,920",293,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1141,Another example is some technique called
cs-410_1_1_293,cs-410,1,1, Natural Language Content Analysis,"00:22:02,280","00:22:04,790",293,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1322,"And, soon after the semester starts,"
cs-410_1_1_294,cs-410,1,1, Natural Language Content Analysis,"00:19:05,920","00:19:11,360",294,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1145,feedback which we will talk about
cs-410_1_1_294,cs-410,1,1, Natural Language Content Analysis,"00:22:04,790","00:22:08,970",294,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1324,we will ask you to think about the topics and you will discuss
cs-410_1_1_295,cs-410,1,1, Natural Language Content Analysis,"00:19:11,360","00:19:16,938",295,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1151,This technique would allow us to add
cs-410_1_1_295,cs-410,1,1, Natural Language Content Analysis,"00:22:08,970","00:22:13,985",295,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1328,the topics and then form teams to submit the proposal.
cs-410_1_1_296,cs-410,1,1, Natural Language Content Analysis,"00:19:16,938","00:19:21,859",296,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1156,those additional words could
cs-410_1_1_296,cs-410,1,1, Natural Language Content Analysis,"00:22:13,985","00:22:17,645",296,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1333,"But of course, during the first 12 weeks,"
cs-410_1_1_297,cs-410,1,1, Natural Language Content Analysis,"00:19:21,859","00:19:26,155",297,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1161,And these words can help matching
cs-410_1_1_297,cs-410,1,1, Natural Language Content Analysis,"00:22:17,645","00:22:21,425",297,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1337,"you will be most occupied by the lecture videos, quizzes,"
cs-410_1_1_298,cs-410,1,1, Natural Language Content Analysis,"00:19:26,155","00:19:27,680",298,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1166,have not occurred.
cs-410_1_1_298,cs-410,1,1, Natural Language Content Analysis,"00:22:21,425","00:22:25,704",298,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1341,and your assignments for programming.
cs-410_1_1_299,cs-410,1,1, Natural Language Content Analysis,"00:19:27,680","00:19:32,500",299,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1167,"So this achieves, to some extent,"
cs-410_1_1_299,cs-410,1,1, Natural Language Content Analysis,"00:22:25,704","00:22:34,103",299,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1345,"So, that's why you will likely have more time to work on the project in the end."
cs-410_1_1_300,cs-410,1,1, Natural Language Content Analysis,"00:19:32,500","00:19:35,350",300,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1172,So those techniques also helped us
cs-410_1_1_300,cs-410,1,1, Natural Language Content Analysis,"00:22:34,103","00:22:40,820",300,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1354,The technology review is kind of designed to extend your knowledge based on your project.
cs-410_1_1_301,cs-410,1,1, Natural Language Content Analysis,"00:19:35,350","00:19:38,890",301,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1175,bypass some of the difficulties
cs-410_1_1_301,cs-410,1,1, Natural Language Content Analysis,"00:22:40,820","00:22:43,760",301,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1360,"But of course, you have the complete freedom"
cs-410_1_1_302,cs-410,1,1, Natural Language Content Analysis,"00:19:40,530","00:19:43,920",302,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1180,"However, in the long run we still need"
cs-410_1_1_302,cs-410,1,1, Natural Language Content Analysis,"00:22:43,760","00:22:46,085",302,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1363,to choose whatever topic that you want to review.
cs-410_1_1_303,cs-410,1,1, Natural Language Content Analysis,"00:19:43,920","00:19:47,280",303,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1183,techniques in order to improve the
cs-410_1_1_303,cs-410,1,1, Natural Language Content Analysis,"00:22:46,085","00:22:49,978",303,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1366,"So, you don't have to tie it to the project."
cs-410_1_1_304,cs-410,1,1, Natural Language Content Analysis,"00:19:47,280","00:19:50,939",304,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1187,And it's particularly needed for
cs-410_1_1_304,cs-410,1,1, Natural Language Content Analysis,"00:22:49,978","00:22:53,075",304,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1369,"And so, we imagine that you would start"
cs-410_1_1_305,cs-410,1,1, Natural Language Content Analysis,"00:19:52,160","00:19:53,390",305,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1192,Or for question and answering.
cs-410_1_1_305,cs-410,1,1, Natural Language Content Analysis,"00:22:53,075","00:22:56,865",305,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1373,"working on it a little bit after you have decided the project,"
cs-410_1_1_306,cs-410,1,1, Natural Language Content Analysis,"00:19:55,310","00:20:00,540",306,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1195,Google has recently launched a knowledge
cs-410_1_1_306,cs-410,1,1, Natural Language Content Analysis,"00:22:56,865","00:22:59,570",306,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1376,"the topic, so that you can decide whether you want to tie"
cs-410_1_1_307,cs-410,1,1, Natural Language Content Analysis,"00:20:00,540","00:20:05,220",307,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1200,"that goal, because knowledge graph would"
cs-410_1_1_307,cs-410,1,1, Natural Language Content Analysis,"00:22:59,570","00:23:03,552",307,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1379,the technology review with your project.
cs-410_1_1_308,cs-410,1,1, Natural Language Content Analysis,"00:20:05,220","00:20:09,170",308,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1205,And this goes beyond the simple
cs-410_1_1_308,cs-410,1,1, Natural Language Content Analysis,"00:23:03,552","00:23:07,530",308,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1383,"So, it's good to keep this picture in mind throughout"
cs-410_1_1_309,cs-410,1,1, Natural Language Content Analysis,"00:20:09,170","00:20:12,950",309,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1209,And such technique should help us
cs-410_1_1_309,cs-410,1,1, Natural Language Content Analysis,"00:23:07,530","00:23:13,470",309,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1387,the semester because you might have irregular schedule sometimes.
cs-410_1_1_310,cs-410,1,1, Natural Language Content Analysis,"00:20:14,180","00:20:19,220",310,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1214,"significantly, although this is the open"
cs-410_1_1_310,cs-410,1,1, Natural Language Content Analysis,"00:23:13,470","00:23:18,500",310,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1393,"For example, you might find that you are very busy in the middle of the semester,"
cs-410_1_1_311,cs-410,1,1, Natural Language Content Analysis,"00:20:19,220","00:20:24,990",311,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1219,"In sum, in this lecture we"
cs-410_1_1_311,cs-410,1,1, Natural Language Content Analysis,"00:23:18,500","00:23:23,700",311,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1398,then this picture can help you adjust your schedule and the degree you
cs-410_1_1_312,cs-410,1,1, Natural Language Content Analysis,"00:20:24,990","00:20:27,820",312,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1224,we've talked about the state
cs-410_1_1_312,cs-410,1,1, Natural Language Content Analysis,"00:23:23,700","00:23:29,325",312,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1403,"probably want to then work more on some of the tasks at the beginning of the semester,"
cs-410_1_1_313,cs-410,1,1, Natural Language Content Analysis,"00:20:27,820","00:20:30,550",313,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1227,"What we can do, what we cannot do."
cs-410_1_1_313,cs-410,1,1, Natural Language Content Analysis,"00:23:29,325","00:23:33,563",313,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1409,so that you don't overwhelm yourself in the middle of the semester.
cs-410_1_1_314,cs-410,1,1, Natural Language Content Analysis,"00:20:30,550","00:20:34,510",314,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1230,"And finally, we also explain why"
cs-410_1_1_314,cs-410,1,1, Natural Language Content Analysis,"00:23:33,563","00:23:38,145",314,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1413,"And similarly, if you expect to be very busy the end of the semester,"
cs-410_1_1_315,cs-410,1,1, Natural Language Content Analysis,"00:20:34,510","00:20:38,290",315,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1234,remains the dominant replantation
cs-410_1_1_315,cs-410,1,1, Natural Language Content Analysis,"00:23:38,145","00:23:43,030",315,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1418,you might want to start working on the project much earlier.
cs-410_1_1_316,cs-410,1,1, Natural Language Content Analysis,"00:20:38,290","00:20:43,258",316,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1238,even though deeper NLP would be needed for
cs-410_1_1_316,cs-410,1,1, Natural Language Content Analysis,"00:23:43,030","00:23:47,100",316,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1423,"So, I hope this picture will be always in your mind,"
cs-410_1_1_317,cs-410,1,1, Natural Language Content Analysis,"00:20:43,258","00:20:46,470",317,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1243,"If you want to know more, you can take"
cs-410_1_1_317,cs-410,1,1, Natural Language Content Analysis,"00:23:47,100","00:23:51,885",317,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1427,and you will be able to adjust your schedule accordingly
cs-410_1_1_318,cs-410,1,1, Natural Language Content Analysis,"00:20:46,470","00:20:49,070",318,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1246,I only cited one here and
cs-410_1_1_318,cs-410,1,1, Natural Language Content Analysis,"00:23:51,885","00:23:56,955",318,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1431,"and to particularly work on tasks proactively,"
cs-410_1_1_319,cs-410,1,1, Natural Language Content Analysis,"00:20:49,070","00:20:52,976",319,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1249,Thanks.
cs-410_1_1_319,cs-410,1,1, Natural Language Content Analysis,"00:23:56,955","00:23:59,820",319,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1436,in case you anticipate any busy time period.
cs-410_1_1_320,cs-410,1,1, Natural Language Content Analysis,"00:20:52,976","00:21:02,976",320,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1252,[MUSIC]
cs-410_1_1_320,cs-410,1,1, Natural Language Content Analysis,"00:23:59,820","00:24:07,620",320,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1439,Some of you have already taken maybe both MOOCs.
cs-410_1_1_321,cs-410,1,1, Natural Language Content Analysis,"00:24:07,620","00:24:10,140",321,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1447,"Now, if you are one of them,"
cs-410_1_1_322,cs-410,1,1, Natural Language Content Analysis,"00:24:10,140","00:24:12,910",322,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1450,"then I think naturally,"
cs-410_1_1_323,cs-410,1,1, Natural Language Content Analysis,"00:24:12,910","00:24:18,907",323,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1452,you will have more time to work on other problems in this course.
cs-410_1_1_324,cs-410,1,1, Natural Language Content Analysis,"00:24:18,907","00:24:22,425",324,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1458,"So, you should take advantage of this"
cs-410_1_1_325,cs-410,1,1, Natural Language Content Analysis,"00:24:22,425","00:24:28,755",325,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1462,to proactively finish some of the tasks much more quickly.
cs-410_1_1_326,cs-410,1,1, Natural Language Content Analysis,"00:24:28,755","00:24:34,680",326,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1468,"In particular, since you have already watched those videos and then you can"
cs-410_1_1_327,cs-410,1,1, Natural Language Content Analysis,"00:24:34,680","00:24:37,500",327,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1474,simply review them and then you try to work on the quizzes
cs-410_1_1_328,cs-410,1,1, Natural Language Content Analysis,"00:24:37,500","00:24:41,130",328,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1477,so that you can finish all the quizzes much more quickly.
cs-410_1_1_329,cs-410,1,1, Natural Language Content Analysis,"00:24:41,130","00:24:45,410",329,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1481,You may be able to finish most of the programming assignments quickly too.
cs-410_1_1_330,cs-410,1,1, Natural Language Content Analysis,"00:24:45,410","00:24:49,710",330,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1485,This is not only going to be helpful for you
cs-410_1_1_331,cs-410,1,1, Natural Language Content Analysis,"00:24:49,710","00:24:54,480",331,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1489,"in the sense that you will have a lot of time to work on course project, but also,"
cs-410_1_1_332,cs-410,1,1, Natural Language Content Analysis,"00:24:54,480","00:24:59,070",332,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1494,it would allow you to help others by answering
cs-410_1_1_333,cs-410,1,1, Natural Language Content Analysis,"00:24:59,070","00:25:04,925",333,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1499,"their questions on forums or helping them in other way,"
cs-410_1_1_334,cs-410,1,1, Natural Language Content Analysis,"00:25:04,925","00:25:09,145",334,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1504,like teaming up with them to work on the project.
cs-410_1_1_335,cs-410,1,1, Natural Language Content Analysis,"00:25:09,145","00:25:13,890",335,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1509,"However, I should also say that there are a few tasks that are,"
cs-410_1_1_336,cs-410,1,1, Natural Language Content Analysis,"00:25:13,890","00:25:20,700",336,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1513,"unfortunately, have very fixed time that you cannot really work on in advance."
cs-410_1_1_337,cs-410,1,1, Natural Language Content Analysis,"00:25:20,700","00:25:23,640",337,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1520,"For example, the two exams will be scheduled on"
cs-410_1_1_338,cs-410,1,1, Natural Language Content Analysis,"00:25:23,640","00:25:31,690",338,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1523,some particular dates that would have no flexibility for you to work on earlier.
cs-410_1_1_339,cs-410,1,1, Natural Language Content Analysis,"00:25:31,690","00:25:35,970",339,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1531,There may be some tasks in the programming assignments that have to be synchronized.
cs-410_1_1_340,cs-410,1,1, Natural Language Content Analysis,"00:25:35,970","00:25:39,605",340,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1535,"For example, we might run competition of"
cs-410_1_1_341,cs-410,1,1, Natural Language Content Analysis,"00:25:39,605","00:25:44,805",341,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1539,"some tasks and there may be some synchronization that's needed,"
cs-410_1_1_342,cs-410,1,1, Natural Language Content Analysis,"00:25:44,805","00:25:51,360",342,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1544,but we would like to minimize the dependency so that you
cs-410_1_1_343,cs-410,1,1, Natural Language Content Analysis,"00:25:51,360","00:25:59,541",343,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1551,could hopefully work on many of those tasks as early as you can.
cs-410_1_1_344,cs-410,1,1, Natural Language Content Analysis,"00:25:59,541","00:26:04,070",344,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1559,"And of course, we encourage you to use more time to finish"
cs-410_1_1_345,cs-410,1,1, Natural Language Content Analysis,"00:26:04,070","00:26:08,700",345,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1564,a more challenging course project or to finish
cs-410_1_1_346,cs-410,1,1, Natural Language Content Analysis,"00:26:08,700","00:26:15,475",346,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1568,a higher quality technology review.
cs-410_1_1_347,cs-410,1,1, Natural Language Content Analysis,"00:26:15,475","00:26:20,685",347,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1575,"Now, we rely a lot on forum discussion,"
cs-410_1_1_348,cs-410,1,1, Natural Language Content Analysis,"00:26:20,685","00:26:25,525",348,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1580,and this is because we are in different time zones
cs-410_1_1_349,cs-410,1,1, Natural Language Content Analysis,"00:26:25,525","00:26:31,495",349,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1585,and it's very hard to find a time that works well for everyone.
cs-410_1_1_350,cs-410,1,1, Natural Language Content Analysis,"00:26:31,495","00:26:34,825",350,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1591,But forum has important advantages
cs-410_1_1_351,cs-410,1,1, Natural Language Content Analysis,"00:26:34,825","00:26:40,775",351,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1594,of being able to accommodate everyone in the discussion.
cs-410_1_1_352,cs-410,1,1, Natural Language Content Analysis,"00:26:40,775","00:26:46,045",352,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1600,"So, this will be the primary way of our interactions and engagement,"
cs-410_1_1_353,cs-410,1,1, Natural Language Content Analysis,"00:26:46,045","00:26:49,360",353,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1606,"and in particular, we will be using Piazza,"
cs-410_1_1_354,cs-410,1,1, Natural Language Content Analysis,"00:26:49,360","00:26:52,825",354,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1609,"which is a forum that we have used for many other courses,"
cs-410_1_1_355,cs-410,1,1, Natural Language Content Analysis,"00:26:52,825","00:26:59,770",355,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1612,and it has proven to be a useful forum with a lot of useful functions.
cs-410_1_1_356,cs-410,1,1, Natural Language Content Analysis,"00:26:59,770","00:27:03,790",356,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1619,The second advantage of forum is it would also enable you
cs-410_1_1_357,cs-410,1,1, Natural Language Content Analysis,"00:27:03,790","00:27:08,565",357,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1623,to ask your questions as soon as you have a question.
cs-410_1_1_358,cs-410,1,1, Natural Language Content Analysis,"00:27:08,565","00:27:14,400",358,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1628,"And therefore, we can hopefully accelerate question answering and"
cs-410_1_1_359,cs-410,1,1, Natural Language Content Analysis,"00:27:14,400","00:27:17,020",359,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1634,so that you can have your question answered quickly on
cs-410_1_1_360,cs-410,1,1, Natural Language Content Analysis,"00:27:17,020","00:27:23,610",360,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1637,the forum without waiting until office hour.
cs-410_1_1_361,cs-410,1,1, Natural Language Content Analysis,"00:27:23,610","00:27:26,620",361,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1643,"Finally, we hope that the forum discussions would help us"
cs-410_1_1_362,cs-410,1,1, Natural Language Content Analysis,"00:27:26,620","00:27:30,325",362,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1646,"identify difficult concepts in those lectures,"
cs-410_1_1_363,cs-410,1,1, Natural Language Content Analysis,"00:27:30,325","00:27:34,900",363,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1650,so that we can focus on discussing these concepts or
cs-410_1_1_364,cs-410,1,1, Natural Language Content Analysis,"00:27:34,900","00:27:40,230",364,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1654,explaining these concepts that are hard to understand in our office hours.
cs-410_1_1_365,cs-410,1,1, Natural Language Content Analysis,"00:27:40,230","00:27:47,775",365,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1660,This would make better use of our office hours to help all of you.
cs-410_1_1_366,cs-410,1,1, Natural Language Content Analysis,"00:27:47,775","00:27:51,595",366,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1667,"Because of these reasons,"
cs-410_1_1_367,cs-410,1,1, Natural Language Content Analysis,"00:27:51,595","00:27:55,792",367,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1671,"so the protocol of question answering will be as follows,"
cs-410_1_1_368,cs-410,1,1, Natural Language Content Analysis,"00:27:55,792","00:28:00,640",368,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1675,and we want to emphasize that it's important to
cs-410_1_1_369,cs-410,1,1, Natural Language Content Analysis,"00:28:00,640","00:28:06,510",369,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1680,"follow this protocol so that we can make effective use of our office hours,"
cs-410_1_1_370,cs-410,1,1, Natural Language Content Analysis,"00:28:06,510","00:28:09,940",370,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1686,"so that you can get your questions answered more quickly,"
cs-410_1_1_371,cs-410,1,1, Natural Language Content Analysis,"00:28:09,940","00:28:13,290",371,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1689,so that you can all help each other in learning.
cs-410_1_1_372,cs-410,1,1, Natural Language Content Analysis,"00:28:13,290","00:28:17,410",372,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1693,"So, first, as soon as you have a question or issue to discuss,"
cs-410_1_1_373,cs-410,1,1, Natural Language Content Analysis,"00:28:17,410","00:28:20,795",373,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1697,post it immediately on the forum.
cs-410_1_1_374,cs-410,1,1, Natural Language Content Analysis,"00:28:20,795","00:28:24,985",374,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1700,"And this has a number of advantages, and first is,"
cs-410_1_1_375,cs-410,1,1, Natural Language Content Analysis,"00:28:24,985","00:28:33,155",375,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1704,"it will give you the opportunity to have the question answered quickly because often,"
cs-410_1_1_376,cs-410,1,1, Natural Language Content Analysis,"00:28:33,155","00:28:39,480",376,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1713,your question may be answered by your peers or may be answered by a teacher or by me.
cs-410_1_1_377,cs-410,1,1, Natural Language Content Analysis,"00:28:39,480","00:28:42,300",377,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1719,"So, by posting the question immediately,"
cs-410_1_1_378,cs-410,1,1, Natural Language Content Analysis,"00:28:42,300","00:28:45,565",378,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1722,"you have a better chance of getting the question answered quickly,"
cs-410_1_1_379,cs-410,1,1, Natural Language Content Analysis,"00:28:45,565","00:28:48,055",379,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1725,so that you won't have to wait.
cs-410_1_1_380,cs-410,1,1, Natural Language Content Analysis,"00:28:48,055","00:28:52,945",380,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1728,"And secondly, your question might also help others because sometimes"
cs-410_1_1_381,cs-410,1,1, Natural Language Content Analysis,"00:28:52,945","00:28:55,680",381,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1732,other students may have a similar question or may not
cs-410_1_1_382,cs-410,1,1, Natural Language Content Analysis,"00:28:55,680","00:28:58,490",382,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1735,"realize that they have encountered this question,"
cs-410_1_1_383,cs-410,1,1, Natural Language Content Analysis,"00:28:58,490","00:29:00,700",383,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1738,but you just articulated this question.
cs-410_1_1_384,cs-410,1,1, Natural Language Content Analysis,"00:29:00,700","00:29:03,525",384,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1740,"So, this is helpful for your peers as well."
cs-410_1_1_385,cs-410,1,1, Natural Language Content Analysis,"00:29:03,525","00:29:08,770",385,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1743,And discussing of this question is often also a good way to learn.
cs-410_1_1_386,cs-410,1,1, Natural Language Content Analysis,"00:29:08,770","00:29:14,085",386,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1748,"However, if your question is not answered in a timely manner on the forum,"
cs-410_1_1_387,cs-410,1,1, Natural Language Content Analysis,"00:29:14,085","00:29:16,840",387,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1754,"or addressed adequately from your perspective,"
cs-410_1_1_388,cs-410,1,1, Natural Language Content Analysis,"00:29:16,840","00:29:22,810",388,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1756,then you should email the question to all of us including me and TAs.
cs-410_1_1_389,cs-410,1,1, Natural Language Content Analysis,"00:29:22,810","00:29:29,384",389,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1762,Please use a subject line that contains the keyword CS410DSO all together.
cs-410_1_1_390,cs-410,1,1, Natural Language Content Analysis,"00:29:29,384","00:29:31,015",390,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1769,"Of course, your subject line,"
cs-410_1_1_391,cs-410,1,1, Natural Language Content Analysis,"00:29:31,015","00:29:36,790",391,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1771,you can contain other keywords based on your question.
cs-410_1_1_392,cs-410,1,1, Natural Language Content Analysis,"00:29:36,790","00:29:40,990",392,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1776,"And then, if you don't receive a reply"
cs-410_1_1_393,cs-410,1,1, Natural Language Content Analysis,"00:29:40,990","00:29:44,730",393,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1780,"from us by email in a timely manner, join an office-hour."
cs-410_1_1_394,cs-410,1,1, Natural Language Content Analysis,"00:29:44,730","00:29:49,330",394,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1784,"Now, we would do our best to reply to your emails but then,"
cs-410_1_1_395,cs-410,1,1, Natural Language Content Analysis,"00:29:49,330","00:29:50,945",395,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1789,"depending on the number of emails,"
cs-410_1_1_396,cs-410,1,1, Natural Language Content Analysis,"00:29:50,945","00:29:53,770",396,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1790,"depending on our own schedule,"
cs-410_1_1_397,cs-410,1,1, Natural Language Content Analysis,"00:29:53,770","00:29:59,567",397,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1793,we may not be able to always reply to your emails in a timely manner.
cs-410_1_1_398,cs-410,1,1, Natural Language Content Analysis,"00:29:59,567","00:30:04,285",398,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1799,"Of course, we would do our best to respond quickly but if we can't,"
cs-410_1_1_399,cs-410,1,1, Natural Language Content Analysis,"00:30:04,285","00:30:07,900",399,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1804,then you should come to one of our office hours.
cs-410_1_1_400,cs-410,1,1, Natural Language Content Analysis,"00:30:07,900","00:30:14,375",400,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1807,We would try to schedule our office hours in different time slots during the day.
cs-410_1_1_401,cs-410,1,1, Natural Language Content Analysis,"00:30:14,375","00:30:17,950",401,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1814,"For example, we would have some time slot in the morning,"
cs-410_1_1_402,cs-410,1,1, Natural Language Content Analysis,"00:30:17,950","00:30:19,885",402,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1817,"or late morning, or early afternoon,"
cs-410_1_1_403,cs-410,1,1, Natural Language Content Analysis,"00:30:19,885","00:30:23,045",403,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1819,and then another time slot in the evening.
cs-410_1_1_404,cs-410,1,1, Natural Language Content Analysis,"00:30:23,045","00:30:25,525",404,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1823,"And this is so that,"
cs-410_1_1_405,cs-410,1,1, Natural Language Content Analysis,"00:30:25,525","00:30:29,440",405,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1825,"we can hopefully accommodate different time zones,"
cs-410_1_1_406,cs-410,1,1, Natural Language Content Analysis,"00:30:29,440","00:30:35,670",406,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1829,because the same slot of time may not be equally convenient with all of you.
cs-410_1_1_407,cs-410,1,1, Natural Language Content Analysis,"00:30:35,670","00:30:37,518",407,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1835,"So, we'll do our best again,"
cs-410_1_1_408,cs-410,1,1, Natural Language Content Analysis,"00:30:37,518","00:30:39,470",408,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1837,"to diversify the time slots,"
cs-410_1_1_409,cs-410,1,1, Natural Language Content Analysis,"00:30:39,470","00:30:42,220",409,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1839,"both in terms of the time in a day,"
cs-410_1_1_410,cs-410,1,1, Natural Language Content Analysis,"00:30:42,220","00:30:46,560",410,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1842,and also in terms of the days in a week.
cs-410_1_1_411,cs-410,1,1, Natural Language Content Analysis,"00:30:47,440","00:30:53,705",411,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1847,"The format of office hours is as follows, and this again,"
cs-410_1_1_412,cs-410,1,1, Natural Language Content Analysis,"00:30:53,705","00:31:00,930",412,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1853,is based on our need to make effective use and
cs-410_1_1_413,cs-410,1,1, Natural Language Content Analysis,"00:31:00,930","00:31:09,315",413,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1860,the efficient use of our limited office hours to help you in the best way.
cs-410_1_1_414,cs-410,1,1, Natural Language Content Analysis,"00:31:09,315","00:31:12,135",414,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1869,"So, we will hold weekly office hours,"
cs-410_1_1_415,cs-410,1,1, Natural Language Content Analysis,"00:31:12,135","00:31:15,065",415,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1872,"as I said, and we'll publish those time slots."
cs-410_1_1_416,cs-410,1,1, Natural Language Content Analysis,"00:31:15,065","00:31:20,420",416,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1875,"And the office hours will be given by using video-teleconference,"
cs-410_1_1_417,cs-410,1,1, Natural Language Content Analysis,"00:31:20,420","00:31:22,895",417,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1880,"and particularly using Zoom,"
cs-410_1_1_418,cs-410,1,1, Natural Language Content Analysis,"00:31:22,895","00:31:27,435",418,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1882,and its used for system that has worked well.
cs-410_1_1_419,cs-410,1,1, Natural Language Content Analysis,"00:31:27,435","00:31:34,905",419,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1887,"And you can join or leave an office hour at any time and that means you can join late,"
cs-410_1_1_420,cs-410,1,1, Natural Language Content Analysis,"00:31:34,905","00:31:36,425",420,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1894,"or you can leave early,"
cs-410_1_1_421,cs-410,1,1, Natural Language Content Analysis,"00:31:36,425","00:31:40,085",421,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1896,and it's very flexible.
cs-410_1_1_422,cs-410,1,1, Natural Language Content Analysis,"00:31:40,085","00:31:45,235",422,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1900,"So, don't feel that you always have to come at the beginning of the office hour."
cs-410_1_1_423,cs-410,1,1, Natural Language Content Analysis,"00:31:45,235","00:31:48,680",423,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1905,"Feel free to stop by in the last ten minutes,"
cs-410_1_1_424,cs-410,1,1, Natural Language Content Analysis,"00:31:48,680","00:31:51,280",424,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1908,if that's the best time for you.
cs-410_1_1_425,cs-410,1,1, Natural Language Content Analysis,"00:31:51,280","00:31:56,990",425,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1911,"And again, by taking advantage of the forum discussion, hopefully,"
cs-410_1_1_426,cs-410,1,1, Natural Language Content Analysis,"00:31:56,990","00:31:59,795",426,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1916,"by the time of going to office hour,"
cs-410_1_1_427,cs-410,1,1, Natural Language Content Analysis,"00:31:59,795","00:32:05,855",427,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1919,we will only need to deal with some of the relatively difficult questions.
cs-410_1_1_428,cs-410,1,1, Natural Language Content Analysis,"00:32:05,855","00:32:12,500",428,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1925,And the priority we will use
cs-410_1_1_429,cs-410,1,1, Natural Language Content Analysis,"00:32:12,500","00:32:20,810",429,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1932,"is to give the highest priority to any issues that have already been posted on forums,"
cs-410_1_1_430,cs-410,1,1, Natural Language Content Analysis,"00:32:20,810","00:32:27,140",430,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1940,but have not been resolved even after some email communications.
cs-410_1_1_431,cs-410,1,1, Natural Language Content Analysis,"00:32:27,140","00:32:31,760",431,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1947,Those are clearly the toughest issues
cs-410_1_1_432,cs-410,1,1, Natural Language Content Analysis,"00:32:31,760","00:32:35,930",432,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1951,"because it has been posted on forum without a good answer,"
cs-410_1_1_433,cs-410,1,1, Natural Language Content Analysis,"00:32:35,930","00:32:41,970",433,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1955,and then was emailed to us and still not satisfactory solved.
cs-410_1_1_434,cs-410,1,1, Natural Language Content Analysis,"00:32:41,970","00:32:45,320",434,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1961,"So, we would have to give those issues the highest priority,"
cs-410_1_1_435,cs-410,1,1, Natural Language Content Analysis,"00:32:45,320","00:32:50,660",435,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1965,"that means if there are such issues being raised during office hour,"
cs-410_1_1_436,cs-410,1,1, Natural Language Content Analysis,"00:32:50,660","00:32:54,925",436,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1970,those issues would be given the first priority.
cs-410_1_1_437,cs-410,1,1, Natural Language Content Analysis,"00:32:54,925","00:32:59,410",437,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1974,"After that, we'll look at the any other unresolved issues on forums."
cs-410_1_1_438,cs-410,1,1, Natural Language Content Analysis,"00:32:59,410","00:33:03,635",438,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1979,"That's why it's very important for you to post the issue on the forum first,"
cs-410_1_1_439,cs-410,1,1, Natural Language Content Analysis,"00:33:03,635","00:33:10,430",439,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1983,and only after resolving those issues that have already been posted
cs-410_1_1_440,cs-410,1,1, Natural Language Content Analysis,"00:33:10,430","00:33:12,665",440,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1990,on forums would we take
cs-410_1_1_441,cs-410,1,1, Natural Language Content Analysis,"00:33:12,665","00:33:17,440",441,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1992,other questions or issues that have not yet been posted on forums.
cs-410_1_1_442,cs-410,1,1, Natural Language Content Analysis,"00:33:17,440","00:33:21,200",442,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1997,"And of course, you should not"
cs-410_1_1_443,cs-410,1,1, Natural Language Content Analysis,"00:33:21,200","00:33:25,850",443,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2001,hesitate to bring any questions that you have or any issues you want to discuss.
cs-410_1_1_444,cs-410,1,1, Natural Language Content Analysis,"00:33:25,850","00:33:31,550",444,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2005,"It's just that, we want to have a policy to prioritize the issues that we want to handle,"
cs-410_1_1_445,cs-410,1,1, Natural Language Content Analysis,"00:33:31,550","00:33:34,700",445,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2011,so that we can provide the maximum benefit to all of
cs-410_1_1_446,cs-410,1,1, Natural Language Content Analysis,"00:33:34,700","00:33:41,210",446,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2014,you by using our office hours efficiently.
cs-410_1_1_447,cs-410,1,1, Natural Language Content Analysis,"00:33:41,210","00:33:44,525",447,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2021,"Finally, I want to just say something"
cs-410_1_1_448,cs-410,1,1, Natural Language Content Analysis,"00:33:44,525","00:33:48,505",448,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2024,in general about how to get the most out of this course.
cs-410_1_1_449,cs-410,1,1, Natural Language Content Analysis,"00:33:48,505","00:33:53,810",449,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2028,"Perhaps, the most important advice is to plan ahead based on your own schedule,"
cs-410_1_1_450,cs-410,1,1, Natural Language Content Analysis,"00:33:53,810","00:33:56,210",450,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2033,because many of you are very busy.
cs-410_1_1_451,cs-410,1,1, Natural Language Content Analysis,"00:33:56,210","00:33:59,745",451,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2036,"So, you want to kind of take a look at"
cs-410_1_1_452,cs-410,1,1, Natural Language Content Analysis,"00:33:59,745","00:34:03,620",452,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2039,"the picture that I showed you earlier about the tasks,"
cs-410_1_1_453,cs-410,1,1, Natural Language Content Analysis,"00:34:03,620","00:34:08,330",453,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2043,and then consider your own schedule.
cs-410_1_1_454,cs-410,1,1, Natural Language Content Analysis,"00:34:08,330","00:34:14,075",454,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2048,Try to imagine which periods will be a relatively busy period
cs-410_1_1_455,cs-410,1,1, Natural Language Content Analysis,"00:34:14,075","00:34:20,225",455,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2054,"for you and identify what tasks you're supposed to work on in that period,"
cs-410_1_1_456,cs-410,1,1, Natural Language Content Analysis,"00:34:20,225","00:34:23,535",456,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2060,"and try to finish those tasks earlier,"
cs-410_1_1_457,cs-410,1,1, Natural Language Content Analysis,"00:34:23,535","00:34:28,625",457,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2063,so that you don't overlap with yourself during that busy period.
cs-410_1_1_458,cs-410,1,1, Natural Language Content Analysis,"00:34:28,625","00:34:29,845",458,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2068,"And of course, at any time,"
cs-410_1_1_459,cs-410,1,1, Natural Language Content Analysis,"00:34:29,845","00:34:32,495",459,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2069,"please let us know how we can help, again,"
cs-410_1_1_460,cs-410,1,1, Natural Language Content Analysis,"00:34:32,495","00:34:36,365",460,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2072,by using the forums as the first step.
cs-410_1_1_461,cs-410,1,1, Natural Language Content Analysis,"00:34:36,365","00:34:42,410",461,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2076,And another thing to mention is also to allocate a sufficient time
cs-410_1_1_462,cs-410,1,1, Natural Language Content Analysis,"00:34:42,410","00:34:48,440",462,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2082,for the preparation of two proctored exams because they will be given only once.
cs-410_1_1_463,cs-410,1,1, Natural Language Content Analysis,"00:34:48,440","00:34:51,801",463,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2088,"That means, you only have one chance to take each exam,"
cs-410_1_1_464,cs-410,1,1, Natural Language Content Analysis,"00:34:51,801","00:34:56,930",464,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2091,so you want to really prepare well for them.
cs-410_1_1_465,cs-410,1,1, Natural Language Content Analysis,"00:34:56,930","00:35:04,580",465,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2096,"However, those exams are mostly to confirm that you have indeed mastered the materials."
cs-410_1_1_466,cs-410,1,1, Natural Language Content Analysis,"00:35:04,580","00:35:11,015",466,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2104,"So, they will have similar questions to the quiz questions that you have already seen."
cs-410_1_1_467,cs-410,1,1, Natural Language Content Analysis,"00:35:11,015","00:35:17,215",467,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2111,"And in fact, some questions may be exactly the same as the questions in the quizzes."
cs-410_1_1_468,cs-410,1,1, Natural Language Content Analysis,"00:35:17,215","00:35:24,020",468,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2117,And we do that because this would allow you
cs-410_1_1_469,cs-410,1,1, Natural Language Content Analysis,"00:35:24,020","00:35:30,618",469,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2124,to have some sense about what the questions might look like in the exams.
cs-410_1_1_470,cs-410,1,1, Natural Language Content Analysis,"00:35:30,618","00:35:35,535",470,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2130,"So, they should not be much surprise if you have actually worked on"
cs-410_1_1_471,cs-410,1,1, Natural Language Content Analysis,"00:35:35,535","00:35:39,140",471,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2135,all the questions in those quizzes and have made
cs-410_1_1_472,cs-410,1,1, Natural Language Content Analysis,"00:35:39,140","00:35:44,580",472,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2139,sure that you have understood the answers to those questions.
cs-410_1_1_473,cs-410,1,1, Natural Language Content Analysis,"00:35:44,580","00:35:48,030",473,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2144,"Of course, if you can,"
cs-410_1_1_474,cs-410,1,1, Natural Language Content Analysis,"00:35:48,030","00:35:52,815",474,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2148,try to complete the quizzes and program assignments ahead of time.
cs-410_1_1_475,cs-410,1,1, Natural Language Content Analysis,"00:35:52,815","00:35:57,600",475,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2152,This would be to your advantage because you can now raise
cs-410_1_1_476,cs-410,1,1, Natural Language Content Analysis,"00:35:57,600","00:35:59,730",476,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2157,your questions earlier that would allow you to have
cs-410_1_1_477,cs-410,1,1, Natural Language Content Analysis,"00:35:59,730","00:36:02,910",477,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2159,more time to get your questions answered.
cs-410_1_1_478,cs-410,1,1, Natural Language Content Analysis,"00:36:02,910","00:36:06,030",478,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2162,"Therefore, by the time when you take the quiz,"
cs-410_1_1_479,cs-410,1,1, Natural Language Content Analysis,"00:36:06,030","00:36:10,115",479,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2166,then you would have all the questions resolved.
cs-410_1_1_480,cs-410,1,1, Natural Language Content Analysis,"00:36:10,115","00:36:17,760",480,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2170,And it also would allow you to actively help others and to discuss any of
cs-410_1_1_481,cs-410,1,1, Natural Language Content Analysis,"00:36:17,760","00:36:25,989",481,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2177,the problems that you have encountered that would help you earn the extra credit also.
cs-410_1_1_482,cs-410,1,1, Natural Language Content Analysis,"00:36:25,989","00:36:30,280",482,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2185,The second advice is to post questions on forum immediately and
cs-410_1_1_483,cs-410,1,1, Natural Language Content Analysis,"00:36:30,280","00:36:35,510",483,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2190,whenever you have difficulty understanding any part of the course materials.
cs-410_1_1_484,cs-410,1,1, Natural Language Content Analysis,"00:36:35,510","00:36:39,975",484,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2195,"And again, I want to emphasize the immediate action of posting any issue,"
cs-410_1_1_485,cs-410,1,1, Natural Language Content Analysis,"00:36:39,975","00:36:41,640",485,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2199,any question on the forums.
cs-410_1_1_486,cs-410,1,1, Natural Language Content Analysis,"00:36:41,640","00:36:46,375",486,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2201,We would do our best to resolve those issues through the forums.
cs-410_1_1_487,cs-410,1,1, Natural Language Content Analysis,"00:36:46,375","00:36:53,430",487,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2206,And it's the effective way to also engage the peers to help each other.
cs-410_1_1_488,cs-410,1,1, Natural Language Content Analysis,"00:36:53,430","00:36:57,390",488,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2213,"So, do not hesitate to post questions and we"
cs-410_1_1_489,cs-410,1,1, Natural Language Content Analysis,"00:36:57,390","00:37:01,290",489,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2217,won't to penalize you for posting many questions.
cs-410_1_1_490,cs-410,1,1, Natural Language Content Analysis,"00:37:01,290","00:37:03,578",490,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2221,"In fact, we'll reward that perhaps,"
cs-410_1_1_491,cs-410,1,1, Natural Language Content Analysis,"00:37:03,578","00:37:11,780",491,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2223,because that's one way to contribute to the forum discussion.
cs-410_1_1_492,cs-410,1,1, Natural Language Content Analysis,"00:37:11,780","00:37:14,645",492,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2231,"Finally, you should leverage collaborative learning,"
cs-410_1_1_493,cs-410,1,1, Natural Language Content Analysis,"00:37:14,645","00:37:18,370",493,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2234,"this is kind of related to you posting your questions on forums,"
cs-410_1_1_494,cs-410,1,1, Natural Language Content Analysis,"00:37:18,370","00:37:23,185",494,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2238,"and again, to actively participate in the forum discussion."
cs-410_1_1_495,cs-410,1,1, Natural Language Content Analysis,"00:37:23,185","00:37:27,345",495,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2243,"You will actually learn a lot from reading other people's posts,"
cs-410_1_1_496,cs-410,1,1, Natural Language Content Analysis,"00:37:27,345","00:37:29,880",496,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2247,even if you know the answers because they are
cs-410_1_1_497,cs-410,1,1, Natural Language Content Analysis,"00:37:29,880","00:37:33,515",497,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2249,often opinions expressed about those materials.
cs-410_1_1_498,cs-410,1,1, Natural Language Content Analysis,"00:37:33,515","00:37:36,960",498,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2253,"So, we do hope that you have active discussion on"
cs-410_1_1_499,cs-410,1,1, Natural Language Content Analysis,"00:37:36,960","00:37:40,763",499,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2256,"forums to help each other and to help each other, particularly,"
cs-410_1_1_500,cs-410,1,1, Natural Language Content Analysis,"00:37:40,763","00:37:44,550",500,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2260,"to save time, to understand the difficult concepts,"
cs-410_1_1_501,cs-410,1,1, Natural Language Content Analysis,"00:37:44,550","00:37:47,340",501,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2264,"to do well in the quizzes and exams,"
cs-410_1_1_502,cs-410,1,1, Natural Language Content Analysis,"00:37:47,340","00:37:51,220",502,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2267,and to finish assignments smoothly.
cs-410_1_1_503,cs-410,1,1, Natural Language Content Analysis,"00:37:51,220","00:37:56,845",503,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2271,"And finally, we do encourage you to help each other understand materials too."
cs-410_1_1_504,cs-410,1,1, Natural Language Content Analysis,"00:37:56,845","00:38:01,320",504,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2276,"So, we encourage you to answer others' questions,"
cs-410_1_1_505,cs-410,1,1, Natural Language Content Analysis,"00:38:01,320","00:38:05,005",505,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2281,and the system will record those answers.
cs-410_1_1_506,cs-410,1,1, Natural Language Content Analysis,"00:38:05,005","00:38:07,069",506,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2285,"We'll have a statistics about that,"
cs-410_1_1_507,cs-410,1,1, Natural Language Content Analysis,"00:38:07,069","00:38:11,255",507,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2287,and then we'll use that to give extra credit.
cs-410_1_1_508,cs-410,1,1, Natural Language Content Analysis,"00:38:11,255","00:38:15,745",508,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2291,"So, this was just the overall introduction to the course."
cs-410_1_1_509,cs-410,1,1, Natural Language Content Analysis,"00:38:15,745","00:38:20,790",509,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2295,"For more information, you can visit the course website on Coursera."
cs-410_1_1_510,cs-410,1,1, Natural Language Content Analysis,"00:38:20,790","00:38:22,545",510,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2300,"We hope you enjoy this course,"
cs-410_1_1_511,cs-410,1,1, Natural Language Content Analysis,"00:38:22,545","00:38:26,000",511,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=2302,and I look forward to working with you. Thank you.
cs-410_1_2_1,cs-410,1,2, Text Access,"00:00:00,012","00:00:09,434",1,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=0,[SOUND]
cs-410_1_2_2,cs-410,1,2, Text Access,"00:00:09,434","00:00:12,223",2,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=9,"In this lecture,"
cs-410_1_2_3,cs-410,1,2, Text Access,"00:00:14,279","00:00:18,349",3,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=14,"In the previous lecture, we talked about"
cs-410_1_2_4,cs-410,1,2, Text Access,"00:00:19,360","00:00:23,970",4,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=19,We explained that the state of the are
cs-410_1_2_5,cs-410,1,2, Text Access,"00:00:23,970","00:00:28,970",5,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=23,are still not good enough to process
cs-410_1_2_6,cs-410,1,2, Text Access,"00:00:28,970","00:00:30,550",6,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=28,in a robust manner.
cs-410_1_2_7,cs-410,1,2, Text Access,"00:00:30,550","00:00:31,360",7,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=30,"As a result,"
cs-410_1_2_8,cs-410,1,2, Text Access,"00:00:31,360","00:00:37,250",8,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=31,bag of words remains very popular in
cs-410_1_2_9,cs-410,1,2, Text Access,"00:00:39,140","00:00:44,100",9,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=39,"In this lecture, we're going to talk"
cs-410_1_2_10,cs-410,1,2, Text Access,"00:00:44,100","00:00:48,120",10,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=44,help users get access to the text data.
cs-410_1_2_11,cs-410,1,2, Text Access,"00:00:48,120","00:00:55,000",11,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=48,This is also important step to convert
cs-410_1_2_12,cs-410,1,2, Text Access,"00:00:55,000","00:00:57,610",12,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=55,That are actually needed
cs-410_1_2_13,cs-410,1,2, Text Access,"00:00:57,610","00:01:02,510",13,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=57,"So the main question we'll address here,"
cs-410_1_2_14,cs-410,1,2, Text Access,"00:01:02,510","00:01:07,450",14,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=62,"can a text information system, help users"
cs-410_1_2_15,cs-410,1,2, Text Access,"00:01:07,450","00:01:11,550",15,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=67,We're going to cover two complimentary
cs-410_1_2_16,cs-410,1,2, Text Access,"00:01:12,610","00:01:17,700",16,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=72,And then we're going to talk about
cs-410_1_2_17,cs-410,1,2, Text Access,"00:01:17,700","00:01:19,080",17,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=77,querying versus browsing.
cs-410_1_2_18,cs-410,1,2, Text Access,"00:01:20,770","00:01:22,860",18,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=80,So first push versus pull.
cs-410_1_2_19,cs-410,1,2, Text Access,"00:01:24,500","00:01:29,250",19,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=84,These are two different ways connect
cs-410_1_2_20,cs-410,1,2, Text Access,"00:01:29,250","00:01:29,900",20,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=89,at the right time.
cs-410_1_2_21,cs-410,1,2, Text Access,"00:01:31,190","00:01:35,900",21,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=91,The difference is which
cs-410_1_2_22,cs-410,1,2, Text Access,"00:01:37,230","00:01:38,740",22,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=97,which party takes the initiative.
cs-410_1_2_23,cs-410,1,2, Text Access,"00:01:40,290","00:01:41,380",23,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=100,"In the pull mode,"
cs-410_1_2_24,cs-410,1,2, Text Access,"00:01:41,380","00:01:46,439",24,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=101,the users take the initiative to
cs-410_1_2_25,cs-410,1,2, Text Access,"00:01:47,700","00:01:53,420",25,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=107,"And in this case, a user typically would"
cs-410_1_2_26,cs-410,1,2, Text Access,"00:01:53,420","00:01:56,100",26,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=113,"For example,"
cs-410_1_2_27,cs-410,1,2, Text Access,"00:01:56,100","00:02:01,640",27,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=116,then browse the results to
cs-410_1_2_28,cs-410,1,2, Text Access,"00:02:02,790","00:02:06,280",28,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=122,So this is usually appropriate for
cs-410_1_2_29,cs-410,1,2, Text Access,"00:02:06,280","00:02:09,340",29,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=126,satisfying a user's ad
cs-410_1_2_30,cs-410,1,2, Text Access,"00:02:10,580","00:02:14,280",30,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=130,An ad hoc information need is
cs-410_1_2_31,cs-410,1,2, Text Access,"00:02:14,280","00:02:17,870",31,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=134,"For example, you want to buy a product so"
cs-410_1_2_32,cs-410,1,2, Text Access,"00:02:17,870","00:02:22,550",32,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=137,you suddenly have a need to read
cs-410_1_2_33,cs-410,1,2, Text Access,"00:02:22,550","00:02:26,620",33,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=142,"But after you have cracked information,"
cs-410_1_2_34,cs-410,1,2, Text Access,"00:02:26,620","00:02:28,880",34,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=146,You generally no longer
cs-410_1_2_35,cs-410,1,2, Text Access,"00:02:28,880","00:02:30,200",35,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=148,it's a temporary information need.
cs-410_1_2_36,cs-410,1,2, Text Access,"00:02:31,360","00:02:35,230",36,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=151,"In such a case, it's very hard for"
cs-410_1_2_37,cs-410,1,2, Text Access,"00:02:35,230","00:02:39,480",37,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=155,it's more proper for
cs-410_1_2_38,cs-410,1,2, Text Access,"00:02:39,480","00:02:42,260",38,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=159,that's why search engines are very useful.
cs-410_1_2_39,cs-410,1,2, Text Access,"00:02:42,260","00:02:48,370",39,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=162,Today because many people have many
cs-410_1_2_40,cs-410,1,2, Text Access,"00:02:48,370","00:02:52,620",40,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=168,So as we're speaking Google is probably
cs-410_1_2_41,cs-410,1,2, Text Access,"00:02:52,620","00:02:55,720",41,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=172,"And those are all, or mostly adequate."
cs-410_1_2_42,cs-410,1,2, Text Access,"00:02:55,720","00:02:56,590",42,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=175,Information needs.
cs-410_1_2_43,cs-410,1,2, Text Access,"00:02:57,950","00:02:59,680",43,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=177,So this is a pull mode.
cs-410_1_2_44,cs-410,1,2, Text Access,"00:02:59,680","00:03:03,570",44,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=179,In contrast in the push mode in
cs-410_1_2_45,cs-410,1,2, Text Access,"00:03:03,570","00:03:07,510",45,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=183,to push the information to the user or
cs-410_1_2_46,cs-410,1,2, Text Access,"00:03:07,510","00:03:11,090",46,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=187,So in this case this is usually
cs-410_1_2_47,cs-410,1,2, Text Access,"00:03:13,100","00:03:15,190",47,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=193,Now this would be appropriate if.
cs-410_1_2_48,cs-410,1,2, Text Access,"00:03:15,190","00:03:16,900",48,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=195,The user has a stable information.
cs-410_1_2_49,cs-410,1,2, Text Access,"00:03:17,900","00:03:22,040",49,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=197,For example you may have a research
cs-410_1_2_50,cs-410,1,2, Text Access,"00:03:22,040","00:03:24,980",50,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=202,that interest tends to stay for a while.
cs-410_1_2_51,cs-410,1,2, Text Access,"00:03:24,980","00:03:26,930",51,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=204,"So, it's rather stable."
cs-410_1_2_52,cs-410,1,2, Text Access,"00:03:26,930","00:03:29,240",52,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=206,Your hobby is another example of.
cs-410_1_2_53,cs-410,1,2, Text Access,"00:03:29,240","00:03:34,100",53,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=209,A stable information need is such a case
cs-410_1_2_54,cs-410,1,2, Text Access,"00:03:34,100","00:03:38,860",54,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=214,"can learn your interest, and"
cs-410_1_2_55,cs-410,1,2, Text Access,"00:03:38,860","00:03:43,710",55,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=218,If the system hasn't seen any
cs-410_1_2_56,cs-410,1,2, Text Access,"00:03:43,710","00:03:47,900",56,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=223,the system could then take the initiative
cs-410_1_2_57,cs-410,1,2, Text Access,"00:03:47,900","00:03:49,940",57,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=227,"So, for example, a news filter or"
cs-410_1_2_58,cs-410,1,2, Text Access,"00:03:49,940","00:03:53,020",58,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=229,news recommended system could
cs-410_1_2_59,cs-410,1,2, Text Access,"00:03:53,020","00:03:56,870",59,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=233,identify interesting news to you and
cs-410_1_2_60,cs-410,1,2, Text Access,"00:03:59,130","00:04:03,960",60,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=239,This mode of information access may be
cs-410_1_2_61,cs-410,1,2, Text Access,"00:04:03,960","00:04:08,790",61,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=243,has good knowledge about the users need
cs-410_1_2_62,cs-410,1,2, Text Access,"00:04:08,790","00:04:11,850",62,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=248,"So for example, when you search for"
cs-410_1_2_63,cs-410,1,2, Text Access,"00:04:11,850","00:04:16,130",63,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=251,a search engine might infer you might be
cs-410_1_2_64,cs-410,1,2, Text Access,"00:04:16,130","00:04:17,530",64,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=256,Formation.
cs-410_1_2_65,cs-410,1,2, Text Access,"00:04:17,530","00:04:20,950",65,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=257,And they would recommend the information
cs-410_1_2_66,cs-410,1,2, Text Access,"00:04:20,950","00:04:24,780",66,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=260,"example, of an advertisement"
cs-410_1_2_67,cs-410,1,2, Text Access,"00:04:27,790","00:04:34,540",67,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=267,So this is about the two high level
cs-410_1_2_68,cs-410,1,2, Text Access,"00:04:35,720","00:04:38,440",68,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=275,Now let's look at the pull
cs-410_1_2_69,cs-410,1,2, Text Access,"00:04:39,900","00:04:43,740",69,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=279,"In the pull mode, we can further"
cs-410_1_2_70,cs-410,1,2, Text Access,"00:04:43,740","00:04:46,010",70,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=283,Querying versus browsing.
cs-410_1_2_71,cs-410,1,2, Text Access,"00:04:46,010","00:04:48,790",71,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=286,"In querying,"
cs-410_1_2_72,cs-410,1,2, Text Access,"00:04:48,790","00:04:50,560",72,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=288,"Typical the keyword query, and"
cs-410_1_2_73,cs-410,1,2, Text Access,"00:04:50,560","00:04:53,430",73,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=290,the search engine system would
cs-410_1_2_74,cs-410,1,2, Text Access,"00:04:54,500","00:05:00,730",74,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=294,And this works well when the user knows
cs-410_1_2_75,cs-410,1,2, Text Access,"00:05:00,730","00:05:02,450",75,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=300,So if you know exactly
cs-410_1_2_76,cs-410,1,2, Text Access,"00:05:02,450","00:05:04,540",76,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=302,you tend to know the right keywords.
cs-410_1_2_77,cs-410,1,2, Text Access,"00:05:04,540","00:05:07,880",77,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=304,"And then query works very well,"
cs-410_1_2_78,cs-410,1,2, Text Access,"00:05:09,290","00:05:12,740",78,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=309,But we also know that sometimes
cs-410_1_2_79,cs-410,1,2, Text Access,"00:05:12,740","00:05:16,970",79,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=312,When you don't know the right
cs-410_1_2_80,cs-410,1,2, Text Access,"00:05:16,970","00:05:21,760",80,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=316,you want to browse information
cs-410_1_2_81,cs-410,1,2, Text Access,"00:05:21,760","00:05:24,780",81,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=321,You use because browsing
cs-410_1_2_82,cs-410,1,2, Text Access,"00:05:24,780","00:05:29,890",82,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=324,"So in this case, in the case of browsing,"
cs-410_1_2_83,cs-410,1,2, Text Access,"00:05:29,890","00:05:33,330",83,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=329,into the relevant information
cs-410_1_2_84,cs-410,1,2, Text Access,"00:05:34,740","00:05:39,850",84,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=334,supported by the structures of documents.
cs-410_1_2_85,cs-410,1,2, Text Access,"00:05:39,850","00:05:42,690",85,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=339,So the system would maintain
cs-410_1_2_86,cs-410,1,2, Text Access,"00:05:42,690","00:05:45,190",86,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=342,then the user could follow
cs-410_1_2_87,cs-410,1,2, Text Access,"00:05:47,370","00:05:53,850",87,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=347,So this really works well when the user
cs-410_1_2_88,cs-410,1,2, Text Access,"00:05:53,850","00:05:59,750",88,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=353,or the user doesn't know what
cs-410_1_2_89,cs-410,1,2, Text Access,"00:05:59,750","00:06:05,070",89,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=359,Or simply because the user finds it
cs-410_1_2_90,cs-410,1,2, Text Access,"00:06:05,070","00:06:10,450",90,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=365,So even if a user knows what query to
cs-410_1_2_91,cs-410,1,2, Text Access,"00:06:10,450","00:06:12,370",91,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=370,to search for information.
cs-410_1_2_92,cs-410,1,2, Text Access,"00:06:12,370","00:06:14,760",92,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=372,It's still harder to enter the query.
cs-410_1_2_93,cs-410,1,2, Text Access,"00:06:14,760","00:06:18,840",93,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=374,"In such a case, again,"
cs-410_1_2_94,cs-410,1,2, Text Access,"00:06:18,840","00:06:23,060",94,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=378,The relationship between browsing and
cs-410_1_2_95,cs-410,1,2, Text Access,"00:06:23,060","00:06:24,130",95,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=383,imagine you're site seeing.
cs-410_1_2_96,cs-410,1,2, Text Access,"00:06:25,230","00:06:27,080",96,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=385,Imagine if you're touring a city.
cs-410_1_2_97,cs-410,1,2, Text Access,"00:06:27,080","00:06:29,800",97,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=387,Now if you know the exact
cs-410_1_2_98,cs-410,1,2, Text Access,"00:06:31,670","00:06:34,900",98,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=391,Taking a taxi there is
cs-410_1_2_99,cs-410,1,2, Text Access,"00:06:34,900","00:06:36,860",99,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=394,You can go directly to the site.
cs-410_1_2_100,cs-410,1,2, Text Access,"00:06:36,860","00:06:40,440",100,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=396,"But if you don't know the exact address,"
cs-410_1_2_101,cs-410,1,2, Text Access,"00:06:40,440","00:06:43,579",101,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=400,Or you can take a taxi to a nearby
cs-410_1_2_102,cs-410,1,2, Text Access,"00:06:44,670","00:06:48,160",102,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=404,It turns out that we do exactly
cs-410_1_2_103,cs-410,1,2, Text Access,"00:06:48,160","00:06:51,480",103,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=408,If you know exactly what you
cs-410_1_2_104,cs-410,1,2, Text Access,"00:06:51,480","00:06:55,360",104,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=411,use the right keywords in your query
cs-410_1_2_105,cs-410,1,2, Text Access,"00:06:55,360","00:06:58,150",105,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=415,"That's usually the fastest way to do,"
cs-410_1_2_106,cs-410,1,2, Text Access,"00:06:59,480","00:07:02,180",106,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=419,But what if you don't know
cs-410_1_2_107,cs-410,1,2, Text Access,"00:07:02,180","00:07:04,369",107,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=422,"Well, you clearly probably won't so well."
cs-410_1_2_108,cs-410,1,2, Text Access,"00:07:04,369","00:07:06,150",108,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=424,You will not related pages.
cs-410_1_2_109,cs-410,1,2, Text Access,"00:07:06,150","00:07:10,160",109,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=426,"And then, you need to also walk"
cs-410_1_2_110,cs-410,1,2, Text Access,"00:07:10,160","00:07:14,110",110,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=430,meaning by following the links or
cs-410_1_2_111,cs-410,1,2, Text Access,"00:07:14,110","00:07:16,430",111,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=434,You can then finally get
cs-410_1_2_112,cs-410,1,2, Text Access,"00:07:17,580","00:07:20,720",112,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=437,If you want to learn about again.
cs-410_1_2_113,cs-410,1,2, Text Access,"00:07:20,720","00:07:24,610",113,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=440,You will likely do a lot of browsing so
cs-410_1_2_114,cs-410,1,2, Text Access,"00:07:24,610","00:07:29,914",114,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=444,just like you are looking around in
cs-410_1_2_115,cs-410,1,2, Text Access,"00:07:29,914","00:07:36,405",115,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=449,interesting attractions
cs-410_1_2_116,cs-410,1,2, Text Access,"00:07:36,405","00:07:39,200",116,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=456,[INAUDIBLE].
cs-410_1_2_117,cs-410,1,2, Text Access,"00:07:39,200","00:07:45,330",117,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=459,So this analogy also tells us that
cs-410_1_2_118,cs-410,1,2, Text Access,"00:07:45,330","00:07:50,600",118,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=465,"query, but we don't really have"
cs-410_1_2_119,cs-410,1,2, Text Access,"00:07:50,600","00:07:54,470",119,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=470,And this is because in order
cs-410_1_2_120,cs-410,1,2, Text Access,"00:07:54,470","00:07:57,840",120,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=474,"we need a map to guide us,"
cs-410_1_2_121,cs-410,1,2, Text Access,"00:07:57,840","00:07:58,410",121,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=477,"Of Chicago,"
cs-410_1_2_122,cs-410,1,2, Text Access,"00:07:58,410","00:08:04,060",122,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=478,"through the city of Chicago, you need a"
cs-410_1_2_123,cs-410,1,2, Text Access,"00:08:04,060","00:08:08,190",123,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=484,So how to construct such a topical
cs-410_1_2_124,cs-410,1,2, Text Access,"00:08:08,190","00:08:12,730",124,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=488,research question that might bring us
cs-410_1_2_125,cs-410,1,2, Text Access,"00:08:12,730","00:08:16,950",125,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=492,more interesting browsing experience
cs-410_1_2_126,cs-410,1,2, Text Access,"00:08:19,170","00:08:21,280",126,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=499,"So, to summarize this lecture,"
cs-410_1_2_127,cs-410,1,2, Text Access,"00:08:21,280","00:08:26,550",127,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=501,we've talked about the two high level
cs-410_1_2_128,cs-410,1,2, Text Access,"00:08:26,550","00:08:29,130",128,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=506,Push tends to be supported by
cs-410_1_2_129,cs-410,1,2, Text Access,"00:08:29,130","00:08:31,770",129,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=509,Pull tends to be supported
cs-410_1_2_130,cs-410,1,2, Text Access,"00:08:31,770","00:08:35,710",130,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=511,"Of course, in the sophisticated"
cs-410_1_2_131,cs-410,1,2, Text Access,"00:08:35,710","00:08:36,780",131,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=515,we should combine the two.
cs-410_1_2_132,cs-410,1,2, Text Access,"00:08:38,590","00:08:41,830",132,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=518,"In the pull mode, we can further this"
cs-410_1_2_133,cs-410,1,2, Text Access,"00:08:41,830","00:08:47,140",133,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=521,Again we generally want to combine
cs-410_1_2_134,cs-410,1,2, Text Access,"00:08:47,140","00:08:50,080",134,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=527,so that you can support
cs-410_1_2_135,cs-410,1,2, Text Access,"00:08:51,220","00:08:55,420",135,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=531,If you want to know more about
cs-410_1_2_136,cs-410,1,2, Text Access,"00:08:55,420","00:08:58,600",136,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=535,"push, you can read this article."
cs-410_1_2_137,cs-410,1,2, Text Access,"00:08:58,600","00:09:03,560",137,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=538,This give excellent discussion of the
cs-410_1_2_138,cs-410,1,2, Text Access,"00:09:03,560","00:09:05,330",138,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=543,information retrieval.
cs-410_1_2_139,cs-410,1,2, Text Access,"00:09:05,330","00:09:10,271",139,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=545,Here informational filtering is similar
cs-410_1_2_140,cs-410,1,2, Text Access,"00:09:10,271","00:09:12,749",140,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=550,the push mode of information access.
cs-410_1_2_141,cs-410,1,2, Text Access,"00:09:12,749","00:09:22,749",141,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=552,[MUSIC]
cs-410_1_3_1,cs-410,1,3, Text Retrieval Problem,"00:00:00,168","00:00:07,728",1,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=0,[MUSIC]
cs-410_1_3_2,cs-410,1,3, Text Retrieval Problem,"00:00:07,728","00:00:10,250",2,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=7,This lecture is about
cs-410_1_3_3,cs-410,1,3, Text Retrieval Problem,"00:00:12,820","00:00:15,710",3,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=12,This picture shows our overall plan for
cs-410_1_3_4,cs-410,1,3, Text Retrieval Problem,"00:00:16,780","00:00:21,780",4,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=16,"In the last lecture, we talked about"
cs-410_1_3_5,cs-410,1,3, Text Retrieval Problem,"00:00:21,780","00:00:24,150",5,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=21,We talked about push versus pull.
cs-410_1_3_6,cs-410,1,3, Text Retrieval Problem,"00:00:25,350","00:00:30,720",6,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=25,Such engines are the main tools for
cs-410_1_3_7,cs-410,1,3, Text Retrieval Problem,"00:00:30,720","00:00:32,690",7,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=30,"Starting from this lecture,"
cs-410_1_3_8,cs-410,1,3, Text Retrieval Problem,"00:00:32,690","00:00:36,270",8,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=32,we're going to talk about the how
cs-410_1_3_9,cs-410,1,3, Text Retrieval Problem,"00:00:38,110","00:00:40,770",9,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=38,So first it's about
cs-410_1_3_10,cs-410,1,3, Text Retrieval Problem,"00:00:42,660","00:00:46,120",10,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=42,We're going to talk about
cs-410_1_3_11,cs-410,1,3, Text Retrieval Problem,"00:00:46,120","00:00:49,650",11,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=46,"First, we define Text Retrieval."
cs-410_1_3_12,cs-410,1,3, Text Retrieval Problem,"00:00:49,650","00:00:54,200",12,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=49,Second we're going to make a comparison
cs-410_1_3_13,cs-410,1,3, Text Retrieval Problem,"00:00:54,200","00:00:56,280",13,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=54,the related task Database Retrieval.
cs-410_1_3_14,cs-410,1,3, Text Retrieval Problem,"00:00:58,240","00:01:02,190",14,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=58,"Finally, we're going to talk about"
cs-410_1_3_15,cs-410,1,3, Text Retrieval Problem,"00:01:02,190","00:01:06,508",15,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=62,Document Ranking as two strategies for
cs-410_1_3_16,cs-410,1,3, Text Retrieval Problem,"00:01:09,728","00:01:11,120",16,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=69,So what is Text Retrieval?
cs-410_1_3_17,cs-410,1,3, Text Retrieval Problem,"00:01:12,850","00:01:14,840",17,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=72,It should be a task that's familiar for
cs-410_1_3_18,cs-410,1,3, Text Retrieval Problem,"00:01:14,840","00:01:18,740",18,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=74,the most of us because we're using
cs-410_1_3_19,cs-410,1,3, Text Retrieval Problem,"00:01:19,920","00:01:24,190",19,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=79,So text retrieval is basically a task
cs-410_1_3_20,cs-410,1,3, Text Retrieval Problem,"00:01:24,190","00:01:29,900",20,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=84,where the system would respond to
cs-410_1_3_21,cs-410,1,3, Text Retrieval Problem,"00:01:29,900","00:01:31,540",21,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=89,"Basically, it's for supporting a query"
cs-410_1_3_22,cs-410,1,3, Text Retrieval Problem,"00:01:32,730","00:01:37,300",22,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=92,as one way to implement the poll
cs-410_1_3_23,cs-410,1,3, Text Retrieval Problem,"00:01:39,250","00:01:40,940",23,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=99,So the situation is the following.
cs-410_1_3_24,cs-410,1,3, Text Retrieval Problem,"00:01:40,940","00:01:43,590",24,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=100,You have a collection of
cs-410_1_3_25,cs-410,1,3, Text Retrieval Problem,"00:01:43,590","00:01:47,364",25,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=103,These documents could be all
cs-410_1_3_26,cs-410,1,3, Text Retrieval Problem,"00:01:47,364","00:01:50,988",26,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=107,all the literature articles
cs-410_1_3_27,cs-410,1,3, Text Retrieval Problem,"00:01:50,988","00:01:56,528",27,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=110,Or maybe all the text
cs-410_1_3_28,cs-410,1,3, Text Retrieval Problem,"00:01:58,528","00:02:04,340",28,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=118,A user will typically give a query to
cs-410_1_3_29,cs-410,1,3, Text Retrieval Problem,"00:02:04,340","00:02:09,480",29,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=124,"And then, the system would return"
cs-410_1_3_30,cs-410,1,3, Text Retrieval Problem,"00:02:09,480","00:02:14,040",30,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=129,Relevant documents refer to those
cs-410_1_3_31,cs-410,1,3, Text Retrieval Problem,"00:02:14,040","00:02:15,500",31,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=134,the user who typed in the query.
cs-410_1_3_32,cs-410,1,3, Text Retrieval Problem,"00:02:16,910","00:02:19,510",32,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=136,All this task is a phone call
cs-410_1_3_33,cs-410,1,3, Text Retrieval Problem,"00:02:21,170","00:02:25,585",33,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=141,But literally information retrieval would
cs-410_1_3_34,cs-410,1,3, Text Retrieval Problem,"00:02:25,585","00:02:30,660",34,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=145,"non-textual information as well,"
cs-410_1_3_35,cs-410,1,3, Text Retrieval Problem,"00:02:30,660","00:02:35,960",35,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=150,It's worth noting that
cs-410_1_3_36,cs-410,1,3, Text Retrieval Problem,"00:02:35,960","00:02:41,610",36,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=155,of information retrieval in
cs-410_1_3_37,cs-410,1,3, Text Retrieval Problem,"00:02:41,610","00:02:47,010",37,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=161,video can be retrieved by
cs-410_1_3_38,cs-410,1,3, Text Retrieval Problem,"00:02:47,010","00:02:52,270",38,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=167,"So for example,"
cs-410_1_3_39,cs-410,1,3, Text Retrieval Problem,"00:02:52,270","00:02:57,390",39,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=172,match a user's query was
cs-410_1_3_40,cs-410,1,3, Text Retrieval Problem,"00:02:59,850","00:03:03,870",40,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=179,This problem is also
cs-410_1_3_41,cs-410,1,3, Text Retrieval Problem,"00:03:05,550","00:03:08,680",41,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=185,And the technology is often called
cs-410_1_3_42,cs-410,1,3, Text Retrieval Problem,"00:03:11,190","00:03:14,540",42,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=191,If you ever take a course in databases it
cs-410_1_3_43,cs-410,1,3, Text Retrieval Problem,"00:03:14,540","00:03:18,400",43,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=194,will be useful to pause
cs-410_1_3_44,cs-410,1,3, Text Retrieval Problem,"00:03:18,400","00:03:25,200",44,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=198,think about the differences between
cs-410_1_3_45,cs-410,1,3, Text Retrieval Problem,"00:03:25,200","00:03:28,450",45,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=205,Now these two tasks
cs-410_1_3_46,cs-410,1,3, Text Retrieval Problem,"00:03:29,530","00:03:31,928",46,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=209,"But, there are some important differences."
cs-410_1_3_47,cs-410,1,3, Text Retrieval Problem,"00:03:33,708","00:03:38,140",47,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=213,"So, spend a moment to think about"
cs-410_1_3_48,cs-410,1,3, Text Retrieval Problem,"00:03:38,140","00:03:43,300",48,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=218,"Think about the data, and the information"
cs-410_1_3_49,cs-410,1,3, Text Retrieval Problem,"00:03:43,300","00:03:46,080",49,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=223,those that are managed
cs-410_1_3_50,cs-410,1,3, Text Retrieval Problem,"00:03:47,350","00:03:51,570",50,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=227,Think about the different between
cs-410_1_3_51,cs-410,1,3, Text Retrieval Problem,"00:03:51,570","00:03:57,389",51,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=231,database system versus queries that
cs-410_1_3_52,cs-410,1,3, Text Retrieval Problem,"00:03:59,180","00:04:00,970",52,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=239,And then finally think about the answers.
cs-410_1_3_53,cs-410,1,3, Text Retrieval Problem,"00:04:02,870","00:04:06,980",53,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=242,What's the difference between the two?
cs-410_1_3_54,cs-410,1,3, Text Retrieval Problem,"00:04:06,980","00:04:11,760",54,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=246,"Okay, so if we think about the information"
cs-410_1_3_55,cs-410,1,3, Text Retrieval Problem,"00:04:11,760","00:04:14,890",55,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=251,we will see that in text retrieval.
cs-410_1_3_56,cs-410,1,3, Text Retrieval Problem,"00:04:14,890","00:04:18,100",56,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=254,"The data is unstructured, it's free text."
cs-410_1_3_57,cs-410,1,3, Text Retrieval Problem,"00:04:18,100","00:04:24,020",57,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=258,"But in databases, they are structured data"
cs-410_1_3_58,cs-410,1,3, Text Retrieval Problem,"00:04:24,020","00:04:30,430",58,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=264,to tell you this column is the names
cs-410_1_3_59,cs-410,1,3, Text Retrieval Problem,"00:04:31,880","00:04:35,020",59,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=271,The unstructured text is not obvious
cs-410_1_3_60,cs-410,1,3, Text Retrieval Problem,"00:04:35,020","00:04:38,420",60,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=275,what are the names of people
cs-410_1_3_61,cs-410,1,3, Text Retrieval Problem,"00:04:40,440","00:04:45,930",61,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=280,"Because of this difference, we also see"
cs-410_1_3_62,cs-410,1,3, Text Retrieval Problem,"00:04:45,930","00:04:52,900",62,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=285,ambiguous and we talk about that in the
cs-410_1_3_63,cs-410,1,3, Text Retrieval Problem,"00:04:52,900","00:04:55,500",63,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=292,But they don't tend to have
cs-410_1_3_64,cs-410,1,3, Text Retrieval Problem,"00:04:58,230","00:05:01,990",64,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=298,The results important
cs-410_1_3_65,cs-410,1,3, Text Retrieval Problem,"00:05:01,990","00:05:05,770",65,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=301,this is partly due to the difference
cs-410_1_3_66,cs-410,1,3, Text Retrieval Problem,"00:05:07,610","00:05:10,960",66,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=307,So test queries tend to be ambiguous.
cs-410_1_3_67,cs-410,1,3, Text Retrieval Problem,"00:05:10,960","00:05:16,290",67,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=310,"Whereas in their research,"
cs-410_1_3_68,cs-410,1,3, Text Retrieval Problem,"00:05:16,290","00:05:22,330",68,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=316,Think about a SQL query that would clearly
cs-410_1_3_69,cs-410,1,3, Text Retrieval Problem,"00:05:22,330","00:05:24,690",69,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=322,So it has very well-defined semantics.
cs-410_1_3_70,cs-410,1,3, Text Retrieval Problem,"00:05:27,230","00:05:32,252",70,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=327,Keyword queries or electronic queries tend
cs-410_1_3_71,cs-410,1,3, Text Retrieval Problem,"00:05:32,252","00:05:37,952",71,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=332,"to be incomplete,"
cs-410_1_3_72,cs-410,1,3, Text Retrieval Problem,"00:05:37,952","00:05:43,390",72,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=337,specify what documents
cs-410_1_3_73,cs-410,1,3, Text Retrieval Problem,"00:05:43,390","00:05:46,370",73,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=343,Whereas complete specification for
cs-410_1_3_74,cs-410,1,3, Text Retrieval Problem,"00:05:47,390","00:05:50,900",74,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=347,"And because of these differences,"
cs-410_1_3_75,cs-410,1,3, Text Retrieval Problem,"00:05:50,900","00:05:56,670",75,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=350,"Being the case of text retrieval, we're"
cs-410_1_3_76,cs-410,1,3, Text Retrieval Problem,"00:05:58,110","00:06:02,740",76,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=358,"In the database search,"
cs-410_1_3_77,cs-410,1,3, Text Retrieval Problem,"00:06:02,740","00:06:07,260",77,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=362,match records with the sequel
cs-410_1_3_78,cs-410,1,3, Text Retrieval Problem,"00:06:09,110","00:06:14,550",78,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=369,"Now in the case of text retrieval,"
cs-410_1_3_79,cs-410,1,3, Text Retrieval Problem,"00:06:14,550","00:06:19,950",79,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=374,"to the query is not very well specified,"
cs-410_1_3_80,cs-410,1,3, Text Retrieval Problem,"00:06:21,140","00:06:25,830",80,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=381,So it's unclear what should be
cs-410_1_3_81,cs-410,1,3, Text Retrieval Problem,"00:06:25,830","00:06:30,510",81,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=385,"And this has very important consequences,"
cs-410_1_3_82,cs-410,1,3, Text Retrieval Problem,"00:06:30,510","00:06:35,108",82,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=390,textual retrieval is
cs-410_1_3_83,cs-410,1,3, Text Retrieval Problem,"00:06:38,578","00:06:44,100",83,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=398,So this is a problem because
cs-410_1_3_84,cs-410,1,3, Text Retrieval Problem,"00:06:44,100","00:06:51,510",84,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=404,then we can not mathematically prove one
cs-410_1_3_85,cs-410,1,3, Text Retrieval Problem,"00:06:52,620","00:06:56,650",85,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=412,That also means we must rely
cs-410_1_3_86,cs-410,1,3, Text Retrieval Problem,"00:06:56,650","00:07:01,120",86,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=416,involving users to know
cs-410_1_3_87,cs-410,1,3, Text Retrieval Problem,"00:07:02,460","00:07:05,080",87,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=422,And that's why we have.
cs-410_1_3_88,cs-410,1,3, Text Retrieval Problem,"00:07:05,080","00:07:09,420",88,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=425,You need more than one lectures
cs-410_1_3_89,cs-410,1,3, Text Retrieval Problem,"00:07:09,420","00:07:12,820",89,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=429,Because this is very important topic for
cs-410_1_3_90,cs-410,1,3, Text Retrieval Problem,"00:07:13,890","00:07:18,902",90,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=433,Without knowing how to evaluate heroism
cs-410_1_3_91,cs-410,1,3, Text Retrieval Problem,"00:07:18,902","00:07:24,563",91,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=438,whether we have got the better or
cs-410_1_3_92,cs-410,1,3, Text Retrieval Problem,"00:07:28,393","00:07:31,155",92,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=448,So now let's look at
cs-410_1_3_93,cs-410,1,3, Text Retrieval Problem,"00:07:32,240","00:07:36,170",93,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=452,"So, this slide shows a formal formulation"
cs-410_1_3_94,cs-410,1,3, Text Retrieval Problem,"00:07:37,460","00:07:43,460",94,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=457,"First, we have our vocabulary set, which"
cs-410_1_3_95,cs-410,1,3, Text Retrieval Problem,"00:07:44,920","00:07:49,140",95,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=464,"Now here,"
cs-410_1_3_96,cs-410,1,3, Text Retrieval Problem,"00:07:49,140","00:07:53,360",96,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=469,"in reality, on the web,"
cs-410_1_3_97,cs-410,1,3, Text Retrieval Problem,"00:07:53,360","00:07:56,180",97,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=473,We have texts that are in
cs-410_1_3_98,cs-410,1,3, Text Retrieval Problem,"00:07:57,530","00:08:01,478",98,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=477,"But here for simplicity, we just"
cs-410_1_3_99,cs-410,1,3, Text Retrieval Problem,"00:08:01,478","00:08:07,088",99,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=481,As the techniques used for retrieving
cs-410_1_3_100,cs-410,1,3, Text Retrieval Problem,"00:08:07,088","00:08:12,783",100,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=487,less similar to the techniques used for
cs-410_1_3_101,cs-410,1,3, Text Retrieval Problem,"00:08:12,783","00:08:18,819",101,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=492,"although there is important difference,"
cs-410_1_3_102,cs-410,1,3, Text Retrieval Problem,"00:08:21,759","00:08:24,725",102,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=501,"Next, we have the query,"
cs-410_1_3_103,cs-410,1,3, Text Retrieval Problem,"00:08:26,015","00:08:28,625",103,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=506,"And so here, you can see"
cs-410_1_3_104,cs-410,1,3, Text Retrieval Problem,"00:08:31,175","00:08:36,482",104,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=511,the query is defined as
cs-410_1_3_105,cs-410,1,3, Text Retrieval Problem,"00:08:36,482","00:08:41,252",105,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=516,Each q sub i is a word in the vocabulary.
cs-410_1_3_106,cs-410,1,3, Text Retrieval Problem,"00:08:42,302","00:08:47,000",106,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=522,"A document is defined in the same way,"
cs-410_1_3_107,cs-410,1,3, Text Retrieval Problem,"00:08:47,000","00:08:51,520",107,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=527,"And here,"
cs-410_1_3_108,cs-410,1,3, Text Retrieval Problem,"00:08:52,920","00:08:55,900",108,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=532,"Now typically, the documents"
cs-410_1_3_109,cs-410,1,3, Text Retrieval Problem,"00:08:57,100","00:09:01,460",109,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=537,But there are also cases where
cs-410_1_3_110,cs-410,1,3, Text Retrieval Problem,"00:09:04,370","00:09:08,530",110,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=544,So you can think about what
cs-410_1_3_111,cs-410,1,3, Text Retrieval Problem,"00:09:09,670","00:09:13,570",111,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=549,I hope you can think of Twitter search.
cs-410_1_3_112,cs-410,1,3, Text Retrieval Problem,"00:09:13,570","00:09:14,992",112,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=553,Tweets are very short.
cs-410_1_3_113,cs-410,1,3, Text Retrieval Problem,"00:09:16,557","00:09:20,560",113,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=556,"But in general,"
cs-410_1_3_114,cs-410,1,3, Text Retrieval Problem,"00:09:22,934","00:09:27,389",114,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=562,"Now, then we have"
cs-410_1_3_115,cs-410,1,3, Text Retrieval Problem,"00:09:27,389","00:09:31,240",115,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=567,and this collection can be very large.
cs-410_1_3_116,cs-410,1,3, Text Retrieval Problem,"00:09:31,240","00:09:32,370",116,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=571,So think about the web.
cs-410_1_3_117,cs-410,1,3, Text Retrieval Problem,"00:09:32,370","00:09:33,820",117,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=572,It could be very large.
cs-410_1_3_118,cs-410,1,3, Text Retrieval Problem,"00:09:36,140","00:09:40,300",118,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=576,And then the goal of text retrieval
cs-410_1_3_119,cs-410,1,3, Text Retrieval Problem,"00:09:40,300","00:09:46,358",119,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=580,"the documents, which we denote by R'(q),"
cs-410_1_3_120,cs-410,1,3, Text Retrieval Problem,"00:09:46,358","00:09:50,290",120,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=586,"And this in general, a subset of all"
cs-410_1_3_121,cs-410,1,3, Text Retrieval Problem,"00:09:52,410","00:09:57,862",121,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=592,"Unfortunately, this set of relevant"
cs-410_1_3_122,cs-410,1,3, Text Retrieval Problem,"00:09:57,862","00:10:03,000",122,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=597,"and user-dependent in the sense that,"
cs-410_1_3_123,cs-410,1,3, Text Retrieval Problem,"00:10:03,000","00:10:08,110",123,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=603,"in by different users, they expect"
cs-410_1_3_124,cs-410,1,3, Text Retrieval Problem,"00:10:09,330","00:10:13,600",124,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=609,The query given to us by
cs-410_1_3_125,cs-410,1,3, Text Retrieval Problem,"00:10:13,600","00:10:15,660",125,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=613,on which document should be in this set.
cs-410_1_3_126,cs-410,1,3, Text Retrieval Problem,"00:10:17,840","00:10:24,940",126,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=617,"And indeed, the user is generally"
cs-410_1_3_127,cs-410,1,3, Text Retrieval Problem,"00:10:24,940","00:10:28,940",127,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=624,"be in this set, especially in the case"
cs-410_1_3_128,cs-410,1,3, Text Retrieval Problem,"00:10:28,940","00:10:32,540",128,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=628,"large, the user doesn't have complete"
cs-410_1_3_129,cs-410,1,3, Text Retrieval Problem,"00:10:34,000","00:10:39,550",129,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=634,So the best search system
cs-410_1_3_130,cs-410,1,3, Text Retrieval Problem,"00:10:39,550","00:10:45,856",130,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=639,an approximation of this
cs-410_1_3_131,cs-410,1,3, Text Retrieval Problem,"00:10:45,856","00:10:50,168",131,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=645,So we denote it by R'(q).
cs-410_1_3_132,cs-410,1,3, Text Retrieval Problem,"00:10:50,168","00:10:54,835",132,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=650,"So formerly,"
cs-410_1_3_133,cs-410,1,3, Text Retrieval Problem,"00:10:54,835","00:10:59,849",133,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=654,R'(q) approximation of
cs-410_1_3_134,cs-410,1,3, Text Retrieval Problem,"00:10:59,849","00:11:01,902",134,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=659,So how can we do that?
cs-410_1_3_135,cs-410,1,3, Text Retrieval Problem,"00:11:01,902","00:11:07,290",135,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=661,Now imagine if you are now asked
cs-410_1_3_136,cs-410,1,3, Text Retrieval Problem,"00:11:08,980","00:11:10,480",136,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=668,What would you do?
cs-410_1_3_137,cs-410,1,3, Text Retrieval Problem,"00:11:10,480","00:11:12,526",137,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=670,Now think for a moment.
cs-410_1_3_138,cs-410,1,3, Text Retrieval Problem,"00:11:12,526","00:11:14,433",138,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=672,"Right, so these are your input."
cs-410_1_3_139,cs-410,1,3, Text Retrieval Problem,"00:11:14,433","00:11:18,425",139,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=674,"The query, the documents."
cs-410_1_3_140,cs-410,1,3, Text Retrieval Problem,"00:11:20,399","00:11:24,021",140,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=680,And then you are to compute
cs-410_1_3_141,cs-410,1,3, Text Retrieval Problem,"00:11:24,021","00:11:28,060",141,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=684,which is a set of documents that
cs-410_1_3_142,cs-410,1,3, Text Retrieval Problem,"00:11:29,770","00:11:31,926",142,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=689,"So, how would you solve the problem?"
cs-410_1_3_143,cs-410,1,3, Text Retrieval Problem,"00:11:31,926","00:11:37,630",143,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=691,"Now in general,"
cs-410_1_3_144,cs-410,1,3, Text Retrieval Problem,"00:11:39,720","00:11:42,970",144,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=699,The first strategy is we do a document
cs-410_1_3_145,cs-410,1,3, Text Retrieval Problem,"00:11:42,970","00:11:47,740",145,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=702,going to have a binary classification
cs-410_1_3_146,cs-410,1,3, Text Retrieval Problem,"00:11:49,350","00:11:52,110",146,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=709,That's a function that
cs-410_1_3_147,cs-410,1,3, Text Retrieval Problem,"00:11:52,110","00:11:55,740",147,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=712,"query as input, and then give a zero or"
cs-410_1_3_148,cs-410,1,3, Text Retrieval Problem,"00:11:55,740","00:12:01,170",148,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=715,one as output to indicate whether this
cs-410_1_3_149,cs-410,1,3, Text Retrieval Problem,"00:12:02,330","00:12:05,310",149,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=722,"So in this case, you can see the document."
cs-410_1_3_150,cs-410,1,3, Text Retrieval Problem,"00:12:08,700","00:12:15,130",150,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=728,"The relevant document is set,"
cs-410_1_3_151,cs-410,1,3, Text Retrieval Problem,"00:12:15,130","00:12:22,340",151,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=735,"It basically, all the documents that"
cs-410_1_3_152,cs-410,1,3, Text Retrieval Problem,"00:12:25,410","00:12:26,040",152,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=745,"So in this case,"
cs-410_1_3_153,cs-410,1,3, Text Retrieval Problem,"00:12:26,040","00:12:29,930",153,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=746,you can see the system must have decide
cs-410_1_3_154,cs-410,1,3, Text Retrieval Problem,"00:12:29,930","00:12:33,680",154,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=749,"Basically, it has to say"
cs-410_1_3_155,cs-410,1,3, Text Retrieval Problem,"00:12:33,680","00:12:36,330",155,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=753,And this is called absolute relevance.
cs-410_1_3_156,cs-410,1,3, Text Retrieval Problem,"00:12:36,330","00:12:38,940",156,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=756,"Basically, it needs to know"
cs-410_1_3_157,cs-410,1,3, Text Retrieval Problem,"00:12:38,940","00:12:39,850",157,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=758,useful to the user.
cs-410_1_3_158,cs-410,1,3, Text Retrieval Problem,"00:12:41,940","00:12:44,910",158,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=761,"Alternatively, there's another"
cs-410_1_3_159,cs-410,1,3, Text Retrieval Problem,"00:12:46,160","00:12:47,150",159,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=766,"Now in this case,"
cs-410_1_3_160,cs-410,1,3, Text Retrieval Problem,"00:12:47,150","00:12:52,290",160,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=767,the system is not going to make a call
cs-410_1_3_161,cs-410,1,3, Text Retrieval Problem,"00:12:52,290","00:12:57,230",161,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=772,But rather the system is going to
cs-410_1_3_162,cs-410,1,3, Text Retrieval Problem,"00:12:58,440","00:13:01,540",162,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=778,That would simply give us a value
cs-410_1_3_163,cs-410,1,3, Text Retrieval Problem,"00:13:01,540","00:13:04,170",163,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=781,that would indicate which
cs-410_1_3_164,cs-410,1,3, Text Retrieval Problem,"00:13:05,740","00:13:08,590",164,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=785,So it's not going to make a call whether
cs-410_1_3_165,cs-410,1,3, Text Retrieval Problem,"00:13:08,590","00:13:12,696",165,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=788,But rather it would say which
cs-410_1_3_166,cs-410,1,3, Text Retrieval Problem,"00:13:12,696","00:13:17,669",166,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=792,So this function then can be
cs-410_1_3_167,cs-410,1,3, Text Retrieval Problem,"00:13:17,669","00:13:22,135",167,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=797,then we're going to let
cs-410_1_3_168,cs-410,1,3, Text Retrieval Problem,"00:13:22,135","00:13:25,296",168,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=802,when the user looks at the document.
cs-410_1_3_169,cs-410,1,3, Text Retrieval Problem,"00:13:25,296","00:13:31,410",169,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=805,So we have a threshold theta
cs-410_1_3_170,cs-410,1,3, Text Retrieval Problem,"00:13:31,410","00:13:37,398",170,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=811,documents should be in
cs-410_1_3_171,cs-410,1,3, Text Retrieval Problem,"00:13:37,398","00:13:40,802",171,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=817,And we're going to assume
cs-410_1_3_172,cs-410,1,3, Text Retrieval Problem,"00:13:40,802","00:13:45,312",172,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=820,are ranked above the threshold
cs-410_1_3_173,cs-410,1,3, Text Retrieval Problem,"00:13:45,312","00:13:49,780",173,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=825,these are the documents that
cs-410_1_3_174,cs-410,1,3, Text Retrieval Problem,"00:13:49,780","00:13:54,490",174,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=829,And theta is a cutoff
cs-410_1_3_175,cs-410,1,3, Text Retrieval Problem,"00:13:56,980","00:14:00,980",175,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=836,So here we've got some collaboration
cs-410_1_3_176,cs-410,1,3, Text Retrieval Problem,"00:14:00,980","00:14:03,330",176,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=840,because we don't really make a cutoff.
cs-410_1_3_177,cs-410,1,3, Text Retrieval Problem,"00:14:03,330","00:14:07,060",177,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=843,And the user kind of helped
cs-410_1_3_178,cs-410,1,3, Text Retrieval Problem,"00:14:08,120","00:14:10,950",178,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=848,"So in this case,"
cs-410_1_3_179,cs-410,1,3, Text Retrieval Problem,"00:14:10,950","00:14:14,440",179,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=850,if one document is more
cs-410_1_3_180,cs-410,1,3, Text Retrieval Problem,"00:14:14,440","00:14:17,990",180,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=854,"And that is, it only needs to"
cs-410_1_3_181,cs-410,1,3, Text Retrieval Problem,"00:14:19,050","00:14:20,770",181,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=859,as opposed to absolute relevance.
cs-410_1_3_182,cs-410,1,3, Text Retrieval Problem,"00:14:22,230","00:14:26,290",182,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=862,Now you can probably already sense that
cs-410_1_3_183,cs-410,1,3, Text Retrieval Problem,"00:14:26,290","00:14:31,560",183,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=866,relative relevance would be easier to
cs-410_1_3_184,cs-410,1,3, Text Retrieval Problem,"00:14:31,560","00:14:32,800",184,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=871,"Because in the first case,"
cs-410_1_3_185,cs-410,1,3, Text Retrieval Problem,"00:14:32,800","00:14:36,420",185,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=872,we have to say exactly whether
cs-410_1_3_186,cs-410,1,3, Text Retrieval Problem,"00:14:37,990","00:14:45,500",186,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=877,And it turns out that ranking is indeed
cs-410_1_3_187,cs-410,1,3, Text Retrieval Problem,"00:14:46,710","00:14:50,240",187,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=886,So let's look at these two
cs-410_1_3_188,cs-410,1,3, Text Retrieval Problem,"00:14:50,240","00:14:53,960",188,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=890,So this picture shows how it works.
cs-410_1_3_189,cs-410,1,3, Text Retrieval Problem,"00:14:53,960","00:14:58,780",189,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=893,"So on the left side,"
cs-410_1_3_190,cs-410,1,3, Text Retrieval Problem,"00:14:58,780","00:15:02,710",190,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=898,we use the pluses to indicate
cs-410_1_3_191,cs-410,1,3, Text Retrieval Problem,"00:15:02,710","00:15:09,990",191,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=902,So we can see the true relevant
cs-410_1_3_192,cs-410,1,3, Text Retrieval Problem,"00:15:09,990","00:15:15,210",192,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=909,"of true relevant documents, consists"
cs-410_1_3_193,cs-410,1,3, Text Retrieval Problem,"00:15:17,450","00:15:20,972",193,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=917,"And with the document selection function,"
cs-410_1_3_194,cs-410,1,3, Text Retrieval Problem,"00:15:20,972","00:15:25,636",194,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=920,we're going to basically
cs-410_1_3_195,cs-410,1,3, Text Retrieval Problem,"00:15:25,636","00:15:30,050",195,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=925,"relevant documents, and non-relevant ones."
cs-410_1_3_196,cs-410,1,3, Text Retrieval Problem,"00:15:30,050","00:15:34,700",196,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=930,"Of course, the classified will not"
cs-410_1_3_197,cs-410,1,3, Text Retrieval Problem,"00:15:34,700","00:15:39,522",197,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=934,"So here we can see, in the approximation"
cs-410_1_3_198,cs-410,1,3, Text Retrieval Problem,"00:15:39,522","00:15:41,660",198,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=939,we have got some number in the documents.
cs-410_1_3_199,cs-410,1,3, Text Retrieval Problem,"00:15:43,090","00:15:44,168",199,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=943,"And similarly,"
cs-410_1_3_200,cs-410,1,3, Text Retrieval Problem,"00:15:44,168","00:15:48,868",200,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=944,there is a relevant document that's
cs-410_1_3_201,cs-410,1,3, Text Retrieval Problem,"00:15:48,868","00:15:53,972",201,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=948,"In the case of document ranking,"
cs-410_1_3_202,cs-410,1,3, Text Retrieval Problem,"00:15:53,972","00:15:59,368",202,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=953,simply ranks all the documents in
cs-410_1_3_203,cs-410,1,3, Text Retrieval Problem,"00:15:59,368","00:16:04,580",203,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=959,"And then, we're going to let the user"
cs-410_1_3_204,cs-410,1,3, Text Retrieval Problem,"00:16:04,580","00:16:07,510",204,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=964,If the user wants to
cs-410_1_3_205,cs-410,1,3, Text Retrieval Problem,"00:16:07,510","00:16:11,630",205,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=967,then the user will scroll down some
cs-410_1_3_206,cs-410,1,3, Text Retrieval Problem,"00:16:11,630","00:16:17,010",206,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=971,But if the user only wants to
cs-410_1_3_207,cs-410,1,3, Text Retrieval Problem,"00:16:17,010","00:16:20,750",207,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=977,the user might stop at the top position.
cs-410_1_3_208,cs-410,1,3, Text Retrieval Problem,"00:16:20,750","00:16:24,200",208,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=980,"So in this case, the user stops at d4."
cs-410_1_3_209,cs-410,1,3, Text Retrieval Problem,"00:16:24,200","00:16:30,950",209,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=984,"So in fact, we have delivered"
cs-410_1_3_210,cs-410,1,3, Text Retrieval Problem,"00:16:33,940","00:16:40,300",210,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=993,So as I said ranking is generally
cs-410_1_3_211,cs-410,1,3, Text Retrieval Problem,"00:16:40,300","00:16:46,410",211,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1000,because the classifier in the case of
cs-410_1_3_212,cs-410,1,3, Text Retrieval Problem,"00:16:46,410","00:16:47,790",212,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1006,Why?
cs-410_1_3_213,cs-410,1,3, Text Retrieval Problem,"00:16:47,790","00:16:51,100",213,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1007,Because the only clue
cs-410_1_3_214,cs-410,1,3, Text Retrieval Problem,"00:16:51,100","00:16:56,550",214,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1011,But the query may not be accurate in the
cs-410_1_3_215,cs-410,1,3, Text Retrieval Problem,"00:16:57,660","00:17:02,860",215,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1017,"For example, you might expect relevant"
cs-410_1_3_216,cs-410,1,3, Text Retrieval Problem,"00:17:04,460","00:17:08,050",216,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1024,topics by using specific vocabulary.
cs-410_1_3_217,cs-410,1,3, Text Retrieval Problem,"00:17:08,050","00:17:13,550",217,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1028,"And as a result,"
cs-410_1_3_218,cs-410,1,3, Text Retrieval Problem,"00:17:13,550","00:17:15,690",218,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1033,"Because in the collection,"
cs-410_1_3_219,cs-410,1,3, Text Retrieval Problem,"00:17:15,690","00:17:19,990",219,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1035,no others have discussed the topic
cs-410_1_3_220,cs-410,1,3, Text Retrieval Problem,"00:17:19,990","00:17:24,380",220,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1039,"So in this case,"
cs-410_1_3_221,cs-410,1,3, Text Retrieval Problem,"00:17:25,970","00:17:31,980",221,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1045,no relevant documents to return in
cs-410_1_3_222,cs-410,1,3, Text Retrieval Problem,"00:17:33,230","00:17:37,892",222,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1053,"On the other hand,"
cs-410_1_3_223,cs-410,1,3, Text Retrieval Problem,"00:17:37,892","00:17:40,430",223,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1057,"for example, if the query"
cs-410_1_3_224,cs-410,1,3, Text Retrieval Problem,"00:17:40,430","00:17:44,610",224,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1060,does not have sufficient descriptive
cs-410_1_3_225,cs-410,1,3, Text Retrieval Problem,"00:17:44,610","00:17:51,100",225,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1064,You may actually end up having of
cs-410_1_3_226,cs-410,1,3, Text Retrieval Problem,"00:17:51,100","00:17:55,840",226,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1071,thought these words my be sufficient
cs-410_1_3_227,cs-410,1,3, Text Retrieval Problem,"00:17:55,840","00:17:58,590",227,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1075,"But, it turns out they"
cs-410_1_3_228,cs-410,1,3, Text Retrieval Problem,"00:17:58,590","00:18:04,280",228,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1078,"there are many distractions,"
cs-410_1_3_229,cs-410,1,3, Text Retrieval Problem,"00:18:04,280","00:18:07,450",229,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1084,"And so, this is a case of over delivery."
cs-410_1_3_230,cs-410,1,3, Text Retrieval Problem,"00:18:08,570","00:18:13,910",230,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1088,"Unfortunately, it's very hard to find the"
cs-410_1_3_231,cs-410,1,3, Text Retrieval Problem,"00:18:15,390","00:18:15,900",231,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1095,Why?
cs-410_1_3_232,cs-410,1,3, Text Retrieval Problem,"00:18:15,900","00:18:19,940",232,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1095,Because whether users looking for
cs-410_1_3_233,cs-410,1,3, Text Retrieval Problem,"00:18:19,940","00:18:24,520",233,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1099,not have a good knowledge about
cs-410_1_3_234,cs-410,1,3, Text Retrieval Problem,"00:18:24,520","00:18:28,800",234,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1104,"And in that case, the user does not"
cs-410_1_3_235,cs-410,1,3, Text Retrieval Problem,"00:18:30,240","00:18:33,770",235,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1110,vocabularies will be used in
cs-410_1_3_236,cs-410,1,3, Text Retrieval Problem,"00:18:33,770","00:18:36,064",236,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1113,So it's very hard for
cs-410_1_3_237,cs-410,1,3, Text Retrieval Problem,"00:18:36,064","00:18:42,061",237,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1116,a user to pre-specify the right
cs-410_1_3_238,cs-410,1,3, Text Retrieval Problem,"00:18:44,569","00:18:49,502",238,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1124,"Even if the classifier is accurate,"
cs-410_1_3_239,cs-410,1,3, Text Retrieval Problem,"00:18:49,502","00:18:54,880",239,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1129,"relevant documents, because they"
cs-410_1_3_240,cs-410,1,3, Text Retrieval Problem,"00:18:56,130","00:18:58,330",240,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1136,Relevance is often a matter of degree.
cs-410_1_3_241,cs-410,1,3, Text Retrieval Problem,"00:18:59,560","00:19:05,250",241,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1139,So we must prioritize these documents for
cs-410_1_3_242,cs-410,1,3, Text Retrieval Problem,"00:19:06,320","00:19:10,690",242,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1146,And note that this
cs-410_1_3_243,cs-410,1,3, Text Retrieval Problem,"00:19:12,300","00:19:15,840",243,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1152,because a user cannot
cs-410_1_3_244,cs-410,1,3, Text Retrieval Problem,"00:19:15,840","00:19:20,720",244,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1155,the user generally would have to
cs-410_1_3_245,cs-410,1,3, Text Retrieval Problem,"00:19:21,750","00:19:29,220",245,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1161,"And therefore, it would make sense to"
cs-410_1_3_246,cs-410,1,3, Text Retrieval Problem,"00:19:29,220","00:19:32,100",246,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1169,And that's what ranking is doing.
cs-410_1_3_247,cs-410,1,3, Text Retrieval Problem,"00:19:32,100","00:19:35,240",247,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1172,"So for these reasons,"
cs-410_1_3_248,cs-410,1,3, Text Retrieval Problem,"00:19:36,330","00:19:39,610",248,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1176,Now this preference also has
cs-410_1_3_249,cs-410,1,3, Text Retrieval Problem,"00:19:39,610","00:19:42,330",249,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1179,this is given by the probability
cs-410_1_3_250,cs-410,1,3, Text Retrieval Problem,"00:19:44,210","00:19:47,930",250,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1184,"In the end of this lecture,"
cs-410_1_3_251,cs-410,1,3, Text Retrieval Problem,"00:19:49,320","00:19:54,260",251,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1189,"This principle says, returning a ranked"
cs-410_1_3_252,cs-410,1,3, Text Retrieval Problem,"00:19:54,260","00:19:57,590",252,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1194,of probability that a document
cs-410_1_3_253,cs-410,1,3, Text Retrieval Problem,"00:19:57,590","00:20:01,270",253,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1197,is the optimal strategy under
cs-410_1_3_254,cs-410,1,3, Text Retrieval Problem,"00:20:02,620","00:20:05,690",254,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1202,"First, the utility of"
cs-410_1_3_255,cs-410,1,3, Text Retrieval Problem,"00:20:05,690","00:20:09,280",255,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1205,Is independent of the utility
cs-410_1_3_256,cs-410,1,3, Text Retrieval Problem,"00:20:10,980","00:20:15,300",256,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1210,"Second, a user would be assumed to"
cs-410_1_3_257,cs-410,1,3, Text Retrieval Problem,"00:20:15,300","00:20:21,775",257,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1215,Now it's easy to understand why these
cs-410_1_3_258,cs-410,1,3, Text Retrieval Problem,"00:20:21,775","00:20:27,130",258,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1221,Site for the ranking strategy.
cs-410_1_3_259,cs-410,1,3, Text Retrieval Problem,"00:20:27,130","00:20:30,930",259,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1227,"Because if the documents are independent,"
cs-410_1_3_260,cs-410,1,3, Text Retrieval Problem,"00:20:30,930","00:20:34,270",260,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1230,then we can evaluate the utility
cs-410_1_3_261,cs-410,1,3, Text Retrieval Problem,"00:20:36,350","00:20:40,270",261,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1236,And this would allow the computer
cs-410_1_3_262,cs-410,1,3, Text Retrieval Problem,"00:20:40,270","00:20:43,920",262,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1240,"And then, we are going to rank these"
cs-410_1_3_263,cs-410,1,3, Text Retrieval Problem,"00:20:45,710","00:20:51,300",263,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1245,The second assumption is to say that the
cs-410_1_3_264,cs-410,1,3, Text Retrieval Problem,"00:20:51,300","00:20:55,050",264,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1251,If the user is not going to follow
cs-410_1_3_265,cs-410,1,3, Text Retrieval Problem,"00:20:55,050","00:20:59,440",265,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1255,"the documents sequentially, then obviously"
cs-410_1_3_266,cs-410,1,3, Text Retrieval Problem,"00:21:00,560","00:21:08,270",266,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1260,"So under these two assumptions, we can"
cs-410_1_3_267,cs-410,1,3, Text Retrieval Problem,"00:21:08,270","00:21:13,170",267,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1268,"is, in fact, the best that you could do."
cs-410_1_3_268,cs-410,1,3, Text Retrieval Problem,"00:21:13,170","00:21:14,700",268,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1273,"Now, I've put one question here."
cs-410_1_3_269,cs-410,1,3, Text Retrieval Problem,"00:21:14,700","00:21:16,330",269,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1274,Do these two assumptions hold?
cs-410_1_3_270,cs-410,1,3, Text Retrieval Problem,"00:21:18,240","00:21:23,110",270,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1278,"I suggest you to pause the lecture,"
cs-410_1_3_271,cs-410,1,3, Text Retrieval Problem,"00:21:27,950","00:21:33,065",271,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1287,"Now, can you think of"
cs-410_1_3_272,cs-410,1,3, Text Retrieval Problem,"00:21:33,065","00:21:39,238",272,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1293,suggest these assumptions
cs-410_1_3_273,cs-410,1,3, Text Retrieval Problem,"00:21:44,462","00:21:46,953",273,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1304,"Now, if you think for a moment,"
cs-410_1_3_274,cs-410,1,3, Text Retrieval Problem,"00:21:46,953","00:21:51,490",274,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1306,you may realize none of
cs-410_1_3_275,cs-410,1,3, Text Retrieval Problem,"00:21:53,230","00:21:57,690",275,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1313,"For example, in the case of"
cs-410_1_3_276,cs-410,1,3, Text Retrieval Problem,"00:21:57,690","00:22:02,590",276,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1317,have documents that have similar or
cs-410_1_3_277,cs-410,1,3, Text Retrieval Problem,"00:22:02,590","00:22:06,620",277,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1322,"If we look at each of them alone,"
cs-410_1_3_278,cs-410,1,3, Text Retrieval Problem,"00:22:07,790","00:22:12,510",278,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1327,But if the user has already seen
cs-410_1_3_279,cs-410,1,3, Text Retrieval Problem,"00:22:12,510","00:22:17,240",279,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1332,generally not very useful for the user to
cs-410_1_3_280,cs-410,1,3, Text Retrieval Problem,"00:22:19,030","00:22:22,040",280,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1339,So clearly the utility
cs-410_1_3_281,cs-410,1,3, Text Retrieval Problem,"00:22:22,040","00:22:25,680",281,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1342,is dependent on other documents
cs-410_1_3_282,cs-410,1,3, Text Retrieval Problem,"00:22:27,350","00:22:32,510",282,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1347,In some other cases you might see
cs-410_1_3_283,cs-410,1,3, Text Retrieval Problem,"00:22:32,510","00:22:38,490",283,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1352,"be useful to the user, but when three"
cs-410_1_3_284,cs-410,1,3, Text Retrieval Problem,"00:22:38,490","00:22:41,070",284,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1358,They provide answers to
cs-410_1_3_285,cs-410,1,3, Text Retrieval Problem,"00:22:42,140","00:22:46,883",285,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1362,So this is a collective relevance and
cs-410_1_3_286,cs-410,1,3, Text Retrieval Problem,"00:22:46,883","00:22:51,542",286,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1366,the value of the document might
cs-410_1_3_287,cs-410,1,3, Text Retrieval Problem,"00:22:53,329","00:22:58,100",287,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1373,Sequential browsing generally would make
cs-410_1_3_288,cs-410,1,3, Text Retrieval Problem,"00:22:59,220","00:23:04,650",288,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1379,"But even if you have a rank list,"
cs-410_1_3_289,cs-410,1,3, Text Retrieval Problem,"00:23:04,650","00:23:10,140",289,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1384,users don't always just go strictly
cs-410_1_3_290,cs-410,1,3, Text Retrieval Problem,"00:23:10,140","00:23:14,910",290,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1390,They sometimes will look at the bottom for
cs-410_1_3_291,cs-410,1,3, Text Retrieval Problem,"00:23:14,910","00:23:17,810",291,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1394,And if you think about the more
cs-410_1_3_292,cs-410,1,3, Text Retrieval Problem,"00:23:17,810","00:23:22,100",292,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1397,we could possibly use like
cs-410_1_3_293,cs-410,1,3, Text Retrieval Problem,"00:23:22,100","00:23:26,820",293,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1402,Where you can put that additional
cs-410_1_3_294,cs-410,1,3, Text Retrieval Problem,"00:23:26,820","00:23:29,379",294,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1406,sequential browsing is a very
cs-410_1_3_295,cs-410,1,3, Text Retrieval Problem,"00:23:32,010","00:23:34,300",295,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1412,So the point here is that
cs-410_1_3_296,cs-410,1,3, Text Retrieval Problem,"00:23:35,740","00:23:39,990",296,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1415,none of these assumptions is
cs-410_1_3_297,cs-410,1,3, Text Retrieval Problem,"00:23:41,100","00:23:45,290",297,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1421,But probability ranking principle
cs-410_1_3_298,cs-410,1,3, Text Retrieval Problem,"00:23:46,870","00:23:51,020",298,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1426,ranking as a primary pattern for
cs-410_1_3_299,cs-410,1,3, Text Retrieval Problem,"00:23:51,020","00:23:53,180",299,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1431,And this has actually been the basis for
cs-410_1_3_300,cs-410,1,3, Text Retrieval Problem,"00:23:53,180","00:23:57,090",300,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1433,a lot of research work in
cs-410_1_3_301,cs-410,1,3, Text Retrieval Problem,"00:23:57,090","00:24:00,530",301,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1437,And many hours have been designed
cs-410_1_3_302,cs-410,1,3, Text Retrieval Problem,"00:24:01,590","00:24:06,410",302,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1441,despite that the assumptions
cs-410_1_3_303,cs-410,1,3, Text Retrieval Problem,"00:24:06,410","00:24:11,570",303,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1446,And we can address this problem
cs-410_1_3_304,cs-410,1,3, Text Retrieval Problem,"00:24:11,570","00:24:15,430",304,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1451,"Of a ranked list, for example,"
cs-410_1_3_305,cs-410,1,3, Text Retrieval Problem,"00:24:20,260","00:24:22,500",305,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1460,"So to summarize this lecture,"
cs-410_1_3_306,cs-410,1,3, Text Retrieval Problem,"00:24:22,500","00:24:28,000",306,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1462,the main points that you can
cs-410_1_3_307,cs-410,1,3, Text Retrieval Problem,"00:24:28,000","00:24:31,760",307,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1468,"First, text retrieval is"
cs-410_1_3_308,cs-410,1,3, Text Retrieval Problem,"00:24:31,760","00:24:37,951",308,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1471,And that means which algorithm is
cs-410_1_3_309,cs-410,1,3, Text Retrieval Problem,"00:24:37,951","00:24:42,500",309,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1477,"Second, document ranking"
cs-410_1_3_310,cs-410,1,3, Text Retrieval Problem,"00:24:42,500","00:24:46,180",310,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1482,And this will help users prioritize
cs-410_1_3_311,cs-410,1,3, Text Retrieval Problem,"00:24:47,410","00:24:52,693",311,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1487,And this is also to bypass the difficulty
cs-410_1_3_312,cs-410,1,3, Text Retrieval Problem,"00:24:52,693","00:24:58,221",312,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1492,Because we can get some help from users
cs-410_1_3_313,cs-410,1,3, Text Retrieval Problem,"00:24:58,221","00:24:59,809",313,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1498,it's more flexible.
cs-410_1_3_314,cs-410,1,3, Text Retrieval Problem,"00:25:01,967","00:25:06,904",314,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1501,"So, this further suggests that the main"
cs-410_1_3_315,cs-410,1,3, Text Retrieval Problem,"00:25:06,904","00:25:09,950",315,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1506,engine is the design
cs-410_1_3_316,cs-410,1,3, Text Retrieval Problem,"00:25:10,970","00:25:16,150",316,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1510,"In other words, we need to define"
cs-410_1_3_317,cs-410,1,3, Text Retrieval Problem,"00:25:16,150","00:25:19,480",317,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1516,on the query and document pair.
cs-410_1_3_318,cs-410,1,3, Text Retrieval Problem,"00:25:21,360","00:25:26,151",318,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1521,How we design such a function is the main
cs-410_1_3_319,cs-410,1,3, Text Retrieval Problem,"00:25:29,123","00:25:32,060",319,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1529,There are two suggested
cs-410_1_3_320,cs-410,1,3, Text Retrieval Problem,"00:25:32,060","00:25:36,180",320,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1532,The first is the classical paper on
cs-410_1_3_321,cs-410,1,3, Text Retrieval Problem,"00:25:37,470","00:25:42,380",321,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1537,The second one is a must-read for anyone
cs-410_1_3_322,cs-410,1,3, Text Retrieval Problem,"00:25:42,380","00:25:49,220",322,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1542,"It's a classic IR book, which has"
cs-410_1_3_323,cs-410,1,3, Text Retrieval Problem,"00:25:49,220","00:25:55,540",323,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1549,and results in early days up to
cs-410_1_3_324,cs-410,1,3, Text Retrieval Problem,"00:25:55,540","00:25:59,762",324,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1555,Chapter six of this book has
cs-410_1_3_325,cs-410,1,3, Text Retrieval Problem,"00:25:59,762","00:26:06,211",325,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1559,the Probability Ranking Principle and
cs-410_1_3_326,cs-410,1,3, Text Retrieval Problem,"00:26:06,211","00:26:16,211",326,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1566,[MUSIC]
cs-410_1_4_1,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:00,000","00:00:07,569",1,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=0,[SOUND]
cs-410_1_4_2,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:07,569","00:00:10,320",2,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=7,This lecture is a overview of
cs-410_1_4_3,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:13,630","00:00:17,820",3,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=13,"In the previous lecture, we introduced"
cs-410_1_4_4,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:17,820","00:00:20,330",4,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=17,We explained that the main problem
cs-410_1_4_5,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:20,330","00:00:24,780",5,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=20,is the design of ranking function
cs-410_1_4_6,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:24,780","00:00:25,510",6,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=24,"In this lecture,"
cs-410_1_4_7,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:25,510","00:00:31,040",7,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=25,we will give an overview of different
cs-410_1_4_8,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:33,840","00:00:35,750",8,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=33,So the problem is the following.
cs-410_1_4_9,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:35,750","00:00:39,310",9,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=35,We have a query that has
cs-410_1_4_10,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:39,310","00:00:42,710",10,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=39,the document that's also
cs-410_1_4_11,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:42,710","00:00:44,509",11,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=42,And we hope to define a function f
cs-410_1_4_12,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:45,770","00:00:49,596",12,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=45,that can compute a score based
cs-410_1_4_13,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:49,596","00:00:54,870",13,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=49,So the main challenge you hear is with
cs-410_1_4_14,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:54,870","00:01:00,275",14,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=54,can rank all the relevant documents
cs-410_1_4_15,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:00,275","00:01:05,544",15,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=60,"Clearly, this means our function"
cs-410_1_4_16,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:05,544","00:01:10,824",16,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=65,the likelihood that a document
cs-410_1_4_17,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:10,824","00:01:16,490",17,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=70,That also means we have to have
cs-410_1_4_18,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:16,490","00:01:19,621",18,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=76,"In particular, in order to"
cs-410_1_4_19,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:19,621","00:01:23,380",19,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=79,we have to have a computational
cs-410_1_4_20,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:23,380","00:01:27,232",20,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=83,And we achieve this goal by
cs-410_1_4_21,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:27,232","00:01:30,370",21,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=87,which gives us
cs-410_1_4_22,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:32,650","00:01:34,110",22,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=92,"Now, over many decades,"
cs-410_1_4_23,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:34,110","00:01:38,420",23,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=94,researchers have designed many
cs-410_1_4_24,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:38,420","00:01:40,680",24,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=98,And they fall into different categories.
cs-410_1_4_25,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:42,290","00:01:48,830",25,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=102,"First, one family of the models"
cs-410_1_4_26,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:50,090","00:01:54,170",26,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=110,"Basically, we assume that if"
cs-410_1_4_27,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:54,170","00:01:57,970",27,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=114,"the query than another document is,"
cs-410_1_4_28,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:57,970","00:02:02,310",28,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=117,then we will say the first document
cs-410_1_4_29,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:02,310","00:02:05,330",29,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=122,"So in this case,"
cs-410_1_4_30,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:05,330","00:02:08,572",30,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=125,the similarity between the query and
cs-410_1_4_31,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:08,572","00:02:13,760",31,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=128,One well known example in this
cs-410_1_4_32,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:13,760","00:02:17,160",32,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=133,which we will cover more in
cs-410_1_4_33,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:20,370","00:02:24,010",33,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=140,A second kind of models
cs-410_1_4_34,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:24,010","00:02:30,600",34,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=144,"In this family of models, we follow a very"
cs-410_1_4_35,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:30,600","00:02:35,200",35,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=150,queries and documents are all
cs-410_1_4_36,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:36,420","00:02:41,220",36,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=156,And we assume there is a binary
cs-410_1_4_37,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:42,370","00:02:45,420",37,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=162,to indicate whether a document
cs-410_1_4_38,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:46,530","00:02:53,090",38,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=166,We then define the score of document with
cs-410_1_4_39,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:53,090","00:02:59,780",39,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=173,"this random variable R is equal to 1,"
cs-410_1_4_40,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:59,780","00:03:04,363",40,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=179,There are different cases
cs-410_1_4_41,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:04,363","00:03:08,003",41,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=184,"One is classic probabilistic model,"
cs-410_1_4_42,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:08,003","00:03:10,800",42,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=188,yet another is divergence
cs-410_1_4_43,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:12,580","00:03:17,865",43,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=192,"In a later lecture, we will talk more"
cs-410_1_4_44,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:17,865","00:03:21,740",44,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=197,A third kind of model are based
cs-410_1_4_45,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:21,740","00:03:27,440",45,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=201,So here the idea is to associate
cs-410_1_4_46,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:27,440","00:03:31,230",46,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=207,and we can then quantify
cs-410_1_4_47,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:31,230","00:03:34,790",47,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=211,show that the query
cs-410_1_4_48,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:37,100","00:03:41,940",48,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=217,"Finally, there is also a family of models"
cs-410_1_4_49,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:41,940","00:03:46,237",49,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=221,that are using axiomatic thinking.
cs-410_1_4_50,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:46,237","00:03:50,849",50,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=226,"Here, an idea is to define"
cs-410_1_4_51,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:50,849","00:03:54,650",51,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=230,hope a good retrieval function to satisfy.
cs-410_1_4_52,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:55,760","00:04:00,572",52,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=235,"So in this case, the problem is"
cs-410_1_4_53,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:00,572","00:04:04,288",53,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=240,that can satisfy all
cs-410_1_4_54,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:05,867","00:04:12,326",54,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=245,"Interestingly, although these different"
cs-410_1_4_55,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:12,326","00:04:17,930",55,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=252,"in the end, the retrieval function"
cs-410_1_4_56,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:17,930","00:04:22,020",56,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=257,And these functions tend to
cs-410_1_4_57,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:22,020","00:04:28,010",57,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=262,So now let's take a look at the common
cs-410_1_4_58,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:28,010","00:04:32,760",58,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=268,and to examine some of the common
cs-410_1_4_59,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:33,900","00:04:38,810",59,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=273,"First, these models are all"
cs-410_1_4_60,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:38,810","00:04:43,060",60,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=278,"of using bag of words to represent text,"
cs-410_1_4_61,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:43,060","00:04:47,500",61,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=283,we explained this in the natural
cs-410_1_4_62,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:47,500","00:04:51,450",62,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=287,Bag of words representation remains
cs-410_1_4_63,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:51,450","00:04:52,320",63,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=291,the search engines.
cs-410_1_4_64,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:53,620","00:04:57,690",64,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=293,"So with this assumption,"
cs-410_1_4_65,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:57,690","00:05:03,300",65,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=297,like a presidential campaign news
cs-410_1_4_66,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:03,300","00:05:08,140",66,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=303,would be based on scores computed
cs-410_1_4_67,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:09,560","00:05:15,710",67,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=309,And that means the score would
cs-410_1_4_68,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:15,710","00:05:19,510",68,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=315,"such as presidential, campaign, and news."
cs-410_1_4_69,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:19,510","00:05:23,749",69,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=319,"Here, we can see there"
cs-410_1_4_70,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:23,749","00:05:29,501",70,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=323,each corresponding to how well the
cs-410_1_4_71,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:31,475","00:05:36,729",71,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=331,"Inside of these functions,"
cs-410_1_4_72,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:38,760","00:05:43,770",72,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=338,"So for example, one factor that"
cs-410_1_4_73,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:43,770","00:05:48,570",73,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=343,is how many times does the word
cs-410_1_4_74,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:48,570","00:05:50,250",74,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=348,"This is called a term frequency, or TF."
cs-410_1_4_75,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:51,710","00:05:56,823",75,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=351,We might also denote as
cs-410_1_4_76,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:56,823","00:06:03,533",76,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=356,"In general, if the word occurs"
cs-410_1_4_77,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:03,533","00:06:08,550",77,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=363,then the value of this
cs-410_1_4_78,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:08,550","00:06:13,610",78,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=368,"Another factor is,"
cs-410_1_4_79,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:13,610","00:06:18,141",79,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=373,And this is to use the document length for
cs-410_1_4_80,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:18,141","00:06:22,910",80,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=378,"In general, if a term occurs in a long"
cs-410_1_4_81,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:22,910","00:06:28,430",81,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=382,"document many times,"
cs-410_1_4_82,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:28,430","00:06:32,678",82,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=388,if it occurred the same number
cs-410_1_4_83,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:32,678","00:06:37,130",83,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=392,"Because in a long document, any term"
cs-410_1_4_84,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:38,980","00:06:42,840",84,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=398,"Finally, there is this factor"
cs-410_1_4_85,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:42,840","00:06:48,020",85,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=402,"That is, we also want to look at how"
cs-410_1_4_86,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:48,020","00:06:55,240",86,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=408,"collection, and we call this document"
cs-410_1_4_87,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:55,240","00:07:01,200",87,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=415,"And in some other models,"
cs-410_1_4_88,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:01,200","00:07:04,620",88,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=421,to characterize this information.
cs-410_1_4_89,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:05,860","00:07:09,770",89,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=425,"So here, I show the probability of"
cs-410_1_4_90,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:10,830","00:07:14,564",90,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=430,So all these are trying to characterize
cs-410_1_4_91,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:14,564","00:07:15,555",91,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=434,the collection.
cs-410_1_4_92,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:15,555","00:07:20,418",92,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=435,"In general, matching a rare term in"
cs-410_1_4_93,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:20,418","00:07:23,860",93,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=440,to the overall score than
cs-410_1_4_94,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:25,720","00:07:30,564",94,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=445,So this captures some of the main ideas
cs-410_1_4_95,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:30,564","00:07:32,349",95,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=450,the art original models.
cs-410_1_4_96,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:34,000","00:07:38,422",96,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=454,"So now, a natural question is,"
cs-410_1_4_97,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:39,834","00:07:45,080",97,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=459,Now it turns out that many
cs-410_1_4_98,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:45,080","00:07:47,700",98,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=465,So here are a list of
cs-410_1_4_99,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:47,700","00:07:52,463",99,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=467,that are generally regarded as
cs-410_1_4_100,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:52,463","00:07:57,920",100,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=472,"pivoted length normalization,"
cs-410_1_4_101,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:57,920","00:08:02,110",101,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=477,"When optimized,"
cs-410_1_4_102,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:02,110","00:08:08,508",102,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=482,And this was discussed in detail in this
cs-410_1_4_103,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:08,508","00:08:13,130",103,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=488,"Among all these,"
cs-410_1_4_104,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:13,130","00:08:17,750",104,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=493,It's most likely that this has been used
cs-410_1_4_105,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:17,750","00:08:21,730",105,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=497,and you will also often see this
cs-410_1_4_106,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:22,800","00:08:27,090",106,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=502,And we'll talk more about this
cs-410_1_4_107,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:30,430","00:08:36,770",107,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=510,"So, to summarize, the main points made"
cs-410_1_4_108,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:36,770","00:08:41,540",108,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=516,of a good ranking function pre-requires a
cs-410_1_4_109,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:41,540","00:08:45,310",109,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=521,we achieve this goal by designing
cs-410_1_4_110,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:47,170","00:08:52,260",110,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=527,"Second, many models are equally effective,"
cs-410_1_4_111,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:52,260","00:08:55,760",111,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=532,Researchers are still active and
cs-410_1_4_112,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:55,760","00:08:58,636",112,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=535,trying to find a truly
cs-410_1_4_113,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:00,865","00:09:03,926",113,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=540,"Finally, the state of the art"
cs-410_1_4_114,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:03,926","00:09:05,920",114,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=543,to rely on the following ideas.
cs-410_1_4_115,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:05,920","00:09:08,970",115,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=545,"First, bag of words representation."
cs-410_1_4_116,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:08,970","00:09:14,740",116,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=548,"Second, TF and"
cs-410_1_4_117,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:14,740","00:09:19,787",117,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=554,Such information is used in
cs-410_1_4_118,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:19,787","00:09:25,028",118,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=559,the overall contribution of matching
cs-410_1_4_119,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:25,028","00:09:29,692",119,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=565,These are often combined in interesting
cs-410_1_4_120,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:29,692","00:09:34,210",120,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=569,exactly they are combined to rank
cs-410_1_4_121,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:36,390","00:09:40,560",121,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=576,There are two suggested additional
cs-410_1_4_122,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:41,760","00:09:45,150",122,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=581,The first is a paper where you can
cs-410_1_4_123,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:45,150","00:09:48,420",123,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=585,comparison of multiple
cs-410_1_4_124,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:49,840","00:09:54,674",124,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=589,The second is a book with
cs-410_1_4_125,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:54,674","00:09:58,507",125,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=594,review of different retrieval models.
cs-410_1_4_126,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:58,507","00:10:08,507",126,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=598,[MUSIC]
cs-410_1_5_1,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:00,008","00:00:07,957",1,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=0,[SOUND]
cs-410_1_5_2,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:07,957","00:00:11,940",2,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=7,This lecture is about the
cs-410_1_5_3,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:11,940","00:00:14,410",3,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=11,We're going to give
cs-410_1_5_4,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:18,800","00:00:23,730",4,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=18,"In the last lecture, we talked about"
cs-410_1_5_5,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:23,730","00:00:29,000",5,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=23,"a retrieval model, which would give"
cs-410_1_5_6,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:30,270","00:00:33,600",6,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=30,"In this lecture, we're going to"
cs-410_1_5_7,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:33,600","00:00:36,640",7,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=33,designing a ramping function called
cs-410_1_5_8,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:37,760","00:00:41,500",8,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=37,And we're going to give a brief
cs-410_1_5_9,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:44,330","00:00:47,320",9,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=44,Vector space model is a special case of
cs-410_1_5_10,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:47,320","00:00:50,800",10,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=47,similarity based models
cs-410_1_5_11,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:50,800","00:00:56,049",11,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=50,Which means we assume relevance
cs-410_1_5_12,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:56,049","00:00:59,450",12,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=56,between the document and the query.
cs-410_1_5_13,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:02,140","00:01:06,280",13,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=62,Now whether is this assumption
cs-410_1_5_14,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:06,280","00:01:09,965",14,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=66,"But in order to solve the search problem,"
cs-410_1_5_15,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:09,965","00:01:15,860",15,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=69,we have to convert the vague notion
cs-410_1_5_16,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:15,860","00:01:21,459",16,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=75,definition that can be implemented
cs-410_1_5_17,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:21,459","00:01:26,430",17,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=81,"So in this process,"
cs-410_1_5_18,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:26,430","00:01:31,510",18,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=86,This is the first assumption
cs-410_1_5_19,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:31,510","00:01:36,091",19,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=91,"Basically, we assume that if a document"
cs-410_1_5_20,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:36,091","00:01:37,419",20,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=96,another document.
cs-410_1_5_21,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:37,419","00:01:42,070",21,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=97,Then the first document will be assumed it
cs-410_1_5_22,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:42,070","00:01:45,310",22,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=102,And this is the basis for
cs-410_1_5_23,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:46,800","00:01:51,970",23,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=106,"Again, it's questionable whether this is"
cs-410_1_5_24,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:51,970","00:01:55,750",24,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=111,As we will see later there
cs-410_1_5_25,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:58,300","00:01:59,790",25,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=118,The basic idea of vectors for
cs-410_1_5_26,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:59,790","00:02:03,070",26,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=119,base retrieval model is actually
cs-410_1_5_27,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:03,070","00:02:10,300",27,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=123,Imagine a high dimensional space where
cs-410_1_5_28,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:11,660","00:02:17,088",28,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=131,So here I issue a three dimensional
cs-410_1_5_29,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:17,088","00:02:21,120",29,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=137,"programming, library and presidential."
cs-410_1_5_30,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:21,120","00:02:23,260",30,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=141,So each term here defines one dimension.
cs-410_1_5_31,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:24,370","00:02:28,966",31,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=144,"Now we can consider vectors in this,"
cs-410_1_5_32,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:28,966","00:02:32,275",32,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=148,And we're going to assume
cs-410_1_5_33,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:32,275","00:02:36,340",33,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=152,the query will be placed
cs-410_1_5_34,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:36,340","00:02:43,526",34,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=156,"So for example, on document might"
cs-410_1_5_35,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:43,526","00:02:48,710",35,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=163,Now this means this document
cs-410_1_5_36,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:48,710","00:02:54,657",36,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=168,"presidential, but"
cs-410_1_5_37,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:54,657","00:03:00,270",37,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=174,What does this mean in terms
cs-410_1_5_38,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:00,270","00:03:04,370",38,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=180,That just means we're going to look at
cs-410_1_5_39,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:04,370","00:03:05,710",39,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=184,this vector.
cs-410_1_5_40,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:05,710","00:03:07,910",40,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=185,We're going to ignore everything else.
cs-410_1_5_41,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:07,910","00:03:12,950",41,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=187,"Basically, what we see here is only"
cs-410_1_5_42,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:14,470","00:03:16,380",42,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=194,"Of course,"
cs-410_1_5_43,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:16,380","00:03:20,223",43,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=196,"For example, the orders of"
cs-410_1_5_44,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:20,223","00:03:25,038",44,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=200,that's because we assume that
cs-410_1_5_45,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:25,038","00:03:29,310",45,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=205,So with this presentation
cs-410_1_5_46,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:29,310","00:03:33,472",46,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=209,d1 simply suggests a [INAUDIBLE] library.
cs-410_1_5_47,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:33,472","00:03:37,949",47,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=213,Now this is different from another
cs-410_1_5_48,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:37,949","00:03:39,906",48,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=217,"a different vector, d2 here."
cs-410_1_5_49,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:39,906","00:03:44,319",49,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=219,"Now in this case, the document that"
cs-410_1_5_50,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:44,319","00:03:46,679",50,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=224,it doesn't talk about presidential.
cs-410_1_5_51,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:46,679","00:03:48,830",51,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=226,So what does this remind you?
cs-410_1_5_52,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:48,830","00:03:54,540",52,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=228,Well you can probably guess the topic
cs-410_1_5_53,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:54,540","00:03:56,840",53,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=234,the library is software lab library.
cs-410_1_5_54,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:58,366","00:04:04,110",54,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=238,So this shows that by using
cs-410_1_5_55,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:04,110","00:04:08,190",55,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=244,we can actually capture the differences
cs-410_1_5_56,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:09,610","00:04:12,190",56,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=249,Now you can also imagine
cs-410_1_5_57,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:12,190","00:04:15,296",57,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=252,"For example,"
cs-410_1_5_58,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:15,296","00:04:17,632",58,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=255,that might be a presidential program.
cs-410_1_5_59,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:17,632","00:04:22,649",59,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=257,And in fact we can place all
cs-410_1_5_60,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:22,649","00:04:26,700",60,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=262,And they will be pointing
cs-410_1_5_61,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:26,700","00:04:27,340",61,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=266,"And similarly,"
cs-410_1_5_62,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:27,340","00:04:31,570",62,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=267,we're going to place our query also
cs-410_1_5_63,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:32,630","00:04:37,226",63,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=272,And then we're going to measure the
cs-410_1_5_64,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:37,226","00:04:39,510",64,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=277,every document vector.
cs-410_1_5_65,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:39,510","00:04:40,740",65,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=279,"So in this case for example,"
cs-410_1_5_66,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:40,740","00:04:47,200",66,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=280,we can easily see d2 seems to be
cs-410_1_5_67,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:47,200","00:04:50,040",67,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=287,"And therefore,"
cs-410_1_5_68,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:51,900","00:04:56,530",68,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=291,So this is basically the main
cs-410_1_5_69,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:58,320","00:05:02,455",69,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=298,"So to be more precise,"
cs-410_1_5_70,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:02,455","00:05:09,000",70,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=302,vector space model is a framework.
cs-410_1_5_71,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:09,000","00:05:12,510",71,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=309,"In this framework,"
cs-410_1_5_72,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:12,510","00:05:17,300",72,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=312,"First, we represent a document and"
cs-410_1_5_73,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:18,680","00:05:21,670",73,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=318,So here a term can be any basic concept.
cs-410_1_5_74,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:21,670","00:05:28,920",74,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=321,"For example, a word or a phrase or"
cs-410_1_5_75,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:28,920","00:05:32,880",75,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=328,Those are just sequence of
cs-410_1_5_76,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:34,460","00:05:37,400",76,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=334,Each term is assumed that will
cs-410_1_5_77,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:37,400","00:05:42,170",77,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=337,"Therefore n terms in our vocabulary,"
cs-410_1_5_78,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:44,060","00:05:48,550",78,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=344,A query vector would consist
cs-410_1_5_79,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:49,610","00:05:53,680",79,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=349,corresponding to the weights
cs-410_1_5_80,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:56,250","00:05:59,540",80,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=356,Each document vector is also similar.
cs-410_1_5_81,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:59,540","00:06:04,518",81,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=359,It has a number of elements and
cs-410_1_5_82,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:04,518","00:06:08,900",82,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=364,indicating the weight of
cs-410_1_5_83,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:08,900","00:06:12,397",83,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=368,"Here, you can see,"
cs-410_1_5_84,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:12,397","00:06:14,445",84,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=372,"Therefore, they are N elements"
cs-410_1_5_85,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:15,525","00:06:18,715",85,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=375,each corresponding to the weight
cs-410_1_5_86,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:21,385","00:06:23,860",86,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=381,So the relevance in this case
cs-410_1_5_87,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:23,860","00:06:28,030",87,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=383,will be assumed to be the similarity
cs-410_1_5_88,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:29,420","00:06:33,500",88,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=389,"Therefore, our ranking function"
cs-410_1_5_89,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:33,500","00:06:35,720",89,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=393,between the query vector and
cs-410_1_5_90,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:37,570","00:06:41,780",90,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=397,Now if I ask you to write a program
cs-410_1_5_91,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:41,780","00:06:42,370",91,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=401,in a search engine.
cs-410_1_5_92,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:44,042","00:06:48,248",92,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=404,You would realize that
cs-410_1_5_93,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:48,248","00:06:50,750",93,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=408,"We haven't said a lot of things in detail,"
cs-410_1_5_94,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:50,750","00:06:56,080",94,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=410,therefore it's impossible to actually
cs-410_1_5_95,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:56,080","00:06:58,110",95,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=416,"That's why I said, this is a framework."
cs-410_1_5_96,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:59,370","00:07:03,310",96,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=419,And this has to be refined
cs-410_1_5_97,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:04,350","00:07:08,630",97,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=424,suggest a particular ranking function
cs-410_1_5_98,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:10,890","00:07:13,810",98,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=430,So what does this framework not say?
cs-410_1_5_99,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:13,810","00:07:17,800",99,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=433,"Well, it actually hasn't said many things"
cs-410_1_5_100,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:17,800","00:07:22,520",100,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=437,that would be required in order
cs-410_1_5_101,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:24,420","00:07:30,340",101,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=444,"First, it did not say how we should define"
cs-410_1_5_102,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:32,580","00:07:36,190",102,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=452,We clearly assume
cs-410_1_5_103,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:36,190","00:07:38,660",103,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=456,"Otherwise, there will be redundancy."
cs-410_1_5_104,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:38,660","00:07:45,309",104,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=458,"For example, if two synonyms or somehow"
cs-410_1_5_105,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:45,309","00:07:50,382",105,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=465,Then they would be defining
cs-410_1_5_106,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:50,382","00:07:54,299",106,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=470,that would clearly cause redundancy here.
cs-410_1_5_107,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:54,299","00:07:59,036",107,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=474,Or all the emphasizing of
cs-410_1_5_108,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:59,036","00:08:03,997",108,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=479,because it would be as if
cs-410_1_5_109,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:03,997","00:08:08,760",109,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=483,when you actually matched
cs-410_1_5_110,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:11,420","00:08:16,020",110,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=491,"Secondly, it did not say how we"
cs-410_1_5_111,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:16,020","00:08:18,200",111,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=496,the query in this space.
cs-410_1_5_112,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:18,200","00:08:22,970",112,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=498,Basically that show you some examples
cs-410_1_5_113,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:22,970","00:08:27,190",113,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=502,But where exactly should the vector for
cs-410_1_5_114,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:29,050","00:08:33,930",114,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=509,So this is equivalent to how
cs-410_1_5_115,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:33,930","00:08:39,237",115,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=513,How do you compute the lose
cs-410_1_5_116,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:39,237","00:08:41,808",116,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=519,"This is a very important question,"
cs-410_1_5_117,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:41,808","00:08:47,220",117,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=521,because term weight in the query vector
cs-410_1_5_118,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:48,820","00:08:51,460",118,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=528,"So depending on how you assign the weight,"
cs-410_1_5_119,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:51,460","00:08:55,620",119,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=531,you might prefer some terms
cs-410_1_5_120,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:56,630","00:08:59,472",120,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=536,"Similarly, the total word in"
cs-410_1_5_121,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:59,472","00:09:03,559",121,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=539,It indicates how well the term
cs-410_1_5_122,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:03,559","00:09:08,620",122,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=543,If you got it wrong then you clearly
cs-410_1_5_123,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:10,150","00:09:15,343",123,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=550,"Finally, how to define the similarity"
cs-410_1_5_124,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:15,343","00:09:20,018",124,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=555,So these questions must be addressed
cs-410_1_5_125,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:20,018","00:09:24,620",125,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=560,function that we can actually
cs-410_1_5_126,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:25,920","00:09:31,767",126,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=565,So how do we solve these problems
cs-410_1_5_127,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:31,767","00:09:38,702",127,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=571,is the main topic of the next lecture.
cs-410_1_5_128,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:38,702","00:09:44,589",128,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=578,[MUSIC]
cs-410_1_6_1,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:00,008","00:00:05,424",1,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=0,In this lecture we're going to talk
cs-410_1_6_2,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:05,424","00:00:12,465",2,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=5,about how to instantiate
cs-410_1_6_3,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:12,465","00:00:19,875",3,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=12,that we can get very
cs-410_1_6_4,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:22,974","00:00:27,888",4,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=22,So this is to continue the discussion
cs-410_1_6_5,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:27,888","00:00:32,810",5,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=27,which is one particular approach
cs-410_1_6_6,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:34,420","00:00:38,551",6,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=34,And we're going to talk about how
cs-410_1_6_7,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:38,551","00:00:42,810",7,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=38,the the vector space
cs-410_1_6_8,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:42,810","00:00:48,270",8,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=42,instantiate the framework to derive
cs-410_1_6_9,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:48,270","00:00:53,380",9,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=48,And we're going to cover the symbolist
cs-410_1_6_10,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:55,360","00:00:58,390",10,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=55,So as we discussed in
cs-410_1_6_11,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:58,390","00:01:00,600",11,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=58,the vector space model
cs-410_1_6_12,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:00,600","00:01:02,619",12,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=60,And this didn't say.
cs-410_1_6_13,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:05,266","00:01:11,040",13,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=65,"As we discussed in the previous lecture,"
cs-410_1_6_14,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:11,040","00:01:13,160",14,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=71,It does not say many things.
cs-410_1_6_15,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:14,710","00:01:15,520",15,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=74,"So, for example,"
cs-410_1_6_16,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:15,520","00:01:19,470",16,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=75,here it shows that it did not say
cs-410_1_6_17,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:20,770","00:01:25,939",17,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=80,It also did not say how we place
cs-410_1_6_18,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:27,130","00:01:31,470",18,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=87,It did not say how we place a query
cs-410_1_6_19,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:32,500","00:01:37,250",19,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=92,"And, finally, it did not say how we"
cs-410_1_6_20,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:37,250","00:01:39,020",20,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=97,the query vector and the document vector.
cs-410_1_6_21,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:40,570","00:01:44,860",21,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=100,"So you can imagine,"
cs-410_1_6_22,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:46,040","00:01:52,940",22,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=106,we have to say specifically
cs-410_1_6_23,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:52,940","00:01:54,830",23,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=112,What is exactly xi?
cs-410_1_6_24,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:54,830","00:01:56,700",24,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=114,And what is exactly yi?
cs-410_1_6_25,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:58,460","00:02:02,260",25,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=118,This will determine where
cs-410_1_6_26,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:02,260","00:02:04,560",26,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=122,where we place a query vector.
cs-410_1_6_27,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:04,560","00:02:05,230",27,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=124,"And, of course,"
cs-410_1_6_28,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:05,230","00:02:08,869",28,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=125,we also need to say exactly what
cs-410_1_6_29,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:11,120","00:02:16,653",29,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=131,So if we can provide a definition
cs-410_1_6_30,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:16,653","00:02:22,590",30,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=136,the dimensions and these xi's or
cs-410_1_6_31,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:22,590","00:02:28,725",31,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=142,"queries and document, then we will be"
cs-410_1_6_32,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:28,725","00:02:33,080",32,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=148,query vectors in this well defined space.
cs-410_1_6_33,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:33,080","00:02:36,414",33,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=153,"And then,"
cs-410_1_6_34,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:36,414","00:02:39,685",34,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=156,then we'll have a well
cs-410_1_6_35,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:41,427","00:02:47,630",35,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=161,So let's see how we can do that and
cs-410_1_6_36,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:47,630","00:02:52,460",36,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=167,"Actually, I would suggest you to"
cs-410_1_6_37,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:52,460","00:02:54,980",37,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=172,spend a couple minutes to think about.
cs-410_1_6_38,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:54,980","00:02:58,280",38,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=174,Suppose you are asked
cs-410_1_6_39,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:59,590","00:03:05,810",39,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=179,You have come up with the idea of vector
cs-410_1_6_40,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:05,810","00:03:10,310",40,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=185,"out how to compute these vectors exactly,"
cs-410_1_6_41,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:10,310","00:03:10,810",41,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=190,What would you do?
cs-410_1_6_42,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:12,540","00:03:15,857",42,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=192,"So, think for a couple of minutes,"
cs-410_1_6_43,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:20,581","00:03:26,460",43,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=200,"So, let's think about some simplest ways"
cs-410_1_6_44,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:26,460","00:03:28,810",44,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=206,"First, how do we define the dimension?"
cs-410_1_6_45,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:28,810","00:03:31,430",45,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=208,"Well, the obvious choice is to use"
cs-410_1_6_46,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:31,430","00:03:34,636",46,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=211,each word in our vocabulary
cs-410_1_6_47,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:34,636","00:03:38,775",47,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=214,And show that there are N
cs-410_1_6_48,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:38,775","00:03:41,160",48,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=218,"Therefore, there are N dimensions."
cs-410_1_6_49,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:41,160","00:03:42,818",49,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=221,Each word defines one dimension.
cs-410_1_6_50,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:42,818","00:03:46,273",50,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=222,And this is basically
cs-410_1_6_51,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:48,965","00:03:52,395",51,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=228,Now let's look at how we
cs-410_1_6_52,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:54,395","00:03:57,175",52,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=234,"Again here, the simplest strategy is to"
cs-410_1_6_53,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:58,700","00:04:03,650",53,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=238,use a Bit Vector to represent
cs-410_1_6_54,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:04,720","00:04:07,937",54,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=244,"And that means each element, xi and"
cs-410_1_6_55,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:07,937","00:04:12,020",55,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=247,yi will be taking a value
cs-410_1_6_56,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:13,270","00:04:14,300",56,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=253,"When it's 1,"
cs-410_1_6_57,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:14,300","00:04:20,750",57,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=254,it means the corresponding word is
cs-410_1_6_58,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:20,750","00:04:25,180",58,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=260,"When it's 0,"
cs-410_1_6_59,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:27,070","00:04:31,210",59,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=267,So you can imagine if the user
cs-410_1_6_60,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:31,210","00:04:35,790",60,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=271,then the query vector will only
cs-410_1_6_61,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:37,630","00:04:41,450",61,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=277,"The document vector,"
cs-410_1_6_62,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:41,450","00:04:46,700",62,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=281,But it will also have many zeros since
cs-410_1_6_63,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:46,700","00:04:50,720",63,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=286,Many words don't really
cs-410_1_6_64,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:52,110","00:04:56,570",64,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=292,Many words will only occasionally
cs-410_1_6_65,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:58,680","00:05:01,770",65,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=298,A lot of words will be absent
cs-410_1_6_66,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:04,390","00:05:09,770",66,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=304,So now we have placed the documents and
cs-410_1_6_67,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:11,450","00:05:14,240",67,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=311,Let's look at how we
cs-410_1_6_68,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:15,770","00:05:19,400",68,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=315,"So, a commonly used similarity"
cs-410_1_6_69,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:20,900","00:05:25,590",69,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=320,The Dot Product of two
cs-410_1_6_70,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:25,590","00:05:30,590",70,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=325,the sum of the products of the
cs-410_1_6_71,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:30,590","00:05:38,596",71,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=330,"So, here we see that it's"
cs-410_1_6_72,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:38,596","00:05:40,228",72,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=338,"So, here."
cs-410_1_6_73,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:40,228","00:05:43,420",73,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=340,"And then, x2 multiplied by y2."
cs-410_1_6_74,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:43,420","00:05:47,100",74,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=343,"And then, finally, xn multiplied by yn."
cs-410_1_6_75,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:47,100","00:05:48,810",75,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=347,"And then, we take a sum here."
cs-410_1_6_76,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:50,630","00:05:52,610",76,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=350,So that's a Dot Product.
cs-410_1_6_77,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:52,610","00:05:57,580",77,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=352,"Now, we can represent this in a more"
cs-410_1_6_78,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:58,740","00:06:04,120",78,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=358,So this is only one of the many different
cs-410_1_6_79,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:04,120","00:06:10,640",79,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=364,"So, now we see that we have"
cs-410_1_6_80,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:10,640","00:06:16,050",80,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=370,"we have defined the vectors, and we have"
cs-410_1_6_81,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:16,050","00:06:21,495",81,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=376,So now we finally have the simplest
cs-410_1_6_82,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:21,495","00:06:26,882",82,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=381,on the bit vector [INAUDIBLE] dot product
cs-410_1_6_83,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:26,882","00:06:30,195",83,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=386,And the formula looks like this.
cs-410_1_6_84,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:30,195","00:06:32,415",84,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=390,So this is our formula.
cs-410_1_6_85,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:32,415","00:06:37,670",85,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=392,And that's actually a particular retrieval
cs-410_1_6_86,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:37,670","00:06:42,573",86,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=397,Now we can finally implement this
cs-410_1_6_87,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:42,573","00:06:45,350",87,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=402,and then rank the documents for query.
cs-410_1_6_88,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:45,350","00:06:50,110",88,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=405,"Now, at this point you should"
cs-410_1_6_89,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:50,110","00:06:53,400",89,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=410,to think about how we can
cs-410_1_6_90,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:53,400","00:06:56,972",90,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=413,"So, we have gone through the process"
cs-410_1_6_91,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:56,972","00:07:00,620",91,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=416,using a vector space model.
cs-410_1_6_92,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:00,620","00:07:05,185",92,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=420,"And then,"
cs-410_1_6_93,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:05,185","00:07:09,780",93,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=425,"vectors in the vector space, and"
cs-410_1_6_94,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:09,780","00:07:14,270",94,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=429,"So in the end, we've got a specific"
cs-410_1_6_95,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:15,370","00:07:18,370",95,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=435,"Now, the next step is to think about"
cs-410_1_6_96,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:18,370","00:07:21,160",96,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=438,"actually makes sense, right?"
cs-410_1_6_97,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:21,160","00:07:24,140",97,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=441,Can we expect this function
cs-410_1_6_98,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:24,140","00:07:27,400",98,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=444,when we used it to rank documents for
cs-410_1_6_99,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:28,790","00:07:35,870",99,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=448,So it's worth thinking about what is
cs-410_1_6_100,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:35,870","00:07:38,220",100,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=455,"So, in the end, we'll get a number."
cs-410_1_6_101,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:38,220","00:07:40,240",101,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=458,But what does this number mean?
cs-410_1_6_102,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:40,240","00:07:40,990",102,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=460,Is it meaningful?
cs-410_1_6_103,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:42,200","00:07:44,390",103,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=462,"So, spend a couple minutes"
cs-410_1_6_104,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:45,880","00:07:46,540",104,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=465,"And, of course,"
cs-410_1_6_105,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:46,540","00:07:52,600",105,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=466,the general question here is do you
cs-410_1_6_106,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:52,600","00:07:54,680",106,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=472,Would it actually work well?
cs-410_1_6_107,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:54,680","00:07:58,329",107,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=474,"So, again,"
cs-410_1_6_108,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:58,329","00:08:00,190",108,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=478,Is it actually meaningful?
cs-410_1_6_109,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:01,280","00:08:03,190",109,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=481,Does it mean something?
cs-410_1_6_110,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:03,190","00:08:06,560",110,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=483,This is related to how well
cs-410_1_6_111,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:08,260","00:08:11,530",111,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=488,"So, in order to assess"
cs-410_1_6_112,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:11,530","00:08:15,520",112,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=491,"vector space model actually works well,"
cs-410_1_6_113,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:17,170","00:08:22,570",113,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=497,"So, here I show some sample documents and"
cs-410_1_6_114,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:22,570","00:08:26,390",114,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=502,The query is news about
cs-410_1_6_115,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:26,390","00:08:28,580",115,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=506,And we have five documents here.
cs-410_1_6_116,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:28,580","00:08:32,280",116,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=508,They cover different terms in the query.
cs-410_1_6_117,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:34,710","00:08:39,890",117,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=514,And if you look at these documents for
cs-410_1_6_118,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:41,880","00:08:47,070",118,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=521,"some documents are probably relevant, and"
cs-410_1_6_119,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:48,300","00:08:54,690",119,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=528,"Now, if I asked you to rank these"
cs-410_1_6_120,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:54,690","00:08:57,270",120,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=534,This is basically our ideal ranking.
cs-410_1_6_121,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:57,270","00:09:01,180",121,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=537,"When humans can examine the documents,"
cs-410_1_6_122,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:03,430","00:09:06,900",122,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=543,"Now, so think for a moment,"
cs-410_1_6_123,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:06,900","00:09:10,210",123,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=546,And perhaps by pausing the lecture.
cs-410_1_6_124,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:12,510","00:09:18,750",124,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=552,So I think most of you would
cs-410_1_6_125,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:18,750","00:09:23,353",125,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=558,better than others because they
cs-410_1_6_126,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:23,353","00:09:26,860",126,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=563,"They match news,"
cs-410_1_6_127,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:27,900","00:09:33,160",127,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=567,"So, it looks like these documents"
cs-410_1_6_128,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:33,160","00:09:37,230",128,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=573,They should be ranked on top.
cs-410_1_6_129,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:37,230","00:09:41,810",129,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=577,"And the other three d2, d1, and"
cs-410_1_6_130,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:41,810","00:09:45,990",130,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=581,So we can also say d4 and
cs-410_1_6_131,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:45,990","00:09:50,150",131,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=585,"d1, d2 and d5 are non-relevant."
cs-410_1_6_132,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:50,150","00:09:55,290",132,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=590,So now let's see if our simplest
cs-410_1_6_133,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:55,290","00:09:57,400",133,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=595,or could do something closer.
cs-410_1_6_134,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:57,400","00:10:01,250",134,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=597,"So, let's first think about"
cs-410_1_6_135,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:01,250","00:10:02,272",135,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=601,to score documents.
cs-410_1_6_136,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:02,272","00:10:04,000",136,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=602,All right.
cs-410_1_6_137,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:04,000","00:10:07,420",137,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=604,"Here I show two documents, d1 and d3."
cs-410_1_6_138,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:07,420","00:10:10,390",138,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=607,And we have the query also here.
cs-410_1_6_139,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:10,390","00:10:15,130",139,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=610,"In the vector space model, of course we"
cs-410_1_6_140,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:15,130","00:10:16,830",140,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=615,these documents and the query.
cs-410_1_6_141,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:16,830","00:10:18,860",141,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=616,"Now, I showed the vocabulary here as well."
cs-410_1_6_142,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:18,860","00:10:22,850",142,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=618,So these are the end dimensions
cs-410_1_6_143,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:22,850","00:10:26,620",143,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=622,So what do you think is the vector for
cs-410_1_6_144,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:27,700","00:10:32,870",144,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=627,Note that we're assuming
cs-410_1_6_145,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:32,870","00:10:39,230",145,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=632,to indicate whether a term is absent or
cs-410_1_6_146,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:39,230","00:10:42,380",146,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=639,"So these are zero,1 bit vectors."
cs-410_1_6_147,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:43,880","00:10:45,790",147,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=643,So what do you think is the query vector?
cs-410_1_6_148,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:47,820","00:10:51,200",148,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=647,"Well, the query has four words here."
cs-410_1_6_149,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:51,200","00:10:54,380",149,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=651,"So for these four words,"
cs-410_1_6_150,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:54,380","00:10:55,980",150,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=654,"And for the rest, there will be zeros."
cs-410_1_6_151,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:57,680","00:10:59,290",151,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=657,"Now, what about the documents?"
cs-410_1_6_152,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:59,290","00:11:00,610",152,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=659,It's the same.
cs-410_1_6_153,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:00,610","00:11:03,430",153,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=660,"So d1 has two rows, news and about."
cs-410_1_6_154,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:03,430","00:11:07,367",154,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=663,"So, there are two 1's here,"
cs-410_1_6_155,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:07,367","00:11:12,220",155,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=667,"Similarly, so now that we"
cs-410_1_6_156,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:12,220","00:11:16,380",156,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=672,"have the two vectors,"
cs-410_1_6_157,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:17,470","00:11:19,550",157,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=677,And we're going to use Do Product.
cs-410_1_6_158,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:19,550","00:11:21,610",158,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=679,"So you can see when we use Dot Product,"
cs-410_1_6_159,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:21,610","00:11:26,030",159,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=681,we just multiply the corresponding
cs-410_1_6_160,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:26,030","00:11:30,894",160,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=686,"So these two will be formal product,"
cs-410_1_6_161,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:30,894","00:11:33,920",161,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=690,and these two will
cs-410_1_6_162,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:33,920","00:11:38,210",162,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=693,and these two will generate yet
cs-410_1_6_163,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:40,020","00:11:46,320",163,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=700,"Now you can easily see if we do that,"
cs-410_1_6_164,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:48,180","00:11:54,170",164,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=708,these zeroes because whenever we have
cs-410_1_6_165,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:54,170","00:11:57,538",165,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=714,So when we take a sum
cs-410_1_6_166,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:57,538","00:12:02,940",166,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=717,then the zero entries will be gone.
cs-410_1_6_167,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:04,400","00:12:08,010",167,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=724,"As long as you have one zero,"
cs-410_1_6_168,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:08,010","00:12:14,710",168,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=728,"So, in the fact, we're just"
cs-410_1_6_169,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:14,710","00:12:18,220",169,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=734,"In this case, we have seen two,"
cs-410_1_6_170,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:18,220","00:12:20,240",170,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=738,So what does that mean?
cs-410_1_6_171,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:20,240","00:12:25,190",171,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=740,"Well, that means this number, or"
cs-410_1_6_172,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:25,190","00:12:33,130",172,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=745,is simply the count of how many unique
cs-410_1_6_173,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:33,130","00:12:39,350",173,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=753,Because if a term is matched in the
cs-410_1_6_174,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:41,390","00:12:44,740",174,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=761,"If it's not, then there will"
cs-410_1_6_175,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:46,310","00:12:50,410",175,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=766,"Similarly, if the document has a term but"
cs-410_1_6_176,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:50,410","00:12:53,220",176,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=770,there will be a zero in the query vector.
cs-410_1_6_177,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:53,220","00:12:55,020",177,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=773,So those don't count.
cs-410_1_6_178,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:55,020","00:12:58,760",178,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=775,"So, as a result,"
cs-410_1_6_179,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:58,760","00:13:03,820",179,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=778,measures how many unique query
cs-410_1_6_180,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:03,820","00:13:05,770",180,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=783,This is how we interpret this score.
cs-410_1_6_181,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:07,150","00:13:10,520",181,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=787,"Now, we can also take a look at d3."
cs-410_1_6_182,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:10,520","00:13:18,003",182,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=790,"In this case, you can see the result"
cs-410_1_6_183,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:18,003","00:13:23,140",183,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=798,"distinctive query words news, presidential"
cs-410_1_6_184,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:23,140","00:13:28,200",184,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=803,"Now in this case, this seems"
cs-410_1_6_185,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:29,260","00:13:33,440",185,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=809,And this simplest vector
cs-410_1_6_186,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:33,440","00:13:35,050",186,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=813,So that looks pretty good.
cs-410_1_6_187,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:35,050","00:13:40,030",187,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=815,"However, if we examine this model in"
cs-410_1_6_188,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:40,030","00:13:44,891",188,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=820,"So, here I'm going to show all"
cs-410_1_6_189,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:44,891","00:13:49,977",189,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=824,And you can easily verify they're
cs-410_1_6_190,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:49,977","00:13:55,070",190,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=829,counting the number of unique query
cs-410_1_6_191,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:56,470","00:13:59,270",191,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=836,Now note that this measure
cs-410_1_6_192,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:59,270","00:14:03,740",192,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=839,It basically means if a document
cs-410_1_6_193,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:03,740","00:14:07,210",193,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=843,then the document will be
cs-410_1_6_194,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:07,210","00:14:09,190",194,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=847,And that seems to make sense.
cs-410_1_6_195,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:09,190","00:14:16,870",195,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=849,The only problem is here we can note that
cs-410_1_6_196,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:16,870","00:14:22,320",196,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=856,And they tied with a 3 as a score.
cs-410_1_6_197,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:25,050","00:14:31,000",197,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=865,"So, that's a problem because if you look"
cs-410_1_6_198,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:31,000","00:14:36,920",198,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=871,should be ranked above d3 because
cs-410_1_6_199,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:36,920","00:14:42,100",199,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=876,"d3 only mentions the presidential once,"
cs-410_1_6_200,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:42,100","00:14:47,634",200,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=882,"In the case of d3,"
cs-410_1_6_201,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:47,634","00:14:51,360",201,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=887,But d4 is clearly above
cs-410_1_6_202,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:51,360","00:14:58,200",202,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=891,Another problem is that d2 and
cs-410_1_6_203,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:58,200","00:15:01,880",203,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=898,But if you look at the three words
cs-410_1_6_204,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:01,880","00:15:07,020",204,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=901,"it matched the news, about and campaign."
cs-410_1_6_205,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:07,020","00:15:11,500",205,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=907,"But in the case of d3, it matched news,"
cs-410_1_6_206,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:12,530","00:15:17,960",206,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=912,So intuitively this reads better
cs-410_1_6_207,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:17,960","00:15:21,920",207,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=917,"is more important than matching about,"
cs-410_1_6_208,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:21,920","00:15:24,910",208,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=921,even though about and
cs-410_1_6_209,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:26,170","00:15:30,730",209,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=926,"So intuitively,"
cs-410_1_6_210,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:30,730","00:15:32,750",210,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=930,But this model doesn't do that.
cs-410_1_6_211,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:33,860","00:15:37,150",211,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=933,So that means this model
cs-410_1_6_212,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:37,150","00:15:39,109",212,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=937,We have to solve these problems.
cs-410_1_6_213,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:41,188","00:15:41,991",213,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=941,"To summarize,"
cs-410_1_6_214,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:41,991","00:15:45,770",214,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=941,in this lecture we talked about how
cs-410_1_6_215,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:47,610","00:15:49,540",215,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=947,We mainly need to do three things.
cs-410_1_6_216,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:49,540","00:15:51,796",216,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=949,One is to define the dimension.
cs-410_1_6_217,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:51,796","00:15:59,896",217,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=951,The second is to decide how to place
cs-410_1_6_218,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:59,896","00:16:05,761",218,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=959,and to also place a query in
cs-410_1_6_219,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:07,862","00:16:11,900",219,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=967,And third is to define
cs-410_1_6_220,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:11,900","00:16:15,790",220,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=971,particularly the query vector and
cs-410_1_6_221,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:17,080","00:16:22,430",221,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=977,We also talked about various simple way
cs-410_1_6_222,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:22,430","00:16:27,910",222,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=982,"Indeed, that's probably the simplest"
cs-410_1_6_223,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:27,910","00:16:31,480",223,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=987,"In this case,"
cs-410_1_6_224,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:31,480","00:16:37,430",224,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=991,"We use a zero, 1 bit vector to"
cs-410_1_6_225,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:37,430","00:16:42,690",225,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=997,"In this case, we basically only care"
cs-410_1_6_226,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:42,690","00:16:43,790",226,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1002,We ignore the frequency.
cs-410_1_6_227,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:45,560","00:16:49,220",227,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1005,And we use the Dot Product
cs-410_1_6_228,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:50,360","00:16:53,304",228,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1010,"And with such a instantiation,"
cs-410_1_6_229,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:53,304","00:16:58,870",229,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1013,we showed that the scoring
cs-410_1_6_230,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:58,870","00:17:03,260",230,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1018,a document based on the number of distinct
cs-410_1_6_231,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:17:04,650","00:17:09,800",231,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1024,We also showed that such a simple vector
cs-410_1_6_232,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:17:09,800","00:17:10,720",232,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1029,we need to improve it.
cs-410_1_6_233,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:17:12,540","00:17:18,797",233,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1032,And this is a topic that we're
cs-410_1_6_234,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:17:18,797","00:17:28,797",234,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1038,[MUSIC]
cs-410_2_1_1,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:00,012","00:00:08,850",1,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=0,[SOUND]
cs-410_2_1_2,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:08,850","00:00:12,546",2,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=8,"In this lecture, we are going to talk about how"
cs-410_2_1_3,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:12,546","00:00:14,288",3,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=12,of the vector space model.
cs-410_2_1_4,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:17,448","00:00:22,110",4,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=17,This is a continued discussion
cs-410_2_1_5,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:22,110","00:00:26,859",5,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=22,We're going to focus on how to improve
cs-410_2_1_6,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:30,259","00:00:32,327",6,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=30,"In the previous lecture,"
cs-410_2_1_7,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:32,327","00:00:38,155",7,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=32,you have seen that with simple
cs-410_2_1_8,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:38,155","00:00:43,889",8,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=38,we can come up with a simple scoring
cs-410_2_1_9,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:43,889","00:00:49,440",9,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=43,an account of how many unique query
cs-410_2_1_10,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:50,540","00:00:56,862",10,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=50,We also have seen that this function
cs-410_2_1_11,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:56,862","00:01:00,226",11,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=56,"In particular,"
cs-410_2_1_12,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:00,226","00:01:05,210",12,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=60,they will all get the same score because
cs-410_2_1_13,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:06,322","00:01:11,330",13,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=66,But intuitively we would like
cs-410_2_1_14,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:11,330","00:01:13,070",14,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=71,d2 is really not relevant.
cs-410_2_1_15,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:14,750","00:01:22,600",15,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=74,So the problem here is that this function
cs-410_2_1_16,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:22,600","00:01:27,504",16,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=82,"First, we would like to give"
cs-410_2_1_17,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:27,504","00:01:31,297",17,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=87,matched presidential more times than d3.
cs-410_2_1_18,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:32,520","00:01:37,657",18,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=92,"Second, intuitively, matching presidential"
cs-410_2_1_19,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:37,657","00:01:42,808",19,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=97,"matching about, because about is a very"
cs-410_2_1_20,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:42,808","00:01:44,970",20,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=102,It doesn't really carry that much content.
cs-410_2_1_21,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:47,480","00:01:48,945",21,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=107,"So in this lecture,"
cs-410_2_1_22,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:48,945","00:01:53,868",22,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=108,let's see how we can improve the model
cs-410_2_1_23,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:53,868","00:01:59,990",23,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=113,It's worth thinking at this point
cs-410_2_1_24,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:01,420","00:02:06,600",24,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=121,If we look back at assumptions we have
cs-410_2_1_25,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:06,600","00:02:11,645",25,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=126,"space model,"
cs-410_2_1_26,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:11,645","00:02:15,200",26,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=131,is really coming from
cs-410_2_1_27,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:15,200","00:02:19,391",27,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=135,"In particular, it has to do with how we"
cs-410_2_1_28,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:22,380","00:02:25,390",28,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=142,"So then naturally,"
cs-410_2_1_29,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:25,390","00:02:27,780",29,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=145,we have to revisit those assumptions.
cs-410_2_1_30,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:27,780","00:02:34,755",30,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=147,Perhaps we will have to use different ways
cs-410_2_1_31,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:34,755","00:02:39,130",31,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=154,"In particular, we have to place"
cs-410_2_1_32,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:41,690","00:02:45,708",32,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=161,So let's see how we can improve this.
cs-410_2_1_33,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:45,708","00:02:50,248",33,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=165,One natural thought is in order to
cs-410_2_1_34,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:50,248","00:02:51,266",34,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=170,"the document,"
cs-410_2_1_35,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:51,266","00:02:57,270",35,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=171,we should consider the term frequency
cs-410_2_1_36,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:57,270","00:03:02,900",36,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=177,In order to consider the difference
cs-410_2_1_37,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:02,900","00:03:07,620",37,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=182,term occurred multiple times and one
cs-410_2_1_38,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:07,620","00:03:12,010",38,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=187,"we have to consider the term frequency,"
cs-410_2_1_39,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:13,130","00:03:18,200",39,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=193,"In the simplest model, we only modeled"
cs-410_2_1_40,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:18,200","00:03:25,106",40,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=198,We ignored the actual number of times
cs-410_2_1_41,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:25,106","00:03:26,566",41,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=205,So let's add this back.
cs-410_2_1_42,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:26,566","00:03:30,592",42,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=206,So we're going to then
cs-410_2_1_43,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:30,592","00:03:34,214",43,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=210,a vector with term frequency as element.
cs-410_2_1_44,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:34,214","00:03:39,573",44,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=214,"So that is to say, now the elements"
cs-410_2_1_45,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:39,573","00:03:43,489",45,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=219,the document vector will not be 0 or
cs-410_2_1_46,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:43,489","00:03:49,490",46,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=223,instead they will be the counts of
cs-410_2_1_47,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:52,140","00:03:55,340",47,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=232,So this would bring in additional
cs-410_2_1_48,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:55,340","00:04:00,650",48,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=235,this can be seen as more accurate
cs-410_2_1_49,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:00,650","00:04:03,849",49,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=240,So now let's see what the formula
cs-410_2_1_50,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:03,849","00:04:05,480",50,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=243,representation.
cs-410_2_1_51,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:05,480","00:04:08,920",51,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=245,"So as you'll see on this slide,"
cs-410_2_1_52,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:10,090","00:04:14,270",52,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=250,And so the formula looks
cs-410_2_1_53,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:14,270","00:04:16,310",53,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=254,"In fact, it looks identical."
cs-410_2_1_54,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:16,310","00:04:21,178",54,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=256,"But inside the sum, of course,"
cs-410_2_1_55,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:21,178","00:04:25,855",55,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=261,They are now the counts of word i in
cs-410_2_1_56,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:25,855","00:04:30,208",56,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=265,the query and in the document.
cs-410_2_1_57,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:30,208","00:04:35,931",57,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=270,Now at this point I also suggest you
cs-410_2_1_58,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:35,931","00:04:41,756",58,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=275,just to think about how we can interpret
cs-410_2_1_59,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:41,756","00:04:47,710",59,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=281,It's doing something very similar
cs-410_2_1_60,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:47,710","00:04:50,501",60,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=287,"But because of the change of the vector,"
cs-410_2_1_61,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:50,501","00:04:54,038",61,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=290,now the new score has
cs-410_2_1_62,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:54,038","00:04:56,118",62,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=294,Can you see the difference?
cs-410_2_1_63,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:56,118","00:05:00,995",63,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=296,And it has to do with the consideration
cs-410_2_1_64,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:00,995","00:05:03,360",64,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=300,the same term in a document.
cs-410_2_1_65,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:03,360","00:05:06,590",65,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=303,"More importantly, we would like to know"
cs-410_2_1_66,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:06,590","00:05:08,830",66,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=306,of the simplest vector space model.
cs-410_2_1_67,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:08,830","00:05:12,320",67,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=308,So let's look at this example again.
cs-410_2_1_68,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:12,320","00:05:16,670",68,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=312,So suppose we change the vector
cs-410_2_1_69,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:16,670","00:05:20,620",69,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=316,Now let's look at these
cs-410_2_1_70,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:20,620","00:05:24,580",70,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=320,The query vector is the same
cs-410_2_1_71,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:24,580","00:05:27,240",71,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=324,exactly once in the query.
cs-410_2_1_72,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:27,240","00:05:30,988",72,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=327,So the vector is still a 01 vector.
cs-410_2_1_73,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:30,988","00:05:35,472",73,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=330,"And in fact, d2 is also essentially"
cs-410_2_1_74,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:35,472","00:05:40,120",74,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=335,because none of these words
cs-410_2_1_75,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:40,120","00:05:43,145",75,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=340,"As a result,"
cs-410_2_1_76,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:45,410","00:05:49,760",76,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=345,"The same is true for d3,"
cs-410_2_1_77,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:51,510","00:05:57,400",77,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=351,"But d4 would be different, because"
cs-410_2_1_78,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:57,400","00:06:02,760",78,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=357,So the ending for presidential in the
cs-410_2_1_79,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:04,240","00:06:08,303",79,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=364,"As a result, now the score for"
cs-410_2_1_80,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:08,303","00:06:09,050",80,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=368,It's a 4 now.
cs-410_2_1_81,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:10,130","00:06:13,380",81,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=370,"So this means by using term frequency,"
cs-410_2_1_82,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:13,380","00:06:17,720",82,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=373,we can now rank d4 above d2 and
cs-410_2_1_83,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:19,250","00:06:23,725",83,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=379,So this solved the problem with d4.
cs-410_2_1_84,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:26,190","00:06:32,548",84,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=386,But we can also see that d2 and
cs-410_2_1_85,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:32,548","00:06:38,290",85,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=392,"They still have identical scores,"
cs-410_2_1_86,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:40,420","00:06:42,434",86,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=400,So how can we fix this problem?
cs-410_2_1_87,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:42,434","00:06:46,261",87,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=402,"Intuitively, we would like"
cs-410_2_1_88,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:46,261","00:06:49,736",88,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=406,matching presidential than matching about.
cs-410_2_1_89,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:49,736","00:06:53,028",89,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=409,But how can we solve
cs-410_2_1_90,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:53,028","00:06:57,651",90,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=413,Is there any way to determine
cs-410_2_1_91,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:57,651","00:07:02,478",91,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=417,more importantly and
cs-410_2_1_92,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:02,478","00:07:09,670",92,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=422,About is such a word which does not
cs-410_2_1_93,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:09,670","00:07:11,760",93,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=429,We can essentially ignore that.
cs-410_2_1_94,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:11,760","00:07:15,110",94,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=431,We sometimes call such
cs-410_2_1_95,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:15,110","00:07:18,710",95,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=435,Those are generally very frequent and
cs-410_2_1_96,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:18,710","00:07:21,570",96,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=438,Matching it doesn't really mean anything.
cs-410_2_1_97,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:21,570","00:07:23,260",97,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=441,But computationally how
cs-410_2_1_98,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:24,960","00:07:27,830",98,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=444,"So again, I encourage you to"
cs-410_2_1_99,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:29,460","00:07:33,000",99,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=449,Can you came up with any statistical
cs-410_2_1_100,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:33,000","00:07:34,358",100,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=453,presidential from about?
cs-410_2_1_101,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:37,109","00:07:39,691",101,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=457,"Now if you think about it for a moment,"
cs-410_2_1_102,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:39,691","00:07:46,170",102,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=459,you'll realize that one difference is
cs-410_2_1_103,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:46,170","00:07:50,764",103,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=466,So if you count the occurrence of
cs-410_2_1_104,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:50,764","00:07:55,852",104,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=470,then we will see that about has much
cs-410_2_1_105,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:55,852","00:07:58,990",105,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=475,which tends to occur
cs-410_2_1_106,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:01,000","00:08:05,887",106,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=481,So this idea suggests
cs-410_2_1_107,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:05,887","00:08:09,396",107,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=485,the global statistics of terms or
cs-410_2_1_108,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:09,396","00:08:14,660",108,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=489,some other information
cs-410_2_1_109,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:14,660","00:08:20,568",109,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=494,the element of about in
cs-410_2_1_110,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:20,568","00:08:24,754",110,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=500,"At the same time,"
cs-410_2_1_111,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:24,754","00:08:29,278",111,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=504,the weight of presidential
cs-410_2_1_112,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:29,278","00:08:34,284",112,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=509,"If we can do that, then we can"
cs-410_2_1_113,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:34,284","00:08:39,036",113,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=514,score to be less than 3 while
cs-410_2_1_114,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:39,036","00:08:42,996",114,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=519,Then we would be able to
cs-410_2_1_115,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:45,138","00:08:47,320",115,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=525,So how can we do this systematically?
cs-410_2_1_116,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:48,730","00:08:52,030",116,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=528,"Again, we can rely on"
cs-410_2_1_117,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:52,030","00:08:57,218",117,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=532,"And in this case, the particular idea"
cs-410_2_1_118,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:57,218","00:09:01,425",118,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=537,Now we have seen document
cs-410_2_1_119,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:01,425","00:09:04,030",119,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=541,the modern retrieval functions.
cs-410_2_1_120,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:05,800","00:09:08,500",120,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=545,We discussed this in a previous lecture.
cs-410_2_1_121,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:08,500","00:09:10,859",121,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=548,So here is the specific way of using it.
cs-410_2_1_122,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:10,859","00:09:15,910",122,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=550,Document frequency is the count of
cs-410_2_1_123,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:15,910","00:09:21,000",123,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=555,Here we say inverse document frequency
cs-410_2_1_124,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:21,000","00:09:22,700",124,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=561,that doesn't occur in many documents.
cs-410_2_1_125,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:24,890","00:09:30,544",125,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=564,And so the way to incorporate this
cs-410_2_1_126,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:30,544","00:09:35,477",126,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=570,is then to modify the frequency
cs-410_2_1_127,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:35,477","00:09:39,918",127,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=575,"the IDF of the corresponding word,"
cs-410_2_1_128,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:39,918","00:09:46,044",128,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=579,"If we can do that,"
cs-410_2_1_129,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:46,044","00:09:50,401",129,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=586,"which generally have a lower IDF, and"
cs-410_2_1_130,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:50,401","00:09:56,138",130,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=590,"reward rare words,"
cs-410_2_1_131,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:56,138","00:09:58,078",131,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=596,"So more specifically,"
cs-410_2_1_132,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:58,078","00:10:03,025",132,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=598,the IDF can be defined as
cs-410_2_1_133,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:03,025","00:10:08,845",133,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=603,where M is the total number of documents
cs-410_2_1_134,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:08,845","00:10:15,058",134,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=608,"document frequency, the total number"
cs-410_2_1_135,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:15,058","00:10:18,596",135,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=615,Now if you plot this
cs-410_2_1_136,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:18,596","00:10:23,430",136,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=618,then you would see the curve
cs-410_2_1_137,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:23,430","00:10:28,273",137,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=623,"In general, you can see it"
cs-410_2_1_138,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:28,273","00:10:30,704",138,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=628,"a low DF word, a rare word."
cs-410_2_1_139,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:34,220","00:10:38,680",139,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=634,You can also see the maximum value
cs-410_2_1_140,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:40,952","00:10:45,158",140,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=640,It would be interesting for you to think
cs-410_2_1_141,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:45,158","00:10:46,900",141,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=645,this function.
cs-410_2_1_142,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:46,900","00:10:48,368",142,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=646,This could be an interesting exercise.
cs-410_2_1_143,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:50,918","00:10:55,238",143,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=650,Now the specific function
cs-410_2_1_144,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:55,238","00:10:59,470",144,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=655,the heuristic to simply
cs-410_2_1_145,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:01,528","00:11:05,800",145,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=661,But it turns out that this particular
cs-410_2_1_146,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:07,340","00:11:12,221",146,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=667,Now whether there's a better
cs-410_2_1_147,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:12,221","00:11:14,939",147,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=672,the open research question.
cs-410_2_1_148,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:14,939","00:11:19,665",148,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=674,But it's also clear that if
cs-410_2_1_149,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:19,665","00:11:22,945",149,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=679,"like what's shown here with this line,"
cs-410_2_1_150,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:22,945","00:11:27,200",150,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=682,then it may not be as
cs-410_2_1_151,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:29,110","00:11:34,270",151,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=689,"In particular, you can see"
cs-410_2_1_152,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:35,940","00:11:39,870",152,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=695,and we somehow have
cs-410_2_1_153,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:41,110","00:11:45,770",153,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=701,"After this point, we're going to say these"
cs-410_2_1_154,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:45,770","00:11:48,180",154,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=705,They can be essentially ignored.
cs-410_2_1_155,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:48,180","00:11:52,110",155,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=708,And this makes sense when
cs-410_2_1_156,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:52,110","00:11:57,310",156,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=712,let's say a term occurs in more
cs-410_2_1_157,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:57,310","00:12:01,700",157,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=717,then the term is unlikely very important
cs-410_2_1_158,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:03,120","00:12:05,150",158,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=723,It's not very important
cs-410_2_1_159,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:05,150","00:12:10,145",159,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=725,So with the standard IDF you can
cs-410_2_1_160,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:10,145","00:12:12,285",160,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=730,they all have low weights.
cs-410_2_1_161,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:12,285","00:12:14,020",161,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=732,There's no difference.
cs-410_2_1_162,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:14,020","00:12:16,413",162,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=734,But if you look at
cs-410_2_1_163,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:16,413","00:12:19,206",163,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=736,at this point that there
cs-410_2_1_164,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:19,206","00:12:26,123",164,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=739,So intuitively we'd want to
cs-410_2_1_165,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:26,123","00:12:31,450",165,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=746,of low DF words rather
cs-410_2_1_166,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:32,990","00:12:37,972",166,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=752,"Well, of course,"
cs-410_2_1_167,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:37,972","00:12:43,168",167,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=757,validated by using the empirically
cs-410_2_1_168,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:43,168","00:12:46,920",168,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=763,And we have to use users to
cs-410_2_1_169,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:48,580","00:12:52,948",169,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=768,So now let's see how
cs-410_2_1_170,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:52,948","00:12:55,000",170,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=772,So now let's look at
cs-410_2_1_171,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:56,100","00:13:00,530",171,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=776,"Now without the IDF weighting before,"
cs-410_2_1_172,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:00,530","00:13:05,810",172,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=780,But with IDF weighting we
cs-410_2_1_173,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:05,810","00:13:09,520",173,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=785,by multiplying with the IDF value.
cs-410_2_1_174,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:09,520","00:13:14,150",174,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=789,"For example,"
cs-410_2_1_175,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:14,150","00:13:19,680",175,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=794,in particular for about there's adjustment
cs-410_2_1_176,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:19,680","00:13:23,980",176,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=799,which is smaller than the IDF
cs-410_2_1_177,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:23,980","00:13:28,930",177,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=803,"So if you look at these,"
cs-410_2_1_178,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:28,930","00:13:34,390",178,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=808,"As a result, adjustment here would be"
cs-410_2_1_179,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:37,190","00:13:44,035",179,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=817,"So if we score with these new vectors,"
cs-410_2_1_180,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:44,035","00:13:48,752",180,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=824,"of course,"
cs-410_2_1_181,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:48,752","00:13:54,830",181,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=828,"campaign, but the matching of"
cs-410_2_1_182,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:54,830","00:14:01,250",182,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=834,"So now as a result of IDF weighting,"
cs-410_2_1_183,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:01,250","00:14:06,460",183,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=841,"because it matched a rare word,"
cs-410_2_1_184,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:06,460","00:14:10,156",184,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=846,So this shows that the IDF
cs-410_2_1_185,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:12,798","00:14:19,434",185,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=852,So how effective is this model in
cs-410_2_1_186,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:19,434","00:14:23,438",186,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=859,"Well, let's look at all these"
cs-410_2_1_187,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:23,438","00:14:28,100",187,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=863,These are the new scores
cs-410_2_1_188,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:28,100","00:14:32,580",188,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=868,But how effective is this new weighting
cs-410_2_1_189,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:33,770","00:14:38,520",189,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=873,So now let's see overall how effective
cs-410_2_1_190,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:38,520","00:14:39,490",190,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=878,with TF-IDF weighting.
cs-410_2_1_191,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:40,630","00:14:44,330",191,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=880,Here we show all the five documents
cs-410_2_1_192,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:44,330","00:14:45,720",192,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=884,these are their scores.
cs-410_2_1_193,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:47,000","00:14:49,760",193,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=887,Now we can see the scores for
cs-410_2_1_194,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:49,760","00:14:56,410",194,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=889,the first four documents here
cs-410_2_1_195,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:56,410","00:14:57,650",195,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=896,They are as we expected.
cs-410_2_1_196,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:58,740","00:15:05,710",196,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=898,"However, we also see a new"
cs-410_2_1_197,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:05,710","00:15:10,490",197,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=905,which did not have a very high score
cs-410_2_1_198,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:10,490","00:15:13,270",198,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=910,now actually has a very high score.
cs-410_2_1_199,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:13,270","00:15:15,110",199,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=913,"In fact, it has the highest score here."
cs-410_2_1_200,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:16,850","00:15:19,002",200,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=916,So this creates a new problem.
cs-410_2_1_201,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:19,002","00:15:23,080",201,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=919,This is actually a common phenomenon
cs-410_2_1_202,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:23,080","00:15:25,570",202,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=923,"Basically, when you try"
cs-410_2_1_203,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:25,570","00:15:27,960",203,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=925,you tend to introduce other problems.
cs-410_2_1_204,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:27,960","00:15:32,674",204,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=927,And that's why it's very tricky how
cs-410_2_1_205,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:32,674","00:15:39,658",205,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=932,And what's the best ranking function
cs-410_2_1_206,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:39,658","00:15:41,170",206,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=939,Researchers are still working on that.
cs-410_2_1_207,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:42,360","00:15:47,530",207,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=942,But in the next few lectures we're going
cs-410_2_1_208,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:47,530","00:15:53,030",208,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=947,ideas to further improve this model and
cs-410_2_1_209,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:55,920","00:16:00,740",209,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=955,"So to summarize this lecture, we've talked"
cs-410_2_1_210,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:00,740","00:16:04,340",210,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=960,"model, and"
cs-410_2_1_211,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:04,340","00:16:08,470",211,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=964,the vector space model
cs-410_2_1_212,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:08,470","00:16:13,573",212,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=968,So the improvement is mostly on
cs-410_2_1_213,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:13,573","00:16:18,673",213,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=973,give high weight to a term that
cs-410_2_1_214,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:18,673","00:16:21,790",214,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=978,infrequently in the whole collection.
cs-410_2_1_215,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:23,630","00:16:26,210",215,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=983,And we have seen that this
cs-410_2_1_216,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:26,210","00:16:29,440",216,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=986,looks better than the simplest
cs-410_2_1_217,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:29,440","00:16:33,268",217,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=989,But it also still has some problems.
cs-410_2_1_218,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:33,268","00:16:40,448",218,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=993,In the next lecture we're going to look at
cs-410_2_1_219,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:40,448","00:16:50,448",219,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=1000,[MUSIC]
cs-410_2_2_1,cs-410,2,2, TF Transformation,"00:00:00,000","00:00:05,293",1,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=0,[MUSIC]
cs-410_2_2_2,cs-410,2,2, TF Transformation,"00:00:10,067","00:00:15,310",2,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=10,"In this lecture, we continue"
cs-410_2_2_3,cs-410,2,2, TF Transformation,"00:00:15,310","00:00:18,810",3,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=15,"In particular, we're going to"
cs-410_2_2_4,cs-410,2,2, TF Transformation,"00:00:18,810","00:00:20,100",4,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=18,"In the previous lecture,"
cs-410_2_2_5,cs-410,2,2, TF Transformation,"00:00:20,100","00:00:25,880",5,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=20,we have derived a TF idea of weighting
cs-410_2_2_6,cs-410,2,2, TF Transformation,"00:00:27,100","00:00:31,760",6,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=27,And we have assumed that this model
cs-410_2_2_7,cs-410,2,2, TF Transformation,"00:00:31,760","00:00:37,302",7,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=31,"these examples as shown on this slide,"
cs-410_2_2_8,cs-410,2,2, TF Transformation,"00:00:37,302","00:00:41,340",8,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=37,"d5, which has received a very high score."
cs-410_2_2_9,cs-410,2,2, TF Transformation,"00:00:41,340","00:00:46,510",9,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=41,"Indeed, it has received the highest"
cs-410_2_2_10,cs-410,2,2, TF Transformation,"00:00:46,510","00:00:51,980",10,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=46,But this document is intuitive and
cs-410_2_2_11,cs-410,2,2, TF Transformation,"00:00:53,240","00:00:55,390",11,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=53,"In this lecture,"
cs-410_2_2_12,cs-410,2,2, TF Transformation,"00:00:55,390","00:00:58,960",12,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=55,how we're going to use TF
cs-410_2_2_13,cs-410,2,2, TF Transformation,"00:01:00,410","00:01:04,870",13,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=60,"Before we discuss the details,"
cs-410_2_2_14,cs-410,2,2, TF Transformation,"00:01:04,870","00:01:08,820",14,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=64,this simple TF-IDF
cs-410_2_2_15,cs-410,2,2, TF Transformation,"00:01:08,820","00:01:13,520",15,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=68,And see why this document has
cs-410_2_2_16,cs-410,2,2, TF Transformation,"00:01:13,520","00:01:17,510",16,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=73,"So this is the formula, and"
cs-410_2_2_17,cs-410,2,2, TF Transformation,"00:01:17,510","00:01:21,730",17,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=77,then you will see it involves a sum
cs-410_2_2_18,cs-410,2,2, TF Transformation,"00:01:23,810","00:01:28,140",18,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=83,"And inside the sum, each matched"
cs-410_2_2_19,cs-410,2,2, TF Transformation,"00:01:28,140","00:01:30,259",19,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=88,And this weight is TF-IDF weighting.
cs-410_2_2_20,cs-410,2,2, TF Transformation,"00:01:31,580","00:01:36,853",20,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=91,"So it has an idea of component,"
cs-410_2_2_21,cs-410,2,2, TF Transformation,"00:01:36,853","00:01:42,586",21,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=96,One is the total number of documents
cs-410_2_2_22,cs-410,2,2, TF Transformation,"00:01:42,586","00:01:45,890",22,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=102,The other is the document of frequency.
cs-410_2_2_23,cs-410,2,2, TF Transformation,"00:01:45,890","00:01:48,220",23,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=105,This is the number of
cs-410_2_2_24,cs-410,2,2, TF Transformation,"00:01:48,220","00:01:49,070",24,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=108,This word w.
cs-410_2_2_25,cs-410,2,2, TF Transformation,"00:01:49,070","00:01:53,810",25,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=109,The other variables
cs-410_2_2_26,cs-410,2,2, TF Transformation,"00:01:53,810","00:01:58,350",26,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=113,involved in the formula include
cs-410_2_2_27,cs-410,2,2, TF Transformation,"00:02:01,440","00:02:06,100",27,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=121,"W in the query, and"
cs-410_2_2_28,cs-410,2,2, TF Transformation,"00:02:07,650","00:02:12,150",28,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=127,"If you look at this document again,"
cs-410_2_2_29,cs-410,2,2, TF Transformation,"00:02:12,150","00:02:16,710",29,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=132,the reason why it hasn't
cs-410_2_2_30,cs-410,2,2, TF Transformation,"00:02:16,710","00:02:21,035",30,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=136,it has a very high count of campaign.
cs-410_2_2_31,cs-410,2,2, TF Transformation,"00:02:21,035","00:02:27,170",31,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=141,So the count of campaign in this document
cs-410_2_2_32,cs-410,2,2, TF Transformation,"00:02:27,170","00:02:31,580",32,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=147,"the other documents, and has contributed"
cs-410_2_2_33,cs-410,2,2, TF Transformation,"00:02:31,580","00:02:35,485",33,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=151,So in treating the amount
cs-410_2_2_34,cs-410,2,2, TF Transformation,"00:02:35,485","00:02:40,695",34,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=155,"this document, we need to somehow"
cs-410_2_2_35,cs-410,2,2, TF Transformation,"00:02:40,695","00:02:44,514",35,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=160,of the matching of this
cs-410_2_2_36,cs-410,2,2, TF Transformation,"00:02:44,514","00:02:49,934",36,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=164,And if you think about the matching
cs-410_2_2_37,cs-410,2,2, TF Transformation,"00:02:49,934","00:02:52,193",37,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=169,"you actually would realize,"
cs-410_2_2_38,cs-410,2,2, TF Transformation,"00:02:52,193","00:02:57,540",38,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=172,we probably shouldn't reward
cs-410_2_2_39,cs-410,2,2, TF Transformation,"00:02:57,540","00:03:02,406",39,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=177,"And by that I mean,"
cs-410_2_2_40,cs-410,2,2, TF Transformation,"00:03:02,406","00:03:06,680",40,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=182,says a lot about
cs-410_2_2_41,cs-410,2,2, TF Transformation,"00:03:06,680","00:03:11,570",41,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=186,because it goes from zero
cs-410_2_2_42,cs-410,2,2, TF Transformation,"00:03:11,570","00:03:15,370",42,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=191,And that increase means a lot.
cs-410_2_2_43,cs-410,2,2, TF Transformation,"00:03:17,160","00:03:19,277",43,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=197,"Once we see a word in the document,"
cs-410_2_2_44,cs-410,2,2, TF Transformation,"00:03:19,277","00:03:23,219",44,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=199,it's very likely that the document
cs-410_2_2_45,cs-410,2,2, TF Transformation,"00:03:23,219","00:03:27,934",45,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=203,If we see a extra occurrence on
cs-410_2_2_46,cs-410,2,2, TF Transformation,"00:03:27,934","00:03:33,493",46,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=207,"that is to go from one to two,"
cs-410_2_2_47,cs-410,2,2, TF Transformation,"00:03:33,493","00:03:39,844",47,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=213,occurrence kind of confirmed that it's
cs-410_2_2_48,cs-410,2,2, TF Transformation,"00:03:39,844","00:03:44,220",48,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=219,Now we are more sure that this
cs-410_2_2_49,cs-410,2,2, TF Transformation,"00:03:44,220","00:03:50,430",49,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=224,"But imagine we have seen, let's say,"
cs-410_2_2_50,cs-410,2,2, TF Transformation,"00:03:50,430","00:03:56,140",50,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=230,"Now, adding one extra occurrence is not"
cs-410_2_2_51,cs-410,2,2, TF Transformation,"00:03:56,140","00:03:59,580",51,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=236,because we're already sure that
cs-410_2_2_52,cs-410,2,2, TF Transformation,"00:04:01,160","00:04:06,656",52,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=241,"So if you're thinking this way, it seems"
cs-410_2_2_53,cs-410,2,2, TF Transformation,"00:04:06,656","00:04:12,785",53,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=246,"of a high count of a term, and"
cs-410_2_2_54,cs-410,2,2, TF Transformation,"00:04:12,785","00:04:17,965",54,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=252,So this transformation function is
cs-410_2_2_55,cs-410,2,2, TF Transformation,"00:04:17,965","00:04:22,990",55,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=257,word into a term frequency weight for
cs-410_2_2_56,cs-410,2,2, TF Transformation,"00:04:22,990","00:04:28,420",56,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=262,"So here I show in x axis that we'll count,"
cs-410_2_2_57,cs-410,2,2, TF Transformation,"00:04:28,420","00:04:31,470",57,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=268,y axis I show the term frequency weight.
cs-410_2_2_58,cs-410,2,2, TF Transformation,"00:04:33,360","00:04:36,370",58,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=273,"So in the previous breaking functions,"
cs-410_2_2_59,cs-410,2,2, TF Transformation,"00:04:36,370","00:04:41,140",59,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=276,we actually have imprison rate
cs-410_2_2_60,cs-410,2,2, TF Transformation,"00:04:41,140","00:04:43,480",60,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=281,"So for example,"
cs-410_2_2_61,cs-410,2,2, TF Transformation,"00:04:44,960","00:04:49,070",61,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=284,we actually use such a transformation
cs-410_2_2_62,cs-410,2,2, TF Transformation,"00:04:49,070","00:04:53,420",62,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=289,"Basically if the count is 0,"
cs-410_2_2_63,cs-410,2,2, TF Transformation,"00:04:53,420","00:04:56,790",63,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=293,otherwise it would have a weight of 1.
cs-410_2_2_64,cs-410,2,2, TF Transformation,"00:04:56,790","00:04:57,940",64,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=296,It's flat.
cs-410_2_2_65,cs-410,2,2, TF Transformation,"00:04:59,550","00:05:04,870",65,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=299,"Now, what about using"
cs-410_2_2_66,cs-410,2,2, TF Transformation,"00:05:04,870","00:05:10,515",66,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=304,"Well, that's a linear function, so it has"
cs-410_2_2_67,cs-410,2,2, TF Transformation,"00:05:11,575","00:05:16,775",67,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=311,Now we have just seen that
cs-410_2_2_68,cs-410,2,2, TF Transformation,"00:05:18,395","00:05:20,695",68,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=318,So what we want is something like this.
cs-410_2_2_69,cs-410,2,2, TF Transformation,"00:05:20,695","00:05:23,160",69,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=320,"So for example,"
cs-410_2_2_70,cs-410,2,2, TF Transformation,"00:05:23,160","00:05:26,620",70,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=323,we can't have a sublinear
cs-410_2_2_71,cs-410,2,2, TF Transformation,"00:05:26,620","00:05:29,850",71,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=326,And this will control the influence
cs-410_2_2_72,cs-410,2,2, TF Transformation,"00:05:29,850","00:05:32,270",72,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=329,because it's going to lower its inference.
cs-410_2_2_73,cs-410,2,2, TF Transformation,"00:05:32,270","00:05:35,060",73,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=332,"Yet, it will retain"
cs-410_2_2_74,cs-410,2,2, TF Transformation,"00:05:36,110","00:05:41,570",74,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=336,Or we might want to even bend the curve
cs-410_2_2_75,cs-410,2,2, TF Transformation,"00:05:42,730","00:05:45,320",75,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=342,Now people have tried all these methods.
cs-410_2_2_76,cs-410,2,2, TF Transformation,"00:05:45,320","00:05:48,870",76,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=345,And they are indeed working better than
cs-410_2_2_77,cs-410,2,2, TF Transformation,"00:05:50,230","00:05:54,820",77,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=350,"But so far, what works the best seems"
cs-410_2_2_78,cs-410,2,2, TF Transformation,"00:05:54,820","00:05:56,620",78,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=354,called a BM25 transformation.
cs-410_2_2_79,cs-410,2,2, TF Transformation,"00:05:58,070","00:05:59,480",79,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=358,BM stands for best matching.
cs-410_2_2_80,cs-410,2,2, TF Transformation,"00:06:01,210","00:06:04,830",80,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=361,"Now in this transformation,"
cs-410_2_2_81,cs-410,2,2, TF Transformation,"00:06:06,460","00:06:10,910",81,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=366,And this k controls the upper
cs-410_2_2_82,cs-410,2,2, TF Transformation,"00:06:10,910","00:06:15,165",82,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=370,It's easy to see this
cs-410_2_2_83,cs-410,2,2, TF Transformation,"00:06:15,165","00:06:21,748",83,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=375,because if you look at the x divided by
cs-410_2_2_84,cs-410,2,2, TF Transformation,"00:06:21,748","00:06:28,060",84,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=381,then the numerator will never be able
cs-410_2_2_85,cs-410,2,2, TF Transformation,"00:06:28,060","00:06:29,820",85,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=388,So it's upper bounded by k+1.
cs-410_2_2_86,cs-410,2,2, TF Transformation,"00:06:29,820","00:06:34,540",86,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=389,"Now, this is also difference between"
cs-410_2_2_87,cs-410,2,2, TF Transformation,"00:06:34,540","00:06:35,660",87,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=394,a logarithm transformation.
cs-410_2_2_88,cs-410,2,2, TF Transformation,"00:06:37,010","00:06:38,450",88,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=397,Which it doesn't have upper bound.
cs-410_2_2_89,cs-410,2,2, TF Transformation,"00:06:39,830","00:06:44,490",89,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=399,"Furthermore, one interesting property"
cs-410_2_2_90,cs-410,2,2, TF Transformation,"00:06:45,610","00:06:50,310",90,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=405,we can actually simulate different
cs-410_2_2_91,cs-410,2,2, TF Transformation,"00:06:50,310","00:06:52,900",91,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=410,Including the two extremes
cs-410_2_2_92,cs-410,2,2, TF Transformation,"00:06:52,900","00:06:57,480",92,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=412,"That is, the 0/1 bit transformation and"
cs-410_2_2_93,cs-410,2,2, TF Transformation,"00:06:57,480","00:07:01,890",93,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=417,"So for example, if we set k to 0,"
cs-410_2_2_94,cs-410,2,2, TF Transformation,"00:07:03,630","00:07:06,710",94,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=423,the function value will be 1.
cs-410_2_2_95,cs-410,2,2, TF Transformation,"00:07:06,710","00:07:13,250",95,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=426,So we precisely recover
cs-410_2_2_96,cs-410,2,2, TF Transformation,"00:07:15,630","00:07:20,040",96,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=435,If you set k to very large
cs-410_2_2_97,cs-410,2,2, TF Transformation,"00:07:20,040","00:07:22,919",97,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=440,it's going to look more like
cs-410_2_2_98,cs-410,2,2, TF Transformation,"00:07:24,980","00:07:29,400",98,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=444,"So in this sense,"
cs-410_2_2_99,cs-410,2,2, TF Transformation,"00:07:29,400","00:07:34,600",99,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=449,It allows us to control
cs-410_2_2_100,cs-410,2,2, TF Transformation,"00:07:34,600","00:07:36,780",100,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=454,It also has a nice property
cs-410_2_2_101,cs-410,2,2, TF Transformation,"00:07:38,020","00:07:42,390",101,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=458,And this upper bound is useful to control
cs-410_2_2_102,cs-410,2,2, TF Transformation,"00:07:43,860","00:07:49,718",102,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=463,And so that we can prevent a spammer
cs-410_2_2_103,cs-410,2,2, TF Transformation,"00:07:49,718","00:07:54,947",103,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=469,of one term to spam all queries
cs-410_2_2_104,cs-410,2,2, TF Transformation,"00:07:57,258","00:08:00,824",104,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=477,"In other words, this upper bound"
cs-410_2_2_105,cs-410,2,2, TF Transformation,"00:08:00,824","00:08:05,330",105,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=480,terms would be counted when we aggregate
cs-410_2_2_106,cs-410,2,2, TF Transformation,"00:08:06,680","00:08:10,620",106,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=486,"As I said, this transformation"
cs-410_2_2_107,cs-410,2,2, TF Transformation,"00:08:12,300","00:08:16,890",107,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=492,"So to summarize this lecture,"
cs-410_2_2_108,cs-410,2,2, TF Transformation,"00:08:16,890","00:08:21,930",108,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=496,"Sublinear TF Transformation,"
cs-410_2_2_109,cs-410,2,2, TF Transformation,"00:08:21,930","00:08:25,550",109,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=501,capture the intuition of diminishing
cs-410_2_2_110,cs-410,2,2, TF Transformation,"00:08:26,620","00:08:30,980",110,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=506,It's also to avoid the dominance by
cs-410_2_2_111,cs-410,2,2, TF Transformation,"00:08:30,980","00:08:37,050",111,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=510,This BM25 transformation that we
cs-410_2_2_112,cs-410,2,2, TF Transformation,"00:08:37,050","00:08:43,130",112,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=517,It's so far one of the best-performing
cs-410_2_2_113,cs-410,2,2, TF Transformation,"00:08:43,130","00:08:46,520",113,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=523,"It has upper bound, and so"
cs-410_2_2_114,cs-410,2,2, TF Transformation,"00:08:47,830","00:08:54,080",114,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=527,Now if we're plugging this function into
cs-410_2_2_115,cs-410,2,2, TF Transformation,"00:08:54,080","00:08:57,730",115,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=534,Then we'd end up having
cs-410_2_2_116,cs-410,2,2, TF Transformation,"00:08:57,730","00:09:00,720",116,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=537,which has a BM25 TF component.
cs-410_2_2_117,cs-410,2,2, TF Transformation,"00:09:01,870","00:09:06,833",117,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=541,"Now, this is already"
cs-410_2_2_118,cs-410,2,2, TF Transformation,"00:09:06,833","00:09:11,537",118,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=546,the odd ranking function called BM25.
cs-410_2_2_119,cs-410,2,2, TF Transformation,"00:09:11,537","00:09:17,890",119,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=551,And we'll discuss how we can further
cs-410_2_2_120,cs-410,2,2, TF Transformation,"00:09:17,890","00:09:27,890",120,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=557,[MUSIC]
cs-410_2_3_1,cs-410,2,3, Doc Length Normalization,"00:00:00,012","00:00:03,576",1,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=0,[SOUND]
cs-410_2_3_2,cs-410,2,3, Doc Length Normalization,"00:00:08,498","00:00:10,214",2,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=8,This lecture is about
cs-410_2_3_3,cs-410,2,3, Doc Length Normalization,"00:00:10,214","00:00:14,988",3,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=10,Document Length Normalization
cs-410_2_3_4,cs-410,2,3, Doc Length Normalization,"00:00:14,988","00:00:19,740",4,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=14,"In this lecture, we will continue"
cs-410_2_3_5,cs-410,2,3, Doc Length Normalization,"00:00:19,740","00:00:23,990",5,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=19,"In particular, we're going to discuss the"
cs-410_2_3_6,cs-410,2,3, Doc Length Normalization,"00:00:25,850","00:00:30,330",6,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=25,So far in the lectures about the vector
cs-410_2_3_7,cs-410,2,3, Doc Length Normalization,"00:00:30,330","00:00:37,480",7,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=30,signals from the document to assess
cs-410_2_3_8,cs-410,2,3, Doc Length Normalization,"00:00:37,480","00:00:40,000",8,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=37,"In particular,"
cs-410_2_3_9,cs-410,2,3, Doc Length Normalization,"00:00:40,000","00:00:42,750",9,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=40,The count of a tone in a document.
cs-410_2_3_10,cs-410,2,3, Doc Length Normalization,"00:00:42,750","00:00:48,055",10,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=42,We have also considered it's
cs-410_2_3_11,cs-410,2,3, Doc Length Normalization,"00:00:48,055","00:00:50,795",11,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=48,"IDF, Inverse Document Frequency."
cs-410_2_3_12,cs-410,2,3, Doc Length Normalization,"00:00:50,795","00:00:53,620",12,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=50,But we have not considered
cs-410_2_3_13,cs-410,2,3, Doc Length Normalization,"00:00:54,855","00:01:00,899",13,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=54,"So here I show two example documents,"
cs-410_2_3_14,cs-410,2,3, Doc Length Normalization,"00:01:01,910","00:01:05,098",14,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=61,"D6 on the other hand, has a 5000 words."
cs-410_2_3_15,cs-410,2,3, Doc Length Normalization,"00:01:05,098","00:01:08,882",15,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=65,If you look at the matching
cs-410_2_3_16,cs-410,2,3, Doc Length Normalization,"00:01:08,882","00:01:13,878",16,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=68,"we see that in d6, there are more"
cs-410_2_3_17,cs-410,2,3, Doc Length Normalization,"00:01:13,878","00:01:18,958",17,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=73,"But one might reason that,"
cs-410_2_3_18,cs-410,2,3, Doc Length Normalization,"00:01:18,958","00:01:23,410",18,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=78,these query words in a scattered manner.
cs-410_2_3_19,cs-410,2,3, Doc Length Normalization,"00:01:24,450","00:01:30,060",19,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=84,"So maybe the topic of d6, is not"
cs-410_2_3_20,cs-410,2,3, Doc Length Normalization,"00:01:31,350","00:01:34,980",20,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=91,"So, the discussion of the campaign"
cs-410_2_3_21,cs-410,2,3, Doc Length Normalization,"00:01:34,980","00:01:38,739",21,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=94,may have nothing to do with the managing
cs-410_2_3_22,cs-410,2,3, Doc Length Normalization,"00:01:40,810","00:01:44,600",22,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=100,"In general,"
cs-410_2_3_23,cs-410,2,3, Doc Length Normalization,"00:01:44,600","00:01:47,370",23,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=104,they would have a higher chance for
cs-410_2_3_24,cs-410,2,3, Doc Length Normalization,"00:01:47,370","00:01:54,760",24,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=107,"In fact, if you generate a long document"
cs-410_2_3_25,cs-410,2,3, Doc Length Normalization,"00:01:54,760","00:01:59,690",25,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=114,"a distribution of words, then eventually"
cs-410_2_3_26,cs-410,2,3, Doc Length Normalization,"00:02:00,760","00:02:05,800",26,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=120,"So in this sense, we should penalize on"
cs-410_2_3_27,cs-410,2,3, Doc Length Normalization,"00:02:05,800","00:02:10,400",27,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=125,"better chance matching to any query, and"
cs-410_2_3_28,cs-410,2,3, Doc Length Normalization,"00:02:12,300","00:02:18,600",28,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=132,We also need to be careful in avoiding
cs-410_2_3_29,cs-410,2,3, Doc Length Normalization,"00:02:19,770","00:02:22,790",29,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=139,"On the one hand,"
cs-410_2_3_30,cs-410,2,3, Doc Length Normalization,"00:02:22,790","00:02:27,202",30,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=142,"But on the other hand,"
cs-410_2_3_31,cs-410,2,3, Doc Length Normalization,"00:02:27,202","00:02:30,790",31,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=147,"Now, the reasoning is because"
cs-410_2_3_32,cs-410,2,3, Doc Length Normalization,"00:02:30,790","00:02:31,309",32,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=150,different reasons.
cs-410_2_3_33,cs-410,2,3, Doc Length Normalization,"00:02:32,770","00:02:36,950",33,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=152,"In one case, the document may be"
cs-410_2_3_34,cs-410,2,3, Doc Length Normalization,"00:02:38,270","00:02:44,460",34,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=158,"So for example, think about the vortex"
cs-410_2_3_35,cs-410,2,3, Doc Length Normalization,"00:02:44,460","00:02:47,620",35,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=164,It would use more words than
cs-410_2_3_36,cs-410,2,3, Doc Length Normalization,"00:02:49,560","00:02:53,140",36,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=169,"So, this is a case where we probably"
cs-410_2_3_37,cs-410,2,3, Doc Length Normalization,"00:02:54,980","00:02:57,278",37,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=174,long documents such as a full paper.
cs-410_2_3_38,cs-410,2,3, Doc Length Normalization,"00:02:57,278","00:03:02,520",38,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=177,When we compare the matching
cs-410_2_3_39,cs-410,2,3, Doc Length Normalization,"00:03:02,520","00:03:06,410",39,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=182,document with matching of
cs-410_2_3_40,cs-410,2,3, Doc Length Normalization,"00:03:07,830","00:03:10,700",40,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=187,"Then long papers in general,"
cs-410_2_3_41,cs-410,2,3, Doc Length Normalization,"00:03:10,700","00:03:15,380",41,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=190,have a higher chance of matching clearer
cs-410_2_3_42,cs-410,2,3, Doc Length Normalization,"00:03:15,380","00:03:18,550",42,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=195,"However, there is another case"
cs-410_2_3_43,cs-410,2,3, Doc Length Normalization,"00:03:18,550","00:03:21,750",43,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=198,that is when the document
cs-410_2_3_44,cs-410,2,3, Doc Length Normalization,"00:03:21,750","00:03:24,040",44,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=201,Now consider another
cs-410_2_3_45,cs-410,2,3, Doc Length Normalization,"00:03:24,040","00:03:29,450",45,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=204,where we simply concatenate a lot
cs-410_2_3_46,cs-410,2,3, Doc Length Normalization,"00:03:29,450","00:03:34,190",46,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=209,"In such a case, obviously, we don't want"
cs-410_2_3_47,cs-410,2,3, Doc Length Normalization,"00:03:34,190","00:03:38,270",47,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=214,"Indeed, we probably don't want to penalize"
cs-410_2_3_48,cs-410,2,3, Doc Length Normalization,"00:03:39,700","00:03:46,490",48,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=219,"So that's why, we need to be careful about"
cs-410_2_3_49,cs-410,2,3, Doc Length Normalization,"00:03:48,360","00:03:52,420",49,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=228,"A method of that has been working well,"
cs-410_2_3_50,cs-410,2,3, Doc Length Normalization,"00:03:52,420","00:03:54,890",50,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=232,is called a pivoted length normalization.
cs-410_2_3_51,cs-410,2,3, Doc Length Normalization,"00:03:54,890","00:03:55,860",51,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=234,"And in this case,"
cs-410_2_3_52,cs-410,2,3, Doc Length Normalization,"00:03:55,860","00:04:01,550",52,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=235,the idea is to use the average document
cs-410_2_3_53,cs-410,2,3, Doc Length Normalization,"00:04:01,550","00:04:05,820",53,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=241,That means we'll assume that for
cs-410_2_3_54,cs-410,2,3, Doc Length Normalization,"00:04:05,820","00:04:10,335",54,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=245,the score is about right so
cs-410_2_3_55,cs-410,2,3, Doc Length Normalization,"00:04:10,335","00:04:13,035",55,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=250,But if the document is longer
cs-410_2_3_56,cs-410,2,3, Doc Length Normalization,"00:04:14,125","00:04:16,275",56,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=254,then there will be some penalization.
cs-410_2_3_57,cs-410,2,3, Doc Length Normalization,"00:04:16,275","00:04:20,785",57,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=256,"Whereas if it's a shorter,"
cs-410_2_3_58,cs-410,2,3, Doc Length Normalization,"00:04:20,785","00:04:26,050",58,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=260,So this is illustrated at
cs-410_2_3_59,cs-410,2,3, Doc Length Normalization,"00:04:26,050","00:04:28,578",59,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=266,x-axis you can see the length of document.
cs-410_2_3_60,cs-410,2,3, Doc Length Normalization,"00:04:28,578","00:04:33,390",60,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=268,"On the y-axis, we show the normalizer."
cs-410_2_3_61,cs-410,2,3, Doc Length Normalization,"00:04:33,390","00:04:39,080",61,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=273,"In this case, the Pivoted Length"
cs-410_2_3_62,cs-410,2,3, Doc Length Normalization,"00:04:39,080","00:04:45,850",62,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=279,is seeing to be interpolation of 1 and
cs-410_2_3_63,cs-410,2,3, Doc Length Normalization,"00:04:45,850","00:04:50,460",63,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=285,the normalize the document in length
cs-410_2_3_64,cs-410,2,3, Doc Length Normalization,"00:04:53,110","00:04:58,640",64,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=293,"So you can see here,"
cs-410_2_3_65,cs-410,2,3, Doc Length Normalization,"00:04:58,640","00:05:03,470",65,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=298,"of the document by the average documents,"
cs-410_2_3_66,cs-410,2,3, Doc Length Normalization,"00:05:03,470","00:05:07,890",66,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=303,sense about how this document is
cs-410_2_3_67,cs-410,2,3, Doc Length Normalization,"00:05:07,890","00:05:16,120",67,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=307,also gives us a benefit of not
cs-410_2_3_68,cs-410,2,3, Doc Length Normalization,"00:05:16,120","00:05:18,990",68,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=316,We can measure the length by words or
cs-410_2_3_69,cs-410,2,3, Doc Length Normalization,"00:05:20,760","00:05:24,260",69,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=320,"Anyway, this normalizer"
cs-410_2_3_70,cs-410,2,3, Doc Length Normalization,"00:05:24,260","00:05:29,660",70,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=324,"First we see that, if we set the parameter"
cs-410_2_3_71,cs-410,2,3, Doc Length Normalization,"00:05:29,660","00:05:33,580",71,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=329,"So, there's no lens normalization at all."
cs-410_2_3_72,cs-410,2,3, Doc Length Normalization,"00:05:33,580","00:05:37,540",72,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=333,"So, b, in this sense,"
cs-410_2_3_73,cs-410,2,3, Doc Length Normalization,"00:05:39,450","00:05:44,980",73,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=339,"Whereas, if we set b to a nonzero value,"
cs-410_2_3_74,cs-410,2,3, Doc Length Normalization,"00:05:44,980","00:05:49,010",74,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=344,"All right, so"
cs-410_2_3_75,cs-410,2,3, Doc Length Normalization,"00:05:49,010","00:05:52,179",75,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=349,documents that are longer than
cs-410_2_3_76,cs-410,2,3, Doc Length Normalization,"00:05:53,860","00:05:56,580",76,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=353,"Whereas, the value of"
cs-410_2_3_77,cs-410,2,3, Doc Length Normalization,"00:05:56,580","00:05:59,460",77,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=356,would be smaller for shorter documents.
cs-410_2_3_78,cs-410,2,3, Doc Length Normalization,"00:05:59,460","00:06:02,720",78,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=359,"So in this sense,"
cs-410_2_3_79,cs-410,2,3, Doc Length Normalization,"00:06:02,720","00:06:07,230",79,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=362,"long documents, and"
cs-410_2_3_80,cs-410,2,3, Doc Length Normalization,"00:06:09,040","00:06:11,500",80,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=369,The degree of penalization
cs-410_2_3_81,cs-410,2,3, Doc Length Normalization,"00:06:11,500","00:06:16,750",81,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=371,"because if we set b to a larger value,"
cs-410_2_3_82,cs-410,2,3, Doc Length Normalization,"00:06:16,750","00:06:20,580",82,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=376,There's even more penalization for
cs-410_2_3_83,cs-410,2,3, Doc Length Normalization,"00:06:20,580","00:06:22,380",83,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=380,the short documents.
cs-410_2_3_84,cs-410,2,3, Doc Length Normalization,"00:06:22,380","00:06:25,440",84,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=382,"By adjusting b, which varies from 0 to 1,"
cs-410_2_3_85,cs-410,2,3, Doc Length Normalization,"00:06:25,440","00:06:29,450",85,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=385,we can control the degree
cs-410_2_3_86,cs-410,2,3, Doc Length Normalization,"00:06:29,450","00:06:35,050",86,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=389,"So, if we plug in this length"
cs-410_2_3_87,cs-410,2,3, Doc Length Normalization,"00:06:35,050","00:06:40,490",87,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=395,"the vector space model, ranking functions"
cs-410_2_3_88,cs-410,2,3, Doc Length Normalization,"00:06:41,510","00:06:45,270",88,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=401,Then we will end up having
cs-410_2_3_89,cs-410,2,3, Doc Length Normalization,"00:06:46,370","00:06:51,569",89,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=406,And these are in fact the state of
cs-410_2_3_90,cs-410,2,3, Doc Length Normalization,"00:06:51,569","00:06:55,290",90,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=411,Let's take a look at each of them.
cs-410_2_3_91,cs-410,2,3, Doc Length Normalization,"00:06:55,290","00:07:00,972",91,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=415,The first one is called a pivoted length
cs-410_2_3_92,cs-410,2,3, Doc Length Normalization,"00:07:00,972","00:07:04,980",92,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=420,and a reference in [INAUDIBLE]
cs-410_2_3_93,cs-410,2,3, Doc Length Normalization,"00:07:04,980","00:07:11,836",93,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=424,"And here we see that, it's basically"
cs-410_2_3_94,cs-410,2,3, Doc Length Normalization,"00:07:11,836","00:07:16,830",94,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=431,the idea of component should
cs-410_2_3_95,cs-410,2,3, Doc Length Normalization,"00:07:18,010","00:07:21,608",95,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=438,There is also a query term
cs-410_2_3_96,cs-410,2,3, Doc Length Normalization,"00:07:24,628","00:07:30,504",96,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=444,"And then, in the middle, there is"
cs-410_2_3_97,cs-410,2,3, Doc Length Normalization,"00:07:30,504","00:07:35,486",97,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=450,we see we use the double logarithm
cs-410_2_3_98,cs-410,2,3, Doc Length Normalization,"00:07:35,486","00:07:40,460",98,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=455,this is to achieve
cs-410_2_3_99,cs-410,2,3, Doc Length Normalization,"00:07:40,460","00:07:45,488",99,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=460,But we also put a document
cs-410_2_3_100,cs-410,2,3, Doc Length Normalization,"00:07:45,488","00:07:50,596",100,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=465,"Right, so this would cause"
cs-410_2_3_101,cs-410,2,3, Doc Length Normalization,"00:07:50,596","00:07:56,698",101,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=470,"because the larger the denominator is,"
cs-410_2_3_102,cs-410,2,3, Doc Length Normalization,"00:07:56,698","00:07:59,660",102,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=476,And this is of course controlled
cs-410_2_3_103,cs-410,2,3, Doc Length Normalization,"00:08:01,420","00:08:06,130",103,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=481,"And you can see again, if b is set to 0"
cs-410_2_3_104,cs-410,2,3, Doc Length Normalization,"00:08:08,760","00:08:16,350",104,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=488,"Okay, so this is one of the two most"
cs-410_2_3_105,cs-410,2,3, Doc Length Normalization,"00:08:16,350","00:08:20,652",105,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=496,"The next one called a BM25 or Okapi,"
cs-410_2_3_106,cs-410,2,3, Doc Length Normalization,"00:08:20,652","00:08:26,971",106,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=500,is also similar in that it
cs-410_2_3_107,cs-410,2,3, Doc Length Normalization,"00:08:26,971","00:08:30,478",107,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=506,and query IDF component here.
cs-410_2_3_108,cs-410,2,3, Doc Length Normalization,"00:08:32,958","00:08:36,150",108,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=512,"But in the middle,"
cs-410_2_3_109,cs-410,2,3, Doc Length Normalization,"00:08:36,150","00:08:41,450",109,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=516,"As we explained,"
cs-410_2_3_110,cs-410,2,3, Doc Length Normalization,"00:08:41,450","00:08:46,460",110,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=521,and that does sublinear
cs-410_2_3_111,cs-410,2,3, Doc Length Normalization,"00:08:48,340","00:08:53,610",111,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=528,In this case we have put the length
cs-410_2_3_112,cs-410,2,3, Doc Length Normalization,"00:08:53,610","00:08:58,160",112,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=533,We're adjusting k but
cs-410_2_3_113,cs-410,2,3, Doc Length Normalization,"00:08:58,160","00:09:02,610",113,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=538,because we put a normalizer
cs-410_2_3_114,cs-410,2,3, Doc Length Normalization,"00:09:02,610","00:09:08,680",114,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=542,"Therefore, again, if a document is longer"
cs-410_2_3_115,cs-410,2,3, Doc Length Normalization,"00:09:10,110","00:09:16,070",115,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=550,So you can see after we have gone through
cs-410_2_3_116,cs-410,2,3, Doc Length Normalization,"00:09:16,070","00:09:24,226",116,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=556,and we have in the end reached
cs-410_2_3_117,cs-410,2,3, Doc Length Normalization,"00:09:24,226","00:09:28,726",117,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=564,"So, So far, we have talked about"
cs-410_2_3_118,cs-410,2,3, Doc Length Normalization,"00:09:28,726","00:09:33,530",118,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=568,mainly how to place the document
cs-410_2_3_119,cs-410,2,3, Doc Length Normalization,"00:09:35,010","00:09:39,752",119,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=575,"And, this has played an important role"
cs-410_2_3_120,cs-410,2,3, Doc Length Normalization,"00:09:39,752","00:09:41,169",120,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=579,the simple function.
cs-410_2_3_121,cs-410,2,3, Doc Length Normalization,"00:09:41,169","00:09:45,728",121,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=581,"But there are also other dimensions,"
cs-410_2_3_122,cs-410,2,3, Doc Length Normalization,"00:09:45,728","00:09:50,343",122,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=585,"For example, can we further"
cs-410_2_3_123,cs-410,2,3, Doc Length Normalization,"00:09:50,343","00:09:53,648",123,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=590,the dimension of the Vector Space Model?
cs-410_2_3_124,cs-410,2,3, Doc Length Normalization,"00:09:53,648","00:09:57,424",124,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=593,"Now, we've just assumed that the bag"
cs-410_2_3_125,cs-410,2,3, Doc Length Normalization,"00:09:57,424","00:10:01,240",125,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=597,"dimension as a word but obviously,"
cs-410_2_3_126,cs-410,2,3, Doc Length Normalization,"00:10:01,240","00:10:07,040",126,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=601,"For example, a stemmed word, those"
cs-410_2_3_127,cs-410,2,3, Doc Length Normalization,"00:10:07,040","00:10:11,110",127,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=607,"into the same root form, so"
cs-410_2_3_128,cs-410,2,3, Doc Length Normalization,"00:10:11,110","00:10:16,510",128,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=611,that computation and computing were all
cs-410_2_3_129,cs-410,2,3, Doc Length Normalization,"00:10:16,510","00:10:18,740",129,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=616,We get those stop word removal.
cs-410_2_3_130,cs-410,2,3, Doc Length Normalization,"00:10:18,740","00:10:25,270",130,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=618,This is to remove some very common words
cs-410_2_3_131,cs-410,2,3, Doc Length Normalization,"00:10:26,760","00:10:29,750",131,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=626,We get use of phrases
cs-410_2_3_132,cs-410,2,3, Doc Length Normalization,"00:10:29,750","00:10:33,630",132,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=629,We can even use later in
cs-410_2_3_133,cs-410,2,3, Doc Length Normalization,"00:10:33,630","00:10:38,540",133,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=633,some clusters of words that represent the
cs-410_2_3_134,cs-410,2,3, Doc Length Normalization,"00:10:39,700","00:10:44,080",134,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=639,"We can also use smaller unit,"
cs-410_2_3_135,cs-410,2,3, Doc Length Normalization,"00:10:44,080","00:10:48,820",135,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=644,are sequences of and
cs-410_2_3_136,cs-410,2,3, Doc Length Normalization,"00:10:50,320","00:10:57,087",136,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=650,"However, in practice, people have found"
cs-410_2_3_137,cs-410,2,3, Doc Length Normalization,"00:10:57,087","00:11:02,148",137,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=657,phrases is still the most effective
cs-410_2_3_138,cs-410,2,3, Doc Length Normalization,"00:11:02,148","00:11:08,930",138,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=662,"So, this is still so far the most"
cs-410_2_3_139,cs-410,2,3, Doc Length Normalization,"00:11:10,120","00:11:12,560",139,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=670,And it's used in all major search engines.
cs-410_2_3_140,cs-410,2,3, Doc Length Normalization,"00:11:13,960","00:11:18,910",140,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=673,"I should also mention, that sometimes"
cs-410_2_3_141,cs-410,2,3, Doc Length Normalization,"00:11:18,910","00:11:21,300",141,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=678,domain specific tokenization.
cs-410_2_3_142,cs-410,2,3, Doc Length Normalization,"00:11:21,300","00:11:27,991",142,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=681,"And this is actually very important, as we"
cs-410_2_3_143,cs-410,2,3, Doc Length Normalization,"00:11:27,991","00:11:33,545",143,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=687,prevent us from matching them with each
cs-410_2_3_144,cs-410,2,3, Doc Length Normalization,"00:11:33,545","00:11:39,660",144,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=693,"In some languages like Chinese,"
cs-410_2_3_145,cs-410,2,3, Doc Length Normalization,"00:11:40,860","00:11:47,290",145,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=700,text to obtain word band rates because
cs-410_2_3_146,cs-410,2,3, Doc Length Normalization,"00:11:47,290","00:11:51,505",146,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=707,A word might correspond to one
cs-410_2_3_147,cs-410,2,3, Doc Length Normalization,"00:11:51,505","00:11:53,248",147,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=711,even three characters.
cs-410_2_3_148,cs-410,2,3, Doc Length Normalization,"00:11:53,248","00:11:58,164",148,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=713,"So, it's easier in English when we"
cs-410_2_3_149,cs-410,2,3, Doc Length Normalization,"00:11:58,164","00:12:02,590",149,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=718,"In some other languages, we may need"
cs-410_2_3_150,cs-410,2,3, Doc Length Normalization,"00:12:02,590","00:12:05,098",150,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=722,figure a way out of what
cs-410_2_3_151,cs-410,2,3, Doc Length Normalization,"00:12:05,098","00:12:10,850",151,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=725,There is also the possibility to
cs-410_2_3_152,cs-410,2,3, Doc Length Normalization,"00:12:10,850","00:12:13,510",152,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=730,And so
cs-410_2_3_153,cs-410,2,3, Doc Length Normalization,"00:12:13,510","00:12:16,137",153,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=733,one can imagine there are other measures.
cs-410_2_3_154,cs-410,2,3, Doc Length Normalization,"00:12:16,137","00:12:20,550",154,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=736,"For example, we can measure the cosine"
cs-410_2_3_155,cs-410,2,3, Doc Length Normalization,"00:12:20,550","00:12:23,740",155,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=740,Or we can use Euclidean distance measure.
cs-410_2_3_156,cs-410,2,3, Doc Length Normalization,"00:12:24,880","00:12:27,280",156,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=744,"And these are all possible, but"
cs-410_2_3_157,cs-410,2,3, Doc Length Normalization,"00:12:27,280","00:12:32,680",157,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=747,dot product seems still the best and
cs-410_2_3_158,cs-410,2,3, Doc Length Normalization,"00:12:33,780","00:12:38,143",158,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=753,"In fact that it's sufficiently general,"
cs-410_2_3_159,cs-410,2,3, Doc Length Normalization,"00:12:38,143","00:12:43,280",159,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=758,if you consider the possibilities
cs-410_2_3_160,cs-410,2,3, Doc Length Normalization,"00:12:44,280","00:12:45,390",160,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=764,"So, for example,"
cs-410_2_3_161,cs-410,2,3, Doc Length Normalization,"00:12:45,390","00:12:50,440",161,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=765,cosine measure can be thought of as the
cs-410_2_3_162,cs-410,2,3, Doc Length Normalization,"00:12:50,440","00:12:54,720",162,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=770,"That means, we first normalize each factor"
cs-410_2_3_163,cs-410,2,3, Doc Length Normalization,"00:12:54,720","00:12:57,720",163,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=774,That would be critical
cs-410_2_3_164,cs-410,2,3, Doc Length Normalization,"00:12:57,720","00:13:03,860",164,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=777,"I just mentioned that the BM25, seems to"
cs-410_2_3_165,cs-410,2,3, Doc Length Normalization,"00:13:04,930","00:13:09,420",165,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=784,But there has been also further
cs-410_2_3_166,cs-410,2,3, Doc Length Normalization,"00:13:09,420","00:13:15,478",166,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=789,"Although, none of these words have"
cs-410_2_3_167,cs-410,2,3, Doc Length Normalization,"00:13:15,478","00:13:20,090",167,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=795,"So in one line work,"
cs-410_2_3_168,cs-410,2,3, Doc Length Normalization,"00:13:20,090","00:13:26,663",168,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=800,"Here, F stands for field, and this is"
cs-410_2_3_169,cs-410,2,3, Doc Length Normalization,"00:13:26,663","00:13:30,960",169,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=806,"So for example, you might consider"
cs-410_2_3_170,cs-410,2,3, Doc Length Normalization,"00:13:30,960","00:13:33,240",170,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=810,or body of the research article.
cs-410_2_3_171,cs-410,2,3, Doc Length Normalization,"00:13:33,240","00:13:39,800",171,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=813,"Or even anchor text on the web page,"
cs-410_2_3_172,cs-410,2,3, Doc Length Normalization,"00:13:39,800","00:13:44,970",172,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=819,links to other pages and
cs-410_2_3_173,cs-410,2,3, Doc Length Normalization,"00:13:44,970","00:13:50,490",173,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=824,a proper way of different fields to help
cs-410_2_3_174,cs-410,2,3, Doc Length Normalization,"00:13:50,490","00:13:55,430",174,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=830,When we use BM25 for such a document and
cs-410_2_3_175,cs-410,2,3, Doc Length Normalization,"00:13:55,430","00:14:00,750",175,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=835,the obvious choice is to apply BM25 for
cs-410_2_3_176,cs-410,2,3, Doc Length Normalization,"00:14:00,750","00:14:06,620",176,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=840,"Basically, the idea of BM25F is"
cs-410_2_3_177,cs-410,2,3, Doc Length Normalization,"00:14:06,620","00:14:11,670",177,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=846,"counts of terms in all the fields,"
cs-410_2_3_178,cs-410,2,3, Doc Length Normalization,"00:14:11,670","00:14:19,430",178,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=851,"Now, this has advantage of avoiding over"
cs-410_2_3_179,cs-410,2,3, Doc Length Normalization,"00:14:19,430","00:14:22,000",179,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=859,Remember in the sublinear
cs-410_2_3_180,cs-410,2,3, Doc Length Normalization,"00:14:22,000","00:14:27,800",180,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=862,the first occurrence is very important and
cs-410_2_3_181,cs-410,2,3, Doc Length Normalization,"00:14:27,800","00:14:29,660",181,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=867,"And if we do that for all the fields,"
cs-410_2_3_182,cs-410,2,3, Doc Length Normalization,"00:14:29,660","00:14:35,110",182,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=869,then the same term might have gained
cs-410_2_3_183,cs-410,2,3, Doc Length Normalization,"00:14:35,110","00:14:38,820",183,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=875,But when we combine these
cs-410_2_3_184,cs-410,2,3, Doc Length Normalization,"00:14:38,820","00:14:42,110",184,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=878,we just do the transformation one time.
cs-410_2_3_185,cs-410,2,3, Doc Length Normalization,"00:14:42,110","00:14:42,870",185,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=882,"At that time,"
cs-410_2_3_186,cs-410,2,3, Doc Length Normalization,"00:14:42,870","00:14:47,170",186,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=882,then the extra occurrences will not be
cs-410_2_3_187,cs-410,2,3, Doc Length Normalization,"00:14:48,790","00:14:54,039",187,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=888,And this method has been working very well
cs-410_2_3_188,cs-410,2,3, Doc Length Normalization,"00:14:55,810","00:14:59,283",188,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=895,The other line of extension
cs-410_2_3_189,cs-410,2,3, Doc Length Normalization,"00:14:59,283","00:15:03,810",189,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=899,"In this line,"
cs-410_2_3_190,cs-410,2,3, Doc Length Normalization,"00:15:03,810","00:15:05,980",190,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=903,over penalization of
cs-410_2_3_191,cs-410,2,3, Doc Length Normalization,"00:15:08,880","00:15:13,990",191,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=908,"So to address this problem,"
cs-410_2_3_192,cs-410,2,3, Doc Length Normalization,"00:15:13,990","00:15:18,180",192,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=913,We can simply add a small constant
cs-410_2_3_193,cs-410,2,3, Doc Length Normalization,"00:15:18,180","00:15:23,400",193,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=918,"But what's interesting is that,"
cs-410_2_3_194,cs-410,2,3, Doc Length Normalization,"00:15:23,400","00:15:28,340",194,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=923,"doing such a small modification,"
cs-410_2_3_195,cs-410,2,3, Doc Length Normalization,"00:15:28,340","00:15:33,570",195,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=928,the problem of over penalization of
cs-410_2_3_196,cs-410,2,3, Doc Length Normalization,"00:15:33,570","00:15:36,380",196,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=933,"So the new formula called BM25+,"
cs-410_2_3_197,cs-410,2,3, Doc Length Normalization,"00:15:36,380","00:15:40,990",197,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=936,is empirically and
cs-410_2_3_198,cs-410,2,3, Doc Length Normalization,"00:15:42,590","00:15:48,432",198,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=942,So to summarize all what we have
cs-410_2_3_199,cs-410,2,3, Doc Length Normalization,"00:15:48,432","00:15:52,100",199,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=948,here are the major take away points.
cs-410_2_3_200,cs-410,2,3, Doc Length Normalization,"00:15:52,100","00:15:57,590",200,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=952,"First, in such a model,"
cs-410_2_3_201,cs-410,2,3, Doc Length Normalization,"00:15:57,590","00:16:01,780",201,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=957,Assuming that relevance of a document
cs-410_2_3_202,cs-410,2,3, Doc Length Normalization,"00:16:02,820","00:16:08,030",202,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=962,basically proportional to the similarity
cs-410_2_3_203,cs-410,2,3, Doc Length Normalization,"00:16:08,030","00:16:10,640",203,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=968,"So naturally,"
cs-410_2_3_204,cs-410,2,3, Doc Length Normalization,"00:16:10,640","00:16:13,830",204,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=970,document must have been
cs-410_2_3_205,cs-410,2,3, Doc Length Normalization,"00:16:13,830","00:16:19,050",205,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=973,"And in this case, we will present them as"
cs-410_2_3_206,cs-410,2,3, Doc Length Normalization,"00:16:19,050","00:16:24,170",206,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=979,"Where the dimensions are defined by words,"
cs-410_2_3_207,cs-410,2,3, Doc Length Normalization,"00:16:25,470","00:16:29,850",207,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=985,"And we generally, need to use a lot of"
cs-410_2_3_208,cs-410,2,3, Doc Length Normalization,"00:16:29,850","00:16:34,560",208,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=989,"We use some examples, which show"
cs-410_2_3_209,cs-410,2,3, Doc Length Normalization,"00:16:34,560","00:16:37,200",209,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=994,including Tf weighting and transformation.
cs-410_2_3_210,cs-410,2,3, Doc Length Normalization,"00:16:38,740","00:16:41,950",210,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=998,"And IDF weighting, and"
cs-410_2_3_211,cs-410,2,3, Doc Length Normalization,"00:16:41,950","00:16:45,890",211,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1001,These major heuristics are the most
cs-410_2_3_212,cs-410,2,3, Doc Length Normalization,"00:16:45,890","00:16:51,544",212,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1005,to ensure such a general ranking function
cs-410_2_3_213,cs-410,2,3, Doc Length Normalization,"00:16:51,544","00:16:55,640",213,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1011,"And finally, BM25 and"
cs-410_2_3_214,cs-410,2,3, Doc Length Normalization,"00:16:55,640","00:16:59,890",214,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1015,to be the most effective formulas
cs-410_2_3_215,cs-410,2,3, Doc Length Normalization,"00:16:59,890","00:17:05,100",215,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1019,"Now I have to say that, I put BM25 in"
cs-410_2_3_216,cs-410,2,3, Doc Length Normalization,"00:17:05,100","00:17:09,759",216,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1025,"in fact, the BM25 has been derived"
cs-410_2_3_217,cs-410,2,3, Doc Length Normalization,"00:17:11,970","00:17:17,470",217,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1031,So the reason why I've put it in
cs-410_2_3_218,cs-410,2,3, Doc Length Normalization,"00:17:17,470","00:17:22,540",218,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1037,the ranking function actually has a nice
cs-410_2_3_219,cs-410,2,3, Doc Length Normalization,"00:17:22,540","00:17:23,450",219,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1042,"We can easily see,"
cs-410_2_3_220,cs-410,2,3, Doc Length Normalization,"00:17:23,450","00:17:27,390",220,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1043,it looks very much like a vector space
cs-410_2_3_221,cs-410,2,3, Doc Length Normalization,"00:17:28,890","00:17:34,640",221,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1048,The second reason is because the original
cs-410_2_3_222,cs-410,2,3, Doc Length Normalization,"00:17:36,070","00:17:39,420",222,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1056,And that form of IDF after
cs-410_2_3_223,cs-410,2,3, Doc Length Normalization,"00:17:39,420","00:17:44,630",223,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1059,well as the standard IDF
cs-410_2_3_224,cs-410,2,3, Doc Length Normalization,"00:17:44,630","00:17:47,910",224,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1064,"So as effective retrieval function,"
cs-410_2_3_225,cs-410,2,3, Doc Length Normalization,"00:17:47,910","00:17:53,360",225,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1067,BM25 should probably use a heuristic
cs-410_2_3_226,cs-410,2,3, Doc Length Normalization,"00:17:53,360","00:17:55,628",226,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1073,To make them even more look
cs-410_2_3_227,cs-410,2,3, Doc Length Normalization,"00:17:59,218","00:18:01,460",227,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1079,There are some additional readings.
cs-410_2_3_228,cs-410,2,3, Doc Length Normalization,"00:18:01,460","00:18:05,330",228,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1081,"The first is, a paper about"
cs-410_2_3_229,cs-410,2,3, Doc Length Normalization,"00:18:05,330","00:18:09,224",229,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1085,It's an excellent example
cs-410_2_3_230,cs-410,2,3, Doc Length Normalization,"00:18:09,224","00:18:13,650",230,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1089,analysis to suggest the need for
cs-410_2_3_231,cs-410,2,3, Doc Length Normalization,"00:18:13,650","00:18:17,590",231,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1093,then further derive the length
cs-410_2_3_232,cs-410,2,3, Doc Length Normalization,"00:18:17,590","00:18:22,830",232,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1097,"The second, is the original paper"
cs-410_2_3_233,cs-410,2,3, Doc Length Normalization,"00:18:24,180","00:18:28,452",233,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1104,"The third paper,"
cs-410_2_3_234,cs-410,2,3, Doc Length Normalization,"00:18:28,452","00:18:31,660",234,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1108,"its extensions, particularly BM25 F."
cs-410_2_3_235,cs-410,2,3, Doc Length Normalization,"00:18:32,860","00:18:38,305",235,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1112,"And finally, in the last paper"
cs-410_2_3_236,cs-410,2,3, Doc Length Normalization,"00:18:38,305","00:18:43,768",236,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1118,BM25 to correct the over
cs-410_2_3_237,cs-410,2,3, Doc Length Normalization,"00:18:43,768","00:18:53,768",237,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1123,[MUSIC]
cs-410_2_4_1,cs-410,2,4, Implementation of TR Systems,"00:00:00,248","00:00:06,368",1,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=0,[MUSIC]
cs-410_2_4_2,cs-410,2,4, Implementation of TR Systems,"00:00:06,368","00:00:10,308",2,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=6,This lecture is about the implementation
cs-410_2_4_3,cs-410,2,4, Implementation of TR Systems,"00:00:12,878","00:00:17,327",3,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=12,In this lecture we will discuss
cs-410_2_4_4,cs-410,2,4, Implementation of TR Systems,"00:00:17,327","00:00:20,768",4,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=17,retrieval method to build a search engine.
cs-410_2_4_5,cs-410,2,4, Implementation of TR Systems,"00:00:20,768","00:00:24,753",5,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=20,The main challenge is to
cs-410_2_4_6,cs-410,2,4, Implementation of TR Systems,"00:00:24,753","00:00:30,698",6,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=24,to enable a query to be answered very
cs-410_2_4_7,cs-410,2,4, Implementation of TR Systems,"00:00:30,698","00:00:34,858",7,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=30,This is a typical text
cs-410_2_4_8,cs-410,2,4, Implementation of TR Systems,"00:00:34,858","00:00:39,805",8,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=34,We can see the documents are first
cs-410_2_4_9,cs-410,2,4, Implementation of TR Systems,"00:00:39,805","00:00:43,498",9,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=39,"get tokenized units, for example, words."
cs-410_2_4_10,cs-410,2,4, Implementation of TR Systems,"00:00:43,498","00:00:48,058",10,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=43,"And then, these words, or"
cs-410_2_4_11,cs-410,2,4, Implementation of TR Systems,"00:00:48,058","00:00:53,188",11,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=48,"a indexer that will create a index,"
cs-410_2_4_12,cs-410,2,4, Implementation of TR Systems,"00:00:53,188","00:00:57,280",12,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=53,the search engine to use
cs-410_2_4_13,cs-410,2,4, Implementation of TR Systems,"00:00:57,280","00:01:01,830",13,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=57,And the query would be going
cs-410_2_4_14,cs-410,2,4, Implementation of TR Systems,"00:01:01,830","00:01:05,761",14,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=61,So the Tokenizer would be
cs-410_2_4_15,cs-410,2,4, Implementation of TR Systems,"00:01:05,761","00:01:09,200",15,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=65,so that the text can be
cs-410_2_4_16,cs-410,2,4, Implementation of TR Systems,"00:01:09,200","00:01:12,960",16,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=69,The same units would be
cs-410_2_4_17,cs-410,2,4, Implementation of TR Systems,"00:01:12,960","00:01:17,604",17,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=72,The query's representation would
cs-410_2_4_18,cs-410,2,4, Implementation of TR Systems,"00:01:17,604","00:01:22,506",18,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=77,which would use the index to quickly
cs-410_2_4_19,cs-410,2,4, Implementation of TR Systems,"00:01:22,506","00:01:25,268",19,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=82,the documents and then ranking them.
cs-410_2_4_20,cs-410,2,4, Implementation of TR Systems,"00:01:25,268","00:01:27,628",20,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=85,The results will be given to the user.
cs-410_2_4_21,cs-410,2,4, Implementation of TR Systems,"00:01:27,628","00:01:32,033",21,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=87,And then the user can look at the results
cs-410_2_4_22,cs-410,2,4, Implementation of TR Systems,"00:01:32,033","00:01:35,126",22,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=92,explicit judgements of both
cs-410_2_4_23,cs-410,2,4, Implementation of TR Systems,"00:01:35,126","00:01:36,603",23,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=95,which documents are bad.
cs-410_2_4_24,cs-410,2,4, Implementation of TR Systems,"00:01:36,603","00:01:43,353",24,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=96,Or implicit feedback such as so that
cs-410_2_4_25,cs-410,2,4, Implementation of TR Systems,"00:01:43,353","00:01:46,187",25,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=103,"End user will just look at the results,"
cs-410_2_4_26,cs-410,2,4, Implementation of TR Systems,"00:01:46,187","00:01:49,193",26,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=106,"skip some, and"
cs-410_2_4_27,cs-410,2,4, Implementation of TR Systems,"00:01:49,193","00:01:55,353",27,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=109,So these interacting signals can be used
cs-410_2_4_28,cs-410,2,4, Implementation of TR Systems,"00:01:55,353","00:02:01,718",28,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=115,accuracy by assuming that viewed documents
cs-410_2_4_29,cs-410,2,4, Implementation of TR Systems,"00:02:01,718","00:02:05,678",29,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=121,So a search engine system then
cs-410_2_4_30,cs-410,2,4, Implementation of TR Systems,"00:02:05,678","00:02:10,738",30,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=125,"The first part is the indexer, and"
cs-410_2_4_31,cs-410,2,4, Implementation of TR Systems,"00:02:10,738","00:02:16,458",31,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=130,"responds to the users query, and"
cs-410_2_4_32,cs-410,2,4, Implementation of TR Systems,"00:02:16,458","00:02:21,072",32,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=136,"Now typically, the Indexer is"
cs-410_2_4_33,cs-410,2,4, Implementation of TR Systems,"00:02:21,072","00:02:24,179",33,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=141,you can pre-process the correct data and
cs-410_2_4_34,cs-410,2,4, Implementation of TR Systems,"00:02:24,179","00:02:29,168",34,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=144,"to build the inventory index,"
cs-410_2_4_35,cs-410,2,4, Implementation of TR Systems,"00:02:29,168","00:02:34,819",35,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=149,And this data structure can then be used
cs-410_2_4_36,cs-410,2,4, Implementation of TR Systems,"00:02:34,819","00:02:40,668",36,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=154,to process a user's query dynamically and
cs-410_2_4_37,cs-410,2,4, Implementation of TR Systems,"00:02:40,668","00:02:45,368",37,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=160,The feedback mechanism can be done online
cs-410_2_4_38,cs-410,2,4, Implementation of TR Systems,"00:02:45,368","00:02:50,367",38,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=165,The implementation of the indexer and
cs-410_2_4_39,cs-410,2,4, Implementation of TR Systems,"00:02:50,367","00:02:55,378",39,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=170,and this is the main topic of this
cs-410_2_4_40,cs-410,2,4, Implementation of TR Systems,"00:02:55,378","00:02:59,843",40,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=175,"The feedback mechanism,"
cs-410_2_4_41,cs-410,2,4, Implementation of TR Systems,"00:02:59,843","00:03:02,378",41,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=179,it depends on which method is used.
cs-410_2_4_42,cs-410,2,4, Implementation of TR Systems,"00:03:02,378","00:03:08,818",42,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=182,So that is usually done in
cs-410_2_4_43,cs-410,2,4, Implementation of TR Systems,"00:03:08,818","00:03:11,538",43,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=188,Let's first talk about the tokenizer.
cs-410_2_4_44,cs-410,2,4, Implementation of TR Systems,"00:03:11,538","00:03:16,578",44,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=191,Tokernization is a normalized lexical
cs-410_2_4_45,cs-410,2,4, Implementation of TR Systems,"00:03:16,578","00:03:21,368",45,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=196,so that semantically similar words
cs-410_2_4_46,cs-410,2,4, Implementation of TR Systems,"00:03:21,368","00:03:25,133",46,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=201,"Now, in the language like English,"
cs-410_2_4_47,cs-410,2,4, Implementation of TR Systems,"00:03:25,133","00:03:29,548",47,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=205,this will map all the inflectional
cs-410_2_4_48,cs-410,2,4, Implementation of TR Systems,"00:03:29,548","00:03:33,047",48,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=209,"So for example, computer, computation, and"
cs-410_2_4_49,cs-410,2,4, Implementation of TR Systems,"00:03:33,047","00:03:37,078",49,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=213,computing can all be matched
cs-410_2_4_50,cs-410,2,4, Implementation of TR Systems,"00:03:37,078","00:03:43,628",50,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=217,This way all these different forms of
cs-410_2_4_51,cs-410,2,4, Implementation of TR Systems,"00:03:43,628","00:03:46,433",51,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=223,"Now normally, this is a good idea,"
cs-410_2_4_52,cs-410,2,4, Implementation of TR Systems,"00:03:46,433","00:03:52,337",52,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=226,to increase the coverage of documents
cs-410_2_4_53,cs-410,2,4, Implementation of TR Systems,"00:03:52,337","00:03:55,553",53,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=232,"But it's also not always beneficial,"
cs-410_2_4_54,cs-410,2,4, Implementation of TR Systems,"00:03:55,553","00:04:00,914",54,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=235,because sometimes the subtlest
cs-410_2_4_55,cs-410,2,4, Implementation of TR Systems,"00:04:00,914","00:04:07,558",55,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=240,computation might still suggest the
cs-410_2_4_56,cs-410,2,4, Implementation of TR Systems,"00:04:07,558","00:04:13,398",56,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=247,"But in most cases,"
cs-410_2_4_57,cs-410,2,4, Implementation of TR Systems,"00:04:13,398","00:04:19,363",57,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=253,When we tokenize the text in some other
cs-410_2_4_58,cs-410,2,4, Implementation of TR Systems,"00:04:19,363","00:04:25,338",58,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=259,face some special challenges in segmenting
cs-410_2_4_59,cs-410,2,4, Implementation of TR Systems,"00:04:25,338","00:04:29,697",59,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=265,Because it's not obvious
cs-410_2_4_60,cs-410,2,4, Implementation of TR Systems,"00:04:29,697","00:04:32,928",60,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=269,there's no space to separate them.
cs-410_2_4_61,cs-410,2,4, Implementation of TR Systems,"00:04:32,928","00:04:41,638",61,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=272,"So here of course, we have to use some"
cs-410_2_4_62,cs-410,2,4, Implementation of TR Systems,"00:04:41,638","00:04:47,144",62,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=281,"Once we do tokenization, then we would"
cs-410_2_4_63,cs-410,2,4, Implementation of TR Systems,"00:04:47,144","00:04:52,748",63,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=287,convert the documents and do some data
cs-410_2_4_64,cs-410,2,4, Implementation of TR Systems,"00:04:52,748","00:04:58,298",64,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=292,The basic idea is to precompute
cs-410_2_4_65,cs-410,2,4, Implementation of TR Systems,"00:04:58,298","00:05:02,848",65,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=298,So the most commonly used index
cs-410_2_4_66,cs-410,2,4, Implementation of TR Systems,"00:05:02,848","00:05:06,555",66,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=302,And this has been used
cs-410_2_4_67,cs-410,2,4, Implementation of TR Systems,"00:05:06,555","00:05:09,768",67,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=306,to support basic search algorithms.
cs-410_2_4_68,cs-410,2,4, Implementation of TR Systems,"00:05:09,768","00:05:13,426",68,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=309,"Sometimes the other indices, for example,"
cs-410_2_4_69,cs-410,2,4, Implementation of TR Systems,"00:05:13,426","00:05:19,498",69,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=313,document index might be needed in order
cs-410_2_4_70,cs-410,2,4, Implementation of TR Systems,"00:05:19,498","00:05:24,106",70,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=319,And these kind of techniques
cs-410_2_4_71,cs-410,2,4, Implementation of TR Systems,"00:05:24,106","00:05:28,828",71,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=324,that they vary a lot according
cs-410_2_4_72,cs-410,2,4, Implementation of TR Systems,"00:05:28,828","00:05:34,549",72,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=328,To understand why we want to use
cs-410_2_4_73,cs-410,2,4, Implementation of TR Systems,"00:05:34,549","00:05:40,698",73,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=334,you to think about how you would
cs-410_2_4_74,cs-410,2,4, Implementation of TR Systems,"00:05:40,698","00:05:44,938",74,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=340,So if you want to use more time to
cs-410_2_4_75,cs-410,2,4, Implementation of TR Systems,"00:05:44,938","00:05:49,584",75,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=344,So think about how you can
cs-410_2_4_76,cs-410,2,4, Implementation of TR Systems,"00:05:49,584","00:05:54,768",76,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=349,that you can quickly respond
cs-410_2_4_77,cs-410,2,4, Implementation of TR Systems,"00:05:54,768","00:05:58,466",77,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=354,Where if you have thought
cs-410_2_4_78,cs-410,2,4, Implementation of TR Systems,"00:05:58,466","00:06:02,811",78,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=358,you might realize that where
cs-410_2_4_79,cs-410,2,4, Implementation of TR Systems,"00:06:02,811","00:06:07,718",79,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=362,the list of documents that match
cs-410_2_4_80,cs-410,2,4, Implementation of TR Systems,"00:06:07,718","00:06:11,788",80,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=367,"In this way, you can basically"
cs-410_2_4_81,cs-410,2,4, Implementation of TR Systems,"00:06:11,788","00:06:17,503",81,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=371,So when you see a term you can simply just
cs-410_2_4_82,cs-410,2,4, Implementation of TR Systems,"00:06:17,503","00:06:20,508",82,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=377,that term and return the list to the user.
cs-410_2_4_83,cs-410,2,4, Implementation of TR Systems,"00:06:20,508","00:06:24,928",83,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=380,So that's the fastest way to
cs-410_2_4_84,cs-410,2,4, Implementation of TR Systems,"00:06:24,928","00:06:30,468",84,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=384,Now the idea of the invert index
cs-410_2_4_85,cs-410,2,4, Implementation of TR Systems,"00:06:30,468","00:06:36,017",85,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=390,We're going to do pre-constructed
cs-410_2_4_86,cs-410,2,4, Implementation of TR Systems,"00:06:36,017","00:06:41,388",86,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=396,us to quickly find all the documents
cs-410_2_4_87,cs-410,2,4, Implementation of TR Systems,"00:06:41,388","00:06:43,878",87,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=401,So let's take a look at this example.
cs-410_2_4_88,cs-410,2,4, Implementation of TR Systems,"00:06:43,878","00:06:45,439",88,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=403,"We have three documents here,"
cs-410_2_4_89,cs-410,2,4, Implementation of TR Systems,"00:06:45,439","00:06:49,168",89,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=405,and these are the documents that you
cs-410_2_4_90,cs-410,2,4, Implementation of TR Systems,"00:06:49,168","00:06:52,916",90,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=409,Suppose that we want to create
cs-410_2_4_91,cs-410,2,4, Implementation of TR Systems,"00:06:52,916","00:06:57,502",91,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=412,"Then we want to maintain a dictionary, in"
cs-410_2_4_92,cs-410,2,4, Implementation of TR Systems,"00:06:57,502","00:07:01,628",92,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=417,each term and we're going to store
cs-410_2_4_93,cs-410,2,4, Implementation of TR Systems,"00:07:01,628","00:07:05,960",93,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=421,"For example, the number of"
cs-410_2_4_94,cs-410,2,4, Implementation of TR Systems,"00:07:05,960","00:07:09,458",94,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=425,the total number of code or
cs-410_2_4_95,cs-410,2,4, Implementation of TR Systems,"00:07:09,458","00:07:14,148",95,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=429,which means we would kind of duplicate
cs-410_2_4_96,cs-410,2,4, Implementation of TR Systems,"00:07:14,148","00:07:17,415",96,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=434,"And so, for example, news,"
cs-410_2_4_97,cs-410,2,4, Implementation of TR Systems,"00:07:17,415","00:07:22,253",97,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=437,this term occur in all
cs-410_2_4_98,cs-410,2,4, Implementation of TR Systems,"00:07:22,253","00:07:26,198",98,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=442,so the count of documents is three.
cs-410_2_4_99,cs-410,2,4, Implementation of TR Systems,"00:07:26,198","00:07:32,820",99,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=446,And you might also realize we needed this
cs-410_2_4_100,cs-410,2,4, Implementation of TR Systems,"00:07:32,820","00:07:38,002",100,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=452,for computing some statistics to
cs-410_2_4_101,cs-410,2,4, Implementation of TR Systems,"00:07:38,002","00:07:42,422",101,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=458,Can you think of that?
cs-410_2_4_102,cs-410,2,4, Implementation of TR Systems,"00:07:42,422","00:07:49,862",102,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=462,So what weighting heuristic
cs-410_2_4_103,cs-410,2,4, Implementation of TR Systems,"00:07:49,862","00:07:53,622",103,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=469,"Well, that's the idea, right,"
cs-410_2_4_104,cs-410,2,4, Implementation of TR Systems,"00:07:53,622","00:07:58,300",104,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=473,"So, IDF is the property of a term,"
cs-410_2_4_105,cs-410,2,4, Implementation of TR Systems,"00:07:58,300","00:08:03,291",105,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=478,"So, with the document that count here,"
cs-410_2_4_106,cs-410,2,4, Implementation of TR Systems,"00:08:03,291","00:08:06,556",106,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=483,"either at this time, or"
cs-410_2_4_107,cs-410,2,4, Implementation of TR Systems,"00:08:06,556","00:08:10,134",107,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=486,At random time when we see a query.
cs-410_2_4_108,cs-410,2,4, Implementation of TR Systems,"00:08:10,134","00:08:13,641",108,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=490,"Now in addition to these basic statistics,"
cs-410_2_4_109,cs-410,2,4, Implementation of TR Systems,"00:08:13,641","00:08:18,380",109,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=493,we'll also store all the documents
cs-410_2_4_110,cs-410,2,4, Implementation of TR Systems,"00:08:18,380","00:08:23,049",110,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=498,and these entries are stored
cs-410_2_4_111,cs-410,2,4, Implementation of TR Systems,"00:08:24,150","00:08:27,595",111,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=504,So in this case it matched
cs-410_2_4_112,cs-410,2,4, Implementation of TR Systems,"00:08:27,595","00:08:31,680",112,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=507,we store information about
cs-410_2_4_113,cs-410,2,4, Implementation of TR Systems,"00:08:31,680","00:08:38,160",113,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=511,"This is the document id,"
cs-410_2_4_114,cs-410,2,4, Implementation of TR Systems,"00:08:38,160","00:08:45,240",114,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=518,"The tf is one for news, in the second"
cs-410_2_4_115,cs-410,2,4, Implementation of TR Systems,"00:08:45,240","00:08:50,864",115,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=525,"So from this list, we can get all"
cs-410_2_4_116,cs-410,2,4, Implementation of TR Systems,"00:08:50,864","00:08:55,320",116,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=530,we can also know the frequency
cs-410_2_4_117,cs-410,2,4, Implementation of TR Systems,"00:08:55,320","00:08:58,214",117,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=535,"So, if the query has just one word,"
cs-410_2_4_118,cs-410,2,4, Implementation of TR Systems,"00:08:58,214","00:09:01,628",118,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=538,we have easily look up to this
cs-410_2_4_119,cs-410,2,4, Implementation of TR Systems,"00:09:01,628","00:09:06,780",119,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=541,go quicker into the postings to fetch
cs-410_2_4_120,cs-410,2,4, Implementation of TR Systems,"00:09:06,780","00:09:08,180",120,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=546,"So, let's take a look at another term."
cs-410_2_4_121,cs-410,2,4, Implementation of TR Systems,"00:09:09,280","00:09:12,600",121,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=549,"This time, let's take a look"
cs-410_2_4_122,cs-410,2,4, Implementation of TR Systems,"00:09:14,130","00:09:17,950",122,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=554,"This would occur in only one document,"
cs-410_2_4_123,cs-410,2,4, Implementation of TR Systems,"00:09:17,950","00:09:23,490",123,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=557,So the document frequency is 1 but
cs-410_2_4_124,cs-410,2,4, Implementation of TR Systems,"00:09:23,490","00:09:29,210",124,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=563,"So the frequency count is two, and"
cs-410_2_4_125,cs-410,2,4, Implementation of TR Systems,"00:09:29,210","00:09:33,250",125,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=569,some other reachable method where
cs-410_2_4_126,cs-410,2,4, Implementation of TR Systems,"00:09:34,490","00:09:38,770",126,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=574,assess the popularity of
cs-410_2_4_127,cs-410,2,4, Implementation of TR Systems,"00:09:38,770","00:09:42,930",127,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=578,Similarly we'll have a pointer
cs-410_2_4_128,cs-410,2,4, Implementation of TR Systems,"00:09:42,930","00:09:47,490",128,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=582,"and in this case,"
cs-410_2_4_129,cs-410,2,4, Implementation of TR Systems,"00:09:48,900","00:09:53,570",129,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=588,the term occurred in just one document and
cs-410_2_4_130,cs-410,2,4, Implementation of TR Systems,"00:09:53,570","00:09:57,320",130,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=593,The document id is 3 and
cs-410_2_4_131,cs-410,2,4, Implementation of TR Systems,"00:09:59,610","00:10:02,550",131,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=599,So this is the basic
cs-410_2_4_132,cs-410,2,4, Implementation of TR Systems,"00:10:02,550","00:10:04,340",132,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=602,"It's actually pretty simple, right?"
cs-410_2_4_133,cs-410,2,4, Implementation of TR Systems,"00:10:06,580","00:10:12,370",133,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=606,With this structure we can easily fetch
cs-410_2_4_134,cs-410,2,4, Implementation of TR Systems,"00:10:12,370","00:10:15,760",134,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=612,And this will be the basis for
cs-410_2_4_135,cs-410,2,4, Implementation of TR Systems,"00:10:15,760","00:10:23,770",135,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=615,Now sometimes we also want to store
cs-410_2_4_136,cs-410,2,4, Implementation of TR Systems,"00:10:25,220","00:10:31,960",136,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=625,So in many of these cases the term
cs-410_2_4_137,cs-410,2,4, Implementation of TR Systems,"00:10:31,960","00:10:34,320",137,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=631,So there's only one position for
cs-410_2_4_138,cs-410,2,4, Implementation of TR Systems,"00:10:35,810","00:10:40,990",138,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=635,"But in this case, the term occurred"
cs-410_2_4_139,cs-410,2,4, Implementation of TR Systems,"00:10:40,990","00:10:44,690",139,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=640,Now the position information is very
cs-410_2_4_140,cs-410,2,4, Implementation of TR Systems,"00:10:44,690","00:10:48,400",140,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=644,the matching of query terms is
cs-410_2_4_141,cs-410,2,4, Implementation of TR Systems,"00:10:48,400","00:10:51,360",141,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=648,"let's say, five words or ten words."
cs-410_2_4_142,cs-410,2,4, Implementation of TR Systems,"00:10:52,410","00:11:00,700",142,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=652,"Or, whether the matching of the two query"
cs-410_2_4_143,cs-410,2,4, Implementation of TR Systems,"00:11:00,700","00:11:04,540",143,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=660,That this can all be checked quickly
cs-410_2_4_144,cs-410,2,4, Implementation of TR Systems,"00:11:05,920","00:11:10,160",144,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=665,"So, why is inverted index good for"
cs-410_2_4_145,cs-410,2,4, Implementation of TR Systems,"00:11:10,160","00:11:16,349",145,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=670,"Well, we just talked about the possibility"
cs-410_2_4_146,cs-410,2,4, Implementation of TR Systems,"00:11:16,349","00:11:17,990",146,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=676,And that's very easy.
cs-410_2_4_147,cs-410,2,4, Implementation of TR Systems,"00:11:17,990","00:11:19,910",147,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=677,What about the multiple term queries?
cs-410_2_4_148,cs-410,2,4, Implementation of TR Systems,"00:11:19,910","00:11:23,800",148,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=679,Well let's first look at the some
cs-410_2_4_149,cs-410,2,4, Implementation of TR Systems,"00:11:23,800","00:11:27,740",149,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=683,A Boolean query is basically
cs-410_2_4_150,cs-410,2,4, Implementation of TR Systems,"00:11:27,740","00:11:36,290",150,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=687,So I want the value in the document
cs-410_2_4_151,cs-410,2,4, Implementation of TR Systems,"00:11:36,290","00:11:38,770",151,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=696,So that's one conjunctive query.
cs-410_2_4_152,cs-410,2,4, Implementation of TR Systems,"00:11:38,770","00:11:45,440",152,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=698,Or I want the web documents
cs-410_2_4_153,cs-410,2,4, Implementation of TR Systems,"00:11:45,440","00:11:46,540",153,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=705,That's a disjunctive query.
cs-410_2_4_154,cs-410,2,4, Implementation of TR Systems,"00:11:46,540","00:11:51,070",154,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=706,But how can we answer such
cs-410_2_4_155,cs-410,2,4, Implementation of TR Systems,"00:11:52,090","00:11:53,860",155,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=712,"Well if you think a bit about it,"
cs-410_2_4_156,cs-410,2,4, Implementation of TR Systems,"00:11:53,860","00:11:58,130",156,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=713,it would be obvious because
cs-410_2_4_157,cs-410,2,4, Implementation of TR Systems,"00:11:58,130","00:12:03,170",157,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=718,the documents that match term A and also
cs-410_2_4_158,cs-410,2,4, Implementation of TR Systems,"00:12:03,170","00:12:08,160",158,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=723,And then just take the intersection
cs-410_2_4_159,cs-410,2,4, Implementation of TR Systems,"00:12:08,160","00:12:13,050",159,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=728,Or to take the union to
cs-410_2_4_160,cs-410,2,4, Implementation of TR Systems,"00:12:13,050","00:12:16,020",160,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=733,So this is all very easy to answer.
cs-410_2_4_161,cs-410,2,4, Implementation of TR Systems,"00:12:16,020","00:12:17,780",161,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=736,It's going to be very quick.
cs-410_2_4_162,cs-410,2,4, Implementation of TR Systems,"00:12:17,780","00:12:20,850",162,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=737,Now what about the multi-term
cs-410_2_4_163,cs-410,2,4, Implementation of TR Systems,"00:12:20,850","00:12:24,390",163,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=740,We talked about the vector space model for
cs-410_2_4_164,cs-410,2,4, Implementation of TR Systems,"00:12:24,390","00:12:28,940",164,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=744,we will do a match such query with
cs-410_2_4_165,cs-410,2,4, Implementation of TR Systems,"00:12:28,940","00:12:32,330",165,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=748,And the score is based on
cs-410_2_4_166,cs-410,2,4, Implementation of TR Systems,"00:12:32,330","00:12:35,670",166,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=752,So in this case it's not
cs-410_2_4_167,cs-410,2,4, Implementation of TR Systems,"00:12:35,670","00:12:38,770",167,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=755,the scoring can be actually
cs-410_2_4_168,cs-410,2,4, Implementation of TR Systems,"00:12:38,770","00:12:42,430",168,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=758,Basically it's similar to
cs-410_2_4_169,cs-410,2,4, Implementation of TR Systems,"00:12:42,430","00:12:45,140",169,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=762,"Basically, it's like A or B."
cs-410_2_4_170,cs-410,2,4, Implementation of TR Systems,"00:12:45,140","00:12:50,680",170,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=765,We take the union of all the documents
cs-410_2_4_171,cs-410,2,4, Implementation of TR Systems,"00:12:50,680","00:12:53,320",171,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=770,then we would aggregate the term weights.
cs-410_2_4_172,cs-410,2,4, Implementation of TR Systems,"00:12:53,320","00:13:01,420",172,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=773,So this is a basic idea of using inverted
cs-410_2_4_173,cs-410,2,4, Implementation of TR Systems,"00:13:01,420","00:13:05,210",173,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=781,And we're going to talk about
cs-410_2_4_174,cs-410,2,4, Implementation of TR Systems,"00:13:05,210","00:13:06,000",174,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=785,"But for now,"
cs-410_2_4_175,cs-410,2,4, Implementation of TR Systems,"00:13:06,000","00:13:12,210",175,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=786,let's just look at the question
cs-410_2_4_176,cs-410,2,4, Implementation of TR Systems,"00:13:12,210","00:13:17,470",176,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=792,Basically why is more efficient than
cs-410_2_4_177,cs-410,2,4, Implementation of TR Systems,"00:13:17,470","00:13:20,770",177,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=797,This is the obvious approach.
cs-410_2_4_178,cs-410,2,4, Implementation of TR Systems,"00:13:20,770","00:13:27,518",178,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=800,You can just compute a score for each
cs-410_2_4_179,cs-410,2,4, Implementation of TR Systems,"00:13:27,518","00:13:29,936",179,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=807,And this is a straightforward method but
cs-410_2_4_180,cs-410,2,4, Implementation of TR Systems,"00:13:29,936","00:13:34,496",180,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=809,this is going to be very slow imagine
cs-410_2_4_181,cs-410,2,4, Implementation of TR Systems,"00:13:34,496","00:13:39,620",181,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=814,If you do this then it will take
cs-410_2_4_182,cs-410,2,4, Implementation of TR Systems,"00:13:39,620","00:13:44,975",182,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=819,So the question now is why would
cs-410_2_4_183,cs-410,2,4, Implementation of TR Systems,"00:13:44,975","00:13:48,780",183,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=824,Well it has to do is the word
cs-410_2_4_184,cs-410,2,4, Implementation of TR Systems,"00:13:48,780","00:13:54,010",184,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=828,"So, here's some common phenomena"
cs-410_2_4_185,cs-410,2,4, Implementation of TR Systems,"00:13:54,010","00:13:58,720",185,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=834,There are some languages independent
cs-410_2_4_186,cs-410,2,4, Implementation of TR Systems,"00:14:00,300","00:14:07,690",186,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=840,And these patterns are basically
cs-410_2_4_187,cs-410,2,4, Implementation of TR Systems,"00:14:07,690","00:14:10,830",187,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=847,A few words like the common
cs-410_2_4_188,cs-410,2,4, Implementation of TR Systems,"00:14:10,830","00:14:14,780",188,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=850,"we occur very, very frequently in text."
cs-410_2_4_189,cs-410,2,4, Implementation of TR Systems,"00:14:14,780","00:14:18,210",189,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=854,So they account for
cs-410_2_4_190,cs-410,2,4, Implementation of TR Systems,"00:14:19,405","00:14:22,885",190,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=859,But most words would occur just rarely.
cs-410_2_4_191,cs-410,2,4, Implementation of TR Systems,"00:14:22,885","00:14:25,615",191,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=862,"There are many words that occur just once,"
cs-410_2_4_192,cs-410,2,4, Implementation of TR Systems,"00:14:25,615","00:14:29,790",192,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=865,"let's say, in a document or"
cs-410_2_4_193,cs-410,2,4, Implementation of TR Systems,"00:14:29,790","00:14:33,306",193,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=869,And there are many such.
cs-410_2_4_194,cs-410,2,4, Implementation of TR Systems,"00:14:33,306","00:14:37,977",194,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=873,It's also true that the most
cs-410_2_4_195,cs-410,2,4, Implementation of TR Systems,"00:14:37,977","00:14:40,462",195,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=877,they have to be rare in another.
cs-410_2_4_196,cs-410,2,4, Implementation of TR Systems,"00:14:40,462","00:14:45,800",196,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=880,That means although the general
cs-410_2_4_197,cs-410,2,4, Implementation of TR Systems,"00:14:45,800","00:14:51,060",197,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=885,was observed in many cases that
cs-410_2_4_198,cs-410,2,4, Implementation of TR Systems,"00:14:51,060","00:14:54,770",198,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=891,may vary from context to context.
cs-410_2_4_199,cs-410,2,4, Implementation of TR Systems,"00:14:54,770","00:14:59,450",199,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=894,So this phenomena is characterized
cs-410_2_4_200,cs-410,2,4, Implementation of TR Systems,"00:14:59,450","00:15:02,210",200,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=899,This law says that the rank of a word
cs-410_2_4_201,cs-410,2,4, Implementation of TR Systems,"00:15:02,210","00:15:06,370",201,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=902,multiplied by the frequency of
cs-410_2_4_202,cs-410,2,4, Implementation of TR Systems,"00:15:07,450","00:15:13,045",202,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=907,So formally if we use F(w)
cs-410_2_4_203,cs-410,2,4, Implementation of TR Systems,"00:15:13,045","00:15:16,310",203,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=913,r(w) to denote the rank of a word.
cs-410_2_4_204,cs-410,2,4, Implementation of TR Systems,"00:15:16,310","00:15:17,390",204,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=916,Then this is the formula.
cs-410_2_4_205,cs-410,2,4, Implementation of TR Systems,"00:15:17,390","00:15:21,300",205,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=917,"It basically says the same thing,"
cs-410_2_4_206,cs-410,2,4, Implementation of TR Systems,"00:15:21,300","00:15:28,510",206,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=921,Where C is basically a constant and
cs-410_2_4_207,cs-410,2,4, Implementation of TR Systems,"00:15:28,510","00:15:34,180",207,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=928,"alpha, that might be adjusted to"
cs-410_2_4_208,cs-410,2,4, Implementation of TR Systems,"00:15:34,180","00:15:38,128",208,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=934,So if I plot the word
cs-410_2_4_209,cs-410,2,4, Implementation of TR Systems,"00:15:38,128","00:15:40,980",209,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=938,then you can see this more easily.
cs-410_2_4_210,cs-410,2,4, Implementation of TR Systems,"00:15:40,980","00:15:43,660",210,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=940,The x axis is basically the word rank.
cs-410_2_4_211,cs-410,2,4, Implementation of TR Systems,"00:15:43,660","00:15:50,393",211,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=943,This is r(w) and
cs-410_2_4_212,cs-410,2,4, Implementation of TR Systems,"00:15:50,393","00:15:57,448",212,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=950,Now this curve shows that the product
cs-410_2_4_213,cs-410,2,4, Implementation of TR Systems,"00:15:57,448","00:16:02,524",213,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=957,"Now if you look at these words, we can see"
cs-410_2_4_214,cs-410,2,4, Implementation of TR Systems,"00:16:02,524","00:16:06,870",214,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=962,"In the middle,"
cs-410_2_4_215,cs-410,2,4, Implementation of TR Systems,"00:16:06,870","00:16:11,440",215,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=966,These words tend to occur
cs-410_2_4_216,cs-410,2,4, Implementation of TR Systems,"00:16:11,440","00:16:14,890",216,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=971,they are not like those
cs-410_2_4_217,cs-410,2,4, Implementation of TR Systems,"00:16:14,890","00:16:17,070",217,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=974,And they are also not very rare.
cs-410_2_4_218,cs-410,2,4, Implementation of TR Systems,"00:16:18,190","00:16:21,620",218,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=978,So they tend to be often used in
cs-410_2_4_219,cs-410,2,4, Implementation of TR Systems,"00:16:22,700","00:16:28,240",219,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=982,queries and they also tend
cs-410_2_4_220,cs-410,2,4, Implementation of TR Systems,"00:16:28,240","00:16:31,290",220,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=988,These intermediate frequency words.
cs-410_2_4_221,cs-410,2,4, Implementation of TR Systems,"00:16:31,290","00:16:34,480",221,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=991,But if you look at the left
cs-410_2_4_222,cs-410,2,4, Implementation of TR Systems,"00:16:35,820","00:16:38,330",222,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=995,these are the highest frequency words.
cs-410_2_4_223,cs-410,2,4, Implementation of TR Systems,"00:16:38,330","00:16:39,620",223,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=998,They are covered very frequently.
cs-410_2_4_224,cs-410,2,4, Implementation of TR Systems,"00:16:39,620","00:16:45,540",224,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=999,"They are usually words,"
cs-410_2_4_225,cs-410,2,4, Implementation of TR Systems,"00:16:45,540","00:16:49,440",225,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1005,"Those words are very, very frequent and"
cs-410_2_4_226,cs-410,2,4, Implementation of TR Systems,"00:16:49,440","00:16:54,226",226,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1009,"discriminated, and they are generally"
cs-410_2_4_227,cs-410,2,4, Implementation of TR Systems,"00:16:54,226","00:17:01,900",227,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1014,So they are often removed and
cs-410_2_4_228,cs-410,2,4, Implementation of TR Systems,"00:17:01,900","00:17:06,960",228,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1021,So you can use pretty much just the kind
cs-410_2_4_229,cs-410,2,4, Implementation of TR Systems,"00:17:06,960","00:17:09,620",229,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1026,infer what words might be stop words.
cs-410_2_4_230,cs-410,2,4, Implementation of TR Systems,"00:17:09,620","00:17:12,690",230,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1029,Those are basically
cs-410_2_4_231,cs-410,2,4, Implementation of TR Systems,"00:17:13,780","00:17:18,500",231,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1033,And they also occupy a lot of
cs-410_2_4_232,cs-410,2,4, Implementation of TR Systems,"00:17:18,500","00:17:23,048",232,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1038,You can imagine the posting entries for
cs-410_2_4_233,cs-410,2,4, Implementation of TR Systems,"00:17:23,048","00:17:24,370",233,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1043,"And then therefore,"
cs-410_2_4_234,cs-410,2,4, Implementation of TR Systems,"00:17:24,370","00:17:28,299",234,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1044,if you can remove such words you can save
cs-410_2_4_235,cs-410,2,4, Implementation of TR Systems,"00:17:29,890","00:17:35,100",235,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1049,"We also show the tail part,"
cs-410_2_4_236,cs-410,2,4, Implementation of TR Systems,"00:17:35,100","00:17:38,470",236,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1055,"Those words don't occur very frequently,"
cs-410_2_4_237,cs-410,2,4, Implementation of TR Systems,"00:17:39,680","00:17:41,330",237,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1059,Those words are actually very useful for
cs-410_2_4_238,cs-410,2,4, Implementation of TR Systems,"00:17:41,330","00:17:45,630",238,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1061,"search also, if a user happens to"
cs-410_2_4_239,cs-410,2,4, Implementation of TR Systems,"00:17:45,630","00:17:49,730",239,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1065,"But because they're rare,"
cs-410_2_4_240,cs-410,2,4, Implementation of TR Systems,"00:17:49,730","00:17:54,030",240,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1069,aren't necessarily
cs-410_2_4_241,cs-410,2,4, Implementation of TR Systems,"00:17:54,030","00:17:58,970",241,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1074,But retain them would allow us to
cs-410_2_4_242,cs-410,2,4, Implementation of TR Systems,"00:17:58,970","00:18:00,610",242,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1078,They generally have very high IDF.
cs-410_2_4_243,cs-410,2,4, Implementation of TR Systems,"00:18:05,559","00:18:10,840",243,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1085,So what kind of data structures should
cs-410_2_4_244,cs-410,2,4, Implementation of TR Systems,"00:18:10,840","00:18:11,970",244,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1090,"Well, it has two parts, right."
cs-410_2_4_245,cs-410,2,4, Implementation of TR Systems,"00:18:11,970","00:18:16,720",245,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1091,"If you recall, we have a dictionary and"
cs-410_2_4_246,cs-410,2,4, Implementation of TR Systems,"00:18:16,720","00:18:21,810",246,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1096,"The dictionary has modest size, although"
cs-410_2_4_247,cs-410,2,4, Implementation of TR Systems,"00:18:21,810","00:18:24,810",247,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1101,large but compare it with
cs-410_2_4_248,cs-410,2,4, Implementation of TR Systems,"00:18:26,220","00:18:29,710",248,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1106,And we also need to have fast
cs-410_2_4_249,cs-410,2,4, Implementation of TR Systems,"00:18:29,710","00:18:32,940",249,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1109,because we're going to look up
cs-410_2_4_250,cs-410,2,4, Implementation of TR Systems,"00:18:32,940","00:18:39,200",250,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1112,"So therefore, we'd prefer to keep such"
cs-410_2_4_251,cs-410,2,4, Implementation of TR Systems,"00:18:39,200","00:18:43,333",251,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1119,"If the collection is not very large,"
cs-410_2_4_252,cs-410,2,4, Implementation of TR Systems,"00:18:43,333","00:18:47,810",252,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1123,if the collection is very large
cs-410_2_4_253,cs-410,2,4, Implementation of TR Systems,"00:18:47,810","00:18:52,100",253,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1127,"If the vocabulary size is very large,"
cs-410_2_4_254,cs-410,2,4, Implementation of TR Systems,"00:18:52,100","00:18:55,800",254,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1132,"So, in general that's how it goes."
cs-410_2_4_255,cs-410,2,4, Implementation of TR Systems,"00:18:55,800","00:18:58,578",255,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1135,So the data structures
cs-410_2_4_256,cs-410,2,4, Implementation of TR Systems,"00:18:58,578","00:19:01,390",256,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1138,"storing dictionary,"
cs-410_2_4_257,cs-410,2,4, Implementation of TR Systems,"00:19:01,390","00:19:04,375",257,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1141,"There are structures like hash table, or"
cs-410_2_4_258,cs-410,2,4, Implementation of TR Systems,"00:19:04,375","00:19:09,090",258,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1144,b-tree if we can't store
cs-410_2_4_259,cs-410,2,4, Implementation of TR Systems,"00:19:09,090","00:19:12,760",259,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1149,And then try to build a structure that
cs-410_2_4_260,cs-410,2,4, Implementation of TR Systems,"00:19:14,530","00:19:16,705",260,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1154,For postings they are huge.
cs-410_2_4_261,cs-410,2,4, Implementation of TR Systems,"00:19:18,045","00:19:24,815",261,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1158,"And in general, we don't have to have"
cs-410_2_4_262,cs-410,2,4, Implementation of TR Systems,"00:19:24,815","00:19:29,145",262,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1164,We generally would just look up
cs-410_2_4_263,cs-410,2,4, Implementation of TR Systems,"00:19:29,145","00:19:32,850",263,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1169,frequencies for all the documents
cs-410_2_4_264,cs-410,2,4, Implementation of TR Systems,"00:19:33,930","00:19:36,570",264,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1173,So would read those entries sequentially.
cs-410_2_4_265,cs-410,2,4, Implementation of TR Systems,"00:19:37,670","00:19:43,704",265,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1177,And therefore because it's large and
cs-410_2_4_266,cs-410,2,4, Implementation of TR Systems,"00:19:43,704","00:19:49,826",266,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1183,they have to stay on disc and they would
cs-410_2_4_267,cs-410,2,4, Implementation of TR Systems,"00:19:49,826","00:19:53,392",267,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1189,term frequency or
cs-410_2_4_268,cs-410,2,4, Implementation of TR Systems,"00:19:53,392","00:19:58,300",268,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1193,"Now because they are very large,"
cs-410_2_4_269,cs-410,2,4, Implementation of TR Systems,"00:19:59,360","00:20:04,390",269,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1199,"Now this is not only to save disc space,"
cs-410_2_4_270,cs-410,2,4, Implementation of TR Systems,"00:20:04,390","00:20:09,080",270,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1204,"one benefit of compression, it It's"
cs-410_2_4_271,cs-410,2,4, Implementation of TR Systems,"00:20:09,080","00:20:11,750",271,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1209,But it's also to help improving speed.
cs-410_2_4_272,cs-410,2,4, Implementation of TR Systems,"00:20:13,110","00:20:15,980",272,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1213,Can you see why?
cs-410_2_4_273,cs-410,2,4, Implementation of TR Systems,"00:20:15,980","00:20:23,470",273,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1215,"Well, we know that input and"
cs-410_2_4_274,cs-410,2,4, Implementation of TR Systems,"00:20:23,470","00:20:28,320",274,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1223,In comparison with the time taken by CPU.
cs-410_2_4_275,cs-410,2,4, Implementation of TR Systems,"00:20:28,320","00:20:33,410",275,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1228,"So, CPU is much faster but"
cs-410_2_4_276,cs-410,2,4, Implementation of TR Systems,"00:20:33,410","00:20:39,335",276,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1233,"so by compressing the inverter index,"
cs-410_2_4_277,cs-410,2,4, Implementation of TR Systems,"00:20:39,335","00:20:45,115",277,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1239,"the entries, that we have the readings,"
cs-410_2_4_278,cs-410,2,4, Implementation of TR Systems,"00:20:45,115","00:20:50,150",278,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1245,"would be smaller, and"
cs-410_2_4_279,cs-410,2,4, Implementation of TR Systems,"00:20:50,150","00:20:55,080",279,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1250,the amount of tracking IO and
cs-410_2_4_280,cs-410,2,4, Implementation of TR Systems,"00:20:55,080","00:21:00,270",280,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1255,"Of course, we have to then do more"
cs-410_2_4_281,cs-410,2,4, Implementation of TR Systems,"00:21:00,270","00:21:03,630",281,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1260,uncompress the data in the memory.
cs-410_2_4_282,cs-410,2,4, Implementation of TR Systems,"00:21:03,630","00:21:05,550",282,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1263,But as I said CPU is fast.
cs-410_2_4_283,cs-410,2,4, Implementation of TR Systems,"00:21:05,550","00:21:07,019",283,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1265,So over all we can still save time.
cs-410_2_4_284,cs-410,2,4, Implementation of TR Systems,"00:21:08,360","00:21:11,301",284,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1268,So compression here is both
cs-410_2_4_285,cs-410,2,4, Implementation of TR Systems,"00:21:11,301","00:21:14,035",285,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1271,to speed up the loading of the index.
cs-410_2_4_286,cs-410,2,4, Implementation of TR Systems,"00:21:14,035","00:21:24,035",286,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1274,[MUSIC]
cs-410_2_5_1,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:00,012","00:00:03,586",1,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=0,[SOUND]
cs-410_2_5_2,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:07,325","00:00:10,038",2,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=7,This lecture is about the inverted index
cs-410_2_5_3,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:10,038","00:00:11,160",3,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=10,construction.
cs-410_2_5_4,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:13,840","00:00:18,520",4,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=13,"In this lecture, we will continue"
cs-410_2_5_5,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:18,520","00:00:22,019",5,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=18,"In particular, we're going to discuss"
cs-410_2_5_6,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:25,096","00:00:29,259",6,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=25,The construction of the inverted index
cs-410_2_5_7,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:29,259","00:00:30,450",7,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=29,very small.
cs-410_2_5_8,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:30,450","00:00:35,430",8,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=30,It's very easy to construct a dictionary
cs-410_2_5_9,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:36,600","00:00:42,280",9,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=36,The problem is that when our data
cs-410_2_5_10,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:42,280","00:00:45,180",10,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=42,then we have to use some
cs-410_2_5_11,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:46,500","00:00:51,900",11,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=46,"And unfortunately, in most retrieval"
cs-410_2_5_12,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:51,900","00:00:55,700",12,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=51,And they generally cannot be
cs-410_2_5_13,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:56,790","00:01:01,843",13,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=56,And there are many approaches to
cs-410_2_5_14,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:01,843","00:01:06,710",14,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=61,method is quite common and
cs-410_2_5_15,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:06,710","00:01:11,480",15,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=66,"First, you collect the local termID,"
cs-410_2_5_16,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:11,480","00:01:16,946",16,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=71,Basically you will locate the terms
cs-410_2_5_17,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:16,946","00:01:24,117",17,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=76,And then once you collect those accounts
cs-410_2_5_18,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:24,117","00:01:29,104",18,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=84,So that you will be able to local
cs-410_2_5_19,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:29,104","00:01:31,310",19,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=89,these are called rounds.
cs-410_2_5_20,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:31,310","00:01:36,930",20,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=91,And then you write them into
cs-410_2_5_21,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:36,930","00:01:38,940",21,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=96,then you merge in step 3.
cs-410_2_5_22,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:38,940","00:01:44,104",22,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=98,"Do pairwise merging of these runs, until"
cs-410_2_5_23,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:44,104","00:01:46,460",23,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=104,generate a single inverted index.
cs-410_2_5_24,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:47,700","00:01:50,823",24,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=107,So this is an illustration of this method.
cs-410_2_5_25,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:50,823","00:01:54,265",25,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=110,On the left you see some documents and
cs-410_2_5_26,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:54,265","00:01:59,942",26,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=114,on the right we have a term lexicon and
cs-410_2_5_27,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:59,942","00:02:08,070",27,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=119,These lexicons are to map string-based
cs-410_2_5_28,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:08,070","00:02:12,261",28,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=128,terms into integer representations or
cs-410_2_5_29,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:12,261","00:02:18,112",29,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=132,map back from integers to
cs-410_2_5_30,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:18,112","00:02:23,010",30,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=138,The reason why we want our interest
cs-410_2_5_31,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:23,010","00:02:26,930",31,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=143,IDs is because integers
cs-410_2_5_32,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:26,930","00:02:29,770",32,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=146,"For example,"
cs-410_2_5_33,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:29,770","00:02:33,240",33,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=149,"array, and they are also easy to compress."
cs-410_2_5_34,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:34,390","00:02:40,530",34,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=154,So this is one reason why we tend
cs-410_2_5_35,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:42,180","00:02:46,710",35,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=162,so that we don't have to
cs-410_2_5_36,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:46,710","00:02:48,070",36,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=166,So how does this approach work?
cs-410_2_5_37,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:48,070","00:02:49,822",37,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=168,"Well, it's very simple."
cs-410_2_5_38,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:49,822","00:02:53,260",38,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=169,We're going to scan these
cs-410_2_5_39,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:53,260","00:02:58,260",39,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=173,then parse the documents and
cs-410_2_5_40,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:58,260","00:03:03,306",40,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=178,And in this stage we generally sort
cs-410_2_5_41,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:03,306","00:03:06,961",41,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=183,because we process each
cs-410_2_5_42,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:06,961","00:03:11,310",42,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=186,So we'll first encounter all
cs-410_2_5_43,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:11,310","00:03:18,786",43,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=191,Therefore the document IDs
cs-410_2_5_44,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:18,786","00:03:25,503",44,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=198,And this will be followed by document IDs
cs-410_2_5_45,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:25,503","00:03:31,280",45,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=205,only just because we process
cs-410_2_5_46,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:31,280","00:03:34,890",46,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=211,"At some point,"
cs-410_2_5_47,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:34,890","00:03:39,080",47,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=214,that would have to write
cs-410_2_5_48,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:39,080","00:03:45,830",48,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=219,Before we do that we 're going to sort
cs-410_2_5_49,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:45,830","00:03:51,948",49,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=225,We can sort them and then this time
cs-410_2_5_50,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:51,948","00:03:59,459",50,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=231,"Note that here,"
cs-410_2_5_51,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:59,459","00:04:03,827",51,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=239,So all the entries that share the same
cs-410_2_5_52,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:03,827","00:04:08,557",52,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=243,"In this case,"
cs-410_2_5_53,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:08,557","00:04:14,090",53,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=248,that match term 1 would
cs-410_2_5_54,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:14,090","00:04:18,850",54,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=254,And we're going to write this into
cs-410_2_5_55,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:18,850","00:04:22,800",55,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=258,And would that allows you to
cs-410_2_5_56,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:22,800","00:04:24,030",56,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=262,makes a batch of documents.
cs-410_2_5_57,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:24,030","00:04:26,670",57,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=264,And we're going to do that for
cs-410_2_5_58,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:26,670","00:04:32,546",58,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=266,So we're going to write a lot of
cs-410_2_5_59,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:32,546","00:04:35,400",59,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=272,And then the next stage is
cs-410_2_5_60,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:35,400","00:04:38,360",60,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=275,We're going to merge them and
cs-410_2_5_61,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:38,360","00:04:41,729",61,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=278,"Eventually, we will get"
cs-410_2_5_62,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:41,729","00:04:45,310",62,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=281,where the entries are sorted
cs-410_2_5_63,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:46,960","00:04:50,870",63,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=286,"And on the top, we're going to see"
cs-410_2_5_64,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:50,870","00:04:53,620",64,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=290,the documents that match term ID 1.
cs-410_2_5_65,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:53,620","00:05:00,300",65,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=293,"So this is basically, how we can do"
cs-410_2_5_66,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:00,300","00:05:06,445",66,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=300,Even though the data cannot be
cs-410_2_5_67,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:06,445","00:05:12,562",67,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=306,"Now, we mention earlier that"
cs-410_2_5_68,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:12,562","00:05:15,848",68,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=312,it's desirable to compress them.
cs-410_2_5_69,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:15,848","00:05:20,481",69,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=315,So let's now take a little bit
cs-410_2_5_70,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:20,481","00:05:24,084",70,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=320,"Well the idea of compression in general,"
cs-410_2_5_71,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:24,084","00:05:28,310",71,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=324,leverage skewed distributions of values.
cs-410_2_5_72,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:28,310","00:05:31,090",72,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=328,And we generally have to use
cs-410_2_5_73,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:31,090","00:05:36,830",73,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=331,instead of the fixed-length
cs-410_2_5_74,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:36,830","00:05:41,080",74,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=336,a program manager like C++.
cs-410_2_5_75,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:41,080","00:05:45,840",75,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=341,And so how can we leverage
cs-410_2_5_76,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:45,840","00:05:48,180",76,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=345,to compress these values?
cs-410_2_5_77,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:48,180","00:05:53,827",77,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=348,"Well in general, we will use few"
cs-410_2_5_78,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:53,827","00:06:00,650",78,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=353,words at the cost of using longer
cs-410_2_5_79,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:00,650","00:06:04,560",79,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=360,"So in our case, let's think about how"
cs-410_2_5_80,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:05,640","00:06:09,640",80,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=365,"Now, if you can picture what"
cs-410_2_5_81,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:09,640","00:06:13,807",81,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=369,"you will see in post things,"
cs-410_2_5_82,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:13,807","00:06:19,089",82,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=373,Those are the frequencies of
cs-410_2_5_83,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:19,089","00:06:25,650",83,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=379,"Now, if you think about it, what kind"
cs-410_2_5_84,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:25,650","00:06:29,980",84,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=385,You probably will be able to guess
cs-410_2_5_85,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:29,980","00:06:32,540",85,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=389,far more frequently than large numbers.
cs-410_2_5_86,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:32,540","00:06:33,990",86,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=392,Why?
cs-410_2_5_87,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:33,990","00:06:39,954",87,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=393,"Well, think about the distribution of"
cs-410_2_5_88,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:39,954","00:06:44,810",88,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=399,and many words occur just rarely so
cs-410_2_5_89,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:44,810","00:06:48,419",89,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=404,"Therefore, we can use fewer bits for"
cs-410_2_5_90,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:48,419","00:06:53,855",90,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=408,highly frequent integers and
cs-410_2_5_91,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:53,855","00:06:57,095",91,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=413,that's cost of using more bits for
cs-410_2_5_92,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:58,445","00:07:00,005",92,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=418,This is a trade off of course.
cs-410_2_5_93,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:00,005","00:07:05,712",93,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=420,"If the values are distributed to uniform,"
cs-410_2_5_94,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:05,712","00:07:10,824",94,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=425,but because we tend to see many small
cs-410_2_5_95,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:10,824","00:07:15,769",95,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=430,We can save on average even though
cs-410_2_5_96,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:15,769","00:07:17,740",96,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=435,we have to use a lot of bits.
cs-410_2_5_97,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:19,750","00:07:23,700",97,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=439,What about the document IDs
cs-410_2_5_98,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:23,700","00:07:27,240",98,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=443,Well they are not distributed
cs-410_2_5_99,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:27,240","00:07:31,840",99,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=447,So how can we deal with that?
cs-410_2_5_100,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:31,840","00:07:35,488",100,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=451,Well it turns out that we can
cs-410_2_5_101,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:35,488","00:07:38,686",101,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=455,that is to store the difference
cs-410_2_5_102,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:38,686","00:07:43,495",102,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=458,And we can imagine if a term has
cs-410_2_5_103,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:43,495","00:07:46,640",103,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=463,there will be longest of document IDs.
cs-410_2_5_104,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:46,640","00:07:52,030",104,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=466,"So when we take the gap, and we take the"
cs-410_2_5_105,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:52,030","00:07:54,340",105,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=472,those gaps will be small.
cs-410_2_5_106,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:54,340","00:07:57,594",106,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=474,"So again, see a lot of small numbers."
cs-410_2_5_107,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:57,594","00:08:00,217",107,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=477,Whereas if a term occurred
cs-410_2_5_108,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:00,217","00:08:04,300",108,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=480,"then the gap would be large,"
cs-410_2_5_109,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:04,300","00:08:06,610",109,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=484,"So this creates some skewed distribution,"
cs-410_2_5_110,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:06,610","00:08:10,669",110,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=486,that would allow us to
cs-410_2_5_111,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:11,850","00:08:15,621",111,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=491,This is also possible because
cs-410_2_5_112,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:15,621","00:08:21,249",112,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=495,"uncompress these document IDs,"
cs-410_2_5_113,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:21,249","00:08:25,484",113,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=501,Because we stored the difference and
cs-410_2_5_114,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:25,484","00:08:29,574",114,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=505,document ID we have to first
cs-410_2_5_115,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:29,574","00:08:34,536",115,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=509,And then we can add the difference to
cs-410_2_5_116,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:34,536","00:08:36,365",116,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=514,the current document ID.
cs-410_2_5_117,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:36,365","00:08:40,834",117,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=516,Now this was possible because we only
cs-410_2_5_118,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:40,834","00:08:42,900",118,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=520,those document IDs.
cs-410_2_5_119,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:42,900","00:08:46,920",119,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=522,"Once we look up the term, we look up all"
cs-410_2_5_120,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:46,920","00:08:48,670",120,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=526,then we sequentially process them.
cs-410_2_5_121,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:48,670","00:08:52,070",121,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=528,"So it's very natural,"
cs-410_2_5_122,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:53,600","00:08:55,760",122,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=533,And there are many different methods for
cs-410_2_5_123,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:55,760","00:09:02,116",123,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=535,So binary code is a commonly used
cs-410_2_5_124,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:02,116","00:09:05,994",124,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=542,We use basically fixed glance in coding.
cs-410_2_5_125,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:05,994","00:09:09,276",125,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=545,"Unary code, gamma code, and"
cs-410_2_5_126,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:09,276","00:09:11,240",126,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=549,there are many other possibilities.
cs-410_2_5_127,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:11,240","00:09:14,130",127,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=551,So let's look at some
cs-410_2_5_128,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:14,130","00:09:16,900",128,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=554,Binary coding is really
cs-410_2_5_129,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:16,900","00:09:20,930",129,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=556,that's a property for
cs-410_2_5_130,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:20,930","00:09:24,700",130,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=560,The unary coding is a variable
cs-410_2_5_131,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:24,700","00:09:28,891",131,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=564,"In this case, integer this 1 will be"
cs-410_2_5_132,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:28,891","00:09:33,630",132,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=568,"encoded as x -1, 1 bit followed by 0."
cs-410_2_5_133,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:33,630","00:09:39,329",133,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=573,"So for example, 3 will be encoded as 2,"
cs-410_2_5_134,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:39,329","00:09:45,042",134,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=579,"whereas 5 will be encoded as 4,"
cs-410_2_5_135,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:45,042","00:09:51,599",135,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=585,So now you can imagine how many bits do we
cs-410_2_5_136,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:51,599","00:09:57,450",136,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=591,So how many bits do you have to
cs-410_2_5_137,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:57,450","00:10:02,549",137,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=597,"Well exactly, we have to use 100 bits."
cs-410_2_5_138,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:02,549","00:10:07,150",138,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=602,So it's the same number of bits
cs-410_2_5_139,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:07,150","00:10:12,360",139,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=607,So this is very inefficient if you
cs-410_2_5_140,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:12,360","00:10:17,620",140,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=612,Imagine if you occasionally see a number
cs-410_2_5_141,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:17,620","00:10:22,894",141,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=617,So this only works well if you
cs-410_2_5_142,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:22,894","00:10:28,082",142,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=622,"no large numbers, mostly very"
cs-410_2_5_143,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:28,082","00:10:30,184",143,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=628,"Now, how do you decode this code?"
cs-410_2_5_144,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:30,184","00:10:33,662",144,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=630,Now since these are variable
cs-410_2_5_145,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:33,662","00:10:37,070",145,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=633,you can't just count how many bits and
cs-410_2_5_146,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:38,500","00:10:44,800",146,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=638,"You can't say 8-bits or 32-bits,"
cs-410_2_5_147,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:44,800","00:10:50,860",147,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=644,"They are variable length, so"
cs-410_2_5_148,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:50,860","00:10:55,192",148,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=650,"In this case for unary, you can see"
cs-410_2_5_149,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:55,192","00:10:59,161",149,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=655,Now you can easily see 0 would
cs-410_2_5_150,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:59,161","00:11:03,120",150,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=659,So you just count up how many 1s you
cs-410_2_5_151,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:03,120","00:11:06,560",151,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=663,"You have finished one number,"
cs-410_2_5_152,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:07,960","00:11:11,266",152,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=667,Now we just saw that unary
cs-410_2_5_153,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:11,266","00:11:13,987",153,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=671,"In rewarding small numbers, and"
cs-410_2_5_154,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:13,987","00:11:20,430",154,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=673,if you occasionally can see a very
cs-410_2_5_155,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:20,430","00:11:24,900",155,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=680,So what about some other
cs-410_2_5_156,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:24,900","00:11:29,200",156,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=684,Well gamma coding's one of them and
cs-410_2_5_157,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:29,200","00:11:34,072",157,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=689,in this method we can use unary coding for
cs-410_2_5_158,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:34,072","00:11:37,239",158,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=694,a transform form of that.
cs-410_2_5_159,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:37,239","00:11:41,210",159,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=697,So it's 1 plus the floor of log of x.
cs-410_2_5_160,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:41,210","00:11:47,781",160,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=701,So the magnitude of this value is
cs-410_2_5_161,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:47,781","00:11:52,703",161,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=707,So that's why we can afford
cs-410_2_5_162,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:52,703","00:11:58,728",162,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=712,And so first I have the unary code for
cs-410_2_5_163,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:58,728","00:12:02,327",163,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=718,And this would be followed by
cs-410_2_5_164,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:02,327","00:12:08,058",164,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=722,"And this basically the same uniform code,"
cs-410_2_5_165,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:08,058","00:12:15,956",165,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=728,And we're going to use this coder to code
cs-410_2_5_166,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:15,956","00:12:22,178",166,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=735,And this is basically precisely
cs-410_2_5_167,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:25,000","00:12:30,376",167,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=745,So the unary code are basically
cs-410_2_5_168,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:30,376","00:12:33,029",168,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=750,well add one there and here.
cs-410_2_5_169,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:33,029","00:12:38,428",169,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=753,But the remaining part
cs-410_2_5_170,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:38,428","00:12:43,413",170,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=758,code through actually code the difference
cs-410_2_5_171,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:43,413","00:12:47,990",171,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=763,between the x and this 2 to the log of x.
cs-410_2_5_172,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:49,530","00:12:53,280",172,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=769,And it's easy to show that for this
cs-410_2_5_173,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:55,790","00:13:00,297",173,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=775,difference we only need to use up
cs-410_2_5_174,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:00,297","00:13:05,390",174,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=780,to this many bits and
cs-410_2_5_175,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:06,530","00:13:08,410",175,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=786,"And this is easy to understand,"
cs-410_2_5_176,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:08,410","00:13:12,910",176,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=788,"if the difference is too large, then we"
cs-410_2_5_177,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:14,330","00:13:19,000",177,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=794,So here are some examples for
cs-410_2_5_178,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:19,000","00:13:22,575",178,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=799,The first two digits are the unary code.
cs-410_2_5_179,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:22,575","00:13:26,706",179,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=802,"So this isn't for the value 2,"
cs-410_2_5_180,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:26,706","00:13:30,990",180,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=806,10 encodes 2 in unary coding.
cs-410_2_5_181,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:32,490","00:13:37,300",181,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=812,And so that means the floor of
cs-410_2_5_182,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:37,300","00:13:42,398",182,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=817,"log of x is 1,"
cs-410_2_5_183,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:42,398","00:13:45,620",183,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=822,"In code 1 plus the flow of log of x,"
cs-410_2_5_184,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:45,620","00:13:50,145",184,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=825,since this is two then we know that
cs-410_2_5_185,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:52,000","00:13:55,720",185,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=832,So that 3 is still larger than 2 to the 1.
cs-410_2_5_186,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:55,720","00:14:00,040",186,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=835,"So the difference is 1, and"
cs-410_2_5_187,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:01,460","00:14:04,690",187,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=841,So that's why we have 101 for 3.
cs-410_2_5_188,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:04,690","00:14:11,554",188,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=844,"Now similarly 5 is encoded as 110,"
cs-410_2_5_189,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:12,970","00:14:17,981",189,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=852,And in this case the unary code in code 3.
cs-410_2_5_190,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:17,981","00:14:25,445",190,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=857,And so this is a unary code 110 and
cs-410_2_5_191,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:25,445","00:14:30,362",191,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=865,And that means we're going to
cs-410_2_5_192,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:30,362","00:14:32,784",192,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=870,the 2 to the 2 and that's 1.
cs-410_2_5_193,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:32,784","00:14:35,803",193,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=872,And so we now have again 1 at the end.
cs-410_2_5_194,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:35,803","00:14:39,226",194,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=875,"But this time we're going to use 2 bits,"
cs-410_2_5_195,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:39,226","00:14:43,570",195,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=879,because with this level
cs-410_2_5_196,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:43,570","00:14:51,040",196,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=883,"We could have more numbers a 5, 6, 7 they"
cs-410_2_5_197,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:51,040","00:14:53,210",197,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=891,"So in order to differentiate them,"
cs-410_2_5_198,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:53,210","00:14:57,690",198,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=893,we have to use 2 bits in
cs-410_2_5_199,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:57,690","00:15:03,670",199,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=897,So you can imagine 6 would be 10 here
cs-410_2_5_200,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:04,710","00:15:10,615",200,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=904,It's also true that the form of
cs-410_2_5_201,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:10,615","00:15:15,155",201,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=910,"odd number of bits, and"
cs-410_2_5_202,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:15,155","00:15:17,305",202,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=915,That's the end of the unary code.
cs-410_2_5_203,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:18,335","00:15:24,385",203,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=918,And before that or on the left side
cs-410_2_5_204,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:24,385","00:15:30,265",204,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=924,"And on the right side of this 0,"
cs-410_2_5_205,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:32,550","00:15:36,540",205,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=932,So how can you decode such code?
cs-410_2_5_206,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:36,540","00:15:39,866",206,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=936,Well you again first do unary coding.
cs-410_2_5_207,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:39,866","00:15:45,371",207,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=939,"Once you hit 0, you have got the unary"
cs-410_2_5_208,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:45,371","00:15:50,408",208,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=945,many bits you have to read further
cs-410_2_5_209,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:50,408","00:15:53,693",209,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=950,So this is how you can
cs-410_2_5_210,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:53,693","00:15:57,998",210,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=953,There is also a delta code that's
cs-410_2_5_211,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:57,998","00:16:01,340",211,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=957,that you replace the unary
cs-410_2_5_212,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:01,340","00:16:04,980",212,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=961,So that's even less
cs-410_2_5_213,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:04,980","00:16:08,910",213,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=964,in terms of wording the small integers.
cs-410_2_5_214,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:08,910","00:16:12,150",214,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=968,"So that means, it's okay if you"
cs-410_2_5_215,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:14,100","00:16:15,190",215,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=974,It's okay with delta code.
cs-410_2_5_216,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:16,810","00:16:23,210",216,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=976,"It's also fine with the gamma code,"
cs-410_2_5_217,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:23,210","00:16:26,710",217,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=983,"And they are all operating of course,"
cs-410_2_5_218,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:26,710","00:16:32,360",218,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=986,at different degrees of favoring short or
cs-410_2_5_219,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:32,360","00:16:38,560",219,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=992,And that also means they would be
cs-410_2_5_220,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:38,560","00:16:41,720",220,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=998,But none of them is perfect for
cs-410_2_5_221,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:41,720","00:16:45,990",221,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1001,And which method works the best would
cs-410_2_5_222,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:45,990","00:16:47,610",222,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1005,in your dataset.
cs-410_2_5_223,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:47,610","00:16:49,660",223,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1007,"For inverted index compression,"
cs-410_2_5_224,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:49,660","00:16:52,990",224,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1009,people have found that gamma
cs-410_2_5_225,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:55,114","00:16:58,340",225,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1015,So how to uncompress inverted index?
cs-410_2_5_226,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:58,340","00:16:59,900",226,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1018,I will just talk about this.
cs-410_2_5_227,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:59,900","00:17:02,920",227,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1019,"Firstly, you decode"
cs-410_2_5_228,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:02,920","00:17:10,877",228,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1022,And we just I think discussed the how we
cs-410_2_5_229,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:10,877","00:17:15,720",229,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1030,What about the document IDs that
cs-410_2_5_230,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:15,720","00:17:19,320",230,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1035,"Well, we're going to do"
cs-410_2_5_231,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:19,320","00:17:23,800",231,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1039,"supposed the encoded I list is x1,"
cs-410_2_5_232,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:23,800","00:17:28,384",232,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1043,We first decode x1 to obtain
cs-410_2_5_233,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:28,384","00:17:29,845",233,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1048,"Then we can decode x2,"
cs-410_2_5_234,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:29,845","00:17:34,610",234,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1049,which is actually the difference between
cs-410_2_5_235,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:34,610","00:17:40,240",235,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1054,So we have to add the decoder
cs-410_2_5_236,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:40,240","00:17:45,630",236,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1060,the value of the ID at
cs-410_2_5_237,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:46,690","00:17:50,420",237,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1066,So this is where you can
cs-410_2_5_238,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:50,420","00:17:52,870",238,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1070,converting document IDs to integers.
cs-410_2_5_239,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:52,870","00:17:55,730",239,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1072,And that allows us to do
cs-410_2_5_240,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:55,730","00:17:59,590",240,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1075,And we just repeat until we
cs-410_2_5_241,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:59,590","00:18:04,004",241,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1079,Every time we use the document ID in
cs-410_2_5_242,cs-410,2,5, System Implementation - Inverted Index Construction,"00:18:04,004","00:18:06,257",242,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1084,the document ID in the next position.
cs-410_2_5_243,cs-410,2,5, System Implementation - Inverted Index Construction,"00:18:08,871","00:18:18,871",243,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1088,[MUSIC]
cs-410_2_6_1,cs-410,2,6, System Implementation - Fast Search,"00:00:00,000","00:00:07,148",1,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=0,[SOUND]
cs-410_2_6_2,cs-410,2,6, System Implementation - Fast Search,"00:00:07,148","00:00:12,770",2,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=7,This lecture is about how to do faster
cs-410_2_6_3,cs-410,2,6, System Implementation - Fast Search,"00:00:14,710","00:00:19,487",3,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=14,"In this lecture, we're going to continue"
cs-410_2_6_4,cs-410,2,6, System Implementation - Fast Search,"00:00:19,487","00:00:20,583",4,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=19,"In particular,"
cs-410_2_6_5,cs-410,2,6, System Implementation - Fast Search,"00:00:20,583","00:00:25,840",5,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=20,we're going to talk about how to support
cs-410_2_6_6,cs-410,2,6, System Implementation - Fast Search,"00:00:26,906","00:00:31,360",6,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=26,So let's think about what a general
cs-410_2_6_7,cs-410,2,6, System Implementation - Fast Search,"00:00:32,730","00:00:37,260",7,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=32,"Now of course, the vector space"
cs-410_2_6_8,cs-410,2,6, System Implementation - Fast Search,"00:00:37,260","00:00:40,750",8,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=37,we can imagine many other retrieval
cs-410_2_6_9,cs-410,2,6, System Implementation - Fast Search,"00:00:42,390","00:00:44,970",9,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=42,So the form of this
cs-410_2_6_10,cs-410,2,6, System Implementation - Fast Search,"00:00:46,060","00:00:49,870",10,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=46,We see this scoring function
cs-410_2_6_11,cs-410,2,6, System Implementation - Fast Search,"00:00:49,870","00:00:55,260",11,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=49,a query Q is defined as
cs-410_2_6_12,cs-410,2,6, System Implementation - Fast Search,"00:00:55,260","00:01:00,350",12,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=55,that adjustment a function that
cs-410_2_6_13,cs-410,2,6, System Implementation - Fast Search,"00:01:00,350","00:01:05,425",13,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=60,"That I'll assume here at the end,"
cs-410_2_6_14,cs-410,2,6, System Implementation - Fast Search,"00:01:05,425","00:01:09,100",14,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=65,f sub d of d and f sub q of q.
cs-410_2_6_15,cs-410,2,6, System Implementation - Fast Search,"00:01:09,100","00:01:14,467",15,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=69,These are adjustment factors
cs-410_2_6_16,cs-410,2,6, System Implementation - Fast Search,"00:01:14,467","00:01:19,387",16,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=74,so they are at the level of a document and
cs-410_2_6_17,cs-410,2,6, System Implementation - Fast Search,"00:01:19,387","00:01:22,719",17,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=79,"So and then inside of this function,"
cs-410_2_6_18,cs-410,2,6, System Implementation - Fast Search,"00:01:22,719","00:01:27,127",18,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=82,we also see there's
cs-410_2_6_19,cs-410,2,6, System Implementation - Fast Search,"00:01:27,127","00:01:33,102",19,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=87,So this is the main part
cs-410_2_6_20,cs-410,2,6, System Implementation - Fast Search,"00:01:33,102","00:01:38,365",20,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=93,these as I just said of
cs-410_2_6_21,cs-410,2,6, System Implementation - Fast Search,"00:01:38,365","00:01:43,931",21,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=98,the level of the whole document and
cs-410_2_6_22,cs-410,2,6, System Implementation - Fast Search,"00:01:43,931","00:01:47,521",22,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=103,"For example, document [INAUDIBLE] and"
cs-410_2_6_23,cs-410,2,6, System Implementation - Fast Search,"00:01:47,521","00:01:52,805",23,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=107,this aggregate punching would
cs-410_2_6_24,cs-410,2,6, System Implementation - Fast Search,"00:01:52,805","00:01:55,847",24,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=112,"Now inside this h function,"
cs-410_2_6_25,cs-410,2,6, System Implementation - Fast Search,"00:01:55,847","00:02:01,300",25,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=115,there are functions that
cs-410_2_6_26,cs-410,2,6, System Implementation - Fast Search,"00:02:01,300","00:02:06,384",26,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=121,of the contribution of
cs-410_2_6_27,cs-410,2,6, System Implementation - Fast Search,"00:02:08,475","00:02:14,305",27,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=128,"So this g,"
cs-410_2_6_28,cs-410,2,6, System Implementation - Fast Search,"00:02:14,305","00:02:19,670",28,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=134,of a matched query term ti in document d.
cs-410_2_6_29,cs-410,2,6, System Implementation - Fast Search,"00:02:23,710","00:02:28,365",29,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=143,And this h function would then
cs-410_2_6_30,cs-410,2,6, System Implementation - Fast Search,"00:02:28,365","00:02:34,390",30,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=148,"So for example,"
cs-410_2_6_31,cs-410,2,6, System Implementation - Fast Search,"00:02:36,110","00:02:39,940",31,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=156,but it can also be a product or it could
cs-410_2_6_32,cs-410,2,6, System Implementation - Fast Search,"00:02:41,250","00:02:46,207",32,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=161,"And then finally, this adjustment"
cs-410_2_6_33,cs-410,2,6, System Implementation - Fast Search,"00:02:46,207","00:02:51,162",33,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=166,the document level or query level
cs-410_2_6_34,cs-410,2,6, System Implementation - Fast Search,"00:02:51,162","00:02:53,697",34,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=171,"for example, document [INAUDIBLE]."
cs-410_2_6_35,cs-410,2,6, System Implementation - Fast Search,"00:02:53,697","00:02:58,960",35,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=173,"So, this general form would cover"
cs-410_2_6_36,cs-410,2,6, System Implementation - Fast Search,"00:02:58,960","00:03:06,610",36,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=178,Let's look at how we can score documents
cs-410_2_6_37,cs-410,2,6, System Implementation - Fast Search,"00:03:07,610","00:03:10,930",37,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=187,"So, here's a general algorithm"
cs-410_2_6_38,cs-410,2,6, System Implementation - Fast Search,"00:03:10,930","00:03:14,670",38,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=190,First this query level and
cs-410_2_6_39,cs-410,2,6, System Implementation - Fast Search,"00:03:14,670","00:03:19,540",39,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=194,document level factors can be
cs-410_2_6_40,cs-410,2,6, System Implementation - Fast Search,"00:03:19,540","00:03:22,810",40,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=199,"Of course, for the query we have to"
cs-410_2_6_41,cs-410,2,6, System Implementation - Fast Search,"00:03:22,810","00:03:28,180",41,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=202,"document, for example,"
cs-410_2_6_42,cs-410,2,6, System Implementation - Fast Search,"00:03:28,180","00:03:32,850",42,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=208,"And then, we maintain a score accumulator"
cs-410_2_6_43,cs-410,2,6, System Implementation - Fast Search,"00:03:34,710","00:03:39,440",43,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=214,An h is an aggregation function
cs-410_2_6_44,cs-410,2,6, System Implementation - Fast Search,"00:03:39,440","00:03:40,530",44,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=219,So how do we do that?
cs-410_2_6_45,cs-410,2,6, System Implementation - Fast Search,"00:03:40,530","00:03:45,770",45,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=220,For each period term we're going to
cs-410_2_6_46,cs-410,2,6, System Implementation - Fast Search,"00:03:45,770","00:03:47,130",46,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=225,from the invert index.
cs-410_2_6_47,cs-410,2,6, System Implementation - Fast Search,"00:03:47,130","00:03:51,290",47,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=227,This will give us all the documents
cs-410_2_6_48,cs-410,2,6, System Implementation - Fast Search,"00:03:52,850","00:03:57,640",48,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=232,"and that includes d1, f1 and so dn fn."
cs-410_2_6_49,cs-410,2,6, System Implementation - Fast Search,"00:03:57,640","00:04:03,394",49,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=237,So each pair is a document ID and
cs-410_2_6_50,cs-410,2,6, System Implementation - Fast Search,"00:04:03,394","00:04:08,268",50,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=243,Then for each entry d sub j and
cs-410_2_6_51,cs-410,2,6, System Implementation - Fast Search,"00:04:08,268","00:04:12,436",51,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=248,of the term in this
cs-410_2_6_52,cs-410,2,6, System Implementation - Fast Search,"00:04:12,436","00:04:17,739",52,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=252,We'll going to compute the function
cs-410_2_6_53,cs-410,2,6, System Implementation - Fast Search,"00:04:17,739","00:04:19,370",53,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=257,"weight of this term, so"
cs-410_2_6_54,cs-410,2,6, System Implementation - Fast Search,"00:04:19,370","00:04:26,170",54,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=259,we're computing the weight completion of
cs-410_2_6_55,cs-410,2,6, System Implementation - Fast Search,"00:04:26,170","00:04:31,152",55,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=266,"And then, we're going to update"
cs-410_2_6_56,cs-410,2,6, System Implementation - Fast Search,"00:04:31,152","00:04:35,820",56,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=271,this document and
cs-410_2_6_57,cs-410,2,6, System Implementation - Fast Search,"00:04:35,820","00:04:41,144",57,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=275,accumulator that would
cs-410_2_6_58,cs-410,2,6, System Implementation - Fast Search,"00:04:41,144","00:04:46,640",58,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=281,So this is basically a general
cs-410_2_6_59,cs-410,2,6, System Implementation - Fast Search,"00:04:46,640","00:04:51,288",59,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=286,functions of this form by
cs-410_2_6_60,cs-410,2,6, System Implementation - Fast Search,"00:04:51,288","00:04:54,621",60,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=291,Note that we don't have to
cs-410_2_6_61,cs-410,2,6, System Implementation - Fast Search,"00:04:54,621","00:04:56,906",61,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=294,that didn't match any query term.
cs-410_2_6_62,cs-410,2,6, System Implementation - Fast Search,"00:04:56,906","00:04:59,096",62,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=296,"Well, this is why it's fast,"
cs-410_2_6_63,cs-410,2,6, System Implementation - Fast Search,"00:04:59,096","00:05:04,418",63,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=299,we only need to process the documents
cs-410_2_6_64,cs-410,2,6, System Implementation - Fast Search,"00:05:04,418","00:05:09,415",64,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=304,"In the end, then we're going to adjust"
cs-410_2_6_65,cs-410,2,6, System Implementation - Fast Search,"00:05:09,415","00:05:11,600",65,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=309,sub a and then we can sort.
cs-410_2_6_66,cs-410,2,6, System Implementation - Fast Search,"00:05:11,600","00:05:14,270",66,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=311,So let's take a look
cs-410_2_6_67,cs-410,2,6, System Implementation - Fast Search,"00:05:14,270","00:05:17,880",67,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=314,"In this case, let's assume the scoring"
cs-410_2_6_68,cs-410,2,6, System Implementation - Fast Search,"00:05:17,880","00:05:24,450",68,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=317,"it just takes the sum of t f, the role of"
cs-410_2_6_69,cs-410,2,6, System Implementation - Fast Search,"00:05:25,830","00:05:31,340",69,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=325,This simplification would help
cs-410_2_6_70,cs-410,2,6, System Implementation - Fast Search,"00:05:31,340","00:05:36,640",70,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=331,It's very easy to extend the computation
cs-410_2_6_71,cs-410,2,6, System Implementation - Fast Search,"00:05:36,640","00:05:43,422",71,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=336,"the transformation of tf, or [INAUDIBLE]"
cs-410_2_6_72,cs-410,2,6, System Implementation - Fast Search,"00:05:43,422","00:05:47,890",72,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=343,"So let's take a look at specific example,"
cs-410_2_6_73,cs-410,2,6, System Implementation - Fast Search,"00:05:48,980","00:05:54,600",73,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=348,and it show some entries of
cs-410_2_6_74,cs-410,2,6, System Implementation - Fast Search,"00:05:54,600","00:05:56,800",74,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=354,Information occurred in four documents and
cs-410_2_6_75,cs-410,2,6, System Implementation - Fast Search,"00:05:56,800","00:06:01,260",75,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=356,"their frequencies are also there,"
cs-410_2_6_76,cs-410,2,6, System Implementation - Fast Search,"00:06:01,260","00:06:07,210",76,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=361,"So let's see how the arrows works, so"
cs-410_2_6_77,cs-410,2,6, System Implementation - Fast Search,"00:06:07,210","00:06:09,580",77,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=367,"and we fetch the first query then,"
cs-410_2_6_78,cs-410,2,6, System Implementation - Fast Search,"00:06:09,580","00:06:12,260",78,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=369,"That's information, right?"
cs-410_2_6_79,cs-410,2,6, System Implementation - Fast Search,"00:06:12,260","00:06:16,360",79,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=372,And imagine we have all these
cs-410_2_6_80,cs-410,2,6, System Implementation - Fast Search,"00:06:17,800","00:06:19,800",80,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=377,scores for these documents.
cs-410_2_6_81,cs-410,2,6, System Implementation - Fast Search,"00:06:19,800","00:06:21,740",81,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=379,We can imagine there will be other but
cs-410_2_6_82,cs-410,2,6, System Implementation - Fast Search,"00:06:21,740","00:06:24,660",82,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=381,then they will only be
cs-410_2_6_83,cs-410,2,6, System Implementation - Fast Search,"00:06:24,660","00:06:28,681",83,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=384,"So before we do any waiting of terms,"
cs-410_2_6_84,cs-410,2,6, System Implementation - Fast Search,"00:06:28,681","00:06:32,979",84,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=388,we don't even need a score of.
cs-410_2_6_85,cs-410,2,6, System Implementation - Fast Search,"00:06:32,979","00:06:36,859",85,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=392,That comes actually we have these score
cs-410_2_6_86,cs-410,2,6, System Implementation - Fast Search,"00:06:38,260","00:06:43,110",86,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=398,So lets fetch the interest from
cs-410_2_6_87,cs-410,2,6, System Implementation - Fast Search,"00:06:43,110","00:06:45,080",87,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=403,"information, that the first one."
cs-410_2_6_88,cs-410,2,6, System Implementation - Fast Search,"00:06:46,260","00:06:50,809",88,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=406,So these four accumulators obviously
cs-410_2_6_89,cs-410,2,6, System Implementation - Fast Search,"00:06:51,830","00:06:54,418",89,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=411,"So, the first entry is d1 and 3,"
cs-410_2_6_90,cs-410,2,6, System Implementation - Fast Search,"00:06:54,418","00:06:58,357",90,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=414,3 is occurrences of
cs-410_2_6_91,cs-410,2,6, System Implementation - Fast Search,"00:06:58,357","00:07:03,617",91,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=418,Since our scoring function assume that the
cs-410_2_6_92,cs-410,2,6, System Implementation - Fast Search,"00:07:03,617","00:07:09,178",92,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=423,We just need to add a 3 to the score
cs-410_2_6_93,cs-410,2,6, System Implementation - Fast Search,"00:07:09,178","00:07:16,388",93,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=429,the increase of score due to matching
cs-410_2_6_94,cs-410,2,6, System Implementation - Fast Search,"00:07:16,388","00:07:19,679",94,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=436,"And then, we go to the next entry,"
cs-410_2_6_95,cs-410,2,6, System Implementation - Fast Search,"00:07:19,679","00:07:22,493",95,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=439,then we add a 4 to the score
cs-410_2_6_96,cs-410,2,6, System Implementation - Fast Search,"00:07:22,493","00:07:27,614",96,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=442,"Of course, at this point, that we will"
cs-410_2_6_97,cs-410,2,6, System Implementation - Fast Search,"00:07:27,614","00:07:33,427",97,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=447,"And so at this point, we allocated"
cs-410_2_6_98,cs-410,2,6, System Implementation - Fast Search,"00:07:33,427","00:07:39,174",98,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=453,"and we add one, we allocate another"
cs-410_2_6_99,cs-410,2,6, System Implementation - Fast Search,"00:07:39,174","00:07:44,370",99,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=459,"And then finally,"
cs-410_2_6_100,cs-410,2,6, System Implementation - Fast Search,"00:07:44,370","00:07:50,450",100,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=464,information occurred five
cs-410_2_6_101,cs-410,2,6, System Implementation - Fast Search,"00:07:50,450","00:07:55,310",101,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=470,"Okay, so this completes the processing of"
cs-410_2_6_102,cs-410,2,6, System Implementation - Fast Search,"00:07:55,310","00:07:56,500",102,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=475,information.
cs-410_2_6_103,cs-410,2,6, System Implementation - Fast Search,"00:07:56,500","00:08:00,080",103,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=476,It processed all the contributions
cs-410_2_6_104,cs-410,2,6, System Implementation - Fast Search,"00:08:00,080","00:08:00,820",104,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=480,four documents.
cs-410_2_6_105,cs-410,2,6, System Implementation - Fast Search,"00:08:01,830","00:08:06,900",105,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=481,"So now, our error will go to"
cs-410_2_6_106,cs-410,2,6, System Implementation - Fast Search,"00:08:06,900","00:08:09,810",106,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=486,"So, we're going to fetch all"
cs-410_2_6_107,cs-410,2,6, System Implementation - Fast Search,"00:08:10,830","00:08:11,520",107,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=490,"So, in this case,"
cs-410_2_6_108,cs-410,2,6, System Implementation - Fast Search,"00:08:11,520","00:08:15,700",108,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=491,"there are three entries, and"
cs-410_2_6_109,cs-410,2,6, System Implementation - Fast Search,"00:08:15,700","00:08:18,410",109,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=495,The first is d2 and 3 and
cs-410_2_6_110,cs-410,2,6, System Implementation - Fast Search,"00:08:18,410","00:08:22,890",110,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=498,that means security occur three
cs-410_2_6_111,cs-410,2,6, System Implementation - Fast Search,"00:08:22,890","00:08:26,300",111,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=502,"Well, we do exactly the same,"
cs-410_2_6_112,cs-410,2,6, System Implementation - Fast Search,"00:08:26,300","00:08:31,557",112,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=506,"So, this time we're going to change the"
cs-410_2_6_113,cs-410,2,6, System Implementation - Fast Search,"00:08:31,557","00:08:36,390",113,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=511,allocated and
cs-410_2_6_114,cs-410,2,6, System Implementation - Fast Search,"00:08:36,390","00:08:40,470",114,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=516,"value which is a 4, so"
cs-410_2_6_115,cs-410,2,6, System Implementation - Fast Search,"00:08:41,530","00:08:46,333",115,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=521,D2 score is increased because the match
cs-410_2_6_116,cs-410,2,6, System Implementation - Fast Search,"00:08:46,333","00:08:47,382",116,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=526,the security.
cs-410_2_6_117,cs-410,2,6, System Implementation - Fast Search,"00:08:47,382","00:08:53,721",117,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=527,"Go to the next entry, that's d4 and"
cs-410_2_6_118,cs-410,2,6, System Implementation - Fast Search,"00:08:53,721","00:08:59,040",118,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=533,"d4 and again, we add 1 to d4 so"
cs-410_2_6_119,cs-410,2,6, System Implementation - Fast Search,"00:08:59,040","00:09:02,449",119,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=539,"Finally, we process d5 and a 3."
cs-410_2_6_120,cs-410,2,6, System Implementation - Fast Search,"00:09:02,449","00:09:07,679",120,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=542,Since we have not yet allocated a score
cs-410_2_6_121,cs-410,2,6, System Implementation - Fast Search,"00:09:07,679","00:09:12,190",121,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=547,"we're going to allocate 1 for d5,"
cs-410_2_6_122,cs-410,2,6, System Implementation - Fast Search,"00:09:12,190","00:09:19,480",122,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=552,"So, those scores, of the last rule,"
cs-410_2_6_123,cs-410,2,6, System Implementation - Fast Search,"00:09:20,480","00:09:25,810",123,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=560,If our scoring function is just
cs-410_2_6_124,cs-410,2,6, System Implementation - Fast Search,"00:09:27,080","00:09:31,600",124,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=567,"Now, what if we, actually,"
cs-410_2_6_125,cs-410,2,6, System Implementation - Fast Search,"00:09:31,600","00:09:35,130",125,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=571,"Well, we going to do the [INAUDIBLE]"
cs-410_2_6_126,cs-410,2,6, System Implementation - Fast Search,"00:09:36,490","00:09:40,020",126,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=576,"So, to summarize this,"
cs-410_2_6_127,cs-410,2,6, System Implementation - Fast Search,"00:09:40,020","00:09:44,660",127,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=580,we first process the information
cs-410_2_6_128,cs-410,2,6, System Implementation - Fast Search,"00:09:44,660","00:09:49,520",128,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=584,we processed all the entries
cs-410_2_6_129,cs-410,2,6, System Implementation - Fast Search,"00:09:49,520","00:09:54,775",129,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=589,"Then we process the security,"
cs-410_2_6_130,cs-410,2,6, System Implementation - Fast Search,"00:09:54,775","00:10:00,916",130,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=594,what should be the order of processing
cs-410_2_6_131,cs-410,2,6, System Implementation - Fast Search,"00:10:00,916","00:10:05,677",131,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=600,It might make a difference especially
cs-410_2_6_132,cs-410,2,6, System Implementation - Fast Search,"00:10:05,677","00:10:07,670",132,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=605,the score accumulators.
cs-410_2_6_133,cs-410,2,6, System Implementation - Fast Search,"00:10:07,670","00:10:12,226",133,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=607,"Let's say, we only want to keep"
cs-410_2_6_134,cs-410,2,6, System Implementation - Fast Search,"00:10:12,226","00:10:15,601",134,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=612,What do you think would be
cs-410_2_6_135,cs-410,2,6, System Implementation - Fast Search,"00:10:15,601","00:10:22,680",135,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=615,Would you process a common term first or
cs-410_2_6_136,cs-410,2,6, System Implementation - Fast Search,"00:10:24,460","00:10:30,597",136,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=624,The answers is we just go to who
cs-410_2_6_137,cs-410,2,6, System Implementation - Fast Search,"00:10:30,597","00:10:35,531",137,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=630,"A rare term would match a few documents,"
cs-410_2_6_138,cs-410,2,6, System Implementation - Fast Search,"00:10:35,531","00:10:38,910",138,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=635,"be higher,"
cs-410_2_6_139,cs-410,2,6, System Implementation - Fast Search,"00:10:38,910","00:10:44,933",139,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=638,"And then, it allows us to attach"
cs-410_2_6_140,cs-410,2,6, System Implementation - Fast Search,"00:10:44,933","00:10:48,042",140,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=644,"So, it helps pruning"
cs-410_2_6_141,cs-410,2,6, System Implementation - Fast Search,"00:10:48,042","00:10:51,901",141,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=648,if we don't need so
cs-410_2_6_142,cs-410,2,6, System Implementation - Fast Search,"00:10:51,901","00:10:55,474",142,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=651,So those are all heuristics for
cs-410_2_6_143,cs-410,2,6, System Implementation - Fast Search,"00:10:55,474","00:10:59,850",143,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=655,Here you can also see how we can
cs-410_2_6_144,cs-410,2,6, System Implementation - Fast Search,"00:10:59,850","00:11:03,192",144,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=659,So they can [INAUDIBLE] when we
cs-410_2_6_145,cs-410,2,6, System Implementation - Fast Search,"00:11:03,192","00:11:04,700",145,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=663,each query time.
cs-410_2_6_146,cs-410,2,6, System Implementation - Fast Search,"00:11:04,700","00:11:08,420",146,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=664,When we fetch the inverted index we
cs-410_2_6_147,cs-410,2,6, System Implementation - Fast Search,"00:11:08,420","00:11:09,990",147,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=668,then we can compute IDF.
cs-410_2_6_148,cs-410,2,6, System Implementation - Fast Search,"00:11:09,990","00:11:13,710",148,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=669,Or maybe perhaps the IDF value
cs-410_2_6_149,cs-410,2,6, System Implementation - Fast Search,"00:11:13,710","00:11:16,890",149,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=673,when we indexed the documents.
cs-410_2_6_150,cs-410,2,6, System Implementation - Fast Search,"00:11:16,890","00:11:22,780",150,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=676,"At that time, we already computed"
cs-410_2_6_151,cs-410,2,6, System Implementation - Fast Search,"00:11:22,780","00:11:26,570",151,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=682,so all these can be done at this time.
cs-410_2_6_152,cs-410,2,6, System Implementation - Fast Search,"00:11:26,570","00:11:29,820",152,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=686,So that would mean when we process
cs-410_2_6_153,cs-410,2,6, System Implementation - Fast Search,"00:11:29,820","00:11:35,300",153,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=689,these words would be adjusted by the same
cs-410_2_6_154,cs-410,2,6, System Implementation - Fast Search,"00:11:36,590","00:11:39,580",154,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=696,So this is the basic idea of using
cs-410_2_6_155,cs-410,2,6, System Implementation - Fast Search,"00:11:39,580","00:11:44,770",155,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=699,it works well for all kinds of
cs-410_2_6_156,cs-410,2,6, System Implementation - Fast Search,"00:11:44,770","00:11:49,726",156,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=704,"And this generally,"
cs-410_2_6_157,cs-410,2,6, System Implementation - Fast Search,"00:11:49,726","00:11:53,397",157,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=709,most state of art retrieval functions.
cs-410_2_6_158,cs-410,2,6, System Implementation - Fast Search,"00:11:53,397","00:11:58,708",158,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=713,So there are some tricks to
cs-410_2_6_159,cs-410,2,6, System Implementation - Fast Search,"00:11:58,708","00:12:02,988",159,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=718,some general techniques
cs-410_2_6_160,cs-410,2,6, System Implementation - Fast Search,"00:12:02,988","00:12:07,756",160,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=722,This is we just store some results of
cs-410_2_6_161,cs-410,2,6, System Implementation - Fast Search,"00:12:07,756","00:12:12,291",161,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=727,"when you see the same query,"
cs-410_2_6_162,cs-410,2,6, System Implementation - Fast Search,"00:12:12,291","00:12:17,781",162,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=732,"Similarly, you can also slow the list"
cs-410_2_6_163,cs-410,2,6, System Implementation - Fast Search,"00:12:17,781","00:12:19,041",163,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=737,a popular term.
cs-410_2_6_164,cs-410,2,6, System Implementation - Fast Search,"00:12:19,041","00:12:21,268",164,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=739,"And if the query term is popular likely,"
cs-410_2_6_165,cs-410,2,6, System Implementation - Fast Search,"00:12:21,268","00:12:25,620",165,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=741,you will soon need to factor the inverted
cs-410_2_6_166,cs-410,2,6, System Implementation - Fast Search,"00:12:25,620","00:12:30,569",166,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=745,"So keeping it in the memory would help,"
cs-410_2_6_167,cs-410,2,6, System Implementation - Fast Search,"00:12:30,569","00:12:32,206",167,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=750,improving efficiency.
cs-410_2_6_168,cs-410,2,6, System Implementation - Fast Search,"00:12:32,206","00:12:36,694",168,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=752,We can also keep only the most promising
cs-410_2_6_169,cs-410,2,6, System Implementation - Fast Search,"00:12:36,694","00:12:39,281",169,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=756,doesn't want to examine so many documents.
cs-410_2_6_170,cs-410,2,6, System Implementation - Fast Search,"00:12:39,281","00:12:44,092",170,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=759,We only need to return high
cs-410_2_6_171,cs-410,2,6, System Implementation - Fast Search,"00:12:44,092","00:12:46,410",171,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=764,likely are ranked on the top.
cs-410_2_6_172,cs-410,2,6, System Implementation - Fast Search,"00:12:47,900","00:12:51,860",172,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=767,"For that purpose,"
cs-410_2_6_173,cs-410,2,6, System Implementation - Fast Search,"00:12:51,860","00:12:53,810",173,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=771,We don't have to store
cs-410_2_6_174,cs-410,2,6, System Implementation - Fast Search,"00:12:53,810","00:12:59,936",174,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=773,"At some point, we just keep"
cs-410_2_6_175,cs-410,2,6, System Implementation - Fast Search,"00:12:59,936","00:13:06,257",175,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=779,Another technique is to do parallel
cs-410_2_6_176,cs-410,2,6, System Implementation - Fast Search,"00:13:06,257","00:13:11,731",176,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=786,really process in such a large
cs-410_2_6_177,cs-410,2,6, System Implementation - Fast Search,"00:13:11,731","00:13:15,869",177,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=791,And you scale up to
cs-410_2_6_178,cs-410,2,6, System Implementation - Fast Search,"00:13:15,869","00:13:20,628",178,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=795,the special techniques you
cs-410_2_6_179,cs-410,2,6, System Implementation - Fast Search,"00:13:20,628","00:13:25,609",179,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=800,to distribute the storage
cs-410_2_6_180,cs-410,2,6, System Implementation - Fast Search,"00:13:25,609","00:13:31,850",180,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=805,So here is a list of some text retrieval
cs-410_2_6_181,cs-410,2,6, System Implementation - Fast Search,"00:13:31,850","00:13:37,160",181,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=811,You can find more information
cs-410_2_6_182,cs-410,2,6, System Implementation - Fast Search,"00:13:37,160","00:13:42,510",182,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=817,"And here, I listed your four here,"
cs-410_2_6_183,cs-410,2,6, System Implementation - Fast Search,"00:13:42,510","00:13:48,361",183,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=822,that can support a lot of applications and
cs-410_2_6_184,cs-410,2,6, System Implementation - Fast Search,"00:13:48,361","00:13:51,900",184,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=828,You can use it to build a search
cs-410_2_6_185,cs-410,2,6, System Implementation - Fast Search,"00:13:51,900","00:13:55,555",185,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=831,The downside is that it's not
cs-410_2_6_186,cs-410,2,6, System Implementation - Fast Search,"00:13:55,555","00:14:01,500",186,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=835,the algorithms implemented they are also
cs-410_2_6_187,cs-410,2,6, System Implementation - Fast Search,"00:14:01,500","00:14:06,294",187,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=841,Lemur or Indri is another
cs-410_2_6_188,cs-410,2,6, System Implementation - Fast Search,"00:14:06,294","00:14:10,068",188,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=846,a nice support web
cs-410_2_6_189,cs-410,2,6, System Implementation - Fast Search,"00:14:10,068","00:14:16,094",189,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=850,it has many advanced search algorithms and
cs-410_2_6_190,cs-410,2,6, System Implementation - Fast Search,"00:14:16,094","00:14:20,735",190,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=856,Terrier is yet another toolkit
cs-410_2_6_191,cs-410,2,6, System Implementation - Fast Search,"00:14:20,735","00:14:25,108",191,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=860,application capability and
cs-410_2_6_192,cs-410,2,6, System Implementation - Fast Search,"00:14:25,108","00:14:30,008",192,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=865,So that's maybe in between Lemur or
cs-410_2_6_193,cs-410,2,6, System Implementation - Fast Search,"00:14:30,008","00:14:34,663",193,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=870,maybe rather combining
cs-410_2_6_194,cs-410,2,6, System Implementation - Fast Search,"00:14:34,663","00:14:38,110",194,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=874,so that's also useful tool kit.
cs-410_2_6_195,cs-410,2,6, System Implementation - Fast Search,"00:14:38,110","00:14:41,920",195,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=878,MeTA is a toolkit that we will use for
cs-410_2_6_196,cs-410,2,6, System Implementation - Fast Search,"00:14:41,920","00:14:46,590",196,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=881,the problem assignment and
cs-410_2_6_197,cs-410,2,6, System Implementation - Fast Search,"00:14:47,690","00:14:54,250",197,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=887,a combination of both text retrieval
cs-410_2_6_198,cs-410,2,6, System Implementation - Fast Search,"00:14:54,250","00:15:01,390",198,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=894,And so talking models are implement they
cs-410_2_6_199,cs-410,2,6, System Implementation - Fast Search,"00:15:01,390","00:15:06,720",199,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=901,implemented in the toolkit as
cs-410_2_6_200,cs-410,2,6, System Implementation - Fast Search,"00:15:06,720","00:15:10,580",200,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=906,So to summarize all the discussion
cs-410_2_6_201,cs-410,2,6, System Implementation - Fast Search,"00:15:11,600","00:15:14,700",201,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=911,here are the major takeaway points.
cs-410_2_6_202,cs-410,2,6, System Implementation - Fast Search,"00:15:14,700","00:15:20,760",202,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=914,Inverted index is the primary data
cs-410_2_6_203,cs-410,2,6, System Implementation - Fast Search,"00:15:20,760","00:15:25,300",203,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=920,and that's the key to enable
cs-410_2_6_204,cs-410,2,6, System Implementation - Fast Search,"00:15:26,350","00:15:31,116",204,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=926,And the basic idea is to preprocess
cs-410_2_6_205,cs-410,2,6, System Implementation - Fast Search,"00:15:31,116","00:15:34,491",205,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=931,we want to do compression
cs-410_2_6_206,cs-410,2,6, System Implementation - Fast Search,"00:15:34,491","00:15:39,625",206,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=934,So that we can save disk space and
cs-410_2_6_207,cs-410,2,6, System Implementation - Fast Search,"00:15:39,625","00:15:43,840",207,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=939,processing of inverted index in general.
cs-410_2_6_208,cs-410,2,6, System Implementation - Fast Search,"00:15:43,840","00:15:48,400",208,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=943,We talked about how to construct the
cs-410_2_6_209,cs-410,2,6, System Implementation - Fast Search,"00:15:48,400","00:15:49,179",209,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=948,the memory.
cs-410_2_6_210,cs-410,2,6, System Implementation - Fast Search,"00:15:49,179","00:15:54,374",210,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=949,And then we talk about faster search using
cs-410_2_6_211,cs-410,2,6, System Implementation - Fast Search,"00:15:54,374","00:15:59,960",211,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=954,the invective index to accumulate a scores
cs-410_2_6_212,cs-410,2,6, System Implementation - Fast Search,"00:15:59,960","00:16:03,760",212,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=959,And we exploit the Zipf's law to
cs-410_2_6_213,cs-410,2,6, System Implementation - Fast Search,"00:16:03,760","00:16:06,052",213,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=963,that don't match any query term and
cs-410_2_6_214,cs-410,2,6, System Implementation - Fast Search,"00:16:06,052","00:16:11,540",214,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=966,this algorithm can actually support
cs-410_2_6_215,cs-410,2,6, System Implementation - Fast Search,"00:16:13,400","00:16:17,630",215,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=973,So these basic techniques
cs-410_2_6_216,cs-410,2,6, System Implementation - Fast Search,"00:16:17,630","00:16:23,570",216,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=977,further scaling up using distributed file
cs-410_2_6_217,cs-410,2,6, System Implementation - Fast Search,"00:16:23,570","00:16:28,410",217,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=983,Here are two additional readings you
cs-410_2_6_218,cs-410,2,6, System Implementation - Fast Search,"00:16:28,410","00:16:31,040",218,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=988,you are interested in
cs-410_2_6_219,cs-410,2,6, System Implementation - Fast Search,"00:16:31,040","00:16:38,156",219,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=991,The first one is a classical
cs-410_2_6_220,cs-410,2,6, System Implementation - Fast Search,"00:16:38,156","00:16:41,590",220,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=998,o inverted index and
cs-410_2_6_221,cs-410,2,6, System Implementation - Fast Search,"00:16:41,590","00:16:46,035",221,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=1001,"And how to,"
cs-410_2_6_222,cs-410,2,6, System Implementation - Fast Search,"00:16:46,035","00:16:49,811",222,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=1006,"any inputs of the space,"
cs-410_2_6_223,cs-410,2,6, System Implementation - Fast Search,"00:16:49,811","00:16:54,802",223,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=1009,The second one is a newer textbook that
cs-410_2_6_224,cs-410,2,6, System Implementation - Fast Search,"00:16:54,802","00:16:56,675",224,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=1014,evaluating search engines.
cs-410_2_6_225,cs-410,2,6, System Implementation - Fast Search,"00:16:58,835","00:17:08,835",225,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=1018,[MUSIC]
cs-410_3_1_1,cs-410,3,1, Evaluation of TR Systems,"00:00:00,000","00:00:03,655",1,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=0,[MUSIC]
cs-410_3_1_2,cs-410,3,1, Evaluation of TR Systems,"00:00:07,739","00:00:14,807",2,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=7,This lecture is about Evaluation of
cs-410_3_1_3,cs-410,3,1, Evaluation of TR Systems,"00:00:14,807","00:00:19,490",3,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=14,"lectures, we have talked about"
cs-410_3_1_4,cs-410,3,1, Evaluation of TR Systems,"00:00:19,490","00:00:22,120",4,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=19,different kinds of ranking functions.
cs-410_3_1_5,cs-410,3,1, Evaluation of TR Systems,"00:00:23,550","00:00:27,040",5,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=23,But how do we know which
cs-410_3_1_6,cs-410,3,1, Evaluation of TR Systems,"00:00:27,040","00:00:28,390",6,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=27,"In order to answer this question,"
cs-410_3_1_7,cs-410,3,1, Evaluation of TR Systems,"00:00:28,390","00:00:33,000",7,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=28,we have to compare them and that means we
cs-410_3_1_8,cs-410,3,1, Evaluation of TR Systems,"00:00:34,790","00:00:37,000",8,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=34,So this is the main topic of this lecture.
cs-410_3_1_9,cs-410,3,1, Evaluation of TR Systems,"00:00:40,462","00:00:42,730",9,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=40,"First, lets think about why"
cs-410_3_1_10,cs-410,3,1, Evaluation of TR Systems,"00:00:42,730","00:00:44,290",10,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=42,I already give one reason.
cs-410_3_1_11,cs-410,3,1, Evaluation of TR Systems,"00:00:44,290","00:00:48,770",11,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=44,"That is, we have to use evaluation"
cs-410_3_1_12,cs-410,3,1, Evaluation of TR Systems,"00:00:48,770","00:00:50,330",12,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=48,works better.
cs-410_3_1_13,cs-410,3,1, Evaluation of TR Systems,"00:00:50,330","00:00:54,310",13,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=50,Now this is very important for
cs-410_3_1_14,cs-410,3,1, Evaluation of TR Systems,"00:00:54,310","00:01:00,173",14,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=54,"Otherwise, we wouldn't know whether a new"
cs-410_3_1_15,cs-410,3,1, Evaluation of TR Systems,"00:01:00,173","00:01:04,710",15,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=60,"In the beginning of this course, we talked"
cs-410_3_1_16,cs-410,3,1, Evaluation of TR Systems,"00:01:04,710","00:01:07,200",16,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=64,We compare it with data base retrieval.
cs-410_3_1_17,cs-410,3,1, Evaluation of TR Systems,"00:01:08,440","00:01:14,390",17,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=68,There we mentioned that text retrieval
cs-410_3_1_18,cs-410,3,1, Evaluation of TR Systems,"00:01:14,390","00:01:18,240",18,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=74,So evaluation must rely on users.
cs-410_3_1_19,cs-410,3,1, Evaluation of TR Systems,"00:01:18,240","00:01:22,210",19,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=78,"Which system works better,"
cs-410_3_1_20,cs-410,3,1, Evaluation of TR Systems,"00:01:25,020","00:01:28,850",20,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=85,"So, this becomes a very"
cs-410_3_1_21,cs-410,3,1, Evaluation of TR Systems,"00:01:28,850","00:01:32,510",21,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=88,because how can we get users
cs-410_3_1_22,cs-410,3,1, Evaluation of TR Systems,"00:01:32,510","00:01:35,281",22,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=92,How can we do a fair comparison
cs-410_3_1_23,cs-410,3,1, Evaluation of TR Systems,"00:01:37,208","00:01:39,970",23,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=97,So just go back to the reasons for
cs-410_3_1_24,cs-410,3,1, Evaluation of TR Systems,"00:01:41,210","00:01:42,660",24,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=101,I listed two reasons here.
cs-410_3_1_25,cs-410,3,1, Evaluation of TR Systems,"00:01:42,660","00:01:47,060",25,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=102,"The second reason, is basically what I"
cs-410_3_1_26,cs-410,3,1, Evaluation of TR Systems,"00:01:47,060","00:01:51,910",26,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=107,reason which is to assess the actual
cs-410_3_1_27,cs-410,3,1, Evaluation of TR Systems,"00:01:51,910","00:01:55,200",27,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=111,Imagine you're building your
cs-410_3_1_28,cs-410,3,1, Evaluation of TR Systems,"00:01:55,200","00:02:01,660",28,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=115,it would be interesting knowing how well
cs-410_3_1_29,cs-410,3,1, Evaluation of TR Systems,"00:02:01,660","00:02:02,350",29,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=121,"So in this case,"
cs-410_3_1_30,cs-410,3,1, Evaluation of TR Systems,"00:02:02,350","00:02:07,850",30,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=122,matches must reflect the utility to
cs-410_3_1_31,cs-410,3,1, Evaluation of TR Systems,"00:02:07,850","00:02:11,840",31,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=127,"And typically, this has to be"
cs-410_3_1_32,cs-410,3,1, Evaluation of TR Systems,"00:02:11,840","00:02:13,960",32,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=131,using the real search engine.
cs-410_3_1_33,cs-410,3,1, Evaluation of TR Systems,"00:02:16,340","00:02:18,630",33,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=136,"In the second case, or the second reason,"
cs-410_3_1_34,cs-410,3,1, Evaluation of TR Systems,"00:02:19,970","00:02:26,130",34,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=139,the measures actually all need to collated
cs-410_3_1_35,cs-410,3,1, Evaluation of TR Systems,"00:02:26,130","00:02:30,120",35,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=146,"Thus, they don't have to accurately"
cs-410_3_1_36,cs-410,3,1, Evaluation of TR Systems,"00:02:31,980","00:02:37,680",36,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=151,So the measure only needs to be good
cs-410_3_1_37,cs-410,3,1, Evaluation of TR Systems,"00:02:38,860","00:02:41,780",37,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=158,And this is usually done
cs-410_3_1_38,cs-410,3,1, Evaluation of TR Systems,"00:02:41,780","00:02:48,110",38,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=161,And this is the main idea that we'll
cs-410_3_1_39,cs-410,3,1, Evaluation of TR Systems,"00:02:48,110","00:02:53,520",39,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=168,This has been very important for
cs-410_3_1_40,cs-410,3,1, Evaluation of TR Systems,"00:02:53,520","00:02:56,910",40,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=173,for improving search
cs-410_3_1_41,cs-410,3,1, Evaluation of TR Systems,"00:02:58,910","00:03:01,880",41,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=178,So let's talk about what to measure.
cs-410_3_1_42,cs-410,3,1, Evaluation of TR Systems,"00:03:01,880","00:03:06,750",42,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=181,There are many aspects of searching
cs-410_3_1_43,cs-410,3,1, Evaluation of TR Systems,"00:03:06,750","00:03:09,000",43,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=186,"And here,"
cs-410_3_1_44,cs-410,3,1, Evaluation of TR Systems,"00:03:09,000","00:03:11,190",44,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=189,"One, is effectiveness or accuracy."
cs-410_3_1_45,cs-410,3,1, Evaluation of TR Systems,"00:03:11,190","00:03:13,710",45,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=191,How accurate are the search results?
cs-410_3_1_46,cs-410,3,1, Evaluation of TR Systems,"00:03:13,710","00:03:18,110",46,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=193,"In this case, we're measuring a system's"
cs-410_3_1_47,cs-410,3,1, Evaluation of TR Systems,"00:03:18,110","00:03:20,150",47,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=198,on top of non relevant ones.
cs-410_3_1_48,cs-410,3,1, Evaluation of TR Systems,"00:03:20,150","00:03:21,850",48,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=200,"The second, is efficiency."
cs-410_3_1_49,cs-410,3,1, Evaluation of TR Systems,"00:03:21,850","00:03:24,470",49,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=201,How quickly can you get the results?
cs-410_3_1_50,cs-410,3,1, Evaluation of TR Systems,"00:03:24,470","00:03:27,710",50,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=204,How much computing resources
cs-410_3_1_51,cs-410,3,1, Evaluation of TR Systems,"00:03:27,710","00:03:31,150",51,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=207,"In this case, we need to measure the space"
cs-410_3_1_52,cs-410,3,1, Evaluation of TR Systems,"00:03:32,540","00:03:34,890",52,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=212,The third aspect is usability.
cs-410_3_1_53,cs-410,3,1, Evaluation of TR Systems,"00:03:34,890","00:03:38,950",53,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=214,"Basically the question is,"
cs-410_3_1_54,cs-410,3,1, Evaluation of TR Systems,"00:03:38,950","00:03:40,840",54,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=218,"Here, obviously, interfaces and"
cs-410_3_1_55,cs-410,3,1, Evaluation of TR Systems,"00:03:40,840","00:03:45,870",55,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=220,many other things also important and
cs-410_3_1_56,cs-410,3,1, Evaluation of TR Systems,"00:03:47,410","00:03:51,670",56,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=227,"Now in this course, we're going to"
cs-410_3_1_57,cs-410,3,1, Evaluation of TR Systems,"00:03:51,670","00:03:52,710",57,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=231,accuracy measures.
cs-410_3_1_58,cs-410,3,1, Evaluation of TR Systems,"00:03:52,710","00:03:55,340",58,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=232,Because the efficiency and
cs-410_3_1_59,cs-410,3,1, Evaluation of TR Systems,"00:03:55,340","00:04:00,230",59,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=235,usability dimensions are not
cs-410_3_1_60,cs-410,3,1, Evaluation of TR Systems,"00:04:00,230","00:04:08,640",60,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=240,And so they are needed for
cs-410_3_1_61,cs-410,3,1, Evaluation of TR Systems,"00:04:08,640","00:04:13,347",61,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=248,And there is also good coverage
cs-410_3_1_62,cs-410,3,1, Evaluation of TR Systems,"00:04:13,347","00:04:18,780",62,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=253,But how to evaluate search
cs-410_3_1_63,cs-410,3,1, Evaluation of TR Systems,"00:04:18,780","00:04:23,110",63,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=258,something unique to text retrieval and
cs-410_3_1_64,cs-410,3,1, Evaluation of TR Systems,"00:04:23,110","00:04:28,428",64,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=263,The main idea that people have proposed
cs-410_3_1_65,cs-410,3,1, Evaluation of TR Systems,"00:04:28,428","00:04:33,850",65,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=268,the text retrieval algorithm is called
cs-410_3_1_66,cs-410,3,1, Evaluation of TR Systems,"00:04:33,850","00:04:40,145",66,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=273,This one actually was developed
cs-410_3_1_67,cs-410,3,1, Evaluation of TR Systems,"00:04:40,145","00:04:44,785",67,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=280,It's a methodology for
cs-410_3_1_68,cs-410,3,1, Evaluation of TR Systems,"00:04:45,985","00:04:49,305",68,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=285,Its sampling methodology that has
cs-410_3_1_69,cs-410,3,1, Evaluation of TR Systems,"00:04:49,305","00:04:50,880",69,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=289,search engine evaluation.
cs-410_3_1_70,cs-410,3,1, Evaluation of TR Systems,"00:04:50,880","00:04:55,930",70,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=290,But also for evaluating virtually
cs-410_3_1_71,cs-410,3,1, Evaluation of TR Systems,"00:04:55,930","00:05:01,180",71,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=295,for example in natural language processing
cs-410_3_1_72,cs-410,3,1, Evaluation of TR Systems,"00:05:01,180","00:05:05,620",72,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=301,"is empirical to find, we typically"
cs-410_3_1_73,cs-410,3,1, Evaluation of TR Systems,"00:05:05,620","00:05:09,450",73,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=305,And today with the big data challenging
cs-410_3_1_74,cs-410,3,1, Evaluation of TR Systems,"00:05:09,450","00:05:13,590",74,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=309,with the use of machine
cs-410_3_1_75,cs-410,3,1, Evaluation of TR Systems,"00:05:13,590","00:05:17,430",75,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=313,"This methodology has been very popular,"
cs-410_3_1_76,cs-410,3,1, Evaluation of TR Systems,"00:05:17,430","00:05:20,100",76,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=317,a search engine application in the 1960s.
cs-410_3_1_77,cs-410,3,1, Evaluation of TR Systems,"00:05:20,100","00:05:25,250",77,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=320,So the basic idea of this approach is
cs-410_3_1_78,cs-410,3,1, Evaluation of TR Systems,"00:05:25,250","00:05:26,200",78,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=325,define measures.
cs-410_3_1_79,cs-410,3,1, Evaluation of TR Systems,"00:05:27,220","00:05:30,350",79,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=327,"Once such a test collection is built,"
cs-410_3_1_80,cs-410,3,1, Evaluation of TR Systems,"00:05:30,350","00:05:32,990",80,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=330,again to test different algorithms.
cs-410_3_1_81,cs-410,3,1, Evaluation of TR Systems,"00:05:32,990","00:05:36,180",81,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=332,And we're going to define measures
cs-410_3_1_82,cs-410,3,1, Evaluation of TR Systems,"00:05:36,180","00:05:39,660",82,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=336,performance of a system and algorithm.
cs-410_3_1_83,cs-410,3,1, Evaluation of TR Systems,"00:05:41,070","00:05:42,960",83,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=341,So how exactly will this work?
cs-410_3_1_84,cs-410,3,1, Evaluation of TR Systems,"00:05:42,960","00:05:47,090",84,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=342,Well we can do have a sample collection of
cs-410_3_1_85,cs-410,3,1, Evaluation of TR Systems,"00:05:47,090","00:05:49,986",85,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=347,the real document collection
cs-410_3_1_86,cs-410,3,1, Evaluation of TR Systems,"00:05:49,986","00:05:53,210",86,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=349,We're going to also have a sample
cs-410_3_1_87,cs-410,3,1, Evaluation of TR Systems,"00:05:53,210","00:05:55,240",87,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=353,This is a little simulator
cs-410_3_1_88,cs-410,3,1, Evaluation of TR Systems,"00:05:56,270","00:05:58,980",88,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=356,"Then, we'll have to have"
cs-410_3_1_89,cs-410,3,1, Evaluation of TR Systems,"00:05:58,980","00:06:03,930",89,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=358,These are judgments of which documents
cs-410_3_1_90,cs-410,3,1, Evaluation of TR Systems,"00:06:03,930","00:06:08,250",90,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=363,"Ideally, they have to be made by"
cs-410_3_1_91,cs-410,3,1, Evaluation of TR Systems,"00:06:08,250","00:06:12,930",91,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=368,Because those are the people that know
cs-410_3_1_92,cs-410,3,1, Evaluation of TR Systems,"00:06:12,930","00:06:14,690",92,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=372,"And finally, we have to have matches for"
cs-410_3_1_93,cs-410,3,1, Evaluation of TR Systems,"00:06:14,690","00:06:19,830",93,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=374,quantify how well our system's result
cs-410_3_1_94,cs-410,3,1, Evaluation of TR Systems,"00:06:19,830","00:06:24,560",94,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=379,That would be constructed base
cs-410_3_1_95,cs-410,3,1, Evaluation of TR Systems,"00:06:24,560","00:06:30,917",95,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=384,So this methodology is very useful for
cs-410_3_1_96,cs-410,3,1, Evaluation of TR Systems,"00:06:30,917","00:06:36,130",96,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=390,because the test can be reused many times.
cs-410_3_1_97,cs-410,3,1, Evaluation of TR Systems,"00:06:36,130","00:06:41,340",97,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=396,And it will also provide a fair
cs-410_3_1_98,cs-410,3,1, Evaluation of TR Systems,"00:06:41,340","00:06:43,370",98,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=401,We have the same criteria or
cs-410_3_1_99,cs-410,3,1, Evaluation of TR Systems,"00:06:43,370","00:06:47,570",99,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=403,same dataset to be used to
cs-410_3_1_100,cs-410,3,1, Evaluation of TR Systems,"00:06:47,570","00:06:50,810",100,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=407,This allows us to compare
cs-410_3_1_101,cs-410,3,1, Evaluation of TR Systems,"00:06:50,810","00:06:55,660",101,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=410,an old algorithm that was divided many
cs-410_3_1_102,cs-410,3,1, Evaluation of TR Systems,"00:06:55,660","00:06:59,580",102,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=415,"So this is the illustration of this works,"
cs-410_3_1_103,cs-410,3,1, Evaluation of TR Systems,"00:06:59,580","00:07:03,930",103,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=419,we need our queries that are showing here.
cs-410_3_1_104,cs-410,3,1, Evaluation of TR Systems,"00:07:03,930","00:07:05,180",104,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=423,"We have Q1, Q2 etc."
cs-410_3_1_105,cs-410,3,1, Evaluation of TR Systems,"00:07:05,180","00:07:08,300",105,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=425,We also need the documents and
cs-410_3_1_106,cs-410,3,1, Evaluation of TR Systems,"00:07:08,300","00:07:10,580",106,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=428,on the right side you will see
cs-410_3_1_107,cs-410,3,1, Evaluation of TR Systems,"00:07:10,580","00:07:19,150",107,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=430,These are basically the binary judgments
cs-410_3_1_108,cs-410,3,1, Evaluation of TR Systems,"00:07:19,150","00:07:23,790",108,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=439,"So for example,"
cs-410_3_1_109,cs-410,3,1, Evaluation of TR Systems,"00:07:23,790","00:07:27,920",109,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=443,"D2 is judged as being relevant as well,"
cs-410_3_1_110,cs-410,3,1, Evaluation of TR Systems,"00:07:27,920","00:07:28,980",110,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=447,And the Q1 etc.
cs-410_3_1_111,cs-410,3,1, Evaluation of TR Systems,"00:07:28,980","00:07:32,490",111,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=448,These will be created by users.
cs-410_3_1_112,cs-410,3,1, Evaluation of TR Systems,"00:07:34,190","00:07:38,460",112,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=454,"Once we have these, and"
cs-410_3_1_113,cs-410,3,1, Evaluation of TR Systems,"00:07:38,460","00:07:43,560",113,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=458,"And then if you have two systems,"
cs-410_3_1_114,cs-410,3,1, Evaluation of TR Systems,"00:07:43,560","00:07:47,260",114,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=463,then you can just run each
cs-410_3_1_115,cs-410,3,1, Evaluation of TR Systems,"00:07:47,260","00:07:50,580",115,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=467,the documents and
cs-410_3_1_116,cs-410,3,1, Evaluation of TR Systems,"00:07:50,580","00:07:56,347",116,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=470,Let's say if the queries Q1 and
cs-410_3_1_117,cs-410,3,1, Evaluation of TR Systems,"00:07:56,347","00:08:02,350",117,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=476,Here I show R sub A as
cs-410_3_1_118,cs-410,3,1, Evaluation of TR Systems,"00:08:02,350","00:08:05,170",118,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=482,"So this is, remember we talked about"
cs-410_3_1_119,cs-410,3,1, Evaluation of TR Systems,"00:08:05,170","00:08:09,750",119,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=485,task of computing approximation
cs-410_3_1_120,cs-410,3,1, Evaluation of TR Systems,"00:08:09,750","00:08:13,910",120,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=489,R sub A is system A's approximation here.
cs-410_3_1_121,cs-410,3,1, Evaluation of TR Systems,"00:08:14,980","00:08:20,000",121,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=494,And R sub B is system B's
cs-410_3_1_122,cs-410,3,1, Evaluation of TR Systems,"00:08:21,100","00:08:22,810",122,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=501,"Now, let's take a look at these results."
cs-410_3_1_123,cs-410,3,1, Evaluation of TR Systems,"00:08:22,810","00:08:24,190",123,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=502,So which is better?
cs-410_3_1_124,cs-410,3,1, Evaluation of TR Systems,"00:08:24,190","00:08:26,810",124,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=504,"Now imagine if a user,"
cs-410_3_1_125,cs-410,3,1, Evaluation of TR Systems,"00:08:26,810","00:08:31,145",125,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=506,Now let's take a look at the both results.
cs-410_3_1_126,cs-410,3,1, Evaluation of TR Systems,"00:08:31,145","00:08:33,270",126,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=511,And there are some differences and
cs-410_3_1_127,cs-410,3,1, Evaluation of TR Systems,"00:08:33,270","00:08:40,160",127,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=513,there are some documents that
cs-410_3_1_128,cs-410,3,1, Evaluation of TR Systems,"00:08:40,160","00:08:44,200",128,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=520,"But if you look at the results,"
cs-410_3_1_129,cs-410,3,1, Evaluation of TR Systems,"00:08:44,200","00:08:48,640",129,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=524,A is better in the sense that we don't
cs-410_3_1_130,cs-410,3,1, Evaluation of TR Systems,"00:08:48,640","00:08:52,260",130,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=528,"And among the three documents returned,"
cs-410_3_1_131,cs-410,3,1, Evaluation of TR Systems,"00:08:52,260","00:08:55,430",131,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=532,"So that's good, it's precise."
cs-410_3_1_132,cs-410,3,1, Evaluation of TR Systems,"00:08:55,430","00:08:58,770",132,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=535,On the other hand one council
cs-410_3_1_133,cs-410,3,1, Evaluation of TR Systems,"00:08:58,770","00:09:01,280",133,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=538,because we've got all of
cs-410_3_1_134,cs-410,3,1, Evaluation of TR Systems,"00:09:01,280","00:09:03,500",134,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=541,We've got three instead of two.
cs-410_3_1_135,cs-410,3,1, Evaluation of TR Systems,"00:09:03,500","00:09:06,690",135,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=543,So which one is better and
cs-410_3_1_136,cs-410,3,1, Evaluation of TR Systems,"00:09:08,820","00:09:12,670",136,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=548,"Well, obviously this question"
cs-410_3_1_137,cs-410,3,1, Evaluation of TR Systems,"00:09:12,670","00:09:14,820",137,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=552,It depends on users as well.
cs-410_3_1_138,cs-410,3,1, Evaluation of TR Systems,"00:09:14,820","00:09:19,950",138,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=554,You might even imagine for
cs-410_3_1_139,cs-410,3,1, Evaluation of TR Systems,"00:09:19,950","00:09:23,747",139,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=559,If the user is not interested in
cs-410_3_1_140,cs-410,3,1, Evaluation of TR Systems,"00:09:23,747","00:09:28,582",140,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=563,"Right, in this case the user doesn't"
cs-410_3_1_141,cs-410,3,1, Evaluation of TR Systems,"00:09:28,582","00:09:31,020",141,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=568,see most of the relevant documents.
cs-410_3_1_142,cs-410,3,1, Evaluation of TR Systems,"00:09:31,020","00:09:34,230",142,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=571,"On the other hand,"
cs-410_3_1_143,cs-410,3,1, Evaluation of TR Systems,"00:09:34,230","00:09:37,130",143,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=574,to have as many random
cs-410_3_1_144,cs-410,3,1, Evaluation of TR Systems,"00:09:37,130","00:09:41,617",144,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=577,"For example, if you're doing a literature"
cs-410_3_1_145,cs-410,3,1, Evaluation of TR Systems,"00:09:41,617","00:09:43,844",145,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=581,and you might find that
cs-410_3_1_146,cs-410,3,1, Evaluation of TR Systems,"00:09:43,844","00:09:48,985",146,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=583,"So in the case, we will have to also"
cs-410_3_1_147,cs-410,3,1, Evaluation of TR Systems,"00:09:48,985","00:09:53,408",147,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=588,And we might need it to define multiple
cs-410_3_1_148,cs-410,3,1, Evaluation of TR Systems,"00:09:53,408","00:09:55,798",148,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=593,perspectives of looking at the results.
cs-410_3_1_149,cs-410,3,1, Evaluation of TR Systems,"00:09:58,259","00:10:08,259",149,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=598,[MUSIC]
cs-410_3_2_1,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:00,012","00:00:06,908",1,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=0,[SOUND]
cs-410_3_2_2,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:06,908","00:00:13,498",2,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=6,lecture is about the basic measures for
cs-410_3_2_3,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:13,498","00:00:18,955",3,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=13,"In this lecture,"
cs-410_3_2_4,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:18,955","00:00:24,528",4,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=18,measures to quantitatively
cs-410_3_2_5,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:24,528","00:00:29,163",5,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=24,This is a slide that you have seen
cs-410_3_2_6,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:29,163","00:00:32,318",6,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=29,about the Granville
cs-410_3_2_7,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:32,318","00:00:39,122",7,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=32,We can have a test faction that consists
cs-410_3_2_8,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:39,122","00:00:47,930",8,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=39,We can then run two systems on these
cs-410_3_2_9,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:47,930","00:00:49,528",9,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=47,Their performance.
cs-410_3_2_10,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:49,528","00:00:54,828",10,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=49,"And we raise the question,"
cs-410_3_2_11,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:54,828","00:00:57,928",11,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=54,Is system A better or is system B better?
cs-410_3_2_12,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:57,928","00:01:02,398",12,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=57,So let's now talk about how to
cs-410_3_2_13,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:02,398","00:01:07,841",13,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=62,Suppose we have a total of 10 relevant
cs-410_3_2_14,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:07,841","00:01:08,848",14,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=67,this query.
cs-410_3_2_15,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:08,848","00:01:15,162",15,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=68,"Now, the relevant judgments show on"
cs-410_3_2_16,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:15,162","00:01:19,908",16,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=75,"And we have only seen 3 [INAUDIBLE] there,"
cs-410_3_2_17,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:19,908","00:01:26,133",17,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=79,"But, we can imagine there are other Random"
cs-410_3_2_18,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:26,133","00:01:30,895",18,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=86,"So now, intuitively,"
cs-410_3_2_19,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:30,895","00:01:35,668",19,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=90,A is better because it
cs-410_3_2_20,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:35,668","00:01:42,019",20,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=95,And in particular we have seen
cs-410_3_2_21,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:42,019","00:01:46,251",21,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=102,"two of them are relevant but in system B,"
cs-410_3_2_22,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:46,251","00:01:52,248",22,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=106,we have five results and
cs-410_3_2_23,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:52,248","00:01:56,418",23,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=112,So intuitively it looks like
cs-410_3_2_24,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:56,418","00:02:00,670",24,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=116,And this infusion can be captured
cs-410_3_2_25,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:00,670","00:02:05,866",25,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=120,where we simply compute to what extent
cs-410_3_2_26,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:05,866","00:02:07,788",26,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=125,"If you have 100% position,"
cs-410_3_2_27,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:07,788","00:02:11,638",27,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=127,that would mean that all
cs-410_3_2_28,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:11,638","00:02:16,476",28,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=131,So in this case system A has
cs-410_3_2_29,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:16,476","00:02:20,606",29,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=136,three System B has some
cs-410_3_2_30,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:20,606","00:02:25,208",30,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=140,this shows that system
cs-410_3_2_31,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:25,208","00:02:30,065",31,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=145,But we also talked about System B
cs-410_3_2_32,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:30,065","00:02:35,220",32,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=150,would like to retrieve as many
cs-410_3_2_33,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:35,220","00:02:39,839",33,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=155,So in that case we'll have to compare
cs-410_3_2_34,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:39,839","00:02:42,940",34,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=159,retrieve and
cs-410_3_2_35,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:42,940","00:02:48,000",35,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=162,This method uses the completeness
cs-410_3_2_36,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:48,000","00:02:51,090",36,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=168,In your retrieval result.
cs-410_3_2_37,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:51,090","00:02:57,500",37,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=171,So we just assume that there are ten
cs-410_3_2_38,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:57,500","00:03:01,510",38,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=177,"And here we've got two of them,"
cs-410_3_2_39,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:01,510","00:03:04,130",39,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=181,So the recall is 2 out of 10.
cs-410_3_2_40,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:04,130","00:03:07,630",40,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=184,"Whereas System B has called a 3,"
cs-410_3_2_41,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:07,630","00:03:11,278",41,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=187,Now we can see by recall
cs-410_3_2_42,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:11,278","00:03:15,240",42,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=191,And these two measures turn out to
cs-410_3_2_43,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:15,240","00:03:16,978",43,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=195,evaluating search engine.
cs-410_3_2_44,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:16,978","00:03:21,824",44,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=196,And they are very important because
cs-410_3_2_45,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:21,824","00:03:24,298",45,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=201,other test evaluation problems.
cs-410_3_2_46,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:24,298","00:03:28,660",46,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=204,"For example, if you look at"
cs-410_3_2_47,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:28,660","00:03:34,030",47,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=208,you tend to see precision recall numbers
cs-410_3_2_48,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:35,290","00:03:38,520",48,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=215,"Okay so, now let's define these"
cs-410_3_2_49,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:38,520","00:03:44,410",49,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=218,And these measures are to evaluate a set
cs-410_3_2_50,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:44,410","00:03:48,950",50,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=224,we are considering that approximation
cs-410_3_2_51,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:50,100","00:03:53,300",51,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=230,We can distinguish 4 cases depending
cs-410_3_2_52,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:53,300","00:03:59,720",52,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=233,A document can be retrieved or
cs-410_3_2_53,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:59,720","00:04:01,570",53,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=239,Because we are talking
cs-410_3_2_54,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:02,710","00:04:05,640",54,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=242,A document can be also relevant or
cs-410_3_2_55,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:05,640","00:04:10,310",55,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=245,not relevant depending on whether the user
cs-410_3_2_56,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:11,950","00:04:16,890",56,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=251,So we can now have counts of documents in.
cs-410_3_2_57,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:16,890","00:04:21,610",57,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=256,Each of the four categories again
cs-410_3_2_58,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:21,610","00:04:24,420",58,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=261,documents that have been retrieved and
cs-410_3_2_59,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:24,420","00:04:30,530",59,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=264,B for documents that are not retrieved but
cs-410_3_2_60,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:31,750","00:04:35,550",60,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=271,No with this table then
cs-410_3_2_61,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:36,690","00:04:42,450",61,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=276,As the ratio of the relevant
cs-410_3_2_62,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:42,450","00:04:47,440",62,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=282,retrieved documents A to the total
cs-410_3_2_63,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:48,450","00:04:53,390",63,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=288,"So, this is just A divided"
cs-410_3_2_64,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:53,390","00:04:55,640",64,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=293,The sum of this column.
cs-410_3_2_65,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:56,820","00:05:04,360",65,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=296,Singularly recall is defined by
cs-410_3_2_66,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:04,360","00:05:07,470",66,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=304,So that's again to divide a by.
cs-410_3_2_67,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:07,470","00:05:10,360",67,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=307,The sum of the row instead of the column.
cs-410_3_2_68,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:10,360","00:05:15,810",68,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=310,"All right, so we can see precision and"
cs-410_3_2_69,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:16,930","00:05:20,000",69,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=316,that's the number of
cs-410_3_2_70,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:20,000","00:05:22,449",70,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=320,But we're going to use
cs-410_3_2_71,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:23,590","00:05:27,300",71,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=323,"Okay, so what would be an ideal result."
cs-410_3_2_72,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:27,300","00:05:31,330",72,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=327,"Well, you can easily see being"
cs-410_3_2_73,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:31,330","00:05:34,060",73,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=331,recall oil to be 1.0.
cs-410_3_2_74,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:34,060","00:05:39,510",74,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=334,That means We have got 1% of
cs-410_3_2_75,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:39,510","00:05:44,770",75,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=339,"in our results, and all of the results"
cs-410_3_2_76,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:44,770","00:05:47,540",76,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=344,At least there's no single
cs-410_3_2_77,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:48,680","00:05:53,920",77,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=348,"In reality, however, high recall tends"
cs-410_3_2_78,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:53,920","00:05:56,210",78,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=353,And you can imagine why that's the case.
cs-410_3_2_79,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:56,210","00:06:00,790",79,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=356,As you go down the to try to get as
cs-410_3_2_80,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:00,790","00:06:05,890",80,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=360,"you tend to encounter a lot of documents,"
cs-410_3_2_81,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:05,890","00:06:11,450",81,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=365,Note that this set can also
cs-410_3_2_82,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:11,450","00:06:15,490",82,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=371,"In the rest of this, that's why although"
cs-410_3_2_83,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:15,490","00:06:20,560",83,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=375,"retrieve the documents, they are actually"
cs-410_3_2_84,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:20,560","00:06:24,270",84,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=380,They are the fundamental measures in
cs-410_3_2_85,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:24,270","00:06:30,010",85,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=384,We often are interested in The precision
cs-410_3_2_86,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:30,010","00:06:33,400",86,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=390,This means we look at how many documents
cs-410_3_2_87,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:33,400","00:06:35,870",87,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=393,among the top ten results
cs-410_3_2_88,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:35,870","00:06:38,290",88,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=395,"Now, this is a very meaningful measure,"
cs-410_3_2_89,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:38,290","00:06:43,780",89,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=398,because it tells us how many relevant
cs-410_3_2_90,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:43,780","00:06:47,790",90,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=403,On the first page of where they
cs-410_3_2_91,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:50,000","00:06:55,780",91,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=410,So precision and recall
cs-410_3_2_92,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:55,780","00:07:02,040",92,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=415,use them to further evaluate a search
cs-410_3_2_93,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:03,460","00:07:07,210",93,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=423,We just said that there tends to be
cs-410_3_2_94,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:07,210","00:07:10,730",94,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=427,so naturally it would be
cs-410_3_2_95,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:10,730","00:07:15,490",95,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=430,"And here's one method that's often used,"
cs-410_3_2_96,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:15,490","00:07:21,020",96,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=435,it's a [INAUDIBLE] mean of precision and
cs-410_3_2_97,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:22,450","00:07:27,741",97,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=442,"So, you can see at first, compute the."
cs-410_3_2_98,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:29,210","00:07:34,360",98,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=449,"Inverse of R and P here,"
cs-410_3_2_99,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:34,360","00:07:41,180",99,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=454,the 2 by using coefficients
cs-410_3_2_100,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:42,850","00:07:47,029",100,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=462,And after some transformation you can
cs-410_3_2_101,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:49,010","00:07:51,790",101,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=469,And in any case it just becomes
cs-410_3_2_102,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:51,790","00:07:56,360",102,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=471,"recall, and beta is a parameter,"
cs-410_3_2_103,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:56,360","00:08:01,572",103,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=476,It can control the emphasis
cs-410_3_2_104,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:01,572","00:08:08,360",104,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=481,set beta to 1 We end up having a special
cs-410_3_2_105,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:08,360","00:08:13,460",105,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=488,This is a popular measure that's often
cs-410_3_2_106,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:13,460","00:08:14,780",106,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=493,And the formula looks very simple.
cs-410_3_2_107,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:16,170","00:08:17,948",107,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=496,"It's just this, here."
cs-410_3_2_108,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:20,718","00:08:24,668",108,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=500,Now it's easy to see that if
cs-410_3_2_109,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:24,668","00:08:28,570",109,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=504,larger recall than f
cs-410_3_2_110,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:28,570","00:08:32,940",110,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=508,"But, what's interesting is that"
cs-410_3_2_111,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:32,940","00:08:36,260",111,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=512,recall is captured
cs-410_3_2_112,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:36,260","00:08:41,000",112,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=516,"So, in order to understand that, we"
cs-410_3_2_113,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:42,170","00:08:48,270",113,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=522,can first look at the natural
cs-410_3_2_114,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:48,270","00:08:53,090",114,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=528,using the symbol arithmetically
cs-410_3_2_115,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:53,090","00:09:00,730",115,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=533,That would be likely the most natural way
cs-410_3_2_116,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:01,870","00:09:05,940",116,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=541,"If you want to think more,"
cs-410_3_2_117,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:07,940","00:09:10,960",117,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=547,So why is this not as good as F1?
cs-410_3_2_118,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:13,550","00:09:15,038",118,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=553,Or what's the problem with this?
cs-410_3_2_119,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:18,121","00:09:23,270",119,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=558,"Now, if you think about"
cs-410_3_2_120,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:23,270","00:09:28,300",120,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=563,you can see this is
cs-410_3_2_121,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:28,300","00:09:31,870",121,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=568,"In this case,"
cs-410_3_2_122,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:31,870","00:09:36,580",122,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=571,"In the case of a sum, the total value"
cs-410_3_2_123,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:36,580","00:09:42,850",123,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=576,that means if you have a very high P or
cs-410_3_2_124,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:42,850","00:09:47,820",124,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=582,don't care about whether the other value
cs-410_3_2_125,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:47,820","00:09:53,920",125,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=587,Now this is not desirable because one
cs-410_3_2_126,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:53,920","00:09:57,110",126,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=593,We have perfect recall easily.
cs-410_3_2_127,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:57,110","00:09:58,140",127,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=597,Can we imagine how?
cs-410_3_2_128,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:59,810","00:10:03,830",128,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=599,It's probably very easy to
cs-410_3_2_129,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:03,830","00:10:06,399",129,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=603,all the documents in the collection and
cs-410_3_2_130,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:07,420","00:10:11,130",130,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=607,And this will give us 0.5 as the average.
cs-410_3_2_131,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:11,130","00:10:15,583",131,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=611,But such results are clearly not
cs-410_3_2_132,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:15,583","00:10:20,350",132,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=615,though the average using this
cs-410_3_2_133,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:21,750","00:10:25,930",133,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=621,In contrast you can see F 1 would
cs-410_3_2_134,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:25,930","00:10:27,750",134,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=625,"recall are roughly That seminar, so"
cs-410_3_2_135,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:27,750","00:10:33,360",135,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=627,it would a case where you had
cs-410_3_2_136,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:35,320","00:10:38,360",136,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=635,So this means f one encodes
cs-410_3_2_137,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:38,360","00:10:43,690",137,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=638,Now this example shows
cs-410_3_2_138,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:43,690","00:10:44,230",138,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=643,Methodology here.
cs-410_3_2_139,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:44,230","00:10:49,950",139,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=644,But when you try to solve a problem you
cs-410_3_2_140,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:49,950","00:10:52,120",140,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=649,let's say in this it's
cs-410_3_2_141,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:53,790","00:10:57,160",141,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=653,But it's important not to
cs-410_3_2_142,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:57,160","00:11:00,790",142,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=657,It's important to think whether you
cs-410_3_2_143,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:02,170","00:11:06,180",143,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=662,And once you think about the multiple
cs-410_3_2_144,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:06,180","00:11:10,930",144,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=666,"difference, and then think about"
cs-410_3_2_145,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:10,930","00:11:13,280",145,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=670,"In this case, if you think more carefully,"
cs-410_3_2_146,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:13,280","00:11:15,920",146,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=673,you will think that F1
cs-410_3_2_147,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:15,920","00:11:18,300",147,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=675,Than the simple.
cs-410_3_2_148,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:18,300","00:11:21,670",148,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=678,Although in other cases there
cs-410_3_2_149,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:21,670","00:11:25,858",149,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=681,But in this case the seems not reasonable.
cs-410_3_2_150,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:25,858","00:11:29,260",150,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=685,But if you don't pay attention
cs-410_3_2_151,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:29,260","00:11:33,780",151,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=689,you might just take a easy way to
cs-410_3_2_152,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:33,780","00:11:37,360",152,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=693,"And here later, you will find that,"
cs-410_3_2_153,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:37,360","00:11:38,620",153,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=697,All right.
cs-410_3_2_154,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:38,620","00:11:43,760",154,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=698,So this methodology is actually very
cs-410_3_2_155,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:43,760","00:11:46,020",155,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=703,Try to think about the best solution.
cs-410_3_2_156,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:46,020","00:11:50,890",156,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=706,"Try to understand the problem very well,"
cs-410_3_2_157,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:50,890","00:11:55,890",157,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=710,"know why you needed this measure, and why"
cs-410_3_2_158,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:55,890","00:11:59,530",158,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=715,And then use that to guide you in
cs-410_3_2_159,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:03,320","00:12:08,510",159,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=723,"To summarize, we talked about"
cs-410_3_2_160,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:08,510","00:12:11,530",160,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=728,are there retrievable
cs-410_3_2_161,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:11,530","00:12:13,690",161,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=731,We also talk about the Recall.
cs-410_3_2_162,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:13,690","00:12:17,260",162,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=733,"Which addresses the question, have all of"
cs-410_3_2_163,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:17,260","00:12:21,250",163,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=737,"These two, are the two,"
cs-410_3_2_164,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:21,250","00:12:25,270",164,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=741,They are used for
cs-410_3_2_165,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:25,270","00:12:28,670",165,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=745,We talk about F measure as a way to
cs-410_3_2_166,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:29,970","00:12:33,600",166,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=749,We also talked about the tradeoff
cs-410_3_2_167,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:33,600","00:12:38,140",167,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=753,And this turns out to depend
cs-410_3_2_168,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:38,140","00:12:42,133",168,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=758,we'll discuss this point
cs-410_3_2_169,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:42,133","00:12:52,133",169,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=762,[MUSIC]
cs-410_3_3_1,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:00,199","00:00:03,699",1,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=0,[MUSIC]
cs-410_3_3_2,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:07,099","00:00:11,070",2,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=7,"This lecture is about,"
cs-410_3_3_3,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:13,290","00:00:17,810",3,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=13,"In this lecture, we will continue"
cs-410_3_3_4,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:17,810","00:00:18,470",4,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=17,"In particular,"
cs-410_3_3_5,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:18,470","00:00:21,799",5,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=18,"we are going to look at, how we can"
cs-410_3_3_6,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:24,970","00:00:30,410",6,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=24,"In the previous lecture,"
cs-410_3_3_7,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:30,410","00:00:33,430",7,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=30,"These are the two basic measures for,"
cs-410_3_3_8,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:33,430","00:00:38,410",8,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=33,quantitatively measuring
cs-410_3_3_9,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:40,420","00:00:44,247",9,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=40,"But, as we talked about, ranking, before,"
cs-410_3_3_10,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:44,247","00:00:49,420",10,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=44,we framed that the text of retrieval
cs-410_3_3_11,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:50,800","00:00:55,820",11,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=50,"So, we also need to evaluate the,"
cs-410_3_3_12,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:56,910","00:01:01,097",12,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=56,How can we use precision-recall
cs-410_3_3_13,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:01,097","00:01:07,180",13,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=61,"Well, naturally, we have to look after the"
cs-410_3_3_14,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:07,180","00:01:12,330",14,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=67,"Because in the end, the approximation"
cs-410_3_3_15,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:12,330","00:01:17,640",15,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=72,"given by a ranked list, is determined"
cs-410_3_3_16,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:17,640","00:01:21,470",16,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=77,Right?
cs-410_3_3_17,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:21,470","00:01:25,520",17,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=81,"the list of results, the user would,"
cs-410_3_3_18,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:25,520","00:01:27,790",18,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=85,that point would determine the set.
cs-410_3_3_19,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:27,790","00:01:31,680",19,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=87,"And then,"
cs-410_3_3_20,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:31,680","00:01:35,400",20,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=91,"that we have to consider,"
cs-410_3_3_21,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:35,400","00:01:37,990",21,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=95,Without knowing where
cs-410_3_3_22,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:37,990","00:01:42,380",22,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=97,"then we have to consider, all"
cs-410_3_3_23,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:42,380","00:01:44,720",23,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=102,"So, let's look at these positions."
cs-410_3_3_24,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:44,720","00:01:49,020",24,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=104,"Look at this slide, and"
cs-410_3_3_25,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:49,020","00:01:51,718",25,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=109,"what if the user stops at the,"
cs-410_3_3_26,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:51,718","00:01:55,140",26,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=111,What's the precision-recall at this point?
cs-410_3_3_27,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:55,140","00:01:55,800",27,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=115,What do you think?
cs-410_3_3_28,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:56,970","00:02:02,920",28,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=116,"Well, it's easy to see, that this document"
cs-410_3_3_29,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:02,920","00:02:05,960",29,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=122,"We have, got one document,"
cs-410_3_3_30,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:05,960","00:02:07,380",30,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=125,What about the recall?
cs-410_3_3_31,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:07,380","00:02:11,990",31,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=127,"Well, note that, we're assuming that,"
cs-410_3_3_32,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:11,990","00:02:14,980",32,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=131,"this query in the collection,"
cs-410_3_3_33,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:16,310","00:02:18,650",33,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=136,What if the user stops
cs-410_3_3_34,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:19,820","00:02:20,320",34,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=139,Top two.
cs-410_3_3_35,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:21,470","00:02:25,820",35,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=141,"Well, the precision is the same,"
cs-410_3_3_36,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:25,820","00:02:27,060",36,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=145,"And, the record is two out of ten."
cs-410_3_3_37,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:28,600","00:02:31,630",37,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=148,What if the user stops
cs-410_3_3_38,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:31,630","00:02:35,980",38,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=151,"Well, this is interesting,"
cs-410_3_3_39,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:35,980","00:02:40,030",39,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=155,"additional relevant document,"
cs-410_3_3_40,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:41,170","00:02:45,600",40,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=161,"But the precision is lower,"
cs-410_3_3_41,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:45,600","00:02:46,680",41,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=165,what's exactly the precision?
cs-410_3_3_42,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:49,110","00:02:52,020",42,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=169,"Well, it's two out of three, right?"
cs-410_3_3_43,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:52,020","00:02:54,920",43,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=172,"And, recall is the same, two out of ten."
cs-410_3_3_44,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:54,920","00:02:58,930",44,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=174,"So, when would see another point,"
cs-410_3_3_45,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:58,930","00:03:02,473",45,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=178,"Now, if you look down the list,"
cs-410_3_3_46,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:02,473","00:03:06,110",46,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=182,"we have, seeing another relevant document."
cs-410_3_3_47,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:06,110","00:03:10,800",47,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=186,"In this case D5, at that point, the,"
cs-410_3_3_48,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:10,800","00:03:13,840",48,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=190,"three out of ten, and,"
cs-410_3_3_49,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:15,150","00:03:20,200",49,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=195,"So, you can see, if we keep doing this,"
cs-410_3_3_50,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:20,200","00:03:23,780",50,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=200,"And then, we will have"
cs-410_3_3_51,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:23,780","00:03:26,150",51,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=203,"because there are eight documents,"
cs-410_3_3_52,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:26,150","00:03:28,200",52,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=206,"And, the recall is a four out of ten."
cs-410_3_3_53,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:29,540","00:03:33,500",53,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=209,"Now, when can we get,"
cs-410_3_3_54,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:33,500","00:03:39,740",54,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=213,"Well, in this list, we don't have it,"
cs-410_3_3_55,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:39,740","00:03:40,560",55,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=219,"We don't know, where it is?"
cs-410_3_3_56,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:40,560","00:03:45,700",56,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=220,"But, as convenience, we often assume that,"
cs-410_3_3_57,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:47,230","00:03:51,890",57,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=227,"at all the, the othe,"
cs-410_3_3_58,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:51,890","00:03:56,550",58,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=231,"all the other levels of recall,"
cs-410_3_3_59,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:56,550","00:03:59,140",59,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=236,"So, of course,"
cs-410_3_3_60,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:59,140","00:04:04,040",60,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=239,"the actual position would be higher,"
cs-410_3_3_61,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:05,230","00:04:09,390",61,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=245,"in order to, have an easy way to,"
cs-410_3_3_62,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:09,390","00:04:12,980",62,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=249,compute another measure called Average
cs-410_3_3_63,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:14,300","00:04:16,560",63,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=254,"Now, I should also say, now, here you see,"
cs-410_3_3_64,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:16,560","00:04:21,230",64,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=256,we make these assumptions that
cs-410_3_3_65,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:22,270","00:04:28,950",65,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=262,"But, this is okay, for"
cs-410_3_3_66,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:28,950","00:04:34,870",66,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=268,"And, this is for the relative comparison,"
cs-410_3_3_67,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:34,870","00:04:39,560",67,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=274,"or actual, actual number deviates"
cs-410_3_3_68,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:39,560","00:04:41,970",68,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=279,"As long as the deviation,"
cs-410_3_3_69,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:41,970","00:04:46,810",69,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=281,is not biased toward any particular
cs-410_3_3_70,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:46,810","00:04:50,560",70,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=286,"We can still,"
cs-410_3_3_71,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:50,560","00:04:53,360",71,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=290,"And, this is important point,"
cs-410_3_3_72,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:53,360","00:04:55,550",72,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=293,"When you compare different algorithms,"
cs-410_3_3_73,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:55,550","00:04:58,810",73,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=295,the key's to avoid any
cs-410_3_3_74,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:58,810","00:05:02,130",74,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=298,"And, as long as, you can avoid that."
cs-410_3_3_75,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:02,130","00:05:06,580",75,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=302,"It's okay, for you to do transformation"
cs-410_3_3_76,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:06,580","00:05:07,640",76,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=306,you can preserve the order.
cs-410_3_3_77,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:09,380","00:05:11,170",77,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=309,"Okay, so, we'll just talk about,"
cs-410_3_3_78,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:11,170","00:05:16,030",78,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=311,we can get a lot of precision-recall
cs-410_3_3_79,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:16,030","00:05:19,000",79,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=316,"So, now, you can imagine,"
cs-410_3_3_80,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:19,000","00:05:22,389",80,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=319,"And, this just shows on the,"
cs-410_3_3_81,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:23,610","00:05:30,110",81,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=323,"And, on the y-axis, we show the precision."
cs-410_3_3_82,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:30,110","00:05:35,336",82,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=330,"So, the precision line was marked as .1,"
cs-410_3_3_83,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:35,336","00:05:35,998",83,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=335,Right?
cs-410_3_3_84,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:35,998","00:05:38,618",84,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=335,"this is, the different, levels of recall."
cs-410_3_3_85,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:38,618","00:05:44,390",85,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=338,"And,, the y-axis also has,"
cs-410_3_3_86,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:45,450","00:05:49,150",86,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=345,"So, we plot the, these, precision-recall"
cs-410_3_3_87,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:49,150","00:05:51,360",87,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=349,as points on this picture.
cs-410_3_3_88,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:51,360","00:05:56,410",88,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=351,"Now, we can further, and"
cs-410_3_3_89,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:56,410","00:05:57,290",89,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=356,"As you'll see,"
cs-410_3_3_90,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:57,290","00:06:02,040",90,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=357,"we assumed all the other, precision"
cs-410_3_3_91,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:02,040","00:06:08,232",91,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=362,"And, that's why, they are down here,"
cs-410_3_3_92,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:08,232","00:06:14,980",92,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=368,"And this, the actual curve probably will"
cs-410_3_3_93,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:14,980","00:06:19,410",93,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=374,"discussed, it, it doesn't matter that"
cs-410_3_3_94,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:20,430","00:06:24,600",94,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=380,"because this would be,"
cs-410_3_3_95,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:25,950","00:06:31,016",95,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=385,"Okay, so, now that we,"
cs-410_3_3_96,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:31,016","00:06:34,290",96,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=391,how can we compare ranked to back list?
cs-410_3_3_97,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:34,290","00:06:37,049",97,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=394,"All right, so, that means,"
cs-410_3_3_98,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:38,430","00:06:40,880",98,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=398,"And here, we show, two cases."
cs-410_3_3_99,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:40,880","00:06:47,260",99,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=400,"Where system A is showing red,"
cs-410_3_3_100,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:48,610","00:06:50,820",100,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=408,"All right, so, which one is better?"
cs-410_3_3_101,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:50,820","00:06:54,080",101,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=410,"I hope you can see,"
cs-410_3_3_102,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:54,080","00:06:56,900",102,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=414,Why?
cs-410_3_3_103,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:58,340","00:07:01,500",103,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=418,"see same level of recall here,"
cs-410_3_3_104,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:01,500","00:07:06,800",104,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=421,"the precision point by system A is better,"
cs-410_3_3_105,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:06,800","00:07:08,260",105,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=426,"So, there's no question."
cs-410_3_3_106,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:08,260","00:07:13,360",106,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=428,"In here, you can imagine, what does the"
cs-410_3_3_107,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:13,360","00:07:17,470",107,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=433,"Well, it has to have perfect,"
cs-410_3_3_108,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:17,470","00:07:18,450",108,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=437,it has to be this line.
cs-410_3_3_109,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:18,450","00:07:21,300",109,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=438,That would be the ideal system.
cs-410_3_3_110,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:21,300","00:07:24,230",110,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=441,"In general, the higher the curve is,"
cs-410_3_3_111,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:24,230","00:07:27,160",111,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=444,"The problem is that,"
cs-410_3_3_112,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:27,160","00:07:29,110",112,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=447,This actually happens often.
cs-410_3_3_113,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:29,110","00:07:30,790",113,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=449,"Like, the two curves cross each other."
cs-410_3_3_114,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:32,430","00:07:34,240",114,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=452,"Now, in this case, which one is better?"
cs-410_3_3_115,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:35,300","00:07:35,800",115,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=455,What do you think?
cs-410_3_3_116,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:38,240","00:07:41,730",116,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=458,"Now, this is a real problem,"
cs-410_3_3_117,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:41,730","00:07:47,150",117,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=461,"Suppose, you build a search engine,"
cs-410_3_3_118,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:47,150","00:07:50,990",118,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=467,"that's shown here in blue, or system B."
cs-410_3_3_119,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:50,990","00:07:53,580",119,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=470,"And, you have come up with a new idea."
cs-410_3_3_120,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:53,580","00:07:54,500",120,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=473,"And, you test it."
cs-410_3_3_121,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:54,500","00:07:58,490",121,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=474,"And, the results are shown in red,"
cs-410_3_3_122,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:59,990","00:08:04,550",122,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=479,"Now, your question is, is your new"
cs-410_3_3_123,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:05,630","00:08:10,510",123,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=485,"Or more, practically,"
cs-410_3_3_124,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:10,510","00:08:15,410",124,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=490,"you're already using, your, in your search"
cs-410_3_3_125,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:15,410","00:08:20,760",125,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=495,"So, should we use system,"
cs-410_3_3_126,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:20,760","00:08:23,250",126,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=500,"This is going to be a real decision,"
cs-410_3_3_127,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:23,250","00:08:29,430",127,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=503,"If you make the replacement, the search"
cs-410_3_3_128,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:29,430","00:08:34,170",128,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=509,"whereas, if you don't do that,"
cs-410_3_3_129,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:34,170","00:08:34,770",129,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=514,"So, what do you do?"
cs-410_3_3_130,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:36,210","00:08:40,580",130,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=516,"Now, if you want to spend more time"
cs-410_3_3_131,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:40,580","00:08:42,840",131,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=520,"And, it's actually very"
cs-410_3_3_132,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:42,840","00:08:46,350",132,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=522,"As I said, it's a real decision that you"
cs-410_3_3_133,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:46,350","00:08:51,329",133,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=526,"search engine, or if you're working, for"
cs-410_3_3_134,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:52,330","00:08:54,630",134,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=532,"Now, if you have thought about this for"
cs-410_3_3_135,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:54,630","00:08:59,630",135,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=534,"a moment, you might realize that,"
cs-410_3_3_136,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:59,630","00:09:04,615",136,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=539,"Now, some users might like a system A,"
cs-410_3_3_137,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:04,615","00:09:05,895",137,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=544,"So, what's the difference here?"
cs-410_3_3_138,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:05,895","00:09:08,545",138,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=545,"Well, the difference is just that,"
cs-410_3_3_139,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:08,545","00:09:14,145",139,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=548,"in the, low level of recall,"
cs-410_3_3_140,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:14,145","00:09:15,845",140,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=554,There's a higher precision.
cs-410_3_3_141,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:15,845","00:09:19,520",141,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=555,"But in high recall region,"
cs-410_3_3_142,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:20,910","00:09:24,040",142,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=560,"Now, so, that also means,"
cs-410_3_3_143,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:24,040","00:09:28,630",143,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=564,"cares about the high recall, or"
cs-410_3_3_144,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:28,630","00:09:32,200",144,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=568,"You can imagine, if someone is just going"
cs-410_3_3_145,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:32,200","00:09:33,489",145,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=572,want to find out something
cs-410_3_3_146,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:34,750","00:09:36,530",146,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=574,"Well, which one is better?"
cs-410_3_3_147,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:36,530","00:09:37,060",147,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=576,What do you think?
cs-410_3_3_148,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:38,110","00:09:41,510",148,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=578,"In this case, clearly, system B is better,"
cs-410_3_3_149,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:41,510","00:09:44,920",149,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=581,because the user is unlikely
cs-410_3_3_150,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:44,920","00:09:46,500",150,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=584,The user doesn't care about high recall.
cs-410_3_3_151,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:47,780","00:09:50,673",151,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=587,"On the other hand,"
cs-410_3_3_152,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:50,673","00:09:54,800",152,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=590,"where a user is doing you are,"
cs-410_3_3_153,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:54,800","00:10:00,320",153,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=594,"You want to find, whether your idea ha,"
cs-410_3_3_154,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:00,320","00:10:03,130",154,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=600,"In that case, you emphasize high recall."
cs-410_3_3_155,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:03,130","00:10:06,630",155,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=603,"So, you want to see,"
cs-410_3_3_156,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:06,630","00:10:09,570",156,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=606,"Therefore, you might, favor, system A."
cs-410_3_3_157,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:09,570","00:10:12,090",157,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=609,"So, that means, which one is better?"
cs-410_3_3_158,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:12,090","00:10:18,420",158,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=612,"That actually depends on users,"
cs-410_3_3_159,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:19,520","00:10:24,040",159,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=619,"So, this means, you may not necessarily"
cs-410_3_3_160,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:25,290","00:10:28,810",160,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=625,that would accurately
cs-410_3_3_161,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:29,860","00:10:31,750",161,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=629,You have to look at the overall picture.
cs-410_3_3_162,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:31,750","00:10:35,620",162,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=631,"Yet, as I said, when you have"
cs-410_3_3_163,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:35,620","00:10:38,210",163,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=635,"whether you replace ours with another,"
cs-410_3_3_164,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:38,210","00:10:44,320",164,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=638,then you may have to actually come up with
cs-410_3_3_165,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:44,320","00:10:49,800",165,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=644,"Or, when we compare many different"
cs-410_3_3_166,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:49,800","00:10:54,590",166,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=649,"one number to compare, them with, so, that"
cs-410_3_3_167,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:54,590","00:11:00,258",167,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=654,"So, for all these reasons, it is desirable"
cs-410_3_3_168,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:00,258","00:11:01,510",168,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=660,"So, how do we do that?"
cs-410_3_3_169,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:01,510","00:11:05,860",169,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=661,"And, that,"
cs-410_3_3_170,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:05,860","00:11:09,560",170,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=665,"So, here again it's"
cs-410_3_3_171,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:09,560","00:11:13,570",171,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=669,"And, one way to summarize"
cs-410_3_3_172,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:13,570","00:11:18,090",172,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=673,"this whole curve,"
cs-410_3_3_173,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:19,330","00:11:21,820",173,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=679,Right?
cs-410_3_3_174,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:21,820","00:11:25,209",174,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=681,"There are other ways to measure that,"
cs-410_3_3_175,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:26,430","00:11:31,110",175,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=686,this particular way of matching
cs-410_3_3_176,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:31,110","00:11:36,260",176,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=691,"has been used, since a long time ago for"
cs-410_3_3_177,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:36,260","00:11:41,140",177,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=696,"basically, in this way, and"
cs-410_3_3_178,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:41,140","00:11:46,260",178,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=701,"Basically, we're going to take a, a look"
cs-410_3_3_179,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:47,600","00:11:49,540",179,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=707,"And then, look out for the precision."
cs-410_3_3_180,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:49,540","00:11:51,930",180,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=709,"So, we know, you know,"
cs-410_3_3_181,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:51,930","00:11:56,590",181,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=711,"And, this is another,"
cs-410_3_3_182,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:56,590","00:11:59,362",182,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=716,"Now, this, we don't count to this one,"
cs-410_3_3_183,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:59,362","00:12:04,511",183,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=719,"because the recall level is the same,"
cs-410_3_3_184,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:04,511","00:12:10,120",184,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=724,"this number, and that's precision at"
cs-410_3_3_185,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:10,120","00:12:13,580",185,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=730,"So, we have all these, you know, added up."
cs-410_3_3_186,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:13,580","00:12:16,180",186,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=733,These are the precisions
cs-410_3_3_187,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:16,180","00:12:21,130",187,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=736,corresponding to retrieving the first
cs-410_3_3_188,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:21,130","00:12:25,260",188,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=741,"then, the third, that follows, et cetera."
cs-410_3_3_189,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:25,260","00:12:29,265",189,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=745,"Now, we missed the many relevant"
cs-410_3_3_190,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:29,265","00:12:32,380",190,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=749,"we just, assume,"
cs-410_3_3_191,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:33,540","00:12:35,740",191,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=753,"And then, finally, we take the average."
cs-410_3_3_192,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:35,740","00:12:37,900",192,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=755,"So, we divide it by ten, and"
cs-410_3_3_193,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:37,900","00:12:40,510",193,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=757,which is the total number of relevant
cs-410_3_3_194,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:41,670","00:12:46,610",194,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=761,"Note that here,"
cs-410_3_3_195,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:46,610","00:12:49,440",195,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=766,Which is a number retrieved
cs-410_3_3_196,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:49,440","00:12:52,980",196,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=769,"Now, imagine, if I divide by four,"
cs-410_3_3_197,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:54,370","00:12:55,820",197,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=774,"Now, think about this, for a moment."
cs-410_3_3_198,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:57,050","00:13:01,300",198,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=777,"It's a common mistake that people,"
cs-410_3_3_199,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:02,720","00:13:08,208",199,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=782,"Right, so, if we, we divide this by four,"
cs-410_3_3_200,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:08,208","00:13:13,180",200,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=788,"In fact, that you are favoring a system,"
cs-410_3_3_201,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:13,180","00:13:18,785",201,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=793,"documents, as in that case,"
cs-410_3_3_202,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:18,785","00:13:22,115",202,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=798,"So, this would be, not a good matching."
cs-410_3_3_203,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:22,115","00:13:25,862",203,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=802,"So, note that this denomina,"
cs-410_3_3_204,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:25,862","00:13:29,170",204,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=805,the total number of relevant documents.
cs-410_3_3_205,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:29,170","00:13:33,620",205,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=809,"And, this will basically ,compute"
cs-410_3_3_206,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:33,620","00:13:40,080",206,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=813,"And, this is the standard method,"
cs-410_3_3_207,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:41,210","00:13:44,860",207,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=821,"Note that, it actually combines"
cs-410_3_3_208,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:44,860","00:13:49,230",208,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=824,"But first, you know, we have"
cs-410_3_3_209,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:49,230","00:13:53,240",209,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=829,"we also consider recall, because if missed"
cs-410_3_3_210,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:53,240","00:13:57,470",210,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=833,"All right, so,"
cs-410_3_3_211,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:57,470","00:14:02,190",211,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=837,"And furthermore, you can see this"
cs-410_3_3_212,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:02,190","00:14:04,770",212,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=842,of a position of a relevant document.
cs-410_3_3_213,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:04,770","00:14:09,520",213,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=844,"Let's say, if I move this relevant"
cs-410_3_3_214,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:09,520","00:14:12,670",214,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=849,"it would increase this means,"
cs-410_3_3_215,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:12,670","00:14:17,630",215,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=852,"Whereas, if I move any relevant document,"
cs-410_3_3_216,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:17,630","00:14:23,720",216,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=857,"document down, then it would decrease,"
cs-410_3_3_217,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:23,720","00:14:25,266",217,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=863,"So, this is a very good,"
cs-410_3_3_218,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:25,266","00:14:30,570",218,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=865,because it's a very sensitive to
cs-410_3_3_219,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:30,570","00:14:34,740",219,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=870,"It can tell, small differences"
cs-410_3_3_220,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:34,740","00:14:35,880",220,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=874,"And, that is what we want,"
cs-410_3_3_221,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:35,880","00:14:40,430",221,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=875,sometimes one algorithm only works
cs-410_3_3_222,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:40,430","00:14:42,440",222,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=880,"And, we want to see this difference."
cs-410_3_3_223,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:42,440","00:14:46,110",223,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=882,"In contrast, if we look at"
cs-410_3_3_224,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:46,110","00:14:49,520",224,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=886,"If we look at this, this whole set, well,"
cs-410_3_3_225,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:49,520","00:14:52,000",225,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=889,"what, what's the precision,"
cs-410_3_3_226,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:52,000","00:14:54,328",226,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=892,"Well, it's easy to see,"
cs-410_3_3_227,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:54,328","00:15:02,200",227,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=894,"So, that precision is very meaningful,"
cs-410_3_3_228,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:02,200","00:15:04,580",228,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=902,"So, that's pretty useful, right?"
cs-410_3_3_229,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:04,580","00:15:07,850",229,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=904,"So, it's a meaningful measure,"
cs-410_3_3_230,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:07,850","00:15:11,770",230,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=907,"But, if we use this measure to"
cs-410_3_3_231,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:11,770","00:15:16,480",231,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=911,because it wouldn't be sensitive to where
cs-410_3_3_232,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:16,480","00:15:21,910",232,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=916,If I move them around the precision
cs-410_3_3_233,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:21,910","00:15:22,570",233,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=921,Right.
cs-410_3_3_234,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:22,570","00:15:25,590",234,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=922,this is not a good measure for
cs-410_3_3_235,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:25,590","00:15:29,990",235,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=925,"In contrast, the average precision"
cs-410_3_3_236,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:29,990","00:15:34,511",236,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=929,"It can tell the difference of, different,"
cs-410_3_3_237,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:34,511","00:15:39,269",237,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=934,"a difference in ranked list in,"
cs-410_3_3_238,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:39,269","00:15:49,269",238,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=939,[MUSIC]
cs-410_3_4_1,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:00,012","00:00:03,467",1,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=0,[SOUND]
cs-410_3_4_2,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:11,647","00:00:13,963",2,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=11,So average precision is computer for
cs-410_3_4_3,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:13,963","00:00:14,690",3,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=13,just one.
cs-410_3_4_4,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:14,690","00:00:18,290",4,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=14,one query.
cs-410_3_4_5,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:18,290","00:00:24,570",5,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=18,different queries and this is to
cs-410_3_4_6,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:24,570","00:00:29,530",6,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=24,Depending on the queries you use you
cs-410_3_4_7,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:29,530","00:00:31,870",7,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=29,"Right, so"
cs-410_3_4_8,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:33,610","00:00:36,580",8,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=33,"If you use more queries then,"
cs-410_3_4_9,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:36,580","00:00:39,850",9,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=36,take the average of the average
cs-410_3_4_10,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:41,600","00:00:42,470",10,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=41,So how can we do that?
cs-410_3_4_11,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:43,560","00:00:46,160",11,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=43,"Well, you can naturally."
cs-410_3_4_12,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:46,160","00:00:49,260",12,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=46,Think of just doing arithmetic mean as we
cs-410_3_4_13,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:50,670","00:00:56,000",13,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=50,"always tend to, to think in, in this way."
cs-410_3_4_14,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:56,000","00:01:02,000",14,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=56,"So, this would give us what's called"
cs-410_3_4_15,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:02,000","00:01:02,540",15,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=62,"In this case,"
cs-410_3_4_16,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:02,540","00:01:08,370",16,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=62,we take arithmetic mean of all the average
cs-410_3_4_17,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:09,930","00:01:13,840",17,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=69,But as I just mentioned in
cs-410_3_4_18,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:15,370","00:01:16,580",18,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=75,We call that.
cs-410_3_4_19,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:16,580","00:01:21,190",19,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=76,We talked about the different ways
cs-410_3_4_20,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:21,190","00:01:27,340",20,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=81,And we conclude that the arithmetic
cs-410_3_4_21,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:27,340","00:01:28,420",21,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=87,But here it's the same.
cs-410_3_4_22,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:28,420","00:01:32,120",22,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=88,We can also think about the alternative
cs-410_3_4_23,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:32,120","00:01:34,850",23,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=92,"Don't just automatically assume that,"
cs-410_3_4_24,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:34,850","00:01:37,785",24,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=94,Let's just also take the arithmetic
cs-410_3_4_25,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:37,785","00:01:38,590",25,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=97,these queries.
cs-410_3_4_26,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:38,590","00:01:42,910",26,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=98,Let's think about what's
cs-410_3_4_27,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:42,910","00:01:46,660",27,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=102,"If you think about the different ways,"
cs-410_3_4_28,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:46,660","00:01:49,880",28,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=106,probably be able to think about
cs-410_3_4_29,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:51,230","00:01:53,680",29,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=111,And we call this kind of average a gMAP.
cs-410_3_4_30,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:55,650","00:01:56,860",30,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=115,This is another way.
cs-410_3_4_31,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:56,860","00:01:59,520",31,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=116,"So now, once you think about"
cs-410_3_4_32,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:59,520","00:02:00,820",32,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=119,Of doing the same thing.
cs-410_3_4_33,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:00,820","00:02:03,400",33,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=120,"The natural question to ask is,"
cs-410_3_4_34,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:03,400","00:02:03,900",34,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=123,So.
cs-410_3_4_35,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:05,230","00:02:08,060",35,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=125,"So, do you use MAP or gMAP?"
cs-410_3_4_36,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:09,830","00:02:11,200",36,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=129,"Again, that's important question."
cs-410_3_4_37,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:11,200","00:02:14,490",37,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=131,Imagine you are again
cs-410_3_4_38,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:14,490","00:02:17,109",38,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=134,by comparing the ways your old
cs-410_3_4_39,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:18,390","00:02:22,080",39,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=138,Now you tested multiple topics.
cs-410_3_4_40,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:22,080","00:02:25,150",40,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=142,Now you've got the average precision for
cs-410_3_4_41,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:25,150","00:02:28,470",41,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=145,Now you are thinking of looking
cs-410_3_4_42,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:28,470","00:02:29,470",42,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=148,You have to take the average.
cs-410_3_4_43,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:30,950","00:02:32,840",43,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=150,"But which, which strategy would you use?"
cs-410_3_4_44,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:34,040","00:02:38,360",44,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=154,"Now first, you should also think about the"
cs-410_3_4_45,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:38,360","00:02:43,100",45,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=158,Can you think of scenarios where using
cs-410_3_4_46,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:43,100","00:02:45,920",46,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=163,That is they would give different
cs-410_3_4_47,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:45,920","00:02:52,440",47,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=165,And that also means depending on
cs-410_3_4_48,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:52,440","00:02:54,450",48,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=172,Average of these average positions.
cs-410_3_4_49,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:55,600","00:02:57,460",49,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=175,You will get different conclusions.
cs-410_3_4_50,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:57,460","00:03:00,379",50,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=177,This makes the question
cs-410_3_4_51,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:01,620","00:03:03,480",51,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=181,Right?
cs-410_3_4_52,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:05,350","00:03:08,320",52,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=185,"Well again, if you look at"
cs-410_3_4_53,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:08,320","00:03:12,750",53,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=188,Different ways of aggregating
cs-410_3_4_54,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:12,750","00:03:18,510",54,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=192,"You'll realize in arithmetic mean,"
cs-410_3_4_55,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:18,510","00:03:20,250",55,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=198,So what does large value here mean?
cs-410_3_4_56,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:20,250","00:03:22,260",56,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=200,It means the query is relatively easy.
cs-410_3_4_57,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:22,260","00:03:24,750",57,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=202,"You can have a high pres,"
cs-410_3_4_58,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:25,950","00:03:29,707",58,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=205,Whereas gMAP tends to be
cs-410_3_4_59,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:30,870","00:03:34,790",59,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=210,And those are the queries that
cs-410_3_4_60,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:34,790","00:03:36,370",60,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=214,The average precision is low.
cs-410_3_4_61,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:37,410","00:03:41,260",61,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=217,"So if you think about the,"
cs-410_3_4_62,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:41,260","00:03:45,840",62,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=221,"those difficult queries,"
cs-410_3_4_63,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:47,480","00:03:50,000",63,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=227,"On the other hand, if you just want to."
cs-410_3_4_64,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:50,000","00:03:50,960",64,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=230,Have improved a lot.
cs-410_3_4_65,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:52,060","00:03:55,750",65,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=232,Over all the kinds of queries or
cs-410_3_4_66,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:55,750","00:04:00,890",66,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=235,easy and you want to make the perfect and
cs-410_3_4_67,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:00,890","00:04:05,150",67,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=240,"So again, the answer depends on"
cs-410_3_4_68,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:05,150","00:04:06,710",68,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=245,"their pref, their preferences."
cs-410_3_4_69,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:08,020","00:04:13,720",69,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=248,So the point that here is to think
cs-410_3_4_70,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:13,720","00:04:18,750",70,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=253,"the same problem, and then compare them,"
cs-410_3_4_71,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:18,750","00:04:20,610",71,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=258,And which one makes more sense.
cs-410_3_4_72,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:20,610","00:04:24,970",72,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=260,"Often, when one of them might"
cs-410_3_4_73,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:24,970","00:04:27,640",73,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=264,another might make more sense
cs-410_3_4_74,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:27,640","00:04:31,620",74,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=267,So it's important to pick out under
cs-410_3_4_75,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:35,209","00:04:38,967",75,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=275,As a special case of the mean average
cs-410_3_4_76,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:38,967","00:04:43,100",76,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=278,the case where there was precisely
cs-410_3_4_77,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:43,100","00:04:47,210",77,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=283,"And this happens often, for example,"
cs-410_3_4_78,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:47,210","00:04:52,670",78,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=287,"Where you know a target page, let's"
cs-410_3_4_79,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:52,670","00:04:56,140",79,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=292,"You have one relevant document there,"
cs-410_3_4_80,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:56,140","00:04:58,250",80,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=296,"That's call a ""known item search""."
cs-410_3_4_81,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:58,250","00:05:01,330",81,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=298,"In that case,"
cs-410_3_4_82,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:01,330","00:05:03,470",82,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=301,"Or in another application,"
cs-410_3_4_83,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:03,470","00:05:04,640",83,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=303,maybe there's only one answer.
cs-410_3_4_84,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:04,640","00:05:05,250",84,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=304,Are there.
cs-410_3_4_85,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:05,250","00:05:07,110",85,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=305,"So if you rank the answers,"
cs-410_3_4_86,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:07,110","00:05:12,100",86,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=307,then your goal is to rank that one
cs-410_3_4_87,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:12,100","00:05:16,480",87,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=312,"So in this case, you can easily"
cs-410_3_4_88,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:16,480","00:05:21,710",88,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=316,will basically boil down
cs-410_3_4_89,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:21,710","00:05:28,220",89,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=321,"That is, 1 over r where r is the rank"
cs-410_3_4_90,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:28,220","00:05:32,210",90,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=328,So if that document is ranked
cs-410_3_4_91,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:32,210","00:05:35,930",91,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=332,then it's 1 for reciprocal rank.
cs-410_3_4_92,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:35,930","00:05:39,515",92,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=335,"If it's ranked at the,"
cs-410_3_4_93,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:39,515","00:05:40,015",93,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=339,Et cetera.
cs-410_3_4_94,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:41,145","00:05:45,335",94,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=341,"And then we can also take a, a average"
cs-410_3_4_95,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:45,335","00:05:48,025",95,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=345,"reciprocal rank over a set of topics, and"
cs-410_3_4_96,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:48,025","00:05:52,555",96,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=348,that would give us something
cs-410_3_4_97,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:52,555","00:05:54,830",97,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=352,It's a very popular measure.
cs-410_3_4_98,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:54,830","00:05:57,273",98,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=354,"For no item search or, you know,"
cs-410_3_4_99,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:57,273","00:06:01,690",99,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=357,an problem where you have
cs-410_3_4_100,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:03,070","00:06:09,170",100,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=363,"Now again here, you can see this"
cs-410_3_4_101,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:09,170","00:06:13,570",101,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=369,And this r is basically
cs-410_3_4_102,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:13,570","00:06:18,700",102,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=373,a user would have to make in order
cs-410_3_4_103,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:18,700","00:06:23,780",103,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=378,If it's ranked on the top it's low effort
cs-410_3_4_104,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:23,780","00:06:26,860",104,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=383,But if it's ranked at 100
cs-410_3_4_105,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:27,940","00:06:32,260",105,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=387,read presumably 100 documents
cs-410_3_4_106,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:32,260","00:06:37,380",106,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=392,"So, in this sense r is also a meaningful"
cs-410_3_4_107,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:37,380","00:06:41,520",107,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=397,"take the reciprocal of r,"
cs-410_3_4_108,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:42,750","00:06:45,895",108,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=402,So my natural question here
cs-410_3_4_109,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:45,895","00:06:50,550",109,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=405,I imagine if you were to design
cs-410_3_4_110,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:50,550","00:06:54,680",110,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=410,"of a random system,"
cs-410_3_4_111,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:55,760","00:07:00,070",111,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=415,You might have thought about
cs-410_3_4_112,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:00,070","00:07:02,906",112,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=420,"After all,"
cs-410_3_4_113,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:02,906","00:07:10,959",113,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=422,"But, think about if you take a average"
cs-410_3_4_114,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:12,200","00:07:13,730",114,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=432,Again it would make a difference.
cs-410_3_4_115,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:13,730","00:07:16,330",115,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=433,"Right, for one single topic, using r or"
cs-410_3_4_116,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:16,330","00:07:19,140",116,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=436,using 1 over r wouldn't
cs-410_3_4_117,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:19,140","00:07:21,640",117,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=439,It's the same.
cs-410_3_4_118,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:21,640","00:07:24,790",118,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=441,Larger r with corresponds
cs-410_3_4_119,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:26,400","00:07:32,700",119,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=446,"But the difference would only show when,"
cs-410_3_4_120,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:32,700","00:07:39,290",120,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=452,"So again, think about the average of Mean"
cs-410_3_4_121,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:39,290","00:07:39,930",121,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=459,What's the difference?
cs-410_3_4_122,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:39,930","00:07:41,730",122,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=459,Do you see any difference?
cs-410_3_4_123,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:41,730","00:07:46,050",123,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=461,"And would, would this difference"
cs-410_3_4_124,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:46,050","00:07:46,760",124,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=466,In our conclusion.
cs-410_3_4_125,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:49,050","00:07:53,380",125,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=469,"And this, it turns out that,"
cs-410_3_4_126,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:53,380","00:07:57,210",126,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=473,"if you think about it, if you want to"
cs-410_3_4_127,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:57,210","00:07:58,230",127,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=477,then pause the video.
cs-410_3_4_128,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:59,410","00:08:04,350",128,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=479,"Basically, the difference is,"
cs-410_3_4_129,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:04,350","00:08:07,810",129,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=484,Again it will be dominated
cs-410_3_4_130,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:07,810","00:08:08,840",130,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=487,So what are those values?
cs-410_3_4_131,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:08,840","00:08:15,240",131,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=488,Those are basically large values that
cs-410_3_4_132,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:15,240","00:08:20,530",132,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=495,That means the relevant items
cs-410_3_4_133,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:20,530","00:08:25,160",133,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=500,And the sum that's also the average
cs-410_3_4_134,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:25,160","00:08:28,328",134,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=505,Where those relevant documents
cs-410_3_4_135,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:28,328","00:08:30,774",135,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=508,in the lower portion of the ranked.
cs-410_3_4_136,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:30,774","00:08:35,850",136,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=510,But from a users perspective we care
cs-410_3_4_137,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:35,850","00:08:39,529",137,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=515,So by taking this transformation
cs-410_3_4_138,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:40,650","00:08:43,920",138,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=520,Here we emphasize more on
cs-410_3_4_139,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:43,920","00:08:48,121",139,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=523,"You know, think about"
cs-410_3_4_140,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:48,121","00:08:52,390",140,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=528,"it would make a big difference, in 1 over"
cs-410_3_4_141,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:52,390","00:08:57,030",141,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=532,where and when won't make much
cs-410_3_4_142,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:57,030","00:09:01,370",142,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=537,But if you use this there will
cs-410_3_4_143,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:01,370","00:09:03,468",143,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=541,"let's say 1,000, right."
cs-410_3_4_144,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:03,468","00:09:05,000",144,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=543,So this is not the desirable.
cs-410_3_4_145,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:06,260","00:09:09,320",145,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=546,"On the other hand, a 1 and"
cs-410_3_4_146,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:09,320","00:09:13,150",146,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=549,So this is yet another case where there
cs-410_3_4_147,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:13,150","00:09:15,840",147,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=553,thing and then you need to figure
cs-410_3_4_148,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:17,470","00:09:22,360",148,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=557,"So to summarize,"
cs-410_3_4_149,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:22,360","00:09:25,738",149,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=562,Can characterize the overall
cs-410_3_4_150,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:25,738","00:09:30,650",150,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=565,And we emphasized that the actual
cs-410_3_4_151,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:30,650","00:09:34,570",151,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=570,on how many top ranked results
cs-410_3_4_152,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:34,570","00:09:37,000",152,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=574,Some users will examine more.
cs-410_3_4_153,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:37,000","00:09:38,390",153,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=577,Than others.
cs-410_3_4_154,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:38,390","00:09:42,100",154,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=578,An average person uses a standard measure
cs-410_3_4_155,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:42,100","00:09:44,837",155,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=582,It combines precision and recall and
cs-410_3_4_156,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:44,837","00:09:48,904",156,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=584,it's sensitive to the rank
cs-410_3_4_157,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:48,904","00:09:58,904",157,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=588,[MUSIC]
cs-410_3_5_1,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:00,883","00:00:05,127",1,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=0,[MUSIC]
cs-410_3_5_2,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:07,433","00:00:12,376",2,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=7,This lecture is about how to evaluate
cs-410_3_5_3,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:12,376","00:00:15,560",3,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=12,multiple levels of judgements.
cs-410_3_5_4,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:15,560","00:00:19,994",4,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=15,"In this lecture, we will continue"
cs-410_3_5_5,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:19,994","00:00:23,410",5,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=19,We're going to look at how to
cs-410_3_5_6,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:23,410","00:00:26,150",6,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=23,when we have multiple
cs-410_3_5_7,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:27,760","00:00:31,180",7,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=27,So far we have talked about
cs-410_3_5_8,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:31,180","00:00:34,169",8,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=31,that means a document is judged as
cs-410_3_5_9,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:35,270","00:00:40,310",9,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=35,"But earlier, we also talk about"
cs-410_3_5_10,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:40,310","00:00:45,580",10,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=40,So we often can distinguish
cs-410_3_5_11,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:45,580","00:00:50,230",11,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=45,"those are very useful documents,"
cs-410_3_5_12,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:50,230","00:00:53,000",12,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=50,"They are okay, they are useful perhaps."
cs-410_3_5_13,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:53,000","00:00:56,170",13,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=53,"And further from now, we're adding"
cs-410_3_5_14,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:57,450","00:01:01,490",14,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=57,So imagine you can have ratings for
cs-410_3_5_15,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:01,490","00:01:05,390",15,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=61,"Then, you would have"
cs-410_3_5_16,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:05,390","00:01:10,803",16,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=65,"For example, here I show example of three"
cs-410_3_5_17,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:10,803","00:01:15,780",17,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=70,"very relevant, 2 for marginally relevant,"
cs-410_3_5_18,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:15,780","00:01:18,990",18,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=75,"Now, how do we evaluate the search"
cs-410_3_5_19,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:18,990","00:01:23,330",19,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=78,"Obvious that the map doesn't work, average"
cs-410_3_5_20,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:23,330","00:01:28,190",20,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=83,"recall doesn't work,"
cs-410_3_5_21,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:28,190","00:01:33,510",21,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=88,So let's look at some top ranked
cs-410_3_5_22,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:33,510","00:01:38,518",22,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=93,Imagine the user would be mostly
cs-410_3_5_23,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:43,122","00:01:48,165",23,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=103,"And we marked the rating levels,"
cs-410_3_5_24,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:48,165","00:01:54,620",24,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=108,"for these documents as shown here,"
cs-410_3_5_25,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:54,620","00:01:57,122",25,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=114,And we call these gain.
cs-410_3_5_26,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:57,122","00:02:02,345",26,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=117,And the reason why we call it
cs-410_3_5_27,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:02,345","00:02:08,860",27,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=122,that we are infusing is called the NDCG
cs-410_3_5_28,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:10,090","00:02:14,900",28,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=130,"So this gain, basically,"
cs-410_3_5_29,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:14,900","00:02:19,790",29,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=134,information a user can obtain by
cs-410_3_5_30,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:19,790","00:02:24,120",30,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=139,"So looking at the first document,"
cs-410_3_5_31,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:24,120","00:02:28,110",31,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=144,Looking at the non-relevant document
cs-410_3_5_32,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:29,510","00:02:32,703",32,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=149,Looking at the moderator or
cs-410_3_5_33,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:32,703","00:02:35,910",33,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=152,"document the user would get 2 points,"
cs-410_3_5_34,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:35,910","00:02:40,560",34,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=155,"So, this gain to each of the measures is"
cs-410_3_5_35,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:40,560","00:02:41,890",35,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=160,perspective.
cs-410_3_5_36,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:41,890","00:02:46,140",36,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=161,"Of course, if we assume the user"
cs-410_3_5_37,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:46,140","00:02:51,060",37,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=166,"we're looking at the cutoff at 10,"
cs-410_3_5_38,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:51,060","00:02:51,774",38,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=171,And what's that?
cs-410_3_5_39,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:51,774","00:02:55,825",39,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=171,"Well, that's simply the sum of these,"
cs-410_3_5_40,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:55,825","00:02:59,275",40,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=175,"So if the user stops after the position 1,"
cs-410_3_5_41,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:59,275","00:03:03,163",41,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=179,"If the user looks at another document,"
cs-410_3_5_42,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:03,163","00:03:08,221",42,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=183,"If the user looks at the more documents,"
cs-410_3_5_43,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:08,221","00:03:13,200",43,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=188,Of course this is at the cost of
cs-410_3_5_44,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:13,200","00:03:16,390",44,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=193,So cumulative gain gives
cs-410_3_5_45,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:16,390","00:03:21,368",45,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=196,much total gain the user would have if
cs-410_3_5_46,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:21,368","00:03:28,140",46,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=201,"Now, in NDCG, we also have another letter"
cs-410_3_5_47,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:29,170","00:03:32,060",47,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=209,"So, why do we want to do discounting?"
cs-410_3_5_48,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:32,060","00:03:35,685",48,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=212,"Well, if you look at this cumulative gain,"
cs-410_3_5_49,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:35,685","00:03:41,975",49,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=215,which is it did not consider the rank
cs-410_3_5_50,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:41,975","00:03:46,115",50,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=221,"So for example, looking at this sum here,"
cs-410_3_5_51,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:46,115","00:03:51,485",51,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=226,and we only know there is 1
cs-410_3_5_52,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:51,485","00:03:54,945",52,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=231,"1 marginally relevant document,"
cs-410_3_5_53,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:54,945","00:03:57,300",53,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=234,We don't really care
cs-410_3_5_54,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:57,300","00:04:02,110",54,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=237,"Ideally, we want these two to be ranked"
cs-410_3_5_55,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:03,120","00:04:06,420",55,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=243,But how can we capture that intuition?
cs-410_3_5_56,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:06,420","00:04:13,209",56,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=246,"Well we have to say, well this is 3 here"
cs-410_3_5_57,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:13,209","00:04:18,114",57,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=253,And that means the contribution
cs-410_3_5_58,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:18,114","00:04:22,750",58,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=258,positions has to be
cs-410_3_5_59,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:22,750","00:04:24,666",59,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=262,"And this is the idea of discounting,"
cs-410_3_5_60,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:24,666","00:04:29,530",60,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=264,"So we're going to to say, well, the first"
cs-410_3_5_61,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:29,530","00:04:33,910",61,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=269,because the user can be assumed
cs-410_3_5_62,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:33,910","00:04:38,030",62,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=273,"But the second one,"
cs-410_3_5_63,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:38,030","00:04:42,370",63,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=278,because there's a small possibility
cs-410_3_5_64,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:42,370","00:04:48,690",64,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=282,So we divide this gain by
cs-410_3_5_65,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:48,690","00:04:52,640",65,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=288,"So log of 2,"
cs-410_3_5_66,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:52,640","00:04:57,080",66,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=292,"And when we go to the third position,"
cs-410_3_5_67,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:57,080","00:05:01,270",67,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=297,"because the normalizer is log of 3,"
cs-410_3_5_68,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:01,270","00:05:06,690",68,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=301,So when we take such a sum that a lower
cs-410_3_5_69,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:06,690","00:05:10,000",69,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=306,that much as a highly ranked document.
cs-410_3_5_70,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:10,000","00:05:15,120",70,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=310,"So that means if you, for example,"
cs-410_3_5_71,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:15,120","00:05:20,726",71,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=315,"this position, and this one, and then"
cs-410_3_5_72,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:20,726","00:05:27,050",72,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=320,for example very relevant
cs-410_3_5_73,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:27,050","00:05:31,290",73,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=327,"Imagine if you put the 3 here,"
cs-410_3_5_74,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:31,290","00:05:34,635",74,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=331,So it's not as good as if
cs-410_3_5_75,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:34,635","00:05:36,650",75,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=334,So this is the idea of discounting.
cs-410_3_5_76,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:37,900","00:05:43,210",76,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=337,"Okay, so now at this point that we have"
cs-410_3_5_77,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:43,210","00:05:50,000",77,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=343,measuring the utility of this ranked
cs-410_3_5_78,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:51,480","00:05:53,125",78,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=351,So are we happy with this?
cs-410_3_5_79,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:53,125","00:05:55,680",79,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=353,"Well, we can use this to rank systems."
cs-410_3_5_80,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:55,680","00:05:58,510",80,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=355,"Now, we still need to do a little bit more"
cs-410_3_5_81,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:58,510","00:06:03,272",81,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=358,in order to make this measure
cs-410_3_5_82,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:03,272","00:06:10,580",82,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=363,"And this is the last step, and by the way,"
cs-410_3_5_83,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:10,580","00:06:16,820",83,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=370,"so this is the total sum of DCG,"
cs-410_3_5_84,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:16,820","00:06:20,880",84,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=376,"So the last step is called N,"
cs-410_3_5_85,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:20,880","00:06:25,240",85,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=380,"And if we do that,"
cs-410_3_5_86,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:25,240","00:06:26,463",86,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=385,So how do we do that?
cs-410_3_5_87,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:26,463","00:06:31,241",87,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=386,"Well, the idea here is we're"
cs-410_3_5_88,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:31,241","00:06:35,280",88,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=391,the ideal DCG at the same cutoff.
cs-410_3_5_89,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:35,280","00:06:37,130",89,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=395,What is the ideal DCG?
cs-410_3_5_90,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:37,130","00:06:40,830",90,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=397,"Well, this is the DCG of an ideal ranking."
cs-410_3_5_91,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:40,830","00:06:47,690",91,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=400,So imagine if we have 9 documents in
cs-410_3_5_92,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:47,690","00:06:52,840",92,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=407,And that means in total we
cs-410_3_5_93,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:53,840","00:07:00,640",93,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=413,Then our ideal rank lister would have put
cs-410_3_5_94,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:00,640","00:07:05,730",94,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=420,So all these would have to be 3 and
cs-410_3_5_95,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:05,730","00:07:10,040",95,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=425,Because that's the best we could
cs-410_3_5_96,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:10,040","00:07:11,800",96,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=430,But all these positions would be 3.
cs-410_3_5_97,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:11,800","00:07:13,938",97,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=431,Right?
cs-410_3_5_98,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:13,938","00:07:16,090",98,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=433,So this would our ideal ranked list.
cs-410_3_5_99,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:18,070","00:07:21,700",99,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=438,And then we had computed the DCG for
cs-410_3_5_100,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:23,250","00:07:27,062",100,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=443,So this would be given by this
cs-410_3_5_101,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:27,062","00:07:35,723",101,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=447,And so this ideal DCG would then
cs-410_3_5_102,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:35,723","00:07:36,845",102,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=455,So here.
cs-410_3_5_103,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:36,845","00:07:40,040",103,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=456,And this idea of DCG would
cs-410_3_5_104,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:40,040","00:07:43,726",104,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=460,"So you can imagine now,"
cs-410_3_5_105,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:43,726","00:07:49,590",105,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=463,compare the actual DCG with the best DCG
cs-410_3_5_106,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:49,590","00:07:51,146",106,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=469,Now why do we want to do this?
cs-410_3_5_107,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:51,146","00:07:56,590",107,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=471,"Well, by doing this we'll map the DCG"
cs-410_3_5_108,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:57,900","00:08:01,650",108,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=477,"So the best value, or the highest value,"
cs-410_3_5_109,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:01,650","00:08:07,500",109,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=481,"That's when your rank list is,"
cs-410_3_5_110,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:07,500","00:08:12,260",110,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=487,"otherwise, in general,"
cs-410_3_5_111,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:13,405","00:08:14,954",111,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=493,"Now, what if we don't do that?"
cs-410_3_5_112,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:14,954","00:08:19,737",112,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=494,"Well, you can see, this transformation,"
cs-410_3_5_113,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:19,737","00:08:24,108",113,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=499,doesn't really affect the relative
cs-410_3_5_114,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:24,108","00:08:29,053",114,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=504,"just one topic, because this ideal"
cs-410_3_5_115,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:29,053","00:08:33,834",115,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=509,so the ranking of systems based on
cs-410_3_5_116,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:33,834","00:08:36,986",116,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=513,if you rank them based
cs-410_3_5_117,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:36,986","00:08:40,760",117,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=516,The difference however is
cs-410_3_5_118,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:40,760","00:08:42,894",118,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=520,"Because if we don't do normalization,"
cs-410_3_5_119,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:42,894","00:08:45,730",119,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=522,different topics will have
cs-410_3_5_120,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:46,740","00:08:51,951",120,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=526,"For a topic like this one,"
cs-410_3_5_121,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:51,951","00:08:56,593",121,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=531,"the DCG can get really high,"
cs-410_3_5_122,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:56,593","00:09:02,794",122,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=536,there are only two very relevant documents
cs-410_3_5_123,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:02,794","00:09:06,124",123,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=542,Then the highest DCG that
cs-410_3_5_124,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:06,124","00:09:09,210",124,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=546,such a topic would not be very high.
cs-410_3_5_125,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:09,210","00:09:15,555",125,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=549,"So again, we face the problem of"
cs-410_3_5_126,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:15,555","00:09:17,028",126,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=555,"When we take an average,"
cs-410_3_5_127,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:17,028","00:09:20,826",127,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=557,we don't want the average to be
cs-410_3_5_128,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:20,826","00:09:23,360",128,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=560,"Those are, again, easy queries."
cs-410_3_5_129,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:23,360","00:09:27,220",129,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=563,"So, by doing the normalization,"
cs-410_3_5_130,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:27,220","00:09:31,690",130,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=567,making all the queries contribute
cs-410_3_5_131,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:31,690","00:09:34,882",131,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=571,"So, this is a idea of NDCG, it's used for"
cs-410_3_5_132,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:34,882","00:09:40,470",132,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=574,measuring a rank list based on multiple
cs-410_3_5_133,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:42,830","00:09:47,951",133,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=582,In a more general way this
cs-410_3_5_134,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:47,951","00:09:55,900",134,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=587,that can be applied to any ranked task
cs-410_3_5_135,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:55,900","00:10:01,111",135,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=595,And the scale of the judgements
cs-410_3_5_136,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:01,111","00:10:07,094",136,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=601,binary not only more than binary they
cs-410_3_5_137,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:07,094","00:10:11,365",137,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=607,"0, 5 or"
cs-410_3_5_138,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:11,365","00:10:15,631",138,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=611,"And the main idea of this measure,"
cs-410_3_5_139,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:15,631","00:10:19,920",139,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=615,is to measure the total utility
cs-410_3_5_140,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:19,920","00:10:24,120",140,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=619,So you always choose a cutoff and
cs-410_3_5_141,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:24,120","00:10:28,700",141,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=624,And it would discount the contribution
cs-410_3_5_142,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:28,700","00:10:31,811",142,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=628,"And then finally,"
cs-410_3_5_143,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:31,811","00:10:37,645",143,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=631,it would do normalization to ensure
cs-410_3_5_144,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:37,645","00:10:43,093",144,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=637,comparability across queries.
cs-410_3_5_145,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:43,093","00:10:48,319",145,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=643,[MUSIC]
cs-410_3_6_1,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:00,004","00:00:06,485",1,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=0,[SOUND].
cs-410_3_6_2,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:06,485","00:00:10,694",2,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=6,This lecture is about some practical
cs-410_3_6_3,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:10,694","00:00:12,939",3,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=10,evaluation of text retrieval systems.
cs-410_3_6_4,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:14,440","00:00:17,730",4,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=14,"In this lecture, we will continue"
cs-410_3_6_5,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:17,730","00:00:21,250",5,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=17,We'll cover some practical
cs-410_3_6_6,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:21,250","00:00:24,240",6,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=21,in actual evaluation of
cs-410_3_6_7,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:25,500","00:00:29,060",7,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=25,"So, in order to create"
cs-410_3_6_8,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:29,060","00:00:31,540",8,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=29,we have to create a set of queries.
cs-410_3_6_9,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:31,540","00:00:34,270",9,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=31,A set of documents and
cs-410_3_6_10,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:35,750","00:00:39,680",10,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=35,It turns out that each is
cs-410_3_6_11,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:39,680","00:00:43,240",11,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=39,"First, the documents and"
cs-410_3_6_12,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:43,240","00:00:47,250",12,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=43,They must represent the real queries and
cs-410_3_6_13,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:48,290","00:00:50,990",13,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=48,And we also have to use many queries and
cs-410_3_6_14,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:50,990","00:00:55,010",14,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=50,many documents in order to
cs-410_3_6_15,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:56,470","00:01:02,560",15,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=56,For the matching of relevant
cs-410_3_6_16,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:02,560","00:01:10,050",16,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=62,We also need to ensure that there exists a
cs-410_3_6_17,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:10,050","00:01:13,900",17,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=70,"If a query has only one, that's"
cs-410_3_6_18,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:13,900","00:01:18,300",18,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=73,It's not very informative to
cs-410_3_6_19,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:18,300","00:01:23,120",19,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=78,using such a query because there's not
cs-410_3_6_20,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:23,120","00:01:27,390",20,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=83,"So ideally, there should be more"
cs-410_3_6_21,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:27,390","00:01:30,469",21,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=87,the queries also should represent
cs-410_3_6_22,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:31,470","00:01:35,240",22,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=91,"In terms of relevance judgments,"
cs-410_3_6_23,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:35,240","00:01:38,970",23,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=95,complete judgments of all
cs-410_3_6_24,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:38,970","00:01:40,670",24,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=98,"Yet, minimizing human and"
cs-410_3_6_25,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:40,670","00:01:44,980",25,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=100,"fault, because we have to use human"
cs-410_3_6_26,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:44,980","00:01:47,690",26,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=104,It's very labor intensive.
cs-410_3_6_27,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:47,690","00:01:52,550",27,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=107,"And as a result, it's impossible to"
cs-410_3_6_28,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:52,550","00:01:57,170",28,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=112,"all the queries, especially considering"
cs-410_3_6_29,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:58,750","00:02:03,590",29,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=118,"So this is actually a major challenge,"
cs-410_3_6_30,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:03,590","00:02:07,160",30,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=123,"For measures, it's also challenging,"
cs-410_3_6_31,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:07,160","00:02:11,680",31,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=127,accurately reflect
cs-410_3_6_32,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:11,680","00:02:15,430",32,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=131,We have to consider carefully
cs-410_3_6_33,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:15,430","00:02:18,530",33,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=135,And then design measures to measure that.
cs-410_3_6_34,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:18,530","00:02:21,482",34,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=138,If your measure is not
cs-410_3_6_35,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:21,482","00:02:23,820",35,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=141,then your conclusion would be misled.
cs-410_3_6_36,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:23,820","00:02:25,040",36,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=143,So it's very important.
cs-410_3_6_37,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:26,880","00:02:29,290",37,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=146,So we're going to talk about
cs-410_3_6_38,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:29,290","00:02:31,360",38,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=149,One is the statistical significance test.
cs-410_3_6_39,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:31,360","00:02:36,350",39,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=151,And this also is a reason why
cs-410_3_6_40,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:36,350","00:02:41,060",40,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=156,And the question here is how sure can
cs-410_3_6_41,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:41,060","00:02:44,800",41,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=161,doesn't simply result from
cs-410_3_6_42,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:44,800","00:02:49,770",42,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=164,So here are some sample results of
cs-410_3_6_43,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:49,770","00:02:53,320",43,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=169,System B into different experiments.
cs-410_3_6_44,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:53,320","00:02:57,540",44,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=173,"And you can see in the bottom,"
cs-410_3_6_45,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:57,540","00:03:02,668",45,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=177,"So the mean, if you look at the mean"
cs-410_3_6_46,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:02,668","00:03:08,300",46,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=182,of positions are exactly the same
cs-410_3_6_47,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:08,300","00:03:13,250",47,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=188,"So you can see this is 0.20,"
cs-410_3_6_48,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:13,250","00:03:18,520",48,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=193,And again here it's also 0.20 and
cs-410_3_6_49,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:18,520","00:03:23,440",49,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=198,"Yet, if you look at these exact average"
cs-410_3_6_50,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:23,440","00:03:29,810",50,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=203,"If you look at these numbers in detail,"
cs-410_3_6_51,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:29,810","00:03:35,090",51,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=209,you would feel that you can trust
cs-410_3_6_52,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:36,100","00:03:41,610",52,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=216,"In the another case, in the other case,"
cs-410_3_6_53,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:41,610","00:03:48,470",53,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=221,"So, why don't you take a look at all these"
cs-410_3_6_54,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:48,470","00:03:52,565",54,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=228,"So, if you look at the average,"
cs-410_3_6_55,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:52,565","00:03:56,660",55,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=232,"we can easily, say that well,"
cs-410_3_6_56,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:56,660","00:03:59,630",56,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=236,"So, after all it's 0.40 and"
cs-410_3_6_57,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:59,630","00:04:05,950",57,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=239,"this is twice as much as 0.20,"
cs-410_3_6_58,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:05,950","00:04:10,120",58,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=245,"But if you look at these two experiments,"
cs-410_3_6_59,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:11,150","00:04:16,170",59,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=251,"You will see that, we've been more"
cs-410_3_6_60,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:16,170","00:04:17,040",60,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=256,in experiment one.
cs-410_3_6_61,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:17,040","00:04:19,160",61,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=257,In this case.
cs-410_3_6_62,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:19,160","00:04:23,360",62,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=259,Because these numbers seem to be
cs-410_3_6_63,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:25,110","00:04:32,342",63,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=265,"Whereas in Experiment 2, we're not sure"
cs-410_3_6_64,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:32,342","00:04:38,000",64,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=272,after System A is better and
cs-410_3_6_65,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:39,335","00:04:43,790",65,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=279,"But yet if we look at only average,"
cs-410_3_6_66,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:45,750","00:04:47,640",66,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=285,"So, what do you think?"
cs-410_3_6_67,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:49,170","00:04:54,150",67,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=289,"How reliable is our conclusion,"
cs-410_3_6_68,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:55,940","00:04:59,430",68,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=295,"Now in this case, intuitively,"
cs-410_3_6_69,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:01,020","00:05:04,630",69,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=301,But how can we quantitate
cs-410_3_6_70,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:04,630","00:05:08,430",70,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=304,And this is why we need to do
cs-410_3_6_71,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:09,440","00:05:13,910",71,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=309,"So, the idea of the statistical"
cs-410_3_6_72,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:13,910","00:05:18,330",72,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=313,assess the variants across
cs-410_3_6_73,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:18,330","00:05:21,160",73,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=318,"If there is a big variance,"
cs-410_3_6_74,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:21,160","00:05:25,880",74,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=321,that means the results could fluctuate
cs-410_3_6_75,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:25,880","00:05:30,740",75,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=325,"Then we should believe that,"
cs-410_3_6_76,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:30,740","00:05:35,210",76,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=330,the results might change if we
cs-410_3_6_77,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:35,210","00:05:39,350",77,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=335,"Right, so this is then not so"
cs-410_3_6_78,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:39,350","00:05:42,660",78,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=339,if you have c high variance
cs-410_3_6_79,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:43,660","00:05:49,390",79,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=343,So let's look at these results
cs-410_3_6_80,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:49,390","00:05:54,200",80,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=349,"So, here we show two different"
cs-410_3_6_81,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:54,200","00:05:57,470",81,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=354,One is a sign test where
cs-410_3_6_82,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:57,470","00:06:01,260",82,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=357,"If System B is better than System A,"
cs-410_3_6_83,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:01,260","00:06:05,400",83,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=361,When System A is better we
cs-410_3_6_84,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:05,400","00:06:09,600",84,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=365,"Using this case, if you see this,"
cs-410_3_6_85,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:09,600","00:06:12,980",85,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=369,We actually have four cases
cs-410_3_6_86,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:12,980","00:06:16,671",86,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=372,"But three cases of System A is better,"
cs-410_3_6_87,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:16,671","00:06:19,880",87,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=376,"this is almost like a random results,"
cs-410_3_6_88,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:19,880","00:06:25,880",88,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=379,So if you just take a random
cs-410_3_6_89,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:25,880","00:06:30,090",89,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=385,if you use plus to denote the head and
cs-410_3_6_90,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:30,090","00:06:34,920",90,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=390,that could easily be the results of just
cs-410_3_6_91,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:34,920","00:06:39,700",91,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=394,"So, the fact that the average is"
cs-410_3_6_92,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:39,700","00:06:41,330",92,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=399,We can't reliably conclude that.
cs-410_3_6_93,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:41,330","00:06:45,890",93,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=401,And this can be quantitatively
cs-410_3_6_94,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:45,890","00:06:48,380",94,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=405,And that basically means
cs-410_3_6_95,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:49,660","00:06:54,480",95,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=409,the probability that this result is
cs-410_3_6_96,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:54,480","00:06:56,140",96,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=414,"In this case, probability is 1.0."
cs-410_3_6_97,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:56,140","00:07:00,050",97,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=416,It means it surely is
cs-410_3_6_98,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:01,310","00:07:06,470",98,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=421,"Now in Willcoxan test,"
cs-410_3_6_99,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:06,470","00:07:09,430",99,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=426,and we would be not only
cs-410_3_6_100,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:09,430","00:07:12,520",100,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=429,we'll be also looking at
cs-410_3_6_101,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:12,520","00:07:14,690",101,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=432,"But we can draw a similar conclusion,"
cs-410_3_6_102,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:14,690","00:07:18,630",102,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=434,where you say it's very
cs-410_3_6_103,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:18,630","00:07:22,395",103,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=438,"To illustrate this, let's think"
cs-410_3_6_104,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:22,395","00:07:23,895",104,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=442,And this is called a now distribution.
cs-410_3_6_105,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:23,895","00:07:26,085",105,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=443,We assume that the mean is zero here.
cs-410_3_6_106,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:26,085","00:07:28,705",106,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=446,Lets say we started with
cs-410_3_6_107,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:28,705","00:07:31,405",107,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=448,no difference between the two systems.
cs-410_3_6_108,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:31,405","00:07:35,230",108,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=451,But we assume that because of random
cs-410_3_6_109,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:35,230","00:07:37,190",109,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=455,we might observe a difference.
cs-410_3_6_110,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:37,190","00:07:41,300",110,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=457,So the actual difference might
cs-410_3_6_111,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:41,300","00:07:42,830",111,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=461,"on the right side here, right?"
cs-410_3_6_112,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:43,920","00:07:48,102",112,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=463,"So, and this curve kind of shows"
cs-410_3_6_113,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:48,102","00:07:52,290",113,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=468,actually observe values that
cs-410_3_6_114,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:53,770","00:07:59,440",114,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=473,"Now, so if we look at this picture then,"
cs-410_3_6_115,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:01,070","00:08:05,530",115,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=481,"if a difference is observed here, then"
cs-410_3_6_116,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:05,530","00:08:11,180",116,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=485,the chance is very high that this is
cs-410_3_6_117,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:11,180","00:08:16,150",117,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=491,We can define a region of
cs-410_3_6_118,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:16,150","00:08:21,890",118,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=496,random fluctuation and
cs-410_3_6_119,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:21,890","00:08:27,894",119,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=501,And in this then the observed may
cs-410_3_6_120,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:28,960","00:08:34,830",120,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=508,But if you observe a value in this
cs-410_3_6_121,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:34,830","00:08:39,880",121,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=514,then the difference is unlikely
cs-410_3_6_122,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:39,880","00:08:44,460",122,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=519,"All right, so there's a very small"
cs-410_3_6_123,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:44,460","00:08:47,400",123,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=524,such a difference just because
cs-410_3_6_124,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:48,400","00:08:52,800",124,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=528,"So in that case, we can then conclude"
cs-410_3_6_125,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:52,800","00:08:54,670",125,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=532,So System B is indeed better.
cs-410_3_6_126,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:56,120","00:08:59,550",126,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=536,So this is the idea of
cs-410_3_6_127,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:59,550","00:09:03,870",127,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=539,The takeaway message here is that you
cs-410_3_6_128,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:03,870","00:09:05,770",128,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=543,jumping into a conclusion.
cs-410_3_6_129,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:05,770","00:09:08,330",129,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=545,"As in this case,"
cs-410_3_6_130,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:09,790","00:09:13,259",130,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=549,There are many different ways of doing
cs-410_3_6_131,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:15,260","00:09:20,270",131,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=555,"So now, let's talk about the other"
cs-410_3_6_132,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:20,270","00:09:24,590",132,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=560,"as we said earlier,"
cs-410_3_6_133,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:24,590","00:09:27,700",133,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=564,completely unless it's
cs-410_3_6_134,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:27,700","00:09:31,530",134,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=567,"So the question is,"
cs-410_3_6_135,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:31,530","00:09:33,880",135,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=571,"in the collection,"
cs-410_3_6_136,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:35,000","00:09:38,230",136,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=575,And the solution here is Pooling.
cs-410_3_6_137,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:38,230","00:09:45,640",137,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=578,And this is a strategy that has been used
cs-410_3_6_138,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:46,710","00:09:49,800",138,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=586,So the idea of Pooling is the following.
cs-410_3_6_139,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:49,800","00:09:54,410",139,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=589,We would first choose a diverse
cs-410_3_6_140,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:54,410","00:09:56,010",140,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=594,These are Text Retrieval systems.
cs-410_3_6_141,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:57,130","00:10:02,830",141,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=597,And we hope these methods can help us
cs-410_3_6_142,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:02,830","00:10:05,400",142,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=602,So the goal is to pick out
cs-410_3_6_143,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:05,400","00:10:08,770",143,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=605,We want to make judgements on relevant
cs-410_3_6_144,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:08,770","00:10:12,720",144,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=608,useful documents from users perspectives.
cs-410_3_6_145,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:12,720","00:10:16,339",145,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=612,So then we're going to have
cs-410_3_6_146,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:17,380","00:10:19,720",146,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=617,The K can vary from systems.
cs-410_3_6_147,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:19,720","00:10:24,370",147,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=619,But the point is to ask them to suggest
cs-410_3_6_148,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:25,530","00:10:29,780",148,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=625,And then we simply combine
cs-410_3_6_149,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:29,780","00:10:34,478",149,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=629,to form a pool of documents for
cs-410_3_6_150,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:34,478","00:10:41,370",150,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=634,"To judge, so imagine you have many"
cs-410_3_6_151,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:41,370","00:10:44,498",151,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=641,"We take the top-K documents,"
cs-410_3_6_152,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:44,498","00:10:48,060",152,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=644,"Now, of course, there are many"
cs-410_3_6_153,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:48,060","00:10:51,860",153,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=648,many systems might have retrieved
cs-410_3_6_154,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:51,860","00:10:55,250",154,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=651,So there will be some duplicate documents.
cs-410_3_6_155,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:56,480","00:11:00,690",155,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=656,And there are also unique documents
cs-410_3_6_156,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:00,690","00:11:03,490",156,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=660,So the idea of having diverse
cs-410_3_6_157,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:03,490","00:11:07,470",157,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=663,set of ranking methods is to
cs-410_3_6_158,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:07,470","00:11:11,140",158,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=667,And can include as many possible
cs-410_3_6_159,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:12,360","00:11:17,180",159,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=672,"And then, the users would,"
cs-410_3_6_160,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:17,180","00:11:21,250",160,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=677,"the judgments on this data set, this pool."
cs-410_3_6_161,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:21,250","00:11:26,710",161,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=681,And the other unjudged the documents are
cs-410_3_6_162,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:26,710","00:11:30,900",162,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=686,"Now if the pool is large enough,"
cs-410_3_6_163,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:32,080","00:11:38,600",163,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=692,"But if the pool is not very large,"
cs-410_3_6_164,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:38,600","00:11:41,190",164,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=698,And we might use other
cs-410_3_6_165,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:41,190","00:11:46,100",165,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=701,there are indeed other
cs-410_3_6_166,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:46,100","00:11:49,840",166,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=706,And such a strategy is generally okay for
cs-410_3_6_167,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:49,840","00:11:54,740",167,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=709,comparing systems that
cs-410_3_6_168,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:54,740","00:11:57,740",168,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=714,That means if you participate
cs-410_3_6_169,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:57,740","00:12:00,850",169,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=717,then it's unlikely that it
cs-410_3_6_170,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:00,850","00:12:03,100",170,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=720,because the problematic
cs-410_3_6_171,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:04,300","00:12:07,060",171,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=724,"However, this is problematic for"
cs-410_3_6_172,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:07,060","00:12:11,880",172,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=727,evaluating a new system that may
cs-410_3_6_173,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:11,880","00:12:16,010",173,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=731,"In this case, a new system might"
cs-410_3_6_174,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:16,010","00:12:20,850",174,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=736,nominated some read only documents
cs-410_3_6_175,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:20,850","00:12:24,370",175,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=740,So those documents might be
cs-410_3_6_176,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:24,370","00:12:26,150",176,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=744,That's unfair.
cs-410_3_6_177,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:26,150","00:12:32,810",177,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=746,So to summarize the whole part of textual
cs-410_3_6_178,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:32,810","00:12:37,150",178,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=752,Because the problem is the empirically
cs-410_3_6_179,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:38,450","00:12:42,470",179,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=758,"don't rely on users, there's no way to"
cs-410_3_6_180,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:43,580","00:12:46,600",180,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=763,If we have in the property
cs-410_3_6_181,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:46,600","00:12:49,710",181,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=766,we might misguide our research or
cs-410_3_6_182,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:49,710","00:12:52,470",182,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=769,And we might just draw wrong conclusions.
cs-410_3_6_183,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:52,470","00:12:55,250",183,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=772,And we have seen this is
cs-410_3_6_184,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:55,250","00:12:58,190",184,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=775,So make sure to get it right for
cs-410_3_6_185,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:00,150","00:13:03,400",185,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=780,The main methodology is the Cranfield
cs-410_3_6_186,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:03,400","00:13:08,230",186,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=783,And they are the main paradigm used in
cs-410_3_6_187,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:08,230","00:13:10,820",187,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=788,not just a search engine variation.
cs-410_3_6_188,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:10,820","00:13:16,020",188,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=790,Map and nDCG are the two main
cs-410_3_6_189,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:16,020","00:13:19,530",189,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=796,know about and they are appropriate for
cs-410_3_6_190,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:19,530","00:13:22,950",190,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=799,You will see them often
cs-410_3_6_191,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:22,950","00:13:27,080",191,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=802,Precision at 10 documents is easier
cs-410_3_6_192,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:27,080","00:13:28,500",192,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=807,So that's also often useful.
cs-410_3_6_193,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:30,580","00:13:37,610",193,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=810,What's not covered is some other
cs-410_3_6_194,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:37,610","00:13:43,720",194,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=817,"Where the system would mix two,"
cs-410_3_6_195,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:43,720","00:13:46,580",195,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=823,And then would show
cs-410_3_6_196,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:46,580","00:13:49,780",196,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=826,"Of course, the users don't see"
cs-410_3_6_197,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:49,780","00:13:52,410",197,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=829,The users would judge those results or
cs-410_3_6_198,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:52,410","00:13:58,096",198,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=832,click on those documents in
cs-410_3_6_199,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:58,096","00:14:02,080",199,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=838,"In this case then, the search engine"
cs-410_3_6_200,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:02,080","00:14:07,250",200,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=842,see if one method has contributed
cs-410_3_6_201,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:07,250","00:14:11,570",201,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=847,"If the user tends to click on one,"
cs-410_3_6_202,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:13,050","00:14:17,730",202,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=853,then it suggests that
cs-410_3_6_203,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:17,730","00:14:21,008",203,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=857,So this is what leverages the real users
cs-410_3_6_204,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:21,008","00:14:25,640",204,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=861,It's called A-B Test and
cs-410_3_6_205,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:25,640","00:14:29,370",205,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=865,the modern search engines or
cs-410_3_6_206,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:29,370","00:14:32,590",206,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=869,Another way to evaluate IR or
cs-410_3_6_207,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:32,590","00:14:36,020",207,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=872,textual retrieval is user studies and
cs-410_3_6_208,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:36,020","00:14:39,390",208,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=876,I've put some references here
cs-410_3_6_209,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:39,390","00:14:40,260",209,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=879,to know more about that.
cs-410_3_6_210,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:41,760","00:14:44,180",210,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=881,"So, there are three"
cs-410_3_6_211,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:44,180","00:14:49,280",211,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=884,These are three mini books about
cs-410_3_6_212,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:49,280","00:14:54,280",212,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=889,in covering a broad review of
cs-410_3_6_213,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:54,280","00:14:58,237",213,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=894,And it covers some of the things
cs-410_3_6_214,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:58,237","00:15:01,085",214,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=898,they also have a lot of others to offer.
cs-410_3_6_215,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:15:02,777","00:15:12,777",215,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=902,[MUSIC]
cs-410_4_1_1,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:00,086","00:00:07,516",1,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=0,[SOUND]
cs-410_4_1_2,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:07,516","00:00:10,282",2,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=7,lecture is about
cs-410_4_1_3,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:10,282","00:00:11,805",3,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=10,"In this lecture,"
cs-410_4_1_4,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:11,805","00:00:17,806",4,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=11,we're going to continue the discussion
cs-410_4_1_5,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:17,806","00:00:22,942",5,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=17,We're going to look at another kind of
cs-410_4_1_6,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:22,942","00:00:27,584",6,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=22,functions than the Vector Space Model
cs-410_4_1_7,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:32,146","00:00:36,589",7,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=32,"In probabilistic models,"
cs-410_4_1_8,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:36,589","00:00:41,822",8,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=36,based on the probability that this
cs-410_4_1_9,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:41,822","00:00:46,802",9,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=41,"In other words, we introduce"
cs-410_4_1_10,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:46,802","00:00:51,400",10,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=46,This is the variable R here.
cs-410_4_1_11,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:51,400","00:00:54,520",11,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=51,And we also assume that the query and
cs-410_4_1_12,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:54,520","00:00:59,830",12,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=54,the documents are all observations
cs-410_4_1_13,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:00,920","00:01:05,810",13,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=60,"Note that in the vector-based models,"
cs-410_4_1_14,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:05,810","00:01:11,120",14,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=65,here we assume they are the data
cs-410_4_1_15,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:11,120","00:01:17,940",15,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=71,"And so, the problem of retrieval becomes"
cs-410_4_1_16,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:19,490","00:01:23,060",16,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=79,"In this category of models,"
cs-410_4_1_17,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:23,060","00:01:27,130",17,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=83,The classic probabilistic model has
cs-410_4_1_18,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:27,130","00:01:30,150",18,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=87,which we discussed in in
cs-410_4_1_19,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:30,150","00:01:33,570",19,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=90,because its a form is actually
cs-410_4_1_20,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:35,260","00:01:40,180",20,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=95,"In this lecture,"
cs-410_4_1_21,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:41,230","00:01:45,550",21,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=101,P class called a language
cs-410_4_1_22,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:45,550","00:01:50,150",22,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=105,"In particular, we're going to discuss"
cs-410_4_1_23,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:51,370","00:01:55,330",23,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=111,which is one of the most effective
cs-410_4_1_24,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:57,050","00:02:01,840",24,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=117,There was also another line called
cs-410_4_1_25,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:01,840","00:02:04,970",25,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=121,"which has led to the PL2 function,"
cs-410_4_1_26,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:06,440","00:02:11,070",26,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=126,it's also one of the most effective
cs-410_4_1_27,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:11,070","00:02:16,847",27,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=131,"In query likelihood, our assumption"
cs-410_4_1_28,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:16,847","00:02:23,002",28,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=136,can be approximated by the probability
cs-410_4_1_29,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:23,002","00:02:29,656",29,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=143,"So intuitively, this probability just"
cs-410_4_1_30,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:29,656","00:02:34,808",30,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=149,"And that is if a user likes document d,"
cs-410_4_1_31,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:34,808","00:02:40,220",31,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=154,"the user enter query q ,in"
cs-410_4_1_32,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:40,220","00:02:47,680",32,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=160,"So we assume that the user likes d,"
cs-410_4_1_33,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:47,680","00:02:52,610",33,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=167,And then we ask the question about how
cs-410_4_1_34,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:52,610","00:02:53,250",34,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=172,from this user?
cs-410_4_1_35,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:54,890","00:02:56,508",35,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=174,So this is the basic idea.
cs-410_4_1_36,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:56,508","00:03:00,676",36,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=176,"Now, to understand this idea,"
cs-410_4_1_37,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:00,676","00:03:03,741",37,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=180,the basic idea of
cs-410_4_1_38,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:03,741","00:03:09,150",38,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=183,"So here, I listed some imagined"
cs-410_4_1_39,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:09,150","00:03:13,599",39,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=189,relevance judgments of queries and
cs-410_4_1_40,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:13,599","00:03:17,576",40,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=193,"For example, in this line,"
cs-410_4_1_41,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:17,576","00:03:24,546",41,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=197,it shows that q1 is a query
cs-410_4_1_42,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:24,546","00:03:28,043",42,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=204,And d1 is a document
cs-410_4_1_43,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:28,043","00:03:33,036",43,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=208,And 1 means the user thinks
cs-410_4_1_44,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:33,036","00:03:38,685",44,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=213,So this R here can be also approximated
cs-410_4_1_45,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:38,685","00:03:44,810",45,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=218,engine can collect by watching how you
cs-410_4_1_46,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:44,810","00:03:47,990",46,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=224,"So in this case, let's say"
cs-410_4_1_47,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:47,990","00:03:49,000",47,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=227,So there's a 1 here.
cs-410_4_1_48,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:50,080","00:03:56,480",48,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=230,"Similarly, the user clicked on d2 also,"
cs-410_4_1_49,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:56,480","00:03:59,630",49,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=236,"In other words,"
cs-410_4_1_50,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:00,700","00:04:05,080",50,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=240,"On the other hand,"
cs-410_4_1_51,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:07,430","00:04:13,485",51,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=247,And d4 is non-relevant and then d5 is
cs-410_4_1_52,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:13,485","00:04:17,860",52,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=253,"And this part, maybe,"
cs-410_4_1_53,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:17,860","00:04:23,009",53,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=257,So this user typed in q1 and then found
cs-410_4_1_54,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:23,009","00:04:26,170",54,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=263,so d1 is actually non-relevant.
cs-410_4_1_55,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:26,170","00:04:31,124",55,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=266,"In contrast, here we see it's relevant."
cs-410_4_1_56,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:31,124","00:04:38,401",56,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=271,Or this could be the same query typed
cs-410_4_1_57,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:38,401","00:04:42,660",57,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=278,"But d2 is also relevant, etc."
cs-410_4_1_58,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:42,660","00:04:47,050",58,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=282,"And then here,"
cs-410_4_1_59,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:48,390","00:04:50,870",59,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=288,"Now, we can imagine we"
cs-410_4_1_60,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:52,940","00:04:54,740",60,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=292,"Now we can ask the question,"
cs-410_4_1_61,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:54,740","00:04:58,460",61,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=294,how can we then estimate
cs-410_4_1_62,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:00,390","00:05:03,690",62,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=300,So how can we compute this
cs-410_4_1_63,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:03,690","00:05:06,230",63,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=303,"Well, intuitively that just means"
cs-410_4_1_64,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:06,230","00:05:10,770",64,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=306,if we look at all the entries
cs-410_4_1_65,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:10,770","00:05:16,010",65,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=310,"this particular q, how likely we'll"
cs-410_4_1_66,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:16,010","00:05:18,500",66,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=316,So basically that just means that
cs-410_4_1_67,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:19,730","00:05:24,536",67,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=319,We can first count how many
cs-410_4_1_68,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:24,536","00:05:29,576",68,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=324,d as a pair in this table and
cs-410_4_1_69,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:29,576","00:05:34,518",69,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=329,we actually have also seen
cs-410_4_1_70,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:34,518","00:05:37,227",70,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=334,"And then, we just compute the ratio."
cs-410_4_1_71,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:39,409","00:05:42,347",71,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=339,So let's take a look at
cs-410_4_1_72,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:42,347","00:05:48,466",72,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=342,Suppose we are trying to compute this
cs-410_4_1_73,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:48,466","00:05:52,240",73,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=348,What is the estimated probability?
cs-410_4_1_74,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:52,240","00:05:54,760",74,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=352,"Now, think about that."
cs-410_4_1_75,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:54,760","00:05:58,823",75,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=354,You can pause the video if needed.
cs-410_4_1_76,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:58,823","00:06:01,560",76,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=358,Try to take a look at the table.
cs-410_4_1_77,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:01,560","00:06:04,606",77,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=361,And try to give your
cs-410_4_1_78,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:07,069","00:06:11,802",78,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=367,"Have you seen that,"
cs-410_4_1_79,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:11,802","00:06:15,050",79,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=371,we'll be looking at these two pairs?
cs-410_4_1_80,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:15,050","00:06:18,020",80,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=375,"And in both cases, well,"
cs-410_4_1_81,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:18,020","00:06:23,190",81,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=378,"actually, in one of the cases, the user"
cs-410_4_1_82,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:23,190","00:06:26,282",82,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=383,So R = 1 in only one of the two cases.
cs-410_4_1_83,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:26,282","00:06:28,244",83,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=386,"In the other case, it's 0."
cs-410_4_1_84,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:28,244","00:06:30,846",84,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=388,So that's one out of two.
cs-410_4_1_85,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:30,846","00:06:34,525",85,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=390,What about the d1 and the d2?
cs-410_4_1_86,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:34,525","00:06:39,127",86,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=394,"Well, they are here, d1 and d2, d1 and d2,"
cs-410_4_1_87,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:39,127","00:06:42,729",87,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=399,"in both cases, in this case, R = 1."
cs-410_4_1_88,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:42,729","00:06:45,700",88,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=402,So it's a two out of two and
cs-410_4_1_89,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:45,700","00:06:48,195",89,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=405,"So you can see with this approach,"
cs-410_4_1_90,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:48,195","00:06:52,679",90,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=408,we can actually score these documents for
cs-410_4_1_91,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:52,679","00:06:56,625",91,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=412,"We now have a score for d1,"
cs-410_4_1_92,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:56,625","00:07:00,334",92,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=416,And we can simply rank them
cs-410_4_1_93,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:00,334","00:07:04,056",93,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=420,so that's the basic idea
cs-410_4_1_94,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:04,056","00:07:06,971",94,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=424,"And you can see it makes a lot of sense,"
cs-410_4_1_95,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:06,971","00:07:10,036",95,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=426,it's going to rank d2 above
cs-410_4_1_96,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:10,036","00:07:15,992",96,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=430,"Because in all the cases,"
cs-410_4_1_97,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:15,992","00:07:18,314",97,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=435,The user clicked on this document.
cs-410_4_1_98,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:18,314","00:07:23,957",98,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=438,So this also should show that
cs-410_4_1_99,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:23,957","00:07:30,830",99,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=443,a search engine can learn a lot from
cs-410_4_1_100,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:30,830","00:07:33,580",100,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=450,This is a simple example
cs-410_4_1_101,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:33,580","00:07:38,760",101,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=453,with small amount of entries here we can
cs-410_4_1_102,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:38,760","00:07:42,160",102,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=458,These probabilities would give us
cs-410_4_1_103,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:42,160","00:07:46,160",103,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=462,might be more relevant or more useful
cs-410_4_1_104,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:47,170","00:07:51,100",104,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=467,"Now, of course, the problems that we"
cs-410_4_1_105,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:51,100","00:07:54,048",105,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=471,all the documents and
cs-410_4_1_106,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:55,320","00:07:57,890",106,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=475,"There would be a lot of unseen documents,"
cs-410_4_1_107,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:57,890","00:08:02,880",107,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=477,we have only collected the data from the
cs-410_4_1_108,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:02,880","00:08:07,370",108,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=482,And there are even more unseen queries
cs-410_4_1_109,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:07,370","00:08:10,060",109,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=487,queries will be typed in by users.
cs-410_4_1_110,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:10,060","00:08:15,090",110,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=490,"So obviously,"
cs-410_4_1_111,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:15,090","00:08:17,190",111,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=495,it to unseen queries or unseen documents.
cs-410_4_1_112,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:18,635","00:08:22,278",112,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=498,"Nevertheless, this shows the basic idea"
cs-410_4_1_113,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:22,278","00:08:23,646",113,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=502,it makes sense intuitively.
cs-410_4_1_114,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:23,646","00:08:28,275",114,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=503,So what do we do in such a case when
cs-410_4_1_115,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:28,275","00:08:29,508",115,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=508,unseen queries?
cs-410_4_1_116,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:29,508","00:08:32,818",116,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=509,"Well, the solutions that we have"
cs-410_4_1_117,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:32,818","00:08:37,003",117,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=512,So in this particular case called
cs-410_4_1_118,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:37,003","00:08:40,784",118,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=517,we just approximate this by
cs-410_4_1_119,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:40,784","00:08:46,682",119,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=520,"p(q given d, R=1)."
cs-410_4_1_120,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:46,682","00:08:51,539",120,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=526,"So in the condition part, we assume that"
cs-410_4_1_121,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:51,539","00:08:54,640",121,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=531,have seen that the user
cs-410_4_1_122,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:56,190","00:08:58,777",122,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=536,And this part shows that
cs-410_4_1_123,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:58,777","00:09:01,438",123,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=538,likely the user would
cs-410_4_1_124,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:01,438","00:09:04,653",124,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=541,How likely we will see this
cs-410_4_1_125,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:04,653","00:09:08,880",125,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=544,"So note that here, we have made"
cs-410_4_1_126,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:08,880","00:09:13,900",126,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=548,"Basically, we're going to do, assume that"
cs-410_4_1_127,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:13,900","00:09:17,970",127,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=553,has something to do with whether
cs-410_4_1_128,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:17,970","00:09:20,740",128,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=557,"In other words,"
cs-410_4_1_129,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:22,160","00:09:27,671",129,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=562,And that is a user formulates a query
cs-410_4_1_130,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:27,671","00:09:30,358",130,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=567,Where if you just look at this
cs-410_4_1_131,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:30,358","00:09:32,629",131,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=570,it's not obvious we
cs-410_4_1_132,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:32,629","00:09:37,941",132,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=572,So what I really meant is that
cs-410_4_1_133,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:37,941","00:09:43,367",133,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=577,"probability to help us score,"
cs-410_4_1_134,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:43,367","00:09:48,794",134,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=583,probability will have to somehow
cs-410_4_1_135,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:48,794","00:09:54,696",135,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=588,conditional probability without
cs-410_4_1_136,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:54,696","00:09:59,306",136,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=594,Otherwise we would be having
cs-410_4_1_137,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:59,306","00:10:04,537",137,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=599,"by making this assumption,"
cs-410_4_1_138,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:04,537","00:10:09,252",138,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=604,and try to just model how the user
cs-410_4_1_139,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:09,252","00:10:13,639",139,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=609,So this is how you can
cs-410_4_1_140,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:13,639","00:10:18,570",140,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=613,that we can derive a specific
cs-410_4_1_141,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:18,570","00:10:22,020",141,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=618,So let's look at how this model work for
cs-410_4_1_142,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:22,020","00:10:23,300",142,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=622,"And basically,"
cs-410_4_1_143,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:23,300","00:10:27,420",143,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=623,what we are going to do in this case
cs-410_4_1_144,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:27,420","00:10:30,760",144,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=627,Which of these documents is most
cs-410_4_1_145,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:30,760","00:10:34,300",145,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=630,document in the user's mind when
cs-410_4_1_146,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:34,300","00:10:38,488",146,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=634,So we ask this question and we quantify
cs-410_4_1_147,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:38,488","00:10:43,443",147,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=638,a conditional probability of observing
cs-410_4_1_148,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:43,443","00:10:47,245",148,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=643,fact the imaginary relevant
cs-410_4_1_149,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:47,245","00:10:51,885",149,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=647,Here you can see we've computed all
cs-410_4_1_150,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:51,885","00:10:55,340",150,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=651,The likelihood of queries
cs-410_4_1_151,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:55,340","00:10:56,880",151,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=655,"Once we have these values,"
cs-410_4_1_152,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:56,880","00:11:00,370",152,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=656,we can then rank these documents
cs-410_4_1_153,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:00,370","00:11:05,420",153,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=660,"So to summarize, the general idea"
cs-410_4_1_154,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:05,420","00:11:11,740",154,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=665,risk model is to assume the we introduce
cs-410_4_1_155,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:11,740","00:11:12,690",155,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=671,"And then,"
cs-410_4_1_156,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:12,690","00:11:16,740",156,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=672,let the scoring function be defined
cs-410_4_1_157,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:16,740","00:11:20,980",157,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=676,We also talked about approximating
cs-410_4_1_158,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:22,450","00:11:27,065",158,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=682,And in this case we have a ranking
cs-410_4_1_159,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:27,065","00:11:31,385",159,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=687,based on the probability of
cs-410_4_1_160,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:31,385","00:11:36,165",160,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=691,And this probability should be interpreted
cs-410_4_1_161,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:36,165","00:11:39,236",161,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=696,"likes document d, would pose query q."
cs-410_4_1_162,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:40,265","00:11:44,645",162,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=700,"Now, the question of course is, how do"
cs-410_4_1_163,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:44,645","00:11:49,500",163,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=704,At this in general has to do with how
cs-410_4_1_164,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:49,500","00:11:51,980",164,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=709,because q is a text.
cs-410_4_1_165,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:51,980","00:11:56,560",165,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=711,And this has to do with a model
cs-410_4_1_166,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:56,560","00:12:00,580",166,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=716,And these kind of models
cs-410_4_1_167,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:02,190","00:12:07,440",167,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=722,"So more specifically, we will be"
cs-410_4_1_168,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:07,440","00:12:12,050",168,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=727,conditional probability
cs-410_4_1_169,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:12,050","00:12:18,463",169,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=732,"If the user liked this document,"
cs-410_4_1_170,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:18,463","00:12:21,884",170,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=738,"And in the next lecture we're going to do,"
cs-410_4_1_171,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:21,884","00:12:27,016",171,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=741,giving introduction to language
cs-410_4_1_172,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:27,016","00:12:32,063",172,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=747,can model text that was a probable
cs-410_4_1_173,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:32,063","00:12:42,063",173,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=752,[MUSIC]
cs-410_4_2_1,cs-410,4,2, Statistical Language Model,"00:00:07,780","00:00:12,596",1,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=7,[SOUND] This lecture is about
cs-410_4_2_2,cs-410,4,2, Statistical Language Model,"00:00:12,596","00:00:13,737",2,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=12,"In this lecture,"
cs-410_4_2_3,cs-410,4,2, Statistical Language Model,"00:00:13,737","00:00:18,445",3,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=13,we're going to give an introduction
cs-410_4_2_4,cs-410,4,2, Statistical Language Model,"00:00:18,445","00:00:23,305",4,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=18,This has to do with how do you model
cs-410_4_2_5,cs-410,4,2, Statistical Language Model,"00:00:23,305","00:00:28,272",5,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=23,So it's related to how we model
cs-410_4_2_6,cs-410,4,2, Statistical Language Model,"00:00:31,828","00:00:34,032",6,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=31,We're going to talk about
cs-410_4_2_7,cs-410,4,2, Statistical Language Model,"00:00:34,032","00:00:37,688",7,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=34,And then we're going to talk about the
cs-410_4_2_8,cs-410,4,2, Statistical Language Model,"00:00:37,688","00:00:42,770",8,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=37,"language model, which also happens to be"
cs-410_4_2_9,cs-410,4,2, Statistical Language Model,"00:00:42,770","00:00:45,420",9,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=42,"And finally, what this class"
cs-410_4_2_10,cs-410,4,2, Statistical Language Model,"00:00:47,200","00:00:48,750",10,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=47,What is a language model?
cs-410_4_2_11,cs-410,4,2, Statistical Language Model,"00:00:48,750","00:00:53,570",11,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=48,"Well, it's just a probability"
cs-410_4_2_12,cs-410,4,2, Statistical Language Model,"00:00:53,570","00:00:54,540",12,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=53,"So here, I'll show one."
cs-410_4_2_13,cs-410,4,2, Statistical Language Model,"00:00:55,870","00:01:00,430",13,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=55,This model gives the sequence Today
cs-410_4_2_14,cs-410,4,2, Statistical Language Model,"00:01:00,430","00:01:03,830",14,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=60,"It give Today Wednesday is a very,"
cs-410_4_2_15,cs-410,4,2, Statistical Language Model,"00:01:03,830","00:01:09,705",15,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=63,very small probability
cs-410_4_2_16,cs-410,4,2, Statistical Language Model,"00:01:11,796","00:01:15,447",16,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=71,You can see the probabilities
cs-410_4_2_17,cs-410,4,2, Statistical Language Model,"00:01:15,447","00:01:19,670",17,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=75,sequences of words can vary
cs-410_4_2_18,cs-410,4,2, Statistical Language Model,"00:01:19,670","00:01:23,256",18,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=79,"Therefore, it's clearly context dependent."
cs-410_4_2_19,cs-410,4,2, Statistical Language Model,"00:01:23,256","00:01:24,552",19,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=83,"In ordinary conversation,"
cs-410_4_2_20,cs-410,4,2, Statistical Language Model,"00:01:24,552","00:01:28,510",20,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=84,probably Today is Wednesday is most
cs-410_4_2_21,cs-410,4,2, Statistical Language Model,"00:01:28,510","00:01:32,132",21,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=88,Imagine in the context of
cs-410_4_2_22,cs-410,4,2, Statistical Language Model,"00:01:32,132","00:01:36,890",22,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=92,"maybe the eigenvalue is positive,"
cs-410_4_2_23,cs-410,4,2, Statistical Language Model,"00:01:36,890","00:01:41,080",23,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=96,This means it can be used to
cs-410_4_2_24,cs-410,4,2, Statistical Language Model,"00:01:42,240","00:01:45,900",24,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=102,The model can also be regarded
cs-410_4_2_25,cs-410,4,2, Statistical Language Model,"00:01:45,900","00:01:46,950",25,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=105,generating text.
cs-410_4_2_26,cs-410,4,2, Statistical Language Model,"00:01:46,950","00:01:51,660",26,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=106,And this is why it's also often
cs-410_4_2_27,cs-410,4,2, Statistical Language Model,"00:01:51,660","00:01:52,910",27,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=111,So what does that mean?
cs-410_4_2_28,cs-410,4,2, Statistical Language Model,"00:01:52,910","00:01:58,540",28,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=112,We can imagine this is a mechanism that's
cs-410_4_2_29,cs-410,4,2, Statistical Language Model,"00:01:58,540","00:02:05,340",29,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=118,visualised here as a stochastic system
cs-410_4_2_30,cs-410,4,2, Statistical Language Model,"00:02:05,340","00:02:08,608",30,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=125,"So, we can ask for a sequence,"
cs-410_4_2_31,cs-410,4,2, Statistical Language Model,"00:02:08,608","00:02:13,548",31,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=128,"a sequence from the device if you want,"
cs-410_4_2_32,cs-410,4,2, Statistical Language Model,"00:02:13,548","00:02:18,420",32,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=133,"Today is Wednesday, but it could"
cs-410_4_2_33,cs-410,4,2, Statistical Language Model,"00:02:18,420","00:02:21,940",33,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=138,"So for example,"
cs-410_4_2_34,cs-410,4,2, Statistical Language Model,"00:02:24,086","00:02:28,418",34,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=144,"So in this sense,"
cs-410_4_2_35,cs-410,4,2, Statistical Language Model,"00:02:28,418","00:02:32,656",35,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=148,a sample observed from
cs-410_4_2_36,cs-410,4,2, Statistical Language Model,"00:02:32,656","00:02:33,840",36,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=152,"So, why is such a model useful?"
cs-410_4_2_37,cs-410,4,2, Statistical Language Model,"00:02:33,840","00:02:39,720",37,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=153,"Well, it's mainly because it can quantify"
cs-410_4_2_38,cs-410,4,2, Statistical Language Model,"00:02:39,720","00:02:41,190",38,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=159,Where do uncertainties come from?
cs-410_4_2_39,cs-410,4,2, Statistical Language Model,"00:02:41,190","00:02:45,690",39,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=161,"Well, one source is simply"
cs-410_4_2_40,cs-410,4,2, Statistical Language Model,"00:02:45,690","00:02:48,870",40,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=165,that we discussed earlier in the lecture.
cs-410_4_2_41,cs-410,4,2, Statistical Language Model,"00:02:48,870","00:02:52,240",41,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=168,Another source is because we don't
cs-410_4_2_42,cs-410,4,2, Statistical Language Model,"00:02:52,240","00:02:55,300",42,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=172,we lack all the knowledge
cs-410_4_2_43,cs-410,4,2, Statistical Language Model,"00:02:55,300","00:02:58,420",43,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=175,"In that case,"
cs-410_4_2_44,cs-410,4,2, Statistical Language Model,"00:02:58,420","00:03:01,800",44,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=178,So let me show some examples of questions
cs-410_4_2_45,cs-410,4,2, Statistical Language Model,"00:03:01,800","00:03:06,220",45,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=181,that would have interesting
cs-410_4_2_46,cs-410,4,2, Statistical Language Model,"00:03:06,220","00:03:11,641",46,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=186,"Given that we see John and feels,"
cs-410_4_2_47,cs-410,4,2, Statistical Language Model,"00:03:11,641","00:03:16,866",47,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=191,as opposed to habit as the next
cs-410_4_2_48,cs-410,4,2, Statistical Language Model,"00:03:16,866","00:03:21,123",48,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=196,"Now, obviously, this would be very useful"
cs-410_4_2_49,cs-410,4,2, Statistical Language Model,"00:03:21,123","00:03:25,180",49,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=201,"habit would have similar acoustic sound,"
cs-410_4_2_50,cs-410,4,2, Statistical Language Model,"00:03:25,180","00:03:28,190",50,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=205,"But, if we look at the language model,"
cs-410_4_2_51,cs-410,4,2, Statistical Language Model,"00:03:28,190","00:03:32,690",51,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=208,we know that John feels happy would be
cs-410_4_2_52,cs-410,4,2, Statistical Language Model,"00:03:35,810","00:03:39,300",52,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=215,"Another example, given that we"
cs-410_4_2_53,cs-410,4,2, Statistical Language Model,"00:03:39,300","00:03:43,700",53,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=219,"game once in a news article,"
cs-410_4_2_54,cs-410,4,2, Statistical Language Model,"00:03:43,700","00:03:47,430",54,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=223,This obviously is related to text
cs-410_4_2_55,cs-410,4,2, Statistical Language Model,"00:03:48,720","00:03:52,150",55,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=228,"Also, given that a user is"
cs-410_4_2_56,cs-410,4,2, Statistical Language Model,"00:03:52,150","00:03:55,570",56,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=232,how likely would the user
cs-410_4_2_57,cs-410,4,2, Statistical Language Model,"00:03:55,570","00:03:58,530",57,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=235,"Now, this is clearly related"
cs-410_4_2_58,cs-410,4,2, Statistical Language Model,"00:03:58,530","00:04:00,185",58,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=238,that we discussed in the previous lecture.
cs-410_4_2_59,cs-410,4,2, Statistical Language Model,"00:04:02,180","00:04:05,710",59,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=242,"So now,"
cs-410_4_2_60,cs-410,4,2, Statistical Language Model,"00:04:05,710","00:04:07,910",60,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=245,called a unigram language model.
cs-410_4_2_61,cs-410,4,2, Statistical Language Model,"00:04:07,910","00:04:09,690",61,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=247,"In such a case,"
cs-410_4_2_62,cs-410,4,2, Statistical Language Model,"00:04:09,690","00:04:13,550",62,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=249,we assume that we generate a text by
cs-410_4_2_63,cs-410,4,2, Statistical Language Model,"00:04:14,760","00:04:19,356",63,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=254,So this means the probability of
cs-410_4_2_64,cs-410,4,2, Statistical Language Model,"00:04:19,356","00:04:22,601",64,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=259,the product of
cs-410_4_2_65,cs-410,4,2, Statistical Language Model,"00:04:22,601","00:04:25,800",65,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=262,"Now normally,"
cs-410_4_2_66,cs-410,4,2, Statistical Language Model,"00:04:25,800","00:04:30,270",66,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=265,So if you have single word in like
cs-410_4_2_67,cs-410,4,2, Statistical Language Model,"00:04:30,270","00:04:35,470",67,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=270,likely to observe model than if
cs-410_4_2_68,cs-410,4,2, Statistical Language Model,"00:04:35,470","00:04:37,780",68,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=275,So this assumption is not
cs-410_4_2_69,cs-410,4,2, Statistical Language Model,"00:04:37,780","00:04:39,920",69,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=277,we make this assumption
cs-410_4_2_70,cs-410,4,2, Statistical Language Model,"00:04:41,210","00:04:47,060",70,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=281,So now the model has precisely N
cs-410_4_2_71,cs-410,4,2, Statistical Language Model,"00:04:47,060","00:04:51,380",71,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=287,"We have one probability for each word, and"
cs-410_4_2_72,cs-410,4,2, Statistical Language Model,"00:04:51,380","00:04:57,450",72,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=291,"So strictly speaking,"
cs-410_4_2_73,cs-410,4,2, Statistical Language Model,"00:05:00,270","00:05:04,495",73,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=300,"As I said,"
cs-410_4_2_74,cs-410,4,2, Statistical Language Model,"00:05:04,495","00:05:06,245",74,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=304,drawn from this word distribution.
cs-410_4_2_75,cs-410,4,2, Statistical Language Model,"00:05:08,080","00:05:11,540",75,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=308,"So for example,"
cs-410_4_2_76,cs-410,4,2, Statistical Language Model,"00:05:11,540","00:05:18,020",76,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=311,the model to stochastically generate
cs-410_4_2_77,cs-410,4,2, Statistical Language Model,"00:05:18,020","00:05:19,988",77,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=318,"So instead of giving a whole sequence,"
cs-410_4_2_78,cs-410,4,2, Statistical Language Model,"00:05:19,988","00:05:23,900",78,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=319,"like Today is Wednesday,"
cs-410_4_2_79,cs-410,4,2, Statistical Language Model,"00:05:23,900","00:05:26,200",79,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=323,And we can get all kinds of words.
cs-410_4_2_80,cs-410,4,2, Statistical Language Model,"00:05:26,200","00:05:28,596",80,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=326,And we can assemble these
cs-410_4_2_81,cs-410,4,2, Statistical Language Model,"00:05:28,596","00:05:32,304",81,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=328,So that will still allow you
cs-410_4_2_82,cs-410,4,2, Statistical Language Model,"00:05:32,304","00:05:36,410",82,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=332,Today is Wednesday as the product
cs-410_4_2_83,cs-410,4,2, Statistical Language Model,"00:05:37,420","00:05:43,380",83,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=337,"As you can see, even though we have not"
cs-410_4_2_84,cs-410,4,2, Statistical Language Model,"00:05:43,380","00:05:48,630",84,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=343,it actually allows us to compute
cs-410_4_2_85,cs-410,4,2, Statistical Language Model,"00:05:48,630","00:05:53,550",85,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=348,this model now only needs N
cs-410_4_2_86,cs-410,4,2, Statistical Language Model,"00:05:53,550","00:05:56,370",86,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=353,That means if we specify
cs-410_4_2_87,cs-410,4,2, Statistical Language Model,"00:05:56,370","00:06:01,850",87,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=356,"all the words, then the model's"
cs-410_4_2_88,cs-410,4,2, Statistical Language Model,"00:06:01,850","00:06:06,220",88,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=361,"Whereas if we don't make this assumption,"
cs-410_4_2_89,cs-410,4,2, Statistical Language Model,"00:06:06,220","00:06:09,720",89,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=366,all kinds of combinations
cs-410_4_2_90,cs-410,4,2, Statistical Language Model,"00:06:11,830","00:06:16,720",90,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=371,"So by making this assumption, it makes it"
cs-410_4_2_91,cs-410,4,2, Statistical Language Model,"00:06:16,720","00:06:18,590",91,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=376,So let's see a specific example here.
cs-410_4_2_92,cs-410,4,2, Statistical Language Model,"00:06:19,810","00:06:25,450",92,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=379,Here I show two unigram language
cs-410_4_2_93,cs-410,4,2, Statistical Language Model,"00:06:25,450","00:06:28,050",93,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=385,And these are high probability
cs-410_4_2_94,cs-410,4,2, Statistical Language Model,"00:06:29,800","00:06:33,290",94,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=389,The first one clearly suggests
cs-410_4_2_95,cs-410,4,2, Statistical Language Model,"00:06:33,290","00:06:37,020",95,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=393,because the high probability
cs-410_4_2_96,cs-410,4,2, Statistical Language Model,"00:06:37,020","00:06:38,700",96,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=397,The second one is more related to health.
cs-410_4_2_97,cs-410,4,2, Statistical Language Model,"00:06:39,790","00:06:41,290",97,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=399,"Now we can ask the question,"
cs-410_4_2_98,cs-410,4,2, Statistical Language Model,"00:06:41,290","00:06:46,520",98,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=401,how likely were observe a particular
cs-410_4_2_99,cs-410,4,2, Statistical Language Model,"00:06:46,520","00:06:49,920",99,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=406,Now suppose we sample
cs-410_4_2_100,cs-410,4,2, Statistical Language Model,"00:06:49,920","00:06:53,150",100,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=409,"Let's say we take the first distribution,"
cs-410_4_2_101,cs-410,4,2, Statistical Language Model,"00:06:53,150","00:06:56,140",101,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=413,What words do you think would be
cs-410_4_2_102,cs-410,4,2, Statistical Language Model,"00:06:56,140","00:06:58,280",102,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=416,maybe mining maybe another word?
cs-410_4_2_103,cs-410,4,2, Statistical Language Model,"00:06:58,280","00:06:58,860",103,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=418,"Even food,"
cs-410_4_2_104,cs-410,4,2, Statistical Language Model,"00:06:58,860","00:07:02,300",104,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=418,"which has a very small probability,"
cs-410_4_2_105,cs-410,4,2, Statistical Language Model,"00:07:03,880","00:07:06,890",105,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=423,"But in general, high probability"
cs-410_4_2_106,cs-410,4,2, Statistical Language Model,"00:07:08,130","00:07:11,200",106,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=428,So we can imagine what general text
cs-410_4_2_107,cs-410,4,2, Statistical Language Model,"00:07:12,230","00:07:14,630",107,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=432,"In fact, with small probability,"
cs-410_4_2_108,cs-410,4,2, Statistical Language Model,"00:07:14,630","00:07:19,940",108,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=434,you might be able to actually generate
cs-410_4_2_109,cs-410,4,2, Statistical Language Model,"00:07:19,940","00:07:23,660",109,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=439,"Now, it will actually be meaningful,"
cs-410_4_2_110,cs-410,4,2, Statistical Language Model,"00:07:23,660","00:07:24,400",110,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=443,very small.
cs-410_4_2_111,cs-410,4,2, Statistical Language Model,"00:07:26,100","00:07:30,220",111,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=446,"In an extreme case, you might"
cs-410_4_2_112,cs-410,4,2, Statistical Language Model,"00:07:30,220","00:07:35,980",112,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=450,a text mining paper that would be
cs-410_4_2_113,cs-410,4,2, Statistical Language Model,"00:07:35,980","00:07:39,866",113,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=455,"And in that case,"
cs-410_4_2_114,cs-410,4,2, Statistical Language Model,"00:07:39,866","00:07:42,152",114,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=459,"But it's a non-zero probability,"
cs-410_4_2_115,cs-410,4,2, Statistical Language Model,"00:07:42,152","00:07:45,850",115,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=462,if we assume none of the words
cs-410_4_2_116,cs-410,4,2, Statistical Language Model,"00:07:47,430","00:07:49,380",116,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=467,"Similarly from the second topic,"
cs-410_4_2_117,cs-410,4,2, Statistical Language Model,"00:07:49,380","00:07:52,660",117,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=469,we can imagine we can generate
cs-410_4_2_118,cs-410,4,2, Statistical Language Model,"00:07:52,660","00:07:58,220",118,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=472,That doesn't mean we cannot generate this
cs-410_4_2_119,cs-410,4,2, Statistical Language Model,"00:07:59,650","00:08:05,030",119,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=479,"We can, but the probability would be very,"
cs-410_4_2_120,cs-410,4,2, Statistical Language Model,"00:08:05,030","00:08:09,300",120,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=485,generating a paper that can be accepted
cs-410_4_2_121,cs-410,4,2, Statistical Language Model,"00:08:10,400","00:08:12,470",121,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=490,So the point is that
cs-410_4_2_122,cs-410,4,2, Statistical Language Model,"00:08:13,590","00:08:18,410",122,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=493,we can talk about the probability of
cs-410_4_2_123,cs-410,4,2, Statistical Language Model,"00:08:18,410","00:08:20,790",123,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=498,Some texts will have higher
cs-410_4_2_124,cs-410,4,2, Statistical Language Model,"00:08:21,800","00:08:23,900",124,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=501,Now let's look at the problem
cs-410_4_2_125,cs-410,4,2, Statistical Language Model,"00:08:23,900","00:08:28,260",125,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=503,Suppose we now have available
cs-410_4_2_126,cs-410,4,2, Statistical Language Model,"00:08:28,260","00:08:31,960",126,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=508,"In this case, many of the abstract or"
cs-410_4_2_127,cs-410,4,2, Statistical Language Model,"00:08:31,960","00:08:34,350",127,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=511,we see these word counts here.
cs-410_4_2_128,cs-410,4,2, Statistical Language Model,"00:08:34,350","00:08:36,846",128,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=514,The total number of words is 100.
cs-410_4_2_129,cs-410,4,2, Statistical Language Model,"00:08:36,846","00:08:39,530",129,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=516,Now the question you ask here
cs-410_4_2_130,cs-410,4,2, Statistical Language Model,"00:08:39,530","00:08:42,000",130,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=519,"We can ask the question which model,"
cs-410_4_2_131,cs-410,4,2, Statistical Language Model,"00:08:42,000","00:08:46,340",131,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=522,which one of these distribution has
cs-410_4_2_132,cs-410,4,2, Statistical Language Model,"00:08:46,340","00:08:50,150",132,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=526,assuming that the text has been generated
cs-410_4_2_133,cs-410,4,2, Statistical Language Model,"00:08:51,970","00:08:53,100",133,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=531,So what would be your guess?
cs-410_4_2_134,cs-410,4,2, Statistical Language Model,"00:08:54,230","00:08:58,220",134,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=534,What we have to decide are what
cs-410_4_2_135,cs-410,4,2, Statistical Language Model,"00:08:58,220","00:08:58,860",135,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=538,would have.
cs-410_4_2_136,cs-410,4,2, Statistical Language Model,"00:09:01,971","00:09:05,260",136,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=541,"Suppose the view for a second, and"
cs-410_4_2_137,cs-410,4,2, Statistical Language Model,"00:09:09,616","00:09:14,109",137,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=549,"If you're like a lot of people,"
cs-410_4_2_138,cs-410,4,2, Statistical Language Model,"00:09:14,109","00:09:18,683",138,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=554,my best guess is text has a probability
cs-410_4_2_139,cs-410,4,2, Statistical Language Model,"00:09:18,683","00:09:23,310",139,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=558,"seen text 10 times, and"
cs-410_4_2_140,cs-410,4,2, Statistical Language Model,"00:09:23,310","00:09:25,990",140,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=563,So we simply normalize these counts.
cs-410_4_2_141,cs-410,4,2, Statistical Language Model,"00:09:27,242","00:09:29,550",141,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=567,"And that's in fact the word justified, and"
cs-410_4_2_142,cs-410,4,2, Statistical Language Model,"00:09:29,550","00:09:33,650",142,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=569,your intuition is consistent
cs-410_4_2_143,cs-410,4,2, Statistical Language Model,"00:09:33,650","00:09:36,170",143,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=573,And this is called the maximum
cs-410_4_2_144,cs-410,4,2, Statistical Language Model,"00:09:36,170","00:09:40,130",144,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=576,"In this estimator,"
cs-410_4_2_145,cs-410,4,2, Statistical Language Model,"00:09:40,130","00:09:44,650",145,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=580,of those that would give our observe
cs-410_4_2_146,cs-410,4,2, Statistical Language Model,"00:09:44,650","00:09:49,050",146,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=584,That means if we change these
cs-410_4_2_147,cs-410,4,2, Statistical Language Model,"00:09:49,050","00:09:53,319",147,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=589,observing the particular text
cs-410_4_2_148,cs-410,4,2, Statistical Language Model,"00:09:55,190","00:09:58,840",148,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=595,"So you can see,"
cs-410_4_2_149,cs-410,4,2, Statistical Language Model,"00:09:58,840","00:10:05,030",149,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=598,"Basically, we just need to look at"
cs-410_4_2_150,cs-410,4,2, Statistical Language Model,"00:10:05,030","00:10:08,987",150,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=605,and then divide it by the total number of
cs-410_4_2_151,cs-410,4,2, Statistical Language Model,"00:10:08,987","00:10:11,670",151,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=608,Normalize the frequency.
cs-410_4_2_152,cs-410,4,2, Statistical Language Model,"00:10:11,670","00:10:13,090",152,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=611,"A consequence of this is,"
cs-410_4_2_153,cs-410,4,2, Statistical Language Model,"00:10:13,090","00:10:18,200",153,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=613,"of course, we're going to assign"
cs-410_4_2_154,cs-410,4,2, Statistical Language Model,"00:10:18,200","00:10:19,730",154,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=618,"If we have an observed word,"
cs-410_4_2_155,cs-410,4,2, Statistical Language Model,"00:10:19,730","00:10:25,300",155,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=619,there will be no incentive to assign a
cs-410_4_2_156,cs-410,4,2, Statistical Language Model,"00:10:25,300","00:10:26,210",156,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=625,Why?
cs-410_4_2_157,cs-410,4,2, Statistical Language Model,"00:10:26,210","00:10:30,840",157,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=626,Because that would take away probability
cs-410_4_2_158,cs-410,4,2, Statistical Language Model,"00:10:30,840","00:10:33,516",158,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=630,And that obviously wouldn't maximize
cs-410_4_2_159,cs-410,4,2, Statistical Language Model,"00:10:33,516","00:10:37,430",159,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=633,the probability of this
cs-410_4_2_160,cs-410,4,2, Statistical Language Model,"00:10:37,430","00:10:42,050",160,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=637,But one has still question whether
cs-410_4_2_161,cs-410,4,2, Statistical Language Model,"00:10:42,050","00:10:47,820",161,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=642,"Well, the answer depends on what kind"
cs-410_4_2_162,cs-410,4,2, Statistical Language Model,"00:10:47,820","00:10:52,320",162,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=647,This estimator gives a best model
cs-410_4_2_163,cs-410,4,2, Statistical Language Model,"00:10:52,320","00:10:57,400",163,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=652,But if you are interested in a model
cs-410_4_2_164,cs-410,4,2, Statistical Language Model,"00:10:57,400","00:11:01,910",164,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=657,"paper for this abstract, then you"
cs-410_4_2_165,cs-410,4,2, Statistical Language Model,"00:11:01,910","00:11:07,330",165,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=661,"So for thing,"
cs-410_4_2_166,cs-410,4,2, Statistical Language Model,"00:11:07,330","00:11:11,570",166,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=667,"of that article, so"
cs-410_4_2_167,cs-410,4,2, Statistical Language Model,"00:11:11,570","00:11:14,390",167,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=671,even though they're not
cs-410_4_2_168,cs-410,4,2, Statistical Language Model,"00:11:14,390","00:11:17,750",168,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=674,So we're going to cover this
cs-410_4_2_169,cs-410,4,2, Statistical Language Model,"00:11:17,750","00:11:22,520",169,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=677,in this class in the query
cs-410_4_2_170,cs-410,4,2, Statistical Language Model,"00:11:24,350","00:11:29,520",170,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=684,So let's take a look at some possible
cs-410_4_2_171,cs-410,4,2, Statistical Language Model,"00:11:29,520","00:11:32,820",171,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=689,One use is simply to use
cs-410_4_2_172,cs-410,4,2, Statistical Language Model,"00:11:32,820","00:11:37,140",172,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=692,So here I show some general
cs-410_4_2_173,cs-410,4,2, Statistical Language Model,"00:11:37,140","00:11:39,830",173,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=697,We can use this text to
cs-410_4_2_174,cs-410,4,2, Statistical Language Model,"00:11:39,830","00:11:41,530",174,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=699,and the model might look like this.
cs-410_4_2_175,cs-410,4,2, Statistical Language Model,"00:11:42,720","00:11:47,845",175,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=702,"Right, so on the top, we have those"
cs-410_4_2_176,cs-410,4,2, Statistical Language Model,"00:11:47,845","00:11:52,610",176,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=707,"etc., and then we'll see some"
cs-410_4_2_177,cs-410,4,2, Statistical Language Model,"00:11:52,610","00:11:55,310",177,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=712,"then some very,"
cs-410_4_2_178,cs-410,4,2, Statistical Language Model,"00:11:55,310","00:11:57,460",178,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=715,This is a background language model.
cs-410_4_2_179,cs-410,4,2, Statistical Language Model,"00:11:57,460","00:12:01,900",179,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=717,It represents the frequency of
cs-410_4_2_180,cs-410,4,2, Statistical Language Model,"00:12:01,900","00:12:04,140",180,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=721,This is the background model.
cs-410_4_2_181,cs-410,4,2, Statistical Language Model,"00:12:04,140","00:12:08,000",181,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=724,"Now let's look at another text,"
cs-410_4_2_182,cs-410,4,2, Statistical Language Model,"00:12:08,000","00:12:09,979",182,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=728,we'll look at the computer
cs-410_4_2_183,cs-410,4,2, Statistical Language Model,"00:12:11,030","00:12:13,800",183,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=731,So we have a collection of
cs-410_4_2_184,cs-410,4,2, Statistical Language Model,"00:12:13,800","00:12:17,454",184,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=733,"we do as mentioned again, we can just"
cs-410_4_2_185,cs-410,4,2, Statistical Language Model,"00:12:17,454","00:12:19,640",185,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=737,where we simply normalize the frequencies.
cs-410_4_2_186,cs-410,4,2, Statistical Language Model,"00:12:20,690","00:12:24,326",186,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=740,"Now in this case, we'll get"
cs-410_4_2_187,cs-410,4,2, Statistical Language Model,"00:12:24,326","00:12:28,141",187,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=744,"On the top, it looks similar because"
cs-410_4_2_188,cs-410,4,2, Statistical Language Model,"00:12:28,141","00:12:29,406",188,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=748,they are very common.
cs-410_4_2_189,cs-410,4,2, Statistical Language Model,"00:12:29,406","00:12:34,243",189,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=749,"But as we go down,"
cs-410_4_2_190,cs-410,4,2, Statistical Language Model,"00:12:34,243","00:12:38,806",190,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=754,"computer science,"
cs-410_4_2_191,cs-410,4,2, Statistical Language Model,"00:12:38,806","00:12:43,146",191,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=758,"And so although here, we might also see"
cs-410_4_2_192,cs-410,4,2, Statistical Language Model,"00:12:43,146","00:12:47,490",192,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=763,we can imagine the probability here is
cs-410_4_2_193,cs-410,4,2, Statistical Language Model,"00:12:47,490","00:12:55,776",193,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=767,And we will see many other words here that
cs-410_4_2_194,cs-410,4,2, Statistical Language Model,"00:12:55,776","00:12:58,737",194,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=775,So you can see this distribution
cs-410_4_2_195,cs-410,4,2, Statistical Language Model,"00:12:58,737","00:13:00,830",195,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=778,the corresponding text.
cs-410_4_2_196,cs-410,4,2, Statistical Language Model,"00:13:00,830","00:13:02,870",196,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=780,We can look at even the smaller text.
cs-410_4_2_197,cs-410,4,2, Statistical Language Model,"00:13:03,970","00:13:06,870",197,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=783,"So in this case,"
cs-410_4_2_198,cs-410,4,2, Statistical Language Model,"00:13:06,870","00:13:10,047",198,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=786,"Now if we do the same,"
cs-410_4_2_199,cs-410,4,2, Statistical Language Model,"00:13:10,047","00:13:12,740",199,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=790,again the can be expected
cs-410_4_2_200,cs-410,4,2, Statistical Language Model,"00:13:12,740","00:13:16,927",200,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=792,"The sooner we see text, mining,"
cs-410_4_2_201,cs-410,4,2, Statistical Language Model,"00:13:16,927","00:13:20,440",201,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=796,these words have relatively
cs-410_4_2_202,cs-410,4,2, Statistical Language Model,"00:13:20,440","00:13:27,540",202,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=800,"In contrast, in this distribution, the"
cs-410_4_2_203,cs-410,4,2, Statistical Language Model,"00:13:27,540","00:13:32,190",203,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=807,"So this means, again,"
cs-410_4_2_204,cs-410,4,2, Statistical Language Model,"00:13:32,190","00:13:36,266",204,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=812,"we can have a different model,"
cs-410_4_2_205,cs-410,4,2, Statistical Language Model,"00:13:36,266","00:13:40,450",205,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=816,So we call this document
cs-410_4_2_206,cs-410,4,2, Statistical Language Model,"00:13:40,450","00:13:42,530",206,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=820,we call this collection language model.
cs-410_4_2_207,cs-410,4,2, Statistical Language Model,"00:13:42,530","00:13:46,580",207,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=822,"And later, you will see how they're"
cs-410_4_2_208,cs-410,4,2, Statistical Language Model,"00:13:47,650","00:13:50,690",208,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=827,"But now,"
cs-410_4_2_209,cs-410,4,2, Statistical Language Model,"00:13:50,690","00:13:55,210",209,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=830,Can we statistically find what words
cs-410_4_2_210,cs-410,4,2, Statistical Language Model,"00:13:56,900","00:13:58,770",210,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=836,Now how do we find such words?
cs-410_4_2_211,cs-410,4,2, Statistical Language Model,"00:13:58,770","00:14:04,230",211,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=838,"Well, our first thought is that let's take"
cs-410_4_2_212,cs-410,4,2, Statistical Language Model,"00:14:04,230","00:14:08,860",212,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=844,So we can take a look at all the documents
cs-410_4_2_213,cs-410,4,2, Statistical Language Model,"00:14:08,860","00:14:10,930",213,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=848,Let's build a language model.
cs-410_4_2_214,cs-410,4,2, Statistical Language Model,"00:14:10,930","00:14:13,220",214,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=850,We can see what words we see there.
cs-410_4_2_215,cs-410,4,2, Statistical Language Model,"00:14:13,220","00:14:19,430",215,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=853,"Well, not surprisingly, we see these"
cs-410_4_2_216,cs-410,4,2, Statistical Language Model,"00:14:19,430","00:14:23,490",216,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=859,"So in this case, this language model gives"
cs-410_4_2_217,cs-410,4,2, Statistical Language Model,"00:14:23,490","00:14:26,260",217,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=863,the word in the context of computer.
cs-410_4_2_218,cs-410,4,2, Statistical Language Model,"00:14:26,260","00:14:29,370",218,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=866,And these common words will
cs-410_4_2_219,cs-410,4,2, Statistical Language Model,"00:14:29,370","00:14:31,750",219,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=869,But we also see the computer itself and
cs-410_4_2_220,cs-410,4,2, Statistical Language Model,"00:14:31,750","00:14:35,490",220,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=871,software will have relatively
cs-410_4_2_221,cs-410,4,2, Statistical Language Model,"00:14:35,490","00:14:37,320",221,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=875,"But if we just use this model,"
cs-410_4_2_222,cs-410,4,2, Statistical Language Model,"00:14:37,320","00:14:42,037",222,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=877,we cannot just say all these words
cs-410_4_2_223,cs-410,4,2, Statistical Language Model,"00:14:43,210","00:14:50,700",223,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=883,"So ultimately, what we'd like to"
cs-410_4_2_224,cs-410,4,2, Statistical Language Model,"00:14:50,700","00:14:51,420",224,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=890,How can we do that?
cs-410_4_2_225,cs-410,4,2, Statistical Language Model,"00:14:52,760","00:14:55,571",225,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=892,It turns out that it's possible
cs-410_4_2_226,cs-410,4,2, Statistical Language Model,"00:14:57,610","00:15:00,020",226,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=897,But I suggest you think about that.
cs-410_4_2_227,cs-410,4,2, Statistical Language Model,"00:15:00,020","00:15:03,510",227,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=900,So how can we know what
cs-410_4_2_228,cs-410,4,2, Statistical Language Model,"00:15:03,510","00:15:06,030",228,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=903,so that we want to kind
cs-410_4_2_229,cs-410,4,2, Statistical Language Model,"00:15:07,730","00:15:10,220",229,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=907,What model will tell us that?
cs-410_4_2_230,cs-410,4,2, Statistical Language Model,"00:15:10,220","00:15:14,180",230,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=910,"Well, maybe you can think about that."
cs-410_4_2_231,cs-410,4,2, Statistical Language Model,"00:15:14,180","00:15:18,170",231,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=914,So the background language model
cs-410_4_2_232,cs-410,4,2, Statistical Language Model,"00:15:18,170","00:15:21,240",232,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=918,It tells us what was
cs-410_4_2_233,cs-410,4,2, Statistical Language Model,"00:15:21,240","00:15:23,510",233,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=921,"So if we use this background model,"
cs-410_4_2_234,cs-410,4,2, Statistical Language Model,"00:15:23,510","00:15:28,390",234,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=923,we would know that these words
cs-410_4_2_235,cs-410,4,2, Statistical Language Model,"00:15:28,390","00:15:31,595",235,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=928,So it's not surprising to observe
cs-410_4_2_236,cs-410,4,2, Statistical Language Model,"00:15:31,595","00:15:36,380",236,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=931,Whereas computer has a very
cs-410_4_2_237,cs-410,4,2, Statistical Language Model,"00:15:36,380","00:15:41,200",237,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=936,it's very surprising that we have seen
cs-410_4_2_238,cs-410,4,2, Statistical Language Model,"00:15:41,200","00:15:42,740",238,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=941,the same is true for software.
cs-410_4_2_239,cs-410,4,2, Statistical Language Model,"00:15:44,220","00:15:48,750",239,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=944,So then we can use these two
cs-410_4_2_240,cs-410,4,2, Statistical Language Model,"00:15:48,750","00:15:52,590",240,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=948,the words that are related to computer.
cs-410_4_2_241,cs-410,4,2, Statistical Language Model,"00:15:52,590","00:15:57,310",241,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=952,"For example, we can simply take the ratio"
cs-410_4_2_242,cs-410,4,2, Statistical Language Model,"00:15:57,310","00:16:01,050",242,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=957,normalize the topic of language model
cs-410_4_2_243,cs-410,4,2, Statistical Language Model,"00:16:01,050","00:16:02,900",243,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=961,the background language model.
cs-410_4_2_244,cs-410,4,2, Statistical Language Model,"00:16:02,900","00:16:07,632",244,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=962,"So if we do that, we take the ratio,"
cs-410_4_2_245,cs-410,4,2, Statistical Language Model,"00:16:07,632","00:16:11,371",245,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=967,"computer is ranked, and"
cs-410_4_2_246,cs-410,4,2, Statistical Language Model,"00:16:11,371","00:16:14,796",246,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=971,"program, all these words"
cs-410_4_2_247,cs-410,4,2, Statistical Language Model,"00:16:14,796","00:16:19,371",247,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=974,Because they occur very frequently in the
cs-410_4_2_248,cs-410,4,2, Statistical Language Model,"00:16:19,371","00:16:23,960",248,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=979,"the whole collection, whereas these common"
cs-410_4_2_249,cs-410,4,2, Statistical Language Model,"00:16:23,960","00:16:27,850",249,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=983,"In fact,"
cs-410_4_2_250,cs-410,4,2, Statistical Language Model,"00:16:27,850","00:16:30,780",250,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=987,because they are not really
cs-410_4_2_251,cs-410,4,2, Statistical Language Model,"00:16:30,780","00:16:34,920",251,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=990,By taking the sample of text
cs-410_4_2_252,cs-410,4,2, Statistical Language Model,"00:16:34,920","00:16:39,240",252,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=994,we don't really see more occurrences
cs-410_4_2_253,cs-410,4,2, Statistical Language Model,"00:16:40,250","00:16:43,310",253,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1000,So this shows that even with
cs-410_4_2_254,cs-410,4,2, Statistical Language Model,"00:16:43,310","00:16:46,450",254,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1003,we can do some limited
cs-410_4_2_255,cs-410,4,2, Statistical Language Model,"00:16:48,370","00:16:52,343",255,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1008,"So in this lecture,"
cs-410_4_2_256,cs-410,4,2, Statistical Language Model,"00:16:52,343","00:16:56,776",256,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1012,which is basically a probability
cs-410_4_2_257,cs-410,4,2, Statistical Language Model,"00:16:56,776","00:17:00,067",257,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1016,We talked about the simplest language
cs-410_4_2_258,cs-410,4,2, Statistical Language Model,"00:17:00,067","00:17:02,720",258,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1020,which is also just a word distribution.
cs-410_4_2_259,cs-410,4,2, Statistical Language Model,"00:17:02,720","00:17:05,320",259,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1022,We talked about the two
cs-410_4_2_260,cs-410,4,2, Statistical Language Model,"00:17:05,320","00:17:10,360",260,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1025,One is we represent the topic in a
cs-410_4_2_261,cs-410,4,2, Statistical Language Model,"00:17:10,360","00:17:12,650",261,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1030,The other is we discover
cs-410_4_2_262,cs-410,4,2, Statistical Language Model,"00:17:16,456","00:17:20,089",262,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1036,"In the next lecture, we're going to talk"
cs-410_4_2_263,cs-410,4,2, Statistical Language Model,"00:17:20,089","00:17:21,510",263,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1040,design a retrieval function.
cs-410_4_2_264,cs-410,4,2, Statistical Language Model,"00:17:23,260","00:17:24,960",264,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1043,Here are two additional readings.
cs-410_4_2_265,cs-410,4,2, Statistical Language Model,"00:17:24,960","00:17:28,850",265,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1044,The first is a textbook on statistical
cs-410_4_2_266,cs-410,4,2, Statistical Language Model,"00:17:30,290","00:17:35,249",266,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1050,The second is an article that
cs-410_4_2_267,cs-410,4,2, Statistical Language Model,"00:17:35,249","00:17:40,326",267,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1055,language models with a lot of
cs-410_4_2_268,cs-410,4,2, Statistical Language Model,"00:17:40,326","00:17:50,326",268,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1060,[MUSIC]
cs-410_4_3_1,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:00,012","00:00:03,532",1,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=0,[SOUND]
cs-410_4_3_2,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:07,767","00:00:10,058",2,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=7,"This lecture is about query likelihood,"
cs-410_4_3_3,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:10,058","00:00:11,960",3,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=10,probabilistic retrieval model.
cs-410_4_3_4,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:14,040","00:00:15,310",4,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=14,"In this lecture,"
cs-410_4_3_5,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:15,310","00:00:19,190",5,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=15,we continue the discussion of
cs-410_4_3_6,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:19,190","00:00:22,830",6,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=19,"In particular, we're going to talk about"
cs-410_4_3_7,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:25,870","00:00:31,073",7,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=25,"In the query light holder retrieval model,"
cs-410_4_3_8,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:31,073","00:00:35,420",8,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=31,How like their user who likes a document
cs-410_4_3_9,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:36,990","00:00:41,462",9,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=36,"So in this case,"
cs-410_4_3_10,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:41,462","00:00:46,663",10,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=41,particular document about
cs-410_4_3_11,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:46,663","00:00:50,410",11,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=46,"Now we assume,"
cs-410_4_3_12,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:50,410","00:00:54,780",12,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=50,a basis to impose a query to try and
cs-410_4_3_13,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:57,340","00:01:03,840",13,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=57,"So again, imagine use a process"
cs-410_4_3_14,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:03,840","00:01:06,940",14,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=63,Where we assume that
cs-410_4_3_15,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:06,940","00:01:08,640",15,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=66,assembling words from the document.
cs-410_4_3_16,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:10,560","00:01:15,880",16,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=70,"So for example, a user might"
cs-410_4_3_17,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:15,880","00:01:19,390",17,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=75,from this document and
cs-410_4_3_18,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:20,600","00:01:24,590",18,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=80,And then the user would pick
cs-410_4_3_19,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:24,590","00:01:25,910",19,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=84,that would be the second query word.
cs-410_4_3_20,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:27,420","00:01:32,400",20,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=87,Now this of course is an assumption
cs-410_4_3_21,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:32,400","00:01:35,008",21,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=92,how a user would pose a query.
cs-410_4_3_22,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:35,008","00:01:39,788",22,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=95,Whether a user actually followed this
cs-410_4_3_23,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:39,788","00:01:45,230",23,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=99,this assumption has allowed us to formerly
cs-410_4_3_24,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:46,390","00:01:50,930",24,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=106,And this allows us to also not rely on
cs-410_4_3_25,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:52,580","00:01:55,750",25,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=112,to use empirical data to
cs-410_4_3_26,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:56,870","00:02:00,820",26,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=116,And this is why we can use this
cs-410_4_3_27,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:00,820","00:02:03,569",27,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=120,retrieval function that we can
cs-410_4_3_28,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:04,900","00:02:08,991",28,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=124,So as you see the assumption
cs-410_4_3_29,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:08,991","00:02:11,558",29,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=128,word is independent of the sample.
cs-410_4_3_30,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:11,558","00:02:17,880",30,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=131,And also each word is basically
cs-410_4_3_31,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:20,910","00:02:24,540",31,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=140,So now let's see how this works exactly.
cs-410_4_3_32,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:24,540","00:02:28,550",32,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=144,"Well, since we are completing"
cs-410_4_3_33,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:29,730","00:02:34,444",33,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=149,then the probability here is just
cs-410_4_3_34,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:34,444","00:02:37,210",34,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=154,which is a sequence of words.
cs-410_4_3_35,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:37,210","00:02:42,140",35,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=157,And we make the assumption that each
cs-410_4_3_36,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:42,140","00:02:46,670",36,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=162,"So as a result, the probability"
cs-410_4_3_37,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:46,670","00:02:48,920",37,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=166,of the probability of each query word.
cs-410_4_3_38,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:50,100","00:02:52,660",38,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=170,Now how do we compute
cs-410_4_3_39,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:52,660","00:02:56,740",39,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=172,"Well, based on the assumption that a word"
cs-410_4_3_40,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:56,740","00:03:01,360",40,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=176,is picked from the document
cs-410_4_3_41,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:01,360","00:03:05,680",41,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=181,Now we know the probability of each word
cs-410_4_3_42,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:05,680","00:03:08,120",42,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=185,word in the document.
cs-410_4_3_43,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:08,120","00:03:13,780",43,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=188,"So for example, the probability of"
cs-410_4_3_44,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:13,780","00:03:17,520",44,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=193,Would be just the count
cs-410_4_3_45,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:17,520","00:03:23,060",45,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=197,divided by the total number of words
cs-410_4_3_46,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:23,060","00:03:28,940",46,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=203,So with these assumptions we now have
cs-410_4_3_47,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:28,940","00:03:30,970",47,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=208,We can use this to rank our documents.
cs-410_4_3_48,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:32,650","00:03:34,200",48,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=212,So does this model work?
cs-410_4_3_49,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:34,200","00:03:35,260",49,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=214,Let's take a look.
cs-410_4_3_50,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:35,260","00:03:38,670",50,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=215,Here are some example documents
cs-410_4_3_51,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:38,670","00:03:42,210",51,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=218,Suppose now the query is
cs-410_4_3_52,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:42,210","00:03:44,880",52,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=222,we see the formula here on the top.
cs-410_4_3_53,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:45,900","00:03:47,490",53,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=225,So how do we score this document?
cs-410_4_3_54,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:47,490","00:03:48,790",54,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=227,"Well, it's very simple."
cs-410_4_3_55,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:48,790","00:03:51,370",55,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=228,We just count how many times do
cs-410_4_3_56,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:51,370","00:03:54,380",56,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=231,"how many times do we have seen campaigns,"
cs-410_4_3_57,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:54,380","00:03:57,458",57,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=234,"And we see here 44, and"
cs-410_4_3_58,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:57,458","00:04:02,297",58,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=237,So that's 2 over the length of
cs-410_4_3_59,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:02,297","00:04:07,710",59,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=242,the length of document 4 for
cs-410_4_3_60,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:07,710","00:04:11,146",60,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=247,"And similarly, we can get probabilities"
cs-410_4_3_61,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:13,189","00:04:17,505",61,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=253,Now if you look at these numbers or
cs-410_4_3_62,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:17,505","00:04:22,030",62,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=257,"scoring all these documents,"
cs-410_4_3_63,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:22,030","00:04:28,436",63,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=262,Because if we assume d3 and
cs-410_4_3_64,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:28,436","00:04:35,628",64,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=268,then looks like a nominal rank d4
cs-410_4_3_65,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:35,628","00:04:40,819",65,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=275,"And as we would expect,"
cs-410_4_3_66,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:40,819","00:04:45,916",66,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=280,"a TF query state, and so"
cs-410_4_3_67,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:45,916","00:04:50,096",67,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=285,"However, if we try a different"
cs-410_4_3_68,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:50,096","00:04:54,854",68,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=290,presidential campaign update
cs-410_4_3_69,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:54,854","00:04:56,608",69,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=294,Well what problem?
cs-410_4_3_70,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:56,608","00:04:58,930",70,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=296,Well think about the update.
cs-410_4_3_71,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:58,930","00:05:02,500",71,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=298,Now none of these documents
cs-410_4_3_72,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:02,500","00:05:08,420",72,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=302,So according to our assumption that a user
cs-410_4_3_73,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:08,420","00:05:15,003",73,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=308,"generate a query, then the probability of"
cs-410_4_3_74,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:15,003","00:05:16,070",74,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=315,Would be 0.
cs-410_4_3_75,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:17,230","00:05:21,380",75,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=317,"So that causes a problem,"
cs-410_4_3_76,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:21,380","00:05:23,710",76,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=321,to have zero probability
cs-410_4_3_77,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:25,330","00:05:31,127",77,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=325,Now why it's fine to have zero probability
cs-410_4_3_78,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:31,127","00:05:33,902",78,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=331,It's not okay to have 0 for d3 and
cs-410_4_3_79,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:33,902","00:05:38,600",79,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=333,d4 because now we no longer
cs-410_4_3_80,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:38,600","00:05:39,135",80,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=338,What's worse?
cs-410_4_3_81,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:39,135","00:05:41,735",81,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=339,We can't even distinguish them from d2.
cs-410_4_3_82,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:41,735","00:05:45,700",82,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=341,So that's obviously not desirable.
cs-410_4_3_83,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:45,700","00:05:48,630",83,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=345,"Now when a [INAUDIBLE] has such result,"
cs-410_4_3_84,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:48,630","00:05:50,960",84,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=348,we should think about what
cs-410_4_3_85,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:52,530","00:05:56,773",85,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=352,So we have to examine what
cs-410_4_3_86,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:56,773","00:05:59,644",86,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=356,as we derive this ranking function.
cs-410_4_3_87,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:59,644","00:06:03,285",87,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=359,Now is you examine those assumptions
cs-410_4_3_88,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:03,285","00:06:04,983",88,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=363,what has caused this problem?
cs-410_4_3_89,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:04,983","00:06:09,080",89,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=364,So take a moment to think about it.
cs-410_4_3_90,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:09,080","00:06:17,179",90,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=369,What do you think is the reason why update
cs-410_4_3_91,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:17,179","00:06:22,092",91,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=377,So if you think about this from the moment
cs-410_4_3_92,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:22,092","00:06:25,317",92,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=382,have made an assumption
cs-410_4_3_93,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:25,317","00:06:29,220",93,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=385,be drawn from the document
cs-410_4_3_94,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:29,220","00:06:33,982",94,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=389,"So in order to fix this, we have to"
cs-410_4_3_95,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:33,982","00:06:36,912",95,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=393,a word not necessarily from the document.
cs-410_4_3_96,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:36,912","00:06:38,930",96,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=396,So that's the improved model.
cs-410_4_3_97,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:38,930","00:06:40,912",97,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=398,"An improvement here is to say that,"
cs-410_4_3_98,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:40,912","00:06:43,687",98,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=400,well instead of drawing
cs-410_4_3_99,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:43,687","00:06:48,064",99,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=403,let's imagine that the user would actually
cs-410_4_3_100,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:48,064","00:06:50,107",100,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=408,And so I show a model here.
cs-410_4_3_101,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:50,107","00:06:54,479",101,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=410,And we assume that this document is
cs-410_4_3_102,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:54,479","00:06:55,920",102,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=414,model.
cs-410_4_3_103,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:55,920","00:07:01,297",103,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=415,"Now, this model doesn't necessarily assign"
cs-410_4_3_104,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:01,297","00:07:05,853",104,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=421,we can assume this model does not
cs-410_4_3_105,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:05,853","00:07:09,621",105,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=425,Now if we're thinking this way then
cs-410_4_3_106,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:09,621","00:07:10,700",106,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=429,different.
cs-410_4_3_107,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:10,700","00:07:14,940",107,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=430,Now the user has this model in mind
cs-410_4_3_108,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:14,940","00:07:18,669",108,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=434,Although the model has to be
cs-410_4_3_109,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:18,669","00:07:22,960",109,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=438,So the user can again generate
cs-410_4_3_110,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:22,960","00:07:27,680",110,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=442,"Namely, pick a word for example,"
cs-410_4_3_111,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:29,020","00:07:32,390",111,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=449,Now the difference is that this time
cs-410_4_3_112,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:32,390","00:07:34,930",112,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=452,even though update doesn't
cs-410_4_3_113,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:34,930","00:07:38,050",113,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=454,to potentially generate
cs-410_4_3_114,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:38,050","00:07:43,840",114,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=458,So that a query was updated
cs-410_4_3_115,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:43,840","00:07:45,720",115,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=463,So this would fix our problem.
cs-410_4_3_116,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:45,720","00:07:50,140",116,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=465,And it's also reasonable because when our
cs-410_4_3_117,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:50,140","00:07:55,160",117,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=470,"in a more general way, that is unique"
cs-410_4_3_118,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:55,160","00:07:57,830",118,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=475,So how do we compute
cs-410_4_3_119,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:57,830","00:08:01,000",119,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=477,If we make this sum wide
cs-410_4_3_120,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:01,000","00:08:07,390",120,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=481,"The first one is compute this model, and"
cs-410_4_3_121,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:07,390","00:08:15,070",121,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=487,"For example, I've shown two pulse models"
cs-410_4_3_122,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:15,070","00:08:19,803",122,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=495,And then given a query like a data mining
cs-410_4_3_123,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:19,803","00:08:22,467",123,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=499,just compute the likelihood of this query.
cs-410_4_3_124,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:22,467","00:08:26,574",124,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=502,And by making independence
cs-410_4_3_125,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:26,574","00:08:30,766",125,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=506,probability as a product of
cs-410_4_3_126,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:30,766","00:08:34,798",126,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=510,"We do this for both documents, and"
cs-410_4_3_127,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:34,798","00:08:35,700",127,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=514,then rank them.
cs-410_4_3_128,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:37,160","00:08:41,310",128,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=517,So that's the basic idea of this
cs-410_4_3_129,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:41,310","00:08:47,890",129,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=521,So more generally this ranking function
cs-410_4_3_130,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:47,890","00:08:51,800",130,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=527,"Here we assume that the query has n words,"
cs-410_4_3_131,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:51,800","00:08:56,540",131,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=531,"w1 through wn, and"
cs-410_4_3_132,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:56,540","00:09:01,500",132,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=536,The ranking function is the probability
cs-410_4_3_133,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:01,500","00:09:06,080",133,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=541,given that the user is
cs-410_4_3_134,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:06,080","00:09:11,970",134,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=546,And this is assume it will be product of
cs-410_4_3_135,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:11,970","00:09:15,360",135,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=551,This is based on independent assumption.
cs-410_4_3_136,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:15,360","00:09:20,256",136,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=555,Now we actually often score
cs-410_4_3_137,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:20,256","00:09:25,250",137,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=560,using log of the query likelihood
cs-410_4_3_138,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:26,710","00:09:30,220",138,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=566,Now we do this to avoid
cs-410_4_3_139,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:30,220","00:09:35,830",139,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=570,"having a lot of small probabilities,"
cs-410_4_3_140,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:35,830","00:09:41,060",140,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=575,And this could cause under flow and we
cs-410_4_3_141,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:41,060","00:09:44,100",141,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=581,the value in our algorithm function.
cs-410_4_3_142,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:44,100","00:09:51,079",142,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=584,We maintain the order of these documents
cs-410_4_3_143,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:51,079","00:09:54,935",143,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=591,And so if we take longer than
cs-410_4_3_144,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:54,935","00:09:59,920",144,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=594,the product would become a sum
cs-410_4_3_145,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:59,920","00:10:03,620",145,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=599,So the sum of all the query
cs-410_4_3_146,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:03,620","00:10:07,670",146,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=603,that is one of the probability of
cs-410_4_3_147,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:09,360","00:10:13,020",147,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=609,And then we can further rewrite
cs-410_4_3_148,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:14,310","00:10:19,960",148,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=614,"So in the first sum here, in this sum,"
cs-410_4_3_149,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:21,910","00:10:28,800",149,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=621,we have it over all the query words and
cs-410_4_3_150,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:28,800","00:10:33,030",150,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=628,And in this sum we have a sum
cs-410_4_3_151,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:33,030","00:10:37,050",151,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=633,But we put a counter here
cs-410_4_3_152,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:37,050","00:10:39,780",152,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=637,Essentially we are only considering
cs-410_4_3_153,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:39,780","00:10:43,570",153,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=639,"because if a word is not in the query,"
cs-410_4_3_154,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:43,570","00:10:46,820",154,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=643,So we're still considering
cs-410_4_3_155,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:46,820","00:10:49,760",155,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=646,But we're using a different form as
cs-410_4_3_156,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:49,760","00:10:51,570",156,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=649,all the words in the vocabulary.
cs-410_4_3_157,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:52,960","00:10:56,435",157,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=652,"And of course, a word might occur"
cs-410_4_3_158,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:56,435","00:10:58,631",158,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=656,That's why we have a count here.
cs-410_4_3_159,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:00,407","00:11:04,168",159,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=660,And then this part is log of
cs-410_4_3_160,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:04,168","00:11:06,815",160,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=664,given by the document language model.
cs-410_4_3_161,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:08,647","00:11:11,497",161,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=668,"So you can see in this retrieval function,"
cs-410_4_3_162,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:11,497","00:11:13,547",162,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=671,we actually know the count
cs-410_4_3_163,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:13,547","00:11:16,507",163,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=673,So the only thing that we don't know
cs-410_4_3_164,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:17,817","00:11:21,310",164,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=677,"Therefore, we have converted"
cs-410_4_3_165,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:21,310","00:11:24,510",165,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=681,include the problem of estimating
cs-410_4_3_166,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:25,920","00:11:30,370",166,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=685,So that we can compute the probability of
cs-410_4_3_167,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:32,260","00:11:36,630",167,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=692,And different estimation methods would
cs-410_4_3_168,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:36,630","00:11:40,980",168,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=696,This is just like a different way to
cs-410_4_3_169,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:40,980","00:11:45,485",169,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=700,which leads to a different ranking
cs-410_4_3_170,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:45,485","00:11:49,353",170,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=705,Here different ways to
cs-410_4_3_171,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:49,353","00:11:54,065",171,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=709,a different ranking function for
cs-410_4_3_172,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:54,065","00:12:04,065",172,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=714,[MUSIC]
cs-410_4_4_1,cs-410,4,4, Statistical Language Model - Part 1,"00:00:00,012","00:00:07,304",1,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=0,[SOUND]
cs-410_4_4_2,cs-410,4,4, Statistical Language Model - Part 1,"00:00:07,304","00:00:10,420",2,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=7,lecture is about smoothing
cs-410_4_4_3,cs-410,4,4, Statistical Language Model - Part 1,"00:00:11,700","00:00:12,390",3,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=11,"In this lecture,"
cs-410_4_4_4,cs-410,4,4, Statistical Language Model - Part 1,"00:00:12,390","00:00:16,110",4,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=12,we're going to continue talking about
cs-410_4_4_5,cs-410,4,4, Statistical Language Model - Part 1,"00:00:16,110","00:00:19,630",5,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=16,"In particular,"
cs-410_4_4_6,cs-410,4,4, Statistical Language Model - Part 1,"00:00:19,630","00:00:22,390",6,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=19,language model in the query
cs-410_4_4_7,cs-410,4,4, Statistical Language Model - Part 1,"00:00:23,820","00:00:27,248",7,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=23,So you have seen this slide
cs-410_4_4_8,cs-410,4,4, Statistical Language Model - Part 1,"00:00:27,248","00:00:30,470",8,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=27,This is the ranking function
cs-410_4_4_9,cs-410,4,4, Statistical Language Model - Part 1,"00:00:32,540","00:00:39,906",9,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=32,"Here, we assume that the independence of"
cs-410_4_4_10,cs-410,4,4, Statistical Language Model - Part 1,"00:00:39,906","00:00:45,367",10,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=39,would look like the following where
cs-410_4_4_11,cs-410,4,4, Statistical Language Model - Part 1,"00:00:45,367","00:00:49,878",11,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=45,And inside the sum there is a log
cs-410_4_4_12,cs-410,4,4, Statistical Language Model - Part 1,"00:00:49,878","00:00:52,700",12,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=49,the document or document image model.
cs-410_4_4_13,cs-410,4,4, Statistical Language Model - Part 1,"00:00:52,700","00:00:57,750",13,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=52,So the main task now is to estimate this
cs-410_4_4_14,cs-410,4,4, Statistical Language Model - Part 1,"00:00:57,750","00:01:02,100",14,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=57,document language model as we
cs-410_4_4_15,cs-410,4,4, Statistical Language Model - Part 1,"00:01:02,100","00:01:06,530",15,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=62,estimating this model would lead
cs-410_4_4_16,cs-410,4,4, Statistical Language Model - Part 1,"00:01:06,530","00:01:10,810",16,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=66,"So in this lecture, we're going to"
cs-410_4_4_17,cs-410,4,4, Statistical Language Model - Part 1,"00:01:10,810","00:01:13,110",17,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=70,So how do we estimate this language model?
cs-410_4_4_18,cs-410,4,4, Statistical Language Model - Part 1,"00:01:13,110","00:01:16,350",18,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=73,Well the obvious choice would be
cs-410_4_4_19,cs-410,4,4, Statistical Language Model - Part 1,"00:01:16,350","00:01:17,990",19,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=76,that we have seen before.
cs-410_4_4_20,cs-410,4,4, Statistical Language Model - Part 1,"00:01:17,990","00:01:22,200",20,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=77,And that is we're going to normalize
cs-410_4_4_21,cs-410,4,4, Statistical Language Model - Part 1,"00:01:24,110","00:01:26,913",21,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=84,And estimate the probability
cs-410_4_4_22,cs-410,4,4, Statistical Language Model - Part 1,"00:01:30,234","00:01:33,194",22,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=90,This is a step function here.
cs-410_4_4_23,cs-410,4,4, Statistical Language Model - Part 1,"00:01:35,934","00:01:38,543",23,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=95,Which means all of the words that have
cs-410_4_4_24,cs-410,4,4, Statistical Language Model - Part 1,"00:01:38,543","00:01:43,016",24,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=98,the same frequency count will
cs-410_4_4_25,cs-410,4,4, Statistical Language Model - Part 1,"00:01:43,016","00:01:48,570",25,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=103,"This is another freedom to count,"
cs-410_4_4_26,cs-410,4,4, Statistical Language Model - Part 1,"00:01:48,570","00:01:51,770",26,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=108,Note that for words that have not
cs-410_4_4_27,cs-410,4,4, Statistical Language Model - Part 1,"00:01:52,850","00:01:55,130",27,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=112,they will have 0 probability.
cs-410_4_4_28,cs-410,4,4, Statistical Language Model - Part 1,"00:01:55,130","00:02:00,880",28,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=115,So we know this is just like the model
cs-410_4_4_29,cs-410,4,4, Statistical Language Model - Part 1,"00:02:00,880","00:02:06,730",29,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=120,Where we assume that the use of
cs-410_4_4_30,cs-410,4,4, Statistical Language Model - Part 1,"00:02:06,730","00:02:07,670",30,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=126,a formula to clear it.
cs-410_4_4_31,cs-410,4,4, Statistical Language Model - Part 1,"00:02:09,200","00:02:13,510",31,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=129,And there's no chance of assembling any
cs-410_4_4_32,cs-410,4,4, Statistical Language Model - Part 1,"00:02:13,510","00:02:14,360",32,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=133,we know that's not good.
cs-410_4_4_33,cs-410,4,4, Statistical Language Model - Part 1,"00:02:15,420","00:02:17,240",33,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=135,So how do we improve this?
cs-410_4_4_34,cs-410,4,4, Statistical Language Model - Part 1,"00:02:17,240","00:02:23,170",34,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=137,Well in order to assign
cs-410_4_4_35,cs-410,4,4, Statistical Language Model - Part 1,"00:02:23,170","00:02:28,710",35,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=143,to words that have not been observed in
cs-410_4_4_36,cs-410,4,4, Statistical Language Model - Part 1,"00:02:28,710","00:02:35,200",36,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=148,some probability mass from the words
cs-410_4_4_37,cs-410,4,4, Statistical Language Model - Part 1,"00:02:35,200","00:02:39,894",37,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=155,"So for example here, we have to take away"
cs-410_4_4_38,cs-410,4,4, Statistical Language Model - Part 1,"00:02:39,894","00:02:45,103",38,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=159,need some extra probability mass for
cs-410_4_4_39,cs-410,4,4, Statistical Language Model - Part 1,"00:02:45,103","00:02:47,870",39,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=165,So all these probabilities must sum to 1.
cs-410_4_4_40,cs-410,4,4, Statistical Language Model - Part 1,"00:02:47,870","00:02:53,224",40,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=167,So to make this transformation and to
cs-410_4_4_41,cs-410,4,4, Statistical Language Model - Part 1,"00:02:53,224","00:03:00,420",41,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=173,by assigning non zero probabilities to
cs-410_4_4_42,cs-410,4,4, Statistical Language Model - Part 1,"00:03:01,970","00:03:06,630",42,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=181,We have to do smoothing and
cs-410_4_4_43,cs-410,4,4, Statistical Language Model - Part 1,"00:03:06,630","00:03:11,140",43,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=186,the estimate by considering
cs-410_4_4_44,cs-410,4,4, Statistical Language Model - Part 1,"00:03:13,970","00:03:17,800",44,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=193,had been asking to write more words for
cs-410_4_4_45,cs-410,4,4, Statistical Language Model - Part 1,"00:03:17,800","00:03:22,910",45,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=197,"the document,"
cs-410_4_4_46,cs-410,4,4, Statistical Language Model - Part 1,"00:03:22,910","00:03:27,050",46,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=202,If you think about this factor
cs-410_4_4_47,cs-410,4,4, Statistical Language Model - Part 1,"00:03:27,050","00:03:30,830",47,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=207,would be a more accurate than
cs-410_4_4_48,cs-410,4,4, Statistical Language Model - Part 1,"00:03:30,830","00:03:35,270",48,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=210,Imagine you have seen an abstract
cs-410_4_4_49,cs-410,4,4, Statistical Language Model - Part 1,"00:03:35,270","00:03:37,230",49,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=215,Let's say this document is abstract.
cs-410_4_4_50,cs-410,4,4, Statistical Language Model - Part 1,"00:03:39,250","00:03:47,844",50,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=219,If we assume and see words in this
cs-410_4_4_51,cs-410,4,4, Statistical Language Model - Part 1,"00:03:47,844","00:03:51,900",51,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=227,That would mean there's
cs-410_4_4_52,cs-410,4,4, Statistical Language Model - Part 1,"00:03:51,900","00:03:57,170",52,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=231,a word outside the abstract
cs-410_4_4_53,cs-410,4,4, Statistical Language Model - Part 1,"00:03:57,170","00:04:02,193",53,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=237,But imagine a user who is interested
cs-410_4_4_54,cs-410,4,4, Statistical Language Model - Part 1,"00:04:02,193","00:04:06,475",54,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=242,The user might actually
cs-410_4_4_55,cs-410,4,4, Statistical Language Model - Part 1,"00:04:06,475","00:04:08,973",55,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=246,that chapter to use as query.
cs-410_4_4_56,cs-410,4,4, Statistical Language Model - Part 1,"00:04:08,973","00:04:13,916",56,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=248,"So obviously,"
cs-410_4_4_57,cs-410,4,4, Statistical Language Model - Part 1,"00:04:13,916","00:04:18,760",57,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=253,author would have written
cs-410_4_4_58,cs-410,4,4, Statistical Language Model - Part 1,"00:04:18,760","00:04:23,627",58,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=258,So smoothing of the language
cs-410_4_4_59,cs-410,4,4, Statistical Language Model - Part 1,"00:04:23,627","00:04:27,642",59,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=263,to recover the model for
cs-410_4_4_60,cs-410,4,4, Statistical Language Model - Part 1,"00:04:27,642","00:04:32,346",60,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=267,"And then of course,"
cs-410_4_4_61,cs-410,4,4, Statistical Language Model - Part 1,"00:04:32,346","00:04:36,310",61,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=272,words that are not
cs-410_4_4_62,cs-410,4,4, Statistical Language Model - Part 1,"00:04:36,310","00:04:39,250",62,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=276,So that's why smoothing is
cs-410_4_4_63,cs-410,4,4, Statistical Language Model - Part 1,"00:04:39,250","00:04:43,670",63,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=279,So let's talk a little more about
cs-410_4_4_64,cs-410,4,4, Statistical Language Model - Part 1,"00:04:43,670","00:04:48,500",64,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=283,"The key question here is, what probability"
cs-410_4_4_65,cs-410,4,4, Statistical Language Model - Part 1,"00:04:50,480","00:04:52,200",65,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=290,And there are many different
cs-410_4_4_66,cs-410,4,4, Statistical Language Model - Part 1,"00:04:53,290","00:04:59,500",66,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=293,"One idea here, that's very useful for"
cs-410_4_4_67,cs-410,4,4, Statistical Language Model - Part 1,"00:04:59,500","00:05:03,790",67,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=299,word be proportional to its probability
cs-410_4_4_68,cs-410,4,4, Statistical Language Model - Part 1,"00:05:03,790","00:05:07,785",68,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=303,That means if you don't observe
cs-410_4_4_69,cs-410,4,4, Statistical Language Model - Part 1,"00:05:07,785","00:05:11,583",69,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=307,We're going to assume that its
cs-410_4_4_70,cs-410,4,4, Statistical Language Model - Part 1,"00:05:11,583","00:05:16,310",70,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=311,by another reference language
cs-410_4_4_71,cs-410,4,4, Statistical Language Model - Part 1,"00:05:16,310","00:05:20,500",71,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=316,It will tell us which unseen words
cs-410_4_4_72,cs-410,4,4, Statistical Language Model - Part 1,"00:05:22,440","00:05:26,060",72,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=322,"In the case of retrieval,"
cs-410_4_4_73,cs-410,4,4, Statistical Language Model - Part 1,"00:05:26,060","00:05:30,080",73,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=326,take the collection language model
cs-410_4_4_74,cs-410,4,4, Statistical Language Model - Part 1,"00:05:30,080","00:05:33,390",74,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=330,"That is to say, if you don't"
cs-410_4_4_75,cs-410,4,4, Statistical Language Model - Part 1,"00:05:33,390","00:05:37,440",75,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=333,we're going to assume that
cs-410_4_4_76,cs-410,4,4, Statistical Language Model - Part 1,"00:05:37,440","00:05:40,658",76,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=337,would be proportional to the probability
cs-410_4_4_77,cs-410,4,4, Statistical Language Model - Part 1,"00:05:40,658","00:05:42,990",77,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=340,"So more formally,"
cs-410_4_4_78,cs-410,4,4, Statistical Language Model - Part 1,"00:05:42,990","00:05:46,790",78,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=342,we'll be estimating the probability
cs-410_4_4_79,cs-410,4,4, Statistical Language Model - Part 1,"00:05:48,220","00:05:54,479",79,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=348,If the word is seen in
cs-410_4_4_80,cs-410,4,4, Statistical Language Model - Part 1,"00:05:54,479","00:06:02,251",80,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=354,would be this counted the maximum
cs-410_4_4_81,cs-410,4,4, Statistical Language Model - Part 1,"00:06:02,251","00:06:07,142",81,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=362,"Otherwise, if the word is not seen in the"
cs-410_4_4_82,cs-410,4,4, Statistical Language Model - Part 1,"00:06:07,142","00:06:12,220",82,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=367,be proportional to the probability
cs-410_4_4_83,cs-410,4,4, Statistical Language Model - Part 1,"00:06:12,220","00:06:17,060",83,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=372,And here the coefficient that offer is to
cs-410_4_4_84,cs-410,4,4, Statistical Language Model - Part 1,"00:06:17,060","00:06:21,360",84,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=377,control the amount of probability
cs-410_4_4_85,cs-410,4,4, Statistical Language Model - Part 1,"00:06:22,450","00:06:25,390",85,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=382,"Obviously, all these"
cs-410_4_4_86,cs-410,4,4, Statistical Language Model - Part 1,"00:06:25,390","00:06:28,300",86,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=385,alpha sub d is constrained in some way.
cs-410_4_4_87,cs-410,4,4, Statistical Language Model - Part 1,"00:06:29,390","00:06:33,370",87,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=389,So what if we plug in this
cs-410_4_4_88,cs-410,4,4, Statistical Language Model - Part 1,"00:06:33,370","00:06:35,150",88,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=393,query likelihood ranking function?
cs-410_4_4_89,cs-410,4,4, Statistical Language Model - Part 1,"00:06:35,150","00:06:36,290",89,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=395,This is what we will get.
cs-410_4_4_90,cs-410,4,4, Statistical Language Model - Part 1,"00:06:37,790","00:06:43,930",90,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=397,"In this formula, we have this"
cs-410_4_4_91,cs-410,4,4, Statistical Language Model - Part 1,"00:06:43,930","00:06:48,900",91,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=403,as a sum over all the query words and
cs-410_4_4_92,cs-410,4,4, Statistical Language Model - Part 1,"00:06:48,900","00:06:54,000",92,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=408,those that we have written here as the sum
cs-410_4_4_93,cs-410,4,4, Statistical Language Model - Part 1,"00:06:54,000","00:06:56,780",93,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=414,This is the sum of all
cs-410_4_4_94,cs-410,4,4, Statistical Language Model - Part 1,"00:06:56,780","00:07:00,310",94,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=416,but not that we have a count
cs-410_4_4_95,cs-410,4,4, Statistical Language Model - Part 1,"00:07:00,310","00:07:04,476",95,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=420,"So in fact, we are just taking"
cs-410_4_4_96,cs-410,4,4, Statistical Language Model - Part 1,"00:07:04,476","00:07:11,820",96,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=424,This is now a common
cs-410_4_4_97,cs-410,4,4, Statistical Language Model - Part 1,"00:07:11,820","00:07:16,170",97,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=431,because of its convenience
cs-410_4_4_98,cs-410,4,4, Statistical Language Model - Part 1,"00:07:18,710","00:07:21,949",98,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=438,"So this is as I said,"
cs-410_4_4_99,cs-410,4,4, Statistical Language Model - Part 1,"00:07:23,130","00:07:26,950",99,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=443,"In our smoothing method,"
cs-410_4_4_100,cs-410,4,4, Statistical Language Model - Part 1,"00:07:26,950","00:07:31,310",100,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=446,are not observed in the method would have
cs-410_4_4_101,cs-410,4,4, Statistical Language Model - Part 1,"00:07:31,310","00:07:33,663",101,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=451,"Name it's four, this foru."
cs-410_4_4_102,cs-410,4,4, Statistical Language Model - Part 1,"00:07:33,663","00:07:37,090",102,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=453,"So we're going to do then,"
cs-410_4_4_103,cs-410,4,4, Statistical Language Model - Part 1,"00:07:38,620","00:07:44,422",103,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=458,One sum is over all the query words
cs-410_4_4_104,cs-410,4,4, Statistical Language Model - Part 1,"00:07:44,422","00:07:49,287",104,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=464,"That means that in this sum, all the words"
cs-410_4_4_105,cs-410,4,4, Statistical Language Model - Part 1,"00:07:49,287","00:07:54,580",105,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=469,have a non zero probability
cs-410_4_4_106,cs-410,4,4, Statistical Language Model - Part 1,"00:07:54,580","00:07:59,740",106,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=474,"Sorry, it's the non zero count"
cs-410_4_4_107,cs-410,4,4, Statistical Language Model - Part 1,"00:07:59,740","00:08:01,220",107,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=479,They all occur in the document.
cs-410_4_4_108,cs-410,4,4, Statistical Language Model - Part 1,"00:08:02,230","00:08:07,800",108,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=482,And they also have to of course
cs-410_4_4_109,cs-410,4,4, Statistical Language Model - Part 1,"00:08:07,800","00:08:13,894",109,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=487,So these are the query words
cs-410_4_4_110,cs-410,4,4, Statistical Language Model - Part 1,"00:08:13,894","00:08:19,153",110,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=493,"On the other hand, in this sum we"
cs-410_4_4_111,cs-410,4,4, Statistical Language Model - Part 1,"00:08:19,153","00:08:23,630",111,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=499,that are not all query was
cs-410_4_4_112,cs-410,4,4, Statistical Language Model - Part 1,"00:08:25,840","00:08:31,250",112,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=505,So they occur in the query
cs-410_4_4_113,cs-410,4,4, Statistical Language Model - Part 1,"00:08:31,250","00:08:33,200",113,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=511,they don't occur in the document.
cs-410_4_4_114,cs-410,4,4, Statistical Language Model - Part 1,"00:08:33,200","00:08:33,920",114,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=513,"In this case,"
cs-410_4_4_115,cs-410,4,4, Statistical Language Model - Part 1,"00:08:33,920","00:08:39,346",115,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=513,these words have this probability because
cs-410_4_4_116,cs-410,4,4, Statistical Language Model - Part 1,"00:08:39,346","00:08:44,880",116,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=519,"That here, these seen words"
cs-410_4_4_117,cs-410,4,4, Statistical Language Model - Part 1,"00:08:47,490","00:08:51,460",117,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=527,"Now, we can go further by"
cs-410_4_4_118,cs-410,4,4, Statistical Language Model - Part 1,"00:08:52,570","00:08:54,790",118,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=532,as a difference of two other sums.
cs-410_4_4_119,cs-410,4,4, Statistical Language Model - Part 1,"00:08:54,790","00:08:58,760",119,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=534,"Basically, the first sum is"
cs-410_4_4_120,cs-410,4,4, Statistical Language Model - Part 1,"00:09:00,060","00:09:05,190",120,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=540,"Now, we know that the original sum"
cs-410_4_4_121,cs-410,4,4, Statistical Language Model - Part 1,"00:09:05,190","00:09:10,760",121,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=545,This is over all the query words that
cs-410_4_4_122,cs-410,4,4, Statistical Language Model - Part 1,"00:09:12,400","00:09:19,740",122,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=552,So here we pretend that they
cs-410_4_4_123,cs-410,4,4, Statistical Language Model - Part 1,"00:09:19,740","00:09:21,920",123,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=559,So we take a sum over all the query words.
cs-410_4_4_124,cs-410,4,4, Statistical Language Model - Part 1,"00:09:21,920","00:09:28,750",124,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=561,"Obviously, this sum has extra"
cs-410_4_4_125,cs-410,4,4, Statistical Language Model - Part 1,"00:09:30,770","00:09:33,710",125,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=570,"Because, here we're taking"
cs-410_4_4_126,cs-410,4,4, Statistical Language Model - Part 1,"00:09:33,710","00:09:37,880",126,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=573,"There, it's not matched in the document."
cs-410_4_4_127,cs-410,4,4, Statistical Language Model - Part 1,"00:09:37,880","00:09:44,370",127,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=577,"So in order to make them equal, we will"
cs-410_4_4_128,cs-410,4,4, Statistical Language Model - Part 1,"00:09:44,370","00:09:48,758",128,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=584,And this is the sum over all the query
cs-410_4_4_129,cs-410,4,4, Statistical Language Model - Part 1,"00:09:51,069","00:09:55,411",129,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=591,"And this makes sense, because here"
cs-410_4_4_130,cs-410,4,4, Statistical Language Model - Part 1,"00:09:55,411","00:09:59,410",130,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=595,And then we subtract the query
cs-410_4_4_131,cs-410,4,4, Statistical Language Model - Part 1,"00:09:59,410","00:10:04,020",131,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=599,That would give us the query that
cs-410_4_4_132,cs-410,4,4, Statistical Language Model - Part 1,"00:10:05,880","00:10:11,100",132,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=605,And this is almost a reverse
cs-410_4_4_133,cs-410,4,4, Statistical Language Model - Part 1,"00:10:12,770","00:10:14,758",133,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=612,And you might wonder why
cs-410_4_4_134,cs-410,4,4, Statistical Language Model - Part 1,"00:10:14,758","00:10:19,510",134,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=614,"Well, that's because if we do this,"
cs-410_4_4_135,cs-410,4,4, Statistical Language Model - Part 1,"00:10:19,510","00:10:25,360",135,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=619,then we have different forms
cs-410_4_4_136,cs-410,4,4, Statistical Language Model - Part 1,"00:10:25,360","00:10:31,370",136,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=625,"So now, you can see in this sum"
cs-410_4_4_137,cs-410,4,4, Statistical Language Model - Part 1,"00:10:31,370","00:10:35,440",137,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=631,the query was matching the document
cs-410_4_4_138,cs-410,4,4, Statistical Language Model - Part 1,"00:10:36,760","00:10:45,750",138,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=636,Here we have another sum over the same set
cs-410_4_4_139,cs-410,4,4, Statistical Language Model - Part 1,"00:10:45,750","00:10:47,870",139,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=645,"But inside the sum, it's different."
cs-410_4_4_140,cs-410,4,4, Statistical Language Model - Part 1,"00:10:49,180","00:10:52,640",140,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=649,But these two sums can clearly be merged.
cs-410_4_4_141,cs-410,4,4, Statistical Language Model - Part 1,"00:10:54,300","00:10:57,530",141,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=654,"So if we do that, we'll get another form"
cs-410_4_4_142,cs-410,4,4, Statistical Language Model - Part 1,"00:10:57,530","00:11:02,140",142,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=657,of the formula that looks like
cs-410_4_4_143,cs-410,4,4, Statistical Language Model - Part 1,"00:11:04,360","00:11:06,966",143,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=664,And note that this is
cs-410_4_4_144,cs-410,4,4, Statistical Language Model - Part 1,"00:11:06,966","00:11:10,796",144,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=666,Because here we combine
cs-410_4_4_145,cs-410,4,4, Statistical Language Model - Part 1,"00:11:10,796","00:11:16,710",145,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=670,some of the query words matching in
cs-410_4_4_146,cs-410,4,4, Statistical Language Model - Part 1,"00:11:19,040","00:11:24,469",146,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=679,And the other sum now is
cs-410_4_4_147,cs-410,4,4, Statistical Language Model - Part 1,"00:11:24,469","00:11:26,988",147,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=684,And these two parts
cs-410_4_4_148,cs-410,4,4, Statistical Language Model - Part 1,"00:11:26,988","00:11:30,130",148,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=686,because these are the probabilities
cs-410_4_4_149,cs-410,4,4, Statistical Language Model - Part 1,"00:11:31,630","00:11:36,419",149,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=691,This formula is very interesting
cs-410_4_4_150,cs-410,4,4, Statistical Language Model - Part 1,"00:11:37,450","00:11:39,970",150,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=697,the match the query terms.
cs-410_4_4_151,cs-410,4,4, Statistical Language Model - Part 1,"00:11:41,340","00:11:44,210",151,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=701,"And just like in the vector space model,"
cs-410_4_4_152,cs-410,4,4, Statistical Language Model - Part 1,"00:11:46,030","00:11:49,930",152,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=706,of terms that are in the intersection of
cs-410_4_4_153,cs-410,4,4, Statistical Language Model - Part 1,"00:11:51,320","00:11:55,620",153,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=711,So it already looks a little bit
cs-410_4_4_154,cs-410,4,4, Statistical Language Model - Part 1,"00:11:55,620","00:12:02,573",154,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=715,"In fact, there's even more similarity"
cs-410_4_4_155,cs-410,4,4, Statistical Language Model - Part 1,"00:12:02,573","00:12:12,573",155,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=722,[MUSIC]
cs-410_4_5_1,cs-410,4,5, Statistical Language Model - Part 2,"00:00:00,005","00:00:03,962",1,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=0,[SOUND]
cs-410_4_5_2,cs-410,4,5, Statistical Language Model - Part 2,"00:00:12,779","00:00:15,641",2,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=12,So I showed you how we rewrite the query
cs-410_4_5_3,cs-410,4,5, Statistical Language Model - Part 2,"00:00:15,641","00:00:20,830",3,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=15,like holder which is a function into
cs-410_4_5_4,cs-410,4,5, Statistical Language Model - Part 2,"00:00:20,830","00:00:25,840",4,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=20,of this slide after if we make
cs-410_4_5_5,cs-410,4,5, Statistical Language Model - Part 2,"00:00:25,840","00:00:30,426",5,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=25,the language model based on
cs-410_4_5_6,cs-410,4,5, Statistical Language Model - Part 2,"00:00:30,426","00:00:36,160",6,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=30,"Now if you look at this rewriting,"
cs-410_4_5_7,cs-410,4,5, Statistical Language Model - Part 2,"00:00:36,160","00:00:42,470",7,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=36,The first benefit is it helps us better
cs-410_4_5_8,cs-410,4,5, Statistical Language Model - Part 2,"00:00:42,470","00:00:47,050",8,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=42,"In particular, we're going to show that"
cs-410_4_5_9,cs-410,4,5, Statistical Language Model - Part 2,"00:00:47,050","00:00:51,340",9,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=47,with the collection language model would
cs-410_4_5_10,cs-410,4,5, Statistical Language Model - Part 2,"00:00:51,340","00:00:52,412",10,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=51,and length normalization.
cs-410_4_5_11,cs-410,4,5, Statistical Language Model - Part 2,"00:00:52,412","00:00:57,645",11,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=52,The second benefit is that
cs-410_4_5_12,cs-410,4,5, Statistical Language Model - Part 2,"00:00:57,645","00:01:02,940",12,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=57,the query like holder more efficiently.
cs-410_4_5_13,cs-410,4,5, Statistical Language Model - Part 2,"00:01:02,940","00:01:06,020",13,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=62,In particular we see that
cs-410_4_5_14,cs-410,4,5, Statistical Language Model - Part 2,"00:01:06,020","00:01:07,860",14,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=66,is a sum over the match
cs-410_4_5_15,cs-410,4,5, Statistical Language Model - Part 2,"00:01:09,670","00:01:14,910",15,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=69,So this is much better than if we
cs-410_4_5_16,cs-410,4,5, Statistical Language Model - Part 2,"00:01:14,910","00:01:20,257",16,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=74,After we smooth the document the damage
cs-410_4_5_17,cs-410,4,5, Statistical Language Model - Part 2,"00:01:20,257","00:01:21,400",17,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=80,for all the words.
cs-410_4_5_18,cs-410,4,5, Statistical Language Model - Part 2,"00:01:21,400","00:01:25,760",18,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=81,So this new form of the formula is
cs-410_4_5_19,cs-410,4,5, Statistical Language Model - Part 2,"00:01:27,580","00:01:29,850",19,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=87,It's also interesting to note that
cs-410_4_5_20,cs-410,4,5, Statistical Language Model - Part 2,"00:01:29,850","00:01:34,420",20,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=89,the last term here is actually
cs-410_4_5_21,cs-410,4,5, Statistical Language Model - Part 2,"00:01:34,420","00:01:36,610",21,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=94,Since our goal is to
cs-410_4_5_22,cs-410,4,5, Statistical Language Model - Part 2,"00:01:36,610","00:01:40,610",22,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=96,the same query we can ignore this term for
cs-410_4_5_23,cs-410,4,5, Statistical Language Model - Part 2,"00:01:40,610","00:01:43,650",23,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=100,Because it's going to be the same for
cs-410_4_5_24,cs-410,4,5, Statistical Language Model - Part 2,"00:01:43,650","00:01:46,630",24,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=103,Ignoring it wouldn't affect
cs-410_4_5_25,cs-410,4,5, Statistical Language Model - Part 2,"00:01:49,070","00:01:51,890",25,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=109,"Inside the sum, we"
cs-410_4_5_26,cs-410,4,5, Statistical Language Model - Part 2,"00:01:52,940","00:01:57,060",26,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=112,also see that each matched query
cs-410_4_5_27,cs-410,4,5, Statistical Language Model - Part 2,"00:01:58,510","00:02:01,990",27,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=118,And this weight actually
cs-410_4_5_28,cs-410,4,5, Statistical Language Model - Part 2,"00:02:01,990","00:02:07,070",28,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=121,is very interesting because it
cs-410_4_5_29,cs-410,4,5, Statistical Language Model - Part 2,"00:02:07,070","00:02:11,830",29,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=127,First we can already see it has
cs-410_4_5_30,cs-410,4,5, Statistical Language Model - Part 2,"00:02:11,830","00:02:14,250",30,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=131,just like in the vector space model.
cs-410_4_5_31,cs-410,4,5, Statistical Language Model - Part 2,"00:02:14,250","00:02:16,240",31,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=134,"When we take a thought product,"
cs-410_4_5_32,cs-410,4,5, Statistical Language Model - Part 2,"00:02:16,240","00:02:20,940",32,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=136,we see the word frequency in
cs-410_4_5_33,cs-410,4,5, Statistical Language Model - Part 2,"00:02:22,250","00:02:27,670",33,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=142,And so naturally this part would
cs-410_4_5_34,cs-410,4,5, Statistical Language Model - Part 2,"00:02:27,670","00:02:31,510",34,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=147,element from the documented vector.
cs-410_4_5_35,cs-410,4,5, Statistical Language Model - Part 2,"00:02:31,510","00:02:34,170",35,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=151,And here indeed we can see it actually
cs-410_4_5_36,cs-410,4,5, Statistical Language Model - Part 2,"00:02:35,430","00:02:39,950",36,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=155,encodes a weight that has similar
cs-410_4_5_37,cs-410,4,5, Statistical Language Model - Part 2,"00:02:41,160","00:02:43,660",37,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=161,"I'll let you examine it, can you see it?"
cs-410_4_5_38,cs-410,4,5, Statistical Language Model - Part 2,"00:02:43,660","00:02:46,110",38,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=163,Can you see which part is capturing TF?
cs-410_4_5_39,cs-410,4,5, Statistical Language Model - Part 2,"00:02:46,110","00:02:49,870",39,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=166,And which part is
cs-410_4_5_40,cs-410,4,5, Statistical Language Model - Part 2,"00:02:51,680","00:02:54,630",40,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=171,So if want you can pause
cs-410_4_5_41,cs-410,4,5, Statistical Language Model - Part 2,"00:02:55,830","00:03:02,640",41,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=175,So have you noticed that this P sub
cs-410_4_5_42,cs-410,4,5, Statistical Language Model - Part 2,"00:03:02,640","00:03:08,240",42,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=182,in the sense that if a word occurs
cs-410_4_5_43,cs-410,4,5, Statistical Language Model - Part 2,"00:03:08,240","00:03:11,980",43,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=188,then the s made through probability
cs-410_4_5_44,cs-410,4,5, Statistical Language Model - Part 2,"00:03:11,980","00:03:17,694",44,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=191,So this means this term is really
cs-410_4_5_45,cs-410,4,5, Statistical Language Model - Part 2,"00:03:17,694","00:03:22,324",45,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=197,Now have you also noticed that
cs-410_4_5_46,cs-410,4,5, Statistical Language Model - Part 2,"00:03:22,324","00:03:26,090",46,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=202,is actually achieving the factor of IDF?
cs-410_4_5_47,cs-410,4,5, Statistical Language Model - Part 2,"00:03:26,090","00:03:29,870",47,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=206,"Why, because this is the popularity"
cs-410_4_5_48,cs-410,4,5, Statistical Language Model - Part 2,"00:03:31,750","00:03:37,110",48,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=211,"But it's in the denominator, so if the"
cs-410_4_5_49,cs-410,4,5, Statistical Language Model - Part 2,"00:03:37,110","00:03:39,700",49,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=217,then the weight is actually smaller.
cs-410_4_5_50,cs-410,4,5, Statistical Language Model - Part 2,"00:03:39,700","00:03:41,790",50,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=219,And this means a popular term.
cs-410_4_5_51,cs-410,4,5, Statistical Language Model - Part 2,"00:03:41,790","00:03:45,990",51,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=221,We actually have a smaller weight and this
cs-410_4_5_52,cs-410,4,5, Statistical Language Model - Part 2,"00:03:47,040","00:03:50,330",52,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=227,Only that we now have
cs-410_4_5_53,cs-410,4,5, Statistical Language Model - Part 2,"00:03:51,550","00:03:55,920",53,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=231,Remember IDF has a logarithm
cs-410_4_5_54,cs-410,4,5, Statistical Language Model - Part 2,"00:03:55,920","00:03:57,290",54,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=235,But here we have something different.
cs-410_4_5_55,cs-410,4,5, Statistical Language Model - Part 2,"00:03:58,300","00:04:02,460",55,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=238,But intuitively it
cs-410_4_5_56,cs-410,4,5, Statistical Language Model - Part 2,"00:04:02,460","00:04:06,550",56,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=242,"Interestingly, we also have something"
cs-410_4_5_57,cs-410,4,5, Statistical Language Model - Part 2,"00:04:07,820","00:04:13,470",57,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=247,"Again, can you see which factor is related"
cs-410_4_5_58,cs-410,4,5, Statistical Language Model - Part 2,"00:04:14,790","00:04:18,350",58,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=254,What I just say is that this term
cs-410_4_5_59,cs-410,4,5, Statistical Language Model - Part 2,"00:04:19,560","00:04:24,700",59,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=259,"This collection probability,"
cs-410_4_5_60,cs-410,4,5, Statistical Language Model - Part 2,"00:04:24,700","00:04:29,360",60,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=264,this term here is actually related
cs-410_4_5_61,cs-410,4,5, Statistical Language Model - Part 2,"00:04:29,360","00:04:35,110",61,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=269,"In particular, F of sub d might"
cs-410_4_5_62,cs-410,4,5, Statistical Language Model - Part 2,"00:04:35,110","00:04:40,480",62,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=275,So it encodes how much probability
cs-410_4_5_63,cs-410,4,5, Statistical Language Model - Part 2,"00:04:41,740","00:04:43,700",63,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=281,How much smoothing do we want to do?
cs-410_4_5_64,cs-410,4,5, Statistical Language Model - Part 2,"00:04:43,700","00:04:46,470",64,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=283,"Intuitively, if a document is long,"
cs-410_4_5_65,cs-410,4,5, Statistical Language Model - Part 2,"00:04:46,470","00:04:50,980",65,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=286,then we need to do less smoothing because
cs-410_4_5_66,cs-410,4,5, Statistical Language Model - Part 2,"00:04:50,980","00:04:55,720",66,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=290,We probably have observed all the words
cs-410_4_5_67,cs-410,4,5, Statistical Language Model - Part 2,"00:04:55,720","00:05:00,900",67,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=295,But if the document is short then r of
cs-410_4_5_68,cs-410,4,5, Statistical Language Model - Part 2,"00:05:00,900","00:05:02,432",68,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=300,We need to do more smoothing.
cs-410_4_5_69,cs-410,4,5, Statistical Language Model - Part 2,"00:05:02,432","00:05:06,110",69,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=302,It's likey there are words that have
cs-410_4_5_70,cs-410,4,5, Statistical Language Model - Part 2,"00:05:06,110","00:05:12,250",70,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=306,So this term appears to paralyze
cs-410_4_5_71,cs-410,4,5, Statistical Language Model - Part 2,"00:05:12,250","00:05:19,100",71,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=312,other sub D would tend to be longer
cs-410_4_5_72,cs-410,4,5, Statistical Language Model - Part 2,"00:05:19,100","00:05:23,065",72,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=319,But note that alpha sub d
cs-410_4_5_73,cs-410,4,5, Statistical Language Model - Part 2,"00:05:23,065","00:05:28,570",73,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=323,this may not actually be necessary
cs-410_4_5_74,cs-410,4,5, Statistical Language Model - Part 2,"00:05:28,570","00:05:30,600",74,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=328,The effect is not so clear yet.
cs-410_4_5_75,cs-410,4,5, Statistical Language Model - Part 2,"00:05:31,930","00:05:36,570",75,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=331,"But as we will see later, when we"
cs-410_4_5_76,cs-410,4,5, Statistical Language Model - Part 2,"00:05:36,570","00:05:40,080",76,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=336,it turns out that they do
cs-410_4_5_77,cs-410,4,5, Statistical Language Model - Part 2,"00:05:40,080","00:05:42,730",77,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=340,Just like in TF-IDF weighting and
cs-410_4_5_78,cs-410,4,5, Statistical Language Model - Part 2,"00:05:42,730","00:05:45,880",78,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=342,document length normalization
cs-410_4_5_79,cs-410,4,5, Statistical Language Model - Part 2,"00:05:47,490","00:05:50,670",79,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=347,"So, that's a very interesting"
cs-410_4_5_80,cs-410,4,5, Statistical Language Model - Part 2,"00:05:50,670","00:05:54,880",80,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=350,we don't even have to think about
cs-410_4_5_81,cs-410,4,5, Statistical Language Model - Part 2,"00:05:54,880","00:05:59,910",81,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=354,We just need to assume that if we smooth
cs-410_4_5_82,cs-410,4,5, Statistical Language Model - Part 2,"00:05:59,910","00:06:05,480",82,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=359,then we would have a formula that
cs-410_4_5_83,cs-410,4,5, Statistical Language Model - Part 2,"00:06:05,480","00:06:06,710",83,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=365,documents length violation.
cs-410_4_5_84,cs-410,4,5, Statistical Language Model - Part 2,"00:06:08,210","00:06:13,150",84,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=368,What's also interesting that we have
cs-410_4_5_85,cs-410,4,5, Statistical Language Model - Part 2,"00:06:14,180","00:06:17,890",85,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=374,And see we have not heuristically
cs-410_4_5_86,cs-410,4,5, Statistical Language Model - Part 2,"00:06:19,310","00:06:23,790",86,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=379,"In fact, you can think about why"
cs-410_4_5_87,cs-410,4,5, Statistical Language Model - Part 2,"00:06:23,790","00:06:28,651",87,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=383,You look at the assumptions that
cs-410_4_5_88,cs-410,4,5, Statistical Language Model - Part 2,"00:06:28,651","00:06:33,720",88,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=388,it's because we have used a logarithm
cs-410_4_5_89,cs-410,4,5, Statistical Language Model - Part 2,"00:06:33,720","00:06:38,090",89,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=393,And we turned the product into a sum
cs-410_4_5_90,cs-410,4,5, Statistical Language Model - Part 2,"00:06:38,090","00:06:39,239",90,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=398,that's why we have this logarithm.
cs-410_4_5_91,cs-410,4,5, Statistical Language Model - Part 2,"00:06:40,470","00:06:44,740",91,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=400,Note that if only want to heuristically
cs-410_4_5_92,cs-410,4,5, Statistical Language Model - Part 2,"00:06:44,740","00:06:48,830",92,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=404,"IDF weighting, we don't necessary"
cs-410_4_5_93,cs-410,4,5, Statistical Language Model - Part 2,"00:06:48,830","00:06:53,700",93,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=408,"Imagine if we drop this logarithm,"
cs-410_4_5_94,cs-410,4,5, Statistical Language Model - Part 2,"00:06:55,010","00:06:59,740",94,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=415,But what's nice with problem risk modeling
cs-410_4_5_95,cs-410,4,5, Statistical Language Model - Part 2,"00:06:59,740","00:07:01,950",95,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=419,the logarithm function here.
cs-410_4_5_96,cs-410,4,5, Statistical Language Model - Part 2,"00:07:01,950","00:07:07,510",96,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=421,And that's basically a fixed form
cs-410_4_5_97,cs-410,4,5, Statistical Language Model - Part 2,"00:07:07,510","00:07:13,010",97,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=427,"really have to heuristically design,"
cs-410_4_5_98,cs-410,4,5, Statistical Language Model - Part 2,"00:07:13,010","00:07:18,110",98,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=433,the logarithm the model probably won't
cs-410_4_5_99,cs-410,4,5, Statistical Language Model - Part 2,"00:07:19,400","00:07:24,300",99,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=439,So a nice property of problem risk
cs-410_4_5_100,cs-410,4,5, Statistical Language Model - Part 2,"00:07:24,300","00:07:28,260",100,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=444,assumptions and the probability rules
cs-410_4_5_101,cs-410,4,5, Statistical Language Model - Part 2,"00:07:28,260","00:07:33,390",101,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=448,And the formula would have
cs-410_4_5_102,cs-410,4,5, Statistical Language Model - Part 2,"00:07:34,600","00:07:38,540",102,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=454,And if we heuristically design
cs-410_4_5_103,cs-410,4,5, Statistical Language Model - Part 2,"00:07:38,540","00:07:40,530",103,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=458,end up having such a specific formula.
cs-410_4_5_104,cs-410,4,5, Statistical Language Model - Part 2,"00:07:41,700","00:07:46,940",104,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=461,"So to summarize, we talked about the need"
cs-410_4_5_105,cs-410,4,5, Statistical Language Model - Part 2,"00:07:46,940","00:07:52,470",105,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=466,Otherwise it would give zero probability
cs-410_4_5_106,cs-410,4,5, Statistical Language Model - Part 2,"00:07:52,470","00:07:57,450",106,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=472,that's not good for
cs-410_4_5_107,cs-410,4,5, Statistical Language Model - Part 2,"00:07:59,370","00:08:03,720",107,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=479,"It's also necessary, in general,"
cs-410_4_5_108,cs-410,4,5, Statistical Language Model - Part 2,"00:08:03,720","00:08:08,730",108,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=483,the model represent
cs-410_4_5_109,cs-410,4,5, Statistical Language Model - Part 2,"00:08:08,730","00:08:16,210",109,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=488,The general idea of smoothing in retrieval
cs-410_4_5_110,cs-410,4,5, Statistical Language Model - Part 2,"00:08:17,800","00:08:22,760",110,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=497,to give us some clue about which unseen
cs-410_4_5_111,cs-410,4,5, Statistical Language Model - Part 2,"00:08:22,760","00:08:26,840",111,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=502,"That is, the probability of an unseen"
cs-410_4_5_112,cs-410,4,5, Statistical Language Model - Part 2,"00:08:26,840","00:08:28,340",112,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=506,to its probability in the collection.
cs-410_4_5_113,cs-410,4,5, Statistical Language Model - Part 2,"00:08:29,610","00:08:34,330",113,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=509,"With this assumption, we've shown that we"
cs-410_4_5_114,cs-410,4,5, Statistical Language Model - Part 2,"00:08:34,330","00:08:38,280",114,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=514,query likelihood that has
cs-410_4_5_115,cs-410,4,5, Statistical Language Model - Part 2,"00:08:38,280","00:08:39,970",115,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=518,document length normalization.
cs-410_4_5_116,cs-410,4,5, Statistical Language Model - Part 2,"00:08:39,970","00:08:42,210",116,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=519,"We also see that, through some rewriting,"
cs-410_4_5_117,cs-410,4,5, Statistical Language Model - Part 2,"00:08:42,210","00:08:47,080",117,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=522,the scoring of such a ranking function
cs-410_4_5_118,cs-410,4,5, Statistical Language Model - Part 2,"00:08:47,080","00:08:50,530",118,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=527,"matched query terms,"
cs-410_4_5_119,cs-410,4,5, Statistical Language Model - Part 2,"00:08:50,530","00:08:54,500",119,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=530,"But, the actual ranking"
cs-410_4_5_120,cs-410,4,5, Statistical Language Model - Part 2,"00:08:54,500","00:08:59,010",120,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=534,automatically by the probability rules and
cs-410_4_5_121,cs-410,4,5, Statistical Language Model - Part 2,"00:08:59,010","00:09:02,210",121,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=539,And like in the vector space model
cs-410_4_5_122,cs-410,4,5, Statistical Language Model - Part 2,"00:09:02,210","00:09:04,580",122,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=542,think about the form of the function.
cs-410_4_5_123,cs-410,4,5, Statistical Language Model - Part 2,"00:09:04,580","00:09:09,234",123,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=544,"However, we still need to address"
cs-410_4_5_124,cs-410,4,5, Statistical Language Model - Part 2,"00:09:09,234","00:09:11,652",124,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=549,smooth the document and the model.
cs-410_4_5_125,cs-410,4,5, Statistical Language Model - Part 2,"00:09:11,652","00:09:14,859",125,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=551,How exactly we should
cs-410_4_5_126,cs-410,4,5, Statistical Language Model - Part 2,"00:09:14,859","00:09:19,223",126,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=554,model based on the connection
cs-410_4_5_127,cs-410,4,5, Statistical Language Model - Part 2,"00:09:19,223","00:09:24,226",127,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=559,the maximum micro is made of and
cs-410_4_5_128,cs-410,4,5, Statistical Language Model - Part 2,"00:09:24,226","00:09:34,226",128,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=564,[MUSIC]
cs-410_4_6_1,cs-410,4,6, Smoothing Methods - Part 1,"00:00:00,008","00:00:03,638",1,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=0,[SOUND]
cs-410_4_6_2,cs-410,4,6, Smoothing Methods - Part 1,"00:00:07,832","00:00:09,846",2,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=7,This lecture is about the specific
cs-410_4_6_3,cs-410,4,6, Smoothing Methods - Part 1,"00:00:09,846","00:00:14,580",3,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=9,smoothing methods for language models
cs-410_4_6_4,cs-410,4,6, Smoothing Methods - Part 1,"00:00:16,560","00:00:21,030",4,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=16,"In this lecture, we will continue"
cs-410_4_6_5,cs-410,4,6, Smoothing Methods - Part 1,"00:00:21,030","00:00:26,020",5,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=21,"information retrieval, particularly"
cs-410_4_6_6,cs-410,4,6, Smoothing Methods - Part 1,"00:00:26,020","00:00:29,485",6,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=26,And we're going to talk about specifically
cs-410_4_6_7,cs-410,4,6, Smoothing Methods - Part 1,"00:00:29,485","00:00:30,856",7,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=29,such a retrieval function.
cs-410_4_6_8,cs-410,4,6, Smoothing Methods - Part 1,"00:00:33,591","00:00:39,021",8,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=33,So this is a slide from a previous
cs-410_4_6_9,cs-410,4,6, Smoothing Methods - Part 1,"00:00:39,021","00:00:44,638",9,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=39,likelihood ranking and smoothing
cs-410_4_6_10,cs-410,4,6, Smoothing Methods - Part 1,"00:00:44,638","00:00:50,002",10,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=44,we add up having a retrieval function
cs-410_4_6_11,cs-410,4,6, Smoothing Methods - Part 1,"00:00:50,002","00:00:57,370",11,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=50,So this is the retrieval function based on
cs-410_4_6_12,cs-410,4,6, Smoothing Methods - Part 1,"00:00:57,370","00:01:02,738",12,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=57,You can see it's a sum of all
cs-410_4_6_13,cs-410,4,6, Smoothing Methods - Part 1,"00:01:02,738","00:01:07,506",13,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=62,And inside its sum is the count
cs-410_4_6_14,cs-410,4,6, Smoothing Methods - Part 1,"00:01:07,506","00:01:11,070",14,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=67,some weight for the term in the document.
cs-410_4_6_15,cs-410,4,6, Smoothing Methods - Part 1,"00:01:12,240","00:01:18,170",15,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=72,"We have t of i, the f weight here, and"
cs-410_4_6_16,cs-410,4,6, Smoothing Methods - Part 1,"00:01:20,300","00:01:24,793",16,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=80,So clearly if we want to implement this
cs-410_4_6_17,cs-410,4,6, Smoothing Methods - Part 1,"00:01:24,793","00:01:27,650",17,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=84,we still need to figure
cs-410_4_6_18,cs-410,4,6, Smoothing Methods - Part 1,"00:01:27,650","00:01:33,730",18,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=87,"In particular, we're going to need to"
cs-410_4_6_19,cs-410,4,6, Smoothing Methods - Part 1,"00:01:33,730","00:01:39,000",19,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=93,of a word exactly and how do we set alpha.
cs-410_4_6_20,cs-410,4,6, Smoothing Methods - Part 1,"00:01:40,270","00:01:44,410",20,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=100,"So in order to answer this question,"
cs-410_4_6_21,cs-410,4,6, Smoothing Methods - Part 1,"00:01:44,410","00:01:47,760",21,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=104,"smoothing methods, and"
cs-410_4_6_22,cs-410,4,6, Smoothing Methods - Part 1,"00:01:48,900","00:01:50,512",22,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=108,We're going to talk about
cs-410_4_6_23,cs-410,4,6, Smoothing Methods - Part 1,"00:01:50,512","00:01:55,575",23,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=110,The first is simple linear
cs-410_4_6_24,cs-410,4,6, Smoothing Methods - Part 1,"00:01:55,575","00:01:59,910",24,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=115,And this is also called
cs-410_4_6_25,cs-410,4,6, Smoothing Methods - Part 1,"00:02:01,170","00:02:04,140",25,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=121,So the idea is actually very simple.
cs-410_4_6_26,cs-410,4,6, Smoothing Methods - Part 1,"00:02:04,140","00:02:09,150",26,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=124,This picture shows how
cs-410_4_6_27,cs-410,4,6, Smoothing Methods - Part 1,"00:02:09,150","00:02:12,440",27,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=129,language model by using
cs-410_4_6_28,cs-410,4,6, Smoothing Methods - Part 1,"00:02:12,440","00:02:17,950",28,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=132,That gives us word counts normalized by
cs-410_4_6_29,cs-410,4,6, Smoothing Methods - Part 1,"00:02:17,950","00:02:21,130",29,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=137,The idea of using this method
cs-410_4_6_30,cs-410,4,6, Smoothing Methods - Part 1,"00:02:22,230","00:02:26,480",30,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=142,is to maximize the probability
cs-410_4_6_31,cs-410,4,6, Smoothing Methods - Part 1,"00:02:26,480","00:02:31,460",31,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=146,"As a result,"
cs-410_4_6_32,cs-410,4,6, Smoothing Methods - Part 1,"00:02:31,460","00:02:36,210",32,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=151,"in the text, it's going to get"
cs-410_4_6_33,cs-410,4,6, Smoothing Methods - Part 1,"00:02:37,810","00:02:42,620",33,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=157,"So the idea of smoothing, then,"
cs-410_4_6_34,cs-410,4,6, Smoothing Methods - Part 1,"00:02:42,620","00:02:47,158",34,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=162,where this word is not going to have
cs-410_4_6_35,cs-410,4,6, Smoothing Methods - Part 1,"00:02:47,158","00:02:50,860",35,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=167,nonzero probability should
cs-410_4_6_36,cs-410,4,6, Smoothing Methods - Part 1,"00:02:50,860","00:02:55,560",36,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=170,So we can note that network has
cs-410_4_6_37,cs-410,4,6, Smoothing Methods - Part 1,"00:02:55,560","00:03:01,367",37,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=175,So in this approach what we do is we do
cs-410_4_6_38,cs-410,4,6, Smoothing Methods - Part 1,"00:03:01,367","00:03:06,655",38,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=181,likelihood placement here and
cs-410_4_6_39,cs-410,4,6, Smoothing Methods - Part 1,"00:03:06,655","00:03:13,040",39,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=186,is computed by the smoothing parameter
cs-410_4_6_40,cs-410,4,6, Smoothing Methods - Part 1,"00:03:13,040","00:03:15,817",40,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=193,So this is a smoothing parameter.
cs-410_4_6_41,cs-410,4,6, Smoothing Methods - Part 1,"00:03:15,817","00:03:20,651",41,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=195,"The larger lambda is,"
cs-410_4_6_42,cs-410,4,6, Smoothing Methods - Part 1,"00:03:20,651","00:03:22,828",42,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=200,"So by mixing them together,"
cs-410_4_6_43,cs-410,4,6, Smoothing Methods - Part 1,"00:03:22,828","00:03:29,060",43,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=202,we achieve the goal of assigning nonzero
cs-410_4_6_44,cs-410,4,6, Smoothing Methods - Part 1,"00:03:29,060","00:03:31,400",44,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=209,So let's see how it works for
cs-410_4_6_45,cs-410,4,6, Smoothing Methods - Part 1,"00:03:32,430","00:03:36,790",45,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=212,"For example, if we compute"
cs-410_4_6_46,cs-410,4,6, Smoothing Methods - Part 1,"00:03:37,940","00:03:41,080",46,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=217,Now the maximum likelihood
cs-410_4_6_47,cs-410,4,6, Smoothing Methods - Part 1,"00:03:41,080","00:03:43,150",47,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=221,that's going to be here.
cs-410_4_6_48,cs-410,4,6, Smoothing Methods - Part 1,"00:03:44,320","00:03:47,740",48,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=224,But the collection probability is this.
cs-410_4_6_49,cs-410,4,6, Smoothing Methods - Part 1,"00:03:47,740","00:03:50,960",49,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=227,So we'll just combine them
cs-410_4_6_50,cs-410,4,6, Smoothing Methods - Part 1,"00:03:53,630","00:04:00,085",50,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=233,"We can also see the word network,"
cs-410_4_6_51,cs-410,4,6, Smoothing Methods - Part 1,"00:04:00,085","00:04:05,305",51,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=240,now is getting a non-zero
cs-410_4_6_52,cs-410,4,6, Smoothing Methods - Part 1,"00:04:05,305","00:04:11,992",52,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=245,And that's because the count is
cs-410_4_6_53,cs-410,4,6, Smoothing Methods - Part 1,"00:04:11,992","00:04:19,097",53,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=251,"But this part is nonzero, and"
cs-410_4_6_54,cs-410,4,6, Smoothing Methods - Part 1,"00:04:19,097","00:04:24,109",54,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=259,Now if you think about this and
cs-410_4_6_55,cs-410,4,6, Smoothing Methods - Part 1,"00:04:24,109","00:04:29,250",55,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=264,sub d in this smoothing
cs-410_4_6_56,cs-410,4,6, Smoothing Methods - Part 1,"00:04:29,250","00:04:34,830",56,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=269,Because that's remember the coefficient
cs-410_4_6_57,cs-410,4,6, Smoothing Methods - Part 1,"00:04:34,830","00:04:40,256",57,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=274,of the word given by the collection
cs-410_4_6_58,cs-410,4,6, Smoothing Methods - Part 1,"00:04:40,256","00:04:43,340",58,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=280,"Okay, so"
cs-410_4_6_59,cs-410,4,6, Smoothing Methods - Part 1,"00:04:43,340","00:04:47,903",59,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=283,The second one is similar but
cs-410_4_6_60,cs-410,4,6, Smoothing Methods - Part 1,"00:04:47,903","00:04:49,565",60,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=287,linear interpolation.
cs-410_4_6_61,cs-410,4,6, Smoothing Methods - Part 1,"00:04:49,565","00:04:52,570",61,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=289,"It's often called Dirichlet Prior,"
cs-410_4_6_62,cs-410,4,6, Smoothing Methods - Part 1,"00:04:54,540","00:04:59,015",62,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=294,So again here we face problem
cs-410_4_6_63,cs-410,4,6, Smoothing Methods - Part 1,"00:04:59,015","00:05:01,565",63,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=299,an unseen word like network.
cs-410_4_6_64,cs-410,4,6, Smoothing Methods - Part 1,"00:05:03,765","00:05:06,957",64,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=303,Again we will use the collection
cs-410_4_6_65,cs-410,4,6, Smoothing Methods - Part 1,"00:05:06,957","00:05:09,707",65,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=306,we're going to combine them
cs-410_4_6_66,cs-410,4,6, Smoothing Methods - Part 1,"00:05:09,707","00:05:14,739",66,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=309,The formula first can be seen as
cs-410_4_6_67,cs-410,4,6, Smoothing Methods - Part 1,"00:05:14,739","00:05:20,258",67,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=314,likelihood estimate and
cs-410_4_6_68,cs-410,4,6, Smoothing Methods - Part 1,"00:05:20,258","00:05:23,580",68,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=320,as in the J-M smoothing method.
cs-410_4_6_69,cs-410,4,6, Smoothing Methods - Part 1,"00:05:23,580","00:05:28,388",69,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=323,Only that the coefficient now
cs-410_4_6_70,cs-410,4,6, Smoothing Methods - Part 1,"00:05:28,388","00:05:31,532",70,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=328,"but a dynamic coefficient in this form,"
cs-410_4_6_71,cs-410,4,6, Smoothing Methods - Part 1,"00:05:31,532","00:05:36,760",71,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=331,"where mu is a parameter,"
cs-410_4_6_72,cs-410,4,6, Smoothing Methods - Part 1,"00:05:36,760","00:05:40,550",72,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=336,And you can see if we
cs-410_4_6_73,cs-410,4,6, Smoothing Methods - Part 1,"00:05:40,550","00:05:44,690",73,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=340,the effect is that a long document would
cs-410_4_6_74,cs-410,4,6, Smoothing Methods - Part 1,"00:05:46,090","00:05:49,200",74,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=346,Because a long document
cs-410_4_6_75,cs-410,4,6, Smoothing Methods - Part 1,"00:05:49,200","00:05:53,140",75,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=349,therefore the coefficient
cs-410_4_6_76,cs-410,4,6, Smoothing Methods - Part 1,"00:05:53,140","00:05:59,949",76,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=353,And so a long document would have
cs-410_4_6_77,cs-410,4,6, Smoothing Methods - Part 1,"00:05:59,949","00:06:05,734",77,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=359,So this seems to make more sense
cs-410_4_6_78,cs-410,4,6, Smoothing Methods - Part 1,"00:06:05,734","00:06:08,979",78,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=365,"Of course,"
cs-410_4_6_79,cs-410,4,6, Smoothing Methods - Part 1,"00:06:08,979","00:06:12,156",79,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=368,that the two coefficients would sum to 1.
cs-410_4_6_80,cs-410,4,6, Smoothing Methods - Part 1,"00:06:12,156","00:06:16,400",80,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=372,Now this is one way to
cs-410_4_6_81,cs-410,4,6, Smoothing Methods - Part 1,"00:06:16,400","00:06:21,080",81,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=376,"Basically, it means it's a dynamic"
cs-410_4_6_82,cs-410,4,6, Smoothing Methods - Part 1,"00:06:22,790","00:06:27,737",82,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=382,There is another way to understand
cs-410_4_6_83,cs-410,4,6, Smoothing Methods - Part 1,"00:06:27,737","00:06:31,620",83,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=387,"easier to remember, and"
cs-410_4_6_84,cs-410,4,6, Smoothing Methods - Part 1,"00:06:33,310","00:06:38,878",84,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=393,So it's easier to see how we can rewrite
cs-410_4_6_85,cs-410,4,6, Smoothing Methods - Part 1,"00:06:38,878","00:06:42,847",85,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=398,Now in this form we can easily
cs-410_4_6_86,cs-410,4,6, Smoothing Methods - Part 1,"00:06:42,847","00:06:47,060",86,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=402,"the maximum likelihood estimate,"
cs-410_4_6_87,cs-410,4,6, Smoothing Methods - Part 1,"00:06:47,060","00:06:53,346",87,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=407,So normalize the count
cs-410_4_6_88,cs-410,4,6, Smoothing Methods - Part 1,"00:06:53,346","00:07:00,750",88,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=413,So in this form we can see what we did is
cs-410_4_6_89,cs-410,4,6, Smoothing Methods - Part 1,"00:07:01,800","00:07:03,230",89,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=421,So what does this mean?
cs-410_4_6_90,cs-410,4,6, Smoothing Methods - Part 1,"00:07:03,230","00:07:08,042",90,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=423,"Well, this is basically something related"
cs-410_4_6_91,cs-410,4,6, Smoothing Methods - Part 1,"00:07:08,042","00:07:09,180",91,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=428,the collection.
cs-410_4_6_92,cs-410,4,6, Smoothing Methods - Part 1,"00:07:10,390","00:07:13,225",92,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=430,And we multiply that by the parameter mu.
cs-410_4_6_93,cs-410,4,6, Smoothing Methods - Part 1,"00:07:14,510","00:07:18,577",93,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=434,And when we combine this
cs-410_4_6_94,cs-410,4,6, Smoothing Methods - Part 1,"00:07:18,577","00:07:24,265",94,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=438,essentially we are adding
cs-410_4_6_95,cs-410,4,6, Smoothing Methods - Part 1,"00:07:24,265","00:07:31,090",95,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=444,We pretend every word has
cs-410_4_6_96,cs-410,4,6, Smoothing Methods - Part 1,"00:07:31,090","00:07:35,290",96,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=451,So the total count would be
cs-410_4_6_97,cs-410,4,6, Smoothing Methods - Part 1,"00:07:35,290","00:07:38,730",97,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=455,the actual count of
cs-410_4_6_98,cs-410,4,6, Smoothing Methods - Part 1,"00:07:39,950","00:07:46,020",98,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=459,"As a result, in total we would"
cs-410_4_6_99,cs-410,4,6, Smoothing Methods - Part 1,"00:07:46,020","00:07:49,640",99,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=466,Why?
cs-410_4_6_100,cs-410,4,6, Smoothing Methods - Part 1,"00:07:50,770","00:07:55,480",100,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=470,"over all the words, then we'll see the"
cs-410_4_6_101,cs-410,4,6, Smoothing Methods - Part 1,"00:07:55,480","00:07:57,380",101,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=475,and that gives us just mu.
cs-410_4_6_102,cs-410,4,6, Smoothing Methods - Part 1,"00:07:57,380","00:08:00,190",102,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=477,So this is the total number of
cs-410_4_6_103,cs-410,4,6, Smoothing Methods - Part 1,"00:08:01,550","00:08:05,270",103,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=481,And so
cs-410_4_6_104,cs-410,4,6, Smoothing Methods - Part 1,"00:08:05,270","00:08:12,590",104,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=485,"So in this case, we can easily"
cs-410_4_6_105,cs-410,4,6, Smoothing Methods - Part 1,"00:08:13,920","00:08:18,130",105,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=493,add this as a pseudocount to this data.
cs-410_4_6_106,cs-410,4,6, Smoothing Methods - Part 1,"00:08:18,130","00:08:22,877",106,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=498,Pretend we actually augment the data
cs-410_4_6_107,cs-410,4,6, Smoothing Methods - Part 1,"00:08:22,877","00:08:26,022",107,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=502,defined by the collection language model.
cs-410_4_6_108,cs-410,4,6, Smoothing Methods - Part 1,"00:08:26,022","00:08:30,201",108,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=506,"As a result, we have more counts is that"
cs-410_4_6_109,cs-410,4,6, Smoothing Methods - Part 1,"00:08:30,201","00:08:35,710",109,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=510,the total counts for
cs-410_4_6_110,cs-410,4,6, Smoothing Methods - Part 1,"00:08:35,710","00:08:41,499",110,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=515,"And as a result, even if a word has zero"
cs-410_4_6_111,cs-410,4,6, Smoothing Methods - Part 1,"00:08:41,499","00:08:47,115",111,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=521,"count here, then it would still have"
cs-410_4_6_112,cs-410,4,6, Smoothing Methods - Part 1,"00:08:47,115","00:08:49,750",112,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=527,So this is how this method works.
cs-410_4_6_113,cs-410,4,6, Smoothing Methods - Part 1,"00:08:49,750","00:08:52,650",113,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=529,Let's also take a look at
cs-410_4_6_114,cs-410,4,6, Smoothing Methods - Part 1,"00:08:52,650","00:08:58,580",114,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=532,So for text again we will
cs-410_4_6_115,cs-410,4,6, Smoothing Methods - Part 1,"00:08:58,580","00:09:03,000",115,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=538,"that we actually observe, but"
cs-410_4_6_116,cs-410,4,6, Smoothing Methods - Part 1,"00:09:03,000","00:09:05,725",116,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=543,And so the probability of
cs-410_4_6_117,cs-410,4,6, Smoothing Methods - Part 1,"00:09:05,725","00:09:11,051",117,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=545,"Naturally, the probability of"
cs-410_4_6_118,cs-410,4,6, Smoothing Methods - Part 1,"00:09:11,051","00:09:14,410",118,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=551,And so here you can also see
cs-410_4_6_119,cs-410,4,6, Smoothing Methods - Part 1,"00:09:15,600","00:09:16,850",119,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=555,Can you see it?
cs-410_4_6_120,cs-410,4,6, Smoothing Methods - Part 1,"00:09:16,850","00:09:19,020",120,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=556,"If you want to think about it,"
cs-410_4_6_121,cs-410,4,6, Smoothing Methods - Part 1,"00:09:20,590","00:09:25,618",121,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=560,But you'll notice that this
cs-410_4_6_122,cs-410,4,6, Smoothing Methods - Part 1,"00:09:25,618","00:09:29,122",122,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=565,"So we can see, in this case,"
cs-410_4_6_123,cs-410,4,6, Smoothing Methods - Part 1,"00:09:29,122","00:09:34,089",123,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=569,"alpha sub d does depend on the document,"
cs-410_4_6_124,cs-410,4,6, Smoothing Methods - Part 1,"00:09:34,089","00:09:39,787",124,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=574,because this length
cs-410_4_6_125,cs-410,4,6, Smoothing Methods - Part 1,"00:09:39,787","00:09:44,609",125,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=579,"whereas in the linear interpolation,"
cs-410_4_6_126,cs-410,4,6, Smoothing Methods - Part 1,"00:09:44,609","00:09:50,622",126,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=584,"the J-M smoothing method,"
cs-410_4_6_127,cs-410,4,6, Smoothing Methods - Part 1,"00:09:50,622","00:09:54,919",127,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=590,[MUSIC]
cs-410_4_7_1,cs-410,4,7, Smoothing Methods - Part 2,"00:00:00,006","00:00:03,253",1,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=0,[SOUND]
cs-410_4_7_2,cs-410,4,7, Smoothing Methods - Part 2,"00:00:13,295","00:00:15,326",2,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=13,So let's plug in these model masses
cs-410_4_7_3,cs-410,4,7, Smoothing Methods - Part 2,"00:00:15,326","00:00:18,610",3,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=15,into the ranking function to
cs-410_4_7_4,cs-410,4,7, Smoothing Methods - Part 2,"00:00:18,610","00:00:20,780",4,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=18,This is a general smoothing.
cs-410_4_7_5,cs-410,4,7, Smoothing Methods - Part 2,"00:00:20,780","00:00:24,570",5,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=20,So a general ranking function for
cs-410_4_7_6,cs-410,4,7, Smoothing Methods - Part 2,"00:00:24,570","00:00:26,500",6,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=24,you have seen this before.
cs-410_4_7_7,cs-410,4,7, Smoothing Methods - Part 2,"00:00:28,060","00:00:32,550",7,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=28,And now we have a very specific smoothing
cs-410_4_7_8,cs-410,4,7, Smoothing Methods - Part 2,"00:00:33,690","00:00:39,190",8,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=33,So now let's see what what's a value for
cs-410_4_7_9,cs-410,4,7, Smoothing Methods - Part 2,"00:00:40,450","00:00:42,900",9,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=40,And what's the value for p sub c here?
cs-410_4_7_10,cs-410,4,7, Smoothing Methods - Part 2,"00:00:42,900","00:00:46,930",10,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=42,"Right, so we may need to decide this"
cs-410_4_7_11,cs-410,4,7, Smoothing Methods - Part 2,"00:00:46,930","00:00:50,470",11,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=46,in order to figure out the exact
cs-410_4_7_12,cs-410,4,7, Smoothing Methods - Part 2,"00:00:50,470","00:00:52,598",12,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=50,And we also need to figure
cs-410_4_7_13,cs-410,4,7, Smoothing Methods - Part 2,"00:00:52,598","00:00:55,910",13,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=52,So let's see.
cs-410_4_7_14,cs-410,4,7, Smoothing Methods - Part 2,"00:00:55,910","00:01:00,666",14,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=55,"Well this ratio is basically this,"
cs-410_4_7_15,cs-410,4,7, Smoothing Methods - Part 2,"00:01:00,666","00:01:05,315",15,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=60,"here, this is the probability"
cs-410_4_7_16,cs-410,4,7, Smoothing Methods - Part 2,"00:01:05,315","00:01:09,330",16,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=65,and this is the probability
cs-410_4_7_17,cs-410,4,7, Smoothing Methods - Part 2,"00:01:09,330","00:01:14,935",17,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=69,in other words basically 11
cs-410_4_7_18,cs-410,4,7, Smoothing Methods - Part 2,"00:01:14,935","00:01:18,530",18,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=74,"this, so it's easy to see that."
cs-410_4_7_19,cs-410,4,7, Smoothing Methods - Part 2,"00:01:18,530","00:01:21,681",19,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=78,This can be then rewritten as this.
cs-410_4_7_20,cs-410,4,7, Smoothing Methods - Part 2,"00:01:21,681","00:01:24,500",20,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=81,Very simple.
cs-410_4_7_21,cs-410,4,7, Smoothing Methods - Part 2,"00:01:24,500","00:01:26,810",21,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=84,So we can plug this into here.
cs-410_4_7_22,cs-410,4,7, Smoothing Methods - Part 2,"00:01:28,650","00:01:30,710",22,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=88,"And then here, what's the value for alpha?"
cs-410_4_7_23,cs-410,4,7, Smoothing Methods - Part 2,"00:01:30,710","00:01:31,660",23,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=90,What do you think?
cs-410_4_7_24,cs-410,4,7, Smoothing Methods - Part 2,"00:01:31,660","00:01:35,250",24,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=91,"So it would be just lambda, right?"
cs-410_4_7_25,cs-410,4,7, Smoothing Methods - Part 2,"00:01:38,250","00:01:43,900",25,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=98,And what would happen if we plug in
cs-410_4_7_26,cs-410,4,7, Smoothing Methods - Part 2,"00:01:43,900","00:01:45,350",26,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=103,What can we say about this?
cs-410_4_7_27,cs-410,4,7, Smoothing Methods - Part 2,"00:01:47,940","00:01:49,640",27,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=107,Does it depend on the document?
cs-410_4_7_28,cs-410,4,7, Smoothing Methods - Part 2,"00:01:50,660","00:01:52,170",28,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=110,"No, so it can be ignored."
cs-410_4_7_29,cs-410,4,7, Smoothing Methods - Part 2,"00:01:53,570","00:01:55,040",29,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=113,Right?
cs-410_4_7_30,cs-410,4,7, Smoothing Methods - Part 2,"00:01:55,040","00:01:58,690",30,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=115,So we'll end up having this
cs-410_4_7_31,cs-410,4,7, Smoothing Methods - Part 2,"00:02:00,520","00:02:02,690",31,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=120,"And in this case you can easy to see,"
cs-410_4_7_32,cs-410,4,7, Smoothing Methods - Part 2,"00:02:02,690","00:02:07,780",32,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=122,this a precisely a vector space
cs-410_4_7_33,cs-410,4,7, Smoothing Methods - Part 2,"00:02:07,780","00:02:13,480",33,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=127,"a sum over all the matched query terms,"
cs-410_4_7_34,cs-410,4,7, Smoothing Methods - Part 2,"00:02:13,480","00:02:16,140",34,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=133,What do you think is a element
cs-410_4_7_35,cs-410,4,7, Smoothing Methods - Part 2,"00:02:18,670","00:02:20,200",35,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=138,"Well it's this, right."
cs-410_4_7_36,cs-410,4,7, Smoothing Methods - Part 2,"00:02:20,200","00:02:23,210",36,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=140,So that's our document left element.
cs-410_4_7_37,cs-410,4,7, Smoothing Methods - Part 2,"00:02:23,210","00:02:29,210",37,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=143,And let's further examine what's
cs-410_4_7_38,cs-410,4,7, Smoothing Methods - Part 2,"00:02:30,370","00:02:32,440",38,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=150,Well one plus this.
cs-410_4_7_39,cs-410,4,7, Smoothing Methods - Part 2,"00:02:32,440","00:02:36,630",39,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=152,"So it's going to be nonnegative,"
cs-410_4_7_40,cs-410,4,7, Smoothing Methods - Part 2,"00:02:36,630","00:02:37,850",40,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=156,"it's going to be at least 1, right?"
cs-410_4_7_41,cs-410,4,7, Smoothing Methods - Part 2,"00:02:39,450","00:02:42,900",41,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=159,"And these, this is a parameter,"
cs-410_4_7_42,cs-410,4,7, Smoothing Methods - Part 2,"00:02:42,900","00:02:44,340",42,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=162,And let's look at this.
cs-410_4_7_43,cs-410,4,7, Smoothing Methods - Part 2,"00:02:44,340","00:02:45,480",43,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=164,Now this is a TF.
cs-410_4_7_44,cs-410,4,7, Smoothing Methods - Part 2,"00:02:45,480","00:02:48,070",44,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=165,Now we see very clearly
cs-410_4_7_45,cs-410,4,7, Smoothing Methods - Part 2,"00:02:49,250","00:02:54,080",45,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=169,"And the larger the count is,"
cs-410_4_7_46,cs-410,4,7, Smoothing Methods - Part 2,"00:02:54,080","00:02:57,080",46,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=174,"We also see IDF weighting,"
cs-410_4_7_47,cs-410,4,7, Smoothing Methods - Part 2,"00:02:58,720","00:03:00,996",47,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=178,And we see docking the lan's
cs-410_4_7_48,cs-410,4,7, Smoothing Methods - Part 2,"00:03:00,996","00:03:03,270",48,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=180,So all these heuristics
cs-410_4_7_49,cs-410,4,7, Smoothing Methods - Part 2,"00:03:04,532","00:03:08,480",49,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=184,What's interesting that
cs-410_4_7_50,cs-410,4,7, Smoothing Methods - Part 2,"00:03:08,480","00:03:12,330",50,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=188,weighting function automatically
cs-410_4_7_51,cs-410,4,7, Smoothing Methods - Part 2,"00:03:12,330","00:03:14,270",51,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=192,"Whereas in the vector space model,"
cs-410_4_7_52,cs-410,4,7, Smoothing Methods - Part 2,"00:03:14,270","00:03:19,330",52,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=194,we had to go through those heuristic
cs-410_4_7_53,cs-410,4,7, Smoothing Methods - Part 2,"00:03:19,330","00:03:21,880",53,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=199,And in this case note that
cs-410_4_7_54,cs-410,4,7, Smoothing Methods - Part 2,"00:03:21,880","00:03:25,120",54,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=201,And when you see whether this
cs-410_4_7_55,cs-410,4,7, Smoothing Methods - Part 2,"00:03:26,690","00:03:31,050",55,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=206,All right so what do you think
cs-410_4_7_56,cs-410,4,7, Smoothing Methods - Part 2,"00:03:31,050","00:03:33,320",56,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=211,This is a math of document.
cs-410_4_7_57,cs-410,4,7, Smoothing Methods - Part 2,"00:03:33,320","00:03:37,340",57,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=213,"Total number of words,"
cs-410_4_7_58,cs-410,4,7, Smoothing Methods - Part 2,"00:03:38,400","00:03:42,727",58,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=218,"given by the collection, right?"
cs-410_4_7_59,cs-410,4,7, Smoothing Methods - Part 2,"00:03:42,727","00:03:48,090",59,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=222,So this actually can be interpreted
cs-410_4_7_60,cs-410,4,7, Smoothing Methods - Part 2,"00:03:48,090","00:03:53,730",60,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=228,"If we're going to draw, a word,"
cs-410_4_7_61,cs-410,4,7, Smoothing Methods - Part 2,"00:03:53,730","00:03:57,980",61,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=233,"And, we're going to draw as many as"
cs-410_4_7_62,cs-410,4,7, Smoothing Methods - Part 2,"00:03:59,310","00:04:02,940",62,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=239,"If you do that,"
cs-410_4_7_63,cs-410,4,7, Smoothing Methods - Part 2,"00:04:02,940","00:04:06,950",63,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=242,would be precisely given
cs-410_4_7_64,cs-410,4,7, Smoothing Methods - Part 2,"00:04:08,240","00:04:14,400",64,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=248,"So, this ratio basically,"
cs-410_4_7_65,cs-410,4,7, Smoothing Methods - Part 2,"00:04:15,860","00:04:21,280",65,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=255,The actual count of the word in the
cs-410_4_7_66,cs-410,4,7, Smoothing Methods - Part 2,"00:04:21,280","00:04:29,570",66,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=261,product if the word is in fact following
cs-410_4_7_67,cs-410,4,7, Smoothing Methods - Part 2,"00:04:29,570","00:04:33,250",67,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=269,And if this counter is larger than
cs-410_4_7_68,cs-410,4,7, Smoothing Methods - Part 2,"00:04:33,250","00:04:34,789",68,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=273,this ratio would be larger than one.
cs-410_4_7_69,cs-410,4,7, Smoothing Methods - Part 2,"00:04:37,100","00:04:40,460",69,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=277,So that's actually a very
cs-410_4_7_70,cs-410,4,7, Smoothing Methods - Part 2,"00:04:40,460","00:04:43,930",70,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=280,"It's very natural and intuitive,"
cs-410_4_7_71,cs-410,4,7, Smoothing Methods - Part 2,"00:04:45,240","00:04:49,580",71,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=285,And this is one advantage of using
cs-410_4_7_72,cs-410,4,7, Smoothing Methods - Part 2,"00:04:49,580","00:04:53,240",72,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=289,where we have made explicit assumptions.
cs-410_4_7_73,cs-410,4,7, Smoothing Methods - Part 2,"00:04:53,240","00:04:56,490",73,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=293,"And, we know precisely why"
cs-410_4_7_74,cs-410,4,7, Smoothing Methods - Part 2,"00:04:56,490","00:04:58,800",74,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=296,"And, why we have these probabilities here."
cs-410_4_7_75,cs-410,4,7, Smoothing Methods - Part 2,"00:05:00,280","00:05:04,290",75,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=300,"And, we also have a formula that"
cs-410_4_7_76,cs-410,4,7, Smoothing Methods - Part 2,"00:05:04,290","00:05:07,190",76,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=304,does TF-IDF weighting and
cs-410_4_7_77,cs-410,4,7, Smoothing Methods - Part 2,"00:05:09,010","00:05:11,440",77,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=309,"Let's look at the,"
cs-410_4_7_78,cs-410,4,7, Smoothing Methods - Part 2,"00:05:11,440","00:05:16,852",78,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=311,It's very similar to
cs-410_4_7_79,cs-410,4,7, Smoothing Methods - Part 2,"00:05:16,852","00:05:21,540",79,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=316,"In this case,"
cs-410_4_7_80,cs-410,4,7, Smoothing Methods - Part 2,"00:05:21,540","00:05:27,660",80,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=321,that's different from
cs-410_4_7_81,cs-410,4,7, Smoothing Methods - Part 2,"00:05:27,660","00:05:30,660",81,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=327,But the format looks very similar.
cs-410_4_7_82,cs-410,4,7, Smoothing Methods - Part 2,"00:05:30,660","00:05:32,570",82,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=330,The form of the function
cs-410_4_7_83,cs-410,4,7, Smoothing Methods - Part 2,"00:05:34,540","00:05:36,730",83,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=334,So we still have linear operation here.
cs-410_4_7_84,cs-410,4,7, Smoothing Methods - Part 2,"00:05:38,090","00:05:40,130",84,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=338,"And when we compute this ratio,"
cs-410_4_7_85,cs-410,4,7, Smoothing Methods - Part 2,"00:05:40,130","00:05:45,460",85,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=340,one will find that is that
cs-410_4_7_86,cs-410,4,7, Smoothing Methods - Part 2,"00:05:46,930","00:05:51,620",86,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=346,And what's interesting here is that we
cs-410_4_7_87,cs-410,4,7, Smoothing Methods - Part 2,"00:05:51,620","00:05:54,440",87,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=351,We're comparing the actual count.
cs-410_4_7_88,cs-410,4,7, Smoothing Methods - Part 2,"00:05:54,440","00:05:59,400",88,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=354,Which is the expected account of the world
cs-410_4_7_89,cs-410,4,7, Smoothing Methods - Part 2,"00:05:59,400","00:06:02,660",89,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=359,the collection world probability.
cs-410_4_7_90,cs-410,4,7, Smoothing Methods - Part 2,"00:06:02,660","00:06:07,266",90,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=362,So note that it's interesting we don't
cs-410_4_7_91,cs-410,4,7, Smoothing Methods - Part 2,"00:06:07,266","00:06:08,910",91,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=367,lighter in the JMs model.
cs-410_4_7_92,cs-410,4,7, Smoothing Methods - Part 2,"00:06:08,910","00:06:13,880",92,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=368,All right so this of course
cs-410_4_7_93,cs-410,4,7, Smoothing Methods - Part 2,"00:06:15,290","00:06:18,200",93,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=375,"So you might wonder, so"
cs-410_4_7_94,cs-410,4,7, Smoothing Methods - Part 2,"00:06:18,200","00:06:23,650",94,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=378,Interestingly the docking lens
cs-410_4_7_95,cs-410,4,7, Smoothing Methods - Part 2,"00:06:23,650","00:06:26,850",95,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=383,this would be plugged into this part.
cs-410_4_7_96,cs-410,4,7, Smoothing Methods - Part 2,"00:06:26,850","00:06:31,860",96,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=386,As a result what we get is
cs-410_4_7_97,cs-410,4,7, Smoothing Methods - Part 2,"00:06:31,860","00:06:35,239",97,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=391,this is again a sum over
cs-410_4_7_98,cs-410,4,7, Smoothing Methods - Part 2,"00:06:36,290","00:06:40,050",98,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=396,"And we're against the queer,"
cs-410_4_7_99,cs-410,4,7, Smoothing Methods - Part 2,"00:06:41,410","00:06:45,425",99,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=401,And you can interpret this as
cs-410_4_7_100,cs-410,4,7, Smoothing Methods - Part 2,"00:06:45,425","00:06:48,700",100,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=405,but this is no longer
cs-410_4_7_101,cs-410,4,7, Smoothing Methods - Part 2,"00:06:50,100","00:06:55,165",101,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=410,"Because we have this part,"
cs-410_4_7_102,cs-410,4,7, Smoothing Methods - Part 2,"00:06:55,165","00:06:57,810",102,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=415,right?
cs-410_4_7_103,cs-410,4,7, Smoothing Methods - Part 2,"00:06:57,810","00:07:01,510",103,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=417,So that just means if
cs-410_4_7_104,cs-410,4,7, Smoothing Methods - Part 2,"00:07:01,510","00:07:05,160",104,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=421,we have to take a sum over
cs-410_4_7_105,cs-410,4,7, Smoothing Methods - Part 2,"00:07:05,160","00:07:09,270",105,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=425,then do some adjustment of
cs-410_4_7_106,cs-410,4,7, Smoothing Methods - Part 2,"00:07:11,510","00:07:15,974",106,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=431,"But it's still, it's still clear"
cs-410_4_7_107,cs-410,4,7, Smoothing Methods - Part 2,"00:07:15,974","00:07:19,765",107,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=435,modulation because this lens
cs-410_4_7_108,cs-410,4,7, Smoothing Methods - Part 2,"00:07:19,765","00:07:23,237",108,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=439,a longer document will
cs-410_4_7_109,cs-410,4,7, Smoothing Methods - Part 2,"00:07:23,237","00:07:27,600",109,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=443,And we can also see it has tf here and
cs-410_4_7_110,cs-410,4,7, Smoothing Methods - Part 2,"00:07:27,600","00:07:32,038",110,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=447,Only that this time the form of the
cs-410_4_7_111,cs-410,4,7, Smoothing Methods - Part 2,"00:07:32,038","00:07:34,580",111,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=452,in JMs one.
cs-410_4_7_112,cs-410,4,7, Smoothing Methods - Part 2,"00:07:34,580","00:07:39,780",112,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=454,But intuitively it still implements TFIDF
cs-410_4_7_113,cs-410,4,7, Smoothing Methods - Part 2,"00:07:39,780","00:07:44,340",113,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=459,the form of the function is dictated
cs-410_4_7_114,cs-410,4,7, Smoothing Methods - Part 2,"00:07:44,340","00:07:45,938",114,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=464,assumptions that we have made.
cs-410_4_7_115,cs-410,4,7, Smoothing Methods - Part 2,"00:07:45,938","00:07:50,420",115,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=465,Now there are also
cs-410_4_7_116,cs-410,4,7, Smoothing Methods - Part 2,"00:07:50,420","00:07:53,600",116,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=470,"And that is, there's no guarantee"
cs-410_4_7_117,cs-410,4,7, Smoothing Methods - Part 2,"00:07:53,600","00:07:55,800",117,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=473,of the formula will actually work well.
cs-410_4_7_118,cs-410,4,7, Smoothing Methods - Part 2,"00:07:55,800","00:08:01,037",118,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=475,"So if we look about at this geo function,"
cs-410_4_7_119,cs-410,4,7, Smoothing Methods - Part 2,"00:08:01,037","00:08:06,860",119,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=481,rendition for example it's unclear whether
cs-410_4_7_120,cs-410,4,7, Smoothing Methods - Part 2,"00:08:06,860","00:08:13,110",120,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=486,Unfortunately we can see here there
cs-410_4_7_121,cs-410,4,7, Smoothing Methods - Part 2,"00:08:13,110","00:08:17,580",121,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=493,"So we do have also the,"
cs-410_4_7_122,cs-410,4,7, Smoothing Methods - Part 2,"00:08:17,580","00:08:20,986",122,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=497,So we do have the sublinear
cs-410_4_7_123,cs-410,4,7, Smoothing Methods - Part 2,"00:08:20,986","00:08:23,320",123,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=500,we do not intentionally do that.
cs-410_4_7_124,cs-410,4,7, Smoothing Methods - Part 2,"00:08:23,320","00:08:27,750",124,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=503,That means there's no guarantee that
cs-410_4_7_125,cs-410,4,7, Smoothing Methods - Part 2,"00:08:27,750","00:08:31,800",125,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=507,"Suppose we don't have logarithm,"
cs-410_4_7_126,cs-410,4,7, Smoothing Methods - Part 2,"00:08:31,800","00:08:35,810",126,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=511,"As we discussed before, perhaps"
cs-410_4_7_127,cs-410,4,7, Smoothing Methods - Part 2,"00:08:35,810","00:08:40,870",127,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=515,So that's an example of the gap
cs-410_4_7_128,cs-410,4,7, Smoothing Methods - Part 2,"00:08:40,870","00:08:43,080",128,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=520,"the relevance that we have to model,"
cs-410_4_7_129,cs-410,4,7, Smoothing Methods - Part 2,"00:08:43,080","00:08:48,720",129,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=523,which is really a subject
cs-410_4_7_130,cs-410,4,7, Smoothing Methods - Part 2,"00:08:50,640","00:08:53,390",130,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=530,So it doesn't mean we cannot fix this.
cs-410_4_7_131,cs-410,4,7, Smoothing Methods - Part 2,"00:08:53,390","00:08:57,390",131,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=533,"For example, imagine if we did"
cs-410_4_7_132,cs-410,4,7, Smoothing Methods - Part 2,"00:08:57,390","00:08:59,250",132,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=537,So we can take a risk and
cs-410_4_7_133,cs-410,4,7, Smoothing Methods - Part 2,"00:08:59,250","00:09:01,935",133,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=539,or we can even add double logarithm.
cs-410_4_7_134,cs-410,4,7, Smoothing Methods - Part 2,"00:09:01,935","00:09:06,200",134,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=541,"But then, it would mean that the function"
cs-410_4_7_135,cs-410,4,7, Smoothing Methods - Part 2,"00:09:06,200","00:09:10,780",135,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=546,So the consequence of
cs-410_4_7_136,cs-410,4,7, Smoothing Methods - Part 2,"00:09:10,780","00:09:14,670",136,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=550,longer as predictable as
cs-410_4_7_137,cs-410,4,7, Smoothing Methods - Part 2,"00:09:15,810","00:09:21,410",137,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=555,"So, that's also why, for example,"
cs-410_4_7_138,cs-410,4,7, Smoothing Methods - Part 2,"00:09:21,410","00:09:26,720",138,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=561,"still, open channel how to use"
cs-410_4_7_139,cs-410,4,7, Smoothing Methods - Part 2,"00:09:26,720","00:09:28,690",139,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=566,better model than the PM25.
cs-410_4_7_140,cs-410,4,7, Smoothing Methods - Part 2,"00:09:30,420","00:09:34,500",140,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=570,In particular how do we use query
cs-410_4_7_141,cs-410,4,7, Smoothing Methods - Part 2,"00:09:34,500","00:09:37,650",141,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=574,that would work consistently
cs-410_4_7_142,cs-410,4,7, Smoothing Methods - Part 2,"00:09:37,650","00:09:39,070",142,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=577,Currently we still cannot do that.
cs-410_4_7_143,cs-410,4,7, Smoothing Methods - Part 2,"00:09:40,240","00:09:41,640",143,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=580,Still interesting open question.
cs-410_4_7_144,cs-410,4,7, Smoothing Methods - Part 2,"00:09:43,450","00:09:46,975",144,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=583,"So to summarize this part, we've talked"
cs-410_4_7_145,cs-410,4,7, Smoothing Methods - Part 2,"00:09:46,975","00:09:52,550",145,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=586,Jelinek-Mercer which is doing the fixed
cs-410_4_7_146,cs-410,4,7, Smoothing Methods - Part 2,"00:09:52,550","00:09:58,430",146,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=592,Dirichlet Prior this is what add a pseudo
cs-410_4_7_147,cs-410,4,7, Smoothing Methods - Part 2,"00:09:58,430","00:10:04,160",147,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=598,interpolation in that the coefficient
cs-410_4_7_148,cs-410,4,7, Smoothing Methods - Part 2,"00:10:05,940","00:10:10,890",148,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=605,"In most cases we can see, by using these"
cs-410_4_7_149,cs-410,4,7, Smoothing Methods - Part 2,"00:10:10,890","00:10:16,670",149,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=610,reach a retrieval function where
cs-410_4_7_150,cs-410,4,7, Smoothing Methods - Part 2,"00:10:16,670","00:10:17,790",150,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=616,So they are less heuristic.
cs-410_4_7_151,cs-410,4,7, Smoothing Methods - Part 2,"00:10:19,090","00:10:23,810",151,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=619,Explaining the results also show
cs-410_4_7_152,cs-410,4,7, Smoothing Methods - Part 2,"00:10:23,810","00:10:31,036",152,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=623,Also are very effective and they are
cs-410_4_7_153,cs-410,4,7, Smoothing Methods - Part 2,"00:10:31,036","00:10:36,260",153,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=631,So this is a major advantage
cs-410_4_7_154,cs-410,4,7, Smoothing Methods - Part 2,"00:10:36,260","00:10:39,480",154,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=636,where we don't have to do
cs-410_4_7_155,cs-410,4,7, Smoothing Methods - Part 2,"00:10:40,770","00:10:44,240",155,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=640,Yet in the end that we naturally
cs-410_4_7_156,cs-410,4,7, Smoothing Methods - Part 2,"00:10:44,240","00:10:45,239",156,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=644,doc length normalization.
cs-410_4_7_157,cs-410,4,7, Smoothing Methods - Part 2,"00:10:46,480","00:10:51,120",157,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=646,Each of these functions also has
cs-410_4_7_158,cs-410,4,7, Smoothing Methods - Part 2,"00:10:51,120","00:10:54,840",158,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=651,In this case of course we still need
cs-410_4_7_159,cs-410,4,7, Smoothing Methods - Part 2,"00:10:54,840","00:10:58,850",159,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=654,There are also methods that can be
cs-410_4_7_160,cs-410,4,7, Smoothing Methods - Part 2,"00:10:59,950","00:11:04,020",160,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=659,"So overall,"
cs-410_4_7_161,cs-410,4,7, Smoothing Methods - Part 2,"00:11:04,020","00:11:08,900",161,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=664,we follow very different strategies
cs-410_4_7_162,cs-410,4,7, Smoothing Methods - Part 2,"00:11:08,900","00:11:12,980",162,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=668,"Yet, in the end, we end up uh,with"
cs-410_4_7_163,cs-410,4,7, Smoothing Methods - Part 2,"00:11:12,980","00:11:15,540",163,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=672,look very similar to
cs-410_4_7_164,cs-410,4,7, Smoothing Methods - Part 2,"00:11:15,540","00:11:21,160",164,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=675,With some advantages in having
cs-410_4_7_165,cs-410,4,7, Smoothing Methods - Part 2,"00:11:21,160","00:11:24,940",165,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=681,"And then, the form dictated"
cs-410_4_7_166,cs-410,4,7, Smoothing Methods - Part 2,"00:11:24,940","00:11:29,740",166,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=684,"Now, this also concludes our discussion of"
cs-410_4_7_167,cs-410,4,7, Smoothing Methods - Part 2,"00:11:29,740","00:11:34,680",167,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=689,And let's recall what
cs-410_4_7_168,cs-410,4,7, Smoothing Methods - Part 2,"00:11:34,680","00:11:39,390",168,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=694,in order to derive the functions
cs-410_4_7_169,cs-410,4,7, Smoothing Methods - Part 2,"00:11:39,390","00:11:42,130",169,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=699,Well we basically have made four
cs-410_4_7_170,cs-410,4,7, Smoothing Methods - Part 2,"00:11:42,130","00:11:48,399",170,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=702,The first assumption is that the relevance
cs-410_4_7_171,cs-410,4,7, Smoothing Methods - Part 2,"00:11:49,470","00:11:53,450",171,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=709,"And the second assumption with med is, are"
cs-410_4_7_172,cs-410,4,7, Smoothing Methods - Part 2,"00:11:53,450","00:11:57,240",172,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=713,that allows us to decompose
cs-410_4_7_173,cs-410,4,7, Smoothing Methods - Part 2,"00:11:57,240","00:12:01,690",173,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=717,into a product of probabilities
cs-410_4_7_174,cs-410,4,7, Smoothing Methods - Part 2,"00:12:03,090","00:12:07,850",174,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=723,"And then,"
cs-410_4_7_175,cs-410,4,7, Smoothing Methods - Part 2,"00:12:07,850","00:12:10,550",175,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=727,"if a word is not seen,"
cs-410_4_7_176,cs-410,4,7, Smoothing Methods - Part 2,"00:12:10,550","00:12:14,870",176,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=730,its probability proportional to
cs-410_4_7_177,cs-410,4,7, Smoothing Methods - Part 2,"00:12:14,870","00:12:17,290",177,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=734,That's a smoothing with
cs-410_4_7_178,cs-410,4,7, Smoothing Methods - Part 2,"00:12:17,290","00:12:20,980",178,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=737,"And finally, we made one of these"
cs-410_4_7_179,cs-410,4,7, Smoothing Methods - Part 2,"00:12:20,980","00:12:24,940",179,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=740,So we either used JM smoothing or
cs-410_4_7_180,cs-410,4,7, Smoothing Methods - Part 2,"00:12:24,940","00:12:28,820",180,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=744,If we make these four assumptions
cs-410_4_7_181,cs-410,4,7, Smoothing Methods - Part 2,"00:12:28,820","00:12:33,430",181,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=748,to take the form of the retrieval
cs-410_4_7_182,cs-410,4,7, Smoothing Methods - Part 2,"00:12:33,430","00:12:37,730",182,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=753,Fortunately the function has a nice
cs-410_4_7_183,cs-410,4,7, Smoothing Methods - Part 2,"00:12:37,730","00:12:44,510",183,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=757,weighting and document machine and
cs-410_4_7_184,cs-410,4,7, Smoothing Methods - Part 2,"00:12:44,510","00:12:45,440",184,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=764,"So in that sense,"
cs-410_4_7_185,cs-410,4,7, Smoothing Methods - Part 2,"00:12:45,440","00:12:48,920",185,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=765,these functions are less heuristic
cs-410_4_7_186,cs-410,4,7, Smoothing Methods - Part 2,"00:12:50,460","00:12:54,282",186,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=770,"And there are many extensions of this,"
cs-410_4_7_187,cs-410,4,7, Smoothing Methods - Part 2,"00:12:54,282","00:12:59,336",187,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=774,you can find the discussion of them in
cs-410_4_7_188,cs-410,4,7, Smoothing Methods - Part 2,"00:13:04,921","00:13:14,921",188,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=784,[MUSIC]
cs-410_5_1_1,cs-410,5,1, Feedback in Text Retrieval,"00:00:00,012","00:00:07,436",1,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=0,[SOUND]
cs-410_5_1_2,cs-410,5,1, Feedback in Text Retrieval,"00:00:07,436","00:00:10,250",2,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=7,lecture is about the feedback
cs-410_5_1_3,cs-410,5,1, Feedback in Text Retrieval,"00:00:12,910","00:00:17,060",3,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=12,"So in this lecture, we will continue with"
cs-410_5_1_4,cs-410,5,1, Feedback in Text Retrieval,"00:00:18,840","00:00:22,103",4,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=18,"In particular, we're going to talk"
cs-410_5_1_5,cs-410,5,1, Feedback in Text Retrieval,"00:00:24,866","00:00:28,380",5,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=24,This is a diagram that shows
cs-410_5_1_6,cs-410,5,1, Feedback in Text Retrieval,"00:00:30,685","00:00:34,895",6,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=30,We can see the user would type in a query.
cs-410_5_1_7,cs-410,5,1, Feedback in Text Retrieval,"00:00:37,365","00:00:41,965",7,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=37,"And then, the query would be"
cs-410_5_1_8,cs-410,5,1, Feedback in Text Retrieval,"00:00:41,965","00:00:46,015",8,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=41,"search engine, and"
cs-410_5_1_9,cs-410,5,1, Feedback in Text Retrieval,"00:00:46,015","00:00:47,865",9,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=46,These results would be issued to the user.
cs-410_5_1_10,cs-410,5,1, Feedback in Text Retrieval,"00:00:49,475","00:00:52,760",10,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=49,"Now, after the user has"
cs-410_5_1_11,cs-410,5,1, Feedback in Text Retrieval,"00:00:52,760","00:00:55,410",11,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=52,the user can actually make judgements.
cs-410_5_1_12,cs-410,5,1, Feedback in Text Retrieval,"00:00:55,410","00:00:59,009",12,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=55,"So for example, the user says,"
cs-410_5_1_13,cs-410,5,1, Feedback in Text Retrieval,"00:00:59,009","00:01:03,097",13,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=59,this document is not very useful and
cs-410_5_1_14,cs-410,5,1, Feedback in Text Retrieval,"00:01:03,097","00:01:07,921",14,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=63,"Now, this is called a relevance judgment"
cs-410_5_1_15,cs-410,5,1, Feedback in Text Retrieval,"00:01:07,921","00:01:12,510",15,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=67,got some feedback information from
cs-410_5_1_16,cs-410,5,1, Feedback in Text Retrieval,"00:01:12,510","00:01:14,930",16,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=72,"And this can be very useful to the system,"
cs-410_5_1_17,cs-410,5,1, Feedback in Text Retrieval,"00:01:14,930","00:01:18,320",17,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=74,knowing what exactly is
cs-410_5_1_18,cs-410,5,1, Feedback in Text Retrieval,"00:01:18,320","00:01:22,790",18,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=78,So the feedback module would
cs-410_5_1_19,cs-410,5,1, Feedback in Text Retrieval,"00:01:22,790","00:01:26,970",19,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=82,also use the document collection
cs-410_5_1_20,cs-410,5,1, Feedback in Text Retrieval,"00:01:26,970","00:01:30,720",20,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=86,Typically it would involve
cs-410_5_1_21,cs-410,5,1, Feedback in Text Retrieval,"00:01:30,720","00:01:34,960",21,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=90,the system can now render the results
cs-410_5_1_22,cs-410,5,1, Feedback in Text Retrieval,"00:01:34,960","00:01:36,897",22,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=94,So this is called relevance feedback.
cs-410_5_1_23,cs-410,5,1, Feedback in Text Retrieval,"00:01:36,897","00:01:42,470",23,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=96,The feedback is based on relevance
cs-410_5_1_24,cs-410,5,1, Feedback in Text Retrieval,"00:01:42,470","00:01:44,660",24,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=102,"Now, these judgements are reliable but"
cs-410_5_1_25,cs-410,5,1, Feedback in Text Retrieval,"00:01:44,660","00:01:50,350",25,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=104,the users generally don't want to make
cs-410_5_1_26,cs-410,5,1, Feedback in Text Retrieval,"00:01:50,350","00:01:54,980",26,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=110,So the down side is that it involves
cs-410_5_1_27,cs-410,5,1, Feedback in Text Retrieval,"00:01:57,250","00:02:00,920",27,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=117,There's another form of feedback
cs-410_5_1_28,cs-410,5,1, Feedback in Text Retrieval,"00:02:00,920","00:02:03,800",28,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=120,"blind feedback,"
cs-410_5_1_29,cs-410,5,1, Feedback in Text Retrieval,"00:02:03,800","00:02:08,380",29,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=123,"In this case, we can see once"
cs-410_5_1_30,cs-410,5,1, Feedback in Text Retrieval,"00:02:08,380","00:02:11,340",30,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=128,in fact we don't have to invoke users.
cs-410_5_1_31,cs-410,5,1, Feedback in Text Retrieval,"00:02:11,340","00:02:13,720",31,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=131,So you can see there's
cs-410_5_1_32,cs-410,5,1, Feedback in Text Retrieval,"00:02:14,730","00:02:19,846",32,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=134,And we simply assume that the top
cs-410_5_1_33,cs-410,5,1, Feedback in Text Retrieval,"00:02:19,846","00:02:23,940",33,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=139,Let's say we have assumed
cs-410_5_1_34,cs-410,5,1, Feedback in Text Retrieval,"00:02:25,250","00:02:31,000",34,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=145,"And then, we will then use this"
cs-410_5_1_35,cs-410,5,1, Feedback in Text Retrieval,"00:02:31,000","00:02:33,110",35,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=151,and to improve the query.
cs-410_5_1_36,cs-410,5,1, Feedback in Text Retrieval,"00:02:34,110","00:02:35,821",36,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=154,"Now, you might wonder,"
cs-410_5_1_37,cs-410,5,1, Feedback in Text Retrieval,"00:02:35,821","00:02:40,887",37,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=155,how could this help if we simply
cs-410_5_1_38,cs-410,5,1, Feedback in Text Retrieval,"00:02:40,887","00:02:46,490",38,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=160,"Well, you can imagine these top"
cs-410_5_1_39,cs-410,5,1, Feedback in Text Retrieval,"00:02:46,490","00:02:52,070",39,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=166,similar to relevant documents
cs-410_5_1_40,cs-410,5,1, Feedback in Text Retrieval,"00:02:52,070","00:02:53,480",40,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=172,They look like relevant documents.
cs-410_5_1_41,cs-410,5,1, Feedback in Text Retrieval,"00:02:53,480","00:02:59,350",41,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=173,So it's possible to learn some related
cs-410_5_1_42,cs-410,5,1, Feedback in Text Retrieval,"00:02:59,350","00:03:03,610",42,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=179,"In fact, you may recall that we"
cs-410_5_1_43,cs-410,5,1, Feedback in Text Retrieval,"00:03:03,610","00:03:08,180",43,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=183,"analyze what association, to learn"
cs-410_5_1_44,cs-410,5,1, Feedback in Text Retrieval,"00:03:09,480","00:03:13,040",44,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=189,"And there, what we did is we"
cs-410_5_1_45,cs-410,5,1, Feedback in Text Retrieval,"00:03:13,040","00:03:15,500",45,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=193,all the documents that contain computer.
cs-410_5_1_46,cs-410,5,1, Feedback in Text Retrieval,"00:03:15,500","00:03:18,761",46,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=195,So imagine now the query
cs-410_5_1_47,cs-410,5,1, Feedback in Text Retrieval,"00:03:18,761","00:03:23,870",47,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=198,"And then, the result will be those"
cs-410_5_1_48,cs-410,5,1, Feedback in Text Retrieval,"00:03:23,870","00:03:29,040",48,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=203,And what we can do then is
cs-410_5_1_49,cs-410,5,1, Feedback in Text Retrieval,"00:03:29,040","00:03:31,860",49,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=209,They can match computer very well.
cs-410_5_1_50,cs-410,5,1, Feedback in Text Retrieval,"00:03:31,860","00:03:36,890",50,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=211,And we're going to count
cs-410_5_1_51,cs-410,5,1, Feedback in Text Retrieval,"00:03:36,890","00:03:42,126",51,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=216,"And then, we're going to then use"
cs-410_5_1_52,cs-410,5,1, Feedback in Text Retrieval,"00:03:42,126","00:03:47,794",52,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=222,the terms that are frequent in this set
cs-410_5_1_53,cs-410,5,1, Feedback in Text Retrieval,"00:03:47,794","00:03:52,364",53,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=227,So if we make a contrast between
cs-410_5_1_54,cs-410,5,1, Feedback in Text Retrieval,"00:03:52,364","00:03:57,360",54,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=232,is that related to terms
cs-410_5_1_55,cs-410,5,1, Feedback in Text Retrieval,"00:03:57,360","00:03:58,528",55,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=237,As we have seen before.
cs-410_5_1_56,cs-410,5,1, Feedback in Text Retrieval,"00:03:58,528","00:04:04,786",56,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=238,And these related words can then be added
cs-410_5_1_57,cs-410,5,1, Feedback in Text Retrieval,"00:04:04,786","00:04:08,770",57,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=244,And this would help us bring the documents
cs-410_5_1_58,cs-410,5,1, Feedback in Text Retrieval,"00:04:08,770","00:04:11,640",58,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=248,match other words like program and
cs-410_5_1_59,cs-410,5,1, Feedback in Text Retrieval,"00:04:11,640","00:04:16,450",59,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=251,So this is very effective for
cs-410_5_1_60,cs-410,5,1, Feedback in Text Retrieval,"00:04:18,590","00:04:21,790",60,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=258,"But of course, pseudo-relevancy"
cs-410_5_1_61,cs-410,5,1, Feedback in Text Retrieval,"00:04:21,790","00:04:24,050",61,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=261,We have to arbitrarily set a cut off.
cs-410_5_1_62,cs-410,5,1, Feedback in Text Retrieval,"00:04:24,050","00:04:27,010",62,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=264,So there's also something in
cs-410_5_1_63,cs-410,5,1, Feedback in Text Retrieval,"00:04:27,010","00:04:31,120",63,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=267,"In this case,"
cs-410_5_1_64,cs-410,5,1, Feedback in Text Retrieval,"00:04:31,120","00:04:33,510",64,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=271,we don't have to ask
cs-410_5_1_65,cs-410,5,1, Feedback in Text Retrieval,"00:04:33,510","00:04:38,730",65,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=273,"Instead, we're going to observe how the"
cs-410_5_1_66,cs-410,5,1, Feedback in Text Retrieval,"00:04:38,730","00:04:41,760",66,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=278,So in this case we'll look
cs-410_5_1_67,cs-410,5,1, Feedback in Text Retrieval,"00:04:41,760","00:04:43,930",67,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=281,So the user clicked on this one.
cs-410_5_1_68,cs-410,5,1, Feedback in Text Retrieval,"00:04:43,930","00:04:45,620",68,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=283,And the user viewed this one.
cs-410_5_1_69,cs-410,5,1, Feedback in Text Retrieval,"00:04:45,620","00:04:47,480",69,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=285,And the user skipped this one.
cs-410_5_1_70,cs-410,5,1, Feedback in Text Retrieval,"00:04:47,480","00:04:49,400",70,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=287,And the user viewed this one again.
cs-410_5_1_71,cs-410,5,1, Feedback in Text Retrieval,"00:04:50,410","00:04:56,880",71,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=290,"Now, this also is a clue about whether"
cs-410_5_1_72,cs-410,5,1, Feedback in Text Retrieval,"00:04:56,880","00:05:01,540",72,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=296,And we can even assume that we're
cs-410_5_1_73,cs-410,5,1, Feedback in Text Retrieval,"00:05:01,540","00:05:05,930",73,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=301,"here in this document,"
cs-410_5_1_74,cs-410,5,1, Feedback in Text Retrieval,"00:05:05,930","00:05:10,810",74,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=305,instead of the actual
cs-410_5_1_75,cs-410,5,1, Feedback in Text Retrieval,"00:05:10,810","00:05:15,370",75,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=310,The link they are saying web search
cs-410_5_1_76,cs-410,5,1, Feedback in Text Retrieval,"00:05:15,370","00:05:20,250",76,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=315,If the user tries to fetch this
cs-410_5_1_77,cs-410,5,1, Feedback in Text Retrieval,"00:05:20,250","00:05:25,400",77,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=320,we can assume these displayed
cs-410_5_1_78,cs-410,5,1, Feedback in Text Retrieval,"00:05:25,400","00:05:29,310",78,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=325,is interesting to you so
cs-410_5_1_79,cs-410,5,1, Feedback in Text Retrieval,"00:05:29,310","00:05:31,830",79,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=329,And this is called interesting feedback.
cs-410_5_1_80,cs-410,5,1, Feedback in Text Retrieval,"00:05:31,830","00:05:35,400",80,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=331,"And we can, again,"
cs-410_5_1_81,cs-410,5,1, Feedback in Text Retrieval,"00:05:35,400","00:05:39,760",81,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=335,This is a very important
cs-410_5_1_82,cs-410,5,1, Feedback in Text Retrieval,"00:05:39,760","00:05:42,080",82,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=339,"Now, think about the Google and Bing and"
cs-410_5_1_83,cs-410,5,1, Feedback in Text Retrieval,"00:05:42,080","00:05:46,990",83,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=342,they can collect a lot of user
cs-410_5_1_84,cs-410,5,1, Feedback in Text Retrieval,"00:05:46,990","00:05:51,320",84,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=346,So they would observe what documents
cs-410_5_1_85,cs-410,5,1, Feedback in Text Retrieval,"00:05:51,320","00:05:54,040",85,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=351,And this information is very valuable.
cs-410_5_1_86,cs-410,5,1, Feedback in Text Retrieval,"00:05:54,040","00:05:57,680",86,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=354,And they can use this to
cs-410_5_1_87,cs-410,5,1, Feedback in Text Retrieval,"00:05:59,040","00:06:03,625",87,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=359,"So to summarize, we talked about"
cs-410_5_1_88,cs-410,5,1, Feedback in Text Retrieval,"00:06:03,625","00:06:07,280",88,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=363,Relevant feedback where the user
cs-410_5_1_89,cs-410,5,1, Feedback in Text Retrieval,"00:06:07,280","00:06:11,200",89,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=367,"It takes some user effort, but"
cs-410_5_1_90,cs-410,5,1, Feedback in Text Retrieval,"00:06:11,200","00:06:15,931",90,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=371,We talk about the pseudo feedback where
cs-410_5_1_91,cs-410,5,1, Feedback in Text Retrieval,"00:06:15,931","00:06:17,310",91,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=375,will be relevant.
cs-410_5_1_92,cs-410,5,1, Feedback in Text Retrieval,"00:06:17,310","00:06:20,540",92,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=377,We don't have to involve the user
cs-410_5_1_93,cs-410,5,1, Feedback in Text Retrieval,"00:06:20,540","00:06:23,590",93,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=380,actually before we return
cs-410_5_1_94,cs-410,5,1, Feedback in Text Retrieval,"00:06:24,850","00:06:28,014",94,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=384,And the third is implicit feedback
cs-410_5_1_95,cs-410,5,1, Feedback in Text Retrieval,"00:06:29,685","00:06:31,530",95,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=389,"Where we involve the users, but"
cs-410_5_1_96,cs-410,5,1, Feedback in Text Retrieval,"00:06:31,530","00:06:34,887",96,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=391,the user doesn't have to make
cs-410_5_1_97,cs-410,5,1, Feedback in Text Retrieval,"00:06:34,887","00:06:36,118",97,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=394,Make judgement.
cs-410_5_1_98,cs-410,5,1, Feedback in Text Retrieval,"00:06:36,118","00:06:46,118",98,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=396,[MUSIC]
cs-410_5_2_1,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:00,012","00:00:07,558",1,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=0,[SOUND]
cs-410_5_2_2,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:07,558","00:00:10,370",2,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=7,lecture is about the feedback
cs-410_5_2_3,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:12,910","00:00:18,040",3,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=12,"In this lecture, we continue talking"
cs-410_5_2_4,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:18,040","00:00:21,210",4,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=18,"Particularly, we're going to talk about"
cs-410_5_2_5,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:23,930","00:00:29,210",5,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=23,"As we have discussed before,"
cs-410_5_2_6,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:29,210","00:00:34,890",6,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=29,of text retrieval system is removed from
cs-410_5_2_7,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:34,890","00:00:37,467",7,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=34,We will have positive examples.
cs-410_5_2_8,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:37,467","00:00:40,669",8,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=37,Those are the documents that
cs-410_5_2_9,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:40,669","00:00:42,610",9,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=40,be charged with being relevant.
cs-410_5_2_10,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:42,610","00:00:45,160",10,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=42,All the documents that
cs-410_5_2_11,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:45,160","00:00:46,910",11,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=45,We also have negative examples.
cs-410_5_2_12,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:46,910","00:00:49,590",12,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=46,Those are documents known
cs-410_5_2_13,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:49,590","00:00:52,960",13,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=49,They can also be the documents
cs-410_5_2_14,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:55,350","00:00:58,570",14,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=55,The general method in
cs-410_5_2_15,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:58,570","00:01:02,690",15,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=58,feedback is to modify our query vector.
cs-410_5_2_16,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:04,010","00:01:08,500",16,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=64,We want to place the query vector in
cs-410_5_2_17,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:10,120","00:01:11,520",17,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=70,And what does that mean exactly?
cs-410_5_2_18,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:11,520","00:01:14,930",18,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=71,"Well, if we think about the query vector"
cs-410_5_2_19,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:14,930","00:01:17,270",19,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=74,something to the vector elements.
cs-410_5_2_20,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:17,270","00:01:21,240",20,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=77,"And in general,"
cs-410_5_2_21,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:21,240","00:01:27,129",21,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=81,Or we might just weight of old terms or
cs-410_5_2_22,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:29,230","00:01:32,780",22,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=89,"As a result, in general,"
cs-410_5_2_23,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:32,780","00:01:35,110",23,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=92,We often call this query expansion.
cs-410_5_2_24,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:37,960","00:01:40,920",24,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=97,The most effective method in
cs-410_5_2_25,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:40,920","00:01:44,900",25,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=100,"is called the Rocchio Feedback, which was"
cs-410_5_2_26,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:47,490","00:01:49,110",26,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=107,So the idea is quite simple.
cs-410_5_2_27,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:49,110","00:01:53,402",27,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=109,We illustrate this idea by
cs-410_5_2_28,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:53,402","00:01:58,231",28,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=113,of all the documents in the collection and
cs-410_5_2_29,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:58,231","00:02:03,935",29,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=118,So now we can see the query
cs-410_5_2_30,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:03,935","00:02:07,428",30,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=123,and these are all the documents.
cs-410_5_2_31,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:07,428","00:02:11,230",31,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=127,So when we use the query back there and
cs-410_5_2_32,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:11,230","00:02:14,780",32,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=131,"the most similar documents,"
cs-410_5_2_33,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:14,780","00:02:18,960",33,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=134,that these documents would be
cs-410_5_2_34,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:18,960","00:02:22,512",34,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=138,"And these process are relevant documents,"
cs-410_5_2_35,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:22,512","00:02:27,762",35,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=142,"these are relevant documents,"
cs-410_5_2_36,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:27,762","00:02:32,360",36,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=147,And then these minuses are negative
cs-410_5_2_37,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:34,310","00:02:40,150",37,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=154,So our goal here is trying to move
cs-410_5_2_38,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:40,150","00:02:42,780",38,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=160,to improve the retrieval accuracy.
cs-410_5_2_39,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:42,780","00:02:48,390",39,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=162,"By looking at this diagram,"
cs-410_5_2_40,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:48,390","00:02:50,650",40,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=168,Where should we move the query vector so
cs-410_5_2_41,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:50,650","00:02:53,930",41,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=170,that we can improve
cs-410_5_2_42,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:53,930","00:02:56,990",42,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=173,"Intuitively, where do you"
cs-410_5_2_43,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:58,050","00:03:01,330",43,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=178,"If you want to think more,"
cs-410_5_2_44,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:02,980","00:03:10,090",44,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=182,"If you think about this picture, you can"
cs-410_5_2_45,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:10,090","00:03:15,520",45,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=190,case you want the query vector to be as
cs-410_5_2_46,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:15,520","00:03:20,462",46,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=195,"That means ideally, you want to place"
cs-410_5_2_47,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:20,462","00:03:24,640",47,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=200,Or we want to move the query
cs-410_5_2_48,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:26,510","00:03:29,100",48,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=206,Now so what exactly is this point?
cs-410_5_2_49,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:29,100","00:03:35,710",49,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=209,"Well, if you want these relevant"
cs-410_5_2_50,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:35,710","00:03:41,340",50,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=215,you want this to be in the center of
cs-410_5_2_51,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:41,340","00:03:44,710",51,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=221,Because then if you draw
cs-410_5_2_52,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:44,710","00:03:47,240",52,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=224,you'll get all these relevant documents.
cs-410_5_2_53,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:47,240","00:03:52,250",53,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=227,So that means we can move the query
cs-410_5_2_54,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:52,250","00:03:54,510",54,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=232,all the relevant document vectors.
cs-410_5_2_55,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:55,680","00:03:59,106",55,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=235,And this is basically the idea of Rocchio.
cs-410_5_2_56,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:59,106","00:04:03,645",56,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=239,"Of course, you can consider"
cs-410_5_2_57,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:03,645","00:04:07,040",57,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=243,we want to move away from
cs-410_5_2_58,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:07,040","00:04:11,971",58,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=247,Now your match that we're talking about
cs-410_5_2_59,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:11,971","00:04:14,202",59,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=251,away from other vectors.
cs-410_5_2_60,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:14,202","00:04:18,340",60,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=254,It just means that we have this formula.
cs-410_5_2_61,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:18,340","00:04:22,891",61,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=258,Here you can see this is
cs-410_5_2_62,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:22,891","00:04:29,680",62,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=262,this average basically is the centroid
cs-410_5_2_63,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:29,680","00:04:32,250",63,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=269,"When we take the average of these vectors,"
cs-410_5_2_64,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:32,250","00:04:35,580",64,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=272,then were computing
cs-410_5_2_65,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:35,580","00:04:41,070",65,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=275,"Similarly, this is the average of"
cs-410_5_2_66,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:41,070","00:04:46,080",66,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=281,So it's essentially of
cs-410_5_2_67,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:46,080","00:04:51,710",67,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=286,"And we have these three parameters here,"
cs-410_5_2_68,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:51,710","00:04:55,200",68,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=291,They are controlling
cs-410_5_2_69,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:55,200","00:04:57,560",69,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=295,"When we add these two vectors together,"
cs-410_5_2_70,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:57,560","00:05:02,290",70,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=297,we're moving the query vector
cs-410_5_2_71,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:03,620","00:05:05,740",71,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=303,This is when we add them together.
cs-410_5_2_72,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:05,740","00:05:08,350",72,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=305,"When we subtracted this part,"
cs-410_5_2_73,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:08,350","00:05:14,660",73,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=308,we kind of move the query
cs-410_5_2_74,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:14,660","00:05:18,420",74,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=314,So this is the main idea
cs-410_5_2_75,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:18,420","00:05:20,720",75,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=318,"And after we have done this,"
cs-410_5_2_76,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:20,720","00:05:25,710",76,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=320,we will get a new query vector which
cs-410_5_2_77,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:25,710","00:05:31,905",77,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=325,"This new query vector,"
cs-410_5_2_78,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:31,905","00:05:38,878",78,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=331,original query vector toward this
cs-410_5_2_79,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:38,878","00:05:42,900",79,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=338,away from the non-relevant value.
cs-410_5_2_80,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:45,110","00:05:48,200",80,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=345,"Okay, so let's take a look at the example."
cs-410_5_2_81,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:48,200","00:05:51,360",81,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=348,This is the example that
cs-410_5_2_82,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:51,360","00:05:55,600",82,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=351,Only that I deemed that display
cs-410_5_2_83,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:55,600","00:05:59,210",83,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=355,I only showed the vector
cs-410_5_2_84,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:59,210","00:06:03,240",84,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=359,We have five documents here and we have
cs-410_5_2_85,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:04,760","00:06:09,667",85,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=364,"to read in the documents here, right."
cs-410_5_2_86,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:09,667","00:06:12,650",86,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=369,And they're displayed in red.
cs-410_5_2_87,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:12,650","00:06:14,760",87,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=372,And these are the term vectors.
cs-410_5_2_88,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:14,760","00:06:18,190",88,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=374,Now I have just assumed some of weights.
cs-410_5_2_89,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:18,190","00:06:20,549",89,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=378,"A lot of terms,"
cs-410_5_2_90,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:20,549","00:06:22,745",90,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=380,Now these are negative arguments.
cs-410_5_2_91,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:22,745","00:06:23,952",91,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=382,There are two here.
cs-410_5_2_92,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:23,952","00:06:26,120",92,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=383,There is another one here.
cs-410_5_2_93,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:26,120","00:06:32,520",93,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=386,"Now in this Rocchio method, we first"
cs-410_5_2_94,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:32,520","00:06:37,540",94,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=392,"And so let's see,"
cs-410_5_2_95,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:37,540","00:06:42,910",95,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=397,"the positive documents, we simply just,"
cs-410_5_2_96,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:42,910","00:06:48,490",96,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=402,We just add this with this one
cs-410_5_2_97,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:48,490","00:06:51,560",97,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=408,And then that's down here and
cs-410_5_2_98,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:51,560","00:06:54,801",98,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=411,And then we're going to add
cs-410_5_2_99,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:54,801","00:06:56,580",99,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=414,then just take the average.
cs-410_5_2_100,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:56,580","00:06:58,790",100,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=416,And so we do this for all this.
cs-410_5_2_101,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:58,790","00:07:02,520",101,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=418,"In the end, what we have is this one."
cs-410_5_2_102,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:02,520","00:07:08,380",102,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=422,"This is the average vector of these two,"
cs-410_5_2_103,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:10,030","00:07:13,770",103,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=430,Let's also look at the centroid
cs-410_5_2_104,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:13,770","00:07:15,052",104,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=433,This is basically the same.
cs-410_5_2_105,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:15,052","00:07:18,150",105,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=435,We're going to take the average
cs-410_5_2_106,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:18,150","00:07:22,420",106,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=438,And these are the corresponding
cs-410_5_2_107,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:22,420","00:07:23,020",107,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=442,on and so forth.
cs-410_5_2_108,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:23,020","00:07:25,120",108,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=443,"So in the end, we have this one."
cs-410_5_2_109,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:26,230","00:07:29,340",109,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=446,Now in the Rocchio feedback
cs-410_5_2_110,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:29,340","00:07:32,920",110,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=449,these with the original
cs-410_5_2_111,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:32,920","00:07:36,083",111,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=452,So now let's see how we
cs-410_5_2_112,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:36,083","00:07:37,420",112,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=456,"Well, that's basically this."
cs-410_5_2_113,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:38,830","00:07:42,385",113,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=458,So we have a parameter alpha
cs-410_5_2_114,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:42,385","00:07:45,210",114,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=462,query times weight that's one.
cs-410_5_2_115,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:45,210","00:07:49,626",115,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=465,And now we have beta to control
cs-410_5_2_116,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:49,626","00:07:52,820",116,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=469,"centroid of the weight, that's 1.5."
cs-410_5_2_117,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:52,820","00:07:54,285",117,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=472,That comes from here.
cs-410_5_2_118,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:54,285","00:08:00,400",118,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=474,"All right, so this goes here."
cs-410_5_2_119,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:00,400","00:08:07,555",119,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=480,And we also have this negative
cs-410_5_2_120,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:07,555","00:08:14,520",120,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=487,"And this way, it has come from,"
cs-410_5_2_121,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:14,520","00:08:19,051",121,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=494,And we do exactly the same for
cs-410_5_2_122,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:22,244","00:08:23,840",122,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=502,And this is our new vector.
cs-410_5_2_123,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:25,700","00:08:31,530",123,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=505,And we're going to use this new query
cs-410_5_2_124,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:31,530","00:08:33,840",124,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=511,"You can imagine what would happen, right?"
cs-410_5_2_125,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:33,840","00:08:38,000",125,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=513,Because of the movement that this one
cs-410_5_2_126,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:38,000","00:08:42,520",126,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=518,better because we moved
cs-410_5_2_127,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:42,520","00:08:47,290",127,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=522,And it's going to penalize these black
cs-410_5_2_128,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:47,290","00:08:49,790",128,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=527,So this is precisely what
cs-410_5_2_129,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:50,820","00:08:57,220",129,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=530,Now of course if we apply this method in
cs-410_5_2_130,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:58,240","00:09:04,290",130,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=538,and that is the original query has
cs-410_5_2_131,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:06,410","00:09:08,480",131,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=546,But after we do query explaining and
cs-410_5_2_132,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:08,480","00:09:13,210",132,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=548,"merging, we'll have many times"
cs-410_5_2_133,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:13,210","00:09:16,580",133,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=553,So the calculation will
cs-410_5_2_134,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:18,090","00:09:22,160",134,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=558,"In practice,"
cs-410_5_2_135,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:22,160","00:09:25,470",135,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=562,only retain the terms
cs-410_5_2_136,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:27,000","00:09:29,440",136,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=567,So let's talk about how we
cs-410_5_2_137,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:30,660","00:09:34,220",137,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=570,I just mentioned that they're
cs-410_5_2_138,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:34,220","00:09:37,400",138,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=574,Consider only a small number of
cs-410_5_2_139,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:37,400","00:09:38,690",139,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=577,the centroid vector.
cs-410_5_2_140,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:38,690","00:09:39,900",140,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=578,This is for efficiency concern.
cs-410_5_2_141,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:41,390","00:09:45,580",141,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=581,"I also said here that negative examples,"
cs-410_5_2_142,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:45,580","00:09:49,430",142,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=585,"tend not to be very useful, especially"
cs-410_5_2_143,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:50,860","00:09:52,500",143,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=590,Now you can think about why.
cs-410_5_2_144,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:55,320","00:09:59,771",144,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=595,One reason is because negative documents
cs-410_5_2_145,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:59,771","00:10:00,645",145,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=599,directions.
cs-410_5_2_146,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:00,645","00:10:02,391",146,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=600,"So, when you take the average,"
cs-410_5_2_147,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:02,391","00:10:06,860",147,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=602,it doesn't really tell you where
cs-410_5_2_148,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:06,860","00:10:10,110",148,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=606,Whereas positive documents
cs-410_5_2_149,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:10,110","00:10:14,569",149,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=610,And they will point you to
cs-410_5_2_150,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:14,569","00:10:19,090",150,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=614,So that also means that sometimes we don't
cs-410_5_2_151,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:19,090","00:10:24,580",151,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=619,"But note that in some cases, in difficult"
cs-410_5_2_152,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:24,580","00:10:26,390",152,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=624,negative feedback after is very useful.
cs-410_5_2_153,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:27,550","00:10:29,370",153,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=627,Another thing is to avoid over-fitting.
cs-410_5_2_154,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:29,370","00:10:34,425",154,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=629,That means we have to keep relatively
cs-410_5_2_155,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:34,425","00:10:35,724",155,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=634,Why?
cs-410_5_2_156,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:35,724","00:10:42,250",156,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=635,Because the sample that we see in
cs-410_5_2_157,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:42,250","00:10:45,580",157,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=642,We don't want to overly
cs-410_5_2_158,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:45,580","00:10:49,390",158,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=645,And the original query terms
cs-410_5_2_159,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:49,390","00:10:51,753",159,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=649,Those terms are heightened by the user and
cs-410_5_2_160,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:51,753","00:10:55,850",160,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=651,the user has decided that those
cs-410_5_2_161,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:55,850","00:11:02,530",161,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=655,So in order to prevent
cs-410_5_2_162,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:02,530","00:11:08,910",162,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=662,"drifting, prevent topic drifting due to"
cs-410_5_2_163,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:08,910","00:11:12,740",163,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=668,We generally would have to keep a pretty
cs-410_5_2_164,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:12,740","00:11:13,980",164,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=672,it was safe to do that.
cs-410_5_2_165,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:15,040","00:11:18,910",165,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=675,And this is especially true for
cs-410_5_2_166,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:18,910","00:11:20,910",166,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=678,"Now, this method can be used for"
cs-410_5_2_167,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:20,910","00:11:23,790",167,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=680,both relevance feedback and
cs-410_5_2_168,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:23,790","00:11:28,780",168,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=683,"In the case of pseudo-feedback, the prime"
cs-410_5_2_169,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:28,780","00:11:32,930",169,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=688,value because the relevant examples
cs-410_5_2_170,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:32,930","00:11:36,780",170,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=692,They're not as reliable as
cs-410_5_2_171,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:36,780","00:11:40,830",171,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=696,"In the case of relevance feedback,"
cs-410_5_2_172,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:40,830","00:11:43,580",172,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=700,"So those parameters,"
cs-410_5_2_173,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:45,020","00:11:48,550",173,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=705,And the Rocchio Method is
cs-410_5_2_174,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:48,550","00:11:51,961",174,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=708,It's still a very popular method for
cs-410_5_2_175,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:51,961","00:12:01,961",175,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=711,[MUSIC]
cs-410_5_3_1,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:00,000","00:00:07,194",1,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=0,[SOUND]
cs-410_5_3_2,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:07,194","00:00:10,660",2,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=7,lecture is about the feedback in
cs-410_5_3_3,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:12,540","00:00:17,520",3,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=12,"In this lecture, we will continue the"
cs-410_5_3_4,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:17,520","00:00:18,089",4,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=17,"In particular,"
cs-410_5_3_5,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:18,089","00:00:20,659",5,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=18,we're going to talk about the feedback
cs-410_5_3_6,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:23,450","00:00:29,280",6,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=23,So we derive the query likelihood ranking
cs-410_5_3_7,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:30,410","00:00:35,860",7,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=30,"As a basic retrieval function,"
cs-410_5_3_8,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:35,860","00:00:39,920",8,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=35,But if we think about the feedback
cs-410_5_3_9,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:39,920","00:00:44,730",9,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=39,"use query likelihood to perform feedback,"
cs-410_5_3_10,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:44,730","00:00:49,620",10,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=44,a lot of times the feedback information is
cs-410_5_3_11,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:49,620","00:00:53,260",11,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=49,But we assume the query has
cs-410_5_3_12,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:53,260","00:00:56,850",12,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=53,from a language model in
cs-410_5_3_13,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:56,850","00:01:03,170",13,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=56,It's kind of unnatural to sample
cs-410_5_3_14,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:03,170","00:01:10,330",14,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=63,"As a result, researchers proposed a way"
cs-410_5_3_15,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:10,330","00:01:14,070",15,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=70,and it's called Kullback-Leibler
cs-410_5_3_16,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:15,450","00:01:20,422",16,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=75,And this model is actually going
cs-410_5_3_17,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:20,422","00:01:25,780",17,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=80,retrieval function much
cs-410_5_3_18,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:25,780","00:01:32,380",18,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=85,Yet this form of the language model
cs-410_5_3_19,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:32,380","00:01:36,560",19,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=92,"query likelihood, in the sense that it can"
cs-410_5_3_20,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:38,180","00:01:39,300",20,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=98,"And in this case,"
cs-410_5_3_21,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:39,300","00:01:44,140",21,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=99,then feedback can be achieved through
cs-410_5_3_22,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:44,140","00:01:48,130",22,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=104,"This is very similar to Rocchio,"
cs-410_5_3_23,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:50,000","00:01:55,720",23,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=110,So let's see what is this
cs-410_5_3_24,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:55,720","00:02:02,306",24,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=115,"So on the top, what you see is a query"
cs-410_5_3_25,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:05,072","00:02:11,465",25,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=125,"And then KL-divergence, or"
cs-410_5_3_26,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:11,465","00:02:16,292",26,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=131,retrieval model is basically to generalize
cs-410_5_3_27,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:16,292","00:02:21,600",27,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=136,the frequency part here
cs-410_5_3_28,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:21,600","00:02:26,910",28,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=141,So basically it's the difference given
cs-410_5_3_29,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:26,910","00:02:32,260",29,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=146,by the probabilistic model here to
cs-410_5_3_30,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:32,260","00:02:34,640",30,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=152,versus the count of query words there.
cs-410_5_3_31,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:35,820","00:02:42,610",31,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=155,And this difference allows us to plug in
cs-410_5_3_32,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:42,610","00:02:45,690",32,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=162,So this can be estimated
cs-410_5_3_33,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:45,690","00:02:48,260",33,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=165,including using feedback information.
cs-410_5_3_34,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:48,260","00:02:51,370",34,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=168,"But this is called a KL-divergence,"
cs-410_5_3_35,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:51,370","00:02:56,232",35,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=171,this can be interpreted as matching
cs-410_5_3_36,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:56,232","00:03:02,770",36,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=176,"One is the query model,"
cs-410_5_3_37,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:02,770","00:03:06,317",37,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=182,One is the document
cs-410_5_3_38,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:06,317","00:03:11,255",38,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=186,smooth them with a collection
cs-410_5_3_39,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:11,255","00:03:15,377",39,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=191,And we are not going to talk
cs-410_5_3_40,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:15,377","00:03:18,107",40,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=195,you'll find it in some references.
cs-410_5_3_41,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:18,107","00:03:22,023",41,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=198,"It's also called cross entropy because,"
cs-410_5_3_42,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:22,023","00:03:26,207",42,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=202,we ignore some terms in
cs-410_5_3_43,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:26,207","00:03:29,690",43,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=206,we will end up having
cs-410_5_3_44,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:29,690","00:03:32,109",44,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=209,And both are terms of information theory.
cs-410_5_3_45,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:34,390","00:03:38,650",45,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=214,"But anyway, for our purposes here,"
cs-410_5_3_46,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:38,650","00:03:42,820",46,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=218,you can just see the two
cs-410_5_3_47,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:42,820","00:03:48,330",47,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=222,except that here we have a probability of
cs-410_5_3_48,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:52,140","00:03:57,730",48,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=232,And here the sum is over all the words
cs-410_5_3_49,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:57,730","00:04:02,340",49,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=237,also with the nonzero probability for
cs-410_5_3_50,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:02,340","00:04:07,510",50,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=242,"So it's kind of, again, a generalization"
cs-410_5_3_51,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:09,930","00:04:15,980",51,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=249,Now you can also easily see we can recover
cs-410_5_3_52,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:15,980","00:04:22,130",52,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=255,by simply setting this query model to the
cs-410_5_3_53,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:23,450","00:04:26,510",53,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=263,This is very easy to
cs-410_5_3_54,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:26,510","00:04:30,005",54,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=266,into here you can eliminate this
cs-410_5_3_55,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:30,005","00:04:33,486",55,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=270,And then you will get exactly like that.
cs-410_5_3_56,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:33,486","00:04:35,879",56,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=273,So you can see the equivalence.
cs-410_5_3_57,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:35,879","00:04:41,581",57,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=275,And that's also why this KL-divergence
cs-410_5_3_58,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:41,581","00:04:47,085",58,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=281,"of query likelihood, because we can cover"
cs-410_5_3_59,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:47,085","00:04:49,730",59,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=287,But it would also allow us
cs-410_5_3_60,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:50,770","00:04:56,104",60,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=290,So this is how we can use the
cs-410_5_3_61,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:56,104","00:05:00,183",61,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=296,The picture shows that we first
cs-410_5_3_62,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:00,183","00:05:04,836",62,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=300,"then we estimate a query language model,"
cs-410_5_3_63,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:04,836","00:05:07,040",63,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=304,This is often denoted by a D here.
cs-410_5_3_64,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:09,560","00:05:14,690",64,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=309,But this basically means this is
cs-410_5_3_65,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:14,690","00:05:19,010",65,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=314,because we compute a vector for the
cs-410_5_3_66,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:19,010","00:05:22,450",66,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=319,"the query, and"
cs-410_5_3_67,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:22,450","00:05:26,580",67,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=322,Only that these vectors are of special
cs-410_5_3_68,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:27,910","00:05:31,680",68,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=327,And then we get the results and
cs-410_5_3_69,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:31,680","00:05:37,420",69,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=331,Let's assume they are mostly
cs-410_5_3_70,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:37,420","00:05:40,400",70,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=337,although we could also consider
cs-410_5_3_71,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:40,400","00:05:44,974",71,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=340,"So what we could do is, like in Rocchio,"
cs-410_5_3_72,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:44,974","00:05:48,570",72,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=344,model called the feedback
cs-410_5_3_73,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:48,570","00:05:52,568",73,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=348,"Again, this is going to be another vector"
cs-410_5_3_74,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:52,568","00:05:53,227",74,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=352,in Rocchio.
cs-410_5_3_75,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:53,227","00:05:58,060",75,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=353,And then this model can be combined
cs-410_5_3_76,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:58,060","00:06:02,800",76,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=358,"a linear interpolation, and"
cs-410_5_3_77,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:02,800","00:06:06,260",77,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=362,"just like, again, in Rocchio."
cs-410_5_3_78,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:06,260","00:06:10,270",78,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=366,So here we can see the parameter alpha
cs-410_5_3_79,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:10,270","00:06:14,170",79,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=370,"If it's set to zero,"
cs-410_5_3_80,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:14,170","00:06:19,050",80,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=374,"If it's set to one, we get full feedback"
cs-410_5_3_81,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:19,050","00:06:21,820",81,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=379,"And this is generally not desirable,"
cs-410_5_3_82,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:21,820","00:06:26,370",82,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=381,So unless you are absolutely sure you
cs-410_5_3_83,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:26,370","00:06:29,250",83,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=386,then the query terms are not important.
cs-410_5_3_84,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:31,180","00:06:34,870",84,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=391,"So of course, the main question here is,"
cs-410_5_3_85,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:34,870","00:06:39,340",85,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=394,"This is the big question here, and"
cs-410_5_3_86,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:39,340","00:06:41,760",86,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=399,So here we will talk about
cs-410_5_3_87,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:41,760","00:06:43,260",87,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=401,"there are many approaches, of course."
cs-410_5_3_88,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:43,260","00:06:45,891",88,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=403,This approach is based
cs-410_5_3_89,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:45,891","00:06:47,823",89,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=405,I'm going to show you how it works.
cs-410_5_3_90,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:47,823","00:06:50,560",90,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=407,This will use a generative mixture model.
cs-410_5_3_91,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:50,560","00:06:55,030",91,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=410,So this picture shows that
cs-410_5_3_92,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:55,030","00:06:57,060",92,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=415,the feedback model that
cs-410_5_3_93,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:58,080","00:07:00,490",93,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=418,And the basis is the feedback documents.
cs-410_5_3_94,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:00,490","00:07:04,110",94,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=420,Let's say we are observing
cs-410_5_3_95,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:04,110","00:07:09,012",95,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=424,These are the clicked documents by users
cs-410_5_3_96,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:09,012","00:07:12,679",96,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=429,or are simply top ranked documents
cs-410_5_3_97,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:14,710","00:07:17,834",97,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=434,Now imagine how we can
cs-410_5_3_98,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:17,834","00:07:20,630",98,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=437,these documents by using language model.
cs-410_5_3_99,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:20,630","00:07:23,330",99,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=440,One approach is simply to assume
cs-410_5_3_100,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:23,330","00:07:26,820",100,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=443,these documents are generated
cs-410_5_3_101,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:26,820","00:07:31,287",101,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=446,"As we did before, what we could do"
cs-410_5_3_102,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:31,287","00:07:34,940",102,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=451,here to here and
cs-410_5_3_103,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:36,210","00:07:41,260",103,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=456,Now the question is whether this
cs-410_5_3_104,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:41,260","00:07:45,430",104,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=461,"Well, you can imagine the top"
cs-410_5_3_105,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:45,430","00:07:46,190",105,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=465,What do you think?
cs-410_5_3_106,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:48,280","00:07:51,560",106,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=468,"Well, those words would be common words."
cs-410_5_3_107,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:51,560","00:07:53,770",107,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=471,"As we always see in a language model,"
cs-410_5_3_108,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:53,770","00:07:57,850",108,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=473,the top ranked words are actually
cs-410_5_3_109,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:57,850","00:08:02,570",109,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=477,"So it's not very good for feedback,"
cs-410_5_3_110,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:02,570","00:08:07,330",110,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=482,words to our query when we interpolate
cs-410_5_3_111,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:08,880","00:08:13,100",111,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=488,"So this was not good, so"
cs-410_5_3_112,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:13,100","00:08:17,059",112,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=493,"In particular, we are trying to"
cs-410_5_3_113,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:17,059","00:08:21,855",113,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=497,And we have seen actually one way
cs-410_5_3_114,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:21,855","00:08:27,020",114,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=501,language model in the case of
cs-410_5_3_115,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:27,020","00:08:30,830",115,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=507,the words that are related
cs-410_5_3_116,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:30,830","00:08:34,590",116,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=510,We could do that and that would be
cs-410_5_3_117,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:34,590","00:08:39,160",117,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=514,are going to talk about another approach
cs-410_5_3_118,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:39,160","00:08:43,990",118,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=519,"In this case, we're going to say well,"
cs-410_5_3_119,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:43,990","00:08:48,818",119,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=523,in these documents that should not
cs-410_5_3_120,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:50,310","00:08:53,527",120,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=530,"So now what we can do is to assume that,"
cs-410_5_3_121,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:53,527","00:08:58,019",121,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=533,those words are generated from
cs-410_5_3_122,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:58,019","00:09:02,020",122,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=538,"they will generate those words like the,"
cs-410_5_3_123,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:02,020","00:09:05,302",123,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=542,"And if we use maximum likelihood estimate,"
cs-410_5_3_124,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:05,302","00:09:10,182",124,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=545,note that if all the words here
cs-410_5_3_125,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:10,182","00:09:15,681",125,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=550,then this model is forced to assign
cs-410_5_3_126,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:15,681","00:09:19,620",126,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=555,because it occurs so frequently here.
cs-410_5_3_127,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:19,620","00:09:25,100",127,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=559,Note that in order to reduce its
cs-410_5_3_128,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:25,100","00:09:31,280",128,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=565,"another model, which is this one,"
cs-410_5_3_129,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:31,280","00:09:32,218",129,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=571,"And in this case,"
cs-410_5_3_130,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:32,218","00:09:37,200",130,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=572,it's not appropriate to use the background
cs-410_5_3_131,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:37,200","00:09:42,320",131,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=577,goal because this model would assign high
cs-410_5_3_132,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:43,370","00:09:46,000",132,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=583,"So in this approach, then,"
cs-410_5_3_133,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:46,000","00:09:50,810",133,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=586,we assume this machine that was generating
cs-410_5_3_134,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:50,810","00:09:53,630",134,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=590,We have a source control up here.
cs-410_5_3_135,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:53,630","00:09:59,110",135,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=593,Imagine we flip a coin here to
cs-410_5_3_136,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:59,110","00:10:03,238",136,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=599,"With probability of lambda,"
cs-410_5_3_137,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:03,238","00:10:05,400",137,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=603,we're going to use
cs-410_5_3_138,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:05,400","00:10:08,540",138,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=605,And we're going to do that in
cs-410_5_3_139,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:08,540","00:10:12,570",139,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=608,"With probability of 1 minus lambda,"
cs-410_5_3_140,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:12,570","00:10:17,460",140,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=612,"to use a known topic model, here,"
cs-410_5_3_141,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:17,460","00:10:20,100",141,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=617,And we're going to then
cs-410_5_3_142,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:20,100","00:10:25,450",142,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=620,If we make this assumption and this whole
cs-410_5_3_143,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:25,450","00:10:30,420",143,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=625,this a mixture model because there are two
cs-410_5_3_144,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:30,420","00:10:33,940",144,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=630,And we actually don't know when
cs-410_5_3_145,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:35,770","00:10:40,320",145,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=635,"So again,"
cs-410_5_3_146,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:42,270","00:10:47,920",146,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=642,and we can still ask for words and it will
cs-410_5_3_147,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:47,920","00:10:51,920",147,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=647,"And of course, which word will show up"
cs-410_5_3_148,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:51,920","00:10:53,003",148,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=651,that distribution.
cs-410_5_3_149,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:53,003","00:10:55,780",149,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=653,"In addition,"
cs-410_5_3_150,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:55,780","00:10:58,751",150,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=655,because if you say lambda is very high and
cs-410_5_3_151,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:58,751","00:11:02,769",151,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=658,"always use the background distribution,"
cs-410_5_3_152,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:02,769","00:11:07,260",152,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=662,"Then if you say, well, lambda is"
cs-410_5_3_153,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:07,260","00:11:12,353",153,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=667,So all of these
cs-410_5_3_154,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:12,353","00:11:15,108",154,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=672,"And then if you're thinking this way,"
cs-410_5_3_155,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:15,108","00:11:19,206",155,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=675,basically we can do exactly
cs-410_5_3_156,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:19,206","00:11:23,445",156,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=679,We're going to use maximum likelihood
cs-410_5_3_157,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:23,445","00:11:25,760",157,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=683,to estimate the parameters.
cs-410_5_3_158,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:25,760","00:11:30,201",158,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=685,Basically we're going to
cs-410_5_3_159,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:30,201","00:11:33,512",159,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=690,that we can best explain all the data.
cs-410_5_3_160,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:33,512","00:11:41,200",160,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=693,The difference now is that we are not
cs-410_5_3_161,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:41,200","00:11:46,633",161,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=701,But rather we are going to ask this whole
cs-410_5_3_162,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:46,633","00:11:50,049",162,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=706,Because it has got some help
cs-410_5_3_163,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:50,049","00:11:54,080",163,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=710,it doesn't have to assign high
cs-410_5_3_164,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:54,080","00:11:58,890",164,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=714,"As a result, it will then assign higher"
cs-410_5_3_165,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:58,890","00:12:04,950",165,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=718,are common here but
cs-410_5_3_166,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:04,950","00:12:06,877",166,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=724,So those would be common here.
cs-410_5_3_167,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:11,321","00:12:14,907",167,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=731,"And if they're common, they would"
cs-410_5_3_168,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:14,907","00:12:17,661",168,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=734,according to a maximum
cs-410_5_3_169,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:17,661","00:12:23,692",169,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=737,"And if they are rare here,"
cs-410_5_3_170,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:23,692","00:12:29,620",170,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=743,much help from this background model.
cs-410_5_3_171,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:29,620","00:12:33,940",171,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=749,"As a result, this topic model"
cs-410_5_3_172,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:33,940","00:12:37,410",172,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=753,"So the high probability words,"
cs-410_5_3_173,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:37,410","00:12:41,630",173,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=757,would be those that are common here but
cs-410_5_3_174,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:43,960","00:12:48,897",174,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=763,So this is basically a little bit
cs-410_5_3_175,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:48,897","00:12:53,664",175,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=768,But this would allow us to achieve the
cs-410_5_3_176,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:53,664","00:12:55,770",176,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=773,are meaningless in the feedback.
cs-410_5_3_177,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:56,780","00:13:01,200",177,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=776,"So mathematically, what we have is"
cs-410_5_3_178,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:01,200","00:13:04,794",178,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=781,"local likelihood,"
cs-410_5_3_179,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:06,200","00:13:08,860",179,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=786,And note that we also have another
cs-410_5_3_180,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:08,860","00:13:13,150",180,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=788,we assume that the lambda denotes
cs-410_5_3_181,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:13,150","00:13:16,010",181,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=793,"So we are going to,"
cs-410_5_3_182,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:16,010","00:13:21,800",182,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=796,Let's say 50% of the words are noise or
cs-410_5_3_183,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:21,800","00:13:24,295",183,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=801,And this can then be
cs-410_5_3_184,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:24,295","00:13:30,896",184,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=804,"If we assume this is fixed, then we only"
cs-410_5_3_185,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:30,896","00:13:35,090",185,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=810,just like in the simple
cs-410_5_3_186,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:35,090","00:13:39,090",186,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=815,"We have n parameters,"
cs-410_5_3_187,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:39,090","00:13:41,289",187,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=819,And then the likelihood
cs-410_5_3_188,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:42,760","00:13:47,643",188,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=822,It's very similar to the global
cs-410_5_3_189,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:47,643","00:13:51,537",189,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=827,except that inside the logarithm
cs-410_5_3_190,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:51,537","00:13:57,070",190,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=831,And this sum is because we
cs-410_5_3_191,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:57,070","00:14:01,300",191,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=837,And which one is used would depend on
cs-410_5_3_192,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:02,460","00:14:08,790",192,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=842,"But mathematically, this is the function"
cs-410_5_3_193,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:08,790","00:14:10,510",193,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=848,So this is just a function.
cs-410_5_3_194,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:10,510","00:14:13,620",194,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=850,All the other values are known except for
cs-410_5_3_195,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:15,010","00:14:19,834",195,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=855,So we can then choose this
cs-410_5_3_196,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:19,834","00:14:21,531",196,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=859,"this log likelihood,"
cs-410_5_3_197,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:21,531","00:14:27,357",197,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=861,the same idea as the maximum likelihood
cs-410_5_3_198,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:27,357","00:14:30,060",198,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=867,We just have to solve this
cs-410_5_3_199,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:30,060","00:14:34,460",199,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=870,We essentially would try all
cs-410_5_3_200,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:34,460","00:14:37,670",200,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=874,that gives this whole thing
cs-410_5_3_201,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:37,670","00:14:39,210",201,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=877,So it's a well-defined math problem.
cs-410_5_3_202,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:40,900","00:14:45,720",202,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=880,"Once we have done that, we obtain this"
cs-410_5_3_203,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:45,720","00:14:47,812",203,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=885,original query model to the feedback.
cs-410_5_3_204,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:50,980","00:14:55,963",204,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=890,So here are some examples of
cs-410_5_3_205,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:55,963","00:14:57,817",205,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=895,document collection.
cs-410_5_3_206,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:57,817","00:15:01,673",206,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=897,And we do pseudo-feedback we just
cs-410_5_3_207,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:01,673","00:15:03,750",207,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=901,we use this mixture model.
cs-410_5_3_208,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:03,750","00:15:06,090",208,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=903,So the query is airport security.
cs-410_5_3_209,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:06,090","00:15:11,480",209,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=906,What we do is we first retrieve ten
cs-410_5_3_210,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:11,480","00:15:14,520",210,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=911,this is of course pseudo-feedback.
cs-410_5_3_211,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:14,520","00:15:20,000",211,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=914,And then we're going to feed that
cs-410_5_3_212,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:21,130","00:15:25,770",212,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=921,And these are the words
cs-410_5_3_213,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:25,770","00:15:30,220",213,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=925,This is the probability of a word given
cs-410_5_3_214,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:31,600","00:15:34,350",214,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=931,So in both cases you can see the highest
cs-410_5_3_215,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:34,350","00:15:38,480",215,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=934,probability words include the very
cs-410_5_3_216,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:38,480","00:15:40,208",216,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=938,"So airport security, for example,"
cs-410_5_3_217,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:40,208","00:15:45,450",217,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=940,these query words still show up as high
cs-410_5_3_218,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:45,450","00:15:48,850",218,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=945,because they occur frequently
cs-410_5_3_219,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:48,850","00:15:53,830",219,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=948,"But we also see beverage,"
cs-410_5_3_220,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:53,830","00:15:59,436",220,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=953,"So these are relevant to this topic,"
cs-410_5_3_221,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:59,436","00:16:05,280",221,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=959,"if combined with original query, can help"
cs-410_5_3_222,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:05,280","00:16:11,200",222,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=965,And also they can help us bring up
cs-410_5_3_223,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:11,200","00:16:16,980",223,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=971,"these other words, maybe, for example,"
cs-410_5_3_224,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:18,070","00:16:20,680",224,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=978,So this is how pseudo-feedback works.
cs-410_5_3_225,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:20,680","00:16:26,790",225,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=980,It shows that this model really works and
cs-410_5_3_226,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:26,790","00:16:31,546",226,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=986,What's also interesting is that if
cs-410_5_3_227,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:31,546","00:16:35,154",227,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=991,"you compare them,"
cs-410_5_3_228,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:35,154","00:16:40,415",228,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=995,"when lambda is set to a small value,"
cs-410_5_3_229,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:40,415","00:16:45,473",229,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1000,"And that means, well,"
cs-410_5_3_230,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:45,473","00:16:48,575",230,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1005,"Remember, lambda confuses the probability"
cs-410_5_3_231,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:48,575","00:16:50,925",231,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1008,to generate the text.
cs-410_5_3_232,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:50,925","00:16:53,245",232,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1010,"If we don't rely much on background model,"
cs-410_5_3_233,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:53,245","00:16:58,100",233,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1013,we still have to use this topic model
cs-410_5_3_234,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:58,100","00:17:01,340",234,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1018,Whereas if we set lambda
cs-410_5_3_235,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:01,340","00:17:05,550",235,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1021,we will use the background model
cs-410_5_3_236,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:05,550","00:17:08,930",236,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1025,Then there's no burden on
cs-410_5_3_237,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:08,930","00:17:11,790",237,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1028,in the feedback documents
cs-410_5_3_238,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:11,790","00:17:17,430",238,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1031,"So as a result, the topic model"
cs-410_5_3_239,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:17,430","00:17:20,060",239,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1037,It contains all the relevant
cs-410_5_3_240,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:21,260","00:17:26,100",240,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1041,So this can be added to the original
cs-410_5_3_241,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:28,140","00:17:29,900",241,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1048,"So to summarize,"
cs-410_5_3_242,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:29,900","00:17:34,470",242,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1049,in this lecture we have talked about
cs-410_5_3_243,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:34,470","00:17:38,290",243,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1054,"In general,"
cs-410_5_3_244,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:38,290","00:17:43,610",244,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1058,"These examples can be assumed examples,"
cs-410_5_3_245,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:43,610","00:17:48,770",245,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1063,like assume the top ten documents
cs-410_5_3_246,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:48,770","00:17:51,419",246,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1068,"They could be based on user interactions,"
cs-410_5_3_247,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:51,419","00:17:55,260",247,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1071,like feedback based on clickthroughs or
cs-410_5_3_248,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:55,260","00:17:59,308",248,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1075,We talked about the three major
cs-410_5_3_249,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:59,308","00:18:01,657",249,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1079,"pseudo feedback, and implicit feedback."
cs-410_5_3_250,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:01,657","00:18:08,108",250,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1081,We talked about how to use Rocchio to
cs-410_5_3_251,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:08,108","00:18:14,047",251,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1088,how to use query model estimation for
cs-410_5_3_252,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:14,047","00:18:18,350",252,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1094,And we briefly talked about
cs-410_5_3_253,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:19,790","00:18:21,650",253,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1099,There are many other methods.
cs-410_5_3_254,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:21,650","00:18:22,170",254,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1101,"For example,"
cs-410_5_3_255,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:22,170","00:18:26,990",255,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1102,the relevance model is a very effective
cs-410_5_3_256,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:26,990","00:18:31,130",256,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1106,So you can read more about these
cs-410_5_3_257,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:32,170","00:18:36,200",257,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1112,are listed at the end of this lecture.
cs-410_5_3_258,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:36,200","00:18:38,420",258,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1116,So there are two additional readings here.
cs-410_5_3_259,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:38,420","00:18:42,047",259,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1118,The first one is a book that
cs-410_5_3_260,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:42,047","00:18:46,170",260,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1122,discussion of language models for
cs-410_5_3_261,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:46,170","00:18:49,745",261,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1126,And the second one is a important research
cs-410_5_3_262,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:49,745","00:18:54,549",262,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1129,paper that's about relevance
cs-410_5_3_263,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:54,549","00:18:59,471",263,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1134,and it's a very effective way
cs-410_5_3_264,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:59,471","00:19:09,471",264,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1139,[MUSIC]
cs-410_5_4_1,cs-410,5,4, Web Search,"00:00:07,440","00:00:09,410",1,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=7,This lecture is about Web Search.
cs-410_5_4_2,cs-410,5,4, Web Search,"00:00:11,950","00:00:14,750",2,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=11,"In this lecture,"
cs-410_5_4_3,cs-410,5,4, Web Search,"00:00:14,750","00:00:19,150",3,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=14,of the most important applications of
cs-410_5_4_4,cs-410,5,4, Web Search,"00:00:19,150","00:00:21,520",4,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=19,So let's first look at some
cs-410_5_4_5,cs-410,5,4, Web Search,"00:00:21,520","00:00:23,380",5,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=21,opportunities in web search.
cs-410_5_4_6,cs-410,5,4, Web Search,"00:00:23,380","00:00:26,010",6,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=23,"Now, many informational"
cs-410_5_4_7,cs-410,5,4, Web Search,"00:00:26,010","00:00:29,010",7,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=26,had been developed
cs-410_5_4_8,cs-410,5,4, Web Search,"00:00:29,010","00:00:33,890",8,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=29,"So when the web was born,"
cs-410_5_4_9,cs-410,5,4, Web Search,"00:00:33,890","00:00:39,890",9,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=33,those algorithms to major application
cs-410_5_4_10,cs-410,5,4, Web Search,"00:00:39,890","00:00:45,780",10,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=39,"So naturally, there have to be some"
cs-410_5_4_11,cs-410,5,4, Web Search,"00:00:45,780","00:00:53,460",11,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=45,search algorithms to address new
cs-410_5_4_12,cs-410,5,4, Web Search,"00:00:53,460","00:00:56,210",12,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=53,So here are some general challenges.
cs-410_5_4_13,cs-410,5,4, Web Search,"00:00:56,210","00:00:58,510",13,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=56,"First, this is a scalability challenge."
cs-410_5_4_14,cs-410,5,4, Web Search,"00:00:58,510","00:01:00,200",14,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=58,How to handle the size of the web and
cs-410_5_4_15,cs-410,5,4, Web Search,"00:01:00,200","00:01:02,750",15,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=60,ensure completeness of
cs-410_5_4_16,cs-410,5,4, Web Search,"00:01:03,870","00:01:07,820",16,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=63,How to serve many users quickly and
cs-410_5_4_17,cs-410,5,4, Web Search,"00:01:07,820","00:01:10,801",17,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=67,And so that's one major challenge and
cs-410_5_4_18,cs-410,5,4, Web Search,"00:01:10,801","00:01:16,480",18,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=70,before the web was born the scale
cs-410_5_4_19,cs-410,5,4, Web Search,"00:01:16,480","00:01:20,190",19,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=76,The second problem is that there's
cs-410_5_4_20,cs-410,5,4, Web Search,"00:01:20,190","00:01:21,960",20,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=80,there are often spams.
cs-410_5_4_21,cs-410,5,4, Web Search,"00:01:21,960","00:01:24,334",21,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=81,The third challenge is
cs-410_5_4_22,cs-410,5,4, Web Search,"00:01:24,334","00:01:31,879",22,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=84,The new pages are constantly create and
cs-410_5_4_23,cs-410,5,4, Web Search,"00:01:31,879","00:01:36,281",23,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=91,so it makes it harder to
cs-410_5_4_24,cs-410,5,4, Web Search,"00:01:36,281","00:01:40,391",24,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=96,So these are some of the challenges
cs-410_5_4_25,cs-410,5,4, Web Search,"00:01:40,391","00:01:42,880",25,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=100,deal with high quality web searching.
cs-410_5_4_26,cs-410,5,4, Web Search,"00:01:44,090","00:01:47,492",26,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=104,On the other hand there are also some
cs-410_5_4_27,cs-410,5,4, Web Search,"00:01:47,492","00:01:49,930",27,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=107,leverage to include the search results.
cs-410_5_4_28,cs-410,5,4, Web Search,"00:01:49,930","00:01:53,330",28,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=109,"There are many additional heuristics,"
cs-410_5_4_29,cs-410,5,4, Web Search,"00:01:55,070","00:02:00,020",29,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=115,using links that we can
cs-410_5_4_30,cs-410,5,4, Web Search,"00:02:00,020","00:02:03,510",30,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=120,Now everything that we talked about
cs-410_5_4_31,cs-410,5,4, Web Search,"00:02:03,510","00:02:04,459",31,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=123,are general algorithms.
cs-410_5_4_32,cs-410,5,4, Web Search,"00:02:05,630","00:02:11,050",32,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=125,They can be applied to any search
cs-410_5_4_33,cs-410,5,4, Web Search,"00:02:11,050","00:02:15,890",33,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=131,"On the other hand, they also don't take"
cs-410_5_4_34,cs-410,5,4, Web Search,"00:02:15,890","00:02:21,375",34,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=135,of pages or documents in the specific
cs-410_5_4_35,cs-410,5,4, Web Search,"00:02:21,375","00:02:23,855",35,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=141,"Web pages are linked with each other,"
cs-410_5_4_36,cs-410,5,4, Web Search,"00:02:23,855","00:02:28,645",36,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=143,the linking is something
cs-410_5_4_37,cs-410,5,4, Web Search,"00:02:28,645","00:02:33,610",37,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=148,"So, because of these challenges and"
cs-410_5_4_38,cs-410,5,4, Web Search,"00:02:33,610","00:02:39,110",38,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=153,that have been developed for
cs-410_5_4_39,cs-410,5,4, Web Search,"00:02:39,110","00:02:41,390",39,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=159,One is parallel indexing and searching and
cs-410_5_4_40,cs-410,5,4, Web Search,"00:02:41,390","00:02:44,410",40,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=161,this is to address
cs-410_5_4_41,cs-410,5,4, Web Search,"00:02:44,410","00:02:49,930",41,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=164,"In particular, Google's imaging of"
cs-410_5_4_42,cs-410,5,4, Web Search,"00:02:49,930","00:02:53,590",42,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=169,has been very helpful in that aspect.
cs-410_5_4_43,cs-410,5,4, Web Search,"00:02:53,590","00:02:56,680",43,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=173,"Second, there are techniques"
cs-410_5_4_44,cs-410,5,4, Web Search,"00:02:56,680","00:03:00,460",44,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=176,"addressing the problem of spams,"
cs-410_5_4_45,cs-410,5,4, Web Search,"00:03:00,460","00:03:03,570",45,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=180,We'll have to prevent those spam
cs-410_5_4_46,cs-410,5,4, Web Search,"00:03:04,680","00:03:07,338",46,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=184,And there are also techniques
cs-410_5_4_47,cs-410,5,4, Web Search,"00:03:07,338","00:03:10,520",47,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=187,And we're going to use a lot
cs-410_5_4_48,cs-410,5,4, Web Search,"00:03:10,520","00:03:15,410",48,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=190,that it's not easy to spam the search
cs-410_5_4_49,cs-410,5,4, Web Search,"00:03:15,410","00:03:19,810",49,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=195,And the third line of techniques is link
cs-410_5_4_50,cs-410,5,4, Web Search,"00:03:19,810","00:03:24,730",50,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=199,analysis and these are techniques that can
cs-410_5_4_51,cs-410,5,4, Web Search,"00:03:24,730","00:03:30,780",51,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=204,allow us to improve such results
cs-410_5_4_52,cs-410,5,4, Web Search,"00:03:30,780","00:03:35,230",52,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=210,"And in general in web searching,"
cs-410_5_4_53,cs-410,5,4, Web Search,"00:03:35,230","00:03:37,690",53,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=215,ranking not just for link analysis.
cs-410_5_4_54,cs-410,5,4, Web Search,"00:03:37,690","00:03:43,300",54,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=217,But also exploring all kinds
cs-410_5_4_55,cs-410,5,4, Web Search,"00:03:43,300","00:03:47,730",55,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=223,anchor text that describes
cs-410_5_4_56,cs-410,5,4, Web Search,"00:03:47,730","00:03:51,310",56,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=227,"So, here's a picture showing"
cs-410_5_4_57,cs-410,5,4, Web Search,"00:03:51,310","00:03:55,820",57,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=231,"Basically, this is the web on the left and"
cs-410_5_4_58,cs-410,5,4, Web Search,"00:03:55,820","00:04:00,550",58,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=235,we're going to help this user to get
cs-410_5_4_59,cs-410,5,4, Web Search,"00:04:00,550","00:04:05,410",59,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=240,And the first component is a Crawler that
cs-410_5_4_60,cs-410,5,4, Web Search,"00:04:05,410","00:04:09,810",60,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=245,component is Indexer that would take
cs-410_5_4_61,cs-410,5,4, Web Search,"00:04:10,920","00:04:15,720",61,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=250,The third component there is a Retriever
cs-410_5_4_62,cs-410,5,4, Web Search,"00:04:15,720","00:04:19,840",62,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=255,answer user's query by talking
cs-410_5_4_63,cs-410,5,4, Web Search,"00:04:19,840","00:04:24,790",63,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=259,And then the search results will be given
cs-410_5_4_64,cs-410,5,4, Web Search,"00:04:24,790","00:04:29,090",64,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=264,"show those results, it allows"
cs-410_5_4_65,cs-410,5,4, Web Search,"00:04:29,090","00:04:32,417",65,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=269,"So, we're going to talk about"
cs-410_5_4_66,cs-410,5,4, Web Search,"00:04:32,417","00:04:37,552",66,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=272,"First of all, we're going to talk about"
cs-410_5_4_67,cs-410,5,4, Web Search,"00:04:37,552","00:04:42,459",67,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=277,software robot that would do something
cs-410_5_4_68,cs-410,5,4, Web Search,"00:04:42,459","00:04:44,954",68,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=282,"To build a toy crawler is relatively easy,"
cs-410_5_4_69,cs-410,5,4, Web Search,"00:04:44,954","00:04:47,875",69,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=284,because you just need to start
cs-410_5_4_70,cs-410,5,4, Web Search,"00:04:47,875","00:04:51,912",70,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=287,And then fetch pages from the web and
cs-410_5_4_71,cs-410,5,4, Web Search,"00:04:51,912","00:04:53,517",71,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=291,figure out new links.
cs-410_5_4_72,cs-410,5,4, Web Search,"00:04:53,517","00:05:00,994",72,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=293,And then add them to the priority que and
cs-410_5_4_73,cs-410,5,4, Web Search,"00:05:00,994","00:05:04,764",73,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=300,But to be able to real crawler
cs-410_5_4_74,cs-410,5,4, Web Search,"00:05:04,764","00:05:09,249",74,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=304,there are some complicated issues
cs-410_5_4_75,cs-410,5,4, Web Search,"00:05:09,249","00:05:13,736",75,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=309,"For example robustness,"
cs-410_5_4_76,cs-410,5,4, Web Search,"00:05:13,736","00:05:18,722",76,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=313,what if there's a trap that generates
cs-410_5_4_77,cs-410,5,4, Web Search,"00:05:18,722","00:05:23,456",77,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=318,that might attract your crawler to
cs-410_5_4_78,cs-410,5,4, Web Search,"00:05:23,456","00:05:26,700",78,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=323,to fetch dynamic generated pages?
cs-410_5_4_79,cs-410,5,4, Web Search,"00:05:26,700","00:05:30,093",79,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=326,The results of this issue
cs-410_5_4_80,cs-410,5,4, Web Search,"00:05:30,093","00:05:35,668",80,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=330,you don't want to overload one particular
cs-410_5_4_81,cs-410,5,4, Web Search,"00:05:35,668","00:05:39,158",81,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=335,you have to respect the robot
cs-410_5_4_82,cs-410,5,4, Web Search,"00:05:39,158","00:05:43,340",82,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=339,You also need to handle different
cs-410_5_4_83,cs-410,5,4, Web Search,"00:05:43,340","00:05:46,019",83,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=343,"PDF files,"
cs-410_5_4_84,cs-410,5,4, Web Search,"00:05:46,019","00:05:50,189",84,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=346,And you have to also
cs-410_5_4_85,cs-410,5,4, Web Search,"00:05:50,189","00:05:56,237",85,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=350,sometimes those are CGI scripts and
cs-410_5_4_86,cs-410,5,4, Web Search,"00:05:56,237","00:06:01,139",86,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=356,"etc, and sometimes you have"
cs-410_5_4_87,cs-410,5,4, Web Search,"00:06:01,139","00:06:03,866",87,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=361,they also create challenges.
cs-410_5_4_88,cs-410,5,4, Web Search,"00:06:03,866","00:06:08,795",88,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=363,And you ideally should also recognize
cs-410_5_4_89,cs-410,5,4, Web Search,"00:06:08,795","00:06:11,475",89,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=368,to duplicate those pages.
cs-410_5_4_90,cs-410,5,4, Web Search,"00:06:11,475","00:06:15,398",90,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=371,"And finally, you may be interested"
cs-410_5_4_91,cs-410,5,4, Web Search,"00:06:15,398","00:06:19,935",91,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=375,Those are URLs that may not be linked
cs-410_5_4_92,cs-410,5,4, Web Search,"00:06:19,935","00:06:24,884",92,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=379,"the URL to a shorter path, you might"
cs-410_5_4_93,cs-410,5,4, Web Search,"00:06:27,008","00:06:29,298",93,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=387,So what are the Major Crawling Strategies?
cs-410_5_4_94,cs-410,5,4, Web Search,"00:06:29,298","00:06:30,040",94,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=389,"In general,"
cs-410_5_4_95,cs-410,5,4, Web Search,"00:06:30,040","00:06:36,560",95,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=390,Breadth-First is most common because
cs-410_5_4_96,cs-410,5,4, Web Search,"00:06:36,560","00:06:41,405",96,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=396,You would not keep probing a particular
cs-410_5_4_97,cs-410,5,4, Web Search,"00:06:42,635","00:06:47,009",97,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=402,Also parallel crawling is very
cs-410_5_4_98,cs-410,5,4, Web Search,"00:06:47,009","00:06:48,554",98,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=407,easy to parallelize.
cs-410_5_4_99,cs-410,5,4, Web Search,"00:06:48,554","00:06:50,887",99,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=408,And there is some variations
cs-410_5_4_100,cs-410,5,4, Web Search,"00:06:50,887","00:06:54,560",100,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=410,and one interesting variation
cs-410_5_4_101,cs-410,5,4, Web Search,"00:06:54,560","00:06:59,850",101,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=414,"In this case, we're going to crawl just"
cs-410_5_4_102,cs-410,5,4, Web Search,"00:06:59,850","00:07:04,316",102,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=419,"For example,"
cs-410_5_4_103,cs-410,5,4, Web Search,"00:07:04,316","00:07:07,885",103,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=424,And this is typically going to
cs-410_5_4_104,cs-410,5,4, Web Search,"00:07:07,885","00:07:12,953",104,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=427,then you can use the query to get some
cs-410_5_4_105,cs-410,5,4, Web Search,"00:07:12,953","00:07:17,052",105,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=432,And then you can start it with those
cs-410_5_4_106,cs-410,5,4, Web Search,"00:07:17,052","00:07:19,544",106,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=437,"The one channel in crawling,"
cs-410_5_4_107,cs-410,5,4, Web Search,"00:07:19,544","00:07:24,230",107,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=439,is you will find the new
cs-410_5_4_108,cs-410,5,4, Web Search,"00:07:24,230","00:07:28,732",108,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=444,people probably are creating
cs-410_5_4_109,cs-410,5,4, Web Search,"00:07:28,732","00:07:33,502",109,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=448,And this is very challenging if
cs-410_5_4_110,cs-410,5,4, Web Search,"00:07:33,502","00:07:35,930",110,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=453,linked to any old pages.
cs-410_5_4_111,cs-410,5,4, Web Search,"00:07:35,930","00:07:41,655",111,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=455,"If they are, then you can probably find"
cs-410_5_4_112,cs-410,5,4, Web Search,"00:07:41,655","00:07:46,946",112,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=461,so these are also some interesting
cs-410_5_4_113,cs-410,5,4, Web Search,"00:07:46,946","00:07:51,257",113,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=466,"And finally, we might face the scenario"
cs-410_5_4_114,cs-410,5,4, Web Search,"00:07:51,257","00:07:53,157",114,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=471,"repeated crawling, right."
cs-410_5_4_115,cs-410,5,4, Web Search,"00:07:53,157","00:07:56,528",115,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=473,"Let's say,"
cs-410_5_4_116,cs-410,5,4, Web Search,"00:07:56,528","00:07:59,448",116,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=476,and you first crawl a lot
cs-410_5_4_117,cs-410,5,4, Web Search,"00:07:59,448","00:08:03,816",117,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=479,"But then,"
cs-410_5_4_118,cs-410,5,4, Web Search,"00:08:03,816","00:08:08,968",118,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=483,in the future you just need
cs-410_5_4_119,cs-410,5,4, Web Search,"00:08:08,968","00:08:13,277",119,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=488,"In general, you don't have to"
cs-410_5_4_120,cs-410,5,4, Web Search,"00:08:13,277","00:08:14,960",120,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=493,It's not necessary.
cs-410_5_4_121,cs-410,5,4, Web Search,"00:08:16,650","00:08:21,563",121,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=496,"So in this case, your goal is to"
cs-410_5_4_122,cs-410,5,4, Web Search,"00:08:21,563","00:08:26,400",122,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=501,by using minimum resources
cs-410_5_4_123,cs-410,5,4, Web Search,"00:08:27,490","00:08:33,986",123,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=507,"So, this is actually a very"
cs-410_5_4_124,cs-410,5,4, Web Search,"00:08:33,986","00:08:40,250",124,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=513,"and this is a open research question,"
cs-410_5_4_125,cs-410,5,4, Web Search,"00:08:40,250","00:08:46,060",125,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=520,standard algorithms established yet
cs-410_5_4_126,cs-410,5,4, Web Search,"00:08:47,300","00:08:51,300",126,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=527,"But in general, you can imagine,"
cs-410_5_4_127,cs-410,5,4, Web Search,"00:08:53,640","00:08:57,040",127,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=533,So the two major factors that
cs-410_5_4_128,cs-410,5,4, Web Search,"00:08:57,040","00:09:00,760",128,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=537,first will this page
cs-410_5_4_129,cs-410,5,4, Web Search,"00:09:00,760","00:09:03,411",129,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=540,And do I have to quote this page again?
cs-410_5_4_130,cs-410,5,4, Web Search,"00:09:03,411","00:09:07,726",130,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=543,If the page is a static page and
cs-410_5_4_131,cs-410,5,4, Web Search,"00:09:07,726","00:09:12,703",131,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=547,you probably don't have to re-crawl it
cs-410_5_4_132,cs-410,5,4, Web Search,"00:09:12,703","00:09:14,401",132,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=552,will changed frequently.
cs-410_5_4_133,cs-410,5,4, Web Search,"00:09:14,401","00:09:20,152",133,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=554,"On the other hand, if it's a sports score"
cs-410_5_4_134,cs-410,5,4, Web Search,"00:09:20,152","00:09:25,840",134,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=560,you may need to re-crawl it and
cs-410_5_4_135,cs-410,5,4, Web Search,"00:09:25,840","00:09:30,956",135,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=565,"The other factor to consider is,"
cs-410_5_4_136,cs-410,5,4, Web Search,"00:09:30,956","00:09:35,485",136,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=570,"If it is, then it means that"
cs-410_5_4_137,cs-410,5,4, Web Search,"00:09:35,485","00:09:40,809",137,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=575,then thus it's more important to
cs-410_5_4_138,cs-410,5,4, Web Search,"00:09:40,809","00:09:45,439",138,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=580,Compared with another page that has
cs-410_5_4_139,cs-410,5,4, Web Search,"00:09:45,439","00:09:49,609",139,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=585,"a year, then even though that"
cs-410_5_4_140,cs-410,5,4, Web Search,"00:09:49,609","00:09:55,164",140,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=589,It's probably not that necessary to
cs-410_5_4_141,cs-410,5,4, Web Search,"00:09:55,164","00:10:01,697",141,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=595,not as urgent as to maintain the freshness
cs-410_5_4_142,cs-410,5,4, Web Search,"00:10:01,697","00:10:05,275",142,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=601,"So to summarize, web search is one of"
cs-410_5_4_143,cs-410,5,4, Web Search,"00:10:05,275","00:10:08,689",143,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=605,retrieval and there are some new
cs-410_5_4_144,cs-410,5,4, Web Search,"00:10:08,689","00:10:10,463",144,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=608,"efficiency, quality information."
cs-410_5_4_145,cs-410,5,4, Web Search,"00:10:10,463","00:10:15,671",145,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=610,There are also new opportunities
cs-410_5_4_146,cs-410,5,4, Web Search,"00:10:15,671","00:10:16,765",146,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=615,"layout, etc."
cs-410_5_4_147,cs-410,5,4, Web Search,"00:10:17,890","00:10:22,500",147,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=617,A crawler is an essential component
cs-410_5_4_148,cs-410,5,4, Web Search,"00:10:22,500","00:10:24,360",148,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=622,"in general, you can find two scenarios."
cs-410_5_4_149,cs-410,5,4, Web Search,"00:10:24,360","00:10:28,730",149,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=624,One is initial crawling and
cs-410_5_4_150,cs-410,5,4, Web Search,"00:10:30,100","00:10:32,970",150,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=630,of the web if you are doing
cs-410_5_4_151,cs-410,5,4, Web Search,"00:10:32,970","00:10:37,560",151,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=632,focused crawling if you want to just
cs-410_5_4_152,cs-410,5,4, Web Search,"00:10:38,610","00:10:43,262",152,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=638,"And then, there is another scenario that's"
cs-410_5_4_153,cs-410,5,4, Web Search,"00:10:43,262","00:10:44,611",153,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=643,incremental crawling.
cs-410_5_4_154,cs-410,5,4, Web Search,"00:10:44,611","00:10:48,692",154,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=644,"In this case,"
cs-410_5_4_155,cs-410,5,4, Web Search,"00:10:48,692","00:10:52,588",155,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=648,try to use minimum resource
cs-410_5_4_156,cs-410,5,4, Web Search,"00:10:54,486","00:11:04,486",156,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=654,[MUSIC]
cs-410_5_5_1,cs-410,5,5, Web Indexing,"00:00:00,000","00:00:03,894",1,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=0,[SOUND]
cs-410_5_5_2,cs-410,5,5, Web Indexing,"00:00:07,481","00:00:09,920",2,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=7,This lecture is about the Web Indexing.
cs-410_5_5_3,cs-410,5,5, Web Indexing,"00:00:11,980","00:00:16,740",3,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=11,"In this lecture, we will continue"
cs-410_5_5_4,cs-410,5,5, Web Indexing,"00:00:16,740","00:00:20,741",4,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=16,we're going to talk about how
cs-410_5_5_5,cs-410,5,5, Web Indexing,"00:00:24,457","00:00:29,720",5,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=24,"So once we crawl the web,"
cs-410_5_5_6,cs-410,5,5, Web Indexing,"00:00:29,720","00:00:33,489",6,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=29,The next step is to use the indexer
cs-410_5_5_7,cs-410,5,5, Web Indexing,"00:00:36,540","00:00:41,150",7,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=36,"In general, we can use the same"
cs-410_5_5_8,cs-410,5,5, Web Indexing,"00:00:41,150","00:00:45,060",8,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=41,creating an index and that is what we
cs-410_5_5_9,cs-410,5,5, Web Indexing,"00:00:45,060","00:00:48,718",9,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=45,but there are there are new
cs-410_5_5_10,cs-410,5,5, Web Indexing,"00:00:48,718","00:00:55,100",10,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=48,"For web scale indexing, and the two main"
cs-410_5_5_11,cs-410,5,5, Web Indexing,"00:00:55,100","00:00:57,450",11,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=55,"The index would be so large,"
cs-410_5_5_12,cs-410,5,5, Web Indexing,"00:00:57,450","00:01:03,220",12,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=57,that it cannot actually fit into
cs-410_5_5_13,cs-410,5,5, Web Indexing,"00:01:03,220","00:01:05,879",13,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=63,So we have to store the data
cs-410_5_5_14,cs-410,5,5, Web Indexing,"00:01:06,910","00:01:10,900",14,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=66,"Also, because the data is so"
cs-410_5_5_15,cs-410,5,5, Web Indexing,"00:01:10,900","00:01:15,700",15,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=70,"process the data in parallel, so"
cs-410_5_5_16,cs-410,5,5, Web Indexing,"00:01:15,700","00:01:20,410",16,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=75,"Now to address these challenges,"
cs-410_5_5_17,cs-410,5,5, Web Indexing,"00:01:20,410","00:01:25,430",17,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=80,One is the Google File System that's
cs-410_5_5_18,cs-410,5,5, Web Indexing,"00:01:25,430","00:01:30,900",18,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=85,programmers manage files stored
cs-410_5_5_19,cs-410,5,5, Web Indexing,"00:01:32,000","00:01:33,159",19,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=92,The second is MapReduce.
cs-410_5_5_20,cs-410,5,5, Web Indexing,"00:01:33,159","00:01:37,140",20,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=93,This is a general software framework for
cs-410_5_5_21,cs-410,5,5, Web Indexing,"00:01:38,960","00:01:44,830",21,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=98,Hadoop is the most well known open
cs-410_5_5_22,cs-410,5,5, Web Indexing,"00:01:44,830","00:01:47,790",22,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=104,Now used in many applications.
cs-410_5_5_23,cs-410,5,5, Web Indexing,"00:01:50,000","00:01:52,510",23,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=110,"So, this is the architecture"
cs-410_5_5_24,cs-410,5,5, Web Indexing,"00:01:53,790","00:01:56,930",24,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=113,It uses a very simple centralized
cs-410_5_5_25,cs-410,5,5, Web Indexing,"00:01:56,930","00:02:01,210",25,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=116,management mechanism to manage
cs-410_5_5_26,cs-410,5,5, Web Indexing,"00:02:01,210","00:02:05,590",26,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=121,"Files, so"
cs-410_5_5_27,cs-410,5,5, Web Indexing,"00:02:05,590","00:02:09,790",27,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=125,look up a table to know where
cs-410_5_5_28,cs-410,5,5, Web Indexing,"00:02:11,040","00:02:16,250",28,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=131,The application client will then
cs-410_5_5_29,cs-410,5,5, Web Indexing,"00:02:16,250","00:02:21,420",29,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=136,that obtains specific locations of
cs-410_5_5_30,cs-410,5,5, Web Indexing,"00:02:22,890","00:02:31,450",30,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=142,And once the GFS file kind obtained
cs-410_5_5_31,cs-410,5,5, Web Indexing,"00:02:31,450","00:02:37,880",31,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=151,then the application client can talk
cs-410_5_5_32,cs-410,5,5, Web Indexing,"00:02:37,880","00:02:43,230",32,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=157,"data actually sits directly, so"
cs-410_5_5_33,cs-410,5,5, Web Indexing,"00:02:43,230","00:02:43,970",33,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=163,In the network.
cs-410_5_5_34,cs-410,5,5, Web Indexing,"00:02:46,020","00:02:53,290",34,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=166,So when this file system stores
cs-410_5_5_35,cs-410,5,5, Web Indexing,"00:02:53,290","00:02:59,650",35,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=173,"with great fixed sizes of chunks, so"
cs-410_5_5_36,cs-410,5,5, Web Indexing,"00:03:00,720","00:03:01,460",36,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=180,Many chunks.
cs-410_5_5_37,cs-410,5,5, Web Indexing,"00:03:01,460","00:03:05,120",37,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=181,"Each chunk is 64 MB, so it's pretty big."
cs-410_5_5_38,cs-410,5,5, Web Indexing,"00:03:05,120","00:03:09,080",38,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=185,And that's appropriate for
cs-410_5_5_39,cs-410,5,5, Web Indexing,"00:03:09,080","00:03:12,510",39,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=189,These chunks are replicated
cs-410_5_5_40,cs-410,5,5, Web Indexing,"00:03:12,510","00:03:17,210",40,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=192,So this is something that the programmer
cs-410_5_5_41,cs-410,5,5, Web Indexing,"00:03:17,210","00:03:22,210",41,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=197,and it's all taken care
cs-410_5_5_42,cs-410,5,5, Web Indexing,"00:03:22,210","00:03:24,110",42,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=202,"So from the application perspective,"
cs-410_5_5_43,cs-410,5,5, Web Indexing,"00:03:24,110","00:03:28,250",43,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=204,the programmer would see this
cs-410_5_5_44,cs-410,5,5, Web Indexing,"00:03:28,250","00:03:32,510",44,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=208,And the programmer doesn't have to
cs-410_5_5_45,cs-410,5,5, Web Indexing,"00:03:32,510","00:03:35,535",45,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=212,can just invoke high level.
cs-410_5_5_46,cs-410,5,5, Web Indexing,"00:03:35,535","00:03:38,275",46,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=215,Operators to process the file.
cs-410_5_5_47,cs-410,5,5, Web Indexing,"00:03:39,975","00:03:44,915",47,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=219,And another feature is that the data
cs-410_5_5_48,cs-410,5,5, Web Indexing,"00:03:44,915","00:03:45,865",48,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=224,and chunk servers.
cs-410_5_5_49,cs-410,5,5, Web Indexing,"00:03:45,865","00:03:48,735",49,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=225,So it's efficient in this sense.
cs-410_5_5_50,cs-410,5,5, Web Indexing,"00:03:51,190","00:03:54,590",50,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=231,"On top of the Google file system, Google"
cs-410_5_5_51,cs-410,5,5, Web Indexing,"00:03:54,590","00:03:59,220",51,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=234,also proposed MapReduce as a general
cs-410_5_5_52,cs-410,5,5, Web Indexing,"00:03:59,220","00:04:05,660",52,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=239,"Now, this is very useful to support"
cs-410_5_5_53,cs-410,5,5, Web Indexing,"00:04:06,670","00:04:10,618",53,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=246,"And so, this framework is,"
cs-410_5_5_54,cs-410,5,5, Web Indexing,"00:04:12,116","00:04:16,170",54,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=252,Hiding a lot of low-level
cs-410_5_5_55,cs-410,5,5, Web Indexing,"00:04:16,170","00:04:21,950",55,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=256,"As a result, the programmer can make"
cs-410_5_5_56,cs-410,5,5, Web Indexing,"00:04:21,950","00:04:26,580",56,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=261,that can be run a large
cs-410_5_5_57,cs-410,5,5, Web Indexing,"00:04:28,990","00:04:33,930",57,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=268,So some of the low level details
cs-410_5_5_58,cs-410,5,5, Web Indexing,"00:04:33,930","00:04:39,410",58,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=273,the specific and network communications or
cs-410_5_5_59,cs-410,5,5, Web Indexing,"00:04:39,410","00:04:44,080",59,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=279,where the task are executed.
cs-410_5_5_60,cs-410,5,5, Web Indexing,"00:04:44,080","00:04:46,600",60,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=284,All these details are hidden
cs-410_5_5_61,cs-410,5,5, Web Indexing,"00:04:47,880","00:04:52,560",61,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=287,There is also a nice feature which
cs-410_5_5_62,cs-410,5,5, Web Indexing,"00:04:52,560","00:04:56,490",62,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=292,"If one server is broken,"
cs-410_5_5_63,cs-410,5,5, Web Indexing,"00:04:56,490","00:05:01,140",63,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=296,"the server is down, and"
cs-410_5_5_64,cs-410,5,5, Web Indexing,"00:05:01,140","00:05:05,300",64,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=301,Then the MapReduce mapper will know
cs-410_5_5_65,cs-410,5,5, Web Indexing,"00:05:05,300","00:05:11,600",65,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=305,So it automatically dispatches a task
cs-410_5_5_66,cs-410,5,5, Web Indexing,"00:05:11,600","00:05:15,400",66,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=311,"And therefore, again the program"
cs-410_5_5_67,cs-410,5,5, Web Indexing,"00:05:15,400","00:05:17,570",67,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=315,here's how MapReduce works.
cs-410_5_5_68,cs-410,5,5, Web Indexing,"00:05:17,570","00:05:23,330",68,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=317,The input data would be separated
cs-410_5_5_69,cs-410,5,5, Web Indexing,"00:05:23,330","00:05:26,460",69,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=323,Now what exactly is in the value
cs-410_5_5_70,cs-410,5,5, Web Indexing,"00:05:26,460","00:05:31,520",70,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=326,it's actually a fairly general framework
cs-410_5_5_71,cs-410,5,5, Web Indexing,"00:05:31,520","00:05:35,450",71,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=331,into different parts and each part
cs-410_5_5_72,cs-410,5,5, Web Indexing,"00:05:37,100","00:05:40,984",72,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=337,Each key value pair would be and
cs-410_5_5_73,cs-410,5,5, Web Indexing,"00:05:40,984","00:05:44,750",73,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=340,"The program was right the map function,"
cs-410_5_5_74,cs-410,5,5, Web Indexing,"00:05:45,890","00:05:50,043",74,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=345,And then the map function will
cs-410_5_5_75,cs-410,5,5, Web Indexing,"00:05:50,043","00:05:53,870",75,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=350,then generate a number of
cs-410_5_5_76,cs-410,5,5, Web Indexing,"00:05:53,870","00:05:58,370",76,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=353,"Of course, the new key is usually"
cs-410_5_5_77,cs-410,5,5, Web Indexing,"00:05:58,370","00:06:02,070",77,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=358,that's given to the map as input.
cs-410_5_5_78,cs-410,5,5, Web Indexing,"00:06:02,070","00:06:06,000",78,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=362,And these key value pairs
cs-410_5_5_79,cs-410,5,5, Web Indexing,"00:06:06,000","00:06:10,420",79,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=366,all the outputs of all the map
cs-410_5_5_80,cs-410,5,5, Web Indexing,"00:06:12,540","00:06:16,670",80,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=372,and then there will be for
cs-410_5_5_81,cs-410,5,5, Web Indexing,"00:06:16,670","00:06:20,600",81,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=376,"And the result is that,"
cs-410_5_5_82,cs-410,5,5, Web Indexing,"00:06:20,600","00:06:24,260",82,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=380,with the same key will be
cs-410_5_5_83,cs-410,5,5, Web Indexing,"00:06:24,260","00:06:30,480",83,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=384,So now we've got a pair of of a key and
cs-410_5_5_84,cs-410,5,5, Web Indexing,"00:06:31,630","00:06:34,960",84,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=391,So this would then be sent
cs-410_5_5_85,cs-410,5,5, Web Indexing,"00:06:36,330","00:06:41,330",85,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=396,"Now, of course, each reduce function"
cs-410_5_5_86,cs-410,5,5, Web Indexing,"00:06:41,330","00:06:45,990",86,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=401,so we will send these output values to
cs-410_5_5_87,cs-410,5,5, Web Indexing,"00:06:45,990","00:06:50,580",87,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=405,multiple reduce functions
cs-410_5_5_88,cs-410,5,5, Web Indexing,"00:06:52,220","00:06:57,980",88,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=412,A reduce function would then
cs-410_5_5_89,cs-410,5,5, Web Indexing,"00:06:57,980","00:07:04,920",89,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=417,a key in a set of values to produce
cs-410_5_5_90,cs-410,5,5, Web Indexing,"00:07:04,920","00:07:08,670",90,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=424,So these output values would
cs-410_5_5_91,cs-410,5,5, Web Indexing,"00:07:08,670","00:07:11,220",91,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=428,to form the final output.
cs-410_5_5_92,cs-410,5,5, Web Indexing,"00:07:12,420","00:07:17,210",92,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=432,"And so, this is the general"
cs-410_5_5_93,cs-410,5,5, Web Indexing,"00:07:17,210","00:07:23,290",93,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=437,Now the programmer only needs to write
cs-410_5_5_94,cs-410,5,5, Web Indexing,"00:07:23,290","00:07:28,090",94,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=443,Everything else is actually taken
cs-410_5_5_95,cs-410,5,5, Web Indexing,"00:07:28,090","00:07:32,920",95,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=448,So you can see the program really
cs-410_5_5_96,cs-410,5,5, Web Indexing,"00:07:32,920","00:07:38,570",96,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=452,"And with such a framework, the input data"
cs-410_5_5_97,cs-410,5,5, Web Indexing,"00:07:38,570","00:07:42,780",97,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=458,"which is processing parallel first by map,"
cs-410_5_5_98,cs-410,5,5, Web Indexing,"00:07:42,780","00:07:50,130",98,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=462,then being the process after
cs-410_5_5_99,cs-410,5,5, Web Indexing,"00:07:50,130","00:07:54,340",99,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=470,The much more reduced if I'm
cs-410_5_5_100,cs-410,5,5, Web Indexing,"00:07:55,720","00:08:00,390",100,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=475,the different keys and
cs-410_5_5_101,cs-410,5,5, Web Indexing,"00:08:00,390","00:08:02,980",101,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=480,"So it achieves some,"
cs-410_5_5_102,cs-410,5,5, Web Indexing,"00:08:05,410","00:08:10,510",102,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=485,it achieves the purpose of parallel
cs-410_5_5_103,cs-410,5,5, Web Indexing,"00:08:10,510","00:08:13,620",103,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=490,So let's take a look at a simple example.
cs-410_5_5_104,cs-410,5,5, Web Indexing,"00:08:13,620","00:08:15,040",104,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=493,And that's Word Counting.
cs-410_5_5_105,cs-410,5,5, Web Indexing,"00:08:16,620","00:08:21,570",105,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=496,"The input is containing words,"
cs-410_5_5_106,cs-410,5,5, Web Indexing,"00:08:21,570","00:08:25,990",106,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=501,and the output that we want to generate is
cs-410_5_5_107,cs-410,5,5, Web Indexing,"00:08:25,990","00:08:27,080",107,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=505,So it's the Word Count.
cs-410_5_5_108,cs-410,5,5, Web Indexing,"00:08:28,270","00:08:32,940",108,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=508,We know this kind of counting
cs-410_5_5_109,cs-410,5,5, Web Indexing,"00:08:32,940","00:08:38,290",109,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=512,assess the popularity of a word in
cs-410_5_5_110,cs-410,5,5, Web Indexing,"00:08:38,290","00:08:41,880",110,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=518,achieving a factor of IDF wading for
cs-410_5_5_111,cs-410,5,5, Web Indexing,"00:08:42,880","00:08:44,200",111,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=522,So how can we solve this problem?
cs-410_5_5_112,cs-410,5,5, Web Indexing,"00:08:44,200","00:08:49,200",112,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=524,"Well, one natural thought is that,"
cs-410_5_5_113,cs-410,5,5, Web Indexing,"00:08:49,200","00:08:53,860",113,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=529,done in parallel by simply counting
cs-410_5_5_114,cs-410,5,5, Web Indexing,"00:08:53,860","00:08:57,000",114,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=533,and then in the end we just
cs-410_5_5_115,cs-410,5,5, Web Indexing,"00:08:57,000","00:09:01,800",115,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=537,And that's precisely the idea of
cs-410_5_5_116,cs-410,5,5, Web Indexing,"00:09:02,900","00:09:06,440",116,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=542,We can parallelize on
cs-410_5_5_117,cs-410,5,5, Web Indexing,"00:09:07,670","00:09:13,100",117,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=547,"So more specifically, we can assume"
cs-410_5_5_118,cs-410,5,5, Web Indexing,"00:09:14,120","00:09:20,450",118,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=554,a key value pair that represents the line
cs-410_5_5_119,cs-410,5,5, Web Indexing,"00:09:20,450","00:09:25,760",119,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=560,"So the first line, for"
cs-410_5_5_120,cs-410,5,5, Web Indexing,"00:09:25,760","00:09:32,240",120,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=565,that is another word by word and
cs-410_5_5_121,cs-410,5,5, Web Indexing,"00:09:32,240","00:09:36,250",121,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=572,So this key value pair would
cs-410_5_5_122,cs-410,5,5, Web Indexing,"00:09:36,250","00:09:40,670",122,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=576,The Map Function then would just
cs-410_5_5_123,cs-410,5,5, Web Indexing,"00:09:41,700","00:09:43,880",123,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=581,"And in this case,"
cs-410_5_5_124,cs-410,5,5, Web Indexing,"00:09:43,880","00:09:46,360",124,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=583,Each world gets a count of one and
cs-410_5_5_125,cs-410,5,5, Web Indexing,"00:09:46,360","00:09:52,770",125,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=586,these are the output that you see here
cs-410_5_5_126,cs-410,5,5, Web Indexing,"00:09:52,770","00:09:56,270",126,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=592,So the map function is really
cs-410_5_5_127,cs-410,5,5, Web Indexing,"00:09:56,270","00:10:00,450",127,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=596,what the pseudocode looks
cs-410_5_5_128,cs-410,5,5, Web Indexing,"00:10:00,450","00:10:05,370",128,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=600,you see it simply needs to iterate
cs-410_5_5_129,cs-410,5,5, Web Indexing,"00:10:05,370","00:10:08,330",129,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=605,And then just collect the function
cs-410_5_5_130,cs-410,5,5, Web Indexing,"00:10:09,390","00:10:14,080",130,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=609,which means it would then send the word
cs-410_5_5_131,cs-410,5,5, Web Indexing,"00:10:14,080","00:10:18,686",131,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=614,The collector would then try to
cs-410_5_5_132,cs-410,5,5, Web Indexing,"00:10:18,686","00:10:21,205",132,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=618,"different Map Functions, right?"
cs-410_5_5_133,cs-410,5,5, Web Indexing,"00:10:21,205","00:10:25,937",133,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=621,So the function is very simple and
cs-410_5_5_134,cs-410,5,5, Web Indexing,"00:10:25,937","00:10:30,300",134,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=625,this function as a way to
cs-410_5_5_135,cs-410,5,5, Web Indexing,"00:10:31,620","00:10:34,780",135,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=631,"Of course, the second line will be"
cs-410_5_5_136,cs-410,5,5, Web Indexing,"00:10:34,780","00:10:36,990",136,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=634,which we will produce a single output.
cs-410_5_5_137,cs-410,5,5, Web Indexing,"00:10:36,990","00:10:40,800",137,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=636,"Okay, now the output from the map"
cs-410_5_5_138,cs-410,5,5, Web Indexing,"00:10:40,800","00:10:45,550",138,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=640,send it to a collector and the collector
cs-410_5_5_139,cs-410,5,5, Web Indexing,"00:10:45,550","00:10:50,220",139,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=645,"So at this stage, you can see,"
cs-410_5_5_140,cs-410,5,5, Web Indexing,"00:10:50,220","00:10:53,850",140,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=650,Each pair is a word and
cs-410_5_5_141,cs-410,5,5, Web Indexing,"00:10:53,850","00:10:58,960",141,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=653,"So, once we see all these pairs."
cs-410_5_5_142,cs-410,5,5, Web Indexing,"00:10:58,960","00:11:03,570",142,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=658,"Then we can sort them based on the key,"
cs-410_5_5_143,cs-410,5,5, Web Indexing,"00:11:03,570","00:11:08,570",143,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=663,So we will collect all the counts
cs-410_5_5_144,cs-410,5,5, Web Indexing,"00:11:09,610","00:11:11,790",144,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=669,"And similarly, we do that for other words."
cs-410_5_5_145,cs-410,5,5, Web Indexing,"00:11:11,790","00:11:13,620",145,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=671,"Like Hadoop, Hello, etc."
cs-410_5_5_146,cs-410,5,5, Web Indexing,"00:11:13,620","00:11:19,040",146,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=673,So each word now is attached to
cs-410_5_5_147,cs-410,5,5, Web Indexing,"00:11:20,700","00:11:27,860",147,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=680,And these counts represent the occurrences
cs-410_5_5_148,cs-410,5,5, Web Indexing,"00:11:27,860","00:11:33,330",148,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=687,So now we have got a new pair of a key and
cs-410_5_5_149,cs-410,5,5, Web Indexing,"00:11:33,330","00:11:38,610",149,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=693,this pair will then be fed into reduce
cs-410_5_5_150,cs-410,5,5, Web Indexing,"00:11:38,610","00:11:44,450",150,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=698,would have to finish the job of counting
cs-410_5_5_151,cs-410,5,5, Web Indexing,"00:11:44,450","00:11:47,020",151,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=704,"Now, it has all ready got all"
cs-410_5_5_152,cs-410,5,5, Web Indexing,"00:11:47,020","00:11:50,370",152,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=707,all it needs to do is
cs-410_5_5_153,cs-410,5,5, Web Indexing,"00:11:50,370","00:11:53,810",153,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=710,So the reduce function here
cs-410_5_5_154,cs-410,5,5, Web Indexing,"00:11:53,810","00:11:57,130",154,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=713,"You have a counter, and"
cs-410_5_5_155,cs-410,5,5, Web Indexing,"00:11:57,130","00:11:59,260",155,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=717,That you'll see in this array.
cs-410_5_5_156,cs-410,5,5, Web Indexing,"00:11:59,260","00:12:02,884",156,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=719,"And that,"
cs-410_5_5_157,cs-410,5,5, Web Indexing,"00:12:02,884","00:12:07,203",157,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=722,"And then finally, you output the P and"
cs-410_5_5_158,cs-410,5,5, Web Indexing,"00:12:07,203","00:12:11,140",158,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=727,And that's precisely what we want as
cs-410_5_5_159,cs-410,5,5, Web Indexing,"00:12:12,220","00:12:14,830",159,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=732,"So you can see,"
cs-410_5_5_160,cs-410,5,5, Web Indexing,"00:12:14,830","00:12:16,842",160,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=734,To building an Invert index.
cs-410_5_5_161,cs-410,5,5, Web Indexing,"00:12:16,842","00:12:21,050",161,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=736,"And if you think about it,"
cs-410_5_5_162,cs-410,5,5, Web Indexing,"00:12:21,050","00:12:24,410",162,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=741,"And we have already got a dictionary,"
cs-410_5_5_163,cs-410,5,5, Web Indexing,"00:12:24,410","00:12:26,440",163,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=744,We have got the count.
cs-410_5_5_164,cs-410,5,5, Web Indexing,"00:12:26,440","00:12:32,776",164,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=746,But what's missing is
cs-410_5_5_165,cs-410,5,5, Web Indexing,"00:12:32,776","00:12:38,240",165,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=752,frequency counts of words
cs-410_5_5_166,cs-410,5,5, Web Indexing,"00:12:38,240","00:12:43,420",166,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=758,So we can modify this slightly to
cs-410_5_5_167,cs-410,5,5, Web Indexing,"00:12:43,420","00:12:45,800",167,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=763,here's one way to do that.
cs-410_5_5_168,cs-410,5,5, Web Indexing,"00:12:45,800","00:12:51,490",168,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=765,"So in this case, we can assume the input"
cs-410_5_5_169,cs-410,5,5, Web Indexing,"00:12:51,490","00:12:56,510",169,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=771,"which denotes the document ID,"
cs-410_5_5_170,cs-410,5,5, Web Indexing,"00:12:56,510","00:13:02,420",170,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=776,"denoting the screen for that document,"
cs-410_5_5_171,cs-410,5,5, Web Indexing,"00:13:02,420","00:13:05,740",171,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=782,"And so, the map function would do"
cs-410_5_5_172,cs-410,5,5, Web Indexing,"00:13:05,740","00:13:07,910",172,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=785,seen in the word campaign example.
cs-410_5_5_173,cs-410,5,5, Web Indexing,"00:13:07,910","00:13:14,640",173,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=787,It simply groups all the counts of
cs-410_5_5_174,cs-410,5,5, Web Indexing,"00:13:14,640","00:13:18,010",174,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=794,And it would then generate
cs-410_5_5_175,cs-410,5,5, Web Indexing,"00:13:18,010","00:13:21,140",175,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=798,"Each key is a word, and"
cs-410_5_5_176,cs-410,5,5, Web Indexing,"00:13:21,140","00:13:27,650",176,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=801,the value is the count of this word in
cs-410_5_5_177,cs-410,5,5, Web Indexing,"00:13:27,650","00:13:32,640",177,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=807,"Now, you can easily see why we need to"
cs-410_5_5_178,cs-410,5,5, Web Indexing,"00:13:32,640","00:13:36,690",178,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=812,"in inverted index, we would like to"
cs-410_5_5_179,cs-410,5,5, Web Indexing,"00:13:36,690","00:13:41,290",179,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=816,"should keep track of it, and this can then"
cs-410_5_5_180,cs-410,5,5, Web Indexing,"00:13:41,290","00:13:46,710",180,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=821,Now similarly another document D2
cs-410_5_5_181,cs-410,5,5, Web Indexing,"00:13:46,710","00:13:50,890",181,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=826,"So in the end, again, there is a sorting"
cs-410_5_5_182,cs-410,5,5, Web Indexing,"00:13:50,890","00:13:55,690",182,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=830,"And then we will have just a key,"
cs-410_5_5_183,cs-410,5,5, Web Indexing,"00:13:55,690","00:14:00,340",183,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=835,associated with all the documents
cs-410_5_5_184,cs-410,5,5, Web Indexing,"00:14:00,340","00:14:02,870",184,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=840,Or all the documents where java occurred.
cs-410_5_5_185,cs-410,5,5, Web Indexing,"00:14:04,500","00:14:09,520",185,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=844,"And the counts, so"
cs-410_5_5_186,cs-410,5,5, Web Indexing,"00:14:09,520","00:14:11,880",186,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=849,And this will be collected together.
cs-410_5_5_187,cs-410,5,5, Web Indexing,"00:14:11,880","00:14:15,840",187,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=851,"And this will be, so"
cs-410_5_5_188,cs-410,5,5, Web Indexing,"00:14:15,840","00:14:20,010",188,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=855,So now you can see the reduce function
cs-410_5_5_189,cs-410,5,5, Web Indexing,"00:14:20,010","00:14:21,800",189,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=860,an inverted index entry.
cs-410_5_5_190,cs-410,5,5, Web Indexing,"00:14:21,800","00:14:27,280",190,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=861,So it's just the word and all
cs-410_5_5_191,cs-410,5,5, Web Indexing,"00:14:27,280","00:14:30,900",191,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=867,the frequencies of the word
cs-410_5_5_192,cs-410,5,5, Web Indexing,"00:14:30,900","00:14:35,240",192,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=870,So all you need to do is
cs-410_5_5_193,cs-410,5,5, Web Indexing,"00:14:37,670","00:14:40,380",193,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=877,into a continuous chunk of data.
cs-410_5_5_194,cs-410,5,5, Web Indexing,"00:14:40,380","00:14:43,650",194,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=880,And this can be done
cs-410_5_5_195,cs-410,5,5, Web Indexing,"00:14:43,650","00:14:47,520",195,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=883,So basically the reduce function
cs-410_5_5_196,cs-410,5,5, Web Indexing,"00:14:47,520","00:14:48,020",196,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=887,Work.
cs-410_5_5_197,cs-410,5,5, Web Indexing,"00:14:49,450","00:14:53,647",197,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=889,"And so, this is a pseudo-code for"
cs-410_5_5_198,cs-410,5,5, Web Indexing,"00:14:53,647","00:14:58,010",198,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=893,[INAUDIBLE] that's construction.
cs-410_5_5_199,cs-410,5,5, Web Indexing,"00:14:58,010","00:15:05,290",199,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=898,"Here we see two functions,"
cs-410_5_5_200,cs-410,5,5, Web Indexing,"00:15:05,290","00:15:13,440",200,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=905,And a programmer would specify these two
cs-410_5_5_201,cs-410,5,5, Web Indexing,"00:15:13,440","00:15:18,990",201,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=913,And you can see basically they
cs-410_5_5_202,cs-410,5,5, Web Indexing,"00:15:18,990","00:15:22,870",202,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=918,"In the case of map, it's going to count"
cs-410_5_5_203,cs-410,5,5, Web Indexing,"00:15:22,870","00:15:27,040",203,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=922,the occurrences of a word
cs-410_5_5_204,cs-410,5,5, Web Indexing,"00:15:27,040","00:15:34,232",204,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=927,And it would output all the counts
cs-410_5_5_205,cs-410,5,5, Web Indexing,"00:15:34,232","00:15:40,350",205,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=934,"So, this is the reduce function,"
cs-410_5_5_206,cs-410,5,5, Web Indexing,"00:15:40,350","00:15:47,380",206,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=940,simply concatenates all the input
cs-410_5_5_207,cs-410,5,5, Web Indexing,"00:15:47,380","00:15:53,580",207,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=947,and then put them together as
cs-410_5_5_208,cs-410,5,5, Web Indexing,"00:15:53,580","00:15:58,250",208,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=953,So this is a very simple
cs-410_5_5_209,cs-410,5,5, Web Indexing,"00:15:58,250","00:16:03,360",209,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=958,it would allow us to construct an inverted
cs-410_5_5_210,cs-410,5,5, Web Indexing,"00:16:03,360","00:16:06,950",210,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=963,the data can be processed
cs-410_5_5_211,cs-410,5,5, Web Indexing,"00:16:06,950","00:16:11,000",211,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=966,And program doesn't have to
cs-410_5_5_212,cs-410,5,5, Web Indexing,"00:16:12,080","00:16:18,930",212,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=972,So this is how we can do parallel
cs-410_5_5_213,cs-410,5,5, Web Indexing,"00:16:20,040","00:16:21,960",213,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=980,"So to summarize,"
cs-410_5_5_214,cs-410,5,5, Web Indexing,"00:16:21,960","00:16:26,040",214,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=981,web scale indexing requires some
cs-410_5_5_215,cs-410,5,5, Web Indexing,"00:16:26,040","00:16:29,230",215,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=986,Standard traditional indexing techniques.
cs-410_5_5_216,cs-410,5,5, Web Indexing,"00:16:29,230","00:16:32,800",216,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=989,"Mainly, we have to store"
cs-410_5_5_217,cs-410,5,5, Web Indexing,"00:16:32,800","00:16:37,990",217,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=992,And this is usually done by using a filing
cs-410_5_5_218,cs-410,5,5, Web Indexing,"00:16:37,990","00:16:40,240",218,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=997,But this should be through a file system.
cs-410_5_5_219,cs-410,5,5, Web Indexing,"00:16:40,240","00:16:45,320",219,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1000,"And secondly, it requires creating"
cs-410_5_5_220,cs-410,5,5, Web Indexing,"00:16:45,320","00:16:50,340",220,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1005,large and takes long time to create
cs-410_5_5_221,cs-410,5,5, Web Indexing,"00:16:50,340","00:16:53,790",221,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1010,"So if we can do it in parallel,"
cs-410_5_5_222,cs-410,5,5, Web Indexing,"00:16:53,790","00:16:56,690",222,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1013,this is done by using
cs-410_5_5_223,cs-410,5,5, Web Indexing,"00:16:57,850","00:17:02,182",223,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1017,Note that both the GFS and
cs-410_5_5_224,cs-410,5,5, Web Indexing,"00:17:02,182","00:17:05,251",224,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1022,they can also support
cs-410_5_5_225,cs-410,5,5, Web Indexing,"00:17:07,795","00:17:17,795",225,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1027,[MUSIC]
cs-410_5_6_1,cs-410,5,6, Link Analysis - Part 1,"00:00:00,025","00:00:06,042",1,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=0,[SOUND] This lecture is about
cs-410_5_6_2,cs-410,5,6, Link Analysis - Part 1,"00:00:06,042","00:00:12,849",2,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=6,link analysis for web search.
cs-410_5_6_3,cs-410,5,6, Link Analysis - Part 1,"00:00:12,849","00:00:18,236",3,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=12,"In this lecture, we're going to talk"
cs-410_5_6_4,cs-410,5,6, Link Analysis - Part 1,"00:00:18,236","00:00:23,310",4,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=18,focusing on how to do link analysis and
cs-410_5_6_5,cs-410,5,6, Link Analysis - Part 1,"00:00:23,310","00:00:31,220",5,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=23,The main topic of this lecture is to look
cs-410_5_6_6,cs-410,5,6, Link Analysis - Part 1,"00:00:32,420","00:00:35,660",6,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=32,In the previous lecture we talked
cs-410_5_6_7,cs-410,5,6, Link Analysis - Part 1,"00:00:35,660","00:00:42,992",7,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=35,"Now that we have index, we want to see"
cs-410_5_6_8,cs-410,5,6, Link Analysis - Part 1,"00:00:42,992","00:00:44,900",8,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=42,The web.
cs-410_5_6_9,cs-410,5,6, Link Analysis - Part 1,"00:00:44,900","00:00:48,320",9,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=44,"Now standard IR models,"
cs-410_5_6_10,cs-410,5,6, Link Analysis - Part 1,"00:00:48,320","00:00:51,410",10,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=48,"In fact,"
cs-410_5_6_11,cs-410,5,6, Link Analysis - Part 1,"00:00:51,410","00:00:54,390",11,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=51,"improve, for supporting web search."
cs-410_5_6_12,cs-410,5,6, Link Analysis - Part 1,"00:00:54,390","00:00:56,380",12,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=54,But they aren't sufficient.
cs-410_5_6_13,cs-410,5,6, Link Analysis - Part 1,"00:00:56,380","00:00:58,550",13,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=56,And mainly for the following reasons.
cs-410_5_6_14,cs-410,5,6, Link Analysis - Part 1,"00:00:58,550","00:01:02,630",14,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=58,"First, on the web, we tend to have"
cs-410_5_6_15,cs-410,5,6, Link Analysis - Part 1,"00:01:02,630","00:01:07,150",15,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=62,"example, people might search for"
cs-410_5_6_16,cs-410,5,6, Link Analysis - Part 1,"00:01:07,150","00:01:11,230",16,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=67,And this is different from
cs-410_5_6_17,cs-410,5,6, Link Analysis - Part 1,"00:01:11,230","00:01:15,870",17,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=71,where people are primarily interested
cs-410_5_6_18,cs-410,5,6, Link Analysis - Part 1,"00:01:15,870","00:01:19,070",18,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=75,So this kind of query is often
cs-410_5_6_19,cs-410,5,6, Link Analysis - Part 1,"00:01:19,070","00:01:23,250",19,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=79,The purpose is to navigate into
cs-410_5_6_20,cs-410,5,6, Link Analysis - Part 1,"00:01:23,250","00:01:28,255",20,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=83,So for such queries we might benefit
cs-410_5_6_21,cs-410,5,6, Link Analysis - Part 1,"00:01:28,255","00:01:33,020",21,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=88,"Secondly, documents have additional"
cs-410_5_6_22,cs-410,5,6, Link Analysis - Part 1,"00:01:33,020","00:01:37,220",22,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=93,"are web format,"
cs-410_5_6_23,cs-410,5,6, Link Analysis - Part 1,"00:01:37,220","00:01:40,538",23,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=97,"such as the layout, the title,"
cs-410_5_6_24,cs-410,5,6, Link Analysis - Part 1,"00:01:40,538","00:01:45,340",24,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=100,So this has provided opportunity to use
cs-410_5_6_25,cs-410,5,6, Link Analysis - Part 1,"00:01:45,340","00:01:49,800",25,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=105,extra context information of
cs-410_5_6_26,cs-410,5,6, Link Analysis - Part 1,"00:01:49,800","00:01:52,440",26,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=109,"And finally,"
cs-410_5_6_27,cs-410,5,6, Link Analysis - Part 1,"00:01:52,440","00:01:56,570",27,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=112,That means we have to consider
cs-410_5_6_28,cs-410,5,6, Link Analysis - Part 1,"00:01:56,570","00:01:58,400",28,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=116,the range in the algorithm.
cs-410_5_6_29,cs-410,5,6, Link Analysis - Part 1,"00:01:58,400","00:02:03,540",29,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=118,This would give us a more robust way
cs-410_5_6_30,cs-410,5,6, Link Analysis - Part 1,"00:02:03,540","00:02:09,350",30,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=123,any spammer to just manipulate the one
cs-410_5_6_31,cs-410,5,6, Link Analysis - Part 1,"00:02:10,500","00:02:11,370",31,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=130,"So as a result,"
cs-410_5_6_32,cs-410,5,6, Link Analysis - Part 1,"00:02:11,370","00:02:15,500",32,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=131,people have made a number of major
cs-410_5_6_33,cs-410,5,6, Link Analysis - Part 1,"00:02:16,830","00:02:21,380",33,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=136,One line is to exploit
cs-410_5_6_34,cs-410,5,6, Link Analysis - Part 1,"00:02:23,020","00:02:24,790",34,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=143,And that's the main topic of this lecture.
cs-410_5_6_35,cs-410,5,6, Link Analysis - Part 1,"00:02:26,380","00:02:31,260",35,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=146,People have also proposed algorithms to
cs-410_5_6_36,cs-410,5,6, Link Analysis - Part 1,"00:02:31,260","00:02:35,370",36,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=151,Feedback information the form of
cs-410_5_6_37,cs-410,5,6, Link Analysis - Part 1,"00:02:35,370","00:02:40,720",37,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=155,in the category of feedback techniques and
cs-410_5_6_38,cs-410,5,6, Link Analysis - Part 1,"00:02:40,720","00:02:45,009",38,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=160,In general in web search the ranking
cs-410_5_6_39,cs-410,5,6, Link Analysis - Part 1,"00:02:45,009","00:02:49,750",39,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=165,algorithms to combine
cs-410_5_6_40,cs-410,5,6, Link Analysis - Part 1,"00:02:49,750","00:02:55,509",40,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=169,Many of them are based on
cs-410_5_6_41,cs-410,5,6, Link Analysis - Part 1,"00:02:55,509","00:03:03,341",41,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=175,as BM25 that we talked about [INAUDIBLE]
cs-410_5_6_42,cs-410,5,6, Link Analysis - Part 1,"00:03:03,341","00:03:09,217",42,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=183,to provide additional features
cs-410_5_6_43,cs-410,5,6, Link Analysis - Part 1,"00:03:09,217","00:03:13,364",43,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=189,but link information
cs-410_5_6_44,cs-410,5,6, Link Analysis - Part 1,"00:03:13,364","00:03:17,660",44,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=193,they provide additional scoring signals.
cs-410_5_6_45,cs-410,5,6, Link Analysis - Part 1,"00:03:17,660","00:03:21,080",45,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=197,So let's look at links in
cs-410_5_6_46,cs-410,5,6, Link Analysis - Part 1,"00:03:21,080","00:03:26,450",46,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=201,So this is a snapshot of some
cs-410_5_6_47,cs-410,5,6, Link Analysis - Part 1,"00:03:26,450","00:03:30,790",47,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=206,So we can see there are many links that
cs-410_5_6_48,cs-410,5,6, Link Analysis - Part 1,"00:03:30,790","00:03:35,730",48,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=210,"And in this case, you can also"
cs-410_5_6_49,cs-410,5,6, Link Analysis - Part 1,"00:03:35,730","00:03:40,400",49,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=215,a description of a link that's pointing
cs-410_5_6_50,cs-410,5,6, Link Analysis - Part 1,"00:03:40,400","00:03:42,850",50,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=220,"Now, this description text"
cs-410_5_6_51,cs-410,5,6, Link Analysis - Part 1,"00:03:44,460","00:03:48,920",51,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=224,"Now if you think about this text,"
cs-410_5_6_52,cs-410,5,6, Link Analysis - Part 1,"00:03:48,920","00:03:53,865",52,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=228,because it provides some extra
cs-410_5_6_53,cs-410,5,6, Link Analysis - Part 1,"00:03:53,865","00:03:59,685",53,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=233,"So for example, if someone wants"
cs-410_5_6_54,cs-410,5,6, Link Analysis - Part 1,"00:03:59,685","00:04:04,555",54,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=239,the person might say the biggest
cs-410_5_6_55,cs-410,5,6, Link Analysis - Part 1,"00:04:04,555","00:04:07,695",55,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=244,"then the link to Amazon, right?"
cs-410_5_6_56,cs-410,5,6, Link Analysis - Part 1,"00:04:07,695","00:04:11,855",56,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=247,"So, the description here after is very"
cs-410_5_6_57,cs-410,5,6, Link Analysis - Part 1,"00:04:11,855","00:04:14,350",57,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=251,the query box when they are looking for
cs-410_5_6_58,cs-410,5,6, Link Analysis - Part 1,"00:04:14,350","00:04:19,950",58,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=254,And that's why it's very useful for
cs-410_5_6_59,cs-410,5,6, Link Analysis - Part 1,"00:04:19,950","00:04:25,058",59,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=259,Suppose someone types in
cs-410_5_6_60,cs-410,5,6, Link Analysis - Part 1,"00:04:25,058","00:04:27,517",60,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=265,biggest online bookstore.
cs-410_5_6_61,cs-410,5,6, Link Analysis - Part 1,"00:04:27,517","00:04:35,980",61,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=267,All right the query would match
cs-410_5_6_62,cs-410,5,6, Link Analysis - Part 1,"00:04:35,980","00:04:39,650",62,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=275,And then this actually
cs-410_5_6_63,cs-410,5,6, Link Analysis - Part 1,"00:04:39,650","00:04:44,090",63,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=279,matching the page that's being
cs-410_5_6_64,cs-410,5,6, Link Analysis - Part 1,"00:04:44,090","00:04:45,650",64,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=284,a entry page.
cs-410_5_6_65,cs-410,5,6, Link Analysis - Part 1,"00:04:45,650","00:04:50,120",65,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=285,So if you match anchor text that
cs-410_5_6_66,cs-410,5,6, Link Analysis - Part 1,"00:04:50,120","00:04:58,080",66,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=290,actually that provides good evidence for
cs-410_5_6_67,cs-410,5,6, Link Analysis - Part 1,"00:04:58,080","00:05:00,480",67,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=298,So anchor text is very useful.
cs-410_5_6_68,cs-410,5,6, Link Analysis - Part 1,"00:05:00,480","00:05:03,970",68,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=300,If you look at the bottom part of this
cs-410_5_6_69,cs-410,5,6, Link Analysis - Part 1,"00:05:03,970","00:05:08,380",69,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=303,patterns of some links and these links
cs-410_5_6_70,cs-410,5,6, Link Analysis - Part 1,"00:05:08,380","00:05:09,230",70,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=308,"So for example,"
cs-410_5_6_71,cs-410,5,6, Link Analysis - Part 1,"00:05:09,230","00:05:14,200",71,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=309,on the right side you'll see this
cs-410_5_6_72,cs-410,5,6, Link Analysis - Part 1,"00:05:14,200","00:05:17,180",72,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=314,Now that means many other pages
cs-410_5_6_73,cs-410,5,6, Link Analysis - Part 1,"00:05:17,180","00:05:20,270",73,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=317,This shows that this page is quite useful.
cs-410_5_6_74,cs-410,5,6, Link Analysis - Part 1,"00:05:21,370","00:05:24,710",74,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=321,On the left side you can see this
cs-410_5_6_75,cs-410,5,6, Link Analysis - Part 1,"00:05:24,710","00:05:25,920",75,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=324,many other pages.
cs-410_5_6_76,cs-410,5,6, Link Analysis - Part 1,"00:05:25,920","00:05:29,040",76,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=325,So this is a director page
cs-410_5_6_77,cs-410,5,6, Link Analysis - Part 1,"00:05:29,040","00:05:31,260",77,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=329,actually see a lot of other pages.
cs-410_5_6_78,cs-410,5,6, Link Analysis - Part 1,"00:05:32,670","00:05:35,990",78,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=332,So we can call the first
cs-410_5_6_79,cs-410,5,6, Link Analysis - Part 1,"00:05:35,990","00:05:41,250",79,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=335,"the second case half page, but this means"
cs-410_5_6_80,cs-410,5,6, Link Analysis - Part 1,"00:05:41,250","00:05:44,080",80,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=341,One is to provide extra text for matching.
cs-410_5_6_81,cs-410,5,6, Link Analysis - Part 1,"00:05:44,080","00:05:49,750",81,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=344,The other is to provide some
cs-410_5_6_82,cs-410,5,6, Link Analysis - Part 1,"00:05:49,750","00:05:53,980",82,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=349,to characterize how likely a page is
cs-410_5_6_83,cs-410,5,6, Link Analysis - Part 1,"00:05:55,820","00:06:02,530",83,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=355,So people then of course and proposed
cs-410_5_6_84,cs-410,5,6, Link Analysis - Part 1,"00:06:02,530","00:06:08,030",84,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=362,Google's PageRank which was the main
cs-410_5_6_85,cs-410,5,6, Link Analysis - Part 1,"00:06:08,030","00:06:13,360",85,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=368,is a good example and
cs-410_5_6_86,cs-410,5,6, Link Analysis - Part 1,"00:06:13,360","00:06:17,070",86,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=373,"popularity, basically to score authority."
cs-410_5_6_87,cs-410,5,6, Link Analysis - Part 1,"00:06:17,070","00:06:21,640",87,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=377,So the intuitions here are links
cs-410_5_6_88,cs-410,5,6, Link Analysis - Part 1,"00:06:21,640","00:06:24,030",88,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=381,Now think about one page
cs-410_5_6_89,cs-410,5,6, Link Analysis - Part 1,"00:06:24,030","00:06:27,440",89,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=384,this is very similar to one
cs-410_5_6_90,cs-410,5,6, Link Analysis - Part 1,"00:06:27,440","00:06:30,360",90,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=387,"So, of course then,"
cs-410_5_6_91,cs-410,5,6, Link Analysis - Part 1,"00:06:30,360","00:06:33,920",91,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=390,then we can assume this page
cs-410_5_6_92,cs-410,5,6, Link Analysis - Part 1,"00:06:35,120","00:06:36,570",92,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=395,So that's a very good intuition.
cs-410_5_6_93,cs-410,5,6, Link Analysis - Part 1,"00:06:38,060","00:06:42,950",93,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=398,Now PageRank is essentially to take
cs-410_5_6_94,cs-410,5,6, Link Analysis - Part 1,"00:06:42,950","00:06:46,650",94,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=402,implement with the principal approach.
cs-410_5_6_95,cs-410,5,6, Link Analysis - Part 1,"00:06:46,650","00:06:51,980",95,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=406,"Intuitively, it is essentially doing"
cs-410_5_6_96,cs-410,5,6, Link Analysis - Part 1,"00:06:51,980","00:06:56,390",96,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=411,It just improves the simple
cs-410_5_6_97,cs-410,5,6, Link Analysis - Part 1,"00:06:56,390","00:06:59,420",97,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=416,One it will consider indirect citations.
cs-410_5_6_98,cs-410,5,6, Link Analysis - Part 1,"00:06:59,420","00:07:04,010",98,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=419,So that means you don't just look
cs-410_5_6_99,cs-410,5,6, Link Analysis - Part 1,"00:07:04,010","00:07:08,550",99,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=424,You also look at what are those
cs-410_5_6_100,cs-410,5,6, Link Analysis - Part 1,"00:07:08,550","00:07:13,530",100,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=428,If those pages themselves have a lot
cs-410_5_6_101,cs-410,5,6, Link Analysis - Part 1,"00:07:13,530","00:07:16,750",101,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=433,"In some sense,"
cs-410_5_6_102,cs-410,5,6, Link Analysis - Part 1,"00:07:16,750","00:07:20,080",102,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=436,But if those pages that
cs-410_5_6_103,cs-410,5,6, Link Analysis - Part 1,"00:07:20,080","00:07:25,095",103,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=440,being pointed to by other pages they
cs-410_5_6_104,cs-410,5,6, Link Analysis - Part 1,"00:07:25,095","00:07:27,360",104,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=445,"then well, you don't get that much."
cs-410_5_6_105,cs-410,5,6, Link Analysis - Part 1,"00:07:27,360","00:07:29,830",105,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=447,So that's the idea of
cs-410_5_6_106,cs-410,5,6, Link Analysis - Part 1,"00:07:29,830","00:07:31,770",106,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=449,"All right, so"
cs-410_5_6_107,cs-410,5,6, Link Analysis - Part 1,"00:07:31,770","00:07:37,060",107,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=451,you can also understand this idea by
cs-410_5_6_108,cs-410,5,6, Link Analysis - Part 1,"00:07:37,060","00:07:42,082",108,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=457,"If you're cited by let's say ten papers,"
cs-410_5_6_109,cs-410,5,6, Link Analysis - Part 1,"00:07:42,082","00:07:48,450",109,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=462,are just workshop papers or some papers
cs-410_5_6_110,cs-410,5,6, Link Analysis - Part 1,"00:07:49,580","00:07:54,340",110,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=469,"So although you've got ten in-links,"
cs-410_5_6_111,cs-410,5,6, Link Analysis - Part 1,"00:07:54,340","00:07:59,910",111,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=474,are cited by ten papers that themselves
cs-410_5_6_112,cs-410,5,6, Link Analysis - Part 1,"00:08:01,770","00:08:06,563",112,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=481,And so in this case where we would
cs-410_5_6_113,cs-410,5,6, Link Analysis - Part 1,"00:08:06,563","00:08:08,530",113,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=486,page does that.
cs-410_5_6_114,cs-410,5,6, Link Analysis - Part 1,"00:08:08,530","00:08:12,174",114,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=488,The other idea is it's
cs-410_5_6_115,cs-410,5,6, Link Analysis - Part 1,"00:08:13,810","00:08:21,120",115,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=493,Assume that basically every page is having
cs-410_5_6_116,cs-410,5,6, Link Analysis - Part 1,"00:08:21,120","00:08:23,950",116,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=501,Essentially you are trying to
cs-410_5_6_117,cs-410,5,6, Link Analysis - Part 1,"00:08:23,950","00:08:27,510",117,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=503,links that will link all
cs-410_5_6_118,cs-410,5,6, Link Analysis - Part 1,"00:08:27,510","00:08:32,740",118,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=507,that you actually get the pseudo
cs-410_5_6_119,cs-410,5,6, Link Analysis - Part 1,"00:08:34,300","00:08:36,760",119,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=514,The reason why they want to do that.
cs-410_5_6_120,cs-410,5,6, Link Analysis - Part 1,"00:08:36,760","00:08:41,980",120,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=516,Is this will allow them
cs-410_5_6_121,cs-410,5,6, Link Analysis - Part 1,"00:08:41,980","00:08:47,348",121,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=521,elegantly with linear algebra technique.
cs-410_5_6_122,cs-410,5,6, Link Analysis - Part 1,"00:08:47,348","00:08:52,354",122,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=527,"So, I think maybe the best"
cs-410_5_6_123,cs-410,5,6, Link Analysis - Part 1,"00:08:52,354","00:08:58,172",123,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=532,the PageRank is to think
cs-410_5_6_124,cs-410,5,6, Link Analysis - Part 1,"00:08:58,172","00:09:04,549",124,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=538,probability of random surfer
cs-410_5_6_125,cs-410,5,6, Link Analysis - Part 1,"00:09:04,549","00:09:14,549",125,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=544,[MUSIC]
cs-410_5_7_1,cs-410,5,7, Link Analysis - Part 2,"00:00:00,049","00:00:03,810",1,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=0,[MUSIC]
cs-410_5_7_2,cs-410,5,7, Link Analysis - Part 2,"00:00:09,183","00:00:12,025",2,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=9,So let's take a look at this in detail.
cs-410_5_7_3,cs-410,5,7, Link Analysis - Part 2,"00:00:12,025","00:00:17,269",3,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=12,So in this random surfing
cs-410_5_7_4,cs-410,5,7, Link Analysis - Part 2,"00:00:17,269","00:00:22,575",4,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=17,random surfer would choose
cs-410_5_7_5,cs-410,5,7, Link Analysis - Part 2,"00:00:22,575","00:00:25,225",5,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=22,So this is a small graph here.
cs-410_5_7_6,cs-410,5,7, Link Analysis - Part 2,"00:00:25,225","00:00:29,490",6,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=25,"That's of course, over simplification"
cs-410_5_7_7,cs-410,5,7, Link Analysis - Part 2,"00:00:29,490","00:00:35,207",7,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=29,But let's say there are four
cs-410_5_7_8,cs-410,5,7, Link Analysis - Part 2,"00:00:35,207","00:00:41,360",8,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=35,And let's assume that a random surfer or
cs-410_5_7_9,cs-410,5,7, Link Analysis - Part 2,"00:00:41,360","00:00:46,373",9,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=41,And then the random
cs-410_5_7_10,cs-410,5,7, Link Analysis - Part 2,"00:00:46,373","00:00:50,439",10,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=46,just randomly jumping to any page or
cs-410_5_7_11,cs-410,5,7, Link Analysis - Part 2,"00:00:50,439","00:00:55,330",11,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=50,follow a link and
cs-410_5_7_12,cs-410,5,7, Link Analysis - Part 2,"00:00:56,440","00:00:59,650",12,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=56,"So if the random surfer is at d1,"
cs-410_5_7_13,cs-410,5,7, Link Analysis - Part 2,"00:01:01,100","00:01:06,260",13,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=61,then there is some probability that
cs-410_5_7_14,cs-410,5,7, Link Analysis - Part 2,"00:01:06,260","00:01:09,510",14,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=66,"Now there are two outlinks here,"
cs-410_5_7_15,cs-410,5,7, Link Analysis - Part 2,"00:01:09,510","00:01:12,740",15,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=69,the other is pointing to d4.
cs-410_5_7_16,cs-410,5,7, Link Analysis - Part 2,"00:01:12,740","00:01:19,020",16,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=72,So the random surfer could pick any
cs-410_5_7_17,cs-410,5,7, Link Analysis - Part 2,"00:01:19,020","00:01:25,800",17,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=79,But it also assumes that the random so
cs-410_5_7_18,cs-410,5,7, Link Analysis - Part 2,"00:01:25,800","00:01:30,586",18,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=85,So the random surfing which decide
cs-410_5_7_19,cs-410,5,7, Link Analysis - Part 2,"00:01:30,586","00:01:34,050",19,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=90,simply randomly jump
cs-410_5_7_20,cs-410,5,7, Link Analysis - Part 2,"00:01:34,050","00:01:39,760",20,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=94,"So if it does that, it would be able"
cs-410_5_7_21,cs-410,5,7, Link Analysis - Part 2,"00:01:39,760","00:01:45,090",21,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=99,"though there's no link you actually,"
cs-410_5_7_22,cs-410,5,7, Link Analysis - Part 2,"00:01:46,170","00:01:49,713",22,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=106,So this is to assume that
cs-410_5_7_23,cs-410,5,7, Link Analysis - Part 2,"00:01:49,713","00:01:54,852",23,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=109,Imagine a random surfer is
cs-410_5_7_24,cs-410,5,7, Link Analysis - Part 2,"00:01:54,852","00:01:59,989",24,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=114,then we can ask the question how
cs-410_5_7_25,cs-410,5,7, Link Analysis - Part 2,"00:01:59,989","00:02:05,864",25,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=119,would actually reach a particular
cs-410_5_7_26,cs-410,5,7, Link Analysis - Part 2,"00:02:05,864","00:02:09,824",26,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=125,That's the average probability of
cs-410_5_7_27,cs-410,5,7, Link Analysis - Part 2,"00:02:09,824","00:02:13,830",27,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=129,this probability is precisely
cs-410_5_7_28,cs-410,5,7, Link Analysis - Part 2,"00:02:13,830","00:02:17,558",28,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=133,So the page rank score of
cs-410_5_7_29,cs-410,5,7, Link Analysis - Part 2,"00:02:17,558","00:02:21,644",29,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=137,probability that the surfer
cs-410_5_7_30,cs-410,5,7, Link Analysis - Part 2,"00:02:21,644","00:02:27,220",30,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=141,"Now intuitively, this would basically"
cs-410_5_7_31,cs-410,5,7, Link Analysis - Part 2,"00:02:27,220","00:02:30,970",31,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=147,"Because if a page has a lot of inlinks,"
cs-410_5_7_32,cs-410,5,7, Link Analysis - Part 2,"00:02:30,970","00:02:34,580",32,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=150,then it would have a higher
cs-410_5_7_33,cs-410,5,7, Link Analysis - Part 2,"00:02:34,580","00:02:37,650",33,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=154,Because there will be more
cs-410_5_7_34,cs-410,5,7, Link Analysis - Part 2,"00:02:37,650","00:02:39,940",34,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=157,follow a link to come to this page.
cs-410_5_7_35,cs-410,5,7, Link Analysis - Part 2,"00:02:41,290","00:02:45,030",35,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=161,And this is why the random surfing model
cs-410_5_7_36,cs-410,5,7, Link Analysis - Part 2,"00:02:45,030","00:02:48,510",36,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=165,actually captures the ID
cs-410_5_7_37,cs-410,5,7, Link Analysis - Part 2,"00:02:48,510","00:02:52,700",37,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=168,Note that it also considers
cs-410_5_7_38,cs-410,5,7, Link Analysis - Part 2,"00:02:52,700","00:02:59,690",38,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=172,Because if the page is that point then
cs-410_5_7_39,cs-410,5,7, Link Analysis - Part 2,"00:02:59,690","00:03:04,550",39,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=179,That would mean the random surfer would
cs-410_5_7_40,cs-410,5,7, Link Analysis - Part 2,"00:03:04,550","00:03:07,680",40,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=184,"therefore, it increase"
cs-410_5_7_41,cs-410,5,7, Link Analysis - Part 2,"00:03:07,680","00:03:13,580",41,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=187,So this is just a nice way to capture
cs-410_5_7_42,cs-410,5,7, Link Analysis - Part 2,"00:03:13,580","00:03:18,440",42,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=193,"So mathematically, how can we compute this"
cs-410_5_7_43,cs-410,5,7, Link Analysis - Part 2,"00:03:18,440","00:03:22,390",43,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=198,we need to take a look at how this
cs-410_5_7_44,cs-410,5,7, Link Analysis - Part 2,"00:03:22,390","00:03:25,184",44,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=202,So first of all let's take a look
cs-410_5_7_45,cs-410,5,7, Link Analysis - Part 2,"00:03:25,184","00:03:29,437",45,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=205,And this is just metrics with
cs-410_5_7_46,cs-410,5,7, Link Analysis - Part 2,"00:03:29,437","00:03:33,273",46,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=209,the random surfer would go
cs-410_5_7_47,cs-410,5,7, Link Analysis - Part 2,"00:03:33,273","00:03:37,230",47,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=213,So each rule stands for a starting page.
cs-410_5_7_48,cs-410,5,7, Link Analysis - Part 2,"00:03:37,230","00:03:41,754",48,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=217,"For example, rule one would"
cs-410_5_7_49,cs-410,5,7, Link Analysis - Part 2,"00:03:41,754","00:03:44,581",49,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=221,to any of the other four pages from d1.
cs-410_5_7_50,cs-410,5,7, Link Analysis - Part 2,"00:03:44,581","00:03:53,097",50,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=224,And here we see there are only
cs-410_5_7_51,cs-410,5,7, Link Analysis - Part 2,"00:03:53,097","00:04:01,492",51,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=233,So this is because if you look at
cs-410_5_7_52,cs-410,5,7, Link Analysis - Part 2,"00:04:01,492","00:04:05,918",52,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=241,There is no link from d1 or d2.
cs-410_5_7_53,cs-410,5,7, Link Analysis - Part 2,"00:04:05,918","00:04:10,579",53,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=245,So we've got 0s for the first 2
cs-410_5_7_54,cs-410,5,7, Link Analysis - Part 2,"00:04:10,579","00:04:15,762",54,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=250,columns and 0.5 for d3 and d4.
cs-410_5_7_55,cs-410,5,7, Link Analysis - Part 2,"00:04:15,762","00:04:19,416",55,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=255,"In general, the M in this matrix,"
cs-410_5_7_56,cs-410,5,7, Link Analysis - Part 2,"00:04:19,416","00:04:24,586",56,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=259,M sub ij is the probability
cs-410_5_7_57,cs-410,5,7, Link Analysis - Part 2,"00:04:24,586","00:04:29,668",57,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=264,"And obviously for each rule,"
cs-410_5_7_58,cs-410,5,7, Link Analysis - Part 2,"00:04:29,668","00:04:36,115",58,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=269,because the surfer would have to go to
cs-410_5_7_59,cs-410,5,7, Link Analysis - Part 2,"00:04:36,115","00:04:39,196",59,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=276,So this is a transition metric.
cs-410_5_7_60,cs-410,5,7, Link Analysis - Part 2,"00:04:39,196","00:04:43,690",60,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=279,Now how can we compute the probability
cs-410_5_7_61,cs-410,5,7, Link Analysis - Part 2,"00:04:44,900","00:04:49,900",61,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=284,Well if you look at the surf
cs-410_5_7_62,cs-410,5,7, Link Analysis - Part 2,"00:04:50,910","00:04:56,280",62,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=290,we can compute the probability
cs-410_5_7_63,cs-410,5,7, Link Analysis - Part 2,"00:04:56,280","00:05:01,140",63,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=296,"So here on the left hand side,"
cs-410_5_7_64,cs-410,5,7, Link Analysis - Part 2,"00:05:02,170","00:05:08,540",64,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=302,"visiting page dj at time plus 1,"
cs-410_5_7_65,cs-410,5,7, Link Analysis - Part 2,"00:05:08,540","00:05:14,740",65,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=308,"On the right hand side, you can see"
cs-410_5_7_66,cs-410,5,7, Link Analysis - Part 2,"00:05:14,740","00:05:20,000",66,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=314,of at page di at time t.
cs-410_5_7_67,cs-410,5,7, Link Analysis - Part 2,"00:05:21,408","00:05:26,020",67,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=321,So you can see the subscript
cs-410_5_7_68,cs-410,5,7, Link Analysis - Part 2,"00:05:26,020","00:05:34,314",68,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=326,that indicates that's the probability that
cs-410_5_7_69,cs-410,5,7, Link Analysis - Part 2,"00:05:34,314","00:05:38,500",69,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=334,"So the equation basically,"
cs-410_5_7_70,cs-410,5,7, Link Analysis - Part 2,"00:05:38,500","00:05:43,790",70,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=338,possibilities of reaching
cs-410_5_7_71,cs-410,5,7, Link Analysis - Part 2,"00:05:43,790","00:05:45,510",71,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=343,What are these two possibilities?
cs-410_5_7_72,cs-410,5,7, Link Analysis - Part 2,"00:05:45,510","00:05:48,200",72,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=345,Well one is through random surfing and
cs-410_5_7_73,cs-410,5,7, Link Analysis - Part 2,"00:05:48,200","00:05:51,930",73,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=348,"one is through following a link,"
cs-410_5_7_74,cs-410,5,7, Link Analysis - Part 2,"00:05:53,500","00:05:56,612",74,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=353,So the first part captures the probability
cs-410_5_7_75,cs-410,5,7, Link Analysis - Part 2,"00:05:56,612","00:06:01,373",75,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=356,that the random surfer would reach
cs-410_5_7_76,cs-410,5,7, Link Analysis - Part 2,"00:06:01,373","00:06:06,283",76,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=361,And you can see the random
cs-410_5_7_77,cs-410,5,7, Link Analysis - Part 2,"00:06:06,283","00:06:10,455",77,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=366,with probability 1 minus
cs-410_5_7_78,cs-410,5,7, Link Analysis - Part 2,"00:06:10,455","00:06:14,200",78,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=370,And so
cs-410_5_7_79,cs-410,5,7, Link Analysis - Part 2,"00:06:14,200","00:06:18,250",79,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=374,But the main party is realist
cs-410_5_7_80,cs-410,5,7, Link Analysis - Part 2,"00:06:18,250","00:06:22,060",80,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=378,that the surfer could have been at time t.
cs-410_5_7_81,cs-410,5,7, Link Analysis - Part 2,"00:06:23,760","00:06:27,890",81,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=383,There are n pages so
cs-410_5_7_82,cs-410,5,7, Link Analysis - Part 2,"00:06:27,890","00:06:31,730",82,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=387,Inside the sum is a product
cs-410_5_7_83,cs-410,5,7, Link Analysis - Part 2,"00:06:31,730","00:06:36,763",83,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=391,One is the probability that the surfer
cs-410_5_7_84,cs-410,5,7, Link Analysis - Part 2,"00:06:36,763","00:06:42,115",84,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=396,"was at di at time t, that's p sub t of di."
cs-410_5_7_85,cs-410,5,7, Link Analysis - Part 2,"00:06:42,115","00:06:47,422",85,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=402,The other is the transition
cs-410_5_7_86,cs-410,5,7, Link Analysis - Part 2,"00:06:47,422","00:06:52,217",86,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=407,"And so in order to reach this dj page,"
cs-410_5_7_87,cs-410,5,7, Link Analysis - Part 2,"00:06:52,217","00:06:57,880",87,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=412,the surfer must first be at di at time t.
cs-410_5_7_88,cs-410,5,7, Link Analysis - Part 2,"00:06:57,880","00:07:03,090",88,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=417,"And then also, would also have to"
cs-410_5_7_89,cs-410,5,7, Link Analysis - Part 2,"00:07:03,090","00:07:09,090",89,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=423,So the probability is the probability
cs-410_5_7_90,cs-410,5,7, Link Analysis - Part 2,"00:07:09,090","00:07:15,950",90,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=429,the probability of going from that
cs-410_5_7_91,cs-410,5,7, Link Analysis - Part 2,"00:07:15,950","00:07:20,792",91,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=435,"The second part is a similar sum, the only"
cs-410_5_7_92,cs-410,5,7, Link Analysis - Part 2,"00:07:20,792","00:07:23,980",92,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=440,probability is a uniform
cs-410_5_7_93,cs-410,5,7, Link Analysis - Part 2,"00:07:23,980","00:07:27,708",93,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=443,1 over n and
cs-410_5_7_94,cs-410,5,7, Link Analysis - Part 2,"00:07:27,708","00:07:31,110",94,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=447,of reaching this page
cs-410_5_7_95,cs-410,5,7, Link Analysis - Part 2,"00:07:32,630","00:07:37,520",95,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=452,So the form is exactly the same and
cs-410_5_7_96,cs-410,5,7, Link Analysis - Part 2,"00:07:37,520","00:07:43,310",96,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=457,see on why PageRank is essentially assumed
cs-410_5_7_97,cs-410,5,7, Link Analysis - Part 2,"00:07:43,310","00:07:49,070",97,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=463,If you think about this 1 over n as
cs-410_5_7_98,cs-410,5,7, Link Analysis - Part 2,"00:07:49,070","00:07:55,320",98,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=469,that has all the elements being
cs-410_5_7_99,cs-410,5,7, Link Analysis - Part 2,"00:07:55,320","00:07:59,621",99,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=475,Then you can see very clearly
cs-410_5_7_100,cs-410,5,7, Link Analysis - Part 2,"00:07:59,621","00:08:01,784",100,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=479,because they are of the same form.
cs-410_5_7_101,cs-410,5,7, Link Analysis - Part 2,"00:08:01,784","00:08:07,310",101,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=481,We can imagine there's a different
cs-410_5_7_102,cs-410,5,7, Link Analysis - Part 2,"00:08:07,310","00:08:11,340",102,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=487,that uniform metrics where
cs-410_5_7_103,cs-410,5,7, Link Analysis - Part 2,"00:08:11,340","00:08:16,347",103,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=491,And in this sense PageRank uses
cs-410_5_7_104,cs-410,5,7, Link Analysis - Part 2,"00:08:16,347","00:08:22,312",104,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=496,ensuring that there's no zero entry
cs-410_5_7_105,cs-410,5,7, Link Analysis - Part 2,"00:08:22,312","00:08:28,530",105,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=502,Now of course this is the time dependent
cs-410_5_7_106,cs-410,5,7, Link Analysis - Part 2,"00:08:28,530","00:08:32,480",106,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=508,"Now we can imagine, if we'll compute"
cs-410_5_7_107,cs-410,5,7, Link Analysis - Part 2,"00:08:32,480","00:08:36,420",107,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=512,the average of probabilities probably
cs-410_5_7_108,cs-410,5,7, Link Analysis - Part 2,"00:08:36,420","00:08:38,320",108,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=516,without considering the time index.
cs-410_5_7_109,cs-410,5,7, Link Analysis - Part 2,"00:08:38,320","00:08:41,780",109,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=518,So let's drop the time index and
cs-410_5_7_110,cs-410,5,7, Link Analysis - Part 2,"00:08:42,910","00:08:47,100",110,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=522,"Now this would give us any equations,"
cs-410_5_7_111,cs-410,5,7, Link Analysis - Part 2,"00:08:47,100","00:08:49,520",111,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=527,each page we have such equation.
cs-410_5_7_112,cs-410,5,7, Link Analysis - Part 2,"00:08:49,520","00:08:52,800",112,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=529,And if you look at the what
cs-410_5_7_113,cs-410,5,7, Link Analysis - Part 2,"00:08:52,800","00:08:55,170",113,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=532,there are also precisely n variables.
cs-410_5_7_114,cs-410,5,7, Link Analysis - Part 2,"00:08:58,280","00:09:03,220",114,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=538,"So this basically means,"
cs-410_5_7_115,cs-410,5,7, Link Analysis - Part 2,"00:09:04,600","00:09:10,260",115,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=544,n equations with n variables and
cs-410_5_7_116,cs-410,5,7, Link Analysis - Part 2,"00:09:10,260","00:09:16,420",116,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=550,"So basically, now the problem boils"
cs-410_5_7_117,cs-410,5,7, Link Analysis - Part 2,"00:09:16,420","00:09:20,950",117,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=556,"And here, I also show"
cs-410_5_7_118,cs-410,5,7, Link Analysis - Part 2,"00:09:20,950","00:09:26,690",118,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=560,It's the vector p here equals a matrix or
cs-410_5_7_119,cs-410,5,7, Link Analysis - Part 2,"00:09:26,690","00:09:31,390",119,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=566,the transpose of the matrix here and
cs-410_5_7_120,cs-410,5,7, Link Analysis - Part 2,"00:09:32,580","00:09:36,890",120,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=572,"Now, if you still remember some knowledge"
cs-410_5_7_121,cs-410,5,7, Link Analysis - Part 2,"00:09:36,890","00:09:42,140",121,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=576,"and then you will realize, this is"
cs-410_5_7_122,cs-410,5,7, Link Analysis - Part 2,"00:09:42,140","00:09:47,690",122,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=582,"When multiply the metrics by this vector,"
cs-410_5_7_123,cs-410,5,7, Link Analysis - Part 2,"00:09:47,690","00:09:52,280",123,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=587,this can be solved by
cs-410_5_7_124,cs-410,5,7, Link Analysis - Part 2,"00:09:54,700","00:09:57,380",124,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=594,So because the equations here
cs-410_5_7_125,cs-410,5,7, Link Analysis - Part 2,"00:09:57,380","00:10:02,002",125,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=597,on the back are basically
cs-410_5_7_126,cs-410,5,7, Link Analysis - Part 2,"00:10:02,002","00:10:09,170",126,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=602,So you'll see the relation between the
cs-410_5_7_127,cs-410,5,7, Link Analysis - Part 2,"00:10:09,170","00:10:13,844",127,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=609,And this iterative approach or
cs-410_5_7_128,cs-410,5,7, Link Analysis - Part 2,"00:10:13,844","00:10:19,093",128,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=613,we simply start with s
cs-410_5_7_129,cs-410,5,7, Link Analysis - Part 2,"00:10:19,093","00:10:24,242",129,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=619,And then we repeatedly
cs-410_5_7_130,cs-410,5,7, Link Analysis - Part 2,"00:10:24,242","00:10:29,970",130,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=624,multiplying the metrics
cs-410_5_7_131,cs-410,5,7, Link Analysis - Part 2,"00:10:31,360","00:10:37,820",131,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=631,I also show a concrete example here.
cs-410_5_7_132,cs-410,5,7, Link Analysis - Part 2,"00:10:37,820","00:10:40,130",132,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=637,So you can see this now.
cs-410_5_7_133,cs-410,5,7, Link Analysis - Part 2,"00:10:40,130","00:10:43,368",133,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=640,"If we assume alpha is 0.2,"
cs-410_5_7_134,cs-410,5,7, Link Analysis - Part 2,"00:10:43,368","00:10:49,066",134,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=643,then with the example that
cs-410_5_7_135,cs-410,5,7, Link Analysis - Part 2,"00:10:49,066","00:10:54,393",135,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=649,we have the original
cs-410_5_7_136,cs-410,5,7, Link Analysis - Part 2,"00:10:54,393","00:10:59,943",136,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=654,"That includes the graph, the actual links"
cs-410_5_7_137,cs-410,5,7, Link Analysis - Part 2,"00:10:59,943","00:11:04,856",137,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=659,"metrics, uniform transition metrics"
cs-410_5_7_138,cs-410,5,7, Link Analysis - Part 2,"00:11:04,856","00:11:09,707",138,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=664,And we can combine them together with
cs-410_5_7_139,cs-410,5,7, Link Analysis - Part 2,"00:11:09,707","00:11:12,260",139,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=669,metric that would be like this.
cs-410_5_7_140,cs-410,5,7, Link Analysis - Part 2,"00:11:12,260","00:11:13,320",140,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=672,"So essentially,"
cs-410_5_7_141,cs-410,5,7, Link Analysis - Part 2,"00:11:13,320","00:11:18,300",141,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=673,we can imagine now the web looks like
cs-410_5_7_142,cs-410,5,7, Link Analysis - Part 2,"00:11:18,300","00:11:22,300",142,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=678,They're all virtual links
cs-410_5_7_143,cs-410,5,7, Link Analysis - Part 2,"00:11:22,300","00:11:27,210",143,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=682,The page we're on now would just
cs-410_5_7_144,cs-410,5,7, Link Analysis - Part 2,"00:11:27,210","00:11:30,270",144,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=687,then just computed the updating of this
cs-410_5_7_145,cs-410,5,7, Link Analysis - Part 2,"00:11:30,270","00:11:34,899",145,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=690,p vector by using this
cs-410_5_7_146,cs-410,5,7, Link Analysis - Part 2,"00:11:36,600","00:11:40,640",146,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=696,Now if you rewrite this
cs-410_5_7_147,cs-410,5,7, Link Analysis - Part 2,"00:11:42,020","00:11:45,080",147,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=702,"terms of individual equations,"
cs-410_5_7_148,cs-410,5,7, Link Analysis - Part 2,"00:11:46,530","00:11:51,435",148,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=706,"And this is basically,"
cs-410_5_7_149,cs-410,5,7, Link Analysis - Part 2,"00:11:51,435","00:11:54,385",149,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=711,this particular pages and page score.
cs-410_5_7_150,cs-410,5,7, Link Analysis - Part 2,"00:11:54,385","00:11:59,364",150,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=714,So you can also see if you want to compute
cs-410_5_7_151,cs-410,5,7, Link Analysis - Part 2,"00:11:59,364","00:12:04,617",151,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=719,You basically multiply
cs-410_5_7_152,cs-410,5,7, Link Analysis - Part 2,"00:12:04,617","00:12:09,379",152,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=724,and we'll take the third
cs-410_5_7_153,cs-410,5,7, Link Analysis - Part 2,"00:12:09,379","00:12:13,950",153,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=729,And that will give us the value for
cs-410_5_7_154,cs-410,5,7, Link Analysis - Part 2,"00:12:16,000","00:12:20,170",154,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=736,So this is how we updated the vector
cs-410_5_7_155,cs-410,5,7, Link Analysis - Part 2,"00:12:20,170","00:12:23,270",155,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=740,these guys for this.
cs-410_5_7_156,cs-410,5,7, Link Analysis - Part 2,"00:12:23,270","00:12:28,080",156,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=743,And then we just revise
cs-410_5_7_157,cs-410,5,7, Link Analysis - Part 2,"00:12:28,080","00:12:31,550",157,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=748,set of scores and
cs-410_5_7_158,cs-410,5,7, Link Analysis - Part 2,"00:12:33,150","00:12:37,590",158,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=753,So we just repeatedly apply this and
cs-410_5_7_159,cs-410,5,7, Link Analysis - Part 2,"00:12:37,590","00:12:41,432",159,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=757,"And when the matrix is like this,"
cs-410_5_7_160,cs-410,5,7, Link Analysis - Part 2,"00:12:41,432","00:12:43,510",160,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=761,it can be guaranteed to converge.
cs-410_5_7_161,cs-410,5,7, Link Analysis - Part 2,"00:12:44,790","00:12:49,765",161,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=764,And at that point the we will just have
cs-410_5_7_162,cs-410,5,7, Link Analysis - Part 2,"00:12:49,765","00:12:53,101",162,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=769,We typically go to sets of
cs-410_5_7_163,cs-410,5,7, Link Analysis - Part 2,"00:12:55,300","00:12:58,543",163,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=775,"So interestingly,"
cs-410_5_7_164,cs-410,5,7, Link Analysis - Part 2,"00:12:58,543","00:13:03,296",164,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=778,also interpreted as propagating
cs-410_5_7_165,cs-410,5,7, Link Analysis - Part 2,"00:13:03,296","00:13:08,847",165,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=783,Or if you look at this formula and
cs-410_5_7_166,cs-410,5,7, Link Analysis - Part 2,"00:13:08,847","00:13:13,440",166,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=788,"can you imagine,"
cs-410_5_7_167,cs-410,5,7, Link Analysis - Part 2,"00:13:13,440","00:13:17,479",167,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=793,essentially propagating
cs-410_5_7_168,cs-410,5,7, Link Analysis - Part 2,"00:13:17,479","00:13:19,801",168,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=797,"I hope you will see that indeed,"
cs-410_5_7_169,cs-410,5,7, Link Analysis - Part 2,"00:13:19,801","00:13:24,565",169,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=799,we can imagine we have values
cs-410_5_7_170,cs-410,5,7, Link Analysis - Part 2,"00:13:24,565","00:13:30,220",170,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=804,So we can have values here and
cs-410_5_7_171,cs-410,5,7, Link Analysis - Part 2,"00:13:30,220","00:13:35,170",171,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=810,And then we're going to use these
cs-410_5_7_172,cs-410,5,7, Link Analysis - Part 2,"00:13:35,170","00:13:42,290",172,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=815,And if you look at the equation here
cs-410_5_7_173,cs-410,5,7, Link Analysis - Part 2,"00:13:42,290","00:13:48,890",173,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=822,to combine the scores of the pages that
cs-410_5_7_174,cs-410,5,7, Link Analysis - Part 2,"00:13:48,890","00:13:54,067",174,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=828,So we'll look at all the pages
cs-410_5_7_175,cs-410,5,7, Link Analysis - Part 2,"00:13:54,067","00:14:00,916",175,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=834,then combine this score and propagate the
cs-410_5_7_176,cs-410,5,7, Link Analysis - Part 2,"00:14:00,916","00:14:06,145",176,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=840,To look at the scores that we present
cs-410_5_7_177,cs-410,5,7, Link Analysis - Part 2,"00:14:06,145","00:14:11,410",177,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=846,surfer would be visiting the other
cs-410_5_7_178,cs-410,5,7, Link Analysis - Part 2,"00:14:11,410","00:14:16,275",178,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=851,And then just do
cs-410_5_7_179,cs-410,5,7, Link Analysis - Part 2,"00:14:16,275","00:14:21,409",179,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=856,"the probability of reaching this page, d1."
cs-410_5_7_180,cs-410,5,7, Link Analysis - Part 2,"00:14:21,409","00:14:23,910",180,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=861,So there are two interpretations here.
cs-410_5_7_181,cs-410,5,7, Link Analysis - Part 2,"00:14:23,910","00:14:26,364",181,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=863,One is just the matrix multiplication.
cs-410_5_7_182,cs-410,5,7, Link Analysis - Part 2,"00:14:26,364","00:14:31,498",182,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=866,We repeat the multiplying
cs-410_5_7_183,cs-410,5,7, Link Analysis - Part 2,"00:14:31,498","00:14:35,204",183,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=871,The other is to just think
cs-410_5_7_184,cs-410,5,7, Link Analysis - Part 2,"00:14:35,204","00:14:38,180",184,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=875,these scores repeatedly on the web.
cs-410_5_7_185,cs-410,5,7, Link Analysis - Part 2,"00:14:38,180","00:14:43,150",185,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=878,"So in practice, the combination of"
cs-410_5_7_186,cs-410,5,7, Link Analysis - Part 2,"00:14:43,150","00:14:48,820",186,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=883,Because the matrices is fast and there
cs-410_5_7_187,cs-410,5,7, Link Analysis - Part 2,"00:14:48,820","00:14:53,820",187,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=888,So that you avoid actually
cs-410_5_7_188,cs-410,5,7, Link Analysis - Part 2,"00:14:53,820","00:14:55,260",188,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=893,all those elements.
cs-410_5_7_189,cs-410,5,7, Link Analysis - Part 2,"00:14:56,670","00:15:00,670",189,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=896,Sometimes you may also normalize the
cs-410_5_7_190,cs-410,5,7, Link Analysis - Part 2,"00:15:00,670","00:15:05,249",190,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=900,"different form of the equation, but"
cs-410_5_7_191,cs-410,5,7, Link Analysis - Part 2,"00:15:06,290","00:15:09,650",191,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=906,The results of this potential
cs-410_5_7_192,cs-410,5,7, Link Analysis - Part 2,"00:15:10,740","00:15:17,540",192,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=910,"In that case, if a page does not have"
cs-410_5_7_193,cs-410,5,7, Link Analysis - Part 2,"00:15:17,540","00:15:22,250",193,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=917,these pages would not sum to 1.
cs-410_5_7_194,cs-410,5,7, Link Analysis - Part 2,"00:15:22,250","00:15:26,349",194,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=922,"Basically, the probability of reaching the"
cs-410_5_7_195,cs-410,5,7, Link Analysis - Part 2,"00:15:26,349","00:15:29,246",195,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=926,"1, mainly because we have lost"
cs-410_5_7_196,cs-410,5,7, Link Analysis - Part 2,"00:15:29,246","00:15:33,588",196,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=929,One would assume there's some probability
cs-410_5_7_197,cs-410,5,7, Link Analysis - Part 2,"00:15:33,588","00:15:37,160",197,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=933,"the links, but"
cs-410_5_7_198,cs-410,5,7, Link Analysis - Part 2,"00:15:37,160","00:15:42,980",198,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=937,And one possible solution is simply to use
cs-410_5_7_199,cs-410,5,7, Link Analysis - Part 2,"00:15:42,980","00:15:45,270",199,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=942,and that could easily fix this.
cs-410_5_7_200,cs-410,5,7, Link Analysis - Part 2,"00:15:46,740","00:15:50,750",200,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=946,"Basically, that's to say alpha would"
cs-410_5_7_201,cs-410,5,7, Link Analysis - Part 2,"00:15:50,750","00:15:54,130",201,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=950,"In that case,"
cs-410_5_7_202,cs-410,5,7, Link Analysis - Part 2,"00:15:54,130","00:15:57,330",202,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=954,randomly jump to another page
cs-410_5_7_203,cs-410,5,7, Link Analysis - Part 2,"00:15:59,120","00:16:05,060",203,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=959,"There are many extensions of PageRank, one"
cs-410_5_7_204,cs-410,5,7, Link Analysis - Part 2,"00:16:05,060","00:16:11,639",204,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=965,Note that PageRank doesn't merely
cs-410_5_7_205,cs-410,5,7, Link Analysis - Part 2,"00:16:11,639","00:16:15,370",205,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=971,So we can make PageRank specific however.
cs-410_5_7_206,cs-410,5,7, Link Analysis - Part 2,"00:16:15,370","00:16:19,260",206,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=975,"So for example,"
cs-410_5_7_207,cs-410,5,7, Link Analysis - Part 2,"00:16:19,260","00:16:22,567",207,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=979,we can simply assume
cs-410_5_7_208,cs-410,5,7, Link Analysis - Part 2,"00:16:22,567","00:16:25,660",208,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=982,The surfer is not randomly
cs-410_5_7_209,cs-410,5,7, Link Analysis - Part 2,"00:16:25,660","00:16:32,320",209,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=985,"Instead, he's going to jump to only those"
cs-410_5_7_210,cs-410,5,7, Link Analysis - Part 2,"00:16:32,320","00:16:36,780",210,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=992,"For example, if the query is not sports"
cs-410_5_7_211,cs-410,5,7, Link Analysis - Part 2,"00:16:36,780","00:16:40,670",211,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=996,"doing random jumping, it's going"
cs-410_5_7_212,cs-410,5,7, Link Analysis - Part 2,"00:16:40,670","00:16:45,350",212,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1000,"By doing this, then we can buy"
cs-410_5_7_213,cs-410,5,7, Link Analysis - Part 2,"00:16:45,350","00:16:49,054",213,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1005,And then if you know the current
cs-410_5_7_214,cs-410,5,7, Link Analysis - Part 2,"00:16:49,054","00:16:53,368",214,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1009,then you can use this specialized
cs-410_5_7_215,cs-410,5,7, Link Analysis - Part 2,"00:16:53,368","00:16:57,754",215,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1013,That would be better than if you
cs-410_5_7_216,cs-410,5,7, Link Analysis - Part 2,"00:16:57,754","00:17:01,877",216,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1017,PageRank is also a channel that can be
cs-410_5_7_217,cs-410,5,7, Link Analysis - Part 2,"00:17:01,877","00:17:06,100",217,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1021,network analysis particularly for
cs-410_5_7_218,cs-410,5,7, Link Analysis - Part 2,"00:17:06,100","00:17:09,970",218,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1026,You can imagine if you compute
cs-410_5_7_219,cs-410,5,7, Link Analysis - Part 2,"00:17:09,970","00:17:14,356",219,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1029,"social network, where a link"
cs-410_5_7_220,cs-410,5,7, Link Analysis - Part 2,"00:17:14,356","00:17:18,744",220,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1034,"a relation, you would get some"
cs-410_5_7_221,cs-410,5,7, Link Analysis - Part 2,"00:17:18,744","00:17:28,744",221,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1038,[MUSIC]
cs-410_5_8_1,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:00,000","00:00:06,073",1,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=0,[SOUND]
cs-410_5_8_2,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:06,073","00:00:13,035",2,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=6,we talked about PageRank as
cs-410_5_8_3,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:14,155","00:00:21,245",3,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=14,"Now, we also looked at some other examples"
cs-410_5_8_4,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:21,245","00:00:24,425",4,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=21,"So there is another algorithm called HITS,"
cs-410_5_8_5,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:24,425","00:00:28,257",5,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=24,that going to compute the scores for
cs-410_5_8_6,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:28,257","00:00:33,167",6,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=28,The intuitions are pages that are widely
cs-410_5_8_7,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:33,167","00:00:38,577",7,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=33,whereas pages that cite many
cs-410_5_8_8,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:38,577","00:00:42,650",8,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=38,I think that the most interesting
cs-410_5_8_9,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:42,650","00:00:46,930",9,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=42,is it's going to use
cs-410_5_8_10,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:46,930","00:00:51,470",10,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=46,to kind of help improve the scoring for
cs-410_5_8_11,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:51,470","00:00:53,318",11,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=51,"And so here's the idea,"
cs-410_5_8_12,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:53,318","00:00:57,809",12,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=53,it was assumed that good
cs-410_5_8_13,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:58,870","00:01:03,847",13,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=58,That means if you are cited by many
cs-410_5_8_14,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:03,847","00:01:07,266",14,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=63,"that inquiry says, you're an authority."
cs-410_5_8_15,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:07,266","00:01:11,740",15,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=67,"And similarly, good hubs are those"
cs-410_5_8_16,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:11,740","00:01:15,560",16,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=71,So if you pointed to a lot
cs-410_5_8_17,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:15,560","00:01:17,880",17,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=75,then your hubs score would be increased.
cs-410_5_8_18,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:17,880","00:01:22,115",18,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=77,So then you will have literally reinforced
cs-410_5_8_19,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:22,115","00:01:22,968",19,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=82,some good hubs.
cs-410_5_8_20,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:22,968","00:01:27,635",20,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=82,And so you have pointed to some good
cs-410_5_8_21,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:27,635","00:01:30,544",21,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=87,whereas those authority
cs-410_5_8_22,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:30,544","00:01:34,736",22,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=90,improved because they
cs-410_5_8_23,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:34,736","00:01:39,380",23,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=94,And this is algorithms is also general it
cs-410_5_8_24,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:39,380","00:01:40,730",24,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=99,network analysis.
cs-410_5_8_25,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:40,730","00:01:43,170",25,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=100,"So just briefly, here's how it works."
cs-410_5_8_26,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:43,170","00:01:47,090",26,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=103,"We first also construct a matrix, but this"
cs-410_5_8_27,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:47,090","00:01:49,750",27,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=107,matrix and
cs-410_5_8_28,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:49,750","00:01:54,100",28,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=109,"So if there's a link there's a 1,"
cs-410_5_8_29,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:54,100","00:01:56,185",29,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=114,"Again, it's the same graph."
cs-410_5_8_30,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:56,185","00:02:01,335",30,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=116,And then we're going to
cs-410_5_8_31,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:01,335","00:02:06,955",31,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=121,as the sum of the authority scores of
cs-410_5_8_32,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:08,270","00:02:09,620",32,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=128,"So whether you are hub,"
cs-410_5_8_33,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:09,620","00:02:14,430",33,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=129,really depends on whether you are pointing
cs-410_5_8_34,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:14,430","00:02:17,080",34,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=134,That's what it says in the first equation.
cs-410_5_8_35,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:17,080","00:02:22,130",35,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=137,"In the second equation,"
cs-410_5_8_36,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:22,130","00:02:27,350",36,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=142,as a sum of the hub scores of all
cs-410_5_8_37,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:27,350","00:02:30,260",37,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=147,So whether you are good authority
cs-410_5_8_38,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:30,260","00:02:33,420",38,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=150,pages that are pointing
cs-410_5_8_39,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:33,420","00:02:37,380",39,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=153,So you can see this forms
cs-410_5_8_40,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:38,770","00:02:44,586",40,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=158,"Now, these three questions can be"
cs-410_5_8_41,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:44,586","00:02:50,707",41,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=164,So what we get here is then the hub
cs-410_5_8_42,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:50,707","00:02:55,770",42,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=170,of the adjacency matrix and
cs-410_5_8_43,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:55,770","00:03:00,026",43,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=175,and this is basically the first equation.
cs-410_5_8_44,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:00,026","00:03:05,292",44,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=180,"And similarly, the second equation"
cs-410_5_8_45,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:05,292","00:03:11,034",45,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=185,vector is equal to the product of
cs-410_5_8_46,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:11,034","00:03:15,820",46,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=191,"Now, these are just different ways"
cs-410_5_8_47,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:15,820","00:03:19,967",47,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=195,But what's interesting is that
cs-410_5_8_48,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:19,967","00:03:26,680",48,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=199,you can also plug in the authority
cs-410_5_8_49,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:26,680","00:03:31,500",49,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=206,"So if you do that, you have actually"
cs-410_5_8_50,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:31,500","00:03:33,930",50,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=211,and you get the equations
cs-410_5_8_51,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:34,980","00:03:39,032",51,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=214,The hubs score vector is
cs-410_5_8_52,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:39,032","00:03:43,522",52,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=219,by a transpose multiplied
cs-410_5_8_53,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:43,522","00:03:47,511",53,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=223,"Similarly, we can do a transformation"
cs-410_5_8_54,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:47,511","00:03:49,440",54,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=227,just the authorities also.
cs-410_5_8_55,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:49,440","00:03:54,370",55,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=229,So although we frame the problem
cs-410_5_8_56,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:54,370","00:03:58,410",56,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=234,we can actually eliminate one of them to
cs-410_5_8_57,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:59,530","00:04:03,810",57,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=239,"Now, the difference between this and page"
cs-410_5_8_58,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:03,810","00:04:07,960",58,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=243,a multiplication of the adjacency
cs-410_5_8_59,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:07,960","00:04:09,840",59,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=247,So this is different from page rank.
cs-410_5_8_60,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:11,250","00:04:15,373",60,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=251,"But mathematically, then we will"
cs-410_5_8_61,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:15,373","00:04:19,777",61,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=255,"So in HITS,"
cs-410_5_8_62,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:19,777","00:04:22,215",62,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=259,"Let's say, 1 for all these values, and"
cs-410_5_8_63,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:22,215","00:04:26,580",63,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=262,then we would iteratively apply
cs-410_5_8_64,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:26,580","00:04:33,100",64,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=266,And this is equivalent to multiply
cs-410_5_8_65,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:34,720","00:04:37,740",65,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=274,So the arrows of these is exactly
cs-410_5_8_66,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:37,740","00:04:43,035",66,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=277,But here because the adjacency
cs-410_5_8_67,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:43,035","00:04:47,463",67,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=283,So what we have to do is after each
cs-410_5_8_68,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:47,463","00:04:50,980",68,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=287,this would allow us to
cs-410_5_8_69,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:50,980","00:04:53,671",69,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=290,Otherwise they would grow larger and
cs-410_5_8_70,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:53,671","00:04:57,180",70,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=293,"And if we do that, and"
cs-410_5_8_71,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:58,360","00:05:03,920",71,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=298,"That was the computer, the hubs scores,"
cs-410_5_8_72,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:03,920","00:05:08,647",72,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=303,And these scores can then be used in
cs-410_5_8_73,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:09,860","00:05:14,525",73,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=309,"So to summarize in this lecture, we have"
cs-410_5_8_74,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:14,525","00:05:19,302",74,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=314,"In particular,"
cs-410_5_8_75,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:19,302","00:05:23,737",75,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=319,increase the text
cs-410_5_8_76,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:23,737","00:05:25,959",76,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=323,And we also talk about the PageRank and
cs-410_5_8_77,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:25,959","00:05:29,380",77,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=325,page anchor as two major
cs-410_5_8_78,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:29,380","00:05:35,930",78,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=329,Both can generate scores for web pages
cs-410_5_8_79,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:35,930","00:05:39,600",79,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=335,Note that PageRank and
cs-410_5_8_80,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:39,600","00:05:46,663",80,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=339,So they have many applications in
cs-410_5_8_81,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:46,663","00:05:56,663",81,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=346,[MUSIC]
cs-410_6_1_1,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:00,000","00:00:02,695",1,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=0,[MUSIC]
cs-410_6_1_2,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:07,363","00:00:10,900",2,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=7,This lecture is about
cs-410_6_1_3,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:10,900","00:00:15,210",3,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=10,"In this lecture, we are going to"
cs-410_6_1_4,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:15,210","00:00:17,860",4,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=15,In particular we're going to talk
cs-410_6_1_5,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:17,860","00:00:21,339",5,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=17,to combine different features
cs-410_6_1_6,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:22,340","00:00:28,500",6,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=22,So the question that we address in
cs-410_6_1_7,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:28,500","00:00:36,230",7,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=28,many features to generate a single ranking
cs-410_6_1_8,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:36,230","00:00:42,140",8,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=36,In the previous lectures we have talked
cs-410_6_1_9,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:42,140","00:00:48,270",9,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=42,We have talked about some retrieval
cs-410_6_1_10,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:48,270","00:00:54,760",10,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=48,They can generate a based this course for
cs-410_6_1_11,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:54,760","00:00:58,130",11,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=54,And we also talked about the link
cs-410_6_1_12,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:59,230","00:01:02,759",12,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=59,that can give additional scores
cs-410_6_1_13,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:03,940","00:01:07,313",13,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=63,"Now the question now is,"
cs-410_6_1_14,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:07,313","00:01:09,843",14,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=67,potentially many other
cs-410_6_1_15,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:09,843","00:01:14,912",15,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=69,And this will be very useful for
cs-410_6_1_16,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:14,912","00:01:19,833",16,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=74,"accuracy, but also to improve"
cs-410_6_1_17,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:19,833","00:01:24,176",17,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=79,So that it's not easy for
cs-410_6_1_18,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:24,176","00:01:26,720",18,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=84,a few features to promote a page.
cs-410_6_1_19,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:27,910","00:01:32,000",19,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=87,So the general idea of learning
cs-410_6_1_20,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:32,000","00:01:36,030",20,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=92,learning to combine this
cs-410_6_1_21,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:36,030","00:01:39,160",21,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=96,on different features to generate
cs-410_6_1_22,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:40,610","00:01:44,720",22,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=100,So we will assume that the given
cs-410_6_1_23,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:44,720","00:01:49,680",23,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=104,we can define a number of features.
cs-410_6_1_24,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:49,680","00:01:55,180",24,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=109,And these features can vary from
cs-410_6_1_25,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:55,180","00:01:59,875",25,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=115,a score of the document with
cs-410_6_1_26,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:59,875","00:02:05,060",26,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=119,a retrieval function such as BM25 or
cs-410_6_1_27,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:05,060","00:02:10,440",27,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=125,of punitive commands from a machine or
cs-410_6_1_28,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:10,440","00:02:15,410",28,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=130,It can also be a link based score like or
cs-410_6_1_29,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:15,410","00:02:23,470",29,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=135,It can be also application of retrieval
cs-410_6_1_30,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:23,470","00:02:28,240",30,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=143,Those are the types of descriptions
cs-410_6_1_31,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:29,520","00:02:33,909",31,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=149,"So, these can all the clues whether"
cs-410_6_1_32,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:35,070","00:02:41,320",32,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=155,We can even include a feature
cs-410_6_1_33,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:41,320","00:02:46,680",33,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=161,has a tilde because this might be
cs-410_6_1_34,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:48,170","00:02:52,180",34,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=168,So all these features can then be combined
cs-410_6_1_35,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:52,180","00:02:53,610",35,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=172,"The question is, of course."
cs-410_6_1_36,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:53,610","00:02:55,250",36,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=173,How can we combine them?
cs-410_6_1_37,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:55,250","00:03:00,580",37,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=175,"In this approach,"
cs-410_6_1_38,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:00,580","00:03:07,730",38,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=180,that this document isn't relevant to this
cs-410_6_1_39,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:07,730","00:03:10,329",39,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=187,So we can hypothesize this
cs-410_6_1_40,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:11,450","00:03:16,730",40,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=191,that the probability of relevance
cs-410_6_1_41,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:16,730","00:03:22,070",41,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=196,through a particular form of
cs-410_6_1_42,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:22,070","00:03:25,510",42,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=202,These parameters can control
cs-410_6_1_43,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:25,510","00:03:29,410",43,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=205,the influence of different
cs-410_6_1_44,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:29,410","00:03:33,820",44,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=209,Now this is of course just an assumption.
cs-410_6_1_45,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:33,820","00:03:38,925",45,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=213,Whether this assumption really
cs-410_6_1_46,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:38,925","00:03:43,570",46,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=218,that's they have to empirically
cs-410_6_1_47,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:45,020","00:03:50,450",47,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=225,But by hypothesizing that
cs-410_6_1_48,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:50,450","00:03:55,783",48,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=230,"features in the particular way, we can"
cs-410_6_1_49,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:55,783","00:04:00,805",49,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=235,the potential more powerful ranking
cs-410_6_1_50,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:00,805","00:04:05,342",50,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=240,Naturally the next question is how
cs-410_6_1_51,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:05,342","00:04:08,732",51,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=245,How do we know which features
cs-410_6_1_52,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:08,732","00:04:11,922",52,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=248,and which features will have lower weight?
cs-410_6_1_53,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:11,922","00:04:15,732",53,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=251,So this is the task of training or
cs-410_6_1_54,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:15,732","00:04:20,000",54,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=255,in this approach what we will
cs-410_6_1_55,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:20,000","00:04:24,910",55,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=260,Those are the data that have
cs-410_6_1_56,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:24,910","00:04:27,370",56,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=264,that we already know
cs-410_6_1_57,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:27,370","00:04:31,443",57,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=267,We already know which documents should
cs-410_6_1_58,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:31,443","00:04:36,074",58,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=271,And this information can be based
cs-410_6_1_59,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:36,074","00:04:41,508",59,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=276,this can also be approximated by just
cs-410_6_1_60,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:41,508","00:04:47,477",60,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=281,where we can assume the clicked documents
cs-410_6_1_61,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:47,477","00:04:53,500",61,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=287,clicked documents are relevant and
cs-410_6_1_62,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:53,500","00:04:58,222",62,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=293,So in general with the fit
cs-410_6_1_63,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:58,222","00:05:00,960",63,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=298,function to the training data
cs-410_6_1_64,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:00,960","00:05:06,650",64,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=300,meaning that we will try to optimize it's
cs-410_6_1_65,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:06,650","00:05:08,920",65,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=306,And we can adjust these parameters to see
cs-410_6_1_66,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:09,960","00:05:14,780",66,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=309,how we can optimize the performance of
cs-410_6_1_67,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:16,030","00:05:19,180",67,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=316,in terms of some measures such as MAP or
cs-410_6_1_68,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:20,600","00:05:25,440",68,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=320,So the training date would
cs-410_6_1_69,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:25,440","00:05:32,800",69,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=325,"Each tuple has three elements, the query,"
cs-410_6_1_70,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:32,800","00:05:37,469",70,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=332,So it looks very much like our
cs-410_6_1_71,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:37,469","00:05:40,933",71,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=337,about in the evaluation
cs-410_6_1_72,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:40,933","00:05:50,933",72,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=340,[MUSIC]
cs-410_6_2_1,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:00,076","00:00:03,466",1,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=0,[MUSIC]
cs-410_6_2_2,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:06,698","00:00:14,732",2,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=6,So now let's take a look at the specific
cs-410_6_2_3,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:14,732","00:00:17,627",3,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=14,"Now, this is one of the many"
cs-410_6_2_4,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:17,627","00:00:19,309",4,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=17,it's one of the simplest methods.
cs-410_6_2_5,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:19,309","00:00:24,550",5,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=19,And I choose this to explain
cs-410_6_2_6,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:26,360","00:00:34,350",6,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=26,"So in this approach, we simply assume"
cs-410_6_2_7,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:34,350","00:00:39,760",7,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=34,respect to a query is related to a linear
cs-410_6_2_8,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:39,760","00:00:47,400",8,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=39,Here I used Xi to denote the feature.
cs-410_6_2_9,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:47,400","00:00:51,770",9,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=47,So Xi of Q and D is a feature.
cs-410_6_2_10,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:51,770","00:00:54,720",10,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=51,And we can have as many
cs-410_6_2_11,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:55,890","00:01:01,670",11,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=55,And we assume that these features
cs-410_6_2_12,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:03,580","00:01:06,150",12,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=63,And each feature is controlled
cs-410_6_2_13,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:06,150","00:01:08,880",13,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=66,and this beta i is a parameter.
cs-410_6_2_14,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:08,880","00:01:10,790",14,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=68,That's a weighting parameter.
cs-410_6_2_15,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:10,790","00:01:15,940",15,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=70,A larger value would mean the feature
cs-410_6_2_16,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:15,940","00:01:18,525",16,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=75,and it would contribute more
cs-410_6_2_17,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:18,525","00:01:23,069",17,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=78,This specific form of the function
cs-410_6_2_18,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:23,069","00:01:27,154",18,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=83,a transformation of
cs-410_6_2_19,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:27,154","00:01:30,428",19,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=87,So this is the probability of relevance.
cs-410_6_2_20,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:30,428","00:01:35,611",20,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=90,And we know that the probability of
cs-410_6_2_21,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:36,870","00:01:41,863",21,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=96,And we could have just assumed that
cs-410_6_2_22,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:41,863","00:01:43,856",22,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=101,this linear combination.
cs-410_6_2_23,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:43,856","00:01:47,479",23,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=103,So we can do a linear regression.
cs-410_6_2_24,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:47,479","00:01:53,940",24,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=107,"But then, the value of this linear"
cs-410_6_2_25,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:53,940","00:01:59,220",25,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=113,So this transformation
cs-410_6_2_26,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:59,220","00:02:05,540",26,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=119,to 1 range to the whole
cs-410_6_2_27,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:05,540","00:02:08,330",27,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=125,you can verify it by yourself.
cs-410_6_2_28,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:10,350","00:02:16,700",28,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=130,So this allows us then to connect
cs-410_6_2_29,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:16,700","00:02:23,010",29,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=136,which is between 0 and 1 to a linear
cs-410_6_2_30,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:23,010","00:02:28,133",30,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=143,And if we rewrite this into a probability
cs-410_6_2_31,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:28,133","00:02:34,299",31,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=148,"So on this equation, now we'll"
cs-410_6_2_32,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:35,690","00:02:39,168",32,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=155,"And on the right hand side,"
cs-410_6_2_33,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:39,168","00:02:42,448",33,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=159,"Now, this form is clearly nonnegative, and"
cs-410_6_2_34,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:42,448","00:02:46,344",34,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=162,it still involves a linear
cs-410_6_2_35,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:46,344","00:02:50,890",35,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=166,"And it's also clear that if this value is,"
cs-410_6_2_36,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:50,890","00:02:58,991",36,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=170,this is actually negative of the linear
cs-410_6_2_37,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:58,991","00:03:04,415",37,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=178,"If this value here is large,"
cs-410_6_2_38,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:04,415","00:03:11,879",38,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=184,then it would mean this value is small.
cs-410_6_2_39,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:11,879","00:03:17,278",39,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=191,"And therefore,"
cs-410_6_2_40,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:17,278","00:03:22,034",40,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=197,"And that's we expect, that basically,"
cs-410_6_2_41,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:22,034","00:03:26,496",41,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=202,"gives us a high value, then"
cs-410_6_2_42,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:26,496","00:03:29,015",42,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=206,So this is our hypothesis.
cs-410_6_2_43,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:29,015","00:03:33,955",43,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=209,"Again, this is not necessarily the best"
cs-410_6_2_44,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:33,955","00:03:39,109",44,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=213,way to connect these features with
cs-410_6_2_45,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:40,470","00:03:44,578",45,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=220,So now we have this combination function.
cs-410_6_2_46,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:44,578","00:03:48,617",46,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=224,The next task is to
cs-410_6_2_47,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:48,617","00:03:52,346",47,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=228,that the function cache will be applied.
cs-410_6_2_48,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:52,346","00:03:57,430",48,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=232,"But without knowing the beta values,"
cs-410_6_2_49,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:58,520","00:04:04,068",49,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=238,So let's see how can
cs-410_6_2_50,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:04,068","00:04:07,190",50,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=244,"All right,"
cs-410_6_2_51,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:08,780","00:04:11,405",51,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=248,"In this example, we have three features."
cs-410_6_2_52,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:11,405","00:04:15,060",52,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=251,One is the BM25 score of the document and
cs-410_6_2_53,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:15,060","00:04:19,044",53,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=255,"One is the PageRank score of the document,"
cs-410_6_2_54,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:19,044","00:04:21,211",54,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=259,might not depend on the query.
cs-410_6_2_55,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:21,211","00:04:25,681",55,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=261,"We might have a topic-sensitive PageRank,"
cs-410_6_2_56,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:25,681","00:04:29,946",56,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=265,"Otherwise, the general PageRank"
cs-410_6_2_57,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:29,946","00:04:35,221",57,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=269,And then we have BM25 score on
cs-410_6_2_58,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:35,221","00:04:40,630",58,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=275,"Now, these are then the feature values for"
cs-410_6_2_59,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:41,910","00:04:47,370",59,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=281,"And in this case, the document is D1 and"
cs-410_6_2_60,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:48,790","00:04:54,547",60,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=288,Here's another training instance and
cs-410_6_2_61,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:54,547","00:04:57,832",61,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=294,"but in this case, it's not relevant."
cs-410_6_2_62,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:57,832","00:05:02,806",62,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=297,This is an oversimplified case where
cs-410_6_2_63,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:02,806","00:05:06,675",63,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=302,it's sufficient to illustrate the point.
cs-410_6_2_64,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:06,675","00:05:09,885",64,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=306,So what we can do is we use
cs-410_6_2_65,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:09,885","00:05:11,797",65,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=309,actually estimate the parameters.
cs-410_6_2_66,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:13,170","00:05:17,801",66,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=313,"Basically, we're going to"
cs-410_6_2_67,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:17,801","00:05:22,040",67,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=317,of the document based
cs-410_6_2_68,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:22,040","00:05:25,534",68,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=322,"That is, given that we observed"
cs-410_6_2_69,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:28,264","00:05:32,653",69,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=328,Can we predict the relevance here?
cs-410_6_2_70,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:32,653","00:05:39,070",70,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=332,"Now, of course, the prediction would be"
cs-410_6_2_71,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:39,070","00:05:42,680",71,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=339,And we hypothesize that the probability
cs-410_6_2_72,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:42,680","00:05:43,920",72,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=342,features in this way.
cs-410_6_2_73,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:43,920","00:05:51,260",73,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=343,"So we are going to see, for what values of"
cs-410_6_2_74,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:51,260","00:05:58,480",74,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=351,What do we mean by predicting
cs-410_6_2_75,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:58,480","00:06:02,037",75,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=358,"Well, we just mean, in the first case, for"
cs-410_6_2_76,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:02,037","00:06:06,667",76,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=362,D1 this expression right here
cs-410_6_2_77,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:06,667","00:06:10,452",77,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=366,"In fact, we'll hope this"
cs-410_6_2_78,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:10,452","00:06:13,470",78,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=370,Why?
cs-410_6_2_79,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:14,750","00:06:17,954",79,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=374,"On the other hand,"
cs-410_6_2_80,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:17,954","00:06:22,310",80,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=377,"we hope this value will be small, right."
cs-410_6_2_81,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:22,310","00:06:23,040",81,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=382,Why?
cs-410_6_2_82,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:23,040","00:06:26,310",82,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=383,Because it's a non-relevant document.
cs-410_6_2_83,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:26,310","00:06:30,250",83,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=386,So now let's see how this can
cs-410_6_2_84,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:30,250","00:06:34,954",84,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=390,And this is similar to expressing
cs-410_6_2_85,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:34,954","00:06:39,657",85,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=394,only that we are not talking about
cs-410_6_2_86,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:39,657","00:06:43,771",86,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=399,talking about the probability
cs-410_6_2_87,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:43,771","00:06:48,736",87,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=403,So what's the probability
cs-410_6_2_88,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:48,736","00:06:52,880",88,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=408,relevant if it has these feature values?
cs-410_6_2_89,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:54,250","00:06:57,890",89,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=414,"Well, this is just this expression."
cs-410_6_2_90,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:57,890","00:07:00,970",90,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=417,We just need to plug in the Xi's.
cs-410_6_2_91,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:00,970","00:07:03,296",91,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=420,So that's what we will get.
cs-410_6_2_92,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:03,296","00:07:08,116",92,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=423,"It's exactly like what we have seen above,"
cs-410_6_2_93,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:08,116","00:07:14,772",93,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=428,only that we replaced these
cs-410_6_2_94,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:14,772","00:07:21,247",94,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=434,"So for example, this 0.7 goes to here and"
cs-410_6_2_95,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:21,247","00:07:25,451",95,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=441,this 0.11 goes to here.
cs-410_6_2_96,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:25,451","00:07:27,369",96,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=445,"And these are different feature values,"
cs-410_6_2_97,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:27,369","00:07:29,405",97,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=447,and we combine them in
cs-410_6_2_98,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:29,405","00:07:31,770",98,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=449,The beta values are still unknown.
cs-410_6_2_99,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:31,770","00:07:37,202",99,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=451,But this gives us the probability
cs-410_6_2_100,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:37,202","00:07:39,342",100,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=457,if we assume such a model.
cs-410_6_2_101,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:39,342","00:07:39,853",101,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=459,Okay?
cs-410_6_2_102,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:39,853","00:07:44,553",102,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=459,"And we want to maximize this probability,"
cs-410_6_2_103,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:44,553","00:07:47,850",103,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=464,What do we do for the second document?
cs-410_6_2_104,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:47,850","00:07:53,309",104,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=467,"Well, we want to compute the probability"
cs-410_6_2_105,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:53,309","00:08:00,257",105,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=473,So this would mean we have to
cs-410_6_2_106,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:00,257","00:08:07,880",106,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=480,since this expression is actually
cs-410_6_2_107,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:07,880","00:08:12,524",107,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=487,So to compute the non-relevance
cs-410_6_2_108,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:12,524","00:08:17,062",108,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=492,we just do 1 minus
cs-410_6_2_109,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:17,062","00:08:18,480",109,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=497,Okay?
cs-410_6_2_110,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:18,480","00:08:24,450",110,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=498,So this whole expression then
cs-410_6_2_111,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:24,450","00:08:29,220",111,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=504,predicting these two relevance values.
cs-410_6_2_112,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:29,220","00:08:32,759",112,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=509,"One is 1 here, one is 0."
cs-410_6_2_113,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:32,759","00:08:37,782",113,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=512,And this whole equation
cs-410_6_2_114,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:37,782","00:08:42,680",114,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=517,observing a 1 here and observing a 0 here.
cs-410_6_2_115,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:44,090","00:08:48,370",115,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=524,"Of course, this probability"
cs-410_6_2_116,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:50,130","00:08:55,318",116,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=530,So then our goal is to adjust
cs-410_6_2_117,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:55,318","00:09:00,121",117,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=535,"thing reach its maximum,"
cs-410_6_2_118,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:00,121","00:09:02,540",118,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=540,So that means we're going to compute this.
cs-410_6_2_119,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:02,540","00:09:07,280",119,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=542,The beta is just the parameter
cs-410_6_2_120,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:07,280","00:09:11,914",120,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=547,maximize this whole likelihood expression.
cs-410_6_2_121,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:11,914","00:09:16,284",121,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=551,"And what it means is,"
cs-410_6_2_122,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:16,284","00:09:21,224",122,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=556,we're going to choose betas to
cs-410_6_2_123,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:21,224","00:09:26,449",123,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=561,"make this also as large as possible,"
cs-410_6_2_124,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:26,449","00:09:29,400",124,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=566,make this part as small as possible.
cs-410_6_2_125,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:30,560","00:09:32,360",125,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=570,And this is precisely what we want.
cs-410_6_2_126,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:34,530","00:09:38,834",126,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=574,"So once we do the training,"
cs-410_6_2_127,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:38,834","00:09:43,330",127,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=578,So then this function
cs-410_6_2_128,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:43,330","00:09:50,690",128,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=583,"Once beta values are known, both this and"
cs-410_6_2_129,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:50,690","00:09:53,380",129,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=590,"So for any new query and new document,"
cs-410_6_2_130,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:53,380","00:09:56,924",130,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=593,we can simply compute the features for
cs-410_6_2_131,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:56,924","00:10:00,941",131,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=596,And then we just use this formula
cs-410_6_2_132,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:10:00,941","00:10:06,700",132,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=600,And this scoring function can be used to
cs-410_6_2_133,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:10:06,700","00:10:11,787",133,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=606,So that's the basic idea
cs-410_6_2_134,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:10:11,787","00:10:21,787",134,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=611,[MUSIC]
cs-410_6_3_1,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:00,008","00:00:07,386",1,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=0,[SOUND]
cs-410_6_3_2,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:07,386","00:00:11,551",2,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=7,more of the Munster learning algorithms
cs-410_6_3_3,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:11,551","00:00:15,009",3,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=11,they generally attempt to direct
cs-410_6_3_4,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:16,690","00:00:18,010",4,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=16,Like a MAP or nDCG.
cs-410_6_3_5,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:19,020","00:00:24,640",5,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=19,Note that the optimization object or
cs-410_6_3_6,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:24,640","00:00:29,610",6,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=24,on the previous slide is not directly
cs-410_6_3_7,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:31,390","00:00:33,870",7,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=31,By maximizing the prediction of one or
cs-410_6_3_8,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:33,870","00:00:39,430",8,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=33,"zero, we don't necessarily optimize"
cs-410_6_3_9,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:39,430","00:00:44,106",9,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=39,One can imagine that our
cs-410_6_3_10,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:44,106","00:00:46,626",10,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=44,And let's say both are around 0.5.
cs-410_6_3_11,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:46,626","00:00:51,230",11,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=46,So it's kind of in the middle of zero and
cs-410_6_3_12,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:51,230","00:00:58,250",12,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=51,"But the ranking can be wrong, so we might"
cs-410_6_3_13,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:00,750","00:01:04,235",13,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=60,So that won't be good from
cs-410_6_3_14,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:04,235","00:01:07,420",14,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=64,"even though function, it's not bad."
cs-410_6_3_15,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:07,420","00:01:11,773",15,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=67,"In contrast, we might have another"
cs-410_6_3_16,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:11,773","00:01:14,000",16,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=71,"around the 0.9, it said."
cs-410_6_3_17,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:14,000","00:01:17,580",17,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=74,"And by the objective function,"
cs-410_6_3_18,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:17,580","00:01:20,500",18,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=77,But if we didn't get the order
cs-410_6_3_19,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:20,500","00:01:22,970",19,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=80,that's actually a better result.
cs-410_6_3_20,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:22,970","00:01:28,070",20,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=82,"So these new, more advanced approaches"
cs-410_6_3_21,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:28,070","00:01:32,120",21,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=88,"Of course, then the challenge is"
cs-410_6_3_22,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:32,120","00:01:33,700",22,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=92,be harder to solve.
cs-410_6_3_23,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:33,700","00:01:39,143",23,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=93,"And then, researchers have posed"
cs-410_6_3_24,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:39,143","00:01:46,153",24,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=99,and you can read more of the references at
cs-410_6_3_25,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:46,153","00:01:50,540",25,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=106,"Now, these learning ranked"
cs-410_6_3_26,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:50,540","00:01:53,530",26,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=110,So there accounts would be be applied
cs-410_6_3_27,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:53,530","00:01:55,350",27,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=113,not just the retrieval problem.
cs-410_6_3_28,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:55,350","00:01:58,993",28,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=115,So some people will go
cs-410_6_3_29,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:58,993","00:02:02,810",29,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=118,"computational advertising,"
cs-410_6_3_30,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:02,810","00:02:08,636",30,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=122,there are many others that you can
cs-410_6_3_31,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:11,157","00:02:15,884",31,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=131,To summarize this lecture we
cs-410_6_3_32,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:15,884","00:02:21,270",32,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=135,learning to combine much more
cs-410_6_3_33,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:22,780","00:02:24,690",33,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=142,Actually the use of machine learning
cs-410_6_3_34,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:25,810","00:02:29,840",34,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=145,in information retrieval has
cs-410_6_3_35,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:29,840","00:02:35,212",35,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=149,"So for example, the Rocchio feedback"
cs-410_6_3_36,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:35,212","00:02:40,700",36,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=155,was a machine learning approach
cs-410_6_3_37,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:40,700","00:02:46,750",37,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=160,But the most recent use of machine
cs-410_6_3_38,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:46,750","00:02:51,000",38,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=166,changes in the environment of
cs-410_6_3_39,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:52,550","00:02:58,650",39,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=172,"First, it's mostly freedom of"
cs-410_6_3_40,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:58,650","00:03:04,250",40,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=178,"in the form of critical, such as"
cs-410_6_3_41,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:04,250","00:03:11,106",41,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=184,So the data can provide a lot of
cs-410_6_3_42,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:11,106","00:03:17,487",42,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=191,machine learning methods can be
cs-410_6_3_43,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:17,487","00:03:21,744",43,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=197,"Secondly, it's also freedom by"
cs-410_6_3_44,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:21,744","00:03:24,464",44,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=201,and this is not only just
cs-410_6_3_45,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:24,464","00:03:29,840",45,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=204,features available on the web that can
cs-410_6_3_46,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:29,840","00:03:36,208",46,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=209,"It's also because by combining them,"
cs-410_6_3_47,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:36,208","00:03:41,168",47,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=216,"of ranking, so this is desired for"
cs-410_6_3_48,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:41,168","00:03:45,887",48,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=221,Modern search engines all use some
cs-410_6_3_49,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:45,887","00:03:48,855",49,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=225,combine many features
cs-410_6_3_50,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:48,855","00:03:53,590",50,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=228,this is a major feature of these
cs-410_6_3_51,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:56,190","00:04:02,368",51,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=236,The topic of learning to rank is still
cs-410_6_3_52,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:04:02,368","00:04:08,265",52,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=242,and so we can expect to see new results
cs-410_6_3_53,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:04:08,265","00:04:09,119",53,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=248,perhaps.
cs-410_6_3_54,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:04:12,753","00:04:17,686",54,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=252,Here are some additional readings
cs-410_6_3_55,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:04:17,686","00:04:22,544",55,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=257,about how learning to rank at works and
cs-410_6_3_56,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:04:25,281","00:04:35,281",56,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=265,[MUSIC]
cs-410_6_4_1,cs-410,6,4, Future of Web Search,"00:00:00,012","00:00:04,047",1,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=0,[SOUND].
cs-410_6_4_2,cs-410,6,4, Future of Web Search,"00:00:07,043","00:00:10,080",2,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=7,This lecture is about
cs-410_6_4_3,cs-410,6,4, Future of Web Search,"00:00:12,660","00:00:17,560",3,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=12,"In this lecture, we're going to talk"
cs-410_6_4_4,cs-410,6,4, Future of Web Search,"00:00:17,560","00:00:22,489",4,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=17,of web search and intelligent information
cs-410_6_4_5,cs-410,6,4, Future of Web Search,"00:00:24,370","00:00:28,561",5,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=24,In order to further improve
cs-410_6_4_6,cs-410,6,4, Future of Web Search,"00:00:28,561","00:00:33,752",6,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=28,it's important that to consider
cs-410_6_4_7,cs-410,6,4, Future of Web Search,"00:00:33,752","00:00:39,056",7,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=33,So one particular trend could be to
cs-410_6_4_8,cs-410,6,4, Future of Web Search,"00:00:39,056","00:00:44,630",8,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=39,"customized search engines, and they"
cs-410_6_4_9,cs-410,6,4, Future of Web Search,"00:00:46,260","00:00:50,180",9,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=46,These vertical search engines can be
cs-410_6_4_10,cs-410,6,4, Future of Web Search,"00:00:50,180","00:00:55,940",10,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=50,the current general search engines
cs-410_6_4_11,cs-410,6,4, Future of Web Search,"00:00:55,940","00:01:02,070",11,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=55,users are a special group of users that
cs-410_6_4_12,cs-410,6,4, Future of Web Search,"00:01:02,070","00:01:06,420",12,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=62,and then the search engine can be
cs-410_6_4_13,cs-410,6,4, Future of Web Search,"00:01:07,970","00:01:12,150",13,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=67,"And because of the customization,"
cs-410_6_4_14,cs-410,6,4, Future of Web Search,"00:01:12,150","00:01:14,360",14,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=72,"So the search can be personalized,"
cs-410_6_4_15,cs-410,6,4, Future of Web Search,"00:01:15,430","00:01:18,190",15,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=75,because we have a better
cs-410_6_4_16,cs-410,6,4, Future of Web Search,"00:01:20,330","00:01:25,550",16,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=80,"Because of the restrictions with domain,"
cs-410_6_4_17,cs-410,6,4, Future of Web Search,"00:01:25,550","00:01:29,590",17,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=85,"in handling the documents, because we can"
cs-410_6_4_18,cs-410,6,4, Future of Web Search,"00:01:29,590","00:01:33,880",18,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=89,"For example, particular words may"
cs-410_6_4_19,cs-410,6,4, Future of Web Search,"00:01:33,880","00:01:36,750",19,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=93,So we can bypass the problem of ambiguity.
cs-410_6_4_20,cs-410,6,4, Future of Web Search,"00:01:38,390","00:01:41,460",20,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=98,"Another trend we can expect to see,"
cs-410_6_4_21,cs-410,6,4, Future of Web Search,"00:01:41,460","00:01:45,600",21,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=101,is the search engine will
cs-410_6_4_22,cs-410,6,4, Future of Web Search,"00:01:45,600","00:01:52,430",22,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=105,It's like a lifetime learning or
cs-410_6_4_23,cs-410,6,4, Future of Web Search,"00:01:52,430","00:01:57,800",23,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=112,very attractive because that means the
cs-410_6_4_24,cs-410,6,4, Future of Web Search,"00:01:57,800","00:02:01,780",24,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=117,"As more people are using it, the search"
cs-410_6_4_25,cs-410,6,4, Future of Web Search,"00:02:01,780","00:02:03,260",25,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=121,"this is already happening,"
cs-410_6_4_26,cs-410,6,4, Future of Web Search,"00:02:03,260","00:02:06,980",26,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=123,because the search engines can learn
cs-410_6_4_27,cs-410,6,4, Future of Web Search,"00:02:06,980","00:02:10,800",27,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=126,"More users use it, and the quality"
cs-410_6_4_28,cs-410,6,4, Future of Web Search,"00:02:10,800","00:02:15,840",28,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=130,the popular queries that are typed in by
cs-410_6_4_29,cs-410,6,4, Future of Web Search,"00:02:15,840","00:02:19,110",29,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=135,so this is sort of another
cs-410_6_4_30,cs-410,6,4, Future of Web Search,"00:02:21,260","00:02:24,600",30,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=141,The third trend might be
cs-410_6_4_31,cs-410,6,4, Future of Web Search,"00:02:24,600","00:02:27,190",31,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=144,bottles of information access.
cs-410_6_4_32,cs-410,6,4, Future of Web Search,"00:02:27,190","00:02:32,050",32,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=147,"So search, navigation, and"
cs-410_6_4_33,cs-410,6,4, Future of Web Search,"00:02:32,050","00:02:37,480",33,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=152,combined to form a full-fledged
cs-410_6_4_34,cs-410,6,4, Future of Web Search,"00:02:37,480","00:02:42,470",34,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=157,"And in the beginning of this course,"
cs-410_6_4_35,cs-410,6,4, Future of Web Search,"00:02:42,470","00:02:47,100",35,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=162,These are different modes of information
cs-410_6_4_36,cs-410,6,4, Future of Web Search,"00:02:48,170","00:02:53,660",36,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=168,"And similarly, in the pull mode, querying"
cs-410_6_4_37,cs-410,6,4, Future of Web Search,"00:02:53,660","00:02:58,390",37,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=173,"And in fact we're doing that basically,"
cs-410_6_4_38,cs-410,6,4, Future of Web Search,"00:02:58,390","00:03:02,000",38,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=178,"We are querying, sometimes browsing,"
cs-410_6_4_39,cs-410,6,4, Future of Web Search,"00:03:02,000","00:03:05,120",39,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=182,Sometimes we've got some
cs-410_6_4_40,cs-410,6,4, Future of Web Search,"00:03:05,120","00:03:11,466",40,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=185,Although most of the cases the information
cs-410_6_4_41,cs-410,6,4, Future of Web Search,"00:03:11,466","00:03:16,305",41,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=191,"But in the future, you can imagine"
cs-410_6_4_42,cs-410,6,4, Future of Web Search,"00:03:16,305","00:03:21,380",42,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=196,"multi-mode for information access, and"
cs-410_6_4_43,cs-410,6,4, Future of Web Search,"00:03:23,160","00:03:27,160",43,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=203,Another trend is that we might see systems
cs-410_6_4_44,cs-410,6,4, Future of Web Search,"00:03:27,160","00:03:30,970",44,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=207,that try to go beyond the searches
cs-410_6_4_45,cs-410,6,4, Future of Web Search,"00:03:30,970","00:03:36,730",45,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=210,"After all, the reason why people want"
cs-410_6_4_46,cs-410,6,4, Future of Web Search,"00:03:36,730","00:03:39,690",46,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=216,to make a decision or perform a task.
cs-410_6_4_47,cs-410,6,4, Future of Web Search,"00:03:39,690","00:03:42,380",47,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=219,For example consumers might search for
cs-410_6_4_48,cs-410,6,4, Future of Web Search,"00:03:42,380","00:03:45,385",48,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=222,opinions about products in
cs-410_6_4_49,cs-410,6,4, Future of Web Search,"00:03:45,385","00:03:50,160",49,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=225,"choose a good product by, so"
cs-410_6_4_50,cs-410,6,4, Future of Web Search,"00:03:50,160","00:03:55,330",50,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=230,support the whole workflow of purchasing
cs-410_6_4_51,cs-410,6,4, Future of Web Search,"00:03:56,732","00:04:00,300",51,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=236,"In this era, after the common search"
cs-410_6_4_52,cs-410,6,4, Future of Web Search,"00:04:00,300","00:04:04,190",52,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=240,"For example, you can sometimes look at the"
cs-410_6_4_53,cs-410,6,4, Future of Web Search,"00:04:04,190","00:04:09,040",53,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=244,you can just click on the button to go the
cs-410_6_4_54,cs-410,6,4, Future of Web Search,"00:04:09,040","00:04:12,840",54,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=249,"But it does not provide a,"
cs-410_6_4_55,cs-410,6,4, Future of Web Search,"00:04:12,840","00:04:14,720",55,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=252,"For example, for researchers,"
cs-410_6_4_56,cs-410,6,4, Future of Web Search,"00:04:14,720","00:04:18,800",56,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=254,you might want to find the realm in
cs-410_6_4_57,cs-410,6,4, Future of Web Search,"00:04:18,800","00:04:26,550",57,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=258,"And then, there's no, not much support for"
cs-410_6_4_58,cs-410,6,4, Future of Web Search,"00:04:26,550","00:04:31,130",58,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=266,"So, in general, I think,"
cs-410_6_4_59,cs-410,6,4, Future of Web Search,"00:04:31,130","00:04:34,980",59,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=271,"So in the following few slides, I'll"
cs-410_6_4_60,cs-410,6,4, Future of Web Search,"00:04:34,980","00:04:39,900",60,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=274,"specific ideas or thoughts that hopefully,"
cs-410_6_4_61,cs-410,6,4, Future of Web Search,"00:04:39,900","00:04:43,720",61,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=279,can help you in imagining new
cs-410_6_4_62,cs-410,6,4, Future of Web Search,"00:04:43,720","00:04:51,330",62,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=283,Some of them might be already relevant
cs-410_6_4_63,cs-410,6,4, Future of Web Search,"00:04:51,330","00:04:55,370",63,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=291,"In general, we can think about any"
cs-410_6_4_64,cs-410,6,4, Future of Web Search,"00:04:55,370","00:05:02,390",64,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=295,"information system, as we specified"
cs-410_6_4_65,cs-410,6,4, Future of Web Search,"00:05:02,390","00:05:05,680",65,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=302,And so
cs-410_6_4_66,cs-410,6,4, Future of Web Search,"00:05:05,680","00:05:09,250",66,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=305,then we'll able to specify
cs-410_6_4_67,cs-410,6,4, Future of Web Search,"00:05:09,250","00:05:12,480",67,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=309,And I call this
cs-410_6_4_68,cs-410,6,4, Future of Web Search,"00:05:12,480","00:05:18,110",68,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=312,So basically the three questions you
cs-410_6_4_69,cs-410,6,4, Future of Web Search,"00:05:18,110","00:05:23,470",69,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=318,what kind of data are you are managing and
cs-410_6_4_70,cs-410,6,4, Future of Web Search,"00:05:24,580","00:05:29,360",70,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=324,"Right there, this would help us"
cs-410_6_4_71,cs-410,6,4, Future of Web Search,"00:05:30,650","00:05:33,400",71,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=330,And there are many different ways
cs-410_6_4_72,cs-410,6,4, Future of Web Search,"00:05:33,400","00:05:36,040",72,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=333,"how you connect them,"
cs-410_6_4_73,cs-410,6,4, Future of Web Search,"00:05:36,040","00:05:37,650",73,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=336,So let me give you some examples.
cs-410_6_4_74,cs-410,6,4, Future of Web Search,"00:05:37,650","00:05:40,470",74,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=337,"On the top,"
cs-410_6_4_75,cs-410,6,4, Future of Web Search,"00:05:40,470","00:05:45,250",75,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=340,"On the left side, you can see different"
cs-410_6_4_76,cs-410,6,4, Future of Web Search,"00:05:45,250","00:05:48,740",76,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=345,"on the bottom,"
cs-410_6_4_77,cs-410,6,4, Future of Web Search,"00:05:48,740","00:05:51,760",77,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=348,Now imagine you can connect
cs-410_6_4_78,cs-410,6,4, Future of Web Search,"00:05:51,760","00:05:55,990",78,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=351,"So, for example, you can connect"
cs-410_6_4_79,cs-410,6,4, Future of Web Search,"00:05:55,990","00:05:59,140",79,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=355,the support search and
cs-410_6_4_80,cs-410,6,4, Future of Web Search,"00:05:59,140","00:06:01,050",80,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=359,"Well, that's web search, right?"
cs-410_6_4_81,cs-410,6,4, Future of Web Search,"00:06:02,440","00:06:07,680",81,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=362,What if we connect UIUC employees with
cs-410_6_4_82,cs-410,6,4, Future of Web Search,"00:06:07,680","00:06:12,720",82,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=367,documents to support the search and
cs-410_6_4_83,cs-410,6,4, Future of Web Search,"00:06:12,720","00:06:17,110",83,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=372,If you connect the scientist
cs-410_6_4_84,cs-410,6,4, Future of Web Search,"00:06:17,110","00:06:22,050",84,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=377,"to provide all kinds of service,"
cs-410_6_4_85,cs-410,6,4, Future of Web Search,"00:06:22,050","00:06:28,310",85,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=382,alert of new random documents or
cs-410_6_4_86,cs-410,6,4, Future of Web Search,"00:06:28,310","00:06:31,490",86,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=388,or provide the task with support or
cs-410_6_4_87,cs-410,6,4, Future of Web Search,"00:06:31,490","00:06:36,600",87,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=391,"For example, we might be,"
cs-410_6_4_88,cs-410,6,4, Future of Web Search,"00:06:36,600","00:06:40,140",88,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=396,automatically generating
cs-410_6_4_89,cs-410,6,4, Future of Web Search,"00:06:40,140","00:06:44,440",89,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=400,"a research paper, and"
cs-410_6_4_90,cs-410,6,4, Future of Web Search,"00:06:44,440","00:06:45,270",90,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=404,Right?
cs-410_6_4_91,cs-410,6,4, Future of Web Search,"00:06:45,270","00:06:48,010",91,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=405,we can imagine this would
cs-410_6_4_92,cs-410,6,4, Future of Web Search,"00:06:48,010","00:06:52,800",92,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=408,If we connect the online shoppers
cs-410_6_4_93,cs-410,6,4, Future of Web Search,"00:06:53,890","00:06:59,825",93,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=413,then we can help these people
cs-410_6_4_94,cs-410,6,4, Future of Web Search,"00:06:59,825","00:07:05,465",94,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=419,"So we can provide, for example data mining"
cs-410_6_4_95,cs-410,6,4, Future of Web Search,"00:07:05,465","00:07:11,950",95,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=425,"to compare products, compare sentiment of"
cs-410_6_4_96,cs-410,6,4, Future of Web Search,"00:07:11,950","00:07:15,950",96,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=431,decision support to have them
cs-410_6_4_97,cs-410,6,4, Future of Web Search,"00:07:15,950","00:07:21,510",97,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=435,Or we can connect customer service
cs-410_6_4_98,cs-410,6,4, Future of Web Search,"00:07:22,630","00:07:27,660",98,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=442,"and, and we can imagine a system"
cs-410_6_4_99,cs-410,6,4, Future of Web Search,"00:07:27,660","00:07:31,460",99,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=447,of these emails to find that the major
cs-410_6_4_100,cs-410,6,4, Future of Web Search,"00:07:31,460","00:07:35,150",100,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=451,We can imagine a system we
cs-410_6_4_101,cs-410,6,4, Future of Web Search,"00:07:35,150","00:07:39,630",101,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=455,by automatically generating
cs-410_6_4_102,cs-410,6,4, Future of Web Search,"00:07:39,630","00:07:45,720",102,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=459,Maybe intelligently attach
cs-410_6_4_103,cs-410,6,4, Future of Web Search,"00:07:45,720","00:07:49,830",103,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=465,"if appropriate, if they detect that that's"
cs-410_6_4_104,cs-410,6,4, Future of Web Search,"00:07:49,830","00:07:55,290",104,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=469,then you might take this opportunity
cs-410_6_4_105,cs-410,6,4, Future of Web Search,"00:07:55,290","00:07:57,770",105,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=475,"Whereas if it's a complaint,"
cs-410_6_4_106,cs-410,6,4, Future of Web Search,"00:07:59,510","00:08:03,810",106,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=479,automatically generate some
cs-410_6_4_107,cs-410,6,4, Future of Web Search,"00:08:03,810","00:08:08,790",107,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=483,tell the customer that he or she can
cs-410_6_4_108,cs-410,6,4, Future of Web Search,"00:08:08,790","00:08:14,210",108,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=488,All of these are trying to help
cs-410_6_4_109,cs-410,6,4, Future of Web Search,"00:08:15,570","00:08:19,850",109,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=495,So this shows that
cs-410_6_4_110,cs-410,6,4, Future of Web Search,"00:08:19,850","00:08:22,090",110,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=499,It's just only restricted
cs-410_6_4_111,cs-410,6,4, Future of Web Search,"00:08:22,090","00:08:27,400",111,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=502,So this picture shows the trend
cs-410_6_4_112,cs-410,6,4, Future of Web Search,"00:08:27,400","00:08:33,770",112,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=507,"it characterizes the, intelligent"
cs-410_6_4_113,cs-410,6,4, Future of Web Search,"00:08:33,770","00:08:39,065",113,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=513,"You can see in the center, there's"
cs-410_6_4_114,cs-410,6,4, Future of Web Search,"00:08:39,065","00:08:41,225",114,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=519,to search a bag of words representation.
cs-410_6_4_115,cs-410,6,4, Future of Web Search,"00:08:41,225","00:08:46,721",115,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=521,That means the current search engines
cs-410_6_4_116,cs-410,6,4, Future of Web Search,"00:08:46,721","00:08:54,085",116,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=526,to users and mostly model
cs-410_6_4_117,cs-410,6,4, Future of Web Search,"00:08:54,085","00:08:59,105",117,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=534,and sees the data through
cs-410_6_4_118,cs-410,6,4, Future of Web Search,"00:08:59,105","00:09:06,190",118,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=539,So it's a very simple approximation of
cs-410_6_4_119,cs-410,6,4, Future of Web Search,"00:09:06,190","00:09:08,500",119,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=546,But that's what the current system does.
cs-410_6_4_120,cs-410,6,4, Future of Web Search,"00:09:08,500","00:09:12,150",120,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=548,It connects these three nodes
cs-410_6_4_121,cs-410,6,4, Future of Web Search,"00:09:12,150","00:09:17,655",121,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=552,it only provides a basic search function
cs-410_6_4_122,cs-410,6,4, Future of Web Search,"00:09:17,655","00:09:24,405",122,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=557,and it doesn't really understand that
cs-410_6_4_123,cs-410,6,4, Future of Web Search,"00:09:24,405","00:09:31,862",123,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=564,"Now, I showed some trends to push each"
cs-410_6_4_124,cs-410,6,4, Future of Web Search,"00:09:31,862","00:09:35,332",124,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=571,"So think about the user node here, right?"
cs-410_6_4_125,cs-410,6,4, Future of Web Search,"00:09:35,332","00:09:39,432",125,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=575,"So we can go beyond the keyword queries,"
cs-410_6_4_126,cs-410,6,4, Future of Web Search,"00:09:39,432","00:09:43,882",126,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=579,and then further model the user
cs-410_6_4_127,cs-410,6,4, Future of Web Search,"00:09:43,882","00:09:49,622",127,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=583,"the user's task environment,"
cs-410_6_4_128,cs-410,6,4, Future of Web Search,"00:09:49,622","00:09:55,120",128,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=589,"Okay, so this is pushing for"
cs-410_6_4_129,cs-410,6,4, Future of Web Search,"00:09:55,120","00:09:58,630",129,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=595,And this is a major
cs-410_6_4_130,cs-410,6,4, Future of Web Search,"00:09:58,630","00:10:01,810",130,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=598,in order to build intelligent
cs-410_6_4_131,cs-410,6,4, Future of Web Search,"00:10:01,810","00:10:05,810",131,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=601,"On the document side,"
cs-410_6_4_132,cs-410,6,4, Future of Web Search,"00:10:05,810","00:10:10,640",132,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=605,go beyond bag of words implementation
cs-410_6_4_133,cs-410,6,4, Future of Web Search,"00:10:10,640","00:10:16,040",133,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=610,"This means we'll recognize people's names,"
cs-410_6_4_134,cs-410,6,4, Future of Web Search,"00:10:16,040","00:10:20,430",134,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=616,And this is already feasible with
cs-410_6_4_135,cs-410,6,4, Future of Web Search,"00:10:20,430","00:10:24,130",135,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=620,And Google is the reason
cs-410_6_4_136,cs-410,6,4, Future of Web Search,"00:10:24,130","00:10:28,310",136,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=624,"If you haven't heard of it,"
cs-410_6_4_137,cs-410,6,4, Future of Web Search,"00:10:28,310","00:10:33,820",137,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=628,And once we can get to that level without
cs-410_6_4_138,cs-410,6,4, Future of Web Search,"00:10:33,820","00:10:38,170",138,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=633,it can enable the search engine
cs-410_6_4_139,cs-410,6,4, Future of Web Search,"00:10:38,170","00:10:41,470",139,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=638,In the future we would like to have
cs-410_6_4_140,cs-410,6,4, Future of Web Search,"00:10:41,470","00:10:45,450",140,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=641,knowledge representation where we
cs-410_6_4_141,cs-410,6,4, Future of Web Search,"00:10:45,450","00:10:47,750",141,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=645,then the search engine would
cs-410_6_4_142,cs-410,6,4, Future of Web Search,"00:10:49,390","00:10:53,490",142,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=649,So this calls for
cs-410_6_4_143,cs-410,6,4, Future of Web Search,"00:10:53,490","00:10:57,150",143,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=653,perhaps this is more feasible for
cs-410_6_4_144,cs-410,6,4, Future of Web Search,"00:10:57,150","00:10:59,800",144,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=657,It's easier to make progress
cs-410_6_4_145,cs-410,6,4, Future of Web Search,"00:10:59,800","00:11:01,240",145,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=659,"Now on the service side,"
cs-410_6_4_146,cs-410,6,4, Future of Web Search,"00:11:01,240","00:11:05,920",146,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=661,we see we need to go beyond the search of
cs-410_6_4_147,cs-410,6,4, Future of Web Search,"00:11:07,510","00:11:13,702",147,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=667,So search is only one way to get access
cs-410_6_4_148,cs-410,6,4, Future of Web Search,"00:11:13,702","00:11:19,980",148,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=673,systems and push and pull so different
cs-410_6_4_149,cs-410,6,4, Future of Web Search,"00:11:19,980","00:11:21,630",149,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=679,"But going beyond access,"
cs-410_6_4_150,cs-410,6,4, Future of Web Search,"00:11:21,630","00:11:25,820",150,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=681,we also need to help people digest the
cs-410_6_4_151,cs-410,6,4, Future of Web Search,"00:11:25,820","00:11:30,560",151,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=685,and this step has to do with analysis
cs-410_6_4_152,cs-410,6,4, Future of Web Search,"00:11:30,560","00:11:35,540",152,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=690,We have to find patterns or
cs-410_6_4_153,cs-410,6,4, Future of Web Search,"00:11:35,540","00:11:38,865",153,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=695,real knowledge that can
cs-410_6_4_154,cs-410,6,4, Future of Web Search,"00:11:38,865","00:11:43,055",154,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=698,actionable knowledge that can be used for
cs-410_6_4_155,cs-410,6,4, Future of Web Search,"00:11:43,055","00:11:47,165",155,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=703,And furthermore the knowledge
cs-410_6_4_156,cs-410,6,4, Future of Web Search,"00:11:47,165","00:11:52,580",156,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=707,"improve productivity in finishing a task,"
cs-410_6_4_157,cs-410,6,4, Future of Web Search,"00:11:52,580","00:11:54,000",157,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=712,"Right, so this is a trend."
cs-410_6_4_158,cs-410,6,4, Future of Web Search,"00:11:54,000","00:11:59,370",158,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=714,"And, and, and so basically,"
cs-410_6_4_159,cs-410,6,4, Future of Web Search,"00:11:59,370","00:12:04,210",159,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=719,in the future intelligent information
cs-410_6_4_160,cs-410,6,4, Future of Web Search,"00:12:04,210","00:12:06,940",160,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=724,interactive task support.
cs-410_6_4_161,cs-410,6,4, Future of Web Search,"00:12:06,940","00:12:11,200",161,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=726,Now I should also emphasize interactive
cs-410_6_4_162,cs-410,6,4, Future of Web Search,"00:12:11,200","00:12:16,790",162,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=731,the combined intelligence of the users and
cs-410_6_4_163,cs-410,6,4, Future of Web Search,"00:12:16,790","00:12:22,140",163,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=736,"So we, we can get some help"
cs-410_6_4_164,cs-410,6,4, Future of Web Search,"00:12:22,140","00:12:26,970",164,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=742,And we don't have to assume the system
cs-410_6_4_165,cs-410,6,4, Future of Web Search,"00:12:26,970","00:12:32,290",165,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=746,"user, and the machine can collaborate in"
cs-410_6_4_166,cs-410,6,4, Future of Web Search,"00:12:32,290","00:12:37,270",166,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=752,then the combined intelligence
cs-410_6_4_167,cs-410,6,4, Future of Web Search,"00:12:37,270","00:12:41,010",167,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=757,we can minimize the user's overall
cs-410_6_4_168,cs-410,6,4, Future of Web Search,"00:12:42,700","00:12:47,947",168,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=762,So this is the big picture of future
cs-410_6_4_169,cs-410,6,4, Future of Web Search,"00:12:47,947","00:12:52,582",169,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=767,and this hopefully can provide
cs-410_6_4_170,cs-410,6,4, Future of Web Search,"00:12:52,582","00:12:57,313",170,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=772,how to make further innovations
cs-410_6_4_171,cs-410,6,4, Future of Web Search,"00:12:57,313","00:13:07,313",171,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=777,[MUSIC]
cs-410_6_5_1,cs-410,6,5, Recommender Systems,"00:00:00,000","00:00:07,248",1,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=0,[MUSIC]
cs-410_6_5_2,cs-410,6,5, Recommender Systems,"00:00:07,248","00:00:09,548",2,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=7,This lecture is about
cs-410_6_5_3,cs-410,6,5, Recommender Systems,"00:00:12,888","00:00:18,400",3,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=12,So far we have talked about a lot
cs-410_6_5_4,cs-410,6,5, Recommender Systems,"00:00:19,680","00:00:24,675",4,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=19,We have talked about the problem
cs-410_6_5_5,cs-410,6,5, Recommender Systems,"00:00:24,675","00:00:30,134",5,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=24,"different methods for ranking,"
cs-410_6_5_6,cs-410,6,5, Recommender Systems,"00:00:30,134","00:00:33,198",6,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=30,"how to evaluate a search engine, etc."
cs-410_6_5_7,cs-410,6,5, Recommender Systems,"00:00:36,028","00:00:40,719",7,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=36,This is important because we know
cs-410_6_5_8,cs-410,6,5, Recommender Systems,"00:00:40,719","00:00:44,980",8,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=40,the most important applications
cs-410_6_5_9,cs-410,6,5, Recommender Systems,"00:00:44,980","00:00:49,820",9,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=44,And they are the most useful tools
cs-410_6_5_10,cs-410,6,5, Recommender Systems,"00:00:49,820","00:00:53,889",10,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=49,data into a small set
cs-410_6_5_11,cs-410,6,5, Recommender Systems,"00:00:56,330","00:01:00,959",11,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=56,Another reason why we spend so
cs-410_6_5_12,cs-410,6,5, Recommender Systems,"00:01:00,959","00:01:06,961",12,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=60,is because many techniques used in search
cs-410_6_5_13,cs-410,6,5, Recommender Systems,"00:01:06,961","00:01:11,266",13,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=66,"Recommender Systems,"
cs-410_6_5_14,cs-410,6,5, Recommender Systems,"00:01:11,266","00:01:16,840",14,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=71,"And so, overall, the two systems"
cs-410_6_5_15,cs-410,6,5, Recommender Systems,"00:01:16,840","00:01:19,110",15,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=76,And there are many techniques
cs-410_6_5_16,cs-410,6,5, Recommender Systems,"00:01:22,690","00:01:24,860",16,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=82,So this is a slide that
cs-410_6_5_17,cs-410,6,5, Recommender Systems,"00:01:24,860","00:01:29,020",17,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=84,when we talked about the two
cs-410_6_5_18,cs-410,6,5, Recommender Systems,"00:01:29,020","00:01:30,230",18,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=89,Pull and the Push.
cs-410_6_5_19,cs-410,6,5, Recommender Systems,"00:01:31,240","00:01:36,362",19,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=91,And we mentioned that recommender
cs-410_6_5_20,cs-410,6,5, Recommender Systems,"00:01:36,362","00:01:42,079",20,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=96,"users in the Push Mode, where the systems"
cs-410_6_5_21,cs-410,6,5, Recommender Systems,"00:01:42,079","00:01:47,228",21,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=102,the information to the user or
cs-410_6_5_22,cs-410,6,5, Recommender Systems,"00:01:47,228","00:01:51,429",22,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=107,And this often works
cs-410_6_5_23,cs-410,6,5, Recommender Systems,"00:01:51,429","00:01:56,341",23,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=111,stable information need
cs-410_6_5_24,cs-410,6,5, Recommender Systems,"00:01:56,341","00:02:01,649",24,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=116,So a Recommender System is sometimes
cs-410_6_5_25,cs-410,6,5, Recommender Systems,"00:02:01,649","00:02:07,431",25,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=121,it's because recommending useful
cs-410_6_5_26,cs-410,6,5, Recommender Systems,"00:02:07,431","00:02:10,749",26,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=127,"filtering out the the useless articles,"
cs-410_6_5_27,cs-410,6,5, Recommender Systems,"00:02:10,749","00:02:14,370",27,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=130,and so
cs-410_6_5_28,cs-410,6,5, Recommender Systems,"00:02:16,070","00:02:20,412",28,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=136,And in all the cases the system
cs-410_6_5_29,cs-410,6,5, Recommender Systems,"00:02:20,412","00:02:24,840",29,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=140,usually there's a dynamic source
cs-410_6_5_30,cs-410,6,5, Recommender Systems,"00:02:24,840","00:02:29,028",30,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=144,that you have some knowledge
cs-410_6_5_31,cs-410,6,5, Recommender Systems,"00:02:29,028","00:02:31,788",31,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=149,And then the system would make a decision
cs-410_6_5_32,cs-410,6,5, Recommender Systems,"00:02:31,788","00:02:34,950",32,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=151,about whether this item is
cs-410_6_5_33,cs-410,6,5, Recommender Systems,"00:02:34,950","00:02:39,678",33,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=154,then if it's interesting then the system
cs-410_6_5_34,cs-410,6,5, Recommender Systems,"00:02:43,008","00:02:49,520",34,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=163,So the basic filtering question here is
cs-410_6_5_35,cs-410,6,5, Recommender Systems,"00:02:49,520","00:02:52,426",35,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=169,Will U like item X?
cs-410_6_5_36,cs-410,6,5, Recommender Systems,"00:02:52,426","00:02:55,640",36,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=172,And there are two ways to answer this
cs-410_6_5_37,cs-410,6,5, Recommender Systems,"00:02:56,738","00:03:00,040",37,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=176,And one is look at what items U likes and
cs-410_6_5_38,cs-410,6,5, Recommender Systems,"00:03:00,040","00:03:03,655",38,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=180,then we can see if X is
cs-410_6_5_39,cs-410,6,5, Recommender Systems,"00:03:05,610","00:03:10,460",39,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=185,"The other is to look at who likes X,"
cs-410_6_5_40,cs-410,6,5, Recommender Systems,"00:03:10,460","00:03:16,000",40,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=190,"user looks like a one of those users,"
cs-410_6_5_41,cs-410,6,5, Recommender Systems,"00:03:16,000","00:03:18,640",41,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=196,And these strategies can be combined.
cs-410_6_5_42,cs-410,6,5, Recommender Systems,"00:03:18,640","00:03:20,800",42,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=198,If we follow the first strategy and
cs-410_6_5_43,cs-410,6,5, Recommender Systems,"00:03:20,800","00:03:26,170",43,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=200,look at item similarity in the case
cs-410_6_5_44,cs-410,6,5, Recommender Systems,"00:03:26,170","00:03:31,460",44,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=206,then we're talking about a content-based
cs-410_6_5_45,cs-410,6,5, Recommender Systems,"00:03:31,460","00:03:38,195",45,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=211,"If we look at the second strategy, then,"
cs-410_6_5_46,cs-410,6,5, Recommender Systems,"00:03:38,195","00:03:43,110",46,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=218,we're user similarity and the technique
cs-410_6_5_47,cs-410,6,5, Recommender Systems,"00:03:46,010","00:03:49,190",47,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=226,"So, let's first look at"
cs-410_6_5_48,cs-410,6,5, Recommender Systems,"00:03:49,190","00:03:51,530",48,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=229,This is what the system would look like.
cs-410_6_5_49,cs-410,6,5, Recommender Systems,"00:03:52,600","00:03:56,420",49,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=232,"Inside the system, there will be"
cs-410_6_5_50,cs-410,6,5, Recommender Systems,"00:03:56,420","00:04:00,860",50,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=236,"knowledge about the user's interests, and"
cs-410_6_5_51,cs-410,6,5, Recommender Systems,"00:04:02,210","00:04:06,815",51,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=242,It maintains this profile to keep
cs-410_6_5_52,cs-410,6,5, Recommender Systems,"00:04:06,815","00:04:10,865",52,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=246,then there is a utility function
cs-410_6_5_53,cs-410,6,5, Recommender Systems,"00:04:10,865","00:04:13,955",53,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=250,a nice plan utility
cs-410_6_5_54,cs-410,6,5, Recommender Systems,"00:04:13,955","00:04:17,977",54,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=253,It helps the system decide
cs-410_6_5_55,cs-410,6,5, Recommender Systems,"00:04:17,977","00:04:21,307",55,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=257,And then the accepted documents will
cs-410_6_5_56,cs-410,6,5, Recommender Systems,"00:04:21,307","00:04:23,457",56,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=261,according to the classified.
cs-410_6_5_57,cs-410,6,5, Recommender Systems,"00:04:23,457","00:04:28,327",57,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=263,There should be also an initialization
cs-410_6_5_58,cs-410,6,5, Recommender Systems,"00:04:28,327","00:04:34,167",58,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=268,maybe from a user's specified keywords or
cs-410_6_5_59,cs-410,6,5, Recommender Systems,"00:04:34,167","00:04:38,519",59,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=274,"etc., and this would be to feed into"
cs-410_6_5_60,cs-410,6,5, Recommender Systems,"00:04:39,900","00:04:43,210",60,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=279,There is also typically a learning
cs-410_6_5_61,cs-410,6,5, Recommender Systems,"00:04:43,210","00:04:45,310",61,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=283,users' feedback over time.
cs-410_6_5_62,cs-410,6,5, Recommender Systems,"00:04:45,310","00:04:49,310",62,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=285,Now note that in this case typical
cs-410_6_5_63,cs-410,6,5, Recommender Systems,"00:04:49,310","00:04:53,420",63,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=289,the system would have a lot more
cs-410_6_5_64,cs-410,6,5, Recommender Systems,"00:04:53,420","00:04:58,590",64,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=293,"If the user has taken a recommended item,"
cs-410_6_5_65,cs-410,6,5, Recommender Systems,"00:04:58,590","00:05:04,020",65,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=298,this a signal to indicate that
cs-410_6_5_66,cs-410,6,5, Recommender Systems,"00:05:04,020","00:05:07,010",66,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=304,"If the user discarded it,"
cs-410_6_5_67,cs-410,6,5, Recommender Systems,"00:05:07,010","00:05:11,640",67,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=307,And so such feedback can be a long term
cs-410_6_5_68,cs-410,6,5, Recommender Systems,"00:05:11,640","00:05:16,660",68,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=311,And the system can collect a lot of
cs-410_6_5_69,cs-410,6,5, Recommender Systems,"00:05:16,660","00:05:19,500",69,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=316,this then can then be used
cs-410_6_5_70,cs-410,6,5, Recommender Systems,"00:05:19,500","00:05:23,780",70,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=319,Now what's the criteria for
cs-410_6_5_71,cs-410,6,5, Recommender Systems,"00:05:24,860","00:05:31,190",71,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=324,How do we know this filtering
cs-410_6_5_72,cs-410,6,5, Recommender Systems,"00:05:31,190","00:05:36,440",72,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=331,Now in this case we cannot use the ranking
cs-410_6_5_73,cs-410,6,5, Recommender Systems,"00:05:36,440","00:05:39,300",73,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=336,because we can't afford waiting for
cs-410_6_5_74,cs-410,6,5, Recommender Systems,"00:05:39,300","00:05:42,960",74,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=339,then rank the documents to
cs-410_6_5_75,cs-410,6,5, Recommender Systems,"00:05:42,960","00:05:47,930",75,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=342,And so the system must make
cs-410_6_5_76,cs-410,6,5, Recommender Systems,"00:05:47,930","00:05:51,830",76,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=347,to decide whether the item is
cs-410_6_5_77,cs-410,6,5, Recommender Systems,"00:05:51,830","00:05:55,520",77,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=351,"So in other words, we're trying"
cs-410_6_5_78,cs-410,6,5, Recommender Systems,"00:05:56,800","00:05:57,899",78,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=356,"So in this case,"
cs-410_6_5_79,cs-410,6,5, Recommender Systems,"00:05:57,899","00:06:03,600",79,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=357,one common user strategy is to use
cs-410_6_5_80,cs-410,6,5, Recommender Systems,"00:06:03,600","00:06:06,560",80,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=363,"So here, I show linear utility function."
cs-410_6_5_81,cs-410,6,5, Recommender Systems,"00:06:06,560","00:06:11,550",81,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=366,That's defined as for example three
cs-410_6_5_82,cs-410,6,5, Recommender Systems,"00:06:11,550","00:06:17,490",82,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=371,"you delivered, minus two multiplied by the"
cs-410_6_5_83,cs-410,6,5, Recommender Systems,"00:06:17,490","00:06:20,775",83,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=377,"So in other words, we could kind of just"
cs-410_6_5_84,cs-410,6,5, Recommender Systems,"00:06:22,245","00:06:26,215",84,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=382,treat this as almost in a gambling game.
cs-410_6_5_85,cs-410,6,5, Recommender Systems,"00:06:26,215","00:06:32,767",85,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=386,"If you delete one good item,"
cs-410_6_5_86,cs-410,6,5, Recommender Systems,"00:06:32,767","00:06:37,660",86,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=392,you gain three dollars but if you deliver
cs-410_6_5_87,cs-410,6,5, Recommender Systems,"00:06:37,660","00:06:41,120",87,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=397,And this utility function
cs-410_6_5_88,cs-410,6,5, Recommender Systems,"00:06:41,120","00:06:45,375",88,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=401,how much money you are get by
cs-410_6_5_89,cs-410,6,5, Recommender Systems,"00:06:45,375","00:06:52,420",89,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=405,And so it's clear that if you want
cs-410_6_5_90,cs-410,6,5, Recommender Systems,"00:06:52,420","00:06:57,760",90,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=412,this strategy should be delivered
cs-410_6_5_91,cs-410,6,5, Recommender Systems,"00:06:57,760","00:07:01,160",91,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=417,and minimize the delivery of bad articles.
cs-410_6_5_92,cs-410,6,5, Recommender Systems,"00:07:01,160","00:07:02,370",92,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=421,"That's obvious, right?"
cs-410_6_5_93,cs-410,6,5, Recommender Systems,"00:07:03,570","00:07:08,130",93,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=423,Now one interesting question here is
cs-410_6_5_94,cs-410,6,5, Recommender Systems,"00:07:08,130","00:07:14,140",94,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=428,I just showed a three and
cs-410_6_5_95,cs-410,6,5, Recommender Systems,"00:07:14,140","00:07:16,950",95,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=434,"But one can ask the question,"
cs-410_6_5_96,cs-410,6,5, Recommender Systems,"00:07:17,990","00:07:19,030",96,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=437,So what do you think?
cs-410_6_5_97,cs-410,6,5, Recommender Systems,"00:07:21,080","00:07:23,450",97,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=441,Do you think that's a reasonable choice?
cs-410_6_5_98,cs-410,6,5, Recommender Systems,"00:07:23,450","00:07:24,510",98,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=443,What about the other choices?
cs-410_6_5_99,cs-410,6,5, Recommender Systems,"00:07:26,220","00:07:33,058",99,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=446,"So for example, we can have 10 and"
cs-410_6_5_100,cs-410,6,5, Recommender Systems,"00:07:33,058","00:07:34,750",100,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=453,What's the difference?
cs-410_6_5_101,cs-410,6,5, Recommender Systems,"00:07:34,750","00:07:35,330",101,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=454,What do you think?
cs-410_6_5_102,cs-410,6,5, Recommender Systems,"00:07:36,920","00:07:41,820",102,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=456,How would this utility function affect
cs-410_6_5_103,cs-410,6,5, Recommender Systems,"00:07:43,600","00:07:45,589",103,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=463,"Right, you can think of"
cs-410_6_5_104,cs-410,6,5, Recommender Systems,"00:07:45,589","00:07:51,284",104,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=465,"(10, -1) + (1, -10), which one do"
cs-410_6_5_105,cs-410,6,5, Recommender Systems,"00:07:51,284","00:07:57,760",105,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=471,system to over do it and which one would
cs-410_6_5_106,cs-410,6,5, Recommender Systems,"00:07:57,760","00:08:03,380",106,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=477,If you think about it you will see that
cs-410_6_5_107,cs-410,6,5, Recommender Systems,"00:08:03,380","00:08:08,410",107,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=483,our good document you incur only a small
cs-410_6_5_108,cs-410,6,5, Recommender Systems,"00:08:08,410","00:08:11,740",108,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=488,"Intuitively, you would be"
cs-410_6_5_109,cs-410,6,5, Recommender Systems,"00:08:11,740","00:08:16,370",109,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=491,And you can try to deliver more in
cs-410_6_5_110,cs-410,6,5, Recommender Systems,"00:08:16,370","00:08:17,870",110,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=496,And then we'll get a big reward.
cs-410_6_5_111,cs-410,6,5, Recommender Systems,"00:08:19,600","00:08:23,364",111,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=499,"So on the other hand,"
cs-410_6_5_112,cs-410,6,5, Recommender Systems,"00:08:23,364","00:08:28,228",112,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=503,you really don't get such a big prize
cs-410_6_5_113,cs-410,6,5, Recommender Systems,"00:08:28,228","00:08:31,250",113,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=508,"On the other hand, you will have"
cs-410_6_5_114,cs-410,6,5, Recommender Systems,"00:08:31,250","00:08:32,710",114,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=511,"You can imagine that,"
cs-410_6_5_115,cs-410,6,5, Recommender Systems,"00:08:32,710","00:08:36,590",115,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=512,the system would be very reluctant
cs-410_6_5_116,cs-410,6,5, Recommender Systems,"00:08:36,590","00:08:41,198",116,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=516,It has to be absolutely
cs-410_6_5_117,cs-410,6,5, Recommender Systems,"00:08:41,198","00:08:45,990",117,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=521,So this utility function has to be
cs-410_6_5_118,cs-410,6,5, Recommender Systems,"00:08:45,990","00:08:49,660",118,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=525,The three basic problems in content-based
cs-410_6_5_119,cs-410,6,5, Recommender Systems,"00:08:49,660","00:08:53,620",119,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=529,"first, it has to make"
cs-410_6_5_120,cs-410,6,5, Recommender Systems,"00:08:53,620","00:08:58,200",120,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=533,"So it has to be a binary decision maker,"
cs-410_6_5_121,cs-410,6,5, Recommender Systems,"00:08:58,200","00:09:03,620",121,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=538,Given a text document and
cs-410_6_5_122,cs-410,6,5, Recommender Systems,"00:09:03,620","00:09:07,040",122,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=543,"it has to say yes or no, whether this"
cs-410_6_5_123,cs-410,6,5, Recommender Systems,"00:09:08,050","00:09:12,375",123,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=548,"So that's a decision module, and"
cs-410_6_5_124,cs-410,6,5, Recommender Systems,"00:09:12,375","00:09:17,220",124,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=552,module as you have seen earlier and
cs-410_6_5_125,cs-410,6,5, Recommender Systems,"00:09:17,220","00:09:22,050",125,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=557,And we have to initialize the system
cs-410_6_5_126,cs-410,6,5, Recommender Systems,"00:09:22,050","00:09:25,250",126,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=562,text exclusion or
cs-410_6_5_127,cs-410,6,5, Recommender Systems,"00:09:26,710","00:09:30,375",127,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=566,And the third model is
cs-410_6_5_128,cs-410,6,5, Recommender Systems,"00:09:30,375","00:09:35,445",128,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=570,has to be able to learn from limited
cs-410_6_5_129,cs-410,6,5, Recommender Systems,"00:09:35,445","00:09:41,100",129,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=575,counted them from the user about their
cs-410_6_5_130,cs-410,6,5, Recommender Systems,"00:09:41,100","00:09:45,702",130,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=581,If we don't deliver document
cs-410_6_5_131,cs-410,6,5, Recommender Systems,"00:09:45,702","00:09:48,900",131,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=585,be able to know whether
cs-410_6_5_132,cs-410,6,5, Recommender Systems,"00:09:50,460","00:09:55,130",132,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=590,And we had accumulate a lot of documents
cs-410_6_5_133,cs-410,6,5, Recommender Systems,"00:09:56,220","00:10:01,470",133,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=596,All these modules will have to be
cs-410_6_5_134,cs-410,6,5, Recommender Systems,"00:10:01,470","00:10:03,050",134,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=601,So how can we deal with such a system?
cs-410_6_5_135,cs-410,6,5, Recommender Systems,"00:10:03,050","00:10:05,260",135,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=603,And there are many different approaches.
cs-410_6_5_136,cs-410,6,5, Recommender Systems,"00:10:05,260","00:10:09,600",136,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=605,Here we're going to talk about
cs-410_6_5_137,cs-410,6,5, Recommender Systems,"00:10:09,600","00:10:12,120",137,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=609,a search engine for information filtering.
cs-410_6_5_138,cs-410,6,5, Recommender Systems,"00:10:12,120","00:10:15,880",138,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=612,"Again, here's why we've spent a lot of"
cs-410_6_5_139,cs-410,6,5, Recommender Systems,"00:10:15,880","00:10:20,830",139,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=615,Because it's actually not very hard
cs-410_6_5_140,cs-410,6,5, Recommender Systems,"00:10:20,830","00:10:22,320",140,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=620,information filtering.
cs-410_6_5_141,cs-410,6,5, Recommender Systems,"00:10:22,320","00:10:26,410",141,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=622,So here's the basic idea for
cs-410_6_5_142,cs-410,6,5, Recommender Systems,"00:10:26,410","00:10:27,960",142,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=626,information filtering.
cs-410_6_5_143,cs-410,6,5, Recommender Systems,"00:10:27,960","00:10:31,180",143,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=627,"First, we can reuse a lot of"
cs-410_6_5_144,cs-410,6,5, Recommender Systems,"00:10:31,180","00:10:34,950",144,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=631,"Right, so we know how to score"
cs-410_6_5_145,cs-410,6,5, Recommender Systems,"00:10:34,950","00:10:39,620",145,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=634,We're going to match the similarity
cs-410_6_5_146,cs-410,6,5, Recommender Systems,"00:10:39,620","00:10:40,930",146,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=639,a document.
cs-410_6_5_147,cs-410,6,5, Recommender Systems,"00:10:40,930","00:10:44,320",147,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=640,And then we can use a score threshold for
cs-410_6_5_148,cs-410,6,5, Recommender Systems,"00:10:44,320","00:10:49,290",148,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=644,We do retrieval and then we kind of find
cs-410_6_5_149,cs-410,6,5, Recommender Systems,"00:10:49,290","00:10:56,890",149,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=649,apply a threshold to see whether the
cs-410_6_5_150,cs-410,6,5, Recommender Systems,"00:10:56,890","00:10:58,230",150,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=656,"And if it's passing the threshold,"
cs-410_6_5_151,cs-410,6,5, Recommender Systems,"00:10:58,230","00:11:02,900",151,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=658,we're going to say it's relevant and
cs-410_6_5_152,cs-410,6,5, Recommender Systems,"00:11:02,900","00:11:08,310",152,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=662,"Another component that we have to add is,"
cs-410_6_5_153,cs-410,6,5, Recommender Systems,"00:11:08,310","00:11:13,080",153,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=668,we had used is the traditional feedback
cs-410_6_5_154,cs-410,6,5, Recommender Systems,"00:11:13,080","00:11:18,632",154,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=673,And we know rock hill can be using for
cs-410_6_5_155,cs-410,6,5, Recommender Systems,"00:11:18,632","00:11:25,008",155,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=678,"And, but we have to develop a new"
cs-410_6_5_156,cs-410,6,5, Recommender Systems,"00:11:25,008","00:11:27,279",156,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=685,And we need to set it initially and
cs-410_6_5_157,cs-410,6,5, Recommender Systems,"00:11:27,279","00:11:32,170",157,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=687,then we have to learn how to
cs-410_6_5_158,cs-410,6,5, Recommender Systems,"00:11:32,170","00:11:37,276",158,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=692,So here's what the system
cs-410_6_5_159,cs-410,6,5, Recommender Systems,"00:11:37,276","00:11:45,040",159,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=697,generalize the vector-space model for
cs-410_6_5_160,cs-410,6,5, Recommender Systems,"00:11:45,040","00:11:49,348",160,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=705,So you can see the document vector could
cs-410_6_5_161,cs-410,6,5, Recommender Systems,"00:11:49,348","00:11:53,820",161,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=709,already exists in a search engine
cs-410_6_5_162,cs-410,6,5, Recommender Systems,"00:11:53,820","00:11:58,630",162,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=713,And the profile will be treated
cs-410_6_5_163,cs-410,6,5, Recommender Systems,"00:11:58,630","00:12:02,100",163,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=718,the profile vector can be matched with
cs-410_6_5_164,cs-410,6,5, Recommender Systems,"00:12:03,130","00:12:06,960",164,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=723,And then this score would be fed into a
cs-410_6_5_165,cs-410,6,5, Recommender Systems,"00:12:06,960","00:12:13,690",165,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=726,"no, and then the evaluation would be based"
cs-410_6_5_166,cs-410,6,5, Recommender Systems,"00:12:13,690","00:12:16,870",166,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=733,If it says yes and then the document
cs-410_6_5_167,cs-410,6,5, Recommender Systems,"00:12:16,870","00:12:19,660",167,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=736,And then user could give some feedback.
cs-410_6_5_168,cs-410,6,5, Recommender Systems,"00:12:19,660","00:12:25,530",168,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=739,The feedback information would be
cs-410_6_5_169,cs-410,6,5, Recommender Systems,"00:12:25,530","00:12:28,500",169,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=745,to adjust the vector representation.
cs-410_6_5_170,cs-410,6,5, Recommender Systems,"00:12:28,500","00:12:33,150",170,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=748,So the vector learning is essentially
cs-410_6_5_171,cs-410,6,5, Recommender Systems,"00:12:33,150","00:12:36,140",171,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=753,feedback in the case of search.
cs-410_6_5_172,cs-410,6,5, Recommender Systems,"00:12:36,140","00:12:39,480",172,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=756,The threshold of learning
cs-410_6_5_173,cs-410,6,5, Recommender Systems,"00:12:39,480","00:12:42,580",173,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=759,that we need to talk
cs-410_6_5_174,cs-410,6,5, Recommender Systems,"00:12:42,580","00:12:52,580",174,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=762,[MUSIC]
cs-410_6_6_1,cs-410,6,6,  Recommender Systems,"00:00:00,012","00:00:03,574",1,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=0,[SOUND]
cs-410_6_6_2,cs-410,6,6,  Recommender Systems,"00:00:09,704","00:00:11,535",2,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=9,There are some interesting challenges
cs-410_6_6_3,cs-410,6,6,  Recommender Systems,"00:00:11,535","00:00:15,015",3,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=11,in threshold for
cs-410_6_6_4,cs-410,6,6,  Recommender Systems,"00:00:15,015","00:00:19,965",4,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=15,So here I show the historical data that
cs-410_6_6_5,cs-410,6,6,  Recommender Systems,"00:00:19,965","00:00:25,195",5,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=19,so you can see the scores and
cs-410_6_6_6,cs-410,6,6,  Recommender Systems,"00:00:25,195","00:00:30,682",6,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=25,So the first one has a score of 36.5 and
cs-410_6_6_7,cs-410,6,6,  Recommender Systems,"00:00:30,682","00:00:34,852",7,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=30,The second one is not relevant and
cs-410_6_6_8,cs-410,6,6,  Recommender Systems,"00:00:34,852","00:00:37,902",8,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=34,"Of course, we have a lot of documents for"
cs-410_6_6_9,cs-410,6,6,  Recommender Systems,"00:00:37,902","00:00:40,910",9,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=37,because we have never
cs-410_6_6_10,cs-410,6,6,  Recommender Systems,"00:00:40,910","00:00:42,060",10,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=40,"So as you can see here,"
cs-410_6_6_11,cs-410,6,6,  Recommender Systems,"00:00:42,060","00:00:47,380",11,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=42,we only see the judgements of
cs-410_6_6_12,cs-410,6,6,  Recommender Systems,"00:00:47,380","00:00:52,100",12,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=47,"So this is not a random sample,"
cs-410_6_6_13,cs-410,6,6,  Recommender Systems,"00:00:52,100","00:00:56,930",13,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=52,"It's kind of biased, so that creates"
cs-410_6_6_14,cs-410,6,6,  Recommender Systems,"00:00:58,366","00:01:04,230",14,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=58,"Secondly, there are in general very little"
cs-410_6_6_15,cs-410,6,6,  Recommender Systems,"00:01:04,230","00:01:07,920",15,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=64,so it's also challenging for
cs-410_6_6_16,cs-410,6,6,  Recommender Systems,"00:01:07,920","00:01:12,550",16,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=67,typically they require more training data.
cs-410_6_6_17,cs-410,6,6,  Recommender Systems,"00:01:13,830","00:01:17,560",17,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=73,And in the extreme case at
cs-410_6_6_18,cs-410,6,6,  Recommender Systems,"00:01:17,560","00:01:18,550",18,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=77,labeled data as well.
cs-410_6_6_19,cs-410,6,6,  Recommender Systems,"00:01:18,550","00:01:20,940",19,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=78,"The system there has to make a decision,"
cs-410_6_6_20,cs-410,6,6,  Recommender Systems,"00:01:20,940","00:01:24,440",20,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=80,so that's a very difficult
cs-410_6_6_21,cs-410,6,6,  Recommender Systems,"00:01:24,440","00:01:29,348",21,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=84,"Finally, there is also this issue of"
cs-410_6_6_22,cs-410,6,6,  Recommender Systems,"00:01:29,348","00:01:34,987",22,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=89,"Now, this means we also want"
cs-410_6_6_23,cs-410,6,6,  Recommender Systems,"00:01:34,987","00:01:39,983",23,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=94,space a little bit and
cs-410_6_6_24,cs-410,6,6,  Recommender Systems,"00:01:39,983","00:01:45,899",24,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=99,interested in documents that
cs-410_6_6_25,cs-410,6,6,  Recommender Systems,"00:01:45,899","00:01:51,330",25,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=105,"So in other words, we're going to"
cs-410_6_6_26,cs-410,6,6,  Recommender Systems,"00:01:51,330","00:01:54,890",26,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=111,by testing whether the user might be
cs-410_6_6_27,cs-410,6,6,  Recommender Systems,"00:01:56,550","00:02:00,530",27,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=116,currently are not matching
cs-410_6_6_28,cs-410,6,6,  Recommender Systems,"00:02:01,660","00:02:02,650",28,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=121,So how do we do that?
cs-410_6_6_29,cs-410,6,6,  Recommender Systems,"00:02:02,650","00:02:06,580",29,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=122,"Well, we could lower the threshold"
cs-410_6_6_30,cs-410,6,6,  Recommender Systems,"00:02:06,580","00:02:11,260",30,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=126,deliver some near misses to the user
cs-410_6_6_31,cs-410,6,6,  Recommender Systems,"00:02:13,160","00:02:18,870",31,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=133,to see how the user would
cs-410_6_6_32,cs-410,6,6,  Recommender Systems,"00:02:20,540","00:02:24,920",32,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=140,"And this is a tradeoff, because on"
cs-410_6_6_33,cs-410,6,6,  Recommender Systems,"00:02:24,920","00:02:28,130",33,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=144,"on the other hand,"
cs-410_6_6_34,cs-410,6,6,  Recommender Systems,"00:02:28,130","00:02:31,960",34,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=148,because then you will over
cs-410_6_6_35,cs-410,6,6,  Recommender Systems,"00:02:31,960","00:02:36,310",35,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=151,So exploitation means you would
cs-410_6_6_36,cs-410,6,6,  Recommender Systems,"00:02:36,310","00:02:39,790",36,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=156,Let's say you know the user is
cs-410_6_6_37,cs-410,6,6,  Recommender Systems,"00:02:39,790","00:02:42,950",37,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=159,"you don't want to deviate that much, but"
cs-410_6_6_38,cs-410,6,6,  Recommender Systems,"00:02:42,950","00:02:47,220",38,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=162,if you don't deviate at all then you don't
cs-410_6_6_39,cs-410,6,6,  Recommender Systems,"00:02:47,220","00:02:50,710",39,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=167,You might miss opportunity to learn
cs-410_6_6_40,cs-410,6,6,  Recommender Systems,"00:02:51,930","00:02:53,700",40,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=171,So this is a dilemma.
cs-410_6_6_41,cs-410,6,6,  Recommender Systems,"00:02:54,790","00:02:57,710",41,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=174,And that's also a difficulty
cs-410_6_6_42,cs-410,6,6,  Recommender Systems,"00:02:58,890","00:03:00,320",42,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=178,"Now, how do we solve these problems?"
cs-410_6_6_43,cs-410,6,6,  Recommender Systems,"00:03:00,320","00:03:04,499",43,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=180,"In general, I think one can use the"
cs-410_6_6_44,cs-410,6,6,  Recommender Systems,"00:03:04,499","00:03:09,611",44,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=184,And this strategy is basically to optimize
cs-410_6_6_45,cs-410,6,6,  Recommender Systems,"00:03:09,611","00:03:12,480",45,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=189,just as you have seen
cs-410_6_6_46,cs-410,6,6,  Recommender Systems,"00:03:12,480","00:03:16,610",46,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=192,"Right, so you can just compute"
cs-410_6_6_47,cs-410,6,6,  Recommender Systems,"00:03:16,610","00:03:18,950",47,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=196,each candidate score threshold.
cs-410_6_6_48,cs-410,6,6,  Recommender Systems,"00:03:18,950","00:03:21,830",48,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=198,"Pretend that, what if I cut at this point."
cs-410_6_6_49,cs-410,6,6,  Recommender Systems,"00:03:21,830","00:03:27,090",49,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=201,What if I cut at the different scoring
cs-410_6_6_50,cs-410,6,6,  Recommender Systems,"00:03:27,090","00:03:28,900",50,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=207,What's utility?
cs-410_6_6_51,cs-410,6,6,  Recommender Systems,"00:03:28,900","00:03:34,030",51,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=208,"Since these are training data,"
cs-410_6_6_52,cs-410,6,6,  Recommender Systems,"00:03:34,030","00:03:38,440",52,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=214,"and we know that relevant status,"
cs-410_6_6_53,cs-410,6,6,  Recommender Systems,"00:03:38,440","00:03:43,220",53,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=218,relevant status based on
cs-410_6_6_54,cs-410,6,6,  Recommender Systems,"00:03:43,220","00:03:47,190",54,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=223,So then we can just choose the threshold
cs-410_6_6_55,cs-410,6,6,  Recommender Systems,"00:03:47,190","00:03:49,810",55,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=227,on the training data.
cs-410_6_6_56,cs-410,6,6,  Recommender Systems,"00:03:49,810","00:03:55,160",56,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=229,"But this of course, doesn't account for"
cs-410_6_6_57,cs-410,6,6,  Recommender Systems,"00:03:56,870","00:04:00,400",57,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=236,And there is also the difficulty of
cs-410_6_6_58,cs-410,6,6,  Recommender Systems,"00:04:01,530","00:04:07,300",58,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=241,"So, in general, we can only get the upper"
cs-410_6_6_59,cs-410,6,6,  Recommender Systems,"00:04:07,300","00:04:13,190",59,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=247,because the threshold might
cs-410_6_6_60,cs-410,6,6,  Recommender Systems,"00:04:13,190","00:04:17,115",60,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=253,"So, it's possible that this could"
cs-410_6_6_61,cs-410,6,6,  Recommender Systems,"00:04:17,115","00:04:18,610",61,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=257,interesting to the user.
cs-410_6_6_62,cs-410,6,6,  Recommender Systems,"00:04:19,790","00:04:21,400",62,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=259,So how do we solve this problem?
cs-410_6_6_63,cs-410,6,6,  Recommender Systems,"00:04:21,400","00:04:22,896",63,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=261,"Well, we generally, and"
cs-410_6_6_64,cs-410,6,6,  Recommender Systems,"00:04:22,896","00:04:27,190",64,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=262,as I said we can low with this
cs-410_6_6_65,cs-410,6,6,  Recommender Systems,"00:04:27,190","00:04:30,760",65,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=267,So here's on particular approach
cs-410_6_6_66,cs-410,6,6,  Recommender Systems,"00:04:30,760","00:04:32,680",66,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=270,So the idea is falling.
cs-410_6_6_67,cs-410,6,6,  Recommender Systems,"00:04:32,680","00:04:37,400",67,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=272,So here I show a ranked list of all the
cs-410_6_6_68,cs-410,6,6,  Recommender Systems,"00:04:37,400","00:04:40,610",68,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=277,"far, and"
cs-410_6_6_69,cs-410,6,6,  Recommender Systems,"00:04:40,610","00:04:44,680",69,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=280,"And on the y axis we show the utility,"
cs-410_6_6_70,cs-410,6,6,  Recommender Systems,"00:04:44,680","00:04:48,670",70,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=284,how you specify the coefficients
cs-410_6_6_71,cs-410,6,6,  Recommender Systems,"00:04:48,670","00:04:53,160",71,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=288,"we can then imagine, that depending on the"
cs-410_6_6_72,cs-410,6,6,  Recommender Systems,"00:04:54,930","00:04:59,828",72,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=294,Suppose I cut at this position and
cs-410_6_6_73,cs-410,6,6,  Recommender Systems,"00:04:59,828","00:05:06,690",73,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=299,"For example,"
cs-410_6_6_74,cs-410,6,6,  Recommender Systems,"00:05:06,690","00:05:11,640",74,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=306,"The optimal point,"
cs-410_6_6_75,cs-410,6,6,  Recommender Systems,"00:05:11,640","00:05:16,355",75,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=311,when it will achieve the maximum utility
cs-410_6_6_76,cs-410,6,6,  Recommender Systems,"00:05:17,510","00:05:23,097",76,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=317,And there is also zero utility threshold.
cs-410_6_6_77,cs-410,6,6,  Recommender Systems,"00:05:23,097","00:05:27,720",77,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=323,You can see at this cutoff
cs-410_6_6_78,cs-410,6,6,  Recommender Systems,"00:05:27,720","00:05:28,740",78,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=327,What does that mean?
cs-410_6_6_79,cs-410,6,6,  Recommender Systems,"00:05:28,740","00:05:34,250",79,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=328,That means if I lower the threshold
cs-410_6_6_80,cs-410,6,6,  Recommender Systems,"00:05:34,250","00:05:41,305",80,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=334,The utility would be lower but
cs-410_6_6_81,cs-410,6,6,  Recommender Systems,"00:05:41,305","00:05:45,835",81,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=341,So it's not as high as
cs-410_6_6_82,cs-410,6,6,  Recommender Systems,"00:05:45,835","00:05:51,492",82,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=345,But it gives us as a safe point
cs-410_6_6_83,cs-410,6,6,  Recommender Systems,"00:05:51,492","00:05:56,052",83,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=351,"as I have explained, it's desirable"
cs-410_6_6_84,cs-410,6,6,  Recommender Systems,"00:05:56,052","00:06:00,622",84,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=356,So it's desirable to lower the threshold
cs-410_6_6_85,cs-410,6,6,  Recommender Systems,"00:06:00,622","00:06:04,850",85,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=360,"So that means, in general, we want to set"
cs-410_6_6_86,cs-410,6,6,  Recommender Systems,"00:06:04,850","00:06:06,730",86,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=364,Let's say we can use the alpha to control
cs-410_6_6_87,cs-410,6,6,  Recommender Systems,"00:06:08,310","00:06:13,210",87,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=368,the deviation from
cs-410_6_6_88,cs-410,6,6,  Recommender Systems,"00:06:13,210","00:06:16,570",88,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=373,So you can see the formula of the
cs-410_6_6_89,cs-410,6,6,  Recommender Systems,"00:06:16,570","00:06:21,210",89,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=376,of the zero utility threshold and
cs-410_6_6_90,cs-410,6,6,  Recommender Systems,"00:06:22,490","00:06:25,600",90,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=382,"Now, the question is,"
cs-410_6_6_91,cs-410,6,6,  Recommender Systems,"00:06:27,420","00:06:31,880",91,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=387,And when should we deviate more
cs-410_6_6_92,cs-410,6,6,  Recommender Systems,"00:06:33,720","00:06:38,450",92,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=393,"Well, this can depend on multiple factors,"
cs-410_6_6_93,cs-410,6,6,  Recommender Systems,"00:06:38,450","00:06:43,880",93,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=398,encourage this threshold
cs-410_6_6_94,cs-410,6,6,  Recommender Systems,"00:06:43,880","00:06:48,630",94,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=403,"up to the zero point, and"
cs-410_6_6_95,cs-410,6,6,  Recommender Systems,"00:06:48,630","00:06:52,990",95,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=408,we're not going to necessarily reach
cs-410_6_6_96,cs-410,6,6,  Recommender Systems,"00:06:52,990","00:06:57,947",96,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=412,"Rather, we're going to use other"
cs-410_6_6_97,cs-410,6,6,  Recommender Systems,"00:06:57,947","00:07:01,030",97,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=417,this specifically is as follows.
cs-410_6_6_98,cs-410,6,6,  Recommender Systems,"00:07:01,030","00:07:06,680",98,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=421,So there will be a beta parameter to
cs-410_6_6_99,cs-410,6,6,  Recommender Systems,"00:07:06,680","00:07:12,000",99,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=426,threshold and this can be based on can
cs-410_6_6_100,cs-410,6,6,  Recommender Systems,"00:07:12,000","00:07:17,960",100,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=432,"to the training data let's say, and so"
cs-410_6_6_101,cs-410,6,6,  Recommender Systems,"00:07:17,960","00:07:20,500",101,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=437,But what's more interesting
cs-410_6_6_102,cs-410,6,6,  Recommender Systems,"00:07:20,500","00:07:25,510",102,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=440,"Here, and you can see in this formula,"
cs-410_6_6_103,cs-410,6,6,  Recommender Systems,"00:07:25,510","00:07:31,134",103,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=445,gamma is controlling the inference
cs-410_6_6_104,cs-410,6,6,  Recommender Systems,"00:07:31,134","00:07:36,210",104,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=451,of the number of examples
cs-410_6_6_105,cs-410,6,6,  Recommender Systems,"00:07:36,210","00:07:43,340",105,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=456,So you can see in this formula as N which
cs-410_6_6_106,cs-410,6,6,  Recommender Systems,"00:07:43,340","00:07:50,820",106,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=463,"becomes bigger, then it would"
cs-410_6_6_107,cs-410,6,6,  Recommender Systems,"00:07:50,820","00:07:55,140",107,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=470,"In other words, when these very"
cs-410_6_6_108,cs-410,6,6,  Recommender Systems,"00:07:55,140","00:07:59,630",108,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=475,And that just means if we have seen few
cs-410_6_6_109,cs-410,6,6,  Recommender Systems,"00:07:59,630","00:08:04,330",109,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=479,examples we're not sure whether we
cs-410_6_6_110,cs-410,6,6,  Recommender Systems,"00:08:04,330","00:08:09,590",110,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=484,So we need to explore but as we have
cs-410_6_6_111,cs-410,6,6,  Recommender Systems,"00:08:09,590","00:08:13,510",111,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=489,many that have we feel that we
cs-410_6_6_112,cs-410,6,6,  Recommender Systems,"00:08:13,510","00:08:17,950",112,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=493,So this gives us a beta gamma for
cs-410_6_6_113,cs-410,6,6,  Recommender Systems,"00:08:17,950","00:08:21,500",113,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=497,The more examples we have seen
cs-410_6_6_114,cs-410,6,6,  Recommender Systems,"00:08:21,500","00:08:25,960",114,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=501,So the threshold would be closer
cs-410_6_6_115,cs-410,6,6,  Recommender Systems,"00:08:25,960","00:08:28,490",115,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=505,that's the basic idea of this approach.
cs-410_6_6_116,cs-410,6,6,  Recommender Systems,"00:08:28,490","00:08:34,120",116,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=508,This approach actually has been working
cs-410_6_6_117,cs-410,6,6,  Recommender Systems,"00:08:34,120","00:08:36,030",117,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=514,particularly effective.
cs-410_6_6_118,cs-410,6,6,  Recommender Systems,"00:08:36,030","00:08:42,300",118,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=516,And also can work on arbitrary utility
cs-410_6_6_119,cs-410,6,6,  Recommender Systems,"00:08:43,710","00:08:48,020",119,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=523,And explicitly addresses
cs-410_6_6_120,cs-410,6,6,  Recommender Systems,"00:08:48,020","00:08:53,234",120,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=528,it kind of uses the zero utility
cs-410_6_6_121,cs-410,6,6,  Recommender Systems,"00:08:53,234","00:08:56,810",121,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=533,exploration-exploitation tradeoff.
cs-410_6_6_122,cs-410,6,6,  Recommender Systems,"00:08:56,810","00:09:02,770",122,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=536,We're not never going to explore
cs-410_6_6_123,cs-410,6,6,  Recommender Systems,"00:09:02,770","00:09:05,530",123,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=542,"So if you take the analogy of gambling,"
cs-410_6_6_124,cs-410,6,6,  Recommender Systems,"00:09:05,530","00:09:08,950",124,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=545,you don't want to risk on losing money.
cs-410_6_6_125,cs-410,6,6,  Recommender Systems,"00:09:08,950","00:09:12,140",125,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=548,"So it's a safe spend, really"
cs-410_6_6_126,cs-410,6,6,  Recommender Systems,"00:09:13,270","00:09:18,250",126,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=553,"And the problem is of course,"
cs-410_6_6_127,cs-410,6,6,  Recommender Systems,"00:09:18,250","00:09:23,643",127,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=558,the zero utility lower boundary is also
cs-410_6_6_128,cs-410,6,6,  Recommender Systems,"00:09:23,643","00:09:28,855",128,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=563,"course, more advance in machine learning"
cs-410_6_6_129,cs-410,6,6,  Recommender Systems,"00:09:28,855","00:09:33,815",129,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=568,solving this problems and
cs-410_6_6_130,cs-410,6,6,  Recommender Systems,"00:09:35,225","00:09:41,550",130,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=575,"So to summarize, there are two"
cs-410_6_6_131,cs-410,6,6,  Recommender Systems,"00:09:41,550","00:09:47,070",131,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=581,"filtering systems, one is content based,"
cs-410_6_6_132,cs-410,6,6,  Recommender Systems,"00:09:47,070","00:09:51,302",132,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=587,and the other is collaborative filtering
cs-410_6_6_133,cs-410,6,6,  Recommender Systems,"00:09:51,302","00:09:56,710",133,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=591,We've covered content-based
cs-410_6_6_134,cs-410,6,6,  Recommender Systems,"00:09:56,710","00:09:59,566",134,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=596,"In the next lecture, we will talk"
cs-410_6_6_135,cs-410,6,6,  Recommender Systems,"00:09:59,566","00:10:07,030",135,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=599,"In content-based filtering system,"
cs-410_6_6_136,cs-410,6,6,  Recommender Systems,"00:10:07,030","00:10:11,750",136,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=607,several problems relative to
cs-410_6_6_137,cs-410,6,6,  Recommender Systems,"00:10:11,750","00:10:17,130",137,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=611,And such a system can actually be
cs-410_6_6_138,cs-410,6,6,  Recommender Systems,"00:10:17,130","00:10:22,978",138,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=617,by adding a threshold mechanism and
cs-410_6_6_139,cs-410,6,6,  Recommender Systems,"00:10:22,978","00:10:28,011",139,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=622,allow the system to learn from
cs-410_6_6_140,cs-410,6,6,  Recommender Systems,"00:10:30,357","00:10:40,357",140,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=630,[MUSIC]
cs-410_6_7_1,cs-410,6,7, Recommender Systems,"00:00:07,400","00:00:09,600",1,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=7,This lecture is about
cs-410_6_7_2,cs-410,6,7, Recommender Systems,"00:00:11,540","00:00:16,250",2,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=11,In this lecture we're going to continue
cs-410_6_7_3,cs-410,6,7, Recommender Systems,"00:00:16,250","00:00:21,390",3,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=16,"In particular, we're going to look at"
cs-410_6_7_4,cs-410,6,7, Recommender Systems,"00:00:21,390","00:00:25,710",4,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=21,You have seen this slide before when
cs-410_6_7_5,cs-410,6,7, Recommender Systems,"00:00:25,710","00:00:30,310",5,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=25,"answer the basic question,"
cs-410_6_7_6,cs-410,6,7, Recommender Systems,"00:00:30,310","00:00:31,290",6,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=30,"In the previous lecture,"
cs-410_6_7_7,cs-410,6,7, Recommender Systems,"00:00:31,290","00:00:36,180",7,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=31,"we looked at the item similarity,"
cs-410_6_7_8,cs-410,6,7, Recommender Systems,"00:00:36,180","00:00:39,580",8,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=36,"In this lecture, we're going to"
cs-410_6_7_9,cs-410,6,7, Recommender Systems,"00:00:39,580","00:00:42,490",9,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=39,"This is a different strategy,"
cs-410_6_7_10,cs-410,6,7, Recommender Systems,"00:00:44,090","00:00:45,630",10,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=44,"So first, what is collaborative filtering?"
cs-410_6_7_11,cs-410,6,7, Recommender Systems,"00:00:47,460","00:00:49,525",11,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=47,It is to make filtering decisions for
cs-410_6_7_12,cs-410,6,7, Recommender Systems,"00:00:49,525","00:00:52,660",12,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=49,individual user based on
cs-410_6_7_13,cs-410,6,7, Recommender Systems,"00:00:54,240","00:00:58,000",13,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=54,And that is to say we will
cs-410_6_7_14,cs-410,6,7, Recommender Systems,"00:00:58,000","00:01:02,080",14,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=58,preferences from that
cs-410_6_7_15,cs-410,6,7, Recommender Systems,"00:01:02,080","00:01:04,530",15,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=62,So the general idea is the following.
cs-410_6_7_16,cs-410,6,7, Recommender Systems,"00:01:04,530","00:01:11,693",16,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=64,"Given a user u, we're going to first"
cs-410_6_7_17,cs-410,6,7, Recommender Systems,"00:01:11,693","00:01:15,581",17,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=71,And then we're going to
cs-410_6_7_18,cs-410,6,7, Recommender Systems,"00:01:15,581","00:01:20,540",18,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=75,based on the preferences of
cs-410_6_7_19,cs-410,6,7, Recommender Systems,"00:01:22,390","00:01:26,960",19,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=82,"Now, the user similarity here can"
cs-410_6_7_20,cs-410,6,7, Recommender Systems,"00:01:26,960","00:01:29,610",20,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=86,the preferences on a common set of items.
cs-410_6_7_21,cs-410,6,7, Recommender Systems,"00:01:31,070","00:01:36,020",21,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=91,Now here you can see the exact
cs-410_6_7_22,cs-410,6,7, Recommender Systems,"00:01:36,020","00:01:40,430",22,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=96,We're going to look at the only the
cs-410_6_7_23,cs-410,6,7, Recommender Systems,"00:01:41,730","00:01:44,120",23,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=101,So this means this
cs-410_6_7_24,cs-410,6,7, Recommender Systems,"00:01:44,120","00:01:49,450",24,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=104,"It can be applied to any items,"
cs-410_6_7_25,cs-410,6,7, Recommender Systems,"00:01:49,450","00:01:53,700",25,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=109,So this approach would work well
cs-410_6_7_26,cs-410,6,7, Recommender Systems,"00:01:53,700","00:01:59,230",26,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=113,"First, users with the same interest"
cs-410_6_7_27,cs-410,6,7, Recommender Systems,"00:01:59,230","00:02:03,570",27,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=119,"Second, the users with similar preferences"
cs-410_6_7_28,cs-410,6,7, Recommender Systems,"00:02:03,570","00:02:08,650",28,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=123,"So for example, if the interest of"
cs-410_6_7_29,cs-410,6,7, Recommender Systems,"00:02:08,650","00:02:12,960",29,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=128,then we can infer the user
cs-410_6_7_30,cs-410,6,7, Recommender Systems,"00:02:14,280","00:02:17,270",30,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=134,So those who are interested in
cs-410_6_7_31,cs-410,6,7, Recommender Systems,"00:02:17,270","00:02:19,840",31,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=137,probably all favor SIGIR papers.
cs-410_6_7_32,cs-410,6,7, Recommender Systems,"00:02:19,840","00:02:21,880",32,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=139,That's an assumption that we make.
cs-410_6_7_33,cs-410,6,7, Recommender Systems,"00:02:21,880","00:02:23,440",33,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=141,"And if this assumption is true,"
cs-410_6_7_34,cs-410,6,7, Recommender Systems,"00:02:23,440","00:02:27,490",34,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=143,then it would help collaborative
cs-410_6_7_35,cs-410,6,7, Recommender Systems,"00:02:27,490","00:02:34,055",35,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=147,We can also assume that if we see
cs-410_6_7_36,cs-410,6,7, Recommender Systems,"00:02:34,055","00:02:38,215",36,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=154,then we can infer their interest
cs-410_6_7_37,cs-410,6,7, Recommender Systems,"00:02:38,215","00:02:43,025",37,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=158,"So in these simple examples,"
cs-410_6_7_38,cs-410,6,7, Recommender Systems,"00:02:43,025","00:02:48,492",38,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=163,in many cases such assumption
cs-410_6_7_39,cs-410,6,7, Recommender Systems,"00:02:48,492","00:02:52,896",39,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=168,So another assumption we have to make
cs-410_6_7_40,cs-410,6,7, Recommender Systems,"00:02:52,896","00:02:56,012",40,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=172,number of user preferences
cs-410_6_7_41,cs-410,6,7, Recommender Systems,"00:02:56,012","00:03:00,722",41,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=176,"So for example, if you see a lot"
cs-410_6_7_42,cs-410,6,7, Recommender Systems,"00:03:00,722","00:03:03,160",42,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=180,those indicate their
cs-410_6_7_43,cs-410,6,7, Recommender Systems,"00:03:03,160","00:03:06,832",43,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=183,"And if you have a lot of such data,"
cs-410_6_7_44,cs-410,6,7, Recommender Systems,"00:03:06,832","00:03:08,689",44,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=186,filtering can be very effective.
cs-410_6_7_45,cs-410,6,7, Recommender Systems,"00:03:09,960","00:03:14,680",45,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=189,"If not, there will be a problem, and"
cs-410_6_7_46,cs-410,6,7, Recommender Systems,"00:03:14,680","00:03:18,640",46,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=194,That means you don't have many
cs-410_6_7_47,cs-410,6,7, Recommender Systems,"00:03:18,640","00:03:23,722",47,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=198,the system could not fully take advantage
cs-410_6_7_48,cs-410,6,7, Recommender Systems,"00:03:23,722","00:03:28,690",48,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=203,So let's look at the filtering
cs-410_6_7_49,cs-410,6,7, Recommender Systems,"00:03:30,340","00:03:33,791",49,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=210,"So this picture shows that we are,"
cs-410_6_7_50,cs-410,6,7, Recommender Systems,"00:03:33,791","00:03:38,075",50,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=213,"in general, considering a lot of users and"
cs-410_6_7_51,cs-410,6,7, Recommender Systems,"00:03:38,075","00:03:42,956",51,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=218,"we're showing m users here, so U1 through."
cs-410_6_7_52,cs-410,6,7, Recommender Systems,"00:03:42,956","00:03:46,040",52,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=222,And we're also considering
cs-410_6_7_53,cs-410,6,7, Recommender Systems,"00:03:46,040","00:03:49,870",53,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=226,Let's say n objects in
cs-410_6_7_54,cs-410,6,7, Recommender Systems,"00:03:49,870","00:03:55,330",54,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=229,And then we will assume that
cs-410_6_7_55,cs-410,6,7, Recommender Systems,"00:03:55,330","00:04:01,510",55,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=235,objects and the user could for
cs-410_6_7_56,cs-410,6,7, Recommender Systems,"00:04:01,510","00:04:06,490",56,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=241,"For example, those items could be movies,"
cs-410_6_7_57,cs-410,6,7, Recommender Systems,"00:04:06,490","00:04:10,500",57,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=246,then the users would give
cs-410_6_7_58,cs-410,6,7, Recommender Systems,"00:04:10,500","00:04:14,829",58,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=250,So what you see here is that we have
cs-410_6_7_59,cs-410,6,7, Recommender Systems,"00:04:14,829","00:04:16,231",59,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=254,some combinations.
cs-410_6_7_60,cs-410,6,7, Recommender Systems,"00:04:16,231","00:04:21,751",60,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=256,"So some users have watched some movies,"
cs-410_6_7_61,cs-410,6,7, Recommender Systems,"00:04:21,751","00:04:26,075",61,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=261,they obviously won't be able
cs-410_6_7_62,cs-410,6,7, Recommender Systems,"00:04:26,075","00:04:30,040",62,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=266,some users may actually
cs-410_6_7_63,cs-410,6,7, Recommender Systems,"00:04:30,040","00:04:34,410",63,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=270,So this is in general a small symmetrics.
cs-410_6_7_64,cs-410,6,7, Recommender Systems,"00:04:34,410","00:04:38,030",64,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=274,So many items and
cs-410_6_7_65,cs-410,6,7, Recommender Systems,"00:04:39,160","00:04:46,070",65,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=279,And what's interesting here is we
cs-410_6_7_66,cs-410,6,7, Recommender Systems,"00:04:46,070","00:04:51,780",66,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=286,of an element in this matrix
cs-410_6_7_67,cs-410,6,7, Recommender Systems,"00:04:51,780","00:04:56,610",67,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=291,And that's after the essential question
cs-410_6_7_68,cs-410,6,7, Recommender Systems,"00:04:56,610","00:04:59,950",68,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=296,we assume there's an unknown
cs-410_6_7_69,cs-410,6,7, Recommender Systems,"00:04:59,950","00:05:04,400",69,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=299,That would map a pair of user and
cs-410_6_7_70,cs-410,6,7, Recommender Systems,"00:05:04,400","00:05:07,610",70,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=304,And we have observed the sum
cs-410_6_7_71,cs-410,6,7, Recommender Systems,"00:05:08,960","00:05:14,296",71,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=308,And we want to infer the value
cs-410_6_7_72,cs-410,6,7, Recommender Systems,"00:05:14,296","00:05:20,168",72,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=314,other pairs that don't have
cs-410_6_7_73,cs-410,6,7, Recommender Systems,"00:05:20,168","00:05:26,198",73,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=320,So this is very similar to other
cs-410_6_7_74,cs-410,6,7, Recommender Systems,"00:05:26,198","00:05:31,784",74,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=326,know the values of the function
cs-410_6_7_75,cs-410,6,7, Recommender Systems,"00:05:31,784","00:05:37,384",75,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=331,And we hope to predict the values of
cs-410_6_7_76,cs-410,6,7, Recommender Systems,"00:05:37,384","00:05:40,344",76,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=337,this is a function approximation.
cs-410_6_7_77,cs-410,6,7, Recommender Systems,"00:05:40,344","00:05:47,440",77,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=340,And how can we pick out the function
cs-410_6_7_78,cs-410,6,7, Recommender Systems,"00:05:47,440","00:05:50,230",78,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=347,So this is the setup.
cs-410_6_7_79,cs-410,6,7, Recommender Systems,"00:05:50,230","00:05:54,680",79,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=350,Now there are many approaches
cs-410_6_7_80,cs-410,6,7, Recommender Systems,"00:05:54,680","00:06:00,415",80,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=354,"In fact,"
cs-410_6_7_81,cs-410,6,7, Recommender Systems,"00:06:00,415","00:06:09,095",81,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=360,reason that there are special
cs-410_6_7_82,cs-410,6,7, Recommender Systems,"00:06:10,419","00:06:15,730",82,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=370,major conference devoted to the problem.
cs-410_6_7_83,cs-410,6,7, Recommender Systems,"00:06:15,730","00:06:20,199",83,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=375,[MUSIC]
cs-410_6_8_1,cs-410,6,8, Recommender Systems,"00:00:00,012","00:00:09,135",1,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=0,[SOUND]
cs-410_6_8_2,cs-410,6,8, Recommender Systems,"00:00:09,135","00:00:12,960",2,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=9,here we're going to talk
cs-410_6_8_3,cs-410,6,8, Recommender Systems,"00:00:12,960","00:00:18,430",3,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=12,And that would be based on
cs-410_6_8_4,cs-410,6,8, Recommender Systems,"00:00:18,430","00:00:23,220",4,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=18,then predicting the rating of and
cs-410_6_8_5,cs-410,6,8, Recommender Systems,"00:00:23,220","00:00:32,540",5,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=23,object by an active user using the ratings
cs-410_6_8_6,cs-410,6,8, Recommender Systems,"00:00:32,540","00:00:38,600",6,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=32,This is called a memory based approach
cs-410_6_8_7,cs-410,6,8, Recommender Systems,"00:00:40,120","00:00:44,460",7,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=40,storing all the user information and
cs-410_6_8_8,cs-410,6,8, Recommender Systems,"00:00:44,460","00:00:49,713",8,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=44,when we are considering a particular
cs-410_6_8_9,cs-410,6,8, Recommender Systems,"00:00:49,713","00:00:56,210",9,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=49,retrieve the rating users or
cs-410_6_8_10,cs-410,6,8, Recommender Systems,"00:00:56,210","00:01:01,140",10,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=56,And then try to use this
cs-410_6_8_11,cs-410,6,8, Recommender Systems,"00:01:01,140","00:01:05,120",11,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=61,to predict the preference of this user.
cs-410_6_8_12,cs-410,6,8, Recommender Systems,"00:01:05,120","00:01:11,700",12,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=65,So here is the general idea and
cs-410_6_8_13,cs-410,6,8, Recommender Systems,"00:01:11,700","00:01:16,570",13,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=71,x sub i j denotes the rating
cs-410_6_8_14,cs-410,6,8, Recommender Systems,"00:01:17,910","00:01:23,460",14,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=77,and n sub i is average rating
cs-410_6_8_15,cs-410,6,8, Recommender Systems,"00:01:26,100","00:01:31,050",15,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=86,So this n i is needed because
cs-410_6_8_16,cs-410,6,8, Recommender Systems,"00:01:31,050","00:01:35,500",16,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=91,we would like to normalize
cs-410_6_8_17,cs-410,6,8, Recommender Systems,"00:01:35,500","00:01:39,190",17,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=95,So how do you do normalization?
cs-410_6_8_18,cs-410,6,8, Recommender Systems,"00:01:39,190","00:01:46,440",18,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=99,"Well, we're going to just subtract"
cs-410_6_8_19,cs-410,6,8, Recommender Systems,"00:01:46,440","00:01:49,890",19,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=106,"Now, this is to normalize these ratings so"
cs-410_6_8_20,cs-410,6,8, Recommender Systems,"00:01:49,890","00:01:53,510",20,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=109,that the ratings from different
cs-410_6_8_21,cs-410,6,8, Recommender Systems,"00:01:55,590","00:02:00,220",21,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=115,"Because some users might be more generous,"
cs-410_6_8_22,cs-410,6,8, Recommender Systems,"00:02:00,220","00:02:05,160",22,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=120,ratings but some others might be
cs-410_6_8_23,cs-410,6,8, Recommender Systems,"00:02:05,160","00:02:10,850",23,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=125,cannot be directly compared with each
cs-410_6_8_24,cs-410,6,8, Recommender Systems,"00:02:10,850","00:02:13,450",24,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=130,So we need to do this normalization.
cs-410_6_8_25,cs-410,6,8, Recommender Systems,"00:02:13,450","00:02:18,420",25,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=133,Another prediction of
cs-410_6_8_26,cs-410,6,8, Recommender Systems,"00:02:18,420","00:02:22,880",26,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=138,by another user or
cs-410_6_8_27,cs-410,6,8, Recommender Systems,"00:02:24,460","00:02:29,419",27,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=144,can be based on the average
cs-410_6_8_28,cs-410,6,8, Recommender Systems,"00:02:30,630","00:02:36,960",28,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=150,So the user u sub a is the user that we
cs-410_6_8_29,cs-410,6,8, Recommender Systems,"00:02:36,960","00:02:42,400",29,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=156,And we now are interested in
cs-410_6_8_30,cs-410,6,8, Recommender Systems,"00:02:42,400","00:02:47,910",30,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=162,So we're interested in knowing how
cs-410_6_8_31,cs-410,6,8, Recommender Systems,"00:02:47,910","00:02:49,370",31,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=167,How do we know that?
cs-410_6_8_32,cs-410,6,8, Recommender Systems,"00:02:50,370","00:02:55,560",32,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=170,Where the idea here is to look at
cs-410_6_8_33,cs-410,6,8, Recommender Systems,"00:02:55,560","00:02:57,260",33,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=175,have liked this object.
cs-410_6_8_34,cs-410,6,8, Recommender Systems,"00:02:59,530","00:03:04,720",34,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=179,So mathematically this is to say
cs-410_6_8_35,cs-410,6,8, Recommender Systems,"00:03:04,720","00:03:12,130",35,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=184,"this user on this app object,"
cs-410_6_8_36,cs-410,6,8, Recommender Systems,"00:03:12,130","00:03:18,640",36,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=192,combination of the normalized
cs-410_6_8_37,cs-410,6,8, Recommender Systems,"00:03:18,640","00:03:23,950",37,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=198,"and in fact here,"
cs-410_6_8_38,cs-410,6,8, Recommender Systems,"00:03:23,950","00:03:29,180",38,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=203,But not all users contribute
cs-410_6_8_39,cs-410,6,8, Recommender Systems,"00:03:29,180","00:03:31,748",39,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=209,and this is conjured by the weights.
cs-410_6_8_40,cs-410,6,8, Recommender Systems,"00:03:31,748","00:03:37,191",40,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=211,So this weight controls the inference
cs-410_6_8_41,cs-410,6,8, Recommender Systems,"00:03:37,191","00:03:41,618",41,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=217,of the user on the prediction.
cs-410_6_8_42,cs-410,6,8, Recommender Systems,"00:03:41,618","00:03:46,763",42,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=221,"And of course,"
cs-410_6_8_43,cs-410,6,8, Recommender Systems,"00:03:46,763","00:03:51,917",43,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=226,the similarity between ua and
cs-410_6_8_44,cs-410,6,8, Recommender Systems,"00:03:51,917","00:03:57,650",44,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=231,"The more similar they are,"
cs-410_6_8_45,cs-410,6,8, Recommender Systems,"00:03:57,650","00:04:02,420",45,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=237,user ui can make in predicting
cs-410_6_8_46,cs-410,6,8, Recommender Systems,"00:04:03,950","00:04:06,060",46,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=243,"So, the formula is extremely simple."
cs-410_6_8_47,cs-410,6,8, Recommender Systems,"00:04:06,060","00:04:10,140",47,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=246,"You can see,"
cs-410_6_8_48,cs-410,6,8, Recommender Systems,"00:04:10,140","00:04:14,040",48,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=250,"And inside the sum we have their ratings,"
cs-410_6_8_49,cs-410,6,8, Recommender Systems,"00:04:14,040","00:04:17,380",49,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=254,their normalized ratings
cs-410_6_8_50,cs-410,6,8, Recommender Systems,"00:04:17,380","00:04:21,400",50,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=257,The ratings need to be normalized in
cs-410_6_8_51,cs-410,6,8, Recommender Systems,"00:04:22,690","00:04:25,739",51,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=262,And then these ratings
cs-410_6_8_52,cs-410,6,8, Recommender Systems,"00:04:26,750","00:04:33,310",52,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=266,So you can imagine w of a and i is just
cs-410_6_8_53,cs-410,6,8, Recommender Systems,"00:04:34,470","00:04:35,350",53,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=274,Now what's k here?
cs-410_6_8_54,cs-410,6,8, Recommender Systems,"00:04:35,350","00:04:39,120",54,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=275,Well k is simply a normalizer.
cs-410_6_8_55,cs-410,6,8, Recommender Systems,"00:04:39,120","00:04:45,670",55,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=279,It's just one over the sum of all
cs-410_6_8_56,cs-410,6,8, Recommender Systems,"00:04:47,860","00:04:54,680",56,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=287,"So this means, basically, if you consider"
cs-410_6_8_57,cs-410,6,8, Recommender Systems,"00:04:54,680","00:04:59,259",57,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=294,we have coefficients of weight that
cs-410_6_8_58,cs-410,6,8, Recommender Systems,"00:05:00,430","00:05:05,690",58,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=300,And it's just a normalization strategy so
cs-410_6_8_59,cs-410,6,8, Recommender Systems,"00:05:05,690","00:05:12,319",59,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=305,in the same range as these ratings
cs-410_6_8_60,cs-410,6,8, Recommender Systems,"00:05:13,650","00:05:14,560",60,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=313,Right?
cs-410_6_8_61,cs-410,6,8, Recommender Systems,"00:05:14,560","00:05:20,320",61,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=314,So this is basically the main idea
cs-410_6_8_62,cs-410,6,8, Recommender Systems,"00:05:20,320","00:05:21,270",62,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=320,collaborative filtering.
cs-410_6_8_63,cs-410,6,8, Recommender Systems,"00:05:22,750","00:05:27,880",63,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=322,"Once we make this prediction,"
cs-410_6_8_64,cs-410,6,8, Recommender Systems,"00:05:27,880","00:05:33,270",64,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=327,back through the rating that
cs-410_6_8_65,cs-410,6,8, Recommender Systems,"00:05:33,270","00:05:38,520",65,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=333,"the user would actually make,"
cs-410_6_8_66,cs-410,6,8, Recommender Systems,"00:05:38,520","00:05:44,110",66,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=338,and this is to further
cs-410_6_8_67,cs-410,6,8, Recommender Systems,"00:05:44,110","00:05:49,980",67,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=344,average rating of this user u
cs-410_6_8_68,cs-410,6,8, Recommender Systems,"00:05:49,980","00:05:54,290",68,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=349,This would recover a meaningful rating for
cs-410_6_8_69,cs-410,6,8, Recommender Systems,"00:05:54,290","00:05:59,410",69,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=354,"So if this user is generous, then"
cs-410_6_8_70,cs-410,6,8, Recommender Systems,"00:05:59,410","00:06:04,580",70,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=359,and when we add that the rating will be
cs-410_6_8_71,cs-410,6,8, Recommender Systems,"00:06:04,580","00:06:10,459",71,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=364,Now when you recommend an item to a user
cs-410_6_8_72,cs-410,6,8, Recommender Systems,"00:06:10,459","00:06:15,093",72,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=370,because you are interested in
cs-410_6_8_73,cs-410,6,8, Recommender Systems,"00:06:15,093","00:06:17,158",73,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=375,that's more meaningful.
cs-410_6_8_74,cs-410,6,8, Recommender Systems,"00:06:17,158","00:06:22,624",74,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=377,But when they evaluate these
cs-410_6_8_75,cs-410,6,8, Recommender Systems,"00:06:22,624","00:06:27,563",75,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=382,they typically assume that
cs-410_6_8_76,cs-410,6,8, Recommender Systems,"00:06:27,563","00:06:32,923",76,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=387,these objects to be unknown and
cs-410_6_8_77,cs-410,6,8, Recommender Systems,"00:06:32,923","00:06:38,938",77,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=392,then you compare the predicted
cs-410_6_8_78,cs-410,6,8, Recommender Systems,"00:06:38,938","00:06:42,020",78,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=398,"So, you do have access"
cs-410_6_8_79,cs-410,6,8, Recommender Systems,"00:06:42,020","00:06:44,100",79,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=402,"But, then you pretend that you don't know,"
cs-410_6_8_80,cs-410,6,8, Recommender Systems,"00:06:44,100","00:06:48,420",80,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=404,then you compare your systems
cs-410_6_8_81,cs-410,6,8, Recommender Systems,"00:06:48,420","00:06:54,130",81,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=408,"In that case, obviously, the systems"
cs-410_6_8_82,cs-410,6,8, Recommender Systems,"00:06:54,130","00:06:59,570",82,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=414,the actual ratings of the user and
cs-410_6_8_83,cs-410,6,8, Recommender Systems,"00:07:01,040","00:07:05,160",83,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=421,Okay so this is the memory based approach.
cs-410_6_8_84,cs-410,6,8, Recommender Systems,"00:07:05,160","00:07:07,000",84,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=425,"Now, of course,"
cs-410_6_8_85,cs-410,6,8, Recommender Systems,"00:07:07,000","00:07:09,430",85,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=427,if you want to write
cs-410_6_8_86,cs-410,6,8, Recommender Systems,"00:07:09,430","00:07:15,510",86,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=429,you still face the problem of
cs-410_6_8_87,cs-410,6,8, Recommender Systems,"00:07:15,510","00:07:20,890",87,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=435,"Once you know the w function, then"
cs-410_6_8_88,cs-410,6,8, Recommender Systems,"00:07:22,740","00:07:28,910",88,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=442,"So, indeed, there are many different ways"
cs-410_6_8_89,cs-410,6,8, Recommender Systems,"00:07:28,910","00:07:33,550",89,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=448,"w, and specific approaches generally"
cs-410_6_8_90,cs-410,6,8, Recommender Systems,"00:07:35,500","00:07:38,220",90,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=455,So here are some possibilities and
cs-410_6_8_91,cs-410,6,8, Recommender Systems,"00:07:38,220","00:07:42,010",91,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=458,you can imagine there
cs-410_6_8_92,cs-410,6,8, Recommender Systems,"00:07:42,010","00:07:46,460",92,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=462,One popular approach is we use
cs-410_6_8_93,cs-410,6,8, Recommender Systems,"00:07:48,130","00:07:52,380",93,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=468,This would be a sum over
cs-410_6_8_94,cs-410,6,8, Recommender Systems,"00:07:52,380","00:07:56,280",94,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=472,And the formula is a standard
cs-410_6_8_95,cs-410,6,8, Recommender Systems,"00:07:56,280","00:07:58,690",95,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=476,coefficient formula as shown here.
cs-410_6_8_96,cs-410,6,8, Recommender Systems,"00:08:00,060","00:08:05,300",96,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=480,So this basically measures
cs-410_6_8_97,cs-410,6,8, Recommender Systems,"00:08:05,300","00:08:10,229",97,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=485,to all give higher ratings to similar
cs-410_6_8_98,cs-410,6,8, Recommender Systems,"00:08:11,780","00:08:15,990",98,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=491,"Another measure is the cosine measure,"
cs-410_6_8_99,cs-410,6,8, Recommender Systems,"00:08:15,990","00:08:20,820",99,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=495,vectors as vectors in the vector space.
cs-410_6_8_100,cs-410,6,8, Recommender Systems,"00:08:20,820","00:08:24,400",100,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=500,"And then,"
cs-410_6_8_101,cs-410,6,8, Recommender Systems,"00:08:24,400","00:08:27,880",101,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=504,compute the cosine of
cs-410_6_8_102,cs-410,6,8, Recommender Systems,"00:08:27,880","00:08:32,590",102,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=507,And this measure has been using the vector
cs-410_6_8_103,cs-410,6,8, Recommender Systems,"00:08:32,590","00:08:36,400",103,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=512,So as you can imagine there are just
cs-410_6_8_104,cs-410,6,8, Recommender Systems,"00:08:36,400","00:08:41,330",104,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=516,"In all these cases, note that the user's"
cs-410_6_8_105,cs-410,6,8, Recommender Systems,"00:08:41,330","00:08:47,135",105,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=521,on items and we did not actually use
cs-410_6_8_106,cs-410,6,8, Recommender Systems,"00:08:47,135","00:08:51,802",106,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=527,"It didn't matter these items are,"
cs-410_6_8_107,cs-410,6,8, Recommender Systems,"00:08:51,802","00:08:55,276",107,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=531,"they can be books, they can be products,"
cs-410_6_8_108,cs-410,6,8, Recommender Systems,"00:08:55,276","00:09:00,541",108,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=535,they can be text documents which
cs-410_6_8_109,cs-410,6,8, Recommender Systems,"00:09:00,541","00:09:07,120",109,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=540,so this allows such approach to be
cs-410_6_8_110,cs-410,6,8, Recommender Systems,"00:09:07,120","00:09:08,920",110,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=547,"Now in some newer approaches of course,"
cs-410_6_8_111,cs-410,6,8, Recommender Systems,"00:09:08,920","00:09:11,830",111,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=548,we would like to use more
cs-410_6_8_112,cs-410,6,8, Recommender Systems,"00:09:11,830","00:09:18,750",112,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=551,"Clearly, we know more about the user,"
cs-410_6_8_113,cs-410,6,8, Recommender Systems,"00:09:18,750","00:09:23,659",113,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=558,"So in the actual filtering system,"
cs-410_6_8_114,cs-410,6,8, Recommender Systems,"00:09:23,659","00:09:27,820",114,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=563,we could also combine that
cs-410_6_8_115,cs-410,6,8, Recommender Systems,"00:09:27,820","00:09:34,040",115,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=567,"We could use more context information,"
cs-410_6_8_116,cs-410,6,8, Recommender Systems,"00:09:34,040","00:09:39,140",116,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=574,"that people are just starting, and"
cs-410_6_8_117,cs-410,6,8, Recommender Systems,"00:09:39,140","00:09:44,147",117,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=579,"But, this memory based approach has"
cs-410_6_8_118,cs-410,6,8, Recommender Systems,"00:09:44,147","00:09:48,750",118,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=584,and it's easy to implement in
cs-410_6_8_119,cs-410,6,8, Recommender Systems,"00:09:48,750","00:09:53,698",119,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=588,a starting point to see if the strategy
cs-410_6_8_120,cs-410,6,8, Recommender Systems,"00:09:56,108","00:10:01,305",120,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=596,"So, there are some obvious ways"
cs-410_6_8_121,cs-410,6,8, Recommender Systems,"00:10:01,305","00:10:07,070",121,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=601,mainly we would like to improve
cs-410_6_8_122,cs-410,6,8, Recommender Systems,"00:10:07,070","00:10:09,690",122,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=607,And there are some practical
cs-410_6_8_123,cs-410,6,8, Recommender Systems,"00:10:09,690","00:10:11,960",123,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=609,"So for example,"
cs-410_6_8_124,cs-410,6,8, Recommender Systems,"00:10:11,960","00:10:12,990",124,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=611,What do you do with them?
cs-410_6_8_125,cs-410,6,8, Recommender Systems,"00:10:12,990","00:10:18,060",125,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=612,"Well, you can set them to default values"
cs-410_6_8_126,cs-410,6,8, Recommender Systems,"00:10:18,060","00:10:20,310",126,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=618,And that would be a simple solution.
cs-410_6_8_127,cs-410,6,8, Recommender Systems,"00:10:20,310","00:10:26,388",127,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=620,But there are advanced approaches that
cs-410_6_8_128,cs-410,6,8, Recommender Systems,"00:10:26,388","00:10:32,878",128,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=626,"missing values, and then use predictive"
cs-410_6_8_129,cs-410,6,8, Recommender Systems,"00:10:32,878","00:10:38,880",129,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=632,So in fact that the memory based apology
cs-410_6_8_130,cs-410,6,8, Recommender Systems,"00:10:38,880","00:10:43,128",130,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=638,So you get you have iterative approach
cs-410_6_8_131,cs-410,6,8, Recommender Systems,"00:10:43,128","00:10:43,895",131,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=643,prediction and
cs-410_6_8_132,cs-410,6,8, Recommender Systems,"00:10:43,895","00:10:48,095",132,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=643,then you can use the predictive values to
cs-410_6_8_133,cs-410,6,8, Recommender Systems,"00:10:49,525","00:10:54,840",133,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=649,So this is a heuristic
cs-410_6_8_134,cs-410,6,8, Recommender Systems,"00:10:54,840","00:10:59,639",134,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=654,And the strategy obviously would affect
cs-410_6_8_135,cs-410,6,8, Recommender Systems,"00:10:59,639","00:11:04,040",135,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=659,just like any other heuristics would
cs-410_6_8_136,cs-410,6,8, Recommender Systems,"00:11:06,290","00:11:10,460",136,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=666,Another idea which is actually very
cs-410_6_8_137,cs-410,6,8, Recommender Systems,"00:11:10,460","00:11:15,150",137,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=670,have seen in text search is called
cs-410_6_8_138,cs-410,6,8, Recommender Systems,"00:11:15,150","00:11:23,980",138,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=675,Now here the idea is to look at where
cs-410_6_8_139,cs-410,6,8, Recommender Systems,"00:11:23,980","00:11:29,092",139,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=683,If the item is a popular item that
cs-410_6_8_140,cs-410,6,8, Recommender Systems,"00:11:29,092","00:11:35,110",140,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=689,seen [INAUDIBLE] to people interested
cs-410_6_8_141,cs-410,6,8, Recommender Systems,"00:11:35,110","00:11:40,620",141,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=695,"interesting but if it's a rare item,"
cs-410_6_8_142,cs-410,6,8, Recommender Systems,"00:11:40,620","00:11:44,770",142,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=700,But these two users deal with this
cs-410_6_8_143,cs-410,6,8, Recommender Systems,"00:11:44,770","00:11:47,370",143,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=704,"And, that says more"
cs-410_6_8_144,cs-410,6,8, Recommender Systems,"00:11:47,370","00:11:52,177",144,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=707,It's kind of to emphasize
cs-410_6_8_145,cs-410,6,8, Recommender Systems,"00:11:52,177","00:11:56,738",145,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=712,on items that are not
cs-410_6_8_146,cs-410,6,8, Recommender Systems,"00:11:56,738","00:12:06,738",146,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=716,[MUSIC]
cs-410_6_9_1,cs-410,6,9, Recommender Systems,"00:00:00,012","00:00:07,878",1,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=0,[SOUND]
cs-410_6_9_2,cs-410,6,9, Recommender Systems,"00:00:07,878","00:00:12,848",2,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=7,to summarize our discussion of
cs-410_6_9_3,cs-410,6,9, Recommender Systems,"00:00:12,848","00:00:16,640",3,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=12,the filtering task for
cs-410_6_9_4,cs-410,6,9, Recommender Systems,"00:00:16,640","00:00:21,020",4,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=16,"in some other sense,"
cs-410_6_9_5,cs-410,6,9, Recommender Systems,"00:00:21,020","00:00:24,230",5,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=21,So it's easy because
cs-410_6_9_6,cs-410,6,9, Recommender Systems,"00:00:24,230","00:00:30,300",6,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=24,In this case the system takes initiative
cs-410_6_9_7,cs-410,6,9, Recommender Systems,"00:00:30,300","00:00:33,230",7,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=30,"The user doesn't really make any effort,"
cs-410_6_9_8,cs-410,6,9, Recommender Systems,"00:00:33,230","00:00:36,100",8,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=33,any recommendation is better than nothing.
cs-410_6_9_9,cs-410,6,9, Recommender Systems,"00:00:36,100","00:00:41,710",9,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=36,All right.
cs-410_6_9_10,cs-410,6,9, Recommender Systems,"00:00:41,710","00:00:44,180",10,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=41,items or useless documents.
cs-410_6_9_11,cs-410,6,9, Recommender Systems,"00:00:44,180","00:00:47,220",11,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=44,If you can recommend
cs-410_6_9_12,cs-410,6,9, Recommender Systems,"00:00:47,220","00:00:52,390",12,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=47,"users generally will appreciate it,"
cs-410_6_9_13,cs-410,6,9, Recommender Systems,"00:00:52,390","00:00:56,810",13,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=52,"However, filtering is actually much harder"
cs-410_6_9_14,cs-410,6,9, Recommender Systems,"00:00:56,810","00:01:01,860",14,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=56,make a binary decision and you can't
cs-410_6_9_15,cs-410,6,9, Recommender Systems,"00:01:01,860","00:01:06,520",15,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=61,then you're going to see whether
cs-410_6_9_16,cs-410,6,9, Recommender Systems,"00:01:06,520","00:01:10,040",16,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=66,You have to make a decision
cs-410_6_9_17,cs-410,6,9, Recommender Systems,"00:01:10,040","00:01:11,260",17,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=70,Think about news filtering.
cs-410_6_9_18,cs-410,6,9, Recommender Systems,"00:01:11,260","00:01:15,060",18,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=71,As soon as you see the news enough
cs-410_6_9_19,cs-410,6,9, Recommender Systems,"00:01:15,060","00:01:16,780",19,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=75,interesting to the user.
cs-410_6_9_20,cs-410,6,9, Recommender Systems,"00:01:16,780","00:01:21,190",20,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=76,"If you wait for a few days, well, even if"
cs-410_6_9_21,cs-410,6,9, Recommender Systems,"00:01:21,190","00:01:26,690",21,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=81,"the most relevant news, the utility is"
cs-410_6_9_22,cs-410,6,9, Recommender Systems,"00:01:28,160","00:01:32,140",22,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=88,Another reason why it's hard
cs-410_6_9_23,cs-410,6,9, Recommender Systems,"00:01:32,140","00:01:34,620",23,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=92,if you think of this
cs-410_6_9_24,cs-410,6,9, Recommender Systems,"00:01:34,620","00:01:36,010",24,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=94,"Collaborative filtering, for"
cs-410_6_9_25,cs-410,6,9, Recommender Systems,"00:01:36,010","00:01:41,030",25,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=96,"example, is purely based on"
cs-410_6_9_26,cs-410,6,9, Recommender Systems,"00:01:41,030","00:01:48,120",26,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=101,So if you don't have many ratings there's
cs-410_6_9_27,cs-410,6,9, Recommender Systems,"00:01:48,120","00:01:51,470",27,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=108,And yeah I just mentioned
cs-410_6_9_28,cs-410,6,9, Recommender Systems,"00:01:51,470","00:01:54,450",28,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=111,"This is actually a very serious,"
cs-410_6_9_29,cs-410,6,9, Recommender Systems,"00:01:54,450","00:01:59,180",29,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=114,But of course there are strategies that
cs-410_6_9_30,cs-410,6,9, Recommender Systems,"00:02:00,680","00:02:04,930",30,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=120,and there are different strategies that
cs-410_6_9_31,cs-410,6,9, Recommender Systems,"00:02:04,930","00:02:09,620",31,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=124,"You can use, for example, more user"
cs-410_6_9_32,cs-410,6,9, Recommender Systems,"00:02:09,620","00:02:14,470",32,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=129,instead of using the preferences
cs-410_6_9_33,cs-410,6,9, Recommender Systems,"00:02:14,470","00:02:19,000",33,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=134,items give me additional information
cs-410_6_9_34,cs-410,6,9, Recommender Systems,"00:02:21,110","00:02:26,840",34,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=141,And we also talk about two strategies for
cs-410_6_9_35,cs-410,6,9, Recommender Systems,"00:02:26,840","00:02:30,140",35,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=146,One is content-based where
cs-410_6_9_36,cs-410,6,9, Recommender Systems,"00:02:30,140","00:02:34,670",36,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=150,is collaborative filtering where
cs-410_6_9_37,cs-410,6,9, Recommender Systems,"00:02:34,670","00:02:37,990",37,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=154,And they obviously can be
cs-410_6_9_38,cs-410,6,9, Recommender Systems,"00:02:37,990","00:02:41,480",38,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=157,You can imagine they generally
cs-410_6_9_39,cs-410,6,9, Recommender Systems,"00:02:41,480","00:02:46,166",39,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=161,So that would give us a hybrid
cs-410_6_9_40,cs-410,6,9, Recommender Systems,"00:02:46,166","00:02:52,620",40,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=166,And we also could recall that we talked
cs-410_6_9_41,cs-410,6,9, Recommender Systems,"00:02:52,620","00:02:58,470",41,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=172,about push versus pull as two strategies
cs-410_6_9_42,cs-410,6,9, Recommender Systems,"00:02:58,470","00:03:03,110",42,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=178,And recommender system easy to
cs-410_6_9_43,cs-410,6,9, Recommender Systems,"00:03:03,110","00:03:06,650",43,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=183,search engines are serving
cs-410_6_9_44,cs-410,6,9, Recommender Systems,"00:03:06,650","00:03:09,740",44,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=186,"Obviously the two should be combined,"
cs-410_6_9_45,cs-410,6,9, Recommender Systems,"00:03:09,740","00:03:13,400",45,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=189,The two have a system
cs-410_6_9_46,cs-410,6,9, Recommender Systems,"00:03:13,400","00:03:16,600",46,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=193,with multiple mode information access.
cs-410_6_9_47,cs-410,6,9, Recommender Systems,"00:03:16,600","00:03:22,870",47,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=196,So in the future we could anticipate such
cs-410_6_9_48,cs-410,6,9, Recommender Systems,"00:03:22,870","00:03:27,570",48,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=202,"And either,"
cs-410_6_9_49,cs-410,6,9, Recommender Systems,"00:03:27,570","00:03:33,740",49,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=207,there are a lot of new algorithms
cs-410_6_9_50,cs-410,6,9, Recommender Systems,"00:03:33,740","00:03:39,070",50,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=213,In particular those new algorithms tend
cs-410_6_9_51,cs-410,6,9, Recommender Systems,"00:03:39,070","00:03:42,850",51,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=219,Now the context here could be
cs-410_6_9_52,cs-410,6,9, Recommender Systems,"00:03:42,850","00:03:44,920",52,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=222,could also be the context of the user.
cs-410_6_9_53,cs-410,6,9, Recommender Systems,"00:03:44,920","00:03:45,750",53,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=224,Items.
cs-410_6_9_54,cs-410,6,9, Recommender Systems,"00:03:45,750","00:03:47,570",54,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=225,The items are not the isolated.
cs-410_6_9_55,cs-410,6,9, Recommender Systems,"00:03:47,570","00:03:50,290",55,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=227,They're connected in many ways.
cs-410_6_9_56,cs-410,6,9, Recommender Systems,"00:03:50,290","00:03:54,590",56,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=230,The users might form
cs-410_6_9_57,cs-410,6,9, Recommender Systems,"00:03:54,590","00:03:58,980",57,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=234,so there's a rich context there
cs-410_6_9_58,cs-410,6,9, Recommender Systems,"00:03:59,980","00:04:04,100",58,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=239,really solve the problem well and
cs-410_6_9_59,cs-410,6,9, Recommender Systems,"00:04:04,100","00:04:09,650",59,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=244,research area where also machine
cs-410_6_9_60,cs-410,6,9, Recommender Systems,"00:04:09,650","00:04:13,624",60,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=249,Here are some additional readings in
cs-410_6_9_61,cs-410,6,9, Recommender Systems,"00:04:13,624","00:04:18,494",61,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=253,the handbook called
cs-410_6_9_62,cs-410,6,9, Recommender Systems,"00:04:18,494","00:04:23,364",62,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=258,has a collection of a lot
cs-410_6_9_63,cs-410,6,9, Recommender Systems,"00:04:23,364","00:04:28,362",63,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=263,can give you an overview
cs-410_6_9_64,cs-410,6,9, Recommender Systems,"00:04:28,362","00:04:33,122",64,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=268,approaches through recommender systems.
cs-410_6_9_65,cs-410,6,9, Recommender Systems,"00:04:33,122","00:04:43,122",65,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=273,[MUSIC]
cs-410_6_10_1,cs-410,6,10, Summary for Exam 1,"00:00:00,012","00:00:03,145",1,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=0,[NOISE]
cs-410_6_10_2,cs-410,6,10, Summary for Exam 1,"00:00:06,848","00:00:10,210",2,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=6,This lecture is a summary of this course.
cs-410_6_10_3,cs-410,6,10, Summary for Exam 1,"00:00:12,890","00:00:17,110",3,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=12,This map shows the major topics
cs-410_6_10_4,cs-410,6,10, Summary for Exam 1,"00:00:19,170","00:00:24,230",4,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=19,And here are some key
cs-410_6_10_5,cs-410,6,10, Summary for Exam 1,"00:00:24,230","00:00:28,020",5,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=24,"First, we talked about natural"
cs-410_6_10_6,cs-410,6,10, Summary for Exam 1,"00:00:29,170","00:00:33,120",6,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=29,Here the main take-away messages
cs-410_6_10_7,cs-410,6,10, Summary for Exam 1,"00:00:33,120","00:00:39,210",7,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=33,"a foundation for text retrieval, but"
cs-410_6_10_8,cs-410,6,10, Summary for Exam 1,"00:00:39,210","00:00:48,540",8,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=39,the battle of wars is generally the main
cs-410_6_10_9,cs-410,6,10, Summary for Exam 1,"00:00:48,540","00:00:52,730",9,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=48,And it's often sufficient before
cs-410_6_10_10,cs-410,6,10, Summary for Exam 1,"00:00:52,730","00:00:58,290",10,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=52,obviously for
cs-410_6_10_11,cs-410,6,10, Summary for Exam 1,"00:00:58,290","00:01:02,640",11,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=58,a deeper natural language
cs-410_6_10_12,cs-410,6,10, Summary for Exam 1,"00:01:02,640","00:01:05,070",12,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=62,We then talked about the high
cs-410_6_10_13,cs-410,6,10, Summary for Exam 1,"00:01:05,070","00:01:09,170",13,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=65,text access and
cs-410_6_10_14,cs-410,6,10, Summary for Exam 1,"00:01:09,170","00:01:12,200",14,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=69,In pull we talked about
cs-410_6_10_15,cs-410,6,10, Summary for Exam 1,"00:01:13,250","00:01:17,800",15,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=73,"Now in general in future search engines,"
cs-410_6_10_16,cs-410,6,10, Summary for Exam 1,"00:01:17,800","00:01:20,466",16,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=77,to provide a math involved
cs-410_6_10_17,cs-410,6,10, Summary for Exam 1,"00:01:23,022","00:01:27,680",17,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=83,And now we'll talk about a number of
cs-410_6_10_18,cs-410,6,10, Summary for Exam 1,"00:01:27,680","00:01:30,350",18,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=87,We talked about the search problem.
cs-410_6_10_19,cs-410,6,10, Summary for Exam 1,"00:01:30,350","00:01:32,280",19,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=90,And we framed that as a ranking problem.
cs-410_6_10_20,cs-410,6,10, Summary for Exam 1,"00:01:34,830","00:01:38,680",20,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=94,And we talked about a number
cs-410_6_10_21,cs-410,6,10, Summary for Exam 1,"00:01:38,680","00:01:42,170",21,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=98,We start with the overview
cs-410_6_10_22,cs-410,6,10, Summary for Exam 1,"00:01:42,170","00:01:46,710",22,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=102,the probabilistic model and then we talked
cs-410_6_10_23,cs-410,6,10, Summary for Exam 1,"00:01:48,400","00:01:53,730",23,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=108,We also later talked about
cs-410_6_10_24,cs-410,6,10, Summary for Exam 1,"00:01:53,730","00:01:56,280",24,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=113,that's probabilistic model.
cs-410_6_10_25,cs-410,6,10, Summary for Exam 1,"00:01:56,280","00:02:01,910",25,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=116,"And here, many take-away message is that"
cs-410_6_10_26,cs-410,6,10, Summary for Exam 1,"00:02:01,910","00:02:07,510",26,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=121,"look similar, and"
cs-410_6_10_27,cs-410,6,10, Summary for Exam 1,"00:02:07,510","00:02:13,730",27,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=127,"Most important ones are TF-IDF weighting,"
cs-410_6_10_28,cs-410,6,10, Summary for Exam 1,"00:02:13,730","00:02:20,460",28,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=133,And the TF is often transformed through
cs-410_6_10_29,cs-410,6,10, Summary for Exam 1,"00:02:22,070","00:02:27,730",29,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=142,And then we talked about how to
cs-410_6_10_30,cs-410,6,10, Summary for Exam 1,"00:02:27,730","00:02:33,940",30,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=147,"the main techniques that we talked about,"
cs-410_6_10_31,cs-410,6,10, Summary for Exam 1,"00:02:33,940","00:02:39,590",31,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=153,that we can prepare the system
cs-410_6_10_32,cs-410,6,10, Summary for Exam 1,"00:02:39,590","00:02:45,100",32,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=159,And we talked about how to do a faster
cs-410_6_10_33,cs-410,6,10, Summary for Exam 1,"00:02:46,180","00:02:50,800",33,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=166,And we then talked about how to
cs-410_6_10_34,cs-410,6,10, Summary for Exam 1,"00:02:50,800","00:02:54,860",34,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=170,mainly introduced to
cs-410_6_10_35,cs-410,6,10, Summary for Exam 1,"00:02:54,860","00:02:58,770",35,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=174,This was a very important
cs-410_6_10_36,cs-410,6,10, Summary for Exam 1,"00:02:58,770","00:03:00,490",36,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=178,applied to many tasks.
cs-410_6_10_37,cs-410,6,10, Summary for Exam 1,"00:03:01,980","00:03:05,450",37,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=181,We talked about the major
cs-410_6_10_38,cs-410,6,10, Summary for Exam 1,"00:03:05,450","00:03:10,800",38,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=185,"So, the most important measures for"
cs-410_6_10_39,cs-410,6,10, Summary for Exam 1,"00:03:10,800","00:03:16,400",39,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=190,"are MAP, mean average precision,"
cs-410_6_10_40,cs-410,6,10, Summary for Exam 1,"00:03:16,400","00:03:20,880",40,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=196,accumulative gain and also precision and
cs-410_6_10_41,cs-410,6,10, Summary for Exam 1,"00:03:22,580","00:03:25,540",41,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=202,And we then talked about
cs-410_6_10_42,cs-410,6,10, Summary for Exam 1,"00:03:25,540","00:03:29,180",42,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=205,And we talked about the Rocchio
cs-410_6_10_43,cs-410,6,10, Summary for Exam 1,"00:03:29,180","00:03:32,200",43,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=209,the mixture model and
cs-410_6_10_44,cs-410,6,10, Summary for Exam 1,"00:03:32,200","00:03:36,630",44,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=212,Feedback is a very important
cs-410_6_10_45,cs-410,6,10, Summary for Exam 1,"00:03:36,630","00:03:41,430",45,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=216,the opportunity of learning from
cs-410_6_10_46,cs-410,6,10, Summary for Exam 1,"00:03:42,960","00:03:45,800",46,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=222,We then talked about Web search.
cs-410_6_10_47,cs-410,6,10, Summary for Exam 1,"00:03:45,800","00:03:50,150",47,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=225,And here we talked about how
cs-410_6_10_48,cs-410,6,10, Summary for Exam 1,"00:03:50,150","00:03:55,330",48,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=230,to solve the scalability issue in that
cs-410_6_10_49,cs-410,6,10, Summary for Exam 1,"00:03:55,330","00:03:59,130",49,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=235,Then we talked about how to use linking
cs-410_6_10_50,cs-410,6,10, Summary for Exam 1,"00:03:59,130","00:04:01,490",50,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=239,We talked about page rank and
cs-410_6_10_51,cs-410,6,10, Summary for Exam 1,"00:04:01,490","00:04:06,010",51,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=241,hits as the major hours is to
cs-410_6_10_52,cs-410,6,10, Summary for Exam 1,"00:04:07,320","00:04:09,562",52,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=247,We then talked about
cs-410_6_10_53,cs-410,6,10, Summary for Exam 1,"00:04:09,562","00:04:14,810",53,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=249,This is the use of machine learning
cs-410_6_10_54,cs-410,6,10, Summary for Exam 1,"00:04:14,810","00:04:16,640",54,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=254,improvement scoring.
cs-410_6_10_55,cs-410,6,10, Summary for Exam 1,"00:04:16,640","00:04:21,460",55,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=256,Not only that the effectiveness can be
cs-410_6_10_56,cs-410,6,10, Summary for Exam 1,"00:04:21,460","00:04:23,620",56,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=261,we can also improve the robustness of the.
cs-410_6_10_57,cs-410,6,10, Summary for Exam 1,"00:04:23,620","00:04:28,560",57,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=263,The ranking function so that it's
cs-410_6_10_58,cs-410,6,10, Summary for Exam 1,"00:04:28,560","00:04:34,540",58,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=268,It just some features to promote the page.
cs-410_6_10_59,cs-410,6,10, Summary for Exam 1,"00:04:36,270","00:04:39,279",59,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=276,And finally we talked about
cs-410_6_10_60,cs-410,6,10, Summary for Exam 1,"00:04:40,460","00:04:45,730",60,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=280,About the some major reactions
cs-410_6_10_61,cs-410,6,10, Summary for Exam 1,"00:04:45,730","00:04:49,390",61,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=285,in the future in improving the count
cs-410_6_10_62,cs-410,6,10, Summary for Exam 1,"00:04:50,610","00:04:54,030",62,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=290,And then finally we talked about
cs-410_6_10_63,cs-410,6,10, Summary for Exam 1,"00:04:54,030","00:04:57,890",63,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=294,these are systems to
cs-410_6_10_64,cs-410,6,10, Summary for Exam 1,"00:04:57,890","00:05:02,120",64,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=297,"And we'll talk about the two approaches,"
cs-410_6_10_65,cs-410,6,10, Summary for Exam 1,"00:05:02,120","00:05:06,240",65,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=302,one is collaborative filtering and
cs-410_6_10_66,cs-410,6,10, Summary for Exam 1,"00:05:07,330","00:05:11,930",66,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=307,"Now, an obvious missing piece"
cs-410_6_10_67,cs-410,6,10, Summary for Exam 1,"00:05:11,930","00:05:16,884",67,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=311,"in this picture is the user,"
cs-410_6_10_68,cs-410,6,10, Summary for Exam 1,"00:05:16,884","00:05:21,620",68,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=316,so user interface is also an important
cs-410_6_10_69,cs-410,6,10, Summary for Exam 1,"00:05:21,620","00:05:25,850",69,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=321,Even though the current search interface
cs-410_6_10_70,cs-410,6,10, Summary for Exam 1,"00:05:25,850","00:05:32,020",70,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=325,done a lot of studies of user interfaces
cs-410_6_10_71,cs-410,6,10, Summary for Exam 1,"00:05:32,020","00:05:34,680",71,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=332,"And this is the topic to that,"
cs-410_6_10_72,cs-410,6,10, Summary for Exam 1,"00:05:34,680","00:05:40,350",72,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=334,you can learn more by reading this book.
cs-410_6_10_73,cs-410,6,10, Summary for Exam 1,"00:05:40,350","00:05:47,430",73,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=340,It's an excellent book about all kinds
cs-410_6_10_74,cs-410,6,10, Summary for Exam 1,"00:05:48,800","00:05:53,360",74,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=348,If you want to know more about
cs-410_6_10_75,cs-410,6,10, Summary for Exam 1,"00:05:53,360","00:05:57,590",75,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=353,you can also read some additional
cs-410_6_10_76,cs-410,6,10, Summary for Exam 1,"00:05:57,590","00:06:01,110",76,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=357,In this short course we only
cs-410_6_10_77,cs-410,6,10, Summary for Exam 1,"00:06:01,110","00:06:03,610",77,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=361,topics in text retrievals and
cs-410_6_10_78,cs-410,6,10, Summary for Exam 1,"00:06:04,770","00:06:09,930",78,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=364,And these resources provide additional
cs-410_6_10_79,cs-410,6,10, Summary for Exam 1,"00:06:09,930","00:06:16,220",79,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=369,they give a more thorough treatment of
cs-410_6_10_80,cs-410,6,10, Summary for Exam 1,"00:06:16,220","00:06:19,410",80,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=376,And a main source is
cs-410_6_10_81,cs-410,6,10, Summary for Exam 1,"00:06:21,570","00:06:26,916",81,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=381,that you can see a lot of short
cs-410_6_10_82,cs-410,6,10, Summary for Exam 1,"00:06:26,916","00:06:30,290",82,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=386,or long tutorials.
cs-410_6_10_83,cs-410,6,10, Summary for Exam 1,"00:06:30,290","00:06:35,260",83,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=390,They tend to provide a lot of
cs-410_6_10_84,cs-410,6,10, Summary for Exam 1,"00:06:35,260","00:06:40,830",84,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=395,And there a lot of series that
cs-410_6_10_85,cs-410,6,10, Summary for Exam 1,"00:06:40,830","00:06:44,660",85,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=400,"One is information concepts,"
cs-410_6_10_86,cs-410,6,10, Summary for Exam 1,"00:06:44,660","00:06:46,310",86,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=404,One is human langauge technology.
cs-410_6_10_87,cs-410,6,10, Summary for Exam 1,"00:06:46,310","00:06:49,452",87,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=406,And yet another is artificial
cs-410_6_10_88,cs-410,6,10, Summary for Exam 1,"00:06:49,452","00:06:55,535",88,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=409,There are also some major journals and
cs-410_6_10_89,cs-410,6,10, Summary for Exam 1,"00:06:55,535","00:07:00,485",89,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=415,tend to have a lot of research papers
cs-410_6_10_90,cs-410,6,10, Summary for Exam 1,"00:07:00,485","00:07:05,370",90,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=420,"And finally, for more information"
cs-410_6_10_91,cs-410,6,10, Summary for Exam 1,"00:07:05,370","00:07:08,930",91,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=425,"tool kits, etc you can check out his URL."
cs-410_6_10_92,cs-410,6,10, Summary for Exam 1,"00:07:10,010","00:07:16,320",92,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=430,"So, if you have not taken the text"
cs-410_6_10_93,cs-410,6,10, Summary for Exam 1,"00:07:16,320","00:07:22,630",93,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=436,specialization series then naturally
cs-410_6_10_94,cs-410,6,10, Summary for Exam 1,"00:07:22,630","00:07:27,900",94,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=442,"As this picture shows,"
cs-410_6_10_95,cs-410,6,10, Summary for Exam 1,"00:07:27,900","00:07:31,800",95,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=447,we generally need two kinds of techniques.
cs-410_6_10_96,cs-410,6,10, Summary for Exam 1,"00:07:31,800","00:07:34,710",96,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=451,"One is text retrieval,"
cs-410_6_10_97,cs-410,6,10, Summary for Exam 1,"00:07:34,710","00:07:39,490",97,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=454,And these techniques will help us
cs-410_6_10_98,cs-410,6,10, Summary for Exam 1,"00:07:39,490","00:07:45,550",98,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=459,"relevant text data, which are actually"
cs-410_6_10_99,cs-410,6,10, Summary for Exam 1,"00:07:45,550","00:07:51,190",99,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=465,Now human plays important role in mining
cs-410_6_10_100,cs-410,6,10, Summary for Exam 1,"00:07:51,190","00:07:54,630",100,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=471,written for humans to consume.
cs-410_6_10_101,cs-410,6,10, Summary for Exam 1,"00:07:54,630","00:08:00,580",101,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=474,So involving humans in the process
cs-410_6_10_102,cs-410,6,10, Summary for Exam 1,"00:08:00,580","00:08:05,050",102,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=480,in this course we have covered
cs-410_6_10_103,cs-410,6,10, Summary for Exam 1,"00:08:05,050","00:08:08,300",103,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=485,access to the most relevant data.
cs-410_6_10_104,cs-410,6,10, Summary for Exam 1,"00:08:08,300","00:08:13,210",104,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=488,These techniques are always so
cs-410_6_10_105,cs-410,6,10, Summary for Exam 1,"00:08:13,210","00:08:17,770",105,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=493,to help provide prominence and
cs-410_6_10_106,cs-410,6,10, Summary for Exam 1,"00:08:17,770","00:08:23,990",106,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=497,patterns that the user will
cs-410_6_10_107,cs-410,6,10, Summary for Exam 1,"00:08:23,990","00:08:27,870",107,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=503,"So, in general, the user would have"
cs-410_6_10_108,cs-410,6,10, Summary for Exam 1,"00:08:27,870","00:08:29,359",108,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=507,better understand the patterns.
cs-410_6_10_109,cs-410,6,10, Summary for Exam 1,"00:08:30,360","00:08:36,200",109,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=510,"So the text mining cause, or rather,"
cs-410_6_10_110,cs-410,6,10, Summary for Exam 1,"00:08:36,200","00:08:41,660",110,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=516,will be dealing with what to do once
cs-410_6_10_111,cs-410,6,10, Summary for Exam 1,"00:08:41,660","00:08:46,010",111,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=521,So this is a second step in this
cs-410_6_10_112,cs-410,6,10, Summary for Exam 1,"00:08:46,010","00:08:48,790",112,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=526,the text data into actionable knowledge.
cs-410_6_10_113,cs-410,6,10, Summary for Exam 1,"00:08:49,830","00:08:55,900",113,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=529,And this has to do with helping users to
cs-410_6_10_114,cs-410,6,10, Summary for Exam 1,"00:08:55,900","00:08:59,750",114,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=535,to find the patterns and
cs-410_6_10_115,cs-410,6,10, Summary for Exam 1,"00:08:59,750","00:09:04,640",115,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=539,In text and such knowledge can
cs-410_6_10_116,cs-410,6,10, Summary for Exam 1,"00:09:04,640","00:09:10,500",116,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=544,systems to help decision making or
cs-410_6_10_117,cs-410,6,10, Summary for Exam 1,"00:09:10,500","00:09:16,624",117,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=550,"So, if you have not taken that course,"
cs-410_6_10_118,cs-410,6,10, Summary for Exam 1,"00:09:16,624","00:09:22,030",118,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=556,that natural next step would
cs-410_6_10_119,cs-410,6,10, Summary for Exam 1,"00:09:24,000","00:09:25,770",119,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=564,Thank you for taking this course.
cs-410_6_10_120,cs-410,6,10, Summary for Exam 1,"00:09:25,770","00:09:29,570",120,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=565,I hope you had fun and
cs-410_6_10_121,cs-410,6,10, Summary for Exam 1,"00:09:29,570","00:09:34,236",121,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=569,And I look forward to interacting
cs-410_6_10_122,cs-410,6,10, Summary for Exam 1,"00:09:34,236","00:09:44,236",122,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=574,[MUSIC]
cs-410_7_1_1,cs-410,7,1,Overview,"00:00:00,012","00:00:06,665",1,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=0,[SOUND]
cs-410_7_1_2,cs-410,7,1,Overview,"00:00:06,665","00:00:11,700",2,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=6,this lecture we give an overview
cs-410_7_1_3,cs-410,7,1,Overview,"00:00:13,743","00:00:19,830",3,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=13,"First, let's define the term text mining,"
cs-410_7_1_4,cs-410,7,1,Overview,"00:00:19,830","00:00:24,200",4,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=19,The title of this course is
cs-410_7_1_5,cs-410,7,1,Overview,"00:00:25,590","00:00:31,250",5,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=25,"But the two terms text mining, and text"
cs-410_7_1_6,cs-410,7,1,Overview,"00:00:32,670","00:00:36,370",6,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=32,So we are not really going to
cs-410_7_1_7,cs-410,7,1,Overview,"00:00:36,370","00:00:38,230",7,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=36,we're going to use them interchangeably.
cs-410_7_1_8,cs-410,7,1,Overview,"00:00:38,230","00:00:42,880",8,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=38,But the reason that we have chosen to use
cs-410_7_1_9,cs-410,7,1,Overview,"00:00:42,880","00:00:47,720",9,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=42,both terms in the title is because
cs-410_7_1_10,cs-410,7,1,Overview,"00:00:47,720","00:00:51,070",10,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=47,if you look at the two phrases literally.
cs-410_7_1_11,cs-410,7,1,Overview,"00:00:52,110","00:00:55,640",11,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=52,Mining emphasizes more on the process.
cs-410_7_1_12,cs-410,7,1,Overview,"00:00:55,640","00:01:01,683",12,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=55,So it gives us a error rate
cs-410_7_1_13,cs-410,7,1,Overview,"00:01:01,683","00:01:06,359",13,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=61,"Analytics, on the other hand"
cs-410_7_1_14,cs-410,7,1,Overview,"00:01:07,600","00:01:09,900",14,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=67,or having a problem in mind.
cs-410_7_1_15,cs-410,7,1,Overview,"00:01:09,900","00:01:14,720",15,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=69,We are going to look at text
cs-410_7_1_16,cs-410,7,1,Overview,"00:01:16,010","00:01:19,940",16,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=76,"But again as I said, we can treat"
cs-410_7_1_17,cs-410,7,1,Overview,"00:01:21,150","00:01:24,820",17,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=81,And I think in the literature
cs-410_7_1_18,cs-410,7,1,Overview,"00:01:24,820","00:01:27,820",18,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=84,So we're not going to really
cs-410_7_1_19,cs-410,7,1,Overview,"00:01:29,850","00:01:35,450",19,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=89,Both text mining and
cs-410_7_1_20,cs-410,7,1,Overview,"00:01:35,450","00:01:40,250",20,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=95,want to turn text data into high quality
cs-410_7_1_21,cs-410,7,1,Overview,"00:01:42,570","00:01:44,020",21,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=102,"So in both cases, we"
cs-410_7_1_22,cs-410,7,1,Overview,"00:01:45,830","00:01:50,670",22,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=105,have the problem of dealing with
cs-410_7_1_23,cs-410,7,1,Overview,"00:01:50,670","00:01:56,090",23,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=110,Turn these text data into something more
cs-410_7_1_24,cs-410,7,1,Overview,"00:01:57,730","00:02:00,380",24,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=117,And here we distinguish
cs-410_7_1_25,cs-410,7,1,Overview,"00:02:00,380","00:02:04,530",25,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=120,"One is high-quality information,"
cs-410_7_1_26,cs-410,7,1,Overview,"00:02:05,740","00:02:08,680",26,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=125,Sometimes the boundary between
cs-410_7_1_27,cs-410,7,1,Overview,"00:02:09,850","00:02:11,690",27,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=129,But I also want to say a little bit about
cs-410_7_1_28,cs-410,7,1,Overview,"00:02:12,780","00:02:17,590",28,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=132,these two different angles of
cs-410_7_1_29,cs-410,7,1,Overview,"00:02:19,250","00:02:22,982",29,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=139,"In the case of high quality information,"
cs-410_7_1_30,cs-410,7,1,Overview,"00:02:22,982","00:02:27,715",30,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=142,concise information about the topic.
cs-410_7_1_31,cs-410,7,1,Overview,"00:02:28,895","00:02:34,205",31,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=148,Which might be much easier for
cs-410_7_1_32,cs-410,7,1,Overview,"00:02:34,205","00:02:37,045",32,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=154,"For example, you might face"
cs-410_7_1_33,cs-410,7,1,Overview,"00:02:38,260","00:02:42,700",33,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=158,A more concise form of information
cs-410_7_1_34,cs-410,7,1,Overview,"00:02:42,700","00:02:46,570",34,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=162,of the major opinions about
cs-410_7_1_35,cs-410,7,1,Overview,"00:02:46,570","00:02:50,874",35,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=166,"Positive about,"
cs-410_7_1_36,cs-410,7,1,Overview,"00:02:53,436","00:02:58,260",36,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=173,Now this kind of results are very useful
cs-410_7_1_37,cs-410,7,1,Overview,"00:02:59,930","00:03:05,030",37,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=179,And so this is to minimize a human effort
cs-410_7_1_38,cs-410,7,1,Overview,"00:03:06,250","00:03:09,880",38,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=186,The other kind of output
cs-410_7_1_39,cs-410,7,1,Overview,"00:03:09,880","00:03:15,300",39,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=189,Here we emphasize the utility
cs-410_7_1_40,cs-410,7,1,Overview,"00:03:15,300","00:03:17,190",40,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=195,knowledge we discover from text data.
cs-410_7_1_41,cs-410,7,1,Overview,"00:03:18,270","00:03:23,830",41,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=198,It's actionable knowledge for some
cs-410_7_1_42,cs-410,7,1,Overview,"00:03:24,990","00:03:31,510",42,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=204,"For example, we might be able to determine"
cs-410_7_1_43,cs-410,7,1,Overview,"00:03:31,510","00:03:36,270",43,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=211,or a better choice for
cs-410_7_1_44,cs-410,7,1,Overview,"00:03:38,115","00:03:43,190",44,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=218,"Now, such an outcome could be"
cs-410_7_1_45,cs-410,7,1,Overview,"00:03:43,190","00:03:49,550",45,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=223,because a consumer can take the knowledge
cs-410_7_1_46,cs-410,7,1,Overview,"00:03:49,550","00:03:55,131",46,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=229,"So, in this case text mining supplies"
cs-410_7_1_47,cs-410,7,1,Overview,"00:03:55,131","00:03:59,424",47,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=235,"But again, the two are not so"
cs-410_7_1_48,cs-410,7,1,Overview,"00:03:59,424","00:04:03,281",48,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=239,we don't necessarily have
cs-410_7_1_49,cs-410,7,1,Overview,"00:04:06,253","00:04:09,821",49,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=246,Text mining is also
cs-410_7_1_50,cs-410,7,1,Overview,"00:04:09,821","00:04:14,380",50,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=249,which is a essential component
cs-410_7_1_51,cs-410,7,1,Overview,"00:04:15,910","00:04:20,434",51,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=255,"Now, text retrieval refers to"
cs-410_7_1_52,cs-410,7,1,Overview,"00:04:20,434","00:04:22,380",52,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=260,a large amount of text data.
cs-410_7_1_53,cs-410,7,1,Overview,"00:04:24,140","00:04:30,210",53,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=264,So I've taught another separate MOOC
cs-410_7_1_54,cs-410,7,1,Overview,"00:04:31,710","00:04:34,680",54,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=271,Where we discussed various techniques for
cs-410_7_1_55,cs-410,7,1,Overview,"00:04:36,360","00:04:41,080",55,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=276,"If you have taken that MOOC,"
cs-410_7_1_56,cs-410,7,1,Overview,"00:04:42,120","00:04:46,700",56,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=282,And it will be useful To know
cs-410_7_1_57,cs-410,7,1,Overview,"00:04:46,700","00:04:50,010",57,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=286,of understanding some of
cs-410_7_1_58,cs-410,7,1,Overview,"00:04:51,750","00:04:54,350",58,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=291,"But, if you have not taken that MOOC,"
cs-410_7_1_59,cs-410,7,1,Overview,"00:04:54,350","00:04:59,440",59,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=294,it's also fine because in this MOOC
cs-410_7_1_60,cs-410,7,1,Overview,"00:04:59,440","00:05:03,260",60,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=299,going to repeat some of the key concepts
cs-410_7_1_61,cs-410,7,1,Overview,"00:05:03,260","00:05:06,540",61,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=303,But they're at the high level and
cs-410_7_1_62,cs-410,7,1,Overview,"00:05:06,540","00:05:10,710",62,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=306,they also explain the relation between
cs-410_7_1_63,cs-410,7,1,Overview,"00:05:12,320","00:05:18,278",63,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=312,Text retrieval is very useful for
cs-410_7_1_64,cs-410,7,1,Overview,"00:05:18,278","00:05:23,200",64,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=318,"First, text retrieval can be"
cs-410_7_1_65,cs-410,7,1,Overview,"00:05:23,200","00:05:27,600",65,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=323,Meaning that it can help
cs-410_7_1_66,cs-410,7,1,Overview,"00:05:27,600","00:05:32,030",66,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=327,a relatively small amount
cs-410_7_1_67,cs-410,7,1,Overview,"00:05:32,030","00:05:35,180",67,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=332,Which is often what's needed for
cs-410_7_1_68,cs-410,7,1,Overview,"00:05:36,580","00:05:41,186",68,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=336,"And in this sense, text retrieval"
cs-410_7_1_69,cs-410,7,1,Overview,"00:05:43,323","00:05:46,365",69,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=343,Text retrieval is also needed for
cs-410_7_1_70,cs-410,7,1,Overview,"00:05:46,365","00:05:50,976",70,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=346,And this roughly corresponds
cs-410_7_1_71,cs-410,7,1,Overview,"00:05:50,976","00:05:56,350",71,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=350,mining as turning text data
cs-410_7_1_72,cs-410,7,1,Overview,"00:05:56,350","00:05:58,970",72,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=356,"Once we find the patterns in text data, or"
cs-410_7_1_73,cs-410,7,1,Overview,"00:05:58,970","00:06:04,040",73,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=358,"actionable knowledge, we generally"
cs-410_7_1_74,cs-410,7,1,Overview,"00:06:04,040","00:06:06,580",74,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=364,By looking at the original text data.
cs-410_7_1_75,cs-410,7,1,Overview,"00:06:06,580","00:06:11,110",75,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=366,So the users would have to have some text
cs-410_7_1_76,cs-410,7,1,Overview,"00:06:11,110","00:06:16,010",76,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=371,text data to interpret the pattern or
cs-410_7_1_77,cs-410,7,1,Overview,"00:06:16,010","00:06:19,910",77,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=376,to verify whether a pattern
cs-410_7_1_78,cs-410,7,1,Overview,"00:06:19,910","00:06:23,830",78,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=379,So this is a high level introduction
cs-410_7_1_79,cs-410,7,1,Overview,"00:06:23,830","00:06:29,530",79,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=383,and the relationship between
cs-410_7_1_80,cs-410,7,1,Overview,"00:06:32,110","00:06:36,554",80,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=392,"Next, let's talk about text"
cs-410_7_1_81,cs-410,7,1,Overview,"00:06:39,689","00:06:45,607",81,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=399,Now it's interesting to
cs-410_7_1_82,cs-410,7,1,Overview,"00:06:45,607","00:06:51,380",82,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=405,generated by humans as subjective sensors.
cs-410_7_1_83,cs-410,7,1,Overview,"00:06:53,200","00:07:03,420",83,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=413,"So, this slide shows an analogy"
cs-410_7_1_84,cs-410,7,1,Overview,"00:07:03,420","00:07:07,832",84,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=423,And between humans as
cs-410_7_1_85,cs-410,7,1,Overview,"00:07:07,832","00:07:13,993",85,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=427,"physical sensors,"
cs-410_7_1_86,cs-410,7,1,Overview,"00:07:16,292","00:07:21,377",86,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=436,So in general a sensor would
cs-410_7_1_87,cs-410,7,1,Overview,"00:07:21,377","00:07:26,483",87,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=441,It would sense some signal
cs-410_7_1_88,cs-410,7,1,Overview,"00:07:26,483","00:07:32,205",88,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=446,"then would report the signal as data,"
cs-410_7_1_89,cs-410,7,1,Overview,"00:07:32,205","00:07:38,346",89,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=452,"For example, a thermometer would watch"
cs-410_7_1_90,cs-410,7,1,Overview,"00:07:38,346","00:07:43,310",90,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=458,then we report the temperature
cs-410_7_1_91,cs-410,7,1,Overview,"00:07:44,962","00:07:49,098",91,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=464,"Similarly, a geo sensor would sense"
cs-410_7_1_92,cs-410,7,1,Overview,"00:07:49,098","00:07:53,740",92,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=469,"The location specification, for"
cs-410_7_1_93,cs-410,7,1,Overview,"00:07:53,740","00:07:57,140",93,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=473,"example, in the form of longitude"
cs-410_7_1_94,cs-410,7,1,Overview,"00:07:57,140","00:08:02,580",94,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=477,A network sends over
cs-410_7_1_95,cs-410,7,1,Overview,"00:08:02,580","00:08:04,873",95,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=482,or activities in the network and
cs-410_7_1_96,cs-410,7,1,Overview,"00:08:04,873","00:08:09,477",96,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=484,Some digital format of data.
cs-410_7_1_97,cs-410,7,1,Overview,"00:08:09,477","00:08:16,460",97,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=489,Similarly we can think of
cs-410_7_1_98,cs-410,7,1,Overview,"00:08:16,460","00:08:22,050",98,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=496,That will observe the real world and
cs-410_7_1_99,cs-410,7,1,Overview,"00:08:22,050","00:08:28,440",99,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=502,And then humans will express what they
cs-410_7_1_100,cs-410,7,1,Overview,"00:08:28,440","00:08:33,330",100,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=508,"So, in this sense, human is actually"
cs-410_7_1_101,cs-410,7,1,Overview,"00:08:33,330","00:08:36,200",101,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=513,sense what's happening in the world and
cs-410_7_1_102,cs-410,7,1,Overview,"00:08:36,200","00:08:43,060",102,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=516,then express what's observed in the form
cs-410_7_1_103,cs-410,7,1,Overview,"00:08:43,060","00:08:47,350",103,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=523,"Now, looking at the text data in"
cs-410_7_1_104,cs-410,7,1,Overview,"00:08:47,350","00:08:50,240",104,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=527,able to integrate all
cs-410_7_1_105,cs-410,7,1,Overview,"00:08:50,240","00:08:54,381",105,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=530,And that's indeed needed in
cs-410_7_1_106,cs-410,7,1,Overview,"00:08:56,123","00:09:01,672",106,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=536,So here we are looking at
cs-410_7_1_107,cs-410,7,1,Overview,"00:09:02,725","00:09:07,518",107,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=542,And in general we would Be
cs-410_7_1_108,cs-410,7,1,Overview,"00:09:07,518","00:09:11,982",108,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=547,about our world that
cs-410_7_1_109,cs-410,7,1,Overview,"00:09:11,982","00:09:17,180",109,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=551,And in general it will be dealing with
cs-410_7_1_110,cs-410,7,1,Overview,"00:09:17,180","00:09:21,348",110,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=557,And of course the non-text data
cs-410_7_1_111,cs-410,7,1,Overview,"00:09:21,348","00:09:26,330",111,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=561,And those non-text data can
cs-410_7_1_112,cs-410,7,1,Overview,"00:09:27,840","00:09:30,830",112,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=567,"Numerical data, categorical,"
cs-410_7_1_113,cs-410,7,1,Overview,"00:09:30,830","00:09:33,800",113,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=570,or multi-media data like video or speech.
cs-410_7_1_114,cs-410,7,1,Overview,"00:09:36,360","00:09:41,900",114,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=576,"So, these non text data are often"
cs-410_7_1_115,cs-410,7,1,Overview,"00:09:41,900","00:09:45,590",115,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=581,"But text data is also very important,"
cs-410_7_1_116,cs-410,7,1,Overview,"00:09:45,590","00:09:50,960",116,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=585,mostly because they contain
cs-410_7_1_117,cs-410,7,1,Overview,"00:09:50,960","00:09:55,930",117,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=590,And they often contain
cs-410_7_1_118,cs-410,7,1,Overview,"00:09:55,930","00:09:58,860",118,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=595,especially preferences and
cs-410_7_1_119,cs-410,7,1,Overview,"00:10:01,360","00:10:07,990",119,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=601,"So, but by treating text data as"
cs-410_7_1_120,cs-410,7,1,Overview,"00:10:07,990","00:10:14,510",120,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=607,we can treat all this data
cs-410_7_1_121,cs-410,7,1,Overview,"00:10:14,510","00:10:18,110",121,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=614,So the data mining problem is
cs-410_7_1_122,cs-410,7,1,Overview,"00:10:18,110","00:10:22,960",122,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=618,turn all the data in your actionable
cs-410_7_1_123,cs-410,7,1,Overview,"00:10:22,960","00:10:26,260",123,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=622,of it to change the real
cs-410_7_1_124,cs-410,7,1,Overview,"00:10:26,260","00:10:31,490",124,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=626,So this means the data mining problem is
cs-410_7_1_125,cs-410,7,1,Overview,"00:10:31,490","00:10:37,450",125,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=631,basically taking a lot of data as input
cs-410_7_1_126,cs-410,7,1,Overview,"00:10:37,450","00:10:42,260",126,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=637,"Inside of the data mining module,"
cs-410_7_1_127,cs-410,7,1,Overview,"00:10:42,260","00:10:46,510",127,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=642,we have a number of different
cs-410_7_1_128,cs-410,7,1,Overview,"00:10:46,510","00:10:49,940",128,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=646,"And this is because, for"
cs-410_7_1_129,cs-410,7,1,Overview,"00:10:49,940","00:10:55,180",129,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=649,we generally need different algorithms for
cs-410_7_1_130,cs-410,7,1,Overview,"00:10:56,390","00:10:57,100",130,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=656,"For example,"
cs-410_7_1_131,cs-410,7,1,Overview,"00:10:57,100","00:11:01,870",131,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=657,video data might require computer
cs-410_7_1_132,cs-410,7,1,Overview,"00:11:01,870","00:11:06,050",132,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=661,And that would facilitate
cs-410_7_1_133,cs-410,7,1,Overview,"00:11:06,050","00:11:11,110",133,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=666,And we also have a lot of general
cs-410_7_1_134,cs-410,7,1,Overview,"00:11:11,110","00:11:16,948",134,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=671,"to all kinds of data and those algorithms,"
cs-410_7_1_135,cs-410,7,1,Overview,"00:11:16,948","00:11:19,692",135,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=676,"Although, for a particular kind of data,"
cs-410_7_1_136,cs-410,7,1,Overview,"00:11:19,692","00:11:23,287",136,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=679,we generally want to also
cs-410_7_1_137,cs-410,7,1,Overview,"00:11:23,287","00:11:27,939",137,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=683,So this course will cover
cs-410_7_1_138,cs-410,7,1,Overview,"00:11:27,939","00:11:31,994",138,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=687,are particularly useful for
cs-410_7_1_139,cs-410,7,1,Overview,"00:11:31,994","00:11:41,994",139,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=691,[MUSIC]
cs-410_7_2_1,cs-410,7,2,Overview,"00:00:00,266","00:00:09,956",1,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=0,[SOUND]
cs-410_7_2_2,cs-410,7,2,Overview,"00:00:09,956","00:00:14,850",2,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=9,looking at the text mining problem more
cs-410_7_2_3,cs-410,7,2,Overview,"00:00:14,850","00:00:20,300",3,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=14,"similar to general data mining, except"
cs-410_7_2_4,cs-410,7,2,Overview,"00:00:21,710","00:00:26,400",4,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=21,And we're going to have text mining
cs-410_7_2_5,cs-410,7,2,Overview,"00:00:26,400","00:00:32,240",5,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=26,into actionable knowledge that
cs-410_7_2_6,cs-410,7,2,Overview,"00:00:32,240","00:00:34,130",6,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=32,"especially for decision making, or"
cs-410_7_2_7,cs-410,7,2,Overview,"00:00:34,130","00:00:39,350",7,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=34,for completing whatever tasks that
cs-410_7_2_8,cs-410,7,2,Overview,"00:00:39,350","00:00:45,000",8,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=39,"Because, in general,"
cs-410_7_2_9,cs-410,7,2,Overview,"00:00:45,000","00:00:49,720",9,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=45,we also tend to have other kinds
cs-410_7_2_10,cs-410,7,2,Overview,"00:00:49,720","00:00:54,950",10,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=49,So a more general picture would be
cs-410_7_2_11,cs-410,7,2,Overview,"00:00:56,000","00:01:00,875",11,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=56,And for this reason we might be
cs-410_7_2_12,cs-410,7,2,Overview,"00:01:00,875","00:01:02,060",12,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=60,non-text data.
cs-410_7_2_13,cs-410,7,2,Overview,"00:01:02,060","00:01:05,860",13,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=62,And so in this course we're
cs-410_7_2_14,cs-410,7,2,Overview,"00:01:05,860","00:01:10,628",14,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=65,but we're also going to also touch how do
cs-410_7_2_15,cs-410,7,2,Overview,"00:01:10,628","00:01:12,450",15,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=70,non-text data.
cs-410_7_2_16,cs-410,7,2,Overview,"00:01:12,450","00:01:16,630",16,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=72,With this problem definition we
cs-410_7_2_17,cs-410,7,2,Overview,"00:01:16,630","00:01:19,419",17,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=76,the topics in text mining and analytics.
cs-410_7_2_18,cs-410,7,2,Overview,"00:01:21,010","00:01:25,770",18,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=81,Now this slide shows the process of
cs-410_7_2_19,cs-410,7,2,Overview,"00:01:27,018","00:01:29,800",19,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=87,"More specifically, a human sensor or"
cs-410_7_2_20,cs-410,7,2,Overview,"00:01:29,800","00:01:33,420",20,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=89,human observer would look at
cs-410_7_2_21,cs-410,7,2,Overview,"00:01:34,660","00:01:38,820",21,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=94,Different people would be looking at
cs-410_7_2_22,cs-410,7,2,Overview,"00:01:38,820","00:01:41,210",22,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=98,they'll pay attention to different things.
cs-410_7_2_23,cs-410,7,2,Overview,"00:01:41,210","00:01:46,090",23,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=101,The same person at different times might
cs-410_7_2_24,cs-410,7,2,Overview,"00:01:46,090","00:01:50,990",24,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=106,of the observed world.
cs-410_7_2_25,cs-410,7,2,Overview,"00:01:50,990","00:01:55,450",25,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=110,And so the humans are able to perceive
cs-410_7_2_26,cs-410,7,2,Overview,"00:01:55,450","00:02:01,480",26,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=115,"And that human, the sensor,"
cs-410_7_2_27,cs-410,7,2,Overview,"00:02:01,480","00:02:05,150",27,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=121,And that can be called the Observed World.
cs-410_7_2_28,cs-410,7,2,Overview,"00:02:05,150","00:02:10,040",28,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=125,"Of course, this would be different from"
cs-410_7_2_29,cs-410,7,2,Overview,"00:02:10,040","00:02:14,830",29,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=130,that the person has taken
cs-410_7_2_30,cs-410,7,2,Overview,"00:02:16,840","00:02:22,642",30,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=136,Now the Observed World can be
cs-410_7_2_31,cs-410,7,2,Overview,"00:02:22,642","00:02:27,535",31,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=142,entity-relation graphs or
cs-410_7_2_32,cs-410,7,2,Overview,"00:02:27,535","00:02:31,890",32,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=147,using knowledge representation language.
cs-410_7_2_33,cs-410,7,2,Overview,"00:02:31,890","00:02:39,190",33,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=151,"But in general, this is basically what"
cs-410_7_2_34,cs-410,7,2,Overview,"00:02:39,190","00:02:43,800",34,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=159,And we don't really know what
cs-410_7_2_35,cs-410,7,2,Overview,"00:02:43,800","00:02:48,250",35,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=163,But then the human would
cs-410_7_2_36,cs-410,7,2,Overview,"00:02:48,250","00:02:52,920",36,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=168,"observed using a natural language,"
cs-410_7_2_37,cs-410,7,2,Overview,"00:02:52,920","00:02:54,760",37,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=172,And the result is text data.
cs-410_7_2_38,cs-410,7,2,Overview,"00:02:55,870","00:03:00,610",38,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=175,Of course a person could have used
cs-410_7_2_39,cs-410,7,2,Overview,"00:03:00,610","00:03:02,660",39,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=180,she has observed.
cs-410_7_2_40,cs-410,7,2,Overview,"00:03:02,660","00:03:08,220",40,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=182,In that case we might have text data of
cs-410_7_2_41,cs-410,7,2,Overview,"00:03:10,590","00:03:15,790",41,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=190,The main goal of text mining
cs-410_7_2_42,cs-410,7,2,Overview,"00:03:15,790","00:03:19,280",42,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=195,process of generating text data.
cs-410_7_2_43,cs-410,7,2,Overview,"00:03:19,280","00:03:24,480",43,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=199,We hope to be able to uncover
cs-410_7_2_44,cs-410,7,2,Overview,"00:03:28,340","00:03:34,480",44,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=208,"Specifically, we can think about mining,"
cs-410_7_2_45,cs-410,7,2,Overview,"00:03:35,560","00:03:40,130",45,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=215,And that means by looking at text data
cs-410_7_2_46,cs-410,7,2,Overview,"00:03:40,130","00:03:46,310",46,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=220,"something about English, some usage"
cs-410_7_2_47,cs-410,7,2,Overview,"00:03:47,780","00:03:52,340",47,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=227,"So this is one type of mining problems,"
cs-410_7_2_48,cs-410,7,2,Overview,"00:03:52,340","00:03:57,100",48,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=232,some knowledge about language which
cs-410_7_2_49,cs-410,7,2,Overview,"00:03:58,920","00:04:00,620",49,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=238,"If you look at the picture,"
cs-410_7_2_50,cs-410,7,2,Overview,"00:04:00,620","00:04:06,380",50,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=240,we can also then mine knowledge
cs-410_7_2_51,cs-410,7,2,Overview,"00:04:06,380","00:04:10,030",51,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=246,And so this has much to do with
cs-410_7_2_52,cs-410,7,2,Overview,"00:04:11,490","00:04:15,640",52,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=251,We're going to look at what the text
cs-410_7_2_53,cs-410,7,2,Overview,"00:04:15,640","00:04:20,820",53,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=255,get the essence of it or
cs-410_7_2_54,cs-410,7,2,Overview,"00:04:20,820","00:04:25,600",54,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=260,about a particular aspect of
cs-410_7_2_55,cs-410,7,2,Overview,"00:04:26,900","00:04:30,890",55,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=266,"For example, everything that has been"
cs-410_7_2_56,cs-410,7,2,Overview,"00:04:30,890","00:04:31,630",56,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=270,a particular entity.
cs-410_7_2_57,cs-410,7,2,Overview,"00:04:31,630","00:04:36,550",57,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=271,And this can be regarded as mining content
cs-410_7_2_58,cs-410,7,2,Overview,"00:04:36,550","00:04:43,330",58,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=276,to describe the observed world in
cs-410_7_2_59,cs-410,7,2,Overview,"00:04:45,020","00:04:50,060",59,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=285,"If you look further,"
cs-410_7_2_60,cs-410,7,2,Overview,"00:04:50,060","00:04:54,710",60,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=290,"we can mine knowledge about this observer,"
cs-410_7_2_61,cs-410,7,2,Overview,"00:04:54,710","00:05:00,630",61,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=294,So this has also to do with
cs-410_7_2_62,cs-410,7,2,Overview,"00:05:00,630","00:05:02,270",62,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=300,some properties of this person.
cs-410_7_2_63,cs-410,7,2,Overview,"00:05:03,380","00:05:07,410",63,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=303,And these properties could
cs-410_7_2_64,cs-410,7,2,Overview,"00:05:07,410","00:05:09,010",64,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=307,sentiment of the person.
cs-410_7_2_65,cs-410,7,2,Overview,"00:05:10,200","00:05:15,250",65,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=310,And note that we distinguish
cs-410_7_2_66,cs-410,7,2,Overview,"00:05:15,250","00:05:21,040",66,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=315,because text data can't describe what the
cs-410_7_2_67,cs-410,7,2,Overview,"00:05:21,040","00:05:25,960",67,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=321,But the description can be also
cs-410_7_2_68,cs-410,7,2,Overview,"00:05:25,960","00:05:30,280",68,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=325,"in general, you can imagine the text"
cs-410_7_2_69,cs-410,7,2,Overview,"00:05:30,280","00:05:34,770",69,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=330,descriptions of the world plus
cs-410_7_2_70,cs-410,7,2,Overview,"00:05:34,770","00:05:37,330",70,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=334,So that's why it's also possible to
cs-410_7_2_71,cs-410,7,2,Overview,"00:05:37,330","00:05:41,970",71,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=337,do text mining to mine
cs-410_7_2_72,cs-410,7,2,Overview,"00:05:41,970","00:05:45,980",72,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=341,"Finally, if you look at the picture"
cs-410_7_2_73,cs-410,7,2,Overview,"00:05:45,980","00:05:50,150",73,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=345,then you can see we can certainly also
cs-410_7_2_74,cs-410,7,2,Overview,"00:05:50,150","00:05:50,880",74,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=350,Right?
cs-410_7_2_75,cs-410,7,2,Overview,"00:05:50,880","00:05:56,860",75,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=350,So indeed we can do text mining to
cs-410_7_2_76,cs-410,7,2,Overview,"00:05:56,860","00:05:59,220",76,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=356,And this is often called
cs-410_7_2_77,cs-410,7,2,Overview,"00:06:00,600","00:06:04,000",77,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=360,And we want to predict the value
cs-410_7_2_78,cs-410,7,2,Overview,"00:06:04,000","00:06:08,935",78,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=364,"So, this picture basically covered"
cs-410_7_2_79,cs-410,7,2,Overview,"00:06:08,935","00:06:13,440",79,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=368,multiple types of knowledge that
cs-410_7_2_80,cs-410,7,2,Overview,"00:06:14,550","00:06:19,260",80,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=374,When we infer other
cs-410_7_2_81,cs-410,7,2,Overview,"00:06:19,260","00:06:24,990",81,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=379,could also use some of the results from
cs-410_7_2_82,cs-410,7,2,Overview,"00:06:24,990","00:06:30,910",82,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=384,mining text data as intermediate
cs-410_7_2_83,cs-410,7,2,Overview,"00:06:30,910","00:06:31,940",83,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=390,"For example,"
cs-410_7_2_84,cs-410,7,2,Overview,"00:06:31,940","00:06:38,050",84,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=391,after we mine the content of text data we
cs-410_7_2_85,cs-410,7,2,Overview,"00:06:38,050","00:06:41,190",85,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=398,And that summary could be then used
cs-410_7_2_86,cs-410,7,2,Overview,"00:06:41,190","00:06:45,003",86,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=401,to help us predict the variables
cs-410_7_2_87,cs-410,7,2,Overview,"00:06:45,003","00:06:51,940",87,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=405,Now of course this is still generated
cs-410_7_2_88,cs-410,7,2,Overview,"00:06:51,940","00:06:58,410",88,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=411,but I want to emphasize here that
cs-410_7_2_89,cs-410,7,2,Overview,"00:06:58,410","00:07:03,780",89,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=418,to generate some features that can help
cs-410_7_2_90,cs-410,7,2,Overview,"00:07:04,960","00:07:10,100",90,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=424,And that's why here we show the results of
cs-410_7_2_91,cs-410,7,2,Overview,"00:07:10,100","00:07:15,010",91,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=430,"some other mining tasks, including"
cs-410_7_2_92,cs-410,7,2,Overview,"00:07:15,010","00:07:19,520",92,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=435,"mining knowledge about the observer,"
cs-410_7_2_93,cs-410,7,2,Overview,"00:07:21,380","00:07:26,690",93,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=441,"In fact, when we have non-text data,"
cs-410_7_2_94,cs-410,7,2,Overview,"00:07:26,690","00:07:31,496",94,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=446,"data to help prediction, and"
cs-410_7_2_95,cs-410,7,2,Overview,"00:07:31,496","00:07:39,260",95,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=451,"In general, non-text data can be very"
cs-410_7_2_96,cs-410,7,2,Overview,"00:07:39,260","00:07:44,530",96,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=459,"For example,"
cs-410_7_2_97,cs-410,7,2,Overview,"00:07:44,530","00:07:49,870",97,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=464,changes of stock prices based on
cs-410_7_2_98,cs-410,7,2,Overview,"00:07:49,870","00:07:53,730",98,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=469,"in social media, then this is an example"
cs-410_7_2_99,cs-410,7,2,Overview,"00:07:53,730","00:07:58,520",99,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=473,of using text data to predict
cs-410_7_2_100,cs-410,7,2,Overview,"00:07:58,520","00:07:59,950",100,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=478,"But in this case, obviously,"
cs-410_7_2_101,cs-410,7,2,Overview,"00:07:59,950","00:08:04,480",101,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=479,the historical stock price data would
cs-410_7_2_102,cs-410,7,2,Overview,"00:08:04,480","00:08:09,633",102,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=484,And so that's an example of
cs-410_7_2_103,cs-410,7,2,Overview,"00:08:09,633","00:08:13,750",103,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=489,useful for the prediction.
cs-410_7_2_104,cs-410,7,2,Overview,"00:08:13,750","00:08:17,161",104,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=493,And we're going to combine both kinds
cs-410_7_2_105,cs-410,7,2,Overview,"00:08:17,161","00:08:24,510",105,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=497,Now non-text data can be also used for
cs-410_7_2_106,cs-410,7,2,Overview,"00:08:25,580","00:08:27,050",106,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=505,"When we look at the text data alone,"
cs-410_7_2_107,cs-410,7,2,Overview,"00:08:27,050","00:08:31,770",107,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=507,we'll be mostly looking at the content
cs-410_7_2_108,cs-410,7,2,Overview,"00:08:32,790","00:08:36,149",108,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=512,But text data generally also
cs-410_7_2_109,cs-410,7,2,Overview,"00:08:37,470","00:08:44,150",109,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=517,"For example, the time and the location"
cs-410_7_2_110,cs-410,7,2,Overview,"00:08:44,150","00:08:47,500",110,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=524,And these are useful context information.
cs-410_7_2_111,cs-410,7,2,Overview,"00:08:48,740","00:08:54,020",111,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=528,And the context can provide interesting
cs-410_7_2_112,cs-410,7,2,Overview,"00:08:54,020","00:08:57,980",112,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=534,"For example, we might partition text"
cs-410_7_2_113,cs-410,7,2,Overview,"00:08:57,980","00:09:00,680",113,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=537,because of the availability of the time.
cs-410_7_2_114,cs-410,7,2,Overview,"00:09:00,680","00:09:06,480",114,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=540,Now we can analyze text data in each
cs-410_7_2_115,cs-410,7,2,Overview,"00:09:06,480","00:09:09,970",115,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=546,Similarly we can partition text
cs-410_7_2_116,cs-410,7,2,Overview,"00:09:09,970","00:09:15,920",116,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=549,any meta data that's associated to
cs-410_7_2_117,cs-410,7,2,Overview,"00:09:15,920","00:09:20,980",117,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=555,"So, in this sense,"
cs-410_7_2_118,cs-410,7,2,Overview,"00:09:20,980","00:09:24,580",118,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=560,interesting angles or
cs-410_7_2_119,cs-410,7,2,Overview,"00:09:24,580","00:09:29,340",119,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=564,And it can help us make context-sensitive
cs-410_7_2_120,cs-410,7,2,Overview,"00:09:29,340","00:09:33,680",120,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=569,analysis of content or
cs-410_7_2_121,cs-410,7,2,Overview,"00:09:36,390","00:09:42,920",121,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=576,the opinions about the observer or
cs-410_7_2_122,cs-410,7,2,Overview,"00:09:42,920","00:09:46,920",122,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=582,We could analyze the sentiment
cs-410_7_2_123,cs-410,7,2,Overview,"00:09:46,920","00:09:54,500",123,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=586,So this is a fairly general landscape of
cs-410_7_2_124,cs-410,7,2,Overview,"00:09:54,500","00:09:59,850",124,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=594,In this course we're going to
cs-410_7_2_125,cs-410,7,2,Overview,"00:09:59,850","00:10:03,796",125,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=599,We actually hope to cover
cs-410_7_2_126,cs-410,7,2,Overview,"00:10:06,675","00:10:11,321",126,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=606,First we're going to cover
cs-410_7_2_127,cs-410,7,2,Overview,"00:10:11,321","00:10:16,053",127,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=611,briefly because this has to do
cs-410_7_2_128,cs-410,7,2,Overview,"00:10:16,053","00:10:21,580",128,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=616,this determines how we can represent
cs-410_7_2_129,cs-410,7,2,Overview,"00:10:21,580","00:10:27,870",129,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=621,"Second, we're going to talk about how to"
cs-410_7_2_130,cs-410,7,2,Overview,"00:10:27,870","00:10:34,340",130,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=627,And word associations is a form of use for
cs-410_7_2_131,cs-410,7,2,Overview,"00:10:34,340","00:10:38,340",131,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=634,"Third, we're going to talk about"
cs-410_7_2_132,cs-410,7,2,Overview,"00:10:38,340","00:10:43,190",132,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=638,And this is only one way to
cs-410_7_2_133,cs-410,7,2,Overview,"00:10:43,190","00:10:46,100",133,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=643,it's a very useful ways
cs-410_7_2_134,cs-410,7,2,Overview,"00:10:46,100","00:10:51,400",134,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=646,It's also one of the most useful
cs-410_7_2_135,cs-410,7,2,Overview,"00:10:53,750","00:10:59,510",135,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=653,Then we're going to talk about
cs-410_7_2_136,cs-410,7,2,Overview,"00:10:59,510","00:11:05,250",136,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=659,So this can be regarded as one example
cs-410_7_2_137,cs-410,7,2,Overview,"00:11:07,140","00:11:11,510",137,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=667,And finally we're going to
cs-410_7_2_138,cs-410,7,2,Overview,"00:11:11,510","00:11:16,020",138,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=671,problems where we try to predict some
cs-410_7_2_139,cs-410,7,2,Overview,"00:11:17,400","00:11:24,880",139,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=677,So this slide also serves as
cs-410_7_2_140,cs-410,7,2,Overview,"00:11:24,880","00:11:27,554",140,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=684,And we're going to use
cs-410_7_2_141,cs-410,7,2,Overview,"00:11:27,554","00:11:30,962",141,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=687,the topics that we'll cover
cs-410_7_2_142,cs-410,7,2,Overview,"00:11:30,962","00:11:40,962",142,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=690,[MUSIC]
cs-410_7_3_1,cs-410,7,3,Natural,"00:00:00,300","00:00:03,380",1,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=0,[SOUND]
cs-410_7_3_2,cs-410,7,3,Natural,"00:00:09,170","00:00:13,893",2,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=9,This lecture is about natural language
cs-410_7_3_3,cs-410,7,3,Natural,"00:00:13,893","00:00:16,330",3,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=13,content analysis.
cs-410_7_3_4,cs-410,7,3,Natural,"00:00:16,330","00:00:21,510",4,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=16,Natural language content analysis
cs-410_7_3_5,cs-410,7,3,Natural,"00:00:21,510","00:00:23,780",5,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=21,So we're going to first talk about this.
cs-410_7_3_6,cs-410,7,3,Natural,"00:00:24,980","00:00:26,330",6,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=24,"And in particular,"
cs-410_7_3_7,cs-410,7,3,Natural,"00:00:26,330","00:00:31,540",7,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=26,natural language processing with
cs-410_7_3_8,cs-410,7,3,Natural,"00:00:33,210","00:00:38,230",8,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=33,And this determines what algorithms can
cs-410_7_3_9,cs-410,7,3,Natural,"00:00:40,820","00:00:44,991",9,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=40,We're going to take a look at the basic
cs-410_7_3_10,cs-410,7,3,Natural,"00:00:46,330","00:00:48,970",10,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=46,And I'm going to explain these concepts
cs-410_7_3_11,cs-410,7,3,Natural,"00:00:48,970","00:00:52,600",11,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=48,using a similar example
cs-410_7_3_12,cs-410,7,3,Natural,"00:00:52,600","00:00:55,650",12,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=52,A dog is chasing a boy on the playground.
cs-410_7_3_13,cs-410,7,3,Natural,"00:00:55,650","00:00:58,310",13,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=55,Now this is a very simple sentence.
cs-410_7_3_14,cs-410,7,3,Natural,"00:00:58,310","00:01:01,160",14,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=58,When we read such a sentence
cs-410_7_3_15,cs-410,7,3,Natural,"00:01:01,160","00:01:05,200",15,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=61,about it to get the meaning of it.
cs-410_7_3_16,cs-410,7,3,Natural,"00:01:05,200","00:01:09,460",16,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=65,But when a computer has to
cs-410_7_3_17,cs-410,7,3,Natural,"00:01:09,460","00:01:12,340",17,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=69,the computer has to go
cs-410_7_3_18,cs-410,7,3,Natural,"00:01:13,430","00:01:16,532",18,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=73,"First, the computer needs"
cs-410_7_3_19,cs-410,7,3,Natural,"00:01:16,532","00:01:18,630",19,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=76,how to segment the words in English.
cs-410_7_3_20,cs-410,7,3,Natural,"00:01:18,630","00:01:22,010",20,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=78,"And this is very easy,"
cs-410_7_3_21,cs-410,7,3,Natural,"00:01:22,010","00:01:26,136",21,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=82,And then the computer will need
cs-410_7_3_22,cs-410,7,3,Natural,"00:01:26,136","00:01:27,870",22,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=86,syntactical categories.
cs-410_7_3_23,cs-410,7,3,Natural,"00:01:27,870","00:01:34,510",23,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=87,"So for example, dog is a noun,"
cs-410_7_3_24,cs-410,7,3,Natural,"00:01:34,510","00:01:37,350",24,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=94,And this is called a Lexical analysis.
cs-410_7_3_25,cs-410,7,3,Natural,"00:01:37,350","00:01:41,590",25,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=97,"In particular, tagging these words"
cs-410_7_3_26,cs-410,7,3,Natural,"00:01:41,590","00:01:43,270",26,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=101,is called a part-of-speech tagging.
cs-410_7_3_27,cs-410,7,3,Natural,"00:01:45,030","00:01:48,383",27,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=105,After that the computer also needs to
cs-410_7_3_28,cs-410,7,3,Natural,"00:01:48,383","00:01:49,040",28,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=108,these words.
cs-410_7_3_29,cs-410,7,3,Natural,"00:01:49,040","00:01:53,300",29,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=109,So a and dog would form a noun phrase.
cs-410_7_3_30,cs-410,7,3,Natural,"00:01:53,300","00:01:57,590",30,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=113,On the playground would be
cs-410_7_3_31,cs-410,7,3,Natural,"00:01:57,590","00:02:01,378",31,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=117,And there is certain way for
cs-410_7_3_32,cs-410,7,3,Natural,"00:02:01,378","00:02:03,620",32,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=121,them to create meaning.
cs-410_7_3_33,cs-410,7,3,Natural,"00:02:03,620","00:02:06,469",33,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=123,Some other combinations
cs-410_7_3_34,cs-410,7,3,Natural,"00:02:07,720","00:02:12,620",34,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=127,"And this is called syntactical parsing, or"
cs-410_7_3_35,cs-410,7,3,Natural,"00:02:12,620","00:02:17,090",35,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=132,"syntactical analysis,"
cs-410_7_3_36,cs-410,7,3,Natural,"00:02:17,090","00:02:21,180",36,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=137,The outcome is a parse tree
cs-410_7_3_37,cs-410,7,3,Natural,"00:02:21,180","00:02:24,050",37,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=141,That tells us the structure
cs-410_7_3_38,cs-410,7,3,Natural,"00:02:24,050","00:02:27,430",38,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=144,that we know how we can
cs-410_7_3_39,cs-410,7,3,Natural,"00:02:27,430","00:02:29,740",39,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=147,But this is not semantics yet.
cs-410_7_3_40,cs-410,7,3,Natural,"00:02:29,740","00:02:34,530",40,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=149,So in order to get the meaning we
cs-410_7_3_41,cs-410,7,3,Natural,"00:02:34,530","00:02:39,860",41,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=154,these structures into some real world
cs-410_7_3_42,cs-410,7,3,Natural,"00:02:39,860","00:02:45,500",42,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=159,"So dog is a concept that we know,"
cs-410_7_3_43,cs-410,7,3,Natural,"00:02:45,500","00:02:50,870",43,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=165,So connecting these phrases
cs-410_7_3_44,cs-410,7,3,Natural,"00:02:52,160","00:02:58,788",44,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=172,"Now for a computer, would have to formally"
cs-410_7_3_45,cs-410,7,3,Natural,"00:02:58,788","00:03:03,630",45,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=178,"So dog, d1 means d1 is a dog."
cs-410_7_3_46,cs-410,7,3,Natural,"00:03:04,690","00:03:09,420",46,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=184,"Boy, b1 means b1 refers to a boy etc."
cs-410_7_3_47,cs-410,7,3,Natural,"00:03:09,420","00:03:13,430",47,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=189,And also represents the chasing
cs-410_7_3_48,cs-410,7,3,Natural,"00:03:13,430","00:03:18,334",48,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=193,"So, chasing is a predicate here with"
cs-410_7_3_49,cs-410,7,3,Natural,"00:03:18,334","00:03:23,720",49,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=198,"three arguments, d1, b1, and p1."
cs-410_7_3_50,cs-410,7,3,Natural,"00:03:23,720","00:03:25,920",50,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=203,Which is playground.
cs-410_7_3_51,cs-410,7,3,Natural,"00:03:25,920","00:03:31,320",51,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=205,So this formal rendition of
cs-410_7_3_52,cs-410,7,3,Natural,"00:03:31,320","00:03:35,950",52,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=211,"Once we reach that level of understanding,"
cs-410_7_3_53,cs-410,7,3,Natural,"00:03:35,950","00:03:42,050",53,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=215,"For example, if we assume there's a rule"
cs-410_7_3_54,cs-410,7,3,Natural,"00:03:42,050","00:03:48,420",54,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=222,"the person can get scared, then we"
cs-410_7_3_55,cs-410,7,3,Natural,"00:03:48,420","00:03:52,800",55,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=228,"This is the inferred meaning,"
cs-410_7_3_56,cs-410,7,3,Natural,"00:03:52,800","00:03:58,485",56,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=232,"And finally, we might even further infer"
cs-410_7_3_57,cs-410,7,3,Natural,"00:03:58,485","00:04:06,170",57,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=238,"what this sentence is requesting,"
cs-410_7_3_58,cs-410,7,3,Natural,"00:04:06,170","00:04:12,920",58,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=246,or why the person who say it in
cs-410_7_3_59,cs-410,7,3,Natural,"00:04:12,920","00:04:18,310",59,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=252,"And so, this has to do with"
cs-410_7_3_60,cs-410,7,3,Natural,"00:04:18,310","00:04:24,550",60,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=258,This is called speech act analysis or
cs-410_7_3_61,cs-410,7,3,Natural,"00:04:24,550","00:04:27,920",61,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=264,Which first to the use of language.
cs-410_7_3_62,cs-410,7,3,Natural,"00:04:27,920","00:04:32,704",62,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=267,"So, in this case a person saying this"
cs-410_7_3_63,cs-410,7,3,Natural,"00:04:32,704","00:04:34,070",63,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=272,bring back the dog.
cs-410_7_3_64,cs-410,7,3,Natural,"00:04:35,240","00:04:42,320",64,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=275,"So this means when saying a sentence,"
cs-410_7_3_65,cs-410,7,3,Natural,"00:04:42,320","00:04:44,769",65,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=282,So the action here is to make a request.
cs-410_7_3_66,cs-410,7,3,Natural,"00:04:46,770","00:04:51,408",66,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=286,"Now, this slide clearly shows that"
cs-410_7_3_67,cs-410,7,3,Natural,"00:04:51,408","00:04:55,720",67,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=291,a sentence there are a lot of
cs-410_7_3_68,cs-410,7,3,Natural,"00:04:55,720","00:05:00,337",68,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=295,"Now, in general it's very hard for"
cs-410_7_3_69,cs-410,7,3,Natural,"00:05:00,337","00:05:04,910",69,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=300,especially if you would want
cs-410_7_3_70,cs-410,7,3,Natural,"00:05:04,910","00:05:06,450",70,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=304,This is very difficult.
cs-410_7_3_71,cs-410,7,3,Natural,"00:05:08,190","00:05:11,094",71,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=308,"Now, the main reason why natural"
cs-410_7_3_72,cs-410,7,3,Natural,"00:05:11,094","00:05:14,820",72,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=311,it's because it's designed it will
cs-410_7_3_73,cs-410,7,3,Natural,"00:05:15,990","00:05:20,040",73,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=315,"As a result, for example,"
cs-410_7_3_74,cs-410,7,3,Natural,"00:05:21,250","00:05:25,150",74,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=321,Because we assume all of
cs-410_7_3_75,cs-410,7,3,Natural,"00:05:25,150","00:05:28,170",75,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=325,there's no need to encode this knowledge.
cs-410_7_3_76,cs-410,7,3,Natural,"00:05:29,780","00:05:31,360",76,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=329,That makes communication efficient.
cs-410_7_3_77,cs-410,7,3,Natural,"00:05:32,480","00:05:37,460",77,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=332,"We also keep a lot of ambiguities,"
cs-410_7_3_78,cs-410,7,3,Natural,"00:05:39,090","00:05:45,130",78,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=339,"And this is again, because we assume we"
cs-410_7_3_79,cs-410,7,3,Natural,"00:05:45,130","00:05:48,800",79,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=345,"So, there's no problem with"
cs-410_7_3_80,cs-410,7,3,Natural,"00:05:48,800","00:05:50,869",80,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=348,possibly different things
cs-410_7_3_81,cs-410,7,3,Natural,"00:05:52,610","00:05:55,880",81,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=352,Yet for
cs-410_7_3_82,cs-410,7,3,Natural,"00:05:55,880","00:06:00,250",82,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=355,because a computer does not have
cs-410_7_3_83,cs-410,7,3,Natural,"00:06:00,250","00:06:03,620",83,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=360,So the computer will be confused indeed.
cs-410_7_3_84,cs-410,7,3,Natural,"00:06:03,620","00:06:06,980",84,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=363,And this makes it hard for
cs-410_7_3_85,cs-410,7,3,Natural,"00:06:06,980","00:06:09,440",85,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=366,"Indeed, it makes it very hard for"
cs-410_7_3_86,cs-410,7,3,Natural,"00:06:09,440","00:06:15,140",86,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=369,every step in the slide
cs-410_7_3_87,cs-410,7,3,Natural,"00:06:16,550","00:06:19,380",87,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=376,Ambiguity is a main killer.
cs-410_7_3_88,cs-410,7,3,Natural,"00:06:19,380","00:06:22,820",88,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=379,Meaning that in every step
cs-410_7_3_89,cs-410,7,3,Natural,"00:06:22,820","00:06:26,790",89,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=382,and the computer would have to
cs-410_7_3_90,cs-410,7,3,Natural,"00:06:26,790","00:06:30,550",90,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=386,that decision can be very difficult
cs-410_7_3_91,cs-410,7,3,Natural,"00:06:31,690","00:06:32,300",91,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=391,"And in general,"
cs-410_7_3_92,cs-410,7,3,Natural,"00:06:32,300","00:06:37,530",92,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=392,we need common sense reasoning in order
cs-410_7_3_93,cs-410,7,3,Natural,"00:06:37,530","00:06:40,595",93,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=397,And computers today don't yet have that.
cs-410_7_3_94,cs-410,7,3,Natural,"00:06:40,595","00:06:42,820",94,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=400,That's why it's very hard for
cs-410_7_3_95,cs-410,7,3,Natural,"00:06:42,820","00:06:47,310",95,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=402,computers to precisely understand
cs-410_7_3_96,cs-410,7,3,Natural,"00:06:48,310","00:06:51,280",96,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=408,So here are some specific
cs-410_7_3_97,cs-410,7,3,Natural,"00:06:51,280","00:06:53,390",97,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=411,Think about the world-level ambiguity.
cs-410_7_3_98,cs-410,7,3,Natural,"00:06:53,390","00:06:56,940",98,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=413,A word like design can be a noun or
cs-410_7_3_99,cs-410,7,3,Natural,"00:06:56,940","00:06:59,200",99,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=416,we've got ambiguous part of speech tag.
cs-410_7_3_100,cs-410,7,3,Natural,"00:07:00,980","00:07:06,190",100,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=420,"Root also has multiple meanings,"
cs-410_7_3_101,cs-410,7,3,Natural,"00:07:06,190","00:07:10,670",101,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=426,"like in the square of, or"
cs-410_7_3_102,cs-410,7,3,Natural,"00:07:12,310","00:07:17,310",102,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=432,Syntactic ambiguity refers
cs-410_7_3_103,cs-410,7,3,Natural,"00:07:19,440","00:07:21,670",103,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=439,of a sentence in terms structures.
cs-410_7_3_104,cs-410,7,3,Natural,"00:07:21,670","00:07:23,010",104,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=441,"So for example,"
cs-410_7_3_105,cs-410,7,3,Natural,"00:07:23,010","00:07:26,219",105,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=443,natural language processing can
cs-410_7_3_106,cs-410,7,3,Natural,"00:07:28,240","00:07:33,410",106,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=448,So one is the ordinary meaning that we
cs-410_7_3_107,cs-410,7,3,Natural,"00:07:33,410","00:07:38,690",107,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=453,will be getting as we're
cs-410_7_3_108,cs-410,7,3,Natural,"00:07:38,690","00:07:41,670",108,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=458,"So, it's processing of natural language."
cs-410_7_3_109,cs-410,7,3,Natural,"00:07:41,670","00:07:44,600",109,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=461,But there's is also another
cs-410_7_3_110,cs-410,7,3,Natural,"00:07:44,600","00:07:47,190",110,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=464,which is to say language
cs-410_7_3_111,cs-410,7,3,Natural,"00:07:48,950","00:07:53,500",111,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=468,"Now we don't generally have this problem,"
cs-410_7_3_112,cs-410,7,3,Natural,"00:07:53,500","00:07:56,960",112,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=473,"the structure, the computer would have"
cs-410_7_3_113,cs-410,7,3,Natural,"00:07:59,040","00:08:03,530",113,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=479,Another classic example is a man
cs-410_7_3_114,cs-410,7,3,Natural,"00:08:03,530","00:08:10,230",114,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=483,And this ambiguity lies in
cs-410_7_3_115,cs-410,7,3,Natural,"00:08:10,230","00:08:13,630",115,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=490,This is called a prepositional
cs-410_7_3_116,cs-410,7,3,Natural,"00:08:14,960","00:08:20,440",116,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=494,Meaning where to attach this
cs-410_7_3_117,cs-410,7,3,Natural,"00:08:20,440","00:08:22,670",117,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=500,Should it modify the boy?
cs-410_7_3_118,cs-410,7,3,Natural,"00:08:22,670","00:08:28,330",118,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=502,"Or should it be modifying, saw, the verb."
cs-410_7_3_119,cs-410,7,3,Natural,"00:08:28,330","00:08:31,330",119,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=508,Another problem is anaphora resolution.
cs-410_7_3_120,cs-410,7,3,Natural,"00:08:31,330","00:08:35,740",120,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=511,In John persuaded Bill to buy a TV for
cs-410_7_3_121,cs-410,7,3,Natural,"00:08:35,740","00:08:37,960",121,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=515,Does himself refer to John or Bill?
cs-410_7_3_122,cs-410,7,3,Natural,"00:08:39,380","00:08:41,790",122,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=519,Presupposition is another difficulty.
cs-410_7_3_123,cs-410,7,3,Natural,"00:08:41,790","00:08:45,459",123,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=521,He has quit smoking implies
cs-410_7_3_124,cs-410,7,3,Natural,"00:08:45,459","00:08:50,180",124,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=525,we need to have such a knowledge in
cs-410_7_3_125,cs-410,7,3,Natural,"00:08:52,630","00:08:57,614",125,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=532,"Because of these problems, the state"
cs-410_7_3_126,cs-410,7,3,Natural,"00:08:57,614","00:09:01,410",126,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=537,techniques can not do anything perfectly.
cs-410_7_3_127,cs-410,7,3,Natural,"00:09:01,410","00:09:04,560",127,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=541,Even for
cs-410_7_3_128,cs-410,7,3,Natural,"00:09:04,560","00:09:07,700",128,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=544,we still can not solve the whole problem.
cs-410_7_3_129,cs-410,7,3,Natural,"00:09:07,700","00:09:12,930",129,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=547,"The accuracy that are listed here,"
cs-410_7_3_130,cs-410,7,3,Natural,"00:09:12,930","00:09:16,100",130,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=552,was just taken from some studies earlier.
cs-410_7_3_131,cs-410,7,3,Natural,"00:09:17,330","00:09:22,840",131,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=557,And these studies obviously have to
cs-410_7_3_132,cs-410,7,3,Natural,"00:09:22,840","00:09:27,640",132,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=562,the numbers here are not
cs-410_7_3_133,cs-410,7,3,Natural,"00:09:27,640","00:09:33,210",133,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=567,take it out of the context of the data
cs-410_7_3_134,cs-410,7,3,Natural,"00:09:33,210","00:09:39,350",134,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=573,But I show these numbers mainly to give
cs-410_7_3_135,cs-410,7,3,Natural,"00:09:39,350","00:09:42,080",135,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=579,or how well we can do things like this.
cs-410_7_3_136,cs-410,7,3,Natural,"00:09:42,080","00:09:47,670",136,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=582,It doesn't mean any data set
cs-410_7_3_137,cs-410,7,3,Natural,"00:09:47,670","00:09:52,780",137,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=587,"But, in general, we can do parsing speech"
cs-410_7_3_138,cs-410,7,3,Natural,"00:09:53,980","00:09:59,030",138,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=593,"Parsing would be more difficult, but for"
cs-410_7_3_139,cs-410,7,3,Natural,"00:09:59,030","00:10:04,870",139,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=599,"phrases correct, we can probably"
cs-410_7_3_140,cs-410,7,3,Natural,"00:10:06,920","00:10:12,330",140,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=606,But to get the complete parse tree
cs-410_7_3_141,cs-410,7,3,Natural,"00:10:13,610","00:10:18,210",141,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=613,"For semantic analysis, we can also do"
cs-410_7_3_142,cs-410,7,3,Natural,"00:10:18,210","00:10:22,570",142,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=618,"particularly, extraction of entities and"
cs-410_7_3_143,cs-410,7,3,Natural,"00:10:22,570","00:10:27,910",143,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=622,"For example, recognizing this is"
cs-410_7_3_144,cs-410,7,3,Natural,"00:10:27,910","00:10:33,380",144,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=627,this person and
cs-410_7_3_145,cs-410,7,3,Natural,"00:10:33,380","00:10:36,470",145,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=633,We can also do word sense to some extent.
cs-410_7_3_146,cs-410,7,3,Natural,"00:10:38,000","00:10:45,360",146,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=638,The occurrence of root in this sentence
cs-410_7_3_147,cs-410,7,3,Natural,"00:10:45,360","00:10:49,330",147,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=645,Sentiment analysis is another aspect
cs-410_7_3_148,cs-410,7,3,Natural,"00:10:50,480","00:10:55,840",148,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=650,That means we can tag the senses
cs-410_7_3_149,cs-410,7,3,Natural,"00:10:55,840","00:11:00,670",149,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=655,it's talking about the product or
cs-410_7_3_150,cs-410,7,3,Natural,"00:11:02,790","00:11:08,600",150,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=662,"Inference, however, is very hard,"
cs-410_7_3_151,cs-410,7,3,Natural,"00:11:08,600","00:11:14,040",151,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=668,any big domain and if it's only
cs-410_7_3_152,cs-410,7,3,Natural,"00:11:14,040","00:11:18,800",152,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=674,And that's a generally difficult
cs-410_7_3_153,cs-410,7,3,Natural,"00:11:18,800","00:11:21,961",153,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=678,Speech act analysis is
cs-410_7_3_154,cs-410,7,3,Natural,"00:11:21,961","00:11:26,480",154,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=681,we can only do this probably for
cs-410_7_3_155,cs-410,7,3,Natural,"00:11:26,480","00:11:32,090",155,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=686,And with a lot of help from humans
cs-410_7_3_156,cs-410,7,3,Natural,"00:11:32,090","00:11:34,180",156,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=692,the computers to learn from.
cs-410_7_3_157,cs-410,7,3,Natural,"00:11:36,380","00:11:38,890",157,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=696,So the slide also shows that
cs-410_7_3_158,cs-410,7,3,Natural,"00:11:38,890","00:11:44,300",158,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=698,computers are far from being able to
cs-410_7_3_159,cs-410,7,3,Natural,"00:11:44,300","00:11:50,320",159,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=704,And that also explains why the text
cs-410_7_3_160,cs-410,7,3,Natural,"00:11:50,320","00:11:54,390",160,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=710,Because we cannot rely on
cs-410_7_3_161,cs-410,7,3,Natural,"00:11:54,390","00:11:58,940",161,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=714,computational methods to
cs-410_7_3_162,cs-410,7,3,Natural,"00:11:58,940","00:12:04,770",162,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=718,"Therefore, we have to use"
cs-410_7_3_163,cs-410,7,3,Natural,"00:12:04,770","00:12:10,090",163,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=724,A particular statistical machine learning
cs-410_7_3_164,cs-410,7,3,Natural,"00:12:10,090","00:12:16,092",164,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=730,to try to get as much meaning
cs-410_7_3_165,cs-410,7,3,Natural,"00:12:16,092","00:12:19,320",165,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=736,"And, later you will see"
cs-410_7_3_166,cs-410,7,3,Natural,"00:12:20,360","00:12:25,450",166,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=740,many such algorithms
cs-410_7_3_167,cs-410,7,3,Natural,"00:12:25,450","00:12:30,790",167,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=745,interesting model from text even though
cs-410_7_3_168,cs-410,7,3,Natural,"00:12:30,790","00:12:36,010",168,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=750,Meaning of all the natural
cs-410_7_3_169,cs-410,7,3,Natural,"00:12:36,010","00:12:46,010",169,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=756,[MUSIC]
cs-410_7_4_1,cs-410,7,4,Natural,"00:00:00,266","00:00:05,440",1,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=0,[SOUND]
cs-410_7_4_2,cs-410,7,4,Natural,"00:00:10,218","00:00:13,988",2,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=10,So here are some specific examples of what
cs-410_7_4_3,cs-410,7,4,Natural,"00:00:13,988","00:00:15,926",3,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=13,we can't do today and
cs-410_7_4_4,cs-410,7,4,Natural,"00:00:15,926","00:00:21,970",4,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=15,part of speech tagging is still
cs-410_7_4_5,cs-410,7,4,Natural,"00:00:21,970","00:00:27,938",5,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=21,"So in the example, he turned off the"
cs-410_7_4_6,cs-410,7,4,Natural,"00:00:27,938","00:00:33,342",6,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=27,the two offs actually have somewhat
cs-410_7_4_7,cs-410,7,4,Natural,"00:00:33,342","00:00:39,633",7,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=33,categories and also its very difficult
cs-410_7_4_8,cs-410,7,4,Natural,"00:00:39,633","00:00:44,450",8,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=39,"Again, the example, a man saw a boy"
cs-410_7_4_9,cs-410,7,4,Natural,"00:00:44,450","00:00:48,400",9,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=44,be very difficult to parse
cs-410_7_4_10,cs-410,7,4,Natural,"00:00:48,400","00:00:52,278",10,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=48,Precise deep semantic
cs-410_7_4_11,cs-410,7,4,Natural,"00:00:52,278","00:00:55,648",11,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=52,"For example, to define the meaning of own,"
cs-410_7_4_12,cs-410,7,4,Natural,"00:00:55,648","00:01:01,493",12,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=55,precisely is very difficult in
cs-410_7_4_13,cs-410,7,4,Natural,"00:01:01,493","00:01:04,737",13,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=61,So the state of the off can
cs-410_7_4_14,cs-410,7,4,Natural,"00:01:04,737","00:01:05,506",14,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=64,Robust and
cs-410_7_4_15,cs-410,7,4,Natural,"00:01:05,506","00:01:11,070",15,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=65,general NLP tends to be shallow while
cs-410_7_4_16,cs-410,7,4,Natural,"00:01:12,430","00:01:18,565",16,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=72,"For this reason in this course,"
cs-410_7_4_17,cs-410,7,4,Natural,"00:01:18,565","00:01:23,610",17,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=78,"general, shallow techniques for"
cs-410_7_4_18,cs-410,7,4,Natural,"00:01:23,610","00:01:29,630",18,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=83,mining text data and they are generally
cs-410_7_4_19,cs-410,7,4,Natural,"00:01:29,630","00:01:34,510",19,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=89,So there are robust and
cs-410_7_4_20,cs-410,7,4,Natural,"00:01:36,540","00:01:39,550",20,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=96,the in category of shallow analysis.
cs-410_7_4_21,cs-410,7,4,Natural,"00:01:39,550","00:01:44,320",21,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=99,So such techniques have
cs-410_7_4_22,cs-410,7,4,Natural,"00:01:44,320","00:01:49,099",22,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=104,applied to any text data in
cs-410_7_4_23,cs-410,7,4,Natural,"00:01:49,099","00:01:55,425",23,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=109,"But the downside is that, they don't"
cs-410_7_4_24,cs-410,7,4,Natural,"00:01:55,425","00:01:59,159",24,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=115,"For that, we have to rely on"
cs-410_7_4_25,cs-410,7,4,Natural,"00:02:00,960","00:02:05,930",25,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=120,That typically would require
cs-410_7_4_26,cs-410,7,4,Natural,"00:02:05,930","00:02:10,940",26,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=125,a lot of examples of analysis that would
cs-410_7_4_27,cs-410,7,4,Natural,"00:02:10,940","00:02:16,120",27,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=130,machine learning techniques and learn from
cs-410_7_4_28,cs-410,7,4,Natural,"00:02:16,120","00:02:21,880",28,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=136,"So in practical applications, we generally"
cs-410_7_4_29,cs-410,7,4,Natural,"00:02:21,880","00:02:29,150",29,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=141,with the general statistical and
cs-410_7_4_30,cs-410,7,4,Natural,"00:02:29,150","00:02:32,010",30,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=149,These can be applied to any text data.
cs-410_7_4_31,cs-410,7,4,Natural,"00:02:32,010","00:02:37,060",31,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=152,"And on top of that, we're going to use"
cs-410_7_4_32,cs-410,7,4,Natural,"00:02:37,060","00:02:42,770",32,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=157,to use supervised machine learning
cs-410_7_4_33,cs-410,7,4,Natural,"00:02:42,770","00:02:48,640",33,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=162,especially for those important
cs-410_7_4_34,cs-410,7,4,Natural,"00:02:48,640","00:02:55,170",34,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=168,to analyze text data more precisely.
cs-410_7_4_35,cs-410,7,4,Natural,"00:02:55,170","00:03:00,036",35,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=175,But this course will cover
cs-410_7_4_36,cs-410,7,4,Natural,"00:03:00,036","00:03:04,177",36,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=180,"that generally,"
cs-410_7_4_37,cs-410,7,4,Natural,"00:03:04,177","00:03:09,386",37,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=184,"So they're practically,"
cs-410_7_4_38,cs-410,7,4,Natural,"00:03:09,386","00:03:16,409",38,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=189,analysis techniques that require a lot of
cs-410_7_4_39,cs-410,7,4,Natural,"00:03:16,409","00:03:21,302",39,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=196,"So to summarize,"
cs-410_7_4_40,cs-410,7,4,Natural,"00:03:21,302","00:03:24,580",40,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=201,is the foundation for text mining.
cs-410_7_4_41,cs-410,7,4,Natural,"00:03:24,580","00:03:27,465",41,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=204,"So obviously, the better we"
cs-410_7_4_42,cs-410,7,4,Natural,"00:03:27,465","00:03:29,090",42,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=207,the better we can do text mining.
cs-410_7_4_43,cs-410,7,4,Natural,"00:03:30,420","00:03:34,930",43,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=210,Computers today are far from being able
cs-410_7_4_44,cs-410,7,4,Natural,"00:03:34,930","00:03:38,030",44,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=214,Deep NLP requires common sense
cs-410_7_4_45,cs-410,7,4,Natural,"00:03:38,030","00:03:42,803",45,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=218,"Thus, only working for"
cs-410_7_4_46,cs-410,7,4,Natural,"00:03:42,803","00:03:44,833",46,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=222,large scale text mining.
cs-410_7_4_47,cs-410,7,4,Natural,"00:03:44,833","00:03:50,003",47,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=224,Shallow NLP based on statistical
cs-410_7_4_48,cs-410,7,4,Natural,"00:03:50,003","00:03:52,543",48,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=230,is the main topic of this course and
cs-410_7_4_49,cs-410,7,4,Natural,"00:03:52,543","00:03:56,763",49,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=232,they are generally applicable
cs-410_7_4_50,cs-410,7,4,Natural,"00:03:56,763","00:04:02,081",50,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=236,"They are in some sense also,"
cs-410_7_4_51,cs-410,7,4,Natural,"00:04:02,081","00:04:06,834",51,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=242,"In practice,"
cs-410_7_4_52,cs-410,7,4,Natural,"00:04:06,834","00:04:11,810",52,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=246,we'll have humans for
cs-410_7_4_53,cs-410,7,4,Natural,"00:04:11,810","00:04:21,810",53,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=251,[MUSIC]
cs-410_7_5_1,cs-410,7,5,Text,"00:00:06,440","00:00:11,320",1,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=6,This lecture is about the
cs-410_7_5_2,cs-410,7,5,Text,"00:00:12,020","00:00:14,100",2,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=12,"In this lecture, we are going"
cs-410_7_5_3,cs-410,7,5,Text,"00:00:14,100","00:00:16,680",3,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=14,to discuss textual
cs-410_7_5_4,cs-410,7,5,Text,"00:00:16,680","00:00:20,475",4,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=16,and discuss how natural
cs-410_7_5_5,cs-410,7,5,Text,"00:00:20,475","00:00:24,450",5,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=20,allow us to represent text
cs-410_7_5_6,cs-410,7,5,Text,"00:00:24,450","00:00:28,590",6,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=24,Let's take a look at this
cs-410_7_5_7,cs-410,7,5,Text,"00:00:28,590","00:00:33,900",7,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=28,We can represent this sentence
cs-410_7_5_8,cs-410,7,5,Text,"00:00:33,900","00:00:37,680",8,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=33,"First, we can always"
cs-410_7_5_9,cs-410,7,5,Text,"00:00:37,680","00:00:42,090",9,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=37,represent such a sentence
cs-410_7_5_10,cs-410,7,5,Text,"00:00:42,090","00:00:45,135",10,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=42,This is true for
cs-410_7_5_11,cs-410,7,5,Text,"00:00:45,135","00:00:49,210",11,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=45,when we store them
cs-410_7_5_12,cs-410,7,5,Text,"00:00:49,310","00:00:53,480",12,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=49,When we store a natural
cs-410_7_5_13,cs-410,7,5,Text,"00:00:53,480","00:00:55,805",13,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=53,"as a string of characters,"
cs-410_7_5_14,cs-410,7,5,Text,"00:00:55,805","00:01:00,350",14,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=55,we have perhaps the most general
cs-410_7_5_15,cs-410,7,5,Text,"00:01:00,350","00:01:02,270",15,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=60,since we always use
cs-410_7_5_16,cs-410,7,5,Text,"00:01:02,270","00:01:05,360",16,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=62,this approach to
cs-410_7_5_17,cs-410,7,5,Text,"00:01:05,360","00:01:10,070",17,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=65,"But unfortunately, using"
cs-410_7_5_18,cs-410,7,5,Text,"00:01:10,070","00:01:12,950",18,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=70,"help us to do semantic analysis,"
cs-410_7_5_19,cs-410,7,5,Text,"00:01:12,950","00:01:14,135",19,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=72,which is often needed
cs-410_7_5_20,cs-410,7,5,Text,"00:01:14,135","00:01:17,600",20,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=74,for many applications
cs-410_7_5_21,cs-410,7,5,Text,"00:01:17,600","00:01:21,650",21,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=77,The reason is because we're
cs-410_7_5_22,cs-410,7,5,Text,"00:01:21,650","00:01:22,880",22,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=81,"So as a string,"
cs-410_7_5_23,cs-410,7,5,Text,"00:01:22,880","00:01:25,100",23,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=82,we're going to keep
cs-410_7_5_24,cs-410,7,5,Text,"00:01:25,100","00:01:29,005",24,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=85,and these ASCII symbols.
cs-410_7_5_25,cs-410,7,5,Text,"00:01:29,005","00:01:32,090",25,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=89,We can perhaps count what's
cs-410_7_5_26,cs-410,7,5,Text,"00:01:32,090","00:01:35,225",26,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=92,the most frequent character
cs-410_7_5_27,cs-410,7,5,Text,"00:01:35,225","00:01:38,960",27,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=95,or the correlation
cs-410_7_5_28,cs-410,7,5,Text,"00:01:38,960","00:01:43,075",28,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=98,but we can't really
cs-410_7_5_29,cs-410,7,5,Text,"00:01:43,075","00:01:47,300",29,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=103,"Yet, this is the most"
cs-410_7_5_30,cs-410,7,5,Text,"00:01:47,300","00:01:49,250",30,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=107,text because we can use
cs-410_7_5_31,cs-410,7,5,Text,"00:01:49,250","00:01:53,045",31,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=109,this to represent any
cs-410_7_5_32,cs-410,7,5,Text,"00:01:53,045","00:01:55,220",32,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=113,If we try to do
cs-410_7_5_33,cs-410,7,5,Text,"00:01:55,220","00:01:57,635",33,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=115,a little bit more natural
cs-410_7_5_34,cs-410,7,5,Text,"00:01:57,635","00:02:00,540",34,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=117,"by doing word segmentation,"
cs-410_7_5_35,cs-410,7,5,Text,"00:02:00,540","00:02:05,210",35,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=120,then we can obtain a
cs-410_7_5_36,cs-410,7,5,Text,"00:02:05,210","00:02:08,315",36,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=125,but in the form of a
cs-410_7_5_37,cs-410,7,5,Text,"00:02:08,315","00:02:11,750",37,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=128,So here we see that
cs-410_7_5_38,cs-410,7,5,Text,"00:02:11,750","00:02:17,100",38,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=131,words like a dog is chasing etc.
cs-410_7_5_39,cs-410,7,5,Text,"00:02:17,230","00:02:20,600",39,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=137,Now with this level
cs-410_7_5_40,cs-410,7,5,Text,"00:02:20,600","00:02:23,965",40,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=140,we certainly can do
cs-410_7_5_41,cs-410,7,5,Text,"00:02:23,965","00:02:27,065",41,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=143,and this is mainly because
cs-410_7_5_42,cs-410,7,5,Text,"00:02:27,065","00:02:30,275",42,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=147,of human communication
cs-410_7_5_43,cs-410,7,5,Text,"00:02:30,275","00:02:33,035",43,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=150,so they are very powerful.
cs-410_7_5_44,cs-410,7,5,Text,"00:02:33,035","00:02:36,080",44,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=153,"By identifying words, we can for"
cs-410_7_5_45,cs-410,7,5,Text,"00:02:36,080","00:02:38,780",45,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=156,example easily count what are
cs-410_7_5_46,cs-410,7,5,Text,"00:02:38,780","00:02:40,820",46,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=158,the most frequent words in
cs-410_7_5_47,cs-410,7,5,Text,"00:02:40,820","00:02:45,185",47,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=160,this document or in
cs-410_7_5_48,cs-410,7,5,Text,"00:02:45,185","00:02:48,200",48,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=165,These words can be used to form
cs-410_7_5_49,cs-410,7,5,Text,"00:02:48,200","00:02:51,940",49,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=168,topics when we combine
cs-410_7_5_50,cs-410,7,5,Text,"00:02:51,940","00:02:54,060",50,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=171,"and some words are positive,"
cs-410_7_5_51,cs-410,7,5,Text,"00:02:54,060","00:02:55,700",51,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=174,"some words negative, so we can"
cs-410_7_5_52,cs-410,7,5,Text,"00:02:55,700","00:02:58,410",52,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=175,also do sentiment analysis.
cs-410_7_5_53,cs-410,7,5,Text,"00:02:58,660","00:03:02,690",53,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=178,So representing text data
cs-410_7_5_54,cs-410,7,5,Text,"00:03:02,690","00:03:06,775",54,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=182,opens up a lot of interesting
cs-410_7_5_55,cs-410,7,5,Text,"00:03:06,775","00:03:09,060",55,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=186,"However, this level of"
cs-410_7_5_56,cs-410,7,5,Text,"00:03:09,060","00:03:12,080",56,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=189,representation is slightly
cs-410_7_5_57,cs-410,7,5,Text,"00:03:12,080","00:03:17,300",57,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=192,of characters because in
cs-410_7_5_58,cs-410,7,5,Text,"00:03:17,300","00:03:21,350",58,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=197,it's actually not
cs-410_7_5_59,cs-410,7,5,Text,"00:03:21,350","00:03:25,010",59,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=201,all the word boundaries
cs-410_7_5_60,cs-410,7,5,Text,"00:03:25,010","00:03:27,685",60,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=205,you see text as a sequence of
cs-410_7_5_61,cs-410,7,5,Text,"00:03:27,685","00:03:31,345",61,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=207,characters with
cs-410_7_5_62,cs-410,7,5,Text,"00:03:31,345","00:03:33,160",62,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=211,So you'll have to rely on
cs-410_7_5_63,cs-410,7,5,Text,"00:03:33,160","00:03:36,925",63,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=213,some special techniques
cs-410_7_5_64,cs-410,7,5,Text,"00:03:36,925","00:03:39,940",64,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=216,"In such a language,"
cs-410_7_5_65,cs-410,7,5,Text,"00:03:39,940","00:03:43,600",65,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=219,we might make mistakes
cs-410_7_5_66,cs-410,7,5,Text,"00:03:43,600","00:03:46,480",66,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=223,So the sequence of
cs-410_7_5_67,cs-410,7,5,Text,"00:03:46,480","00:03:50,230",67,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=226,not as robust as
cs-410_7_5_68,cs-410,7,5,Text,"00:03:50,230","00:03:53,230",68,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=230,"But in English, it's very"
cs-410_7_5_69,cs-410,7,5,Text,"00:03:53,230","00:03:56,005",69,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=233,easy to obtain this level
cs-410_7_5_70,cs-410,7,5,Text,"00:03:56,005","00:03:59,270",70,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=236,so we can do that all the time.
cs-410_7_5_71,cs-410,7,5,Text,"00:04:00,860","00:04:03,295",71,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=240,"Now, if we go further"
cs-410_7_5_72,cs-410,7,5,Text,"00:04:03,295","00:04:04,645",72,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=243,to do naturally
cs-410_7_5_73,cs-410,7,5,Text,"00:04:04,645","00:04:08,125",73,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=244,we can add a part of speech tags.
cs-410_7_5_74,cs-410,7,5,Text,"00:04:08,125","00:04:09,955",74,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=248,"Now once we do that,"
cs-410_7_5_75,cs-410,7,5,Text,"00:04:09,955","00:04:11,850",75,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=249,"we can count, for example,"
cs-410_7_5_76,cs-410,7,5,Text,"00:04:11,850","00:04:14,680",76,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=251,the most frequent
cs-410_7_5_77,cs-410,7,5,Text,"00:04:14,680","00:04:18,220",77,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=254,nouns are associated with
cs-410_7_5_78,cs-410,7,5,Text,"00:04:18,220","00:04:19,480",78,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=258,So this opens up
cs-410_7_5_79,cs-410,7,5,Text,"00:04:19,480","00:04:23,020",79,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=259,a little bit more
cs-410_7_5_80,cs-410,7,5,Text,"00:04:23,020","00:04:24,625",80,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=263,for further analysis.
cs-410_7_5_81,cs-410,7,5,Text,"00:04:24,625","00:04:28,270",81,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=264,Note that I use a plus sign
cs-410_7_5_82,cs-410,7,5,Text,"00:04:28,270","00:04:32,305",82,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=268,representing text as a sequence
cs-410_7_5_83,cs-410,7,5,Text,"00:04:32,305","00:04:35,395",83,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=272,we don't necessarily replace
cs-410_7_5_84,cs-410,7,5,Text,"00:04:35,395","00:04:37,840",84,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=275,the original word
cs-410_7_5_85,cs-410,7,5,Text,"00:04:37,840","00:04:40,975",85,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=277,"Instead, we add this as"
cs-410_7_5_86,cs-410,7,5,Text,"00:04:40,975","00:04:44,050",86,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=280,an additional way of
cs-410_7_5_87,cs-410,7,5,Text,"00:04:44,050","00:04:47,230",87,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=284,so that now the data is
cs-410_7_5_88,cs-410,7,5,Text,"00:04:47,230","00:04:50,965",88,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=287,of words and a sequence
cs-410_7_5_89,cs-410,7,5,Text,"00:04:50,965","00:04:54,055",89,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=290,This enriches the
cs-410_7_5_90,cs-410,7,5,Text,"00:04:54,055","00:04:59,160",90,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=294,and thus also enables
cs-410_7_5_91,cs-410,7,5,Text,"00:05:00,340","00:05:04,040",91,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=300,"If we go further, then we'll"
cs-410_7_5_92,cs-410,7,5,Text,"00:05:04,040","00:05:08,020",92,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=304,often to obtain
cs-410_7_5_93,cs-410,7,5,Text,"00:05:08,020","00:05:09,700",93,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=308,"Now this of course,"
cs-410_7_5_94,cs-410,7,5,Text,"00:05:09,700","00:05:12,230",94,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=309,further open up
cs-410_7_5_95,cs-410,7,5,Text,"00:05:12,230","00:05:14,840",95,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=312,"of, for example,"
cs-410_7_5_96,cs-410,7,5,Text,"00:05:14,840","00:05:22,520",96,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=314,the writing styles or
cs-410_7_5_97,cs-410,7,5,Text,"00:05:22,520","00:05:26,440",97,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=322,If we go further for
cs-410_7_5_98,cs-410,7,5,Text,"00:05:26,440","00:05:31,684",98,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=326,then we might be able to
cs-410_7_5_99,cs-410,7,5,Text,"00:05:31,684","00:05:35,515",99,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=331,and we also can recognize
cs-410_7_5_100,cs-410,7,5,Text,"00:05:35,515","00:05:38,055",100,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=335,and playground as a location.
cs-410_7_5_101,cs-410,7,5,Text,"00:05:38,055","00:05:41,335",101,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=338,We can further analyze
cs-410_7_5_102,cs-410,7,5,Text,"00:05:41,335","00:05:45,830",102,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=341,dog is chasing the boy and
cs-410_7_5_103,cs-410,7,5,Text,"00:05:45,830","00:05:48,875",103,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=345,Now this will add
cs-410_7_5_104,cs-410,7,5,Text,"00:05:48,875","00:05:52,945",104,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=348,relations through
cs-410_7_5_105,cs-410,7,5,Text,"00:05:52,945","00:05:54,790",105,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=352,"At this level,"
cs-410_7_5_106,cs-410,7,5,Text,"00:05:54,790","00:05:57,605",106,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=354,then we can do even more
cs-410_7_5_107,cs-410,7,5,Text,"00:05:57,605","00:05:59,795",107,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=357,"For example, now we"
cs-410_7_5_108,cs-410,7,5,Text,"00:05:59,795","00:06:02,360",108,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=359,the most frequent person that's
cs-410_7_5_109,cs-410,7,5,Text,"00:06:02,360","00:06:06,284",109,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=362,mentioning this whole collection
cs-410_7_5_110,cs-410,7,5,Text,"00:06:06,284","00:06:09,205",110,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=366,or whenever you
cs-410_7_5_111,cs-410,7,5,Text,"00:06:09,205","00:06:13,655",111,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=369,you also tend to see mentioning
cs-410_7_5_112,cs-410,7,5,Text,"00:06:13,655","00:06:19,480",112,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=373,So this is a very
cs-410_7_5_113,cs-410,7,5,Text,"00:06:19,480","00:06:21,830",113,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=379,and it's also related to
cs-410_7_5_114,cs-410,7,5,Text,"00:06:21,830","00:06:24,500",114,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=381,the knowledge graph that
cs-410_7_5_115,cs-410,7,5,Text,"00:06:24,500","00:06:27,620",115,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=384,of that Google is doing as
cs-410_7_5_116,cs-410,7,5,Text,"00:06:27,620","00:06:31,690",116,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=387,a more semantic way of
cs-410_7_5_117,cs-410,7,5,Text,"00:06:31,690","00:06:39,080",117,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=391,"However, it's also less robust"
cs-410_7_5_118,cs-410,7,5,Text,"00:06:39,080","00:06:42,410",118,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=399,even syntactical analysis
cs-410_7_5_119,cs-410,7,5,Text,"00:06:42,410","00:06:43,985",119,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=402,always easy to identify
cs-410_7_5_120,cs-410,7,5,Text,"00:06:43,985","00:06:46,160",120,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=403,all the entities with
cs-410_7_5_121,cs-410,7,5,Text,"00:06:46,160","00:06:47,735",121,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=406,"and we might make mistakes,"
cs-410_7_5_122,cs-410,7,5,Text,"00:06:47,735","00:06:50,270",122,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=407,and relations are
cs-410_7_5_123,cs-410,7,5,Text,"00:06:50,270","00:06:52,685",123,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=410,and we might make mistakes.
cs-410_7_5_124,cs-410,7,5,Text,"00:06:52,685","00:06:56,120",124,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=412,So this makes this level of
cs-410_7_5_125,cs-410,7,5,Text,"00:06:56,120","00:06:58,190",125,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=416,yet it's very useful.
cs-410_7_5_126,cs-410,7,5,Text,"00:06:58,190","00:07:01,700",126,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=418,Now if we move further
cs-410_7_5_127,cs-410,7,5,Text,"00:07:01,700","00:07:05,465",127,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=421,then we can have predicates
cs-410_7_5_128,cs-410,7,5,Text,"00:07:05,465","00:07:08,630",128,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=425,"With inference rules, we can"
cs-410_7_5_129,cs-410,7,5,Text,"00:07:08,630","00:07:13,700",129,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=428,infer interesting derived
cs-410_7_5_130,cs-410,7,5,Text,"00:07:13,700","00:07:15,020",130,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=433,so that's very useful.
cs-410_7_5_131,cs-410,7,5,Text,"00:07:15,020","00:07:17,420",131,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=435,"But unfortunately,"
cs-410_7_5_132,cs-410,7,5,Text,"00:07:17,420","00:07:19,940",132,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=437,representation is even less
cs-410_7_5_133,cs-410,7,5,Text,"00:07:19,940","00:07:22,010",133,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=439,robust and we can make
cs-410_7_5_134,cs-410,7,5,Text,"00:07:22,010","00:07:25,120",134,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=442,mistakes and we can't do
cs-410_7_5_135,cs-410,7,5,Text,"00:07:25,120","00:07:28,885",135,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=445,that all the time for
cs-410_7_5_136,cs-410,7,5,Text,"00:07:28,885","00:07:33,820",136,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=448,"Finally, speech acts would"
cs-410_7_5_137,cs-410,7,5,Text,"00:07:33,820","00:07:38,605",137,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=453,of repetition of the intent
cs-410_7_5_138,cs-410,7,5,Text,"00:07:38,605","00:07:40,000",138,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=458,"So in this case,"
cs-410_7_5_139,cs-410,7,5,Text,"00:07:40,000","00:07:41,485",139,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=460,it might be a request.
cs-410_7_5_140,cs-410,7,5,Text,"00:07:41,485","00:07:44,650",140,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=461,So knowing that would
cs-410_7_5_141,cs-410,7,5,Text,"00:07:44,650","00:07:47,140",141,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=464,even more interesting
cs-410_7_5_142,cs-410,7,5,Text,"00:07:47,140","00:07:51,765",142,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=467,this observer or the author
cs-410_7_5_143,cs-410,7,5,Text,"00:07:51,765","00:07:53,895",143,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=471,What's the intention
cs-410_7_5_144,cs-410,7,5,Text,"00:07:53,895","00:07:57,210",144,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=473,What's scenarios? What kind
cs-410_7_5_145,cs-410,7,5,Text,"00:07:57,210","00:08:02,740",145,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=477,So this is another level
cs-410_7_5_146,cs-410,7,5,Text,"00:08:02,740","00:08:05,755",146,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=482,of analysis that would
cs-410_7_5_147,cs-410,7,5,Text,"00:08:05,755","00:08:10,250",147,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=485,So this picture shows
cs-410_7_5_148,cs-410,7,5,Text,"00:08:10,250","00:08:12,530",148,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=490,we generally see
cs-410_7_5_149,cs-410,7,5,Text,"00:08:12,530","00:08:15,535",149,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=492,natural language processing
cs-410_7_5_150,cs-410,7,5,Text,"00:08:15,535","00:08:18,080",150,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=495,"Unfortunately,"
cs-410_7_5_151,cs-410,7,5,Text,"00:08:18,080","00:08:20,330",151,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=498,"require more human effort,"
cs-410_7_5_152,cs-410,7,5,Text,"00:08:20,330","00:08:23,060",152,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=500,and they are less accurate.
cs-410_7_5_153,cs-410,7,5,Text,"00:08:23,060","00:08:26,570",153,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=503,That means there are mistakes.
cs-410_7_5_154,cs-410,7,5,Text,"00:08:26,570","00:08:29,945",154,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=506,So if we add an texts that are at
cs-410_7_5_155,cs-410,7,5,Text,"00:08:29,945","00:08:32,240",155,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=509,the levels that are
cs-410_7_5_156,cs-410,7,5,Text,"00:08:32,240","00:08:34,970",156,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=512,representing deeper
cs-410_7_5_157,cs-410,7,5,Text,"00:08:34,970","00:08:37,790",157,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=514,then we have to
cs-410_7_5_158,cs-410,7,5,Text,"00:08:37,790","00:08:42,380",158,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=517,So that also means it's
cs-410_7_5_159,cs-410,7,5,Text,"00:08:42,380","00:08:46,835",159,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=522,such deep analysis with
cs-410_7_5_160,cs-410,7,5,Text,"00:08:46,835","00:08:48,695",160,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=526,"for example, sequence of words."
cs-410_7_5_161,cs-410,7,5,Text,"00:08:48,695","00:08:50,675",161,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=528,"On the right side,"
cs-410_7_5_162,cs-410,7,5,Text,"00:08:50,675","00:08:55,240",162,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=530,you'll see the arrow points
cs-410_7_5_163,cs-410,7,5,Text,"00:08:55,240","00:08:56,960",163,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=535,"As we go down,"
cs-410_7_5_164,cs-410,7,5,Text,"00:08:56,960","00:08:59,780",164,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=536,we are representation
cs-410_7_5_165,cs-410,7,5,Text,"00:08:59,780","00:09:02,600",165,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=539,to knowledge representation
cs-410_7_5_166,cs-410,7,5,Text,"00:09:02,600","00:09:08,210",166,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=542,and need for solving
cs-410_7_5_167,cs-410,7,5,Text,"00:09:08,210","00:09:11,750",167,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=548,Now this is desirable because as
cs-410_7_5_168,cs-410,7,5,Text,"00:09:11,750","00:09:15,110",168,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=551,we can represent text at
cs-410_7_5_169,cs-410,7,5,Text,"00:09:15,110","00:09:17,315",169,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=555,we can easily extract
cs-410_7_5_170,cs-410,7,5,Text,"00:09:17,315","00:09:19,280",170,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=557,That's the purpose
cs-410_7_5_171,cs-410,7,5,Text,"00:09:19,280","00:09:21,965",171,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=559,So there is a trade-off
cs-410_7_5_172,cs-410,7,5,Text,"00:09:21,965","00:09:24,920",172,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=561,here between doing
cs-410_7_5_173,cs-410,7,5,Text,"00:09:24,920","00:09:27,260",173,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=564,might have errors but would give
cs-410_7_5_174,cs-410,7,5,Text,"00:09:27,260","00:09:31,225",174,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=567,us direct knowledge that
cs-410_7_5_175,cs-410,7,5,Text,"00:09:31,225","00:09:33,910",175,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=571,"Doing shallow analysis, which"
cs-410_7_5_176,cs-410,7,5,Text,"00:09:33,910","00:09:37,010",176,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=573,is more robust but
cs-410_7_5_177,cs-410,7,5,Text,"00:09:37,010","00:09:42,665",177,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=577,give us the necessary deeper
cs-410_7_5_178,cs-410,7,5,Text,"00:09:42,665","00:09:45,740",178,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=582,I should also say that
cs-410_7_5_179,cs-410,7,5,Text,"00:09:45,740","00:09:49,085",179,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=585,humans and are meant to
cs-410_7_5_180,cs-410,7,5,Text,"00:09:49,085","00:09:52,340",180,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=589,"So as a result, in"
cs-410_7_5_181,cs-410,7,5,Text,"00:09:52,340","00:09:56,090",181,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=592,text-mining humans play
cs-410_7_5_182,cs-410,7,5,Text,"00:09:56,090","00:09:58,010",182,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=596,they are always in the loop.
cs-410_7_5_183,cs-410,7,5,Text,"00:09:58,010","00:10:00,650",183,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=598,Meaning that we should optimize
cs-410_7_5_184,cs-410,7,5,Text,"00:10:00,650","00:10:03,695",184,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=600,the collaboration of
cs-410_7_5_185,cs-410,7,5,Text,"00:10:03,695","00:10:05,540",185,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=603,"So in that sense,"
cs-410_7_5_186,cs-410,7,5,Text,"00:10:05,540","00:10:08,480",186,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=605,it's okay that computers
cs-410_7_5_187,cs-410,7,5,Text,"00:10:08,480","00:10:12,920",187,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=608,to have compute accurately
cs-410_7_5_188,cs-410,7,5,Text,"00:10:12,920","00:10:15,290",188,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=612,and the patterns
cs-410_7_5_189,cs-410,7,5,Text,"00:10:15,290","00:10:18,035",189,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=615,from text data can be
cs-410_7_5_190,cs-410,7,5,Text,"00:10:18,035","00:10:20,840",190,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=618,and humans can
cs-410_7_5_191,cs-410,7,5,Text,"00:10:20,840","00:10:24,650",191,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=620,to do more accurate analysis
cs-410_7_5_192,cs-410,7,5,Text,"00:10:24,650","00:10:28,640",192,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=624,by providing features
cs-410_7_5_193,cs-410,7,5,Text,"00:10:28,640","00:10:33,870",193,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=628,learning programs to make
cs-410_7_6_1,cs-410,7,6,Text,"00:00:00,532","00:00:08,683",1,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=0,[SOUND].
cs-410_7_6_2,cs-410,7,6,Text,"00:00:08,683","00:00:11,442",2,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=8,"So, as we explained the different text"
cs-410_7_6_3,cs-410,7,6,Text,"00:00:11,442","00:00:15,299",3,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=11,representation tends to
cs-410_7_6_4,cs-410,7,6,Text,"00:00:16,560","00:00:19,780",4,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=16,"In particular,"
cs-410_7_6_5,cs-410,7,6,Text,"00:00:19,780","00:00:24,720",5,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=19,more deeper analysis results
cs-410_7_6_6,cs-410,7,6,Text,"00:00:24,720","00:00:27,810",6,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=24,And that would open up a more
cs-410_7_6_7,cs-410,7,6,Text,"00:00:29,520","00:00:33,780",7,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=29,opportunities and
cs-410_7_6_8,cs-410,7,6,Text,"00:00:33,780","00:00:37,470",8,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=33,"So, this table summarizes"
cs-410_7_6_9,cs-410,7,6,Text,"00:00:37,470","00:00:39,800",9,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=37,So the first column shows
cs-410_7_6_10,cs-410,7,6,Text,"00:00:39,800","00:00:44,820",10,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=39,The second visualizes the generality
cs-410_7_6_11,cs-410,7,6,Text,"00:00:44,820","00:00:48,430",11,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=44,Meaning whether we can do this
cs-410_7_6_12,cs-410,7,6,Text,"00:00:48,430","00:00:51,880",12,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=48,all the text data or only some of them.
cs-410_7_6_13,cs-410,7,6,Text,"00:00:51,880","00:00:54,970",13,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=51,And the third column shows
cs-410_7_6_14,cs-410,7,6,Text,"00:00:56,040","00:01:00,130",14,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=56,And the final column shows some
cs-410_7_6_15,cs-410,7,6,Text,"00:01:00,130","00:01:04,670",15,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=60,can be achieved through this
cs-410_7_6_16,cs-410,7,6,Text,"00:01:04,670","00:01:06,310",16,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=64,So let's take a look at them.
cs-410_7_6_17,cs-410,7,6,Text,"00:01:06,310","00:01:12,180",17,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=66,So as a stream text can only be processed
cs-410_7_6_18,cs-410,7,6,Text,"00:01:12,180","00:01:14,050",18,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=72,"It's very robust, it's general."
cs-410_7_6_19,cs-410,7,6,Text,"00:01:15,100","00:01:17,690",19,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=75,And there was still some interesting
cs-410_7_6_20,cs-410,7,6,Text,"00:01:17,690","00:01:18,290",20,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=77,at this level.
cs-410_7_6_21,cs-410,7,6,Text,"00:01:18,290","00:01:20,380",21,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=78,"For example, compression of text."
cs-410_7_6_22,cs-410,7,6,Text,"00:01:20,380","00:01:24,080",22,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=80,Doesn't necessarily need to
cs-410_7_6_23,cs-410,7,6,Text,"00:01:24,080","00:01:27,270",23,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=84,Although knowing word boundaries
cs-410_7_6_24,cs-410,7,6,Text,"00:01:28,540","00:01:32,470",24,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=88,Word base repetition is a very
cs-410_7_6_25,cs-410,7,6,Text,"00:01:32,470","00:01:34,630",25,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=92,It's quite general and
cs-410_7_6_26,cs-410,7,6,Text,"00:01:34,630","00:01:39,140",26,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=94,"relatively robust, indicating they"
cs-410_7_6_27,cs-410,7,6,Text,"00:01:39,140","00:01:44,480",27,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=99,"Such as word relation analysis,"
cs-410_7_6_28,cs-410,7,6,Text,"00:01:44,480","00:01:48,930",28,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=104,And there are many applications that can
cs-410_7_6_29,cs-410,7,6,Text,"00:01:48,930","00:01:54,930",29,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=108,"For example, thesaurus discovery has"
cs-410_7_6_30,cs-410,7,6,Text,"00:01:54,930","00:02:00,550",30,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=114,And topic and
cs-410_7_6_31,cs-410,7,6,Text,"00:02:00,550","00:02:03,360",31,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=120,"And there are, for example, people"
cs-410_7_6_32,cs-410,7,6,Text,"00:02:03,360","00:02:08,190",32,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=123,might be interesting in knowing the major
cs-410_7_6_33,cs-410,7,6,Text,"00:02:08,190","00:02:12,730",33,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=128,And this can be the case
cs-410_7_6_34,cs-410,7,6,Text,"00:02:12,730","00:02:18,500",34,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=132,And scientists want to know what are the
cs-410_7_6_35,cs-410,7,6,Text,"00:02:18,500","00:02:22,950",35,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=138,Or customer service people might want to
cs-410_7_6_36,cs-410,7,6,Text,"00:02:22,950","00:02:28,480",36,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=142,customers by mining their e-mail messages.
cs-410_7_6_37,cs-410,7,6,Text,"00:02:28,480","00:02:33,850",37,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=148,And business intelligence
cs-410_7_6_38,cs-410,7,6,Text,"00:02:33,850","00:02:38,090",38,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=153,understanding consumers' opinions about
cs-410_7_6_39,cs-410,7,6,Text,"00:02:38,090","00:02:42,060",39,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=158,products to figure out what are the
cs-410_7_6_40,cs-410,7,6,Text,"00:02:43,170","00:02:47,140",40,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=163,"And, in general, there are many"
cs-410_7_6_41,cs-410,7,6,Text,"00:02:47,140","00:02:51,300",41,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=167,applications that can be enabled by
cs-410_7_6_42,cs-410,7,6,Text,"00:02:53,720","00:02:58,550",42,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=173,"Now, moving down, we'll see we can"
cs-410_7_6_43,cs-410,7,6,Text,"00:02:58,550","00:03:01,640",43,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=178,"By adding syntactical structures,"
cs-410_7_6_44,cs-410,7,6,Text,"00:03:01,640","00:03:03,890",44,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=181,syntactical graph analysis.
cs-410_7_6_45,cs-410,7,6,Text,"00:03:03,890","00:03:09,490",45,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=183,We can use graph mining algorithms
cs-410_7_6_46,cs-410,7,6,Text,"00:03:09,490","00:03:13,550",46,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=189,And some applications are related
cs-410_7_6_47,cs-410,7,6,Text,"00:03:13,550","00:03:14,090",47,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=193,"For example,"
cs-410_7_6_48,cs-410,7,6,Text,"00:03:14,090","00:03:18,440",48,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=194,stylistic analysis generally requires
cs-410_7_6_49,cs-410,7,6,Text,"00:03:22,000","00:03:26,240",49,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=202,We can also generate
cs-410_7_6_50,cs-410,7,6,Text,"00:03:26,240","00:03:32,090",50,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=206,And those are features that might help us
cs-410_7_6_51,cs-410,7,6,Text,"00:03:32,090","00:03:37,320",51,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=212,categories by looking at the structures
cs-410_7_6_52,cs-410,7,6,Text,"00:03:37,320","00:03:39,350",52,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=217,It can be more accurate.
cs-410_7_6_53,cs-410,7,6,Text,"00:03:39,350","00:03:43,360",53,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=219,"For example,"
cs-410_7_6_54,cs-410,7,6,Text,"00:03:45,120","00:03:49,298",54,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=225,different categories corresponding
cs-410_7_6_55,cs-410,7,6,Text,"00:03:49,298","00:03:56,320",55,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=229,You want to figure out which of
cs-410_7_6_56,cs-410,7,6,Text,"00:03:56,320","00:04:01,440",56,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=236,"this article, then you generally need"
cs-410_7_6_57,cs-410,7,6,Text,"00:04:03,340","00:04:05,400",57,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=243,"When we add entities and relations,"
cs-410_7_6_58,cs-410,7,6,Text,"00:04:05,400","00:04:09,690",58,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=245,then we can enable other techniques
cs-410_7_6_59,cs-410,7,6,Text,"00:04:09,690","00:04:13,920",59,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=249,"answers, or information network and"
cs-410_7_6_60,cs-410,7,6,Text,"00:04:13,920","00:04:20,956",60,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=253,And this analysis enable
cs-410_7_6_61,cs-410,7,6,Text,"00:04:22,285","00:04:22,875",61,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=262,"For example,"
cs-410_7_6_62,cs-410,7,6,Text,"00:04:22,875","00:04:27,525",62,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=262,discovery of all the knowledge and
cs-410_7_6_63,cs-410,7,6,Text,"00:04:28,865","00:04:31,825",63,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=268,You can also use this level representation
cs-410_7_6_64,cs-410,7,6,Text,"00:04:31,825","00:04:35,820",64,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=271,to integrate everything about
cs-410_7_6_65,cs-410,7,6,Text,"00:04:37,520","00:04:40,280",65,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=277,"Finally, when we add logical predicates,"
cs-410_7_6_66,cs-410,7,6,Text,"00:04:40,280","00:04:44,330",66,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=280,"that would enable large inference,"
cs-410_7_6_67,cs-410,7,6,Text,"00:04:44,330","00:04:46,190",67,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=284,And this can be very useful for
cs-410_7_6_68,cs-410,7,6,Text,"00:04:46,190","00:04:48,780",68,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=286,integrating analysis of
cs-410_7_6_69,cs-410,7,6,Text,"00:04:50,190","00:04:53,560",69,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=290,"For example,"
cs-410_7_6_70,cs-410,7,6,Text,"00:04:54,920","00:04:58,370",70,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=294,"extracted the information from text,"
cs-410_7_6_71,cs-410,7,6,Text,"00:04:59,830","00:05:04,470",71,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=299,A good of example of application in this
cs-410_7_6_72,cs-410,7,6,Text,"00:05:04,470","00:05:07,375",72,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=304,is a knowledge assistant for biologists.
cs-410_7_6_73,cs-410,7,6,Text,"00:05:07,375","00:05:14,535",73,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=307,And this program that can help a biologist
cs-410_7_6_74,cs-410,7,6,Text,"00:05:14,535","00:05:21,040",74,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=314,literature about a research problem such
cs-410_7_6_75,cs-410,7,6,Text,"00:05:22,070","00:05:27,143",75,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=322,And the computer can make inferences
cs-410_7_6_76,cs-410,7,6,Text,"00:05:27,143","00:05:32,490",76,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=327,about some of the hypothesis that
cs-410_7_6_77,cs-410,7,6,Text,"00:05:32,490","00:05:36,110",77,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=332,"For example,"
cs-410_7_6_78,cs-410,7,6,Text,"00:05:36,110","00:05:42,135",78,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=336,then the intelligent program can read the
cs-410_7_6_79,cs-410,7,6,Text,"00:05:42,135","00:05:45,250",79,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=342,doing compiling and
cs-410_7_6_80,cs-410,7,6,Text,"00:05:45,250","00:05:50,891",80,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=345,And then using a logic system to
cs-410_7_6_81,cs-410,7,6,Text,"00:05:50,891","00:05:56,060",81,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=350,to researchers questioning about what
cs-410_7_6_82,cs-410,7,6,Text,"00:05:57,990","00:06:01,240",82,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=357,So in order to support
cs-410_7_6_83,cs-410,7,6,Text,"00:06:01,240","00:06:04,910",83,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=361,we need to go as far as
cs-410_7_6_84,cs-410,7,6,Text,"00:06:04,910","00:06:10,585",84,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=364,"Now, this course is covering techniques"
cs-410_7_6_85,cs-410,7,6,Text,"00:06:12,090","00:06:14,610",85,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=372,And these techniques are general and
cs-410_7_6_86,cs-410,7,6,Text,"00:06:14,610","00:06:19,460",86,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=374,robust and that's more widely
cs-410_7_6_87,cs-410,7,6,Text,"00:06:21,120","00:06:26,565",87,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=381,"In fact, in virtually all the text mining"
cs-410_7_6_88,cs-410,7,6,Text,"00:06:26,565","00:06:32,368",88,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=386,representation and then techniques that
cs-410_7_6_89,cs-410,7,6,Text,"00:06:35,909","00:06:39,652",89,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=395,But obviously all these other
cs-410_7_6_90,cs-410,7,6,Text,"00:06:39,652","00:06:45,440",90,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=399,should be combined in order to support
cs-410_7_6_91,cs-410,7,6,Text,"00:06:45,440","00:06:48,790",91,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=405,"So to summarize,"
cs-410_7_6_92,cs-410,7,6,Text,"00:06:48,790","00:06:53,615",92,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=408,Text representation determines what
cs-410_7_6_93,cs-410,7,6,Text,"00:06:53,615","00:06:57,908",93,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=413,And there are multiple ways to
cs-410_7_6_94,cs-410,7,6,Text,"00:06:57,908","00:07:03,099",94,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=417,"syntactic structures, entity-relation"
cs-410_7_6_95,cs-410,7,6,Text,"00:07:03,099","00:07:08,326",95,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=423,And these different
cs-410_7_6_96,cs-410,7,6,Text,"00:07:08,326","00:07:13,540",96,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=428,be combined in real applications
cs-410_7_6_97,cs-410,7,6,Text,"00:07:13,540","00:07:20,263",97,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=433,"For example, even if we cannot"
cs-410_7_6_98,cs-410,7,6,Text,"00:07:20,263","00:07:25,380",98,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=440,"of syntactic structures, we can state"
cs-410_7_6_99,cs-410,7,6,Text,"00:07:25,380","00:07:29,610",99,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=445,"And if we can recognize some entities,"
cs-410_7_6_100,cs-410,7,6,Text,"00:07:29,610","00:07:32,660",100,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=449,So in general we want to
cs-410_7_6_101,cs-410,7,6,Text,"00:07:34,570","00:07:37,210",101,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=454,And when different levels
cs-410_7_6_102,cs-410,7,6,Text,"00:07:37,210","00:07:41,520",102,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=457,"we can enable a richer analysis,"
cs-410_7_6_103,cs-410,7,6,Text,"00:07:42,830","00:07:46,610",103,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=462,This course however focuses
cs-410_7_6_104,cs-410,7,6,Text,"00:07:46,610","00:07:52,170",104,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=466,Such techniques have also several
cs-410_7_6_105,cs-410,7,6,Text,"00:07:52,170","00:07:55,460",105,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=472,"robust, so they are applicable"
cs-410_7_6_106,cs-410,7,6,Text,"00:07:55,460","00:07:59,780",106,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=475,That's a big advantage over
cs-410_7_6_107,cs-410,7,6,Text,"00:07:59,780","00:08:03,510",107,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=479,more fragile natural language
cs-410_7_6_108,cs-410,7,6,Text,"00:08:03,510","00:08:07,680",108,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=483,"Secondly, it does not require"
cs-410_7_6_109,cs-410,7,6,Text,"00:08:07,680","00:08:11,520",109,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=487,"sometimes, it does not"
cs-410_7_6_110,cs-410,7,6,Text,"00:08:11,520","00:08:14,037",110,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=491,"So that's, again, an important benefit,"
cs-410_7_6_111,cs-410,7,6,Text,"00:08:14,037","00:08:17,962",111,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=494,because that means that you can apply
cs-410_7_6_112,cs-410,7,6,Text,"00:08:20,910","00:08:25,373",112,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=500,"Third, these techniques are actually"
cs-410_7_6_113,cs-410,7,6,Text,"00:08:25,373","00:08:27,690",113,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=505,effective form in implications.
cs-410_7_6_114,cs-410,7,6,Text,"00:08:29,210","00:08:32,180",114,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=509,Although not all of course
cs-410_7_6_115,cs-410,7,6,Text,"00:08:34,340","00:08:38,460",115,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=514,Now they are very effective
cs-410_7_6_116,cs-410,7,6,Text,"00:08:38,460","00:08:44,610",116,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=518,are invented by humans as basically
cs-410_7_6_117,cs-410,7,6,Text,"00:08:45,610","00:08:51,120",117,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=525,So they are actually quite sufficient for
cs-410_7_6_118,cs-410,7,6,Text,"00:08:53,680","00:09:00,310",118,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=533,So that makes this kind of word-based
cs-410_7_6_119,cs-410,7,6,Text,"00:09:00,310","00:09:05,010",119,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=540,"And finally, such a word-based"
cs-410_7_6_120,cs-410,7,6,Text,"00:09:05,010","00:09:11,690",120,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=545,by such a representation can be combined
cs-410_7_6_121,cs-410,7,6,Text,"00:09:14,020","00:09:15,191",121,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=554,So they're not competing with each other.
cs-410_7_6_122,cs-410,7,6,Text,"00:09:15,191","00:09:25,191",122,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=555,[MUSIC]
cs-410_7_7_1,cs-410,7,7,Word,"00:00:00,025","00:00:04,546",1,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=0,[SOUND] This lecture is
cs-410_7_7_2,cs-410,7,7,Word,"00:00:04,546","00:00:10,323",2,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=4,about the word association
cs-410_7_7_3,cs-410,7,7,Word,"00:00:10,323","00:00:15,100",3,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=10,mining and analysis.
cs-410_7_7_4,cs-410,7,7,Word,"00:00:15,100","00:00:19,884",4,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=15,"In this lecture,"
cs-410_7_7_5,cs-410,7,7,Word,"00:00:19,884","00:00:22,902",5,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=19,associations of words from text.
cs-410_7_7_6,cs-410,7,7,Word,"00:00:22,902","00:00:27,900",6,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=22,Now this is an example of knowledge
cs-410_7_7_7,cs-410,7,7,Word,"00:00:27,900","00:00:29,960",7,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=27,we can mine from text data.
cs-410_7_7_8,cs-410,7,7,Word,"00:00:33,942","00:00:35,090",8,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=33,Here's the outline.
cs-410_7_7_9,cs-410,7,7,Word,"00:00:35,090","00:00:39,828",9,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=35,We're going to first talk about
cs-410_7_7_10,cs-410,7,7,Word,"00:00:39,828","00:00:45,100",10,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=39,then explain why discovering such
cs-410_7_7_11,cs-410,7,7,Word,"00:00:45,100","00:00:50,070",11,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=45,we're going to talk about some general
cs-410_7_7_12,cs-410,7,7,Word,"00:00:50,070","00:00:55,209",12,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=50,In general there are two word
cs-410_7_7_13,cs-410,7,7,Word,"00:00:56,680","00:00:58,680",13,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=56,One is called a paradigmatic relation.
cs-410_7_7_14,cs-410,7,7,Word,"00:00:58,680","00:01:03,000",14,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=58,The other is syntagmatic relation.
cs-410_7_7_15,cs-410,7,7,Word,"00:01:03,000","00:01:07,780",15,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=63,A and B have paradigmatic relation
cs-410_7_7_16,cs-410,7,7,Word,"00:01:07,780","00:01:11,700",16,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=67,if they can be substituted for each other.
cs-410_7_7_17,cs-410,7,7,Word,"00:01:11,700","00:01:17,910",17,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=71,That means the two words that
cs-410_7_7_18,cs-410,7,7,Word,"00:01:17,910","00:01:23,130",18,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=77,"would be in the same semantic class,"
cs-410_7_7_19,cs-410,7,7,Word,"00:01:23,130","00:01:26,910",19,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=83,And we can in general
cs-410_7_7_20,cs-410,7,7,Word,"00:01:26,910","00:01:30,310",20,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=86,without affecting
cs-410_7_7_21,cs-410,7,7,Word,"00:01:30,310","00:01:33,810",21,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=90,That means we would still
cs-410_7_7_22,cs-410,7,7,Word,"00:01:33,810","00:01:41,530",22,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=93,"For example, cat and dog, these two"
cs-410_7_7_23,cs-410,7,7,Word,"00:01:41,530","00:01:47,710",23,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=101,because they are in
cs-410_7_7_24,cs-410,7,7,Word,"00:01:47,710","00:01:51,827",24,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=107,"And in general,"
cs-410_7_7_25,cs-410,7,7,Word,"00:01:51,827","00:01:56,880",25,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=111,the sentence would still be a valid
cs-410_7_7_26,cs-410,7,7,Word,"00:01:58,320","00:02:01,990",26,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=118,Similarly Monday and
cs-410_7_7_27,cs-410,7,7,Word,"00:02:04,930","00:02:09,390",27,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=124,The second kind of relation is
cs-410_7_7_28,cs-410,7,7,Word,"00:02:10,610","00:02:17,200",28,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=130,"In this case, the two words that have this"
cs-410_7_7_29,cs-410,7,7,Word,"00:02:17,200","00:02:22,190",29,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=137,So A and B have syntagmatic relation if
cs-410_7_7_30,cs-410,7,7,Word,"00:02:22,190","00:02:29,500",30,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=142,"a sentence, that means these two"
cs-410_7_7_31,cs-410,7,7,Word,"00:02:30,720","00:02:36,830",31,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=150,"So for example, cat and sit are related"
cs-410_7_7_32,cs-410,7,7,Word,"00:02:38,060","00:02:43,870",32,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=158,"Similarly, car and"
cs-410_7_7_33,cs-410,7,7,Word,"00:02:43,870","00:02:47,550",33,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=163,they can be combined with
cs-410_7_7_34,cs-410,7,7,Word,"00:02:47,550","00:02:54,150",34,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=167,"However, in general, we can not"
cs-410_7_7_35,cs-410,7,7,Word,"00:02:54,150","00:02:59,590",35,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=174,car with drive in the sentence
cs-410_7_7_36,cs-410,7,7,Word,"00:02:59,590","00:03:03,950",36,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=179,"meaning that if we do that, the sentence"
cs-410_7_7_37,cs-410,7,7,Word,"00:03:03,950","00:03:10,135",37,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=183,So this is different from
cs-410_7_7_38,cs-410,7,7,Word,"00:03:10,135","00:03:15,875",38,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=190,And these two relations are in fact so
cs-410_7_7_39,cs-410,7,7,Word,"00:03:17,365","00:03:24,180",39,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=197,generalized to capture basic relations
cs-410_7_7_40,cs-410,7,7,Word,"00:03:24,180","00:03:27,880",40,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=204,And definitely they can be
cs-410_7_7_41,cs-410,7,7,Word,"00:03:27,880","00:03:31,630",41,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=207,relations of any items in a language.
cs-410_7_7_42,cs-410,7,7,Word,"00:03:31,630","00:03:36,620",42,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=211,"So, A and B don't have to be words and"
cs-410_7_7_43,cs-410,7,7,Word,"00:03:37,960","00:03:44,710",43,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=217,And they can even be more complex
cs-410_7_7_44,cs-410,7,7,Word,"00:03:44,710","00:03:48,820",44,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=224,If you think about the general
cs-410_7_7_45,cs-410,7,7,Word,"00:03:48,820","00:03:53,066",45,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=228,then we can think about the units
cs-410_7_7_46,cs-410,7,7,Word,"00:03:53,066","00:03:58,980",46,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=233,Then we think of paradigmatic
cs-410_7_7_47,cs-410,7,7,Word,"00:03:58,980","00:04:05,890",47,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=238,are applied to units that tend to occur
cs-410_7_7_48,cs-410,7,7,Word,"00:04:05,890","00:04:11,660",48,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=245,or in a sequence of data
cs-410_7_7_49,cs-410,7,7,Word,"00:04:11,660","00:04:20,980",49,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=251,So they occur in similar locations
cs-410_7_7_50,cs-410,7,7,Word,"00:04:20,980","00:04:25,415",50,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=260,Syntagmatical relation on
cs-410_7_7_51,cs-410,7,7,Word,"00:04:25,415","00:04:30,210",51,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=265,co-occurrent elements that tend
cs-410_7_7_52,cs-410,7,7,Word,"00:04:33,150","00:04:38,470",52,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=273,So these two are complimentary and
cs-410_7_7_53,cs-410,7,7,Word,"00:04:38,470","00:04:42,810",53,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=278,And we're interested in discovering
cs-410_7_7_54,cs-410,7,7,Word,"00:04:42,810","00:04:46,420",54,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=282,Discovering such worded
cs-410_7_7_55,cs-410,7,7,Word,"00:04:47,480","00:04:52,920",55,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=287,"First, such relations can be directly"
cs-410_7_7_56,cs-410,7,7,Word,"00:04:52,920","00:04:58,880",56,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=292,"tasks, and this is because this is part"
cs-410_7_7_57,cs-410,7,7,Word,"00:04:58,880","00:05:02,440",57,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=298,So if you know these two words
cs-410_7_7_58,cs-410,7,7,Word,"00:05:02,440","00:05:04,970",58,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=302,and then you can help a lot of tasks.
cs-410_7_7_59,cs-410,7,7,Word,"00:05:05,980","00:05:10,970",59,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=305,And grammar learning can be also
cs-410_7_7_60,cs-410,7,7,Word,"00:05:10,970","00:05:15,130",60,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=310,Because if we can learn
cs-410_7_7_61,cs-410,7,7,Word,"00:05:15,130","00:05:20,000",61,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=315,"then we form classes of words,"
cs-410_7_7_62,cs-410,7,7,Word,"00:05:20,000","00:05:25,630",62,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=320,"And if we learn syntagmatic relations,"
cs-410_7_7_63,cs-410,7,7,Word,"00:05:25,630","00:05:32,400",63,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=325,the rules for putting together a larger
cs-410_7_7_64,cs-410,7,7,Word,"00:05:32,400","00:05:37,390",64,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=332,So we learn the structure and
cs-410_7_7_65,cs-410,7,7,Word,"00:05:39,855","00:05:43,070",65,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=339,Word relations can be also very useful for
cs-410_7_7_66,cs-410,7,7,Word,"00:05:43,070","00:05:46,580",66,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=343,many applications in text retrieval and
cs-410_7_7_67,cs-410,7,7,Word,"00:05:46,580","00:05:50,520",67,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=346,"For example, in search and"
cs-410_7_7_68,cs-410,7,7,Word,"00:05:50,520","00:05:55,930",68,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=350,"associations to modify a query,"
cs-410_7_7_69,cs-410,7,7,Word,"00:05:55,930","00:06:00,480",69,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=355,introduce additional related words into
cs-410_7_7_70,cs-410,7,7,Word,"00:06:01,590","00:06:03,390",70,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=361,It's often called a query expansion.
cs-410_7_7_71,cs-410,7,7,Word,"00:06:05,290","00:06:10,030",71,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=365,Or you can use related words to
cs-410_7_7_72,cs-410,7,7,Word,"00:06:10,030","00:06:11,660",72,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=370,to explore the information space.
cs-410_7_7_73,cs-410,7,7,Word,"00:06:12,740","00:06:15,610",73,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=372,Another application is to
cs-410_7_7_74,cs-410,7,7,Word,"00:06:15,610","00:06:19,790",74,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=375,automatically construct the top
cs-410_7_7_75,cs-410,7,7,Word,"00:06:19,790","00:06:24,540",75,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=379,We can have words as nodes and
cs-410_7_7_76,cs-410,7,7,Word,"00:06:24,540","00:06:27,930",76,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=384,A user could navigate from
cs-410_7_7_77,cs-410,7,7,Word,"00:06:28,990","00:06:31,180",77,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=388,find information in the information space.
cs-410_7_7_78,cs-410,7,7,Word,"00:06:33,620","00:06:40,620",78,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=393,"Finally, such word associations can also"
cs-410_7_7_79,cs-410,7,7,Word,"00:06:40,620","00:06:45,680",79,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=400,"For example, we might be interested"
cs-410_7_7_80,cs-410,7,7,Word,"00:06:45,680","00:06:48,620",80,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=405,negative opinions about the iPhone 6.
cs-410_7_7_81,cs-410,7,7,Word,"00:06:48,620","00:06:55,180",81,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=408,"In order to do that, we can look at what"
cs-410_7_7_82,cs-410,7,7,Word,"00:06:55,180","00:07:01,630",82,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=415,a feature word like battery in
cs-410_7_7_83,cs-410,7,7,Word,"00:07:01,630","00:07:05,147",83,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=421,Such a syntagmatical
cs-410_7_7_84,cs-410,7,7,Word,"00:07:05,147","00:07:08,854",84,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=425,show the detailed opinions
cs-410_7_7_85,cs-410,7,7,Word,"00:07:16,696","00:07:20,837",85,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=436,"So, how can we discover such"
cs-410_7_7_86,cs-410,7,7,Word,"00:07:20,837","00:07:24,450",86,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=440,"Now, here are some intuitions"
cs-410_7_7_87,cs-410,7,7,Word,"00:07:24,450","00:07:27,479",87,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=444,Now let's first look at
cs-410_7_7_88,cs-410,7,7,Word,"00:07:29,080","00:07:32,940",88,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=449,Here we essentially can take
cs-410_7_7_89,cs-410,7,7,Word,"00:07:34,150","00:07:38,440",89,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=454,So here you see some simple
cs-410_7_7_90,cs-410,7,7,Word,"00:07:38,440","00:07:43,416",90,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=458,You can see they generally
cs-410_7_7_91,cs-410,7,7,Word,"00:07:43,416","00:07:48,390",91,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=463,and that after all is the definition
cs-410_7_7_92,cs-410,7,7,Word,"00:07:49,540","00:07:54,510",92,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=469,On the right side you can kind
cs-410_7_7_93,cs-410,7,7,Word,"00:07:54,510","00:07:59,090",93,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=474,the context of cat and
cs-410_7_7_94,cs-410,7,7,Word,"00:08:00,640","00:08:05,230",94,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=480,I've taken away cat and
cs-410_7_7_95,cs-410,7,7,Word,"00:08:05,230","00:08:07,280",95,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=485,that you can see just the context.
cs-410_7_7_96,cs-410,7,7,Word,"00:08:08,810","00:08:12,660",96,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=488,"Now, of course we can have different"
cs-410_7_7_97,cs-410,7,7,Word,"00:08:13,810","00:08:19,528",97,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=493,"For example, we can look at"
cs-410_7_7_98,cs-410,7,7,Word,"00:08:19,528","00:08:24,222",98,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=499,part of this context.
cs-410_7_7_99,cs-410,7,7,Word,"00:08:24,222","00:08:28,000",99,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=504,So we can call this left context.
cs-410_7_7_100,cs-410,7,7,Word,"00:08:28,000","00:08:34,800",100,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=508,What words occur before we see cat or dog?
cs-410_7_7_101,cs-410,7,7,Word,"00:08:34,800","00:08:39,910",101,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=514,"So, you can see in this case, clearly"
cs-410_7_7_102,cs-410,7,7,Word,"00:08:41,810","00:08:47,860",102,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=521,You generally say his cat or my cat and
cs-410_7_7_103,cs-410,7,7,Word,"00:08:47,860","00:08:52,290",103,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=527,So that makes them similar
cs-410_7_7_104,cs-410,7,7,Word,"00:08:53,660","00:08:58,880",104,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=533,"Similarly, if you look at the words"
cs-410_7_7_105,cs-410,7,7,Word,"00:08:58,880","00:09:03,970",105,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=538,"which we can call right context,"
cs-410_7_7_106,cs-410,7,7,Word,"00:09:03,970","00:09:07,490",106,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=543,"Of course, it's an extreme case,"
cs-410_7_7_107,cs-410,7,7,Word,"00:09:08,670","00:09:12,883",107,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=548,"And in general,"
cs-410_7_7_108,cs-410,7,7,Word,"00:09:12,883","00:09:15,170",108,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=552,that can't follow cat and dog.
cs-410_7_7_109,cs-410,7,7,Word,"00:09:17,830","00:09:21,700",109,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=557,You can also even look
cs-410_7_7_110,cs-410,7,7,Word,"00:09:21,700","00:09:24,690",110,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=561,And that might include all
cs-410_7_7_111,cs-410,7,7,Word,"00:09:24,690","00:09:26,640",111,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=564,in sentences around this word.
cs-410_7_7_112,cs-410,7,7,Word,"00:09:27,658","00:09:34,300",112,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=567,"And even in the general context, you also"
cs-410_7_7_113,cs-410,7,7,Word,"00:09:35,400","00:09:41,480",113,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=575,So this was just a suggestion
cs-410_7_7_114,cs-410,7,7,Word,"00:09:41,480","00:09:47,000",114,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=581,relation by looking at
cs-410_7_7_115,cs-410,7,7,Word,"00:09:47,000","00:09:50,900",115,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=587,"So, for example,"
cs-410_7_7_116,cs-410,7,7,Word,"00:09:50,900","00:09:54,760",116,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=590,How similar are context of cat and
cs-410_7_7_117,cs-410,7,7,Word,"00:09:56,240","00:10:01,630",117,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=596,In contrast how similar are context
cs-410_7_7_118,cs-410,7,7,Word,"00:10:02,660","00:10:07,610",118,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=602,"Now, intuitively,"
cs-410_7_7_119,cs-410,7,7,Word,"00:10:07,610","00:10:11,030",119,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=607,the context of dog would
cs-410_7_7_120,cs-410,7,7,Word,"00:10:11,030","00:10:16,550",120,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=611,the context of cat and
cs-410_7_7_121,cs-410,7,7,Word,"00:10:16,550","00:10:20,680",121,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=616,"That means, in the first case"
cs-410_7_7_122,cs-410,7,7,Word,"00:10:21,910","00:10:25,940",122,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=621,between the context of cat and
cs-410_7_7_123,cs-410,7,7,Word,"00:10:25,940","00:10:30,248",123,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=625,the similarity between context of cat and
cs-410_7_7_124,cs-410,7,7,Word,"00:10:30,248","00:10:35,750",124,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=630,because they all not having a paradigmatic
cs-410_7_7_125,cs-410,7,7,Word,"00:10:35,750","00:10:40,550",125,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=635,relationship and imagine what words
cs-410_7_7_126,cs-410,7,7,Word,"00:10:40,550","00:10:44,900",126,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=640,It would be very different from
cs-410_7_7_127,cs-410,7,7,Word,"00:10:46,620","00:10:50,340",127,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=646,So this is the basic idea of what
cs-410_7_7_128,cs-410,7,7,Word,"00:10:52,040","00:10:54,180",128,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=652,What about the syntagmatic relation?
cs-410_7_7_129,cs-410,7,7,Word,"00:10:54,180","00:10:58,550",129,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=654,"Well, here we're going to explore"
cs-410_7_7_130,cs-410,7,7,Word,"00:10:58,550","00:11:02,430",130,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=658,again based on the definition
cs-410_7_7_131,cs-410,7,7,Word,"00:11:03,990","00:11:05,600",131,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=663,Here you see the same sample of text.
cs-410_7_7_132,cs-410,7,7,Word,"00:11:06,640","00:11:10,710",132,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=666,But here we're interested in knowing
cs-410_7_7_133,cs-410,7,7,Word,"00:11:10,710","00:11:14,780",133,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=670,with the verb eats and
cs-410_7_7_134,cs-410,7,7,Word,"00:11:16,380","00:11:20,880",134,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=676,And if you look at the right
cs-410_7_7_135,cs-410,7,7,Word,"00:11:20,880","00:11:25,245",135,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=680,"you see,"
cs-410_7_7_136,cs-410,7,7,Word,"00:11:27,110","00:11:30,140",136,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=687,I've taken away the word to its left and
cs-410_7_7_137,cs-410,7,7,Word,"00:11:30,140","00:11:33,970",137,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=690,also the word to its
cs-410_7_7_138,cs-410,7,7,Word,"00:11:35,340","00:11:41,900",138,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=695,"And then we ask the question, what words"
cs-410_7_7_139,cs-410,7,7,Word,"00:11:43,650","00:11:47,960",139,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=703,And what words tend to
cs-410_7_7_140,cs-410,7,7,Word,"00:11:49,560","00:11:54,997",140,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=709,Now thinking about this question
cs-410_7_7_141,cs-410,7,7,Word,"00:11:54,997","00:12:00,830",141,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=714,relations because syntagmatic relations
cs-410_7_7_142,cs-410,7,7,Word,"00:12:03,070","00:12:07,290",142,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=723,So the important question to ask for
cs-410_7_7_143,cs-410,7,7,Word,"00:12:07,290","00:12:14,570",143,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=727,"whenever eats occurs,"
cs-410_7_7_144,cs-410,7,7,Word,"00:12:16,180","00:12:19,120",144,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=736,So the question here has
cs-410_7_7_145,cs-410,7,7,Word,"00:12:19,120","00:12:23,940",145,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=739,are some other words that tend
cs-410_7_7_146,cs-410,7,7,Word,"00:12:23,940","00:12:28,240",146,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=743,Meaning that whenever you see eats
cs-410_7_7_147,cs-410,7,7,Word,"00:12:29,620","00:12:34,660",147,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=749,"And if you don't see eats, probably,"
cs-410_7_7_148,cs-410,7,7,Word,"00:12:36,560","00:12:40,210",148,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=756,So this intuition can help
cs-410_7_7_149,cs-410,7,7,Word,"00:12:41,530","00:12:43,200",149,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=761,"Now again, consider example."
cs-410_7_7_150,cs-410,7,7,Word,"00:12:44,210","00:12:48,170",150,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=764,How helpful is occurrence of eats for
cs-410_7_7_151,cs-410,7,7,Word,"00:12:49,870","00:12:53,056",151,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=769,Right.
cs-410_7_7_152,cs-410,7,7,Word,"00:12:53,056","00:12:58,930",152,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=773,in a sentence would generally help us
cs-410_7_7_153,cs-410,7,7,Word,"00:12:58,930","00:13:01,801",153,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=778,"And if we see eats occur in the sentence,"
cs-410_7_7_154,cs-410,7,7,Word,"00:13:01,801","00:13:05,770",154,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=781,that should increase the chance
cs-410_7_7_155,cs-410,7,7,Word,"00:13:08,490","00:13:12,150",155,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=788,"In contrast,"
cs-410_7_7_156,cs-410,7,7,Word,"00:13:12,150","00:13:15,710",156,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=792,how helpful is the occurrence of eats for
cs-410_7_7_157,cs-410,7,7,Word,"00:13:17,330","00:13:20,270",157,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=797,Because eats and
cs-410_7_7_158,cs-410,7,7,Word,"00:13:20,270","00:13:24,840",158,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=800,knowing whether eats occurred
cs-410_7_7_159,cs-410,7,7,Word,"00:13:24,840","00:13:30,140",159,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=804,"really help us predict the weather,"
cs-410_7_7_160,cs-410,7,7,Word,"00:13:30,140","00:13:34,100",160,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=810,So this is in contrast to
cs-410_7_7_161,cs-410,7,7,Word,"00:13:35,550","00:13:38,790",161,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=815,This also helps explain that intuition
cs-410_7_7_162,cs-410,7,7,Word,"00:13:38,790","00:13:43,100",162,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=818,behind the methods of what
cs-410_7_7_163,cs-410,7,7,Word,"00:13:43,100","00:13:49,090",163,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=823,Mainly we need to capture the correlation
cs-410_7_7_164,cs-410,7,7,Word,"00:13:50,440","00:13:52,860",164,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=830,So to summarize the general ideas for
cs-410_7_7_165,cs-410,7,7,Word,"00:13:52,860","00:13:55,810",165,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=832,discovering word associations
cs-410_7_7_166,cs-410,7,7,Word,"00:13:56,880","00:14:02,240",166,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=836,"For paradigmatic relation,"
cs-410_7_7_167,cs-410,7,7,Word,"00:14:02,240","00:14:04,830",167,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=842,And then compute its context similarity.
cs-410_7_7_168,cs-410,7,7,Word,"00:14:04,830","00:14:09,030",168,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=844,We're going to assume the words
cs-410_7_7_169,cs-410,7,7,Word,"00:14:09,030","00:14:12,260",169,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=849,to have paradigmatic relation.
cs-410_7_7_170,cs-410,7,7,Word,"00:14:14,640","00:14:19,970",170,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=854,"For syntagmatic relation, we will count"
cs-410_7_7_171,cs-410,7,7,Word,"00:14:19,970","00:14:25,180",171,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=859,"in a context, which can be a sentence,"
cs-410_7_7_172,cs-410,7,7,Word,"00:14:25,180","00:14:28,180",172,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=865,And we're going to compare
cs-410_7_7_173,cs-410,7,7,Word,"00:14:28,180","00:14:31,660",173,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=868,their co-occurrences with
cs-410_7_7_174,cs-410,7,7,Word,"00:14:33,280","00:14:36,660",174,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=873,We're going to assume words
cs-410_7_7_175,cs-410,7,7,Word,"00:14:36,660","00:14:42,335",175,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=876,relatively low individual occurrences
cs-410_7_7_176,cs-410,7,7,Word,"00:14:42,335","00:14:46,581",176,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=882,because they attempt to occur together and
cs-410_7_7_177,cs-410,7,7,Word,"00:14:46,581","00:14:51,635",177,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=886,Note that the paradigmatic relation and
cs-410_7_7_178,cs-410,7,7,Word,"00:14:51,635","00:14:57,065",178,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=891,are actually closely related
cs-410_7_7_179,cs-410,7,7,Word,"00:14:57,065","00:15:02,810",179,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=897,related words tend to have syntagmatic
cs-410_7_7_180,cs-410,7,7,Word,"00:15:02,810","00:15:05,420",180,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=902,They tend to be associated
cs-410_7_7_181,cs-410,7,7,Word,"00:15:05,420","00:15:10,870",181,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=905,that suggests that we can also do join
cs-410_7_7_182,cs-410,7,7,Word,"00:15:10,870","00:15:15,190",182,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=910,So these general ideas can be
cs-410_7_7_183,cs-410,7,7,Word,"00:15:15,190","00:15:19,129",183,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=915,"And the course won't cover all of them,"
cs-410_7_7_184,cs-410,7,7,Word,"00:15:19,129","00:15:24,774",184,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=919,we will cover at least some of
cs-410_7_7_185,cs-410,7,7,Word,"00:15:24,774","00:15:27,669",185,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=924,discovering these relations.
cs-410_7_7_186,cs-410,7,7,Word,"00:15:27,669","00:15:37,669",186,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=927,[MUSIC]
cs-410_7_8_1,cs-410,7,8,Paradigmatic,"00:00:00,025","00:00:07,935",1,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=0,[SOUND]
cs-410_7_8_2,cs-410,7,8,Paradigmatic,"00:00:07,935","00:00:14,253",2,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=7,lecture is about
cs-410_7_8_3,cs-410,7,8,Paradigmatic,"00:00:14,253","00:00:19,131",3,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=14,In this lecture we are going to talk about
cs-410_7_8_4,cs-410,7,8,Paradigmatic,"00:00:19,131","00:00:22,160",4,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=19,association called
cs-410_7_8_5,cs-410,7,8,Paradigmatic,"00:00:25,400","00:00:30,307",5,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=25,"By definition,"
cs-410_7_8_6,cs-410,7,8,Paradigmatic,"00:00:30,307","00:00:34,503",6,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=30,related if they share a similar context.
cs-410_7_8_7,cs-410,7,8,Paradigmatic,"00:00:34,503","00:00:39,086",7,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=34,"Namely, they occur in"
cs-410_7_8_8,cs-410,7,8,Paradigmatic,"00:00:39,086","00:00:44,280",8,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=39,So naturally our idea of discovering such
cs-410_7_8_9,cs-410,7,8,Paradigmatic,"00:00:44,280","00:00:49,080",9,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=44,of each word and then try to compute
cs-410_7_8_10,cs-410,7,8,Paradigmatic,"00:00:50,160","00:00:54,360",10,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=50,So here is an example of
cs-410_7_8_11,cs-410,7,8,Paradigmatic,"00:00:55,800","00:01:01,690",11,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=55,Here I have taken the word
cs-410_7_8_12,cs-410,7,8,Paradigmatic,"00:01:01,690","00:01:08,080",12,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=61,you can see we are seeing some remaining
cs-410_7_8_13,cs-410,7,8,Paradigmatic,"00:01:09,610","00:01:12,479",13,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=69,"Now, we can do the same thing for"
cs-410_7_8_14,cs-410,7,8,Paradigmatic,"00:01:13,660","00:01:18,370",14,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=73,So in general we would like to capture
cs-410_7_8_15,cs-410,7,8,Paradigmatic,"00:01:18,370","00:01:23,340",15,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=78,the similarity of the context of cat and
cs-410_7_8_16,cs-410,7,8,Paradigmatic,"00:01:24,790","00:01:29,970",16,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=84,So now the question is how can we
cs-410_7_8_17,cs-410,7,8,Paradigmatic,"00:01:29,970","00:01:31,458",17,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=89,then define the similarity function.
cs-410_7_8_18,cs-410,7,8,Paradigmatic,"00:01:33,340","00:01:38,560",18,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=93,"So first, we note that the context"
cs-410_7_8_19,cs-410,7,8,Paradigmatic,"00:01:38,560","00:01:43,637",19,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=98,"So, they can be regarded as"
cs-410_7_8_20,cs-410,7,8,Paradigmatic,"00:01:43,637","00:01:49,370",20,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=103,"document, but there are also different"
cs-410_7_8_21,cs-410,7,8,Paradigmatic,"00:01:49,370","00:01:57,470",21,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=109,"For example, we can look at the word"
cs-410_7_8_22,cs-410,7,8,Paradigmatic,"00:01:57,470","00:02:00,440",22,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=117,We can call this context Left1 context.
cs-410_7_8_23,cs-410,7,8,Paradigmatic,"00:02:00,440","00:02:04,980",23,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=120,"All right, so in this case you"
cs-410_7_8_24,cs-410,7,8,Paradigmatic,"00:02:04,980","00:02:07,430",24,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=124,"big, a, the, et cetera."
cs-410_7_8_25,cs-410,7,8,Paradigmatic,"00:02:07,430","00:02:12,690",25,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=127,These are the words that can
cs-410_7_8_26,cs-410,7,8,Paradigmatic,"00:02:12,690","00:02:19,280",26,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=132,"So we say my cat, his cat,"
cs-410_7_8_27,cs-410,7,8,Paradigmatic,"00:02:19,280","00:02:24,180",27,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=139,"Similarly, we can also collect the words"
cs-410_7_8_28,cs-410,7,8,Paradigmatic,"00:02:24,180","00:02:28,156",28,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=144,"We can call this context Right1, and"
cs-410_7_8_29,cs-410,7,8,Paradigmatic,"00:02:28,156","00:02:34,128",29,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=148,"here we see words like eats,"
cs-410_7_8_30,cs-410,7,8,Paradigmatic,"00:02:34,128","00:02:35,907",30,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=154,"Or, more generally,"
cs-410_7_8_31,cs-410,7,8,Paradigmatic,"00:02:35,907","00:02:41,253",31,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=155,we can look at all the words in
cs-410_7_8_32,cs-410,7,8,Paradigmatic,"00:02:41,253","00:02:46,960",32,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=161,"Here, let's say we can take a window"
cs-410_7_8_33,cs-410,7,8,Paradigmatic,"00:02:46,960","00:02:48,720",33,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=166,We call this context Window8.
cs-410_7_8_34,cs-410,7,8,Paradigmatic,"00:02:49,850","00:02:54,680",34,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=169,"Now, of course, you can see all"
cs-410_7_8_35,cs-410,7,8,Paradigmatic,"00:02:54,680","00:02:58,829",35,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=174,so we'll have a bag of words in
cs-410_7_8_36,cs-410,7,8,Paradigmatic,"00:03:01,270","00:03:06,410",36,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=181,"Now, such a word based representation"
cs-410_7_8_37,cs-410,7,8,Paradigmatic,"00:03:06,410","00:03:12,230",37,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=186,an interesting way to define the
cs-410_7_8_38,cs-410,7,8,Paradigmatic,"00:03:12,230","00:03:15,911",38,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=192,Because if you look at just
cs-410_7_8_39,cs-410,7,8,Paradigmatic,"00:03:15,911","00:03:21,750",39,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=195,then we'll see words that share
cs-410_7_8_40,cs-410,7,8,Paradigmatic,"00:03:21,750","00:03:27,650",40,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=201,and we kind of ignored the other words
cs-410_7_8_41,cs-410,7,8,Paradigmatic,"00:03:27,650","00:03:32,380",41,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=207,So that gives us one perspective to
cs-410_7_8_42,cs-410,7,8,Paradigmatic,"00:03:32,380","00:03:34,244",42,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=212,"if we only use the Right1 context,"
cs-410_7_8_43,cs-410,7,8,Paradigmatic,"00:03:34,244","00:03:38,420",43,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=214,we will capture this narrative
cs-410_7_8_44,cs-410,7,8,Paradigmatic,"00:03:38,420","00:03:43,040",44,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=218,Using both the Left1 and
cs-410_7_8_45,cs-410,7,8,Paradigmatic,"00:03:43,040","00:03:47,720",45,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=223,the similarity with even
cs-410_7_8_46,cs-410,7,8,Paradigmatic,"00:03:49,910","00:03:54,744",46,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=229,"So in general, context may contain"
cs-410_7_8_47,cs-410,7,8,Paradigmatic,"00:03:54,744","00:03:59,575",47,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=234,"my, that you see here, or"
cs-410_7_8_48,cs-410,7,8,Paradigmatic,"00:03:59,575","00:04:02,961",48,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=239,"Tuesday, or"
cs-410_7_8_49,cs-410,7,8,Paradigmatic,"00:04:05,461","00:04:10,174",49,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=245,And this flexibility also allows us
cs-410_7_8_50,cs-410,7,8,Paradigmatic,"00:04:10,174","00:04:11,660",50,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=250,different ways.
cs-410_7_8_51,cs-410,7,8,Paradigmatic,"00:04:11,660","00:04:13,500",51,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=251,"Sometimes this is useful,"
cs-410_7_8_52,cs-410,7,8,Paradigmatic,"00:04:13,500","00:04:19,130",52,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=253,as we might want to capture
cs-410_7_8_53,cs-410,7,8,Paradigmatic,"00:04:19,130","00:04:25,270",53,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=259,That would give us loosely
cs-410_7_8_54,cs-410,7,8,Paradigmatic,"00:04:25,270","00:04:29,340",54,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=265,Whereas if you use only the words
cs-410_7_8_55,cs-410,7,8,Paradigmatic,"00:04:29,340","00:04:35,520",55,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=269,"to the right of the word, then you"
cs-410_7_8_56,cs-410,7,8,Paradigmatic,"00:04:35,520","00:04:39,950",56,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=275,much related by their syntactical
cs-410_7_8_57,cs-410,7,8,Paradigmatic,"00:04:41,170","00:04:46,304",57,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=281,So the general idea of discovering
cs-410_7_8_58,cs-410,7,8,Paradigmatic,"00:04:46,304","00:04:50,754",58,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=286,is to compute the similarity
cs-410_7_8_59,cs-410,7,8,Paradigmatic,"00:04:50,754","00:04:55,264",59,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=290,"So here, for example,"
cs-410_7_8_60,cs-410,7,8,Paradigmatic,"00:04:55,264","00:04:59,110",60,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=295,dog based on the similarity
cs-410_7_8_61,cs-410,7,8,Paradigmatic,"00:04:59,110","00:05:02,890",61,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=299,"In general, we can combine all"
cs-410_7_8_62,cs-410,7,8,Paradigmatic,"00:05:02,890","00:05:06,395",62,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=302,"And so the similarity function is,"
cs-410_7_8_63,cs-410,7,8,Paradigmatic,"00:05:06,395","00:05:10,336",63,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=306,a combination of similarities
cs-410_7_8_64,cs-410,7,8,Paradigmatic,"00:05:10,336","00:05:14,849",64,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=310,"And of course, we can also assign"
cs-410_7_8_65,cs-410,7,8,Paradigmatic,"00:05:14,849","00:05:20,170",65,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=314,similarities to allow us to focus
cs-410_7_8_66,cs-410,7,8,Paradigmatic,"00:05:20,170","00:05:24,395",66,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=320,And this would be naturally
cs-410_7_8_67,cs-410,7,8,Paradigmatic,"00:05:24,395","00:05:28,935",67,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=324,here the main idea for discovering
cs-410_7_8_68,cs-410,7,8,Paradigmatic,"00:05:28,935","00:05:32,470",68,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=328,to computer the similarity
cs-410_7_8_69,cs-410,7,8,Paradigmatic,"00:05:32,470","00:05:37,670",69,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=332,So next let's see how we exactly
cs-410_7_8_70,cs-410,7,8,Paradigmatic,"00:05:37,670","00:05:42,235",70,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=337,"Now to answer this question,"
cs-410_7_8_71,cs-410,7,8,Paradigmatic,"00:05:42,235","00:05:46,520",71,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=342,representation as vectors
cs-410_7_8_72,cs-410,7,8,Paradigmatic,"00:05:48,340","00:05:53,016",72,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=348,Now those of you who have been
cs-410_7_8_73,cs-410,7,8,Paradigmatic,"00:05:53,016","00:05:57,936",73,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=353,textual retrieval techniques would
cs-410_7_8_74,cs-410,7,8,Paradigmatic,"00:05:57,936","00:06:02,711",74,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=357,been used frequently for
cs-410_7_8_75,cs-410,7,8,Paradigmatic,"00:06:02,711","00:06:08,115",75,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=362,But here we also find it convenient
cs-410_7_8_76,cs-410,7,8,Paradigmatic,"00:06:08,115","00:06:11,130",76,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=368,paradigmatic relation discovery.
cs-410_7_8_77,cs-410,7,8,Paradigmatic,"00:06:11,130","00:06:15,440",77,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=371,So the idea of this
cs-410_7_8_78,cs-410,7,8,Paradigmatic,"00:06:15,440","00:06:20,140",78,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=375,word in our vocabulary as defining one
cs-410_7_8_79,cs-410,7,8,Paradigmatic,"00:06:20,140","00:06:23,615",79,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=380,So we have N words in
cs-410_7_8_80,cs-410,7,8,Paradigmatic,"00:06:23,615","00:06:27,462",80,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=383,"then we have N dimensions,"
cs-410_7_8_81,cs-410,7,8,Paradigmatic,"00:06:27,462","00:06:34,311",81,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=387,"And on the bottom, you can see a frequency"
cs-410_7_8_82,cs-410,7,8,Paradigmatic,"00:06:34,311","00:06:39,855",82,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=394,and here we see where eats
cs-410_7_8_83,cs-410,7,8,Paradigmatic,"00:06:39,855","00:06:43,140",83,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=399,"ate occurred 3 times, et cetera."
cs-410_7_8_84,cs-410,7,8,Paradigmatic,"00:06:43,140","00:06:48,003",84,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=403,So this vector can then be placed
cs-410_7_8_85,cs-410,7,8,Paradigmatic,"00:06:48,003","00:06:53,347",85,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=408,"So in general,"
cs-410_7_8_86,cs-410,7,8,Paradigmatic,"00:06:53,347","00:06:58,933",86,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=413,"context of cat as one vector,"
cs-410_7_8_87,cs-410,7,8,Paradigmatic,"00:06:58,933","00:07:04,045",87,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=418,"dog, might give us a different context,"
cs-410_7_8_88,cs-410,7,8,Paradigmatic,"00:07:04,045","00:07:07,880",88,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=424,And then we can measure
cs-410_7_8_89,cs-410,7,8,Paradigmatic,"00:07:07,880","00:07:10,980",89,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=427,So by viewing context in
cs-410_7_8_90,cs-410,7,8,Paradigmatic,"00:07:10,980","00:07:15,100",90,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=430,we convert the problem of
cs-410_7_8_91,cs-410,7,8,Paradigmatic,"00:07:15,100","00:07:18,820",91,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=435,into the problem of computing
cs-410_7_8_92,cs-410,7,8,Paradigmatic,"00:07:20,300","00:07:24,170",92,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=440,So the two questions that we
cs-410_7_8_93,cs-410,7,8,Paradigmatic,"00:07:24,170","00:07:28,750",93,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=444,"how to compute each vector, and"
cs-410_7_8_94,cs-410,7,8,Paradigmatic,"00:07:31,050","00:07:33,579",94,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=451,And the other question is how
cs-410_7_8_95,cs-410,7,8,Paradigmatic,"00:07:35,580","00:07:40,515",95,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=455,"Now in general, there are many approaches"
cs-410_7_8_96,cs-410,7,8,Paradigmatic,"00:07:40,515","00:07:43,795",96,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=460,most of them are developed for
cs-410_7_8_97,cs-410,7,8,Paradigmatic,"00:07:43,795","00:07:47,821",97,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=463,And they have been shown to work well for
cs-410_7_8_98,cs-410,7,8,Paradigmatic,"00:07:47,821","00:07:52,712",98,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=467,matching a query vector and
cs-410_7_8_99,cs-410,7,8,Paradigmatic,"00:07:52,712","00:07:57,555",99,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=472,But we can adapt many of
cs-410_7_8_100,cs-410,7,8,Paradigmatic,"00:07:57,555","00:08:01,378",100,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=477,of context documents for our purpose here.
cs-410_7_8_101,cs-410,7,8,Paradigmatic,"00:08:01,378","00:08:05,829",101,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=481,So let's first look at
cs-410_7_8_102,cs-410,7,8,Paradigmatic,"00:08:05,829","00:08:10,481",102,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=485,where we try to match
cs-410_7_8_103,cs-410,7,8,Paradigmatic,"00:08:10,481","00:08:15,150",103,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=490,"the expected overlap of words,"
cs-410_7_8_104,cs-410,7,8,Paradigmatic,"00:08:17,020","00:08:22,495",104,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=497,So the idea here is to represent
cs-410_7_8_105,cs-410,7,8,Paradigmatic,"00:08:22,495","00:08:28,438",105,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=502,where each word has a weight
cs-410_7_8_106,cs-410,7,8,Paradigmatic,"00:08:28,438","00:08:35,336",106,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=508,that a randomly picked word from
cs-410_7_8_107,cs-410,7,8,Paradigmatic,"00:08:35,336","00:08:39,956",107,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=515,"So in other words,"
cs-410_7_8_108,cs-410,7,8,Paradigmatic,"00:08:39,956","00:08:43,476",108,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=519,"account of word wi in the context, and"
cs-410_7_8_109,cs-410,7,8,Paradigmatic,"00:08:43,476","00:08:48,756",109,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=523,this can be interpreted as
cs-410_7_8_110,cs-410,7,8,Paradigmatic,"00:08:48,756","00:08:54,600",110,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=528,actually pick this word from d1
cs-410_7_8_111,cs-410,7,8,Paradigmatic,"00:08:56,760","00:09:01,620",111,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=536,"Now, of course these xi's would sum to one"
cs-410_7_8_112,cs-410,7,8,Paradigmatic,"00:09:02,930","00:09:05,750",112,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=542,and this means the vector is
cs-410_7_8_113,cs-410,7,8,Paradigmatic,"00:09:05,750","00:09:08,193",113,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=545,actually probability of
cs-410_7_8_114,cs-410,7,8,Paradigmatic,"00:09:10,500","00:09:15,883",114,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=550,"So, the vector d2 can be also"
cs-410_7_8_115,cs-410,7,8,Paradigmatic,"00:09:15,883","00:09:23,540",115,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=555,this would give us then two probability
cs-410_7_8_116,cs-410,7,8,Paradigmatic,"00:09:24,840","00:09:28,220",116,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=564,"So, that addresses the problem"
cs-410_7_8_117,cs-410,7,8,Paradigmatic,"00:09:28,220","00:09:31,760",117,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=568,next let's see how we can define
cs-410_7_8_118,cs-410,7,8,Paradigmatic,"00:09:31,760","00:09:35,668",118,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=571,"Well, here, we simply define"
cs-410_7_8_119,cs-410,7,8,Paradigmatic,"00:09:35,668","00:09:39,890",119,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=575,"vectors, and"
cs-410_7_8_120,cs-410,7,8,Paradigmatic,"00:09:41,410","00:09:43,960",120,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=581,of the corresponding
cs-410_7_8_121,cs-410,7,8,Paradigmatic,"00:09:46,630","00:09:51,847",121,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=586,"Now, it's interesting to see"
cs-410_7_8_122,cs-410,7,8,Paradigmatic,"00:09:51,847","00:09:57,360",122,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=591,"actually has a nice interpretation,"
cs-410_7_8_123,cs-410,7,8,Paradigmatic,"00:09:57,360","00:10:02,548",123,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=597,"Dot product, in fact that gives"
cs-410_7_8_124,cs-410,7,8,Paradigmatic,"00:10:02,548","00:10:08,570",124,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=602,randomly picked words from
cs-410_7_8_125,cs-410,7,8,Paradigmatic,"00:10:08,570","00:10:12,630",125,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=608,That means if we try to pick a word
cs-410_7_8_126,cs-410,7,8,Paradigmatic,"00:10:12,630","00:10:17,860",126,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=612,"word from another context, we can then"
cs-410_7_8_127,cs-410,7,8,Paradigmatic,"00:10:17,860","00:10:22,650",127,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=617,"If the two contexts are very similar,"
cs-410_7_8_128,cs-410,7,8,Paradigmatic,"00:10:22,650","00:10:27,390",128,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=622,see the two words picked from
cs-410_7_8_129,cs-410,7,8,Paradigmatic,"00:10:27,390","00:10:30,900",129,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=627,"If they are very different,"
cs-410_7_8_130,cs-410,7,8,Paradigmatic,"00:10:30,900","00:10:34,890",130,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=630,identical words being picked from
cs-410_7_8_131,cs-410,7,8,Paradigmatic,"00:10:34,890","00:10:39,865",131,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=634,"So this intuitively makes sense, right,"
cs-410_7_8_132,cs-410,7,8,Paradigmatic,"00:10:41,490","00:10:46,819",132,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=641,Now you might want to also take
cs-410_7_8_133,cs-410,7,8,Paradigmatic,"00:10:46,819","00:10:51,627",133,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=646,see why this can be interpreted
cs-410_7_8_134,cs-410,7,8,Paradigmatic,"00:10:51,627","00:10:55,410",134,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=651,two randomly picked words are identical.
cs-410_7_8_135,cs-410,7,8,Paradigmatic,"00:10:57,440","00:11:04,550",135,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=657,So if you just stare at the formula
cs-410_7_8_136,cs-410,7,8,Paradigmatic,"00:11:04,550","00:11:12,034",136,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=664,then you will see basically in each
cs-410_7_8_137,cs-410,7,8,Paradigmatic,"00:11:12,034","00:11:17,170",137,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=672,we will see an overlap on
cs-410_7_8_138,cs-410,7,8,Paradigmatic,"00:11:17,170","00:11:23,661",138,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=677,And where xi gives us a probability that
cs-410_7_8_139,cs-410,7,8,Paradigmatic,"00:11:23,661","00:11:28,503",139,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=683,and yi gives us the probability
cs-410_7_8_140,cs-410,7,8,Paradigmatic,"00:11:28,503","00:11:32,024",140,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=688,And when we pick the same
cs-410_7_8_141,cs-410,7,8,Paradigmatic,"00:11:32,024","00:11:34,920",141,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=692,"then we have an identical pick, right so."
cs-410_7_8_142,cs-410,7,8,Paradigmatic,"00:11:34,920","00:11:42,380",142,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=694,"That's one possible approach, EOWC,"
cs-410_7_8_143,cs-410,7,8,Paradigmatic,"00:11:42,380","00:11:49,440",143,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=702,"Now as always, we would like to assess"
cs-410_7_8_144,cs-410,7,8,Paradigmatic,"00:11:49,440","00:11:52,880",144,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=709,"Now of course, ultimately we have to"
cs-410_7_8_145,cs-410,7,8,Paradigmatic,"00:11:52,880","00:11:56,259",145,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=712,see if it gives us really
cs-410_7_8_146,cs-410,7,8,Paradigmatic,"00:11:57,730","00:12:01,010",146,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=717,"Really give us paradigmatical relations,"
cs-410_7_8_147,cs-410,7,8,Paradigmatic,"00:12:01,010","00:12:05,380",147,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=721,analytically we can also analyze
cs-410_7_8_148,cs-410,7,8,Paradigmatic,"00:12:05,380","00:12:11,020",148,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=725,"So first, as I said,"
cs-410_7_8_149,cs-410,7,8,Paradigmatic,"00:12:11,020","00:12:15,802",149,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=731,formula will give a higher score if there
cs-410_7_8_150,cs-410,7,8,Paradigmatic,"00:12:15,802","00:12:17,988",150,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=735,So that's exactly what we want.
cs-410_7_8_151,cs-410,7,8,Paradigmatic,"00:12:17,988","00:12:21,170",151,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=737,But if you analyze
cs-410_7_8_152,cs-410,7,8,Paradigmatic,"00:12:21,170","00:12:24,286",152,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=741,then you also see there might
cs-410_7_8_153,cs-410,7,8,Paradigmatic,"00:12:24,286","00:12:27,735",153,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=744,and specifically there
cs-410_7_8_154,cs-410,7,8,Paradigmatic,"00:12:27,735","00:12:33,935",154,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=747,"First, it might favor matching"
cs-410_7_8_155,cs-410,7,8,Paradigmatic,"00:12:33,935","00:12:35,795",155,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=753,over matching more distinct terms.
cs-410_7_8_156,cs-410,7,8,Paradigmatic,"00:12:36,825","00:12:44,300",156,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=756,"And that is because in the dot product,"
cs-410_7_8_157,cs-410,7,8,Paradigmatic,"00:12:44,300","00:12:50,190",157,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=764,element is shared by both contexts and
cs-410_7_8_158,cs-410,7,8,Paradigmatic,"00:12:51,250","00:12:55,710",158,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=771,it might indeed make the score
cs-410_7_8_159,cs-410,7,8,Paradigmatic,"00:12:55,710","00:13:01,150",159,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=775,where the two vectors actually have
cs-410_7_8_160,cs-410,7,8,Paradigmatic,"00:13:01,150","00:13:06,878",160,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=781,But each term has a relatively low
cs-410_7_8_161,cs-410,7,8,Paradigmatic,"00:13:06,878","00:13:09,586",161,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=786,"Of course, this might be"
cs-410_7_8_162,cs-410,7,8,Paradigmatic,"00:13:09,586","00:13:14,527",162,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=789,"But in our case, we should intuitively"
cs-410_7_8_163,cs-410,7,8,Paradigmatic,"00:13:14,527","00:13:19,645",163,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=794,"more different terms in the context,"
cs-410_7_8_164,cs-410,7,8,Paradigmatic,"00:13:19,645","00:13:24,253",164,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=799,in saying that the two words
cs-410_7_8_165,cs-410,7,8,Paradigmatic,"00:13:24,253","00:13:27,020",165,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=804,If you only rely on one term and
cs-410_7_8_166,cs-410,7,8,Paradigmatic,"00:13:27,020","00:13:32,465",166,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=807,"that's a little bit questionable,"
cs-410_7_8_167,cs-410,7,8,Paradigmatic,"00:13:34,675","00:13:38,795",167,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=814,Now the second problem is that it
cs-410_7_8_168,cs-410,7,8,Paradigmatic,"00:13:38,795","00:13:42,131",168,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=818,So if you match a word like the and
cs-410_7_8_169,cs-410,7,8,Paradigmatic,"00:13:42,131","00:13:47,443",169,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=822,it will be the same as
cs-410_7_8_170,cs-410,7,8,Paradigmatic,"00:13:47,443","00:13:52,388",170,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=827,intuitively we know
cs-410_7_8_171,cs-410,7,8,Paradigmatic,"00:13:52,388","00:13:57,816",171,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=832,surprising because the occurs everywhere.
cs-410_7_8_172,cs-410,7,8,Paradigmatic,"00:13:57,816","00:14:02,787",172,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=837,So matching the is not as such
cs-410_7_8_173,cs-410,7,8,Paradigmatic,"00:14:02,787","00:14:07,956",173,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=842,"a word like eats,"
cs-410_7_8_174,cs-410,7,8,Paradigmatic,"00:14:07,956","00:14:11,216",174,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=847,So this is another
cs-410_7_8_175,cs-410,7,8,Paradigmatic,"00:14:13,426","00:14:19,003",175,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=853,In the next chapter we are going to talk
cs-410_7_8_176,cs-410,7,8,Paradigmatic,"00:14:19,003","00:14:29,003",176,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=859,[MUSIC]
cs-410_7_9_1,cs-410,7,9,Paradigmatic,"00:00:05,960","00:00:08,625",1,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=5,"In this lecture, we continue"
cs-410_7_9_2,cs-410,7,9,Paradigmatic,"00:00:08,625","00:00:11,565",2,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=8,discussing Paradigmatical
cs-410_7_9_3,cs-410,7,9,Paradigmatic,"00:00:11,565","00:00:14,175",3,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=11,Earlier we introduced
cs-410_7_9_4,cs-410,7,9,Paradigmatic,"00:00:14,175","00:00:16,935",4,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=14,Expected Overlap of
cs-410_7_9_5,cs-410,7,9,Paradigmatic,"00:00:16,935","00:00:21,090",5,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=16,"In this method, we"
cs-410_7_9_6,cs-410,7,9,Paradigmatic,"00:00:21,090","00:00:23,040",6,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=21,a word vector that represents
cs-410_7_9_7,cs-410,7,9,Paradigmatic,"00:00:23,040","00:00:26,490",7,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=23,the probability of a
cs-410_7_9_8,cs-410,7,9,Paradigmatic,"00:00:26,490","00:00:30,345",8,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=26,We measure the similarity
cs-410_7_9_9,cs-410,7,9,Paradigmatic,"00:00:30,345","00:00:34,320",9,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=30,which can be interpreted as
cs-410_7_9_10,cs-410,7,9,Paradigmatic,"00:00:34,320","00:00:36,240",10,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=34,randomly picked words from
cs-410_7_9_11,cs-410,7,9,Paradigmatic,"00:00:36,240","00:00:38,585",11,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=36,the two contexts are identical.
cs-410_7_9_12,cs-410,7,9,Paradigmatic,"00:00:38,585","00:00:42,515",12,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=38,We also discussed
cs-410_7_9_13,cs-410,7,9,Paradigmatic,"00:00:42,515","00:00:45,920",13,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=42,The first is that
cs-410_7_9_14,cs-410,7,9,Paradigmatic,"00:00:45,920","00:00:47,900",14,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=45,one frequent term very well over
cs-410_7_9_15,cs-410,7,9,Paradigmatic,"00:00:47,900","00:00:50,390",15,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=47,matching more distinct terms.
cs-410_7_9_16,cs-410,7,9,Paradigmatic,"00:00:50,390","00:00:55,235",16,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=50,It put too much emphasis on
cs-410_7_9_17,cs-410,7,9,Paradigmatic,"00:00:55,235","00:01:00,350",17,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=55,The second is that it
cs-410_7_9_18,cs-410,7,9,Paradigmatic,"00:01:00,350","00:01:03,995",18,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=60,Even a common word like
cs-410_7_9_19,cs-410,7,9,Paradigmatic,"00:01:03,995","00:01:08,885",19,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=63,equally as content
cs-410_7_9_20,cs-410,7,9,Paradigmatic,"00:01:08,885","00:01:11,270",20,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=68,So now we are
cs-410_7_9_21,cs-410,7,9,Paradigmatic,"00:01:11,270","00:01:13,715",21,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=71,going to talk about how
cs-410_7_9_22,cs-410,7,9,Paradigmatic,"00:01:13,715","00:01:15,965",22,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=73,"More specifically, we're"
cs-410_7_9_23,cs-410,7,9,Paradigmatic,"00:01:15,965","00:01:19,790",23,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=75,some retrieval heuristics
cs-410_7_9_24,cs-410,7,9,Paradigmatic,"00:01:19,790","00:01:23,900",24,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=79,These heuristics can effectively
cs-410_7_9_25,cs-410,7,9,Paradigmatic,"00:01:23,900","00:01:26,975",25,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=83,as these problems also
cs-410_7_9_26,cs-410,7,9,Paradigmatic,"00:01:26,975","00:01:30,680",26,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=86,when we match a query that
cs-410_7_9_27,cs-410,7,9,Paradigmatic,"00:01:30,680","00:01:32,920",27,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=90,"So to address the first problem,"
cs-410_7_9_28,cs-410,7,9,Paradigmatic,"00:01:32,920","00:01:36,385",28,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=92,we can use a sublinear
cs-410_7_9_29,cs-410,7,9,Paradigmatic,"00:01:36,385","00:01:37,970",29,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=96,"That is, we don't have to use"
cs-410_7_9_30,cs-410,7,9,Paradigmatic,"00:01:37,970","00:01:39,650",30,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=97,the raw frequency count of
cs-410_7_9_31,cs-410,7,9,Paradigmatic,"00:01:39,650","00:01:42,140",31,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=99,a term to represent the context.
cs-410_7_9_32,cs-410,7,9,Paradigmatic,"00:01:42,140","00:01:44,780",32,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=102,We can transform
cs-410_7_9_33,cs-410,7,9,Paradigmatic,"00:01:44,780","00:01:48,025",33,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=104,that wouldn't emphasize so
cs-410_7_9_34,cs-410,7,9,Paradigmatic,"00:01:48,025","00:01:50,130",34,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=108,To address the
cs-410_7_9_35,cs-410,7,9,Paradigmatic,"00:01:50,130","00:01:53,195",35,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=110,we can put more weight
cs-410_7_9_36,cs-410,7,9,Paradigmatic,"00:01:53,195","00:01:56,330",36,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=113,That is we can reward
cs-410_7_9_37,cs-410,7,9,Paradigmatic,"00:01:56,330","00:01:58,760",37,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=116,This heuristic is called the IDF
cs-410_7_9_38,cs-410,7,9,Paradigmatic,"00:01:58,760","00:02:01,135",38,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=118,term weighting in text retrieval.
cs-410_7_9_39,cs-410,7,9,Paradigmatic,"00:02:01,135","00:02:05,085",39,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=121,IDF stands for
cs-410_7_9_40,cs-410,7,9,Paradigmatic,"00:02:05,085","00:02:07,010",40,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=125,"So now, we're going to talk about"
cs-410_7_9_41,cs-410,7,9,Paradigmatic,"00:02:07,010","00:02:10,130",41,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=127,the two heuristics
cs-410_7_9_42,cs-410,7,9,Paradigmatic,"00:02:10,130","00:02:13,930",42,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=130,First let's talk about
cs-410_7_9_43,cs-410,7,9,Paradigmatic,"00:02:13,930","00:02:16,400",43,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=133,That is to convert
cs-410_7_9_44,cs-410,7,9,Paradigmatic,"00:02:16,400","00:02:19,565",44,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=136,a word in the document
cs-410_7_9_45,cs-410,7,9,Paradigmatic,"00:02:19,565","00:02:23,195",45,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=139,that reflects our belief
cs-410_7_9_46,cs-410,7,9,Paradigmatic,"00:02:23,195","00:02:27,200",46,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=143,about how important
cs-410_7_9_47,cs-410,7,9,Paradigmatic,"00:02:27,200","00:02:32,370",47,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=147,So that will be
cs-410_7_9_48,cs-410,7,9,Paradigmatic,"00:02:32,370","00:02:36,415",48,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=152,That's shown in the y-axis.
cs-410_7_9_49,cs-410,7,9,Paradigmatic,"00:02:36,415","00:02:39,920",49,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=156,"Now, in general, there are"
cs-410_7_9_50,cs-410,7,9,Paradigmatic,"00:02:39,920","00:02:44,250",50,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=159,Let's first look at
cs-410_7_9_51,cs-410,7,9,Paradigmatic,"00:02:44,250","00:02:47,920",51,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=164,"In this case, we're"
cs-410_7_9_52,cs-410,7,9,Paradigmatic,"00:02:47,920","00:02:51,510",52,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=167,any non-zero counts
cs-410_7_9_53,cs-410,7,9,Paradigmatic,"00:02:51,510","00:02:55,450",53,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=171,one and the zero count
cs-410_7_9_54,cs-410,7,9,Paradigmatic,"00:02:55,450","00:02:56,990",54,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=175,So with this mapping
cs-410_7_9_55,cs-410,7,9,Paradigmatic,"00:02:56,990","00:02:59,240",55,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=176,all the frequencies will be
cs-410_7_9_56,cs-410,7,9,Paradigmatic,"00:02:59,240","00:03:02,605",56,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=179,mapped to only two
cs-410_7_9_57,cs-410,7,9,Paradigmatic,"00:03:02,605","00:03:11,015",57,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=182,The mapping function is shown
cs-410_7_9_58,cs-410,7,9,Paradigmatic,"00:03:11,015","00:03:14,030",58,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=191,"Now, this is naive"
cs-410_7_9_59,cs-410,7,9,Paradigmatic,"00:03:14,030","00:03:16,660",59,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=194,because it's not
cs-410_7_9_60,cs-410,7,9,Paradigmatic,"00:03:16,660","00:03:20,195",60,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=196,"However, this actually"
cs-410_7_9_61,cs-410,7,9,Paradigmatic,"00:03:20,195","00:03:25,700",61,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=200,emphasizing matching all
cs-410_7_9_62,cs-410,7,9,Paradigmatic,"00:03:25,700","00:03:27,725",62,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=205,So it does not allow
cs-410_7_9_63,cs-410,7,9,Paradigmatic,"00:03:27,725","00:03:30,505",63,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=207,a frequency of word to
cs-410_7_9_64,cs-410,7,9,Paradigmatic,"00:03:30,505","00:03:32,930",64,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=210,"Now, the approach"
cs-410_7_9_65,cs-410,7,9,Paradigmatic,"00:03:32,930","00:03:36,650",65,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=212,earlier in the expected
cs-410_7_9_66,cs-410,7,9,Paradigmatic,"00:03:36,650","00:03:38,225",66,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=216,is a linear transformation.
cs-410_7_9_67,cs-410,7,9,Paradigmatic,"00:03:38,225","00:03:41,870",67,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=218,"We basically, take"
cs-410_7_9_68,cs-410,7,9,Paradigmatic,"00:03:41,870","00:03:45,445",68,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=221,So we use the raw count
cs-410_7_9_69,cs-410,7,9,Paradigmatic,"00:03:45,445","00:03:48,140",69,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=225,That created the problem
cs-410_7_9_70,cs-410,7,9,Paradigmatic,"00:03:48,140","00:03:50,360",70,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=228,that we just talked about namely;
cs-410_7_9_71,cs-410,7,9,Paradigmatic,"00:03:50,360","00:03:54,935",71,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=230,it emphasize too much on just
cs-410_7_9_72,cs-410,7,9,Paradigmatic,"00:03:54,935","00:03:58,520",72,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=234,Matching one frequent term
cs-410_7_9_73,cs-410,7,9,Paradigmatic,"00:03:58,520","00:04:02,750",73,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=238,So we can have a lot
cs-410_7_9_74,cs-410,7,9,Paradigmatic,"00:04:02,750","00:04:04,475",74,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=242,of other interesting
cs-410_7_9_75,cs-410,7,9,Paradigmatic,"00:04:04,475","00:04:06,875",75,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=244,"in between the two extremes,"
cs-410_7_9_76,cs-410,7,9,Paradigmatic,"00:04:06,875","00:04:10,640",76,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=246,and they generally form
cs-410_7_9_77,cs-410,7,9,Paradigmatic,"00:04:10,640","00:04:13,340",77,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=250,"So for example,"
cs-410_7_9_78,cs-410,7,9,Paradigmatic,"00:04:13,340","00:04:16,080",78,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=253,"logarithm of the raw count,"
cs-410_7_9_79,cs-410,7,9,Paradigmatic,"00:04:16,080","00:04:19,400",79,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=256,and this will give us curve
cs-410_7_9_80,cs-410,7,9,Paradigmatic,"00:04:19,400","00:04:21,260",80,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=259,that you are seeing here.
cs-410_7_9_81,cs-410,7,9,Paradigmatic,"00:04:21,260","00:04:25,295",81,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=261,"In this case, you can see"
cs-410_7_9_82,cs-410,7,9,Paradigmatic,"00:04:25,295","00:04:29,330",82,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=265,The high counts are
cs-410_7_9_83,cs-410,7,9,Paradigmatic,"00:04:29,330","00:04:33,470",83,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=269,so the curve is a sublinear
cs-410_7_9_84,cs-410,7,9,Paradigmatic,"00:04:33,470","00:04:39,240",84,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=273,the weight of
cs-410_7_9_85,cs-410,7,9,Paradigmatic,"00:04:39,240","00:04:42,875",85,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=279,"This is what we want,"
cs-410_7_9_86,cs-410,7,9,Paradigmatic,"00:04:42,875","00:04:47,340",86,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=282,terms from dominating
cs-410_7_9_87,cs-410,7,9,Paradigmatic,"00:04:48,620","00:04:50,870",87,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=288,"Now, there is also"
cs-410_7_9_88,cs-410,7,9,Paradigmatic,"00:04:50,870","00:04:52,760",88,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=290,another interesting
cs-410_7_9_89,cs-410,7,9,Paradigmatic,"00:04:52,760","00:04:55,430",89,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=292,a BM25 transformation which
cs-410_7_9_90,cs-410,7,9,Paradigmatic,"00:04:55,430","00:04:59,945",90,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=295,has been shown to be very
cs-410_7_9_91,cs-410,7,9,Paradigmatic,"00:04:59,945","00:05:02,735",91,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=299,"In this transformation, we have"
cs-410_7_9_92,cs-410,7,9,Paradigmatic,"00:05:02,735","00:05:07,225",92,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=302,a form that looks like this.
cs-410_7_9_93,cs-410,7,9,Paradigmatic,"00:05:07,225","00:05:11,640",93,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=307,So it's k plus one multiplied
cs-410_7_9_94,cs-410,7,9,Paradigmatic,"00:05:11,640","00:05:13,800",94,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=311,"where k is a parameter,"
cs-410_7_9_95,cs-410,7,9,Paradigmatic,"00:05:13,800","00:05:16,485",95,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=313,"x is the count,"
cs-410_7_9_96,cs-410,7,9,Paradigmatic,"00:05:16,485","00:05:18,690",96,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=316,the raw count of a word.
cs-410_7_9_97,cs-410,7,9,Paradigmatic,"00:05:18,690","00:05:22,190",97,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=318,"Now, the transformation"
cs-410_7_9_98,cs-410,7,9,Paradigmatic,"00:05:22,190","00:05:25,430",98,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=322,that it can actually go from
cs-410_7_9_99,cs-410,7,9,Paradigmatic,"00:05:25,430","00:05:28,910",99,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=325,one extreme to the other
cs-410_7_9_100,cs-410,7,9,Paradigmatic,"00:05:28,910","00:05:34,725",100,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=328,k. It also interesting
cs-410_7_9_101,cs-410,7,9,Paradigmatic,"00:05:34,725","00:05:37,135",101,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=334,k plus one in this case.
cs-410_7_9_102,cs-410,7,9,Paradigmatic,"00:05:37,135","00:05:41,435",102,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=337,So this puts
cs-410_7_9_103,cs-410,7,9,Paradigmatic,"00:05:41,435","00:05:43,040",103,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=341,"on high frequency terms,"
cs-410_7_9_104,cs-410,7,9,Paradigmatic,"00:05:43,040","00:05:46,460",104,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=343,because their weight would
cs-410_7_9_105,cs-410,7,9,Paradigmatic,"00:05:46,460","00:05:50,900",105,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=346,"As we vary k, if we can"
cs-410_7_9_106,cs-410,7,9,Paradigmatic,"00:05:50,900","00:05:52,590",106,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=350,"So when k is set to zero,"
cs-410_7_9_107,cs-410,7,9,Paradigmatic,"00:05:52,590","00:05:55,680",107,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=352,"we roughly have the 0,1 vector."
cs-410_7_9_108,cs-410,7,9,Paradigmatic,"00:05:55,680","00:05:59,090",108,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=355,Whereas when we set k
cs-410_7_9_109,cs-410,7,9,Paradigmatic,"00:05:59,090","00:06:02,075",109,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=359,it will behave more like
cs-410_7_9_110,cs-410,7,9,Paradigmatic,"00:06:02,075","00:06:05,270",110,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=362,So this transformation
cs-410_7_9_111,cs-410,7,9,Paradigmatic,"00:06:05,270","00:06:07,880",111,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=365,far the most effective
cs-410_7_9_112,cs-410,7,9,Paradigmatic,"00:06:07,880","00:06:10,880",112,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=367,text retrieval and it also makes
cs-410_7_9_113,cs-410,7,9,Paradigmatic,"00:06:10,880","00:06:14,285",113,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=370,sense for our problem setup.
cs-410_7_9_114,cs-410,7,9,Paradigmatic,"00:06:14,285","00:06:17,195",114,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=374,So we just talked about how
cs-410_7_9_115,cs-410,7,9,Paradigmatic,"00:06:17,195","00:06:20,660",115,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=377,overemphasizing a frequency term
cs-410_7_9_116,cs-410,7,9,Paradigmatic,"00:06:20,660","00:06:22,850",116,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=380,Now let's look at
cs-410_7_9_117,cs-410,7,9,Paradigmatic,"00:06:22,850","00:06:26,585",117,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=382,and that is how we can
cs-410_7_9_118,cs-410,7,9,Paradigmatic,"00:06:26,585","00:06:28,935",118,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=386,"Matching ""the"" is not surprising,"
cs-410_7_9_119,cs-410,7,9,Paradigmatic,"00:06:28,935","00:06:30,645",119,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=388,"because ""the"" occurs everywhere."
cs-410_7_9_120,cs-410,7,9,Paradigmatic,"00:06:30,645","00:06:33,020",120,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=390,"But matching ""eats"""
cs-410_7_9_121,cs-410,7,9,Paradigmatic,"00:06:33,020","00:06:35,105",121,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=393,So how can we address
cs-410_7_9_122,cs-410,7,9,Paradigmatic,"00:06:35,105","00:06:38,965",122,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=395,"Now in this case, we can"
cs-410_7_9_123,cs-410,7,9,Paradigmatic,"00:06:38,965","00:06:42,095",123,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=398,That's commonly
cs-410_7_9_124,cs-410,7,9,Paradigmatic,"00:06:42,095","00:06:45,065",124,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=402,IDF stands for
cs-410_7_9_125,cs-410,7,9,Paradigmatic,"00:06:45,065","00:06:47,675",125,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=405,Document frequency
cs-410_7_9_126,cs-410,7,9,Paradigmatic,"00:06:47,675","00:06:49,370",126,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=407,of the total number of
cs-410_7_9_127,cs-410,7,9,Paradigmatic,"00:06:49,370","00:06:52,235",127,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=409,documents that contain
cs-410_7_9_128,cs-410,7,9,Paradigmatic,"00:06:52,235","00:06:57,200",128,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=412,So here we show that the IDF
cs-410_7_9_129,cs-410,7,9,Paradigmatic,"00:06:57,200","00:07:00,230",129,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=417,a logarithm function
cs-410_7_9_130,cs-410,7,9,Paradigmatic,"00:07:00,230","00:07:05,065",130,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=420,of documents that match a
cs-410_7_9_131,cs-410,7,9,Paradigmatic,"00:07:05,065","00:07:08,870",131,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=425,So K is the number of
cs-410_7_9_132,cs-410,7,9,Paradigmatic,"00:07:08,870","00:07:11,630",132,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=428,document frequency and M
cs-410_7_9_133,cs-410,7,9,Paradigmatic,"00:07:11,630","00:07:14,615",133,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=431,here is the total number of
cs-410_7_9_134,cs-410,7,9,Paradigmatic,"00:07:14,615","00:07:21,200",134,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=434,The IDF function is giving
cs-410_7_9_135,cs-410,7,9,Paradigmatic,"00:07:21,200","00:07:24,815",135,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=441,meaning that it
cs-410_7_9_136,cs-410,7,9,Paradigmatic,"00:07:24,815","00:07:28,805",136,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=444,The maximum value is
cs-410_7_9_137,cs-410,7,9,Paradigmatic,"00:07:28,805","00:07:33,650",137,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=448,That's when the word occurred
cs-410_7_9_138,cs-410,7,9,Paradigmatic,"00:07:33,650","00:07:37,235",138,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=453,"So that's a very rare term,"
cs-410_7_9_139,cs-410,7,9,Paradigmatic,"00:07:37,235","00:07:40,745",139,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=457,the rare is term in
cs-410_7_9_140,cs-410,7,9,Paradigmatic,"00:07:40,745","00:07:46,700",140,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=460,The lowest value you can
cs-410_7_9_141,cs-410,7,9,Paradigmatic,"00:07:46,700","00:07:49,115",141,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=466,its maximum which would be M.
cs-410_7_9_142,cs-410,7,9,Paradigmatic,"00:07:49,115","00:07:53,880",142,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=469,So that would be
cs-410_7_9_143,cs-410,7,9,Paradigmatic,"00:07:53,990","00:07:57,340",143,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=473,close to zero in fact.
cs-410_7_9_144,cs-410,7,9,Paradigmatic,"00:07:57,470","00:08:02,360",144,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=477,So this of course measure
cs-410_7_9_145,cs-410,7,9,Paradigmatic,"00:08:02,360","00:08:06,740",145,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=482,is used in search where we
cs-410_7_9_146,cs-410,7,9,Paradigmatic,"00:08:06,740","00:08:09,960",146,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=486,"In our case, what would"
cs-410_7_9_147,cs-410,7,9,Paradigmatic,"00:08:09,960","00:08:13,040",147,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=489,"Well, we can also"
cs-410_7_9_148,cs-410,7,9,Paradigmatic,"00:08:13,040","00:08:16,610",148,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=493,we can collect all the words
cs-410_7_9_149,cs-410,7,9,Paradigmatic,"00:08:16,610","00:08:18,590",149,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=496,"That is to say,"
cs-410_7_9_150,cs-410,7,9,Paradigmatic,"00:08:18,590","00:08:22,225",150,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=498,a word that's popular in
cs-410_7_9_151,cs-410,7,9,Paradigmatic,"00:08:22,225","00:08:25,650",151,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=502,would also have a low IDF.
cs-410_7_9_152,cs-410,7,9,Paradigmatic,"00:08:25,650","00:08:29,445",152,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=505,"Because depending on the dataset,"
cs-410_7_9_153,cs-410,7,9,Paradigmatic,"00:08:29,445","00:08:35,105",153,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=509,we can construct the context
cs-410_7_9_154,cs-410,7,9,Paradigmatic,"00:08:35,105","00:08:38,010",154,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=515,But in the end if a term is
cs-410_7_9_155,cs-410,7,9,Paradigmatic,"00:08:38,010","00:08:41,024",155,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=518,very frequent in
cs-410_7_9_156,cs-410,7,9,Paradigmatic,"00:08:41,024","00:08:43,210",156,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=521,then it will still be frequent
cs-410_7_9_157,cs-410,7,9,Paradigmatic,"00:08:43,210","00:08:47,220",157,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=523,in the collective
cs-410_7_9_158,cs-410,7,9,Paradigmatic,"00:08:47,620","00:08:52,355",158,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=527,So how can we add
cs-410_7_9_159,cs-410,7,9,Paradigmatic,"00:08:52,355","00:08:56,910",159,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=532,improve our similarity function?
cs-410_7_9_160,cs-410,7,9,Paradigmatic,"00:08:56,910","00:08:58,565",160,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=536,"Well, here's one way"
cs-410_7_9_161,cs-410,7,9,Paradigmatic,"00:08:58,565","00:09:00,920",161,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=538,many other ways
cs-410_7_9_162,cs-410,7,9,Paradigmatic,"00:09:00,920","00:09:02,520",162,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=540,"But this is a reasonable way,"
cs-410_7_9_163,cs-410,7,9,Paradigmatic,"00:09:02,520","00:09:05,825",163,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=542,where we can adapt
cs-410_7_9_164,cs-410,7,9,Paradigmatic,"00:09:05,825","00:09:09,520",164,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=545,for paradigmatical
cs-410_7_9_165,cs-410,7,9,Paradigmatic,"00:09:14,120","00:09:20,555",165,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=554,"In this case, we define the"
cs-410_7_9_166,cs-410,7,9,Paradigmatic,"00:09:20,555","00:09:26,825",166,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=560,elements representing
cs-410_7_9_167,cs-410,7,9,Paradigmatic,"00:09:26,825","00:09:29,810",167,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=566,So in this
cs-410_7_9_168,cs-410,7,9,Paradigmatic,"00:09:29,810","00:09:36,985",168,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=569,we take sum over all
cs-410_7_9_169,cs-410,7,9,Paradigmatic,"00:09:36,985","00:09:42,155",169,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=576,normalize the weight of
cs-410_7_9_170,cs-410,7,9,Paradigmatic,"00:09:42,155","00:09:48,210",170,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=582,of the weights of all the words.
cs-410_7_9_171,cs-410,7,9,Paradigmatic,"00:09:48,210","00:09:51,030",171,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=588,This is to again ensure all the
cs-410_7_9_172,cs-410,7,9,Paradigmatic,"00:09:51,030","00:09:53,975",172,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=591,xi's will sum to
cs-410_7_9_173,cs-410,7,9,Paradigmatic,"00:09:53,975","00:09:57,800",173,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=593,So this would be very similar
cs-410_7_9_174,cs-410,7,9,Paradigmatic,"00:09:57,800","00:09:59,420",174,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=597,in that this vector is
cs-410_7_9_175,cs-410,7,9,Paradigmatic,"00:09:59,420","00:10:04,015",175,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=599,actually something similar
cs-410_7_9_176,cs-410,7,9,Paradigmatic,"00:10:04,015","00:10:06,685",176,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=604,all the xi's will sum to one.
cs-410_7_9_177,cs-410,7,9,Paradigmatic,"00:10:06,685","00:10:13,560",177,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=606,"Now, the weight of BM25 for"
cs-410_7_9_178,cs-410,7,9,Paradigmatic,"00:10:14,460","00:10:18,940",178,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=614,If you compare this with
cs-410_7_9_179,cs-410,7,9,Paradigmatic,"00:10:18,940","00:10:22,930",179,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=618,have a normalized count
cs-410_7_9_180,cs-410,7,9,Paradigmatic,"00:10:22,930","00:10:26,320",180,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=622,So we only have this one
cs-410_7_9_181,cs-410,7,9,Paradigmatic,"00:10:26,320","00:10:31,090",181,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=626,the total counts of words in
cs-410_7_9_182,cs-410,7,9,Paradigmatic,"00:10:31,090","00:10:33,430",182,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=631,and that's what we had before.
cs-410_7_9_183,cs-410,7,9,Paradigmatic,"00:10:33,430","00:10:36,039",183,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=633,But now with the BM25
cs-410_7_9_184,cs-410,7,9,Paradigmatic,"00:10:36,039","00:10:38,335",184,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=636,we introduced something else.
cs-410_7_9_185,cs-410,7,9,Paradigmatic,"00:10:38,335","00:10:42,040",185,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=638,"First, of course,"
cs-410_7_9_186,cs-410,7,9,Paradigmatic,"00:10:42,040","00:10:43,420",186,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=642,this count is just to
cs-410_7_9_187,cs-410,7,9,Paradigmatic,"00:10:43,420","00:10:46,075",187,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=643,achieve the sub-linear
cs-410_7_9_188,cs-410,7,9,Paradigmatic,"00:10:46,075","00:10:50,155",188,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=646,But we also see we introduced
cs-410_7_9_189,cs-410,7,9,Paradigmatic,"00:10:50,155","00:10:56,110",189,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=650,and this parameter is
cs-410_7_9_190,cs-410,7,9,Paradigmatic,"00:10:56,110","00:10:58,810",190,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=656,although zero is also possible.
cs-410_7_9_191,cs-410,7,9,Paradigmatic,"00:10:58,810","00:11:02,950",191,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=658,But this controls
cs-410_7_9_192,cs-410,7,9,Paradigmatic,"00:11:02,950","00:11:06,535",192,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=662,and also controls to what extent
cs-410_7_9_193,cs-410,7,9,Paradigmatic,"00:11:06,535","00:11:11,240",193,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=666,it simulates the
cs-410_7_9_194,cs-410,7,9,Paradigmatic,"00:11:11,250","00:11:14,830",194,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=671,"So this is one parameter,"
cs-410_7_9_195,cs-410,7,9,Paradigmatic,"00:11:14,830","00:11:17,140",195,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=674,but we also see there is
cs-410_7_9_196,cs-410,7,9,Paradigmatic,"00:11:17,140","00:11:21,115",196,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=677,"b, and this would be"
cs-410_7_9_197,cs-410,7,9,Paradigmatic,"00:11:21,115","00:11:25,405",197,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=681,This is a parameter to
cs-410_7_9_198,cs-410,7,9,Paradigmatic,"00:11:25,405","00:11:27,294",198,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=685,"In this case,"
cs-410_7_9_199,cs-410,7,9,Paradigmatic,"00:11:27,294","00:11:29,200",199,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=687,the normalization formula has
cs-410_7_9_200,cs-410,7,9,Paradigmatic,"00:11:29,200","00:11:31,885",200,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=689,a average document lens here.
cs-410_7_9_201,cs-410,7,9,Paradigmatic,"00:11:31,885","00:11:35,770",201,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=691,This is computed up
cs-410_7_9_202,cs-410,7,9,Paradigmatic,"00:11:35,770","00:11:39,880",202,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=695,of the lenses of all the
cs-410_7_9_203,cs-410,7,9,Paradigmatic,"00:11:39,880","00:11:41,605",203,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=699,"In this case, all the lenses of"
cs-410_7_9_204,cs-410,7,9,Paradigmatic,"00:11:41,605","00:11:45,340",204,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=701,all the context of documents
cs-410_7_9_205,cs-410,7,9,Paradigmatic,"00:11:45,340","00:11:48,175",205,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=705,So this average documents
cs-410_7_9_206,cs-410,7,9,Paradigmatic,"00:11:48,175","00:11:50,425",206,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=708,will be a constant for
cs-410_7_9_207,cs-410,7,9,Paradigmatic,"00:11:50,425","00:11:52,795",207,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=710,So it actually is only
cs-410_7_9_208,cs-410,7,9,Paradigmatic,"00:11:52,795","00:11:56,530",208,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=712,affecting the effect
cs-410_7_9_209,cs-410,7,9,Paradigmatic,"00:11:56,530","00:12:01,180",209,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=716,"b, here because"
cs-410_7_9_210,cs-410,7,9,Paradigmatic,"00:12:01,180","00:12:07,780",210,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=721,But I kept it here because
cs-410_7_9_211,cs-410,7,9,Paradigmatic,"00:12:07,780","00:12:10,840",211,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=727,for in retrieval where it would
cs-410_7_9_212,cs-410,7,9,Paradigmatic,"00:12:10,840","00:12:14,770",212,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=730,give us a stabilized
cs-410_7_9_213,cs-410,7,9,Paradigmatic,"00:12:14,770","00:12:16,570",213,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=734,"But for our purpose,"
cs-410_7_9_214,cs-410,7,9,Paradigmatic,"00:12:16,570","00:12:21,430",214,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=736,this will be a constant so
cs-410_7_9_215,cs-410,7,9,Paradigmatic,"00:12:21,430","00:12:28,550",215,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=741,the lens normalization
cs-410_7_9_216,cs-410,7,9,Paradigmatic,"00:12:29,400","00:12:33,295",216,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=749,"Now, with this definition then,"
cs-410_7_9_217,cs-410,7,9,Paradigmatic,"00:12:33,295","00:12:37,810",217,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=753,we have a new way to define
cs-410_7_9_218,cs-410,7,9,Paradigmatic,"00:12:37,810","00:12:41,785",218,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=757,and we can compute
cs-410_7_9_219,cs-410,7,9,Paradigmatic,"00:12:41,785","00:12:43,255",219,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=761,The difference is that
cs-410_7_9_220,cs-410,7,9,Paradigmatic,"00:12:43,255","00:12:44,950",220,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=763,the high-frequency terms will now
cs-410_7_9_221,cs-410,7,9,Paradigmatic,"00:12:44,950","00:12:46,930",221,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=764,have a somewhat lower weights.
cs-410_7_9_222,cs-410,7,9,Paradigmatic,"00:12:46,930","00:12:49,690",222,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=766,This would help us control
cs-410_7_9_223,cs-410,7,9,Paradigmatic,"00:12:49,690","00:12:53,575",223,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=769,the inference of
cs-410_7_9_224,cs-410,7,9,Paradigmatic,"00:12:53,575","00:12:58,000",224,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=773,"Now, the idea can be added"
cs-410_7_9_225,cs-410,7,9,Paradigmatic,"00:12:58,000","00:12:59,905",225,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=778,That means we'll
cs-410_7_9_226,cs-410,7,9,Paradigmatic,"00:12:59,905","00:13:01,990",226,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=779,for matching each term.
cs-410_7_9_227,cs-410,7,9,Paradigmatic,"00:13:01,990","00:13:05,650",227,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=781,So you may recall
cs-410_7_9_228,cs-410,7,9,Paradigmatic,"00:13:05,650","00:13:08,305",228,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=785,all the possible words
cs-410_7_9_229,cs-410,7,9,Paradigmatic,"00:13:08,305","00:13:11,365",229,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=788,overlap between the two contexts.
cs-410_7_9_230,cs-410,7,9,Paradigmatic,"00:13:11,365","00:13:15,790",230,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=791,The x_i and the y_i
cs-410_7_9_231,cs-410,7,9,Paradigmatic,"00:13:15,790","00:13:20,245",231,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=795,of picking the word
cs-410_7_9_232,cs-410,7,9,Paradigmatic,"00:13:20,245","00:13:22,330",232,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=800,"Therefore, it"
cs-410_7_9_233,cs-410,7,9,Paradigmatic,"00:13:22,330","00:13:24,805",233,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=802,we'll see a match on this word.
cs-410_7_9_234,cs-410,7,9,Paradigmatic,"00:13:24,805","00:13:26,695",234,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=804,"Now, IDF would give us"
cs-410_7_9_235,cs-410,7,9,Paradigmatic,"00:13:26,695","00:13:29,200",235,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=806,the importance of
cs-410_7_9_236,cs-410,7,9,Paradigmatic,"00:13:29,200","00:13:33,700",236,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=809,A common word will be worth
cs-410_7_9_237,cs-410,7,9,Paradigmatic,"00:13:33,700","00:13:36,715",237,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=813,So we emphasize more on
cs-410_7_9_238,cs-410,7,9,Paradigmatic,"00:13:36,715","00:13:38,785",238,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=816,"So with this modification,"
cs-410_7_9_239,cs-410,7,9,Paradigmatic,"00:13:38,785","00:13:40,660",239,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=818,then the new function will
cs-410_7_9_240,cs-410,7,9,Paradigmatic,"00:13:40,660","00:13:43,270",240,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=820,likely address
cs-410_7_9_241,cs-410,7,9,Paradigmatic,"00:13:43,270","00:13:45,310",241,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=823,"Now, interestingly"
cs-410_7_9_242,cs-410,7,9,Paradigmatic,"00:13:45,310","00:13:49,825",242,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=825,this approach to discover
cs-410_7_9_243,cs-410,7,9,Paradigmatic,"00:13:49,825","00:13:57,430",243,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=829,"In general, when we re-brand"
cs-410_7_9_244,cs-410,7,9,Paradigmatic,"00:13:57,430","00:13:59,365",244,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=837,"a context with a term vector,"
cs-410_7_9_245,cs-410,7,9,Paradigmatic,"00:13:59,365","00:14:01,900",245,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=839,we would likely see
cs-410_7_9_246,cs-410,7,9,Paradigmatic,"00:14:01,900","00:14:04,135",246,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=841,some terms have high weights
cs-410_7_9_247,cs-410,7,9,Paradigmatic,"00:14:04,135","00:14:06,040",247,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=844,and other terms have low weights.
cs-410_7_9_248,cs-410,7,9,Paradigmatic,"00:14:06,040","00:14:09,490",248,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=846,Depending on how we assign
cs-410_7_9_249,cs-410,7,9,Paradigmatic,"00:14:09,490","00:14:11,650",249,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=849,we might be able to
cs-410_7_9_250,cs-410,7,9,Paradigmatic,"00:14:11,650","00:14:13,720",250,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=851,discover the words that
cs-410_7_9_251,cs-410,7,9,Paradigmatic,"00:14:13,720","00:14:15,700",251,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=853,are strongly associated with
cs-410_7_9_252,cs-410,7,9,Paradigmatic,"00:14:15,700","00:14:18,400",252,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=855,the candidate word
cs-410_7_9_253,cs-410,7,9,Paradigmatic,"00:14:18,400","00:14:20,560",253,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=858,So let's take a look at
cs-410_7_9_254,cs-410,7,9,Paradigmatic,"00:14:20,560","00:14:23,815",254,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=860,the term vector in
cs-410_7_9_255,cs-410,7,9,Paradigmatic,"00:14:23,815","00:14:29,885",255,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=863,We have each x_i
cs-410_7_9_256,cs-410,7,9,Paradigmatic,"00:14:29,885","00:14:33,610",256,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=869,defined as the normalized
cs-410_7_9_257,cs-410,7,9,Paradigmatic,"00:14:33,610","00:14:37,420",257,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=873,"Now, this weight alone only"
cs-410_7_9_258,cs-410,7,9,Paradigmatic,"00:14:37,420","00:14:41,110",258,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=877,reflects how frequent the word
cs-410_7_9_259,cs-410,7,9,Paradigmatic,"00:14:41,110","00:14:43,345",259,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=881,But we can't just say
cs-410_7_9_260,cs-410,7,9,Paradigmatic,"00:14:43,345","00:14:44,500",260,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=883,any frequent term in
cs-410_7_9_261,cs-410,7,9,Paradigmatic,"00:14:44,500","00:14:46,560",261,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=884,the context that would
cs-410_7_9_262,cs-410,7,9,Paradigmatic,"00:14:46,560","00:14:50,235",262,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=886,the candidate word because
cs-410_7_9_263,cs-410,7,9,Paradigmatic,"00:14:50,235","00:14:51,990",263,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=890,many common words like 'the' will
cs-410_7_9_264,cs-410,7,9,Paradigmatic,"00:14:51,990","00:14:54,540",264,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=891,occur frequently in
cs-410_7_9_265,cs-410,7,9,Paradigmatic,"00:14:54,540","00:14:59,645",265,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=894,But if we apply IDF
cs-410_7_9_266,cs-410,7,9,Paradigmatic,"00:14:59,645","00:15:07,090",266,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=899,we can then re-weight
cs-410_7_9_267,cs-410,7,9,Paradigmatic,"00:15:07,090","00:15:09,220",267,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=907,That means the words that are
cs-410_7_9_268,cs-410,7,9,Paradigmatic,"00:15:09,220","00:15:11,920",268,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=909,common like 'the'
cs-410_7_9_269,cs-410,7,9,Paradigmatic,"00:15:11,920","00:15:14,920",269,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=911,So now the highest
cs-410_7_9_270,cs-410,7,9,Paradigmatic,"00:15:14,920","00:15:18,220",270,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=914,those common terms because
cs-410_7_9_271,cs-410,7,9,Paradigmatic,"00:15:18,220","00:15:20,980",271,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=918,"Instead, those terms would"
cs-410_7_9_272,cs-410,7,9,Paradigmatic,"00:15:20,980","00:15:23,920",272,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=920,be the terms that are
cs-410_7_9_273,cs-410,7,9,Paradigmatic,"00:15:23,920","00:15:26,080",273,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=923,but not frequent
cs-410_7_9_274,cs-410,7,9,Paradigmatic,"00:15:26,080","00:15:29,590",274,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=926,So those are clearly the words
cs-410_7_9_275,cs-410,7,9,Paradigmatic,"00:15:29,590","00:15:33,820",275,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=929,the context of the candidate
cs-410_7_9_276,cs-410,7,9,Paradigmatic,"00:15:33,820","00:15:35,365",276,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=933,"So for this reason,"
cs-410_7_9_277,cs-410,7,9,Paradigmatic,"00:15:35,365","00:15:39,865",277,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=935,the highly weighted terms in
cs-410_7_9_278,cs-410,7,9,Paradigmatic,"00:15:39,865","00:15:42,310",278,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=939,can also be assumed to
cs-410_7_9_279,cs-410,7,9,Paradigmatic,"00:15:42,310","00:15:45,940",279,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=942,be candidates for
cs-410_7_9_280,cs-410,7,9,Paradigmatic,"00:15:45,940","00:15:48,895",280,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=945,"Now, of course, this is"
cs-410_7_9_281,cs-410,7,9,Paradigmatic,"00:15:48,895","00:15:53,560",281,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=948,our approach for discovering
cs-410_7_9_282,cs-410,7,9,Paradigmatic,"00:15:53,560","00:15:57,025",282,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=953,"In the next lecture, we're"
cs-410_7_9_283,cs-410,7,9,Paradigmatic,"00:15:57,025","00:16:01,850",283,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=957,how to discover
cs-410_7_9_284,cs-410,7,9,Paradigmatic,"00:16:02,280","00:16:05,305",284,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=962,But it clearly shows the relation
cs-410_7_9_285,cs-410,7,9,Paradigmatic,"00:16:05,305","00:16:08,995",285,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=965,between discovering
cs-410_7_9_286,cs-410,7,9,Paradigmatic,"00:16:08,995","00:16:12,670",286,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=968,Indeed they can be discovered in
cs-410_7_9_287,cs-410,7,9,Paradigmatic,"00:16:12,670","00:16:18,340",287,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=972,a joint manner by leveraging
cs-410_7_9_288,cs-410,7,9,Paradigmatic,"00:16:18,340","00:16:22,600",288,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=978,"So to summarize,"
cs-410_7_9_289,cs-410,7,9,Paradigmatic,"00:16:22,600","00:16:26,050",289,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=982,paradigmatic relations is to
cs-410_7_9_290,cs-410,7,9,Paradigmatic,"00:16:26,050","00:16:27,610",290,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=986,collect the context of
cs-410_7_9_291,cs-410,7,9,Paradigmatic,"00:16:27,610","00:16:30,460",291,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=987,a candidate word to
cs-410_7_9_292,cs-410,7,9,Paradigmatic,"00:16:30,460","00:16:33,685",292,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=990,This is typically represented
cs-410_7_9_293,cs-410,7,9,Paradigmatic,"00:16:33,685","00:16:35,890",293,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=993,Then compute the similarity of
cs-410_7_9_294,cs-410,7,9,Paradigmatic,"00:16:35,890","00:16:38,005",294,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=995,the corresponding
cs-410_7_9_295,cs-410,7,9,Paradigmatic,"00:16:38,005","00:16:40,540",295,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=998,of two candidate words.
cs-410_7_9_296,cs-410,7,9,Paradigmatic,"00:16:40,540","00:16:45,910",296,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1000,Then we can take
cs-410_7_9_297,cs-410,7,9,Paradigmatic,"00:16:45,910","00:16:50,305",297,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1005,and treat them as having
cs-410_7_9_298,cs-410,7,9,Paradigmatic,"00:16:50,305","00:16:53,395",298,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1010,These are the words that
cs-410_7_9_299,cs-410,7,9,Paradigmatic,"00:16:53,395","00:16:55,540",299,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1013,There are many different ways to
cs-410_7_9_300,cs-410,7,9,Paradigmatic,"00:16:55,540","00:16:58,090",300,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1015,implement this general idea.
cs-410_7_9_301,cs-410,7,9,Paradigmatic,"00:16:58,090","00:17:01,435",301,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1018,We just talked about
cs-410_7_9_302,cs-410,7,9,Paradigmatic,"00:17:01,435","00:17:04,510",302,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1021,"More specifically, we"
cs-410_7_9_303,cs-410,7,9,Paradigmatic,"00:17:04,510","00:17:07,765",303,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1024,text retrieval models to help us
cs-410_7_9_304,cs-410,7,9,Paradigmatic,"00:17:07,765","00:17:10,690",304,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1027,design effective
cs-410_7_9_305,cs-410,7,9,Paradigmatic,"00:17:10,690","00:17:15,170",305,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1030,compute the
cs-410_7_9_306,cs-410,7,9,Paradigmatic,"00:17:15,960","00:17:19,330",306,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1035,"More specifically, we have used"
cs-410_7_9_307,cs-410,7,9,Paradigmatic,"00:17:19,330","00:17:23,020",307,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1039,the BM25 and IDF weighting
cs-410_7_9_308,cs-410,7,9,Paradigmatic,"00:17:23,020","00:17:27,250",308,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1043,to discover
cs-410_7_9_309,cs-410,7,9,Paradigmatic,"00:17:27,250","00:17:30,100",309,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1047,These approaches also represent
cs-410_7_9_310,cs-410,7,9,Paradigmatic,"00:17:30,100","00:17:33,310",310,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1050,the state of the art in
cs-410_7_9_311,cs-410,7,9,Paradigmatic,"00:17:33,310","00:17:37,165",311,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1053,"Finally, syntagmatic relations"
cs-410_7_9_312,cs-410,7,9,Paradigmatic,"00:17:37,165","00:17:42,140",312,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1057,as a by-product when we discover
cs-410_8_1_1,cs-410,8,1,Syntagmatic,"00:00:00,250","00:00:06,380",1,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=0,[SOUND].
cs-410_8_1_2,cs-410,8,1,Syntagmatic,"00:00:06,380","00:00:13,220",2,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=6,This lecture is about the syntagmatic
cs-410_8_1_3,cs-410,8,1,Syntagmatic,"00:00:13,220","00:00:17,760",3,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=13,"In this lecture, we're going to continue"
cs-410_8_1_4,cs-410,8,1,Syntagmatic,"00:00:17,760","00:00:22,420",4,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=17,"In particular, we're going to talk about"
cs-410_8_1_5,cs-410,8,1,Syntagmatic,"00:00:22,420","00:00:25,770",5,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=22,And we're going to start with
cs-410_8_1_6,cs-410,8,1,Syntagmatic,"00:00:25,770","00:00:29,860",6,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=25,which is the basis for designing some
cs-410_8_1_7,cs-410,8,1,Syntagmatic,"00:00:32,480","00:00:33,110",7,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=32,"By definition,"
cs-410_8_1_8,cs-410,8,1,Syntagmatic,"00:00:33,110","00:00:39,890",8,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=33,syntagmatic relations hold between words
cs-410_8_1_9,cs-410,8,1,Syntagmatic,"00:00:39,890","00:00:44,190",9,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=39,"That means,"
cs-410_8_1_10,cs-410,8,1,Syntagmatic,"00:00:44,190","00:00:47,350",10,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=44,we tend to see the occurrence
cs-410_8_1_11,cs-410,8,1,Syntagmatic,"00:00:48,370","00:00:53,560",11,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=48,"So, take a more specific example, here."
cs-410_8_1_12,cs-410,8,1,Syntagmatic,"00:00:53,560","00:00:55,470",12,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=53,"We can ask the question,"
cs-410_8_1_13,cs-410,8,1,Syntagmatic,"00:00:55,470","00:00:59,750",13,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=55,"whenever eats occurs,"
cs-410_8_1_14,cs-410,8,1,Syntagmatic,"00:01:01,140","00:01:06,000",14,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=61,"Looking at the sentences on the left,"
cs-410_8_1_15,cs-410,8,1,Syntagmatic,"00:01:06,000","00:01:11,030",15,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=66,"together with eats, like cat,"
cs-410_8_1_16,cs-410,8,1,Syntagmatic,"00:01:11,030","00:01:15,870",16,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=71,But if I take them out and
cs-410_8_1_17,cs-410,8,1,Syntagmatic,"00:01:15,870","00:01:21,550",17,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=75,"only show eats and some other words,"
cs-410_8_1_18,cs-410,8,1,Syntagmatic,"00:01:21,550","00:01:27,050",18,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=81,Can you predict what other words
cs-410_8_1_19,cs-410,8,1,Syntagmatic,"00:01:28,315","00:01:31,040",19,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=88,Right so
cs-410_8_1_20,cs-410,8,1,Syntagmatic,"00:01:31,040","00:01:33,630",20,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=91,other words are associated with eats.
cs-410_8_1_21,cs-410,8,1,Syntagmatic,"00:01:33,630","00:01:37,610",21,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=93,"If they are associated with eats,"
cs-410_8_1_22,cs-410,8,1,Syntagmatic,"00:01:38,625","00:01:43,060",22,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=98,More specifically our
cs-410_8_1_23,cs-410,8,1,Syntagmatic,"00:01:43,060","00:01:47,072",23,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=103,"any text segment which can be a sentence,"
cs-410_8_1_24,cs-410,8,1,Syntagmatic,"00:01:47,072","00:01:51,340",24,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=107,"And then ask I the question,"
cs-410_8_1_25,cs-410,8,1,Syntagmatic,"00:01:51,340","00:01:52,640",25,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=111,absent in this segment?
cs-410_8_1_26,cs-410,8,1,Syntagmatic,"00:01:54,550","00:01:57,400",26,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=114,Right here we ask about the word W.
cs-410_8_1_27,cs-410,8,1,Syntagmatic,"00:01:57,400","00:02:00,160",27,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=117,Is W present or absent in this segment?
cs-410_8_1_28,cs-410,8,1,Syntagmatic,"00:02:02,400","00:02:05,100",28,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=122,Now what's interesting is that
cs-410_8_1_29,cs-410,8,1,Syntagmatic,"00:02:05,100","00:02:08,230",29,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=125,some words are actually easier
cs-410_8_1_30,cs-410,8,1,Syntagmatic,"00:02:10,150","00:02:14,570",30,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=130,If you take a look at the three
cs-410_8_1_31,cs-410,8,1,Syntagmatic,"00:02:14,570","00:02:17,970",31,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=134,"unicorn, which one do you"
cs-410_8_1_32,cs-410,8,1,Syntagmatic,"00:02:20,630","00:02:23,530",32,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=140,Now if you think about it for
cs-410_8_1_33,cs-410,8,1,Syntagmatic,"00:02:24,530","00:02:27,910",33,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=144,the is easier to predict because
cs-410_8_1_34,cs-410,8,1,Syntagmatic,"00:02:27,910","00:02:30,770",34,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=147,"So I can just say,"
cs-410_8_1_35,cs-410,8,1,Syntagmatic,"00:02:31,940","00:02:37,946",35,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=151,Unicorn is also relatively easy
cs-410_8_1_36,cs-410,8,1,Syntagmatic,"00:02:37,946","00:02:41,470",36,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=157,And I can bet that it doesn't
cs-410_8_1_37,cs-410,8,1,Syntagmatic,"00:02:42,780","00:02:46,080",37,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=162,But meat is somewhere in
cs-410_8_1_38,cs-410,8,1,Syntagmatic,"00:02:46,080","00:02:50,580",38,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=166,And it makes it harder to predict because
cs-410_8_1_39,cs-410,8,1,Syntagmatic,"00:02:50,580","00:02:52,520",39,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=170,"or the segment, more accurately."
cs-410_8_1_40,cs-410,8,1,Syntagmatic,"00:02:53,842","00:02:58,820",40,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=173,"But it may also not occur in the sentence,"
cs-410_8_1_41,cs-410,8,1,Syntagmatic,"00:02:58,820","00:03:01,500",41,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=178,now let's study this
cs-410_8_1_42,cs-410,8,1,Syntagmatic,"00:03:02,680","00:03:06,090",42,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=182,So the problem can be formally defined
cs-410_8_1_43,cs-410,8,1,Syntagmatic,"00:03:06,090","00:03:10,030",43,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=186,as predicting the value of
cs-410_8_1_44,cs-410,8,1,Syntagmatic,"00:03:10,030","00:03:14,080",44,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=190,"Here we denote it by X sub w,"
cs-410_8_1_45,cs-410,8,1,Syntagmatic,"00:03:14,080","00:03:17,340",45,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=194,this random variable is associated
cs-410_8_1_46,cs-410,8,1,Syntagmatic,"00:03:18,380","00:03:23,020",46,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=198,"When the value of the variable is 1,"
cs-410_8_1_47,cs-410,8,1,Syntagmatic,"00:03:23,020","00:03:26,110",47,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=203,"When it's 0, it means the word is absent."
cs-410_8_1_48,cs-410,8,1,Syntagmatic,"00:03:26,110","00:03:31,010",48,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=206,"And naturally, the probabilities for"
cs-410_8_1_49,cs-410,8,1,Syntagmatic,"00:03:31,010","00:03:34,187",49,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=211,because a word is either present or
cs-410_8_1_50,cs-410,8,1,Syntagmatic,"00:03:35,240","00:03:36,070",50,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=215,There's no other choice.
cs-410_8_1_51,cs-410,8,1,Syntagmatic,"00:03:38,290","00:03:43,610",51,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=218,So the intuition with this concept earlier
cs-410_8_1_52,cs-410,8,1,Syntagmatic,"00:03:43,610","00:03:48,280",52,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=223,"The more random this random variable is,"
cs-410_8_1_53,cs-410,8,1,Syntagmatic,"00:03:49,710","00:03:53,600",53,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=229,Now the question is how does one
cs-410_8_1_54,cs-410,8,1,Syntagmatic,"00:03:53,600","00:03:55,590",54,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=233,a random variable like X sub w?
cs-410_8_1_55,cs-410,8,1,Syntagmatic,"00:03:56,940","00:04:01,850",55,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=236,"How in general, can we quantify"
cs-410_8_1_56,cs-410,8,1,Syntagmatic,"00:04:01,850","00:04:04,690",56,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=241,that's why we need a measure
cs-410_8_1_57,cs-410,8,1,Syntagmatic,"00:04:04,690","00:04:10,560",57,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=244,this measure introduced in information
cs-410_8_1_58,cs-410,8,1,Syntagmatic,"00:04:10,560","00:04:13,790",58,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=250,There is also some connection
cs-410_8_1_59,cs-410,8,1,Syntagmatic,"00:04:13,790","00:04:15,620",59,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=253,that is beyond the scope of this course.
cs-410_8_1_60,cs-410,8,1,Syntagmatic,"00:04:17,460","00:04:20,750",60,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=257,So for
cs-410_8_1_61,cs-410,8,1,Syntagmatic,"00:04:20,750","00:04:22,910",61,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=260,as a function defined
cs-410_8_1_62,cs-410,8,1,Syntagmatic,"00:04:22,910","00:04:27,000",62,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=262,"In this case, it is a binary random"
cs-410_8_1_63,cs-410,8,1,Syntagmatic,"00:04:27,000","00:04:30,930",63,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=267,be easily generalized for
cs-410_8_1_64,cs-410,8,1,Syntagmatic,"00:04:32,070","00:04:34,950",64,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=272,"Now the function form looks like this,"
cs-410_8_1_65,cs-410,8,1,Syntagmatic,"00:04:34,950","00:04:39,410",65,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=274,there's the sum of all the possible
cs-410_8_1_66,cs-410,8,1,Syntagmatic,"00:04:39,410","00:04:44,030",66,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=279,Inside the sum for each value we
cs-410_8_1_67,cs-410,8,1,Syntagmatic,"00:04:45,210","00:04:52,060",67,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=285,that the random variable equals this
cs-410_8_1_68,cs-410,8,1,Syntagmatic,"00:04:53,380","00:04:55,250",68,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=293,And note that there is also
cs-410_8_1_69,cs-410,8,1,Syntagmatic,"00:04:56,270","00:04:59,900",69,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=296,Now entropy in general is non-negative.
cs-410_8_1_70,cs-410,8,1,Syntagmatic,"00:04:59,900","00:05:01,480",70,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=299,And that can be mathematically proved.
cs-410_8_1_71,cs-410,8,1,Syntagmatic,"00:05:02,620","00:05:10,320",71,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=302,"So if we expand this sum, we'll see that"
cs-410_8_1_72,cs-410,8,1,Syntagmatic,"00:05:10,320","00:05:14,130",72,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=310,Where I explicitly plugged
cs-410_8_1_73,cs-410,8,1,Syntagmatic,"00:05:14,130","00:05:18,370",73,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=314,"And sometimes when we have 0 log of 0,"
cs-410_8_1_74,cs-410,8,1,Syntagmatic,"00:05:18,370","00:05:25,960",74,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=318,"we would generally define that as 0,"
cs-410_8_1_75,cs-410,8,1,Syntagmatic,"00:05:28,480","00:05:30,330",75,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=328,So this is the entropy function.
cs-410_8_1_76,cs-410,8,1,Syntagmatic,"00:05:30,330","00:05:33,020",76,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=330,And this function will
cs-410_8_1_77,cs-410,8,1,Syntagmatic,"00:05:33,020","00:05:35,520",77,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=333,different distributions
cs-410_8_1_78,cs-410,8,1,Syntagmatic,"00:05:37,260","00:05:40,650",78,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=337,And it clearly depends on the probability
cs-410_8_1_79,cs-410,8,1,Syntagmatic,"00:05:40,650","00:05:43,850",79,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=340,that the random variable
cs-410_8_1_80,cs-410,8,1,Syntagmatic,"00:05:43,850","00:05:49,780",80,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=343,If we plot this function against
cs-410_8_1_81,cs-410,8,1,Syntagmatic,"00:05:49,780","00:05:55,114",81,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=349,the probability that the random
cs-410_8_1_82,cs-410,8,1,Syntagmatic,"00:05:56,990","00:05:59,080",82,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=356,And then the function looks like this.
cs-410_8_1_83,cs-410,8,1,Syntagmatic,"00:06:01,310","00:06:06,820",83,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=361,"At the two ends,"
cs-410_8_1_84,cs-410,8,1,Syntagmatic,"00:06:07,950","00:06:13,698",84,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=367,"equals 1 is very small or very large,"
cs-410_8_1_85,cs-410,8,1,Syntagmatic,"00:06:13,698","00:06:18,280",85,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=373,When it's 0.5 in the middle
cs-410_8_1_86,cs-410,8,1,Syntagmatic,"00:06:20,180","00:06:24,150",86,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=380,Now if we plot the function
cs-410_8_1_87,cs-410,8,1,Syntagmatic,"00:06:25,950","00:06:31,090",87,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=385,is taking a value of 0 and the function
cs-410_8_1_88,cs-410,8,1,Syntagmatic,"00:06:31,090","00:06:37,810",88,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=391,"would show exactly the same curve here,"
cs-410_8_1_89,cs-410,8,1,Syntagmatic,"00:06:37,810","00:06:40,620",89,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=397,And so that's because
cs-410_8_1_90,cs-410,8,1,Syntagmatic,"00:06:42,340","00:06:46,730",90,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=402,"the two probabilities are symmetric,"
cs-410_8_1_91,cs-410,8,1,Syntagmatic,"00:06:48,740","00:06:52,850",91,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=408,So an interesting question you
cs-410_8_1_92,cs-410,8,1,Syntagmatic,"00:06:52,850","00:06:59,390",92,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=412,what kind of X does entropy
cs-410_8_1_93,cs-410,8,1,Syntagmatic,"00:06:59,390","00:07:02,960",93,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=419,And we can in particular think
cs-410_8_1_94,cs-410,8,1,Syntagmatic,"00:07:02,960","00:07:07,700",94,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=422,"For example, in one case,"
cs-410_8_1_95,cs-410,8,1,Syntagmatic,"00:07:08,840","00:07:10,600",95,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=428,always takes a value of 1.
cs-410_8_1_96,cs-410,8,1,Syntagmatic,"00:07:10,600","00:07:14,304",96,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=430,The probability is 1.
cs-410_8_1_97,cs-410,8,1,Syntagmatic,"00:07:16,390","00:07:18,650",97,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=436,Or there's a random variable that
cs-410_8_1_98,cs-410,8,1,Syntagmatic,"00:07:19,890","00:07:24,320",98,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=439,is equally likely taking a value of one or
cs-410_8_1_99,cs-410,8,1,Syntagmatic,"00:07:24,320","00:07:28,750",99,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=444,So in this case the probability
cs-410_8_1_100,cs-410,8,1,Syntagmatic,"00:07:30,700","00:07:32,250",100,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=450,Now which one has a higher entropy?
cs-410_8_1_101,cs-410,8,1,Syntagmatic,"00:07:34,650","00:07:38,530",101,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=454,It's easier to look at the problem
cs-410_8_1_102,cs-410,8,1,Syntagmatic,"00:07:40,800","00:07:42,380",102,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=460,using coin tossing.
cs-410_8_1_103,cs-410,8,1,Syntagmatic,"00:07:43,420","00:07:47,660",103,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=463,So when we think about random
cs-410_8_1_104,cs-410,8,1,Syntagmatic,"00:07:48,770","00:07:55,740",104,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=468,"it gives us a random variable,"
cs-410_8_1_105,cs-410,8,1,Syntagmatic,"00:07:55,740","00:07:57,860",105,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=475,It can be head or tail.
cs-410_8_1_106,cs-410,8,1,Syntagmatic,"00:07:57,860","00:08:03,040",106,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=477,So we can define a random variable
cs-410_8_1_107,cs-410,8,1,Syntagmatic,"00:08:03,040","00:08:08,470",107,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=483,"when the coin shows up as head,"
cs-410_8_1_108,cs-410,8,1,Syntagmatic,"00:08:09,800","00:08:15,390",108,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=489,So now we can compute the entropy
cs-410_8_1_109,cs-410,8,1,Syntagmatic,"00:08:15,390","00:08:20,050",109,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=495,And this entropy indicates how
cs-410_8_1_110,cs-410,8,1,Syntagmatic,"00:08:22,050","00:08:22,890",110,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=502,of a coin toss.
cs-410_8_1_111,cs-410,8,1,Syntagmatic,"00:08:25,440","00:08:27,530",111,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=505,So we can think about the two cases.
cs-410_8_1_112,cs-410,8,1,Syntagmatic,"00:08:27,530","00:08:29,590",112,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=507,"One is a fair coin, it's completely fair."
cs-410_8_1_113,cs-410,8,1,Syntagmatic,"00:08:29,590","00:08:34,160",113,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=509,The coin shows up as head or
cs-410_8_1_114,cs-410,8,1,Syntagmatic,"00:08:34,160","00:08:39,160",114,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=514,So the two probabilities would be a half.
cs-410_8_1_115,cs-410,8,1,Syntagmatic,"00:08:39,160","00:08:42,890",115,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=519,Right?
cs-410_8_1_116,cs-410,8,1,Syntagmatic,"00:08:44,680","00:08:47,620",116,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=524,Another extreme case is
cs-410_8_1_117,cs-410,8,1,Syntagmatic,"00:08:47,620","00:08:50,420",117,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=527,where the coin always shows up as heads.
cs-410_8_1_118,cs-410,8,1,Syntagmatic,"00:08:50,420","00:08:52,760",118,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=530,So it's a completely biased coin.
cs-410_8_1_119,cs-410,8,1,Syntagmatic,"00:08:54,670","00:08:57,910",119,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=534,Now let's think about
cs-410_8_1_120,cs-410,8,1,Syntagmatic,"00:08:57,910","00:09:04,850",120,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=537,And if you plug in these values you can
cs-410_8_1_121,cs-410,8,1,Syntagmatic,"00:09:04,850","00:09:09,524",121,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=544,For a fair coin we see the entropy
cs-410_8_1_122,cs-410,8,1,Syntagmatic,"00:09:11,270","00:09:14,460",122,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=551,"For the completely biased coin,"
cs-410_8_1_123,cs-410,8,1,Syntagmatic,"00:09:14,460","00:09:17,360",123,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=554,And that intuitively makes a lot of sense.
cs-410_8_1_124,cs-410,8,1,Syntagmatic,"00:09:17,360","00:09:20,490",124,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=557,Because a fair coin is
cs-410_8_1_125,cs-410,8,1,Syntagmatic,"00:09:22,080","00:09:24,950",125,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=562,Whereas a completely biased
cs-410_8_1_126,cs-410,8,1,Syntagmatic,"00:09:24,950","00:09:26,860",126,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=564,"We can always say, well, it's a head."
cs-410_8_1_127,cs-410,8,1,Syntagmatic,"00:09:26,860","00:09:29,190",127,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=566,Because it is a head all the time.
cs-410_8_1_128,cs-410,8,1,Syntagmatic,"00:09:29,190","00:09:34,400",128,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=569,So they can be shown on
cs-410_8_1_129,cs-410,8,1,Syntagmatic,"00:09:34,400","00:09:40,300",129,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=574,So the fair coin corresponds to the middle
cs-410_8_1_130,cs-410,8,1,Syntagmatic,"00:09:40,300","00:09:45,410",130,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=580,The completely biased coin
cs-410_8_1_131,cs-410,8,1,Syntagmatic,"00:09:45,410","00:09:48,058",131,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=585,point where we have a probability
cs-410_8_1_132,cs-410,8,1,Syntagmatic,"00:09:48,058","00:09:54,870",132,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=588,"So, now let's see how we can use"
cs-410_8_1_133,cs-410,8,1,Syntagmatic,"00:09:54,870","00:09:59,670",133,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=594,Let's think about our problem is
cs-410_8_1_134,cs-410,8,1,Syntagmatic,"00:09:59,670","00:10:01,650",134,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=599,absent in this segment.
cs-410_8_1_135,cs-410,8,1,Syntagmatic,"00:10:01,650","00:10:05,300",135,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=601,"Again, think about the three words,"
cs-410_8_1_136,cs-410,8,1,Syntagmatic,"00:10:06,540","00:10:10,130",136,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=606,Now we can assume high entropy
cs-410_8_1_137,cs-410,8,1,Syntagmatic,"00:10:11,910","00:10:18,790",137,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=611,And so we now have a quantitative way to
cs-410_8_1_138,cs-410,8,1,Syntagmatic,"00:10:20,890","00:10:25,810",138,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=620,"Now if you look at the three words meat,"
cs-410_8_1_139,cs-410,8,1,Syntagmatic,"00:10:25,810","00:10:33,310",139,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=625,we clearly would expect meat to have
cs-410_8_1_140,cs-410,8,1,Syntagmatic,"00:10:33,310","00:10:39,180",140,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=633,"In fact if you look at the entropy of the,"
cs-410_8_1_141,cs-410,8,1,Syntagmatic,"00:10:39,180","00:10:41,570",141,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=639,Because it occurs everywhere.
cs-410_8_1_142,cs-410,8,1,Syntagmatic,"00:10:41,570","00:10:43,390",142,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=641,So it's like a completely biased coin.
cs-410_8_1_143,cs-410,8,1,Syntagmatic,"00:10:44,610","00:10:46,380",143,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=644,Therefore the entropy is zero.
cs-410_8_1_144,cs-410,8,1,Syntagmatic,"00:10:48,710","00:10:58,710",144,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=648,[MUSIC]
cs-410_8_2_1,cs-410,8,2,Syntagmatic,"00:00:00,025","00:00:05,819",1,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=0,[SOUND] This lecture is
cs-410_8_2_2,cs-410,8,2,Syntagmatic,"00:00:05,819","00:00:12,090",2,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=5,relation discovery and
cs-410_8_2_3,cs-410,8,2,Syntagmatic,"00:00:12,090","00:00:12,963",3,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=12,"In this lecture,"
cs-410_8_2_4,cs-410,8,2,Syntagmatic,"00:00:12,963","00:00:16,939",4,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=12,we're going to continue the discussion
cs-410_8_2_5,cs-410,8,2,Syntagmatic,"00:00:18,060","00:00:22,930",5,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=18,We're going to talk about the conditional
cs-410_8_2_6,cs-410,8,2,Syntagmatic,"00:00:22,930","00:00:25,700",6,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=22,discovering syntagmatic relations.
cs-410_8_2_7,cs-410,8,2,Syntagmatic,"00:00:25,700","00:00:29,400",7,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=25,"Earlier, we talked about"
cs-410_8_2_8,cs-410,8,2,Syntagmatic,"00:00:29,400","00:00:33,030",8,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=29,how easy it is to predict the presence or
cs-410_8_2_9,cs-410,8,2,Syntagmatic,"00:00:34,180","00:00:37,700",9,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=34,"Now, we'll address"
cs-410_8_2_10,cs-410,8,2,Syntagmatic,"00:00:37,700","00:00:41,320",10,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=37,we assume that we know something
cs-410_8_2_11,cs-410,8,2,Syntagmatic,"00:00:41,320","00:00:48,830",11,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=41,"So now the question is, suppose we know"
cs-410_8_2_12,cs-410,8,2,Syntagmatic,"00:00:48,830","00:00:51,150",12,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=48,How would that help us
cs-410_8_2_13,cs-410,8,2,Syntagmatic,"00:00:51,150","00:00:53,990",13,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=51,"absence of water, like in meat?"
cs-410_8_2_14,cs-410,8,2,Syntagmatic,"00:00:53,990","00:00:58,060",14,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=53,"And in particular, we want to"
cs-410_8_2_15,cs-410,8,2,Syntagmatic,"00:00:58,060","00:01:00,959",15,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=58,has helped us predict
cs-410_8_2_16,cs-410,8,2,Syntagmatic,"00:01:02,020","00:01:05,040",16,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=62,"And if we frame this using entrophy,"
cs-410_8_2_17,cs-410,8,2,Syntagmatic,"00:01:05,040","00:01:10,700",17,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=65,that would mean we are interested
cs-410_8_2_18,cs-410,8,2,Syntagmatic,"00:01:10,700","00:01:15,100",18,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=70,the presence of eats could reduce
cs-410_8_2_19,cs-410,8,2,Syntagmatic,"00:01:15,100","00:01:18,800",19,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=75,"Or, reduce the entrophy"
cs-410_8_2_20,cs-410,8,2,Syntagmatic,"00:01:18,800","00:01:23,430",20,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=78,corresponding to the presence or
cs-410_8_2_21,cs-410,8,2,Syntagmatic,"00:01:23,430","00:01:27,950",21,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=83,"We can also ask as a question,"
cs-410_8_2_22,cs-410,8,2,Syntagmatic,"00:01:28,950","00:01:33,010",22,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=88,Would that also help us predict
cs-410_8_2_23,cs-410,8,2,Syntagmatic,"00:01:34,720","00:01:39,415",23,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=94,These questions can be
cs-410_8_2_24,cs-410,8,2,Syntagmatic,"00:01:39,415","00:01:43,120",24,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=99,concept called a conditioning entropy.
cs-410_8_2_25,cs-410,8,2,Syntagmatic,"00:01:43,120","00:01:48,460",25,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=103,"So to explain this concept, let's first"
cs-410_8_2_26,cs-410,8,2,Syntagmatic,"00:01:48,460","00:01:51,218",26,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=108,when we know nothing about the segment.
cs-410_8_2_27,cs-410,8,2,Syntagmatic,"00:01:51,218","00:01:56,522",27,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=111,So we have these probabilities indicating
cs-410_8_2_28,cs-410,8,2,Syntagmatic,"00:01:56,522","00:01:58,830",28,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=116,or it doesn't occur in the segment.
cs-410_8_2_29,cs-410,8,2,Syntagmatic,"00:01:58,830","00:02:02,650",29,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=118,And we have an entropy function that
cs-410_8_2_30,cs-410,8,2,Syntagmatic,"00:02:03,810","00:02:07,410",30,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=123,"Now suppose we know eats is present, so"
cs-410_8_2_31,cs-410,8,2,Syntagmatic,"00:02:07,410","00:02:11,330",31,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=127,now we know the value of another
cs-410_8_2_32,cs-410,8,2,Syntagmatic,"00:02:12,730","00:02:15,270",32,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=132,"Now, that would change all"
cs-410_8_2_33,cs-410,8,2,Syntagmatic,"00:02:15,270","00:02:17,550",33,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=135,conditional probabilities.
cs-410_8_2_34,cs-410,8,2,Syntagmatic,"00:02:17,550","00:02:20,580",34,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=137,Where we look at the presence or
cs-410_8_2_35,cs-410,8,2,Syntagmatic,"00:02:21,800","00:02:25,570",35,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=141,given that we know eats
cs-410_8_2_36,cs-410,8,2,Syntagmatic,"00:02:25,570","00:02:27,480",36,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=145,"So as a result,"
cs-410_8_2_37,cs-410,8,2,Syntagmatic,"00:02:27,480","00:02:31,820",37,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=147,if we replace these probabilities
cs-410_8_2_38,cs-410,8,2,Syntagmatic,"00:02:31,820","00:02:36,320",38,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=151,"probabilities in the entropy function,"
cs-410_8_2_39,cs-410,8,2,Syntagmatic,"00:02:37,650","00:02:42,522",39,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=157,So this equation now here would be
cs-410_8_2_40,cs-410,8,2,Syntagmatic,"00:02:42,522","00:02:46,900",40,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=162,the conditional entropy.
cs-410_8_2_41,cs-410,8,2,Syntagmatic,"00:02:46,900","00:02:49,150",41,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=166,Conditional on the presence of eats.
cs-410_8_2_42,cs-410,8,2,Syntagmatic,"00:02:52,180","00:02:57,070",42,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=172,"So, you can see this is essentially"
cs-410_8_2_43,cs-410,8,2,Syntagmatic,"00:02:57,070","00:03:01,900",43,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=177,"seen before, except that all"
cs-410_8_2_44,cs-410,8,2,Syntagmatic,"00:03:04,420","00:03:09,550",44,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=184,And this then tells us
cs-410_8_2_45,cs-410,8,2,Syntagmatic,"00:03:09,550","00:03:13,020",45,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=189,after we have known eats
cs-410_8_2_46,cs-410,8,2,Syntagmatic,"00:03:14,380","00:03:17,770",46,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=194,"And of course, we can also define"
cs-410_8_2_47,cs-410,8,2,Syntagmatic,"00:03:17,770","00:03:20,540",47,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=197,the scenario where we don't see eats.
cs-410_8_2_48,cs-410,8,2,Syntagmatic,"00:03:20,540","00:03:25,150",48,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=200,So if we know it did not occur in
cs-410_8_2_49,cs-410,8,2,Syntagmatic,"00:03:25,150","00:03:30,710",49,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=205,entropy would capture the instances
cs-410_8_2_50,cs-410,8,2,Syntagmatic,"00:03:30,710","00:03:34,110",50,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=210,"So now,"
cs-410_8_2_51,cs-410,8,2,Syntagmatic,"00:03:34,110","00:03:37,609",51,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=214,we have the completed definition
cs-410_8_2_52,cs-410,8,2,Syntagmatic,"00:03:39,250","00:03:48,520",52,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=219,"Basically, we're going to consider both"
cs-410_8_2_53,cs-410,8,2,Syntagmatic,"00:03:48,520","00:03:54,280",53,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=228,and this gives us a probability
cs-410_8_2_54,cs-410,8,2,Syntagmatic,"00:03:54,280","00:03:58,040",54,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=234,"Basically, whether eats is present or"
cs-410_8_2_55,cs-410,8,2,Syntagmatic,"00:03:58,040","00:03:59,150",55,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=238,"And this of course,"
cs-410_8_2_56,cs-410,8,2,Syntagmatic,"00:03:59,150","00:04:04,310",56,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=239,is the conditional entropy of
cs-410_8_2_57,cs-410,8,2,Syntagmatic,"00:04:05,510","00:04:10,110",57,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=245,"So if you expanded this entropy,"
cs-410_8_2_58,cs-410,8,2,Syntagmatic,"00:04:10,110","00:04:14,330",58,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=250,then you have the following equation.
cs-410_8_2_59,cs-410,8,2,Syntagmatic,"00:04:15,760","00:04:19,429",59,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=255,Where you see the involvement of
cs-410_8_2_60,cs-410,8,2,Syntagmatic,"00:04:21,530","00:04:26,330",60,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=261,"Now in general, for any discrete"
cs-410_8_2_61,cs-410,8,2,Syntagmatic,"00:04:27,940","00:04:35,240",61,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=267,the conditional entropy is no larger
cs-410_8_2_62,cs-410,8,2,Syntagmatic,"00:04:35,240","00:04:41,950",62,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=275,"So basically, this is upper bound for"
cs-410_8_2_63,cs-410,8,2,Syntagmatic,"00:04:41,950","00:04:46,380",63,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=281,That means by knowing more
cs-410_8_2_64,cs-410,8,2,Syntagmatic,"00:04:46,380","00:04:49,630",64,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=286,we want to be able to
cs-410_8_2_65,cs-410,8,2,Syntagmatic,"00:04:49,630","00:04:51,570",65,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=289,We can only reduce uncertainty.
cs-410_8_2_66,cs-410,8,2,Syntagmatic,"00:04:51,570","00:04:56,180",66,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=291,And that intuitively makes sense
cs-410_8_2_67,cs-410,8,2,Syntagmatic,"00:04:56,180","00:05:00,180",67,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=296,it should always help
cs-410_8_2_68,cs-410,8,2,Syntagmatic,"00:05:00,180","00:05:04,000",68,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=300,And cannot hurt
cs-410_8_2_69,cs-410,8,2,Syntagmatic,"00:05:05,420","00:05:08,880",69,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=305,"Now, what's interesting here is also to"
cs-410_8_2_70,cs-410,8,2,Syntagmatic,"00:05:08,880","00:05:11,770",70,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=308,value of this conditional entropy?
cs-410_8_2_71,cs-410,8,2,Syntagmatic,"00:05:11,770","00:05:16,270",71,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=311,"Now, we know that the maximum"
cs-410_8_2_72,cs-410,8,2,Syntagmatic,"00:05:17,940","00:05:20,313",72,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=317,"But what about the minimum,"
cs-410_8_2_73,cs-410,8,2,Syntagmatic,"00:05:22,883","00:05:28,552",73,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=322,I hope you can reach the conclusion that
cs-410_8_2_74,cs-410,8,2,Syntagmatic,"00:05:28,552","00:05:33,090",74,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=328,And it will be interesting to think about
cs-410_8_2_75,cs-410,8,2,Syntagmatic,"00:05:34,120","00:05:37,860",75,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=334,"So, let's see how we can use conditional"
cs-410_8_2_76,cs-410,8,2,Syntagmatic,"00:05:39,420","00:05:44,250",76,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=339,"Now of course,"
cs-410_8_2_77,cs-410,8,2,Syntagmatic,"00:05:44,250","00:05:48,300",77,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=344,one way to measure
cs-410_8_2_78,cs-410,8,2,Syntagmatic,"00:05:48,300","00:05:53,750",78,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=348,"Because it tells us to what extent,"
cs-410_8_2_79,cs-410,8,2,Syntagmatic,"00:05:53,750","00:05:58,995",79,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=353,word given that we know the presence or
cs-410_8_2_80,cs-410,8,2,Syntagmatic,"00:05:58,995","00:06:03,900",80,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=358,Now before we look at the intuition
cs-410_8_2_81,cs-410,8,2,Syntagmatic,"00:06:03,900","00:06:09,090",81,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=363,"syntagmatic relations, it's useful to"
cs-410_8_2_82,cs-410,8,2,Syntagmatic,"00:06:09,090","00:06:17,910",82,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=369,"That is, the conditional entropy"
cs-410_8_2_83,cs-410,8,2,Syntagmatic,"00:06:19,000","00:06:22,980",83,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=379,"So here,"
cs-410_8_2_84,cs-410,8,2,Syntagmatic,"00:06:22,980","00:06:28,420",84,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=382,we listed this conditional
cs-410_8_2_85,cs-410,8,2,Syntagmatic,"00:06:28,420","00:06:31,280",85,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=388,"So, it's here."
cs-410_8_2_86,cs-410,8,2,Syntagmatic,"00:06:33,550","00:06:35,100",86,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=393,"So, what is the value of this?"
cs-410_8_2_87,cs-410,8,2,Syntagmatic,"00:06:36,380","00:06:43,370",87,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=396,"Now, this means we know where"
cs-410_8_2_88,cs-410,8,2,Syntagmatic,"00:06:43,370","00:06:47,717",88,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=403,And we hope to predict whether
cs-410_8_2_89,cs-410,8,2,Syntagmatic,"00:06:47,717","00:06:52,518",89,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=407,"And of course, this is 0 because"
cs-410_8_2_90,cs-410,8,2,Syntagmatic,"00:06:52,518","00:06:55,862",90,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=412,Once we know whether the word
cs-410_8_2_91,cs-410,8,2,Syntagmatic,"00:06:55,862","00:06:59,132",91,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=415,we'll already know the answer
cs-410_8_2_92,cs-410,8,2,Syntagmatic,"00:06:59,132","00:07:00,410",92,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=419,So this is zero.
cs-410_8_2_93,cs-410,8,2,Syntagmatic,"00:07:00,410","00:07:03,390",93,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=420,And that's also when this conditional
cs-410_8_2_94,cs-410,8,2,Syntagmatic,"00:07:06,280","00:07:08,280",94,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=426,"So now, let's look at some other cases."
cs-410_8_2_95,cs-410,8,2,Syntagmatic,"00:07:09,530","00:07:15,840",95,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=429,So this is a case of knowing the and
cs-410_8_2_96,cs-410,8,2,Syntagmatic,"00:07:15,840","00:07:20,840",96,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=435,And this is a case of knowing eats and
cs-410_8_2_97,cs-410,8,2,Syntagmatic,"00:07:20,840","00:07:22,870",97,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=440,Which one do you think is smaller?
cs-410_8_2_98,cs-410,8,2,Syntagmatic,"00:07:22,870","00:07:27,763",98,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=442,No doubt smaller entropy means easier for
cs-410_8_2_99,cs-410,8,2,Syntagmatic,"00:07:31,511","00:07:33,260",99,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=451,Which one do you think is higher?
cs-410_8_2_100,cs-410,8,2,Syntagmatic,"00:07:33,260","00:07:34,820",100,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=453,Which one is not smaller?
cs-410_8_2_101,cs-410,8,2,Syntagmatic,"00:07:36,800","00:07:41,732",101,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=456,"Well, if you at the uncertainty,"
cs-410_8_2_102,cs-410,8,2,Syntagmatic,"00:07:41,732","00:07:45,730",102,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=461,the doesn't really tell
cs-410_8_2_103,cs-410,8,2,Syntagmatic,"00:07:45,730","00:07:51,520",103,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=465,So knowing the occurrence of the doesn't
cs-410_8_2_104,cs-410,8,2,Syntagmatic,"00:07:51,520","00:07:56,465",104,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=471,So it stays fairly close to
cs-410_8_2_105,cs-410,8,2,Syntagmatic,"00:07:56,465","00:08:01,120",105,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=476,"Whereas in the case of eats,"
cs-410_8_2_106,cs-410,8,2,Syntagmatic,"00:08:01,120","00:08:04,420",106,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=481,So knowing presence of eats or
cs-410_8_2_107,cs-410,8,2,Syntagmatic,"00:08:04,420","00:08:07,780",107,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=484,would help us predict whether meat occurs.
cs-410_8_2_108,cs-410,8,2,Syntagmatic,"00:08:07,780","00:08:14,290",108,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=487,So it can help us reduce entropy of meat.
cs-410_8_2_109,cs-410,8,2,Syntagmatic,"00:08:14,290","00:08:20,470",109,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=494,"So we should expect the sigma term, namely"
cs-410_8_2_110,cs-410,8,2,Syntagmatic,"00:08:21,630","00:08:25,870",110,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=501,And that means there is a stronger
cs-410_8_2_111,cs-410,8,2,Syntagmatic,"00:08:29,070","00:08:36,360",111,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=509,So we now also know when
cs-410_8_2_112,cs-410,8,2,Syntagmatic,"00:08:36,360","00:08:41,400",112,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=516,"meat, then the conditional entropy"
cs-410_8_2_113,cs-410,8,2,Syntagmatic,"00:08:41,400","00:08:45,300",113,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=521,And for what kind of words
cs-410_8_2_114,cs-410,8,2,Syntagmatic,"00:08:45,300","00:08:49,885",114,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=525,"Well, that's when this stuff"
cs-410_8_2_115,cs-410,8,2,Syntagmatic,"00:08:49,885","00:08:55,339",115,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=529,"And like the for example,"
cs-410_8_2_116,cs-410,8,2,Syntagmatic,"00:08:55,339","00:08:58,480",116,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=535,which is the entropy of meat itself.
cs-410_8_2_117,cs-410,8,2,Syntagmatic,"00:08:59,970","00:09:03,050",117,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=539,So this suggests that when you
cs-410_8_2_118,cs-410,8,2,Syntagmatic,"00:09:03,050","00:09:07,710",118,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=543,"mining syntagmatic relations,"
cs-410_8_2_119,cs-410,8,2,Syntagmatic,"00:09:10,140","00:09:14,780",119,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=550,"For each word W1, we're going to"
cs-410_8_2_120,cs-410,8,2,Syntagmatic,"00:09:14,780","00:09:21,020",120,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=554,"And then, we can compute"
cs-410_8_2_121,cs-410,8,2,Syntagmatic,"00:09:22,170","00:09:26,630",121,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=562,We thought all the candidate was in
cs-410_8_2_122,cs-410,8,2,Syntagmatic,"00:09:26,630","00:09:30,090",122,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=566,"because we're out of favor,"
cs-410_8_2_123,cs-410,8,2,Syntagmatic,"00:09:30,090","00:09:34,637",123,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=570,Meaning that it helps us predict
cs-410_8_2_124,cs-410,8,2,Syntagmatic,"00:09:34,637","00:09:38,378",124,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=574,"And then, we're going to take the top ring"
cs-410_8_2_125,cs-410,8,2,Syntagmatic,"00:09:38,378","00:09:40,480",125,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=578,potential syntagmatic relations with W1.
cs-410_8_2_126,cs-410,8,2,Syntagmatic,"00:09:41,910","00:09:47,700",126,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=581,Note that we need to use
cs-410_8_2_127,cs-410,8,2,Syntagmatic,"00:09:47,700","00:09:51,474",127,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=587,The stresser can be the number
cs-410_8_2_128,cs-410,8,2,Syntagmatic,"00:09:51,474","00:09:54,550",128,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=591,absolute value for
cs-410_8_2_129,cs-410,8,2,Syntagmatic,"00:09:55,900","00:10:00,110",129,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=595,"Now, this would allow us to mine the most"
cs-410_8_2_130,cs-410,8,2,Syntagmatic,"00:10:00,110","00:10:03,700",130,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=600,strongly correlated words with
cs-410_8_2_131,cs-410,8,2,Syntagmatic,"00:10:06,380","00:10:10,560",131,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=606,"But, this algorithm does not"
cs-410_8_2_132,cs-410,8,2,Syntagmatic,"00:10:10,560","00:10:14,800",132,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=610,that K syntagmatical relations
cs-410_8_2_133,cs-410,8,2,Syntagmatic,"00:10:14,800","00:10:19,370",133,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=614,"Because in order to do that, we have to"
cs-410_8_2_134,cs-410,8,2,Syntagmatic,"00:10:19,370","00:10:24,010",134,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=619,are comparable across different words.
cs-410_8_2_135,cs-410,8,2,Syntagmatic,"00:10:24,010","00:10:28,470",135,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=624,In this case of discovering
cs-410_8_2_136,cs-410,8,2,Syntagmatic,"00:10:28,470","00:10:33,520",136,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=628,"a targeted word like W1, we only need"
cs-410_8_2_137,cs-410,8,2,Syntagmatic,"00:10:34,980","00:10:38,600",137,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=634,"for W1, given different words."
cs-410_8_2_138,cs-410,8,2,Syntagmatic,"00:10:38,600","00:10:40,780",138,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=638,"And in this case, they are comparable."
cs-410_8_2_139,cs-410,8,2,Syntagmatic,"00:10:41,860","00:10:43,690",139,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=641,All right.
cs-410_8_2_140,cs-410,8,2,Syntagmatic,"00:10:43,690","00:10:48,040",140,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=643,"So, the conditional entropy of W1, given"
cs-410_8_2_141,cs-410,8,2,Syntagmatic,"00:10:48,040","00:10:49,770",141,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=648,given W3 are comparable.
cs-410_8_2_142,cs-410,8,2,Syntagmatic,"00:10:51,100","00:10:55,490",142,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=651,They all measure how hard
cs-410_8_2_143,cs-410,8,2,Syntagmatic,"00:10:55,490","00:11:00,070",143,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=655,"But, if we think about the two pairs,"
cs-410_8_2_144,cs-410,8,2,Syntagmatic,"00:11:00,070","00:11:06,370",144,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=660,"where we share W2 in the same condition,"
cs-410_8_2_145,cs-410,8,2,Syntagmatic,"00:11:06,370","00:11:11,296",145,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=666,"Then, the conditional entropies"
cs-410_8_2_146,cs-410,8,2,Syntagmatic,"00:11:11,296","00:11:15,925",146,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=671,You can think of about this question.
cs-410_8_2_147,cs-410,8,2,Syntagmatic,"00:11:15,925","00:11:17,022",147,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=675,Why?
cs-410_8_2_148,cs-410,8,2,Syntagmatic,"00:11:17,022","00:11:19,870",148,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=677,So why are they not comfortable?
cs-410_8_2_149,cs-410,8,2,Syntagmatic,"00:11:19,870","00:11:23,210",149,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=679,"Well, that was because they"
cs-410_8_2_150,cs-410,8,2,Syntagmatic,"00:11:23,210","00:11:25,690",150,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=683,Right?
cs-410_8_2_151,cs-410,8,2,Syntagmatic,"00:11:25,690","00:11:29,230",151,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=685,the entropy of W1 and the entropy of W3.
cs-410_8_2_152,cs-410,8,2,Syntagmatic,"00:11:29,230","00:11:31,150",152,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=689,And they have different upper bounds.
cs-410_8_2_153,cs-410,8,2,Syntagmatic,"00:11:31,150","00:11:35,000",153,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=691,So we cannot really
cs-410_8_2_154,cs-410,8,2,Syntagmatic,"00:11:35,000","00:11:36,420",154,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=695,So how do we address this problem?
cs-410_8_2_155,cs-410,8,2,Syntagmatic,"00:11:38,000","00:11:45,219",155,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=698,"Well later, we'll discuss, we can use"
cs-410_8_2_156,cs-410,8,2,Syntagmatic,"00:11:45,219","00:11:55,219",156,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=705,[MUSIC]
cs-410_8_3_1,cs-410,8,3,Syntagmatic,"00:00:00,025","00:00:07,457",1,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=0,[SOUND].
cs-410_8_3_2,cs-410,8,3,Syntagmatic,"00:00:07,457","00:00:11,800",2,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=7,This lecture is about the syntagmatic
cs-410_8_3_3,cs-410,8,3,Syntagmatic,"00:00:13,400","00:00:18,196",3,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=13,In this lecture we are going to continue
cs-410_8_3_4,cs-410,8,3,Syntagmatic,"00:00:18,196","00:00:20,850",4,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=18,"In particular,"
cs-410_8_3_5,cs-410,8,3,Syntagmatic,"00:00:20,850","00:00:24,880",5,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=20,"the concept in the information series,"
cs-410_8_3_6,cs-410,8,3,Syntagmatic,"00:00:24,880","00:00:28,760",6,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=24,how it can be used to discover
cs-410_8_3_7,cs-410,8,3,Syntagmatic,"00:00:28,760","00:00:32,880",7,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=28,Before we talked about the problem
cs-410_8_3_8,cs-410,8,3,Syntagmatic,"00:00:32,880","00:00:38,014",8,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=32,that is the conditional entropy
cs-410_8_3_9,cs-410,8,3,Syntagmatic,"00:00:38,014","00:00:42,600",9,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=38,"It is not really comparable, so"
cs-410_8_3_10,cs-410,8,3,Syntagmatic,"00:00:42,600","00:00:48,360",10,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=42,strong synagmatic relations
cs-410_8_3_11,cs-410,8,3,Syntagmatic,"00:00:48,360","00:00:53,050",11,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=48,So now we are going to introduce mutual
cs-410_8_3_12,cs-410,8,3,Syntagmatic,"00:00:53,050","00:00:57,370",12,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=53,in the information series
cs-410_8_3_13,cs-410,8,3,Syntagmatic,"00:00:57,370","00:01:03,460",13,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=57,normalize the conditional entropy to make
cs-410_8_3_14,cs-410,8,3,Syntagmatic,"00:01:04,930","00:01:10,090",14,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=64,"In particular, mutual information"
cs-410_8_3_15,cs-410,8,3,Syntagmatic,"00:01:10,090","00:01:17,380",15,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=70,matches the entropy reduction
cs-410_8_3_16,cs-410,8,3,Syntagmatic,"00:01:17,380","00:01:22,270",16,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=77,More specifically the question we
cs-410_8_3_17,cs-410,8,3,Syntagmatic,"00:01:22,270","00:01:25,463",17,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=82,of an entropy of X can
cs-410_8_3_18,cs-410,8,3,Syntagmatic,"00:01:27,220","00:01:31,940",18,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=87,So mathematically it can be
cs-410_8_3_19,cs-410,8,3,Syntagmatic,"00:01:31,940","00:01:36,670",19,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=91,"the original entropy of X, and"
cs-410_8_3_20,cs-410,8,3,Syntagmatic,"00:01:37,970","00:01:42,730",20,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=97,"And you might see,"
cs-410_8_3_21,cs-410,8,3,Syntagmatic,"00:01:42,730","00:01:47,790",21,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=102,as reduction of entropy of
cs-410_8_3_22,cs-410,8,3,Syntagmatic,"00:01:48,930","00:01:54,070",22,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=108,Now normally the two conditional
cs-410_8_3_23,cs-410,8,3,Syntagmatic,"00:01:54,070","00:01:58,240",23,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=114,"the entropy of Y given X are not equal,"
cs-410_8_3_24,cs-410,8,3,Syntagmatic,"00:01:58,240","00:02:05,476",24,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=118,the reduction of entropy by knowing
cs-410_8_3_25,cs-410,8,3,Syntagmatic,"00:02:05,476","00:02:12,805",25,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=125,"So, this quantity is called a Mutual"
cs-410_8_3_26,cs-410,8,3,Syntagmatic,"00:02:12,805","00:02:17,085",26,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=132,And this function has some interesting
cs-410_8_3_27,cs-410,8,3,Syntagmatic,"00:02:17,085","00:02:21,415",27,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=137,This is easy to understand because
cs-410_8_3_28,cs-410,8,3,Syntagmatic,"00:02:22,782","00:02:29,132",28,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=142,not going to be lower than the possibility
cs-410_8_3_29,cs-410,8,3,Syntagmatic,"00:02:29,132","00:02:33,512",29,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=149,"In other words, the conditional entropy"
cs-410_8_3_30,cs-410,8,3,Syntagmatic,"00:02:33,512","00:02:37,784",30,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=153,Knowing some information can
cs-410_8_3_31,cs-410,8,3,Syntagmatic,"00:02:37,784","00:02:40,282",31,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=157,will not hurt us in predicting x.
cs-410_8_3_32,cs-410,8,3,Syntagmatic,"00:02:41,510","00:02:46,375",32,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=161,The signal property is that it
cs-410_8_3_33,cs-410,8,3,Syntagmatic,"00:02:46,375","00:02:51,142",33,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=166,"entropy is not symmetrical,"
cs-410_8_3_34,cs-410,8,3,Syntagmatic,"00:02:51,142","00:02:56,394",34,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=171,the third property is that It
cs-410_8_3_35,cs-410,8,3,Syntagmatic,"00:02:56,394","00:03:01,580",35,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=176,only if the two random variables
cs-410_8_3_36,cs-410,8,3,Syntagmatic,"00:03:01,580","00:03:07,949",36,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=181,That means knowing one of them does not
cs-410_8_3_37,cs-410,8,3,Syntagmatic,"00:03:07,949","00:03:14,626",37,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=187,this last property can be verified by
cs-410_8_3_38,cs-410,8,3,Syntagmatic,"00:03:14,626","00:03:19,144",38,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=194,it reaches 0 if and
cs-410_8_3_39,cs-410,8,3,Syntagmatic,"00:03:19,144","00:03:24,102",39,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=199,[INAUDIBLE] Y is exactly the same
cs-410_8_3_40,cs-410,8,3,Syntagmatic,"00:03:24,102","00:03:28,344",40,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=204,So that means knowing why it did not
cs-410_8_3_41,cs-410,8,3,Syntagmatic,"00:03:28,344","00:03:30,520",41,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=208,a Y are completely independent.
cs-410_8_3_42,cs-410,8,3,Syntagmatic,"00:03:32,120","00:03:37,880",42,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=212,Now when we fix X to rank different
cs-410_8_3_43,cs-410,8,3,Syntagmatic,"00:03:37,880","00:03:44,180",43,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=217,would give the same order as
cs-410_8_3_44,cs-410,8,3,Syntagmatic,"00:03:44,180","00:03:49,940",44,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=224,"because in the function here,"
cs-410_8_3_45,cs-410,8,3,Syntagmatic,"00:03:49,940","00:03:53,820",45,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=229,So ranking based on mutual entropy is
cs-410_8_3_46,cs-410,8,3,Syntagmatic,"00:03:53,820","00:03:57,600",46,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=233,"the conditional entropy of X given Y, but"
cs-410_8_3_47,cs-410,8,3,Syntagmatic,"00:03:57,600","00:04:03,058",47,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=237,the mutual information allows us to
cs-410_8_3_48,cs-410,8,3,Syntagmatic,"00:04:03,058","00:04:07,990",48,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=243,"So, that is why mutual information is"
cs-410_8_3_49,cs-410,8,3,Syntagmatic,"00:04:10,688","00:04:14,420",49,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=250,"So, let us examine the intuition"
cs-410_8_3_50,cs-410,8,3,Syntagmatic,"00:04:14,420","00:04:15,880",50,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=254,Syntagmatical Relation Mining.
cs-410_8_3_51,cs-410,8,3,Syntagmatic,"00:04:17,150","00:04:20,430",51,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=257,"Now, the question we ask forcing"
cs-410_8_3_52,cs-410,8,3,Syntagmatic,"00:04:20,430","00:04:24,300",52,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=260,"whenever ""eats"" occurs,"
cs-410_8_3_53,cs-410,8,3,Syntagmatic,"00:04:25,610","00:04:30,710",53,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=265,So this question can be framed as
cs-410_8_3_54,cs-410,8,3,Syntagmatic,"00:04:30,710","00:04:33,055",54,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=270,which words have high mutual
cs-410_8_3_55,cs-410,8,3,Syntagmatic,"00:04:33,055","00:04:37,700",55,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=273,so computer the missing information
cs-410_8_3_56,cs-410,8,3,Syntagmatic,"00:04:39,050","00:04:44,520",56,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=279,"And if we do that, and it is basically"
cs-410_8_3_57,cs-410,8,3,Syntagmatic,"00:04:44,520","00:04:48,990",57,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=284,we will see that words that
cs-410_8_3_58,cs-410,8,3,Syntagmatic,"00:04:48,990","00:04:50,960",58,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=288,will have a high point.
cs-410_8_3_59,cs-410,8,3,Syntagmatic,"00:04:50,960","00:04:55,200",59,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=290,Whereas words that are not related
cs-410_8_3_60,cs-410,8,3,Syntagmatic,"00:04:55,200","00:04:58,530",60,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=295,"For this, I will give some example here."
cs-410_8_3_61,cs-410,8,3,Syntagmatic,"00:04:58,530","00:05:01,220",61,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=298,"The mutual information between ""eats"" and"
cs-410_8_3_62,cs-410,8,3,Syntagmatic,"00:05:01,220","00:05:05,650",62,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=301,"which is the same as between ""meats"" and"
cs-410_8_3_63,cs-410,8,3,Syntagmatic,"00:05:05,650","00:05:10,960",63,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=305,symmetrical is expected to be higher than
cs-410_8_3_64,cs-410,8,3,Syntagmatic,"00:05:10,960","00:05:14,638",64,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=310,"the, because knowing the does not"
cs-410_8_3_65,cs-410,8,3,Syntagmatic,"00:05:14,638","00:05:17,998",65,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=314,"It is similar, and"
cs-410_8_3_66,cs-410,8,3,Syntagmatic,"00:05:17,998","00:05:22,280",66,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=317,the as well.
cs-410_8_3_67,cs-410,8,3,Syntagmatic,"00:05:22,280","00:05:26,970",67,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=322,And you also can easily
cs-410_8_3_68,cs-410,8,3,Syntagmatic,"00:05:26,970","00:05:32,030",68,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=326,information between a word and
cs-410_8_3_69,cs-410,8,3,Syntagmatic,"00:05:32,030","00:05:37,890",69,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=332,which is equal to
cs-410_8_3_70,cs-410,8,3,Syntagmatic,"00:05:37,890","00:05:42,740",70,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=337,"so, because in this case the reduction is"
cs-410_8_3_71,cs-410,8,3,Syntagmatic,"00:05:42,740","00:05:48,530",71,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=342,maximum because knowing one allows
cs-410_8_3_72,cs-410,8,3,Syntagmatic,"00:05:48,530","00:05:50,570",72,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=348,"So the conditional entropy is zero,"
cs-410_8_3_73,cs-410,8,3,Syntagmatic,"00:05:50,570","00:05:54,472",73,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=350,therefore the mutual information
cs-410_8_3_74,cs-410,8,3,Syntagmatic,"00:05:54,472","00:06:02,520",74,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=354,"It is going to be larger, then are equal"
cs-410_8_3_75,cs-410,8,3,Syntagmatic,"00:06:02,520","00:06:05,420",75,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=362,In other words picking any other word and
cs-410_8_3_76,cs-410,8,3,Syntagmatic,"00:06:05,420","00:06:08,588",76,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=365,the computer picking between eats and
cs-410_8_3_77,cs-410,8,3,Syntagmatic,"00:06:08,588","00:06:13,511",77,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=368,You will not get any information larger
cs-410_8_3_78,cs-410,8,3,Syntagmatic,"00:06:16,386","00:06:21,390",78,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=376,So now let us look at how to
cs-410_8_3_79,cs-410,8,3,Syntagmatic,"00:06:21,390","00:06:23,490",79,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=381,"Now in order to do that, we often"
cs-410_8_3_80,cs-410,8,3,Syntagmatic,"00:06:25,110","00:06:29,100",80,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=385,use a different form of mutual
cs-410_8_3_81,cs-410,8,3,Syntagmatic,"00:06:29,100","00:06:34,190",81,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=389,rewrite the mutual information
cs-410_8_3_82,cs-410,8,3,Syntagmatic,"00:06:34,190","00:06:38,655",82,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=394,Where we essentially see
cs-410_8_3_83,cs-410,8,3,Syntagmatic,"00:06:38,655","00:06:43,075",83,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=398,called a KL-divergence or divergence.
cs-410_8_3_84,cs-410,8,3,Syntagmatic,"00:06:43,075","00:06:45,615",84,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=403,This is another term
cs-410_8_3_85,cs-410,8,3,Syntagmatic,"00:06:45,615","00:06:48,865",85,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=405,It measures the divergence
cs-410_8_3_86,cs-410,8,3,Syntagmatic,"00:06:50,615","00:06:54,645",86,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=410,"Now, if you look at the formula,"
cs-410_8_3_87,cs-410,8,3,Syntagmatic,"00:06:54,645","00:06:58,190",87,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=414,different values of the two random
cs-410_8_3_88,cs-410,8,3,Syntagmatic,"00:06:58,190","00:07:04,110",88,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=418,mainly we are doing a comparison
cs-410_8_3_89,cs-410,8,3,Syntagmatic,"00:07:04,110","00:07:06,690",89,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=424,"The numerator has the joint,"
cs-410_8_3_90,cs-410,8,3,Syntagmatic,"00:07:06,690","00:07:11,110",90,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=426,actual observed the joint distribution
cs-410_8_3_91,cs-410,8,3,Syntagmatic,"00:07:12,690","00:07:15,720",91,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=432,The bottom part or the denominator can be
cs-410_8_3_92,cs-410,8,3,Syntagmatic,"00:07:15,720","00:07:20,695",92,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=435,interpreted as the expected joint
cs-410_8_3_93,cs-410,8,3,Syntagmatic,"00:07:20,695","00:07:26,782",93,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=440,if they were independent because when
cs-410_8_3_94,cs-410,8,3,Syntagmatic,"00:07:26,782","00:07:32,810",94,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=446,they are joined distribution is equal to
cs-410_8_3_95,cs-410,8,3,Syntagmatic,"00:07:35,300","00:07:39,800",95,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=455,So this comparison will tell us whether
cs-410_8_3_96,cs-410,8,3,Syntagmatic,"00:07:39,800","00:07:43,170",96,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=459,If they are indeed independent then we
cs-410_8_3_97,cs-410,8,3,Syntagmatic,"00:07:44,390","00:07:49,470",97,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=464,but if the numerator is different
cs-410_8_3_98,cs-410,8,3,Syntagmatic,"00:07:49,470","00:07:54,530",98,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=469,the two variables are not independent and
cs-410_8_3_99,cs-410,8,3,Syntagmatic,"00:07:56,120","00:08:00,110",99,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=476,The sum is simply to take into
cs-410_8_3_100,cs-410,8,3,Syntagmatic,"00:08:00,110","00:08:04,180",100,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=480,of the values of these
cs-410_8_3_101,cs-410,8,3,Syntagmatic,"00:08:04,180","00:08:08,750",101,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=484,"In our case, each random variable"
cs-410_8_3_102,cs-410,8,3,Syntagmatic,"00:08:08,750","00:08:13,950",102,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=488,"zero or one, so"
cs-410_8_3_103,cs-410,8,3,Syntagmatic,"00:08:13,950","00:08:17,330",103,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=493,If we look at this form of mutual
cs-410_8_3_104,cs-410,8,3,Syntagmatic,"00:08:17,330","00:08:21,230",104,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=497,information matches the divergence
cs-410_8_3_105,cs-410,8,3,Syntagmatic,"00:08:21,230","00:08:25,800",105,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=501,from the expected distribution
cs-410_8_3_106,cs-410,8,3,Syntagmatic,"00:08:25,800","00:08:30,144",106,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=505,"The larger this divergence is, the higher"
cs-410_8_3_107,cs-410,8,3,Syntagmatic,"00:08:33,507","00:08:37,091",107,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=513,So now let us further look at what
cs-410_8_3_108,cs-410,8,3,Syntagmatic,"00:08:37,091","00:08:39,840",108,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=517,involved in this formula
cs-410_8_3_109,cs-410,8,3,Syntagmatic,"00:08:41,300","00:08:45,080",109,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=521,"And here, this is all the probabilities"
cs-410_8_3_110,cs-410,8,3,Syntagmatic,"00:08:45,080","00:08:46,500",110,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=525,you to verify that.
cs-410_8_3_111,cs-410,8,3,Syntagmatic,"00:08:46,500","00:08:51,610",111,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=526,"Basically, we have first to"
cs-410_8_3_112,cs-410,8,3,Syntagmatic,"00:08:51,610","00:08:56,380",112,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=531,corresponding to the presence or
cs-410_8_3_113,cs-410,8,3,Syntagmatic,"00:08:56,380","00:08:59,610",113,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=536,"So, for w1,"
cs-410_8_3_114,cs-410,8,3,Syntagmatic,"00:09:02,600","00:09:07,995",114,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=542,"They should sum to one, because a word"
cs-410_8_3_115,cs-410,8,3,Syntagmatic,"00:09:07,995","00:09:13,260",115,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=547,"In the segment, and similarly for"
cs-410_8_3_116,cs-410,8,3,Syntagmatic,"00:09:13,260","00:09:18,230",116,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=553,"the second word, we also have two"
cs-410_8_3_117,cs-410,8,3,Syntagmatic,"00:09:18,230","00:09:20,920",117,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=558,"absences of this word, and"
cs-410_8_3_118,cs-410,8,3,Syntagmatic,"00:09:21,920","00:09:26,162",118,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=561,"And finally, we have a lot of"
cs-410_8_3_119,cs-410,8,3,Syntagmatic,"00:09:26,162","00:09:31,100",119,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=566,the scenarios of co-occurrences of
cs-410_8_3_120,cs-410,8,3,Syntagmatic,"00:09:34,513","00:09:39,107",120,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=574,And they sum to one because the two
cs-410_8_3_121,cs-410,8,3,Syntagmatic,"00:09:39,107","00:09:41,420",121,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=579,possible scenarios.
cs-410_8_3_122,cs-410,8,3,Syntagmatic,"00:09:41,420","00:09:43,730",122,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=581,"Either they both occur, so"
cs-410_8_3_123,cs-410,8,3,Syntagmatic,"00:09:43,730","00:09:49,500",123,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=583,in that case both variables will have
cs-410_8_3_124,cs-410,8,3,Syntagmatic,"00:09:49,500","00:09:50,579",124,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=589,There are two scenarios.
cs-410_8_3_125,cs-410,8,3,Syntagmatic,"00:09:51,660","00:09:55,910",125,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=591,In these two cases one of the random
cs-410_8_3_126,cs-410,8,3,Syntagmatic,"00:09:55,910","00:10:03,560",126,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=595,the other will be zero and finally we have
cs-410_8_3_127,cs-410,8,3,Syntagmatic,"00:10:03,560","00:10:06,420",127,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=603,This is when the two variables
cs-410_8_3_128,cs-410,8,3,Syntagmatic,"00:10:07,620","00:10:12,855",128,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=607,So these are the probabilities involved
cs-410_8_3_129,cs-410,8,3,Syntagmatic,"00:10:12,855","00:10:13,600",129,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=612,over here.
cs-410_8_3_130,cs-410,8,3,Syntagmatic,"00:10:16,007","00:10:18,416",130,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=616,Once we know how to calculate
cs-410_8_3_131,cs-410,8,3,Syntagmatic,"00:10:18,416","00:10:20,670",131,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=618,we can easily calculate
cs-410_8_3_132,cs-410,8,3,Syntagmatic,"00:10:24,063","00:10:28,231",132,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=624,It is also interesting to know that
cs-410_8_3_133,cs-410,8,3,Syntagmatic,"00:10:28,231","00:10:32,960",133,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=628,"constraint among these probabilities,"
cs-410_8_3_134,cs-410,8,3,Syntagmatic,"00:10:32,960","00:10:36,400",134,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=632,"So in the previous slide,"
cs-410_8_3_135,cs-410,8,3,Syntagmatic,"00:10:36,400","00:10:41,830",135,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=636,that you have seen that
cs-410_8_3_136,cs-410,8,3,Syntagmatic,"00:10:41,830","00:10:46,114",136,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=641,words sum to one and
cs-410_8_3_137,cs-410,8,3,Syntagmatic,"00:10:46,114","00:10:53,190",137,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=646,that says the two words have these
cs-410_8_3_138,cs-410,8,3,Syntagmatic,"00:10:53,190","00:10:57,370",138,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=653,but we also have some additional
cs-410_8_3_139,cs-410,8,3,Syntagmatic,"00:10:58,600","00:11:03,670",139,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=658,"For example, this one means if we add up"
cs-410_8_3_140,cs-410,8,3,Syntagmatic,"00:11:03,670","00:11:07,890",140,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=663,the probabilities that we observe
cs-410_8_3_141,cs-410,8,3,Syntagmatic,"00:11:07,890","00:11:12,500",141,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=667,the probabilities when the first word
cs-410_8_3_142,cs-410,8,3,Syntagmatic,"00:11:12,500","00:11:16,860",142,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=672,We get exactly the probability
cs-410_8_3_143,cs-410,8,3,Syntagmatic,"00:11:16,860","00:11:20,040",143,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=676,"In other words, when the word is observed."
cs-410_8_3_144,cs-410,8,3,Syntagmatic,"00:11:20,040","00:11:22,210",144,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=680,"When the first word is observed, and"
cs-410_8_3_145,cs-410,8,3,Syntagmatic,"00:11:22,210","00:11:27,640",145,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=682,"there are only two scenarios, depending on"
cs-410_8_3_146,cs-410,8,3,Syntagmatic,"00:11:27,640","00:11:31,750",146,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=687,"So, this probability captures the first"
cs-410_8_3_147,cs-410,8,3,Syntagmatic,"00:11:31,750","00:11:33,860",147,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=691,"actually is also observed, and"
cs-410_8_3_148,cs-410,8,3,Syntagmatic,"00:11:33,860","00:11:38,130",148,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=693,this captures the second scenario
cs-410_8_3_149,cs-410,8,3,Syntagmatic,"00:11:38,130","00:11:40,145",149,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=698,"So, we only see the first word, and"
cs-410_8_3_150,cs-410,8,3,Syntagmatic,"00:11:40,145","00:11:45,410",150,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=700,it is easy to see the other equations
cs-410_8_3_151,cs-410,8,3,Syntagmatic,"00:11:46,980","00:11:50,980",151,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=706,Now these equations allow us to
cs-410_8_3_152,cs-410,8,3,Syntagmatic,"00:11:50,980","00:11:54,610",152,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=710,"other probabilities, and"
cs-410_8_3_153,cs-410,8,3,Syntagmatic,"00:11:55,750","00:12:01,010",153,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=715,"So more specifically,"
cs-410_8_3_154,cs-410,8,3,Syntagmatic,"00:12:01,010","00:12:06,490",154,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=721,"a word is present, like in this case,"
cs-410_8_3_155,cs-410,8,3,Syntagmatic,"00:12:06,490","00:12:12,630",155,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=726,if we know the probability of
cs-410_8_3_156,cs-410,8,3,Syntagmatic,"00:12:12,630","00:12:17,002",156,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=732,then we can easily compute
cs-410_8_3_157,cs-410,8,3,Syntagmatic,"00:12:17,002","00:12:22,770",157,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=737,It is very easy to use this
cs-410_8_3_158,cs-410,8,3,Syntagmatic,"00:12:22,770","00:12:27,820",158,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=742,we take care of the computation of
cs-410_8_3_159,cs-410,8,3,Syntagmatic,"00:12:27,820","00:12:29,950",159,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=747,absence of each word.
cs-410_8_3_160,cs-410,8,3,Syntagmatic,"00:12:29,950","00:12:33,146",160,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=749,Now let's look at
cs-410_8_3_161,cs-410,8,3,Syntagmatic,"00:12:33,146","00:12:36,460",161,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=753,Let us assume that we also have available
cs-410_8_3_162,cs-410,8,3,Syntagmatic,"00:12:36,460","00:12:39,548",162,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=756,the probability that
cs-410_8_3_163,cs-410,8,3,Syntagmatic,"00:12:39,548","00:12:44,220",163,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=759,Now it is easy to see that we can
cs-410_8_3_164,cs-410,8,3,Syntagmatic,"00:12:44,220","00:12:45,829",164,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=764,probabilities based on these.
cs-410_8_3_165,cs-410,8,3,Syntagmatic,"00:12:46,870","00:12:51,170",165,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=766,Specifically for
cs-410_8_3_166,cs-410,8,3,Syntagmatic,"00:12:51,170","00:12:56,260",166,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=771,the probability that the first word
cs-410_8_3_167,cs-410,8,3,Syntagmatic,"00:12:56,260","00:13:02,020",167,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=776,because we know these probabilities in
cs-410_8_3_168,cs-410,8,3,Syntagmatic,"00:13:02,020","00:13:05,364",168,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=782,equation we can compute the probability
cs-410_8_3_169,cs-410,8,3,Syntagmatic,"00:13:05,364","00:13:06,000",169,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=785,Word.
cs-410_8_3_170,cs-410,8,3,Syntagmatic,"00:13:06,000","00:13:10,421",170,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=786,"And then finally,"
cs-410_8_3_171,cs-410,8,3,Syntagmatic,"00:13:10,421","00:13:14,745",171,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=790,by using this equation because
cs-410_8_3_172,cs-410,8,3,Syntagmatic,"00:13:14,745","00:13:19,282",172,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=794,"this is also known, and"
cs-410_8_3_173,cs-410,8,3,Syntagmatic,"00:13:19,282","00:13:23,120",173,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=799,So this can be easier to calculate.
cs-410_8_3_174,cs-410,8,3,Syntagmatic,"00:13:23,120","00:13:24,430",174,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=803,So now this can be calculated.
cs-410_8_3_175,cs-410,8,3,Syntagmatic,"00:13:26,080","00:13:30,989",175,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=806,So this slide shows that we only
cs-410_8_3_176,cs-410,8,3,Syntagmatic,"00:13:30,989","00:13:35,800",176,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=810,these three probabilities
cs-410_8_3_177,cs-410,8,3,Syntagmatic,"00:13:35,800","00:13:43,092",177,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=815,naming the presence of each word and the
cs-410_8_3_178,cs-410,8,3,Syntagmatic,"00:13:43,092","00:13:53,092",178,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=823,[MUSIC]
cs-410_8_4_1,cs-410,8,4,Syntagmatic,"00:00:00,000","00:00:04,714",1,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=0,[SOUND]
cs-410_8_4_2,cs-410,8,4,Syntagmatic,"00:00:06,455","00:00:09,677",2,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=6,"In general, we can use the empirical count"
cs-410_8_4_3,cs-410,8,4,Syntagmatic,"00:00:09,677","00:00:15,340",3,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=9,of events in the observed data
cs-410_8_4_4,cs-410,8,4,Syntagmatic,"00:00:15,340","00:00:19,080",4,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=15,And a commonly used technique is
cs-410_8_4_5,cs-410,8,4,Syntagmatic,"00:00:19,080","00:00:22,600",5,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=19,where we simply normalize
cs-410_8_4_6,cs-410,8,4,Syntagmatic,"00:00:22,600","00:00:30,330",6,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=22,"So if we do that, we can see, we can"
cs-410_8_4_7,cs-410,8,4,Syntagmatic,"00:00:30,330","00:00:36,811",7,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=30,For estimating the probability that
cs-410_8_4_8,cs-410,8,4,Syntagmatic,"00:00:36,811","00:00:42,773",8,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=36,we simply normalize the count of
cs-410_8_4_9,cs-410,8,4,Syntagmatic,"00:00:42,773","00:00:47,278",9,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=42,So let's first take
cs-410_8_4_10,cs-410,8,4,Syntagmatic,"00:00:47,278","00:00:52,970",10,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=47,"On the right side, you see a list of some,"
cs-410_8_4_11,cs-410,8,4,Syntagmatic,"00:00:52,970","00:00:55,010",11,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=52,These are segments.
cs-410_8_4_12,cs-410,8,4,Syntagmatic,"00:00:55,010","00:00:59,860",12,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=55,And in some segments you see both words
cs-410_8_4_13,cs-410,8,4,Syntagmatic,"00:00:59,860","00:01:01,630",13,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=59,both columns.
cs-410_8_4_14,cs-410,8,4,Syntagmatic,"00:01:01,630","00:01:05,830",14,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=61,"In some other cases only one will occur,"
cs-410_8_4_15,cs-410,8,4,Syntagmatic,"00:01:05,830","00:01:07,590",15,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=65,the other column has zero.
cs-410_8_4_16,cs-410,8,4,Syntagmatic,"00:01:07,590","00:01:11,130",16,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=67,"And in all, of course, in some other"
cs-410_8_4_17,cs-410,8,4,Syntagmatic,"00:01:11,130","00:01:13,930",17,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=71,so they are both zeros.
cs-410_8_4_18,cs-410,8,4,Syntagmatic,"00:01:13,930","00:01:19,310",18,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=73,"And for estimating these probabilities, we"
cs-410_8_4_19,cs-410,8,4,Syntagmatic,"00:01:20,340","00:01:23,560",19,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=80,"So the three counts are first,"
cs-410_8_4_20,cs-410,8,4,Syntagmatic,"00:01:23,560","00:01:27,337",20,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=83,And that's the total number of
cs-410_8_4_21,cs-410,8,4,Syntagmatic,"00:01:27,337","00:01:30,950",21,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=87,It's just as the ones in the column of W1.
cs-410_8_4_22,cs-410,8,4,Syntagmatic,"00:01:30,950","00:01:34,470",22,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=90,We can count how many
cs-410_8_4_23,cs-410,8,4,Syntagmatic,"00:01:34,470","00:01:40,460",23,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=94,"The segment count is for word 2, and we"
cs-410_8_4_24,cs-410,8,4,Syntagmatic,"00:01:40,460","00:01:45,425",24,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=100,And these will give us the total
cs-410_8_4_25,cs-410,8,4,Syntagmatic,"00:01:45,425","00:01:49,650",25,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=105,The third count is when both words occur.
cs-410_8_4_26,cs-410,8,4,Syntagmatic,"00:01:49,650","00:01:55,370",26,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=109,"So this time, we're going to count"
cs-410_8_4_27,cs-410,8,4,Syntagmatic,"00:01:56,580","00:02:00,060",27,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=116,"And then, so this would give us"
cs-410_8_4_28,cs-410,8,4,Syntagmatic,"00:02:00,060","00:02:03,510",28,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=120,where we have seen both W1 and W2.
cs-410_8_4_29,cs-410,8,4,Syntagmatic,"00:02:03,510","00:02:08,112",29,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=123,"Once we have these counts,"
cs-410_8_4_30,cs-410,8,4,Syntagmatic,"00:02:08,112","00:02:11,019",30,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=128,"which is the total number of segments, and"
cs-410_8_4_31,cs-410,8,4,Syntagmatic,"00:02:11,019","00:02:16,706",31,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=131,this will give us the probabilities that
cs-410_8_4_32,cs-410,8,4,Syntagmatic,"00:02:16,706","00:02:22,301",32,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=136,"Now, there is a small problem,"
cs-410_8_4_33,cs-410,8,4,Syntagmatic,"00:02:22,301","00:02:27,458",33,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=142,"And in this case, we don't want a zero"
cs-410_8_4_34,cs-410,8,4,Syntagmatic,"00:02:27,458","00:02:33,365",34,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=147,"a small sample and in general, we would"
cs-410_8_4_35,cs-410,8,4,Syntagmatic,"00:02:33,365","00:02:35,806",35,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=153,a [INAUDIBLE] to avoid any context.
cs-410_8_4_36,cs-410,8,4,Syntagmatic,"00:02:35,806","00:02:39,630",36,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=155,"So, to address this problem,"
cs-410_8_4_37,cs-410,8,4,Syntagmatic,"00:02:39,630","00:02:43,780",37,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=159,And that's basically to add some
cs-410_8_4_38,cs-410,8,4,Syntagmatic,"00:02:43,780","00:02:48,410",38,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=163,and so that we don't get
cs-410_8_4_39,cs-410,8,4,Syntagmatic,"00:02:48,410","00:02:54,250",39,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=168,"Now, the best way to understand smoothing"
cs-410_8_4_40,cs-410,8,4,Syntagmatic,"00:02:54,250","00:03:00,310",40,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=174,"data than we actually have, because we'll"
cs-410_8_4_41,cs-410,8,4,Syntagmatic,"00:03:00,310","00:03:04,650",41,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=180,"I illustrated on the top,"
cs-410_8_4_42,cs-410,8,4,Syntagmatic,"00:03:04,650","00:03:10,095",42,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=184,And these pseudo-segments would
cs-410_8_4_43,cs-410,8,4,Syntagmatic,"00:03:10,095","00:03:15,047",43,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=190,of these words so
cs-410_8_4_44,cs-410,8,4,Syntagmatic,"00:03:15,047","00:03:18,169",44,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=195,"Now, in particular we introduce"
cs-410_8_4_45,cs-410,8,4,Syntagmatic,"00:03:18,169","00:03:20,990",45,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=198,Each is weighted at one quarter.
cs-410_8_4_46,cs-410,8,4,Syntagmatic,"00:03:20,990","00:03:25,930",46,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=200,And these represent the four different
cs-410_8_4_47,cs-410,8,4,Syntagmatic,"00:03:25,930","00:03:30,490",47,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=205,"So now each event,"
cs-410_8_4_48,cs-410,8,4,Syntagmatic,"00:03:30,490","00:03:35,390",48,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=210,at least one count or at least a non-zero
cs-410_8_4_49,cs-410,8,4,Syntagmatic,"00:03:35,390","00:03:39,380",49,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=215,"So, in the actual segments"
cs-410_8_4_50,cs-410,8,4,Syntagmatic,"00:03:39,380","00:03:44,231",50,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=219,it's okay if we haven't observed
cs-410_8_4_51,cs-410,8,4,Syntagmatic,"00:03:44,231","00:03:49,671",51,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=224,"So more specifically, you can see"
cs-410_8_4_52,cs-410,8,4,Syntagmatic,"00:03:49,671","00:03:55,560",52,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=229,"ones in the two pseudo-segments,"
cs-410_8_4_53,cs-410,8,4,Syntagmatic,"00:03:55,560","00:03:59,315",53,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=235,"We add them up, we get 0.5."
cs-410_8_4_54,cs-410,8,4,Syntagmatic,"00:03:59,315","00:04:03,319",54,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=239,"And similar to this,"
cs-410_8_4_55,cs-410,8,4,Syntagmatic,"00:04:03,319","00:04:08,240",55,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=243,pseudo-segment that indicates
cs-410_8_4_56,cs-410,8,4,Syntagmatic,"00:04:09,450","00:04:14,000",56,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=249,And of course in the denominator we add
cs-410_8_4_57,cs-410,8,4,Syntagmatic,"00:04:14,000","00:04:17,520",57,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=254,"we add, in this case,"
cs-410_8_4_58,cs-410,8,4,Syntagmatic,"00:04:17,520","00:04:21,780",58,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=257,Each is weighed at one quarter so
cs-410_8_4_59,cs-410,8,4,Syntagmatic,"00:04:21,780","00:04:24,110",59,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=261,"So, that's why in the denominator"
cs-410_8_4_60,cs-410,8,4,Syntagmatic,"00:04:25,990","00:04:31,460",60,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=265,"So, this basically concludes"
cs-410_8_4_61,cs-410,8,4,Syntagmatic,"00:04:31,460","00:04:33,920",61,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=271,four syntagmatic relation discoveries.
cs-410_8_4_62,cs-410,8,4,Syntagmatic,"00:04:36,090","00:04:42,050",62,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=276,"Now, so to summarize,"
cs-410_8_4_63,cs-410,8,4,Syntagmatic,"00:04:42,050","00:04:46,240",63,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=282,be discovered by measuring correlations
cs-410_8_4_64,cs-410,8,4,Syntagmatic,"00:04:46,240","00:04:49,580",64,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=286,We've introduced the three
cs-410_8_4_65,cs-410,8,4,Syntagmatic,"00:04:49,580","00:04:53,230",65,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=289,"Entropy, which measures the uncertainty"
cs-410_8_4_66,cs-410,8,4,Syntagmatic,"00:04:53,230","00:04:59,060",66,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=293,"Conditional entropy, which measures"
cs-410_8_4_67,cs-410,8,4,Syntagmatic,"00:04:59,060","00:05:04,530",67,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=299,"And mutual information of X and Y,"
cs-410_8_4_68,cs-410,8,4,Syntagmatic,"00:05:04,530","00:05:11,240",68,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=304,"due to knowing Y, or"
cs-410_8_4_69,cs-410,8,4,Syntagmatic,"00:05:11,240","00:05:12,660",69,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=311,They are the same.
cs-410_8_4_70,cs-410,8,4,Syntagmatic,"00:05:12,660","00:05:17,111",70,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=312,So these three concepts are actually very
cs-410_8_4_71,cs-410,8,4,Syntagmatic,"00:05:17,111","00:05:20,340",71,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=317,That's why we spent some time
cs-410_8_4_72,cs-410,8,4,Syntagmatic,"00:05:20,340","00:05:23,150",72,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=320,"But in particular,"
cs-410_8_4_73,cs-410,8,4,Syntagmatic,"00:05:23,150","00:05:25,960",73,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=323,discovering syntagmatic relations.
cs-410_8_4_74,cs-410,8,4,Syntagmatic,"00:05:25,960","00:05:30,142",74,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=325,"In particular,"
cs-410_8_4_75,cs-410,8,4,Syntagmatic,"00:05:30,142","00:05:32,370",75,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=330,discovering such a relation.
cs-410_8_4_76,cs-410,8,4,Syntagmatic,"00:05:32,370","00:05:37,241",76,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=332,It allows us to have values
cs-410_8_4_77,cs-410,8,4,Syntagmatic,"00:05:37,241","00:05:42,211",77,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=337,words that are comparable and
cs-410_8_4_78,cs-410,8,4,Syntagmatic,"00:05:42,211","00:05:48,208",78,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=342,discover the strongest syntagmatic
cs-410_8_4_79,cs-410,8,4,Syntagmatic,"00:05:48,208","00:05:53,700",79,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=348,"Now, note that there is some relation"
cs-410_8_4_80,cs-410,8,4,Syntagmatic,"00:05:53,700","00:05:55,910",80,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=353,[INAUDIBLE] relation discovery.
cs-410_8_4_81,cs-410,8,4,Syntagmatic,"00:05:55,910","00:06:01,835",81,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=355,So we already discussed the possibility
cs-410_8_4_82,cs-410,8,4,Syntagmatic,"00:06:01,835","00:06:06,683",82,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=361,terms in the context to potentially
cs-410_8_4_83,cs-410,8,4,Syntagmatic,"00:06:06,683","00:06:11,187",83,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=366,that have syntagmatic relations
cs-410_8_4_84,cs-410,8,4,Syntagmatic,"00:06:11,187","00:06:17,958",84,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=371,"But here, once we use mutual information"
cs-410_8_4_85,cs-410,8,4,Syntagmatic,"00:06:17,958","00:06:24,436",85,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=377,we can also represent the context with
cs-410_8_4_86,cs-410,8,4,Syntagmatic,"00:06:24,436","00:06:29,567",86,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=384,So this would give us
cs-410_8_4_87,cs-410,8,4,Syntagmatic,"00:06:29,567","00:06:33,490",87,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=389,"the context of a word, like a cat."
cs-410_8_4_88,cs-410,8,4,Syntagmatic,"00:06:33,490","00:06:37,394",88,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=393,"And if we do the same for all the words,"
cs-410_8_4_89,cs-410,8,4,Syntagmatic,"00:06:37,394","00:06:42,320",89,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=397,compare the similarity between these
cs-410_8_4_90,cs-410,8,4,Syntagmatic,"00:06:42,320","00:06:45,850",90,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=402,So this provides yet
cs-410_8_4_91,cs-410,8,4,Syntagmatic,"00:06:45,850","00:06:48,800",91,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=405,paradigmatic relation discovery.
cs-410_8_4_92,cs-410,8,4,Syntagmatic,"00:06:48,800","00:06:55,770",92,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=408,And so to summarize this whole part
cs-410_8_4_93,cs-410,8,4,Syntagmatic,"00:06:55,770","00:06:59,190",93,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=415,"We introduce two basic associations,"
cs-410_8_4_94,cs-410,8,4,Syntagmatic,"00:06:59,190","00:07:01,000",94,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=419,a syntagmatic relations.
cs-410_8_4_95,cs-410,8,4,Syntagmatic,"00:07:01,000","00:07:05,710",95,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=421,"These are fairly general, they apply"
cs-410_8_4_96,cs-410,8,4,Syntagmatic,"00:07:05,710","00:07:10,009",96,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=425,"the units don't have to be words,"
cs-410_8_4_97,cs-410,8,4,Syntagmatic,"00:07:11,120","00:07:16,235",97,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=431,We introduced multiple statistical
cs-410_8_4_98,cs-410,8,4,Syntagmatic,"00:07:16,235","00:07:20,762",98,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=436,mainly showing that pure
cs-410_8_4_99,cs-410,8,4,Syntagmatic,"00:07:20,762","00:07:24,840",99,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=440,are variable for
cs-410_8_4_100,cs-410,8,4,Syntagmatic,"00:07:24,840","00:07:28,800",100,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=444,And they can be combined to
cs-410_8_4_101,cs-410,8,4,Syntagmatic,"00:07:28,800","00:07:35,040",101,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=448,These approaches can be applied
cs-410_8_4_102,cs-410,8,4,Syntagmatic,"00:07:35,040","00:07:39,940",102,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=455,mostly because they are based
cs-410_8_4_103,cs-410,8,4,Syntagmatic,"00:07:39,940","00:07:42,690",103,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=459,they can actually discover
cs-410_8_4_104,cs-410,8,4,Syntagmatic,"00:07:44,360","00:07:47,880",104,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=464,We can also use different ways with
cs-410_8_4_105,cs-410,8,4,Syntagmatic,"00:07:47,880","00:07:51,360",105,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=467,this would lead us to some interesting
cs-410_8_4_106,cs-410,8,4,Syntagmatic,"00:07:51,360","00:07:56,190",106,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=471,"For example, the context can be very"
cs-410_8_4_107,cs-410,8,4,Syntagmatic,"00:07:56,190","00:08:00,760",107,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=476,"a sentence, or maybe paragraphs,"
cs-410_8_4_108,cs-410,8,4,Syntagmatic,"00:08:00,760","00:08:05,330",108,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=480,allows to discover different flavors
cs-410_8_4_109,cs-410,8,4,Syntagmatic,"00:08:05,330","00:08:09,362",109,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=485,"And similarly,"
cs-410_8_4_110,cs-410,8,4,Syntagmatic,"00:08:09,362","00:08:13,380",110,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=489,visual information to discover
cs-410_8_4_111,cs-410,8,4,Syntagmatic,"00:08:13,380","00:08:19,110",111,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=493,"We also have to define the segment, and"
cs-410_8_4_112,cs-410,8,4,Syntagmatic,"00:08:19,110","00:08:22,560",112,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=499,text window or a longer text article.
cs-410_8_4_113,cs-410,8,4,Syntagmatic,"00:08:22,560","00:08:26,508",113,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=502,And this would give us different
cs-410_8_4_114,cs-410,8,4,Syntagmatic,"00:08:26,508","00:08:32,677",114,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=506,These discovery associations can
cs-410_8_4_115,cs-410,8,4,Syntagmatic,"00:08:32,677","00:08:37,701",115,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=512,in both information retrieval and
cs-410_8_4_116,cs-410,8,4,Syntagmatic,"00:08:37,701","00:08:44,100",116,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=517,"So here are some recommended readings,"
cs-410_8_4_117,cs-410,8,4,Syntagmatic,"00:08:44,100","00:08:46,880",117,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=524,The first is a book with
cs-410_8_4_118,cs-410,8,4,Syntagmatic,"00:08:46,880","00:08:50,810",118,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=526,which is quite relevant to
cs-410_8_4_119,cs-410,8,4,Syntagmatic,"00:08:50,810","00:08:55,120",119,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=530,The second is an article
cs-410_8_4_120,cs-410,8,4,Syntagmatic,"00:08:55,120","00:08:58,160",120,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=535,statistical measures to
cs-410_8_4_121,cs-410,8,4,Syntagmatic,"00:08:58,160","00:09:03,764",121,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=538,Those are phrases that
cs-410_8_4_122,cs-410,8,4,Syntagmatic,"00:09:03,764","00:09:07,560",122,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=543,"For example,"
cs-410_8_4_123,cs-410,8,4,Syntagmatic,"00:09:08,610","00:09:11,550",123,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=548,blue chip is not a chip that's blue.
cs-410_8_4_124,cs-410,8,4,Syntagmatic,"00:09:11,550","00:09:16,180",124,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=551,And the paper has a discussion about some
cs-410_8_4_125,cs-410,8,4,Syntagmatic,"00:09:17,400","00:09:23,227",125,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=557,The third one is a new paper on a unified
cs-410_8_4_126,cs-410,8,4,Syntagmatic,"00:09:23,227","00:09:29,441",126,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=563,"relations and a syntagmatical relations,"
cs-410_8_4_127,cs-410,8,4,Syntagmatic,"00:09:29,441","00:09:39,441",127,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=569,[SOUND]
cs-410_8_5_1,cs-410,8,5,Topic,"00:00:00,025","00:00:06,885",1,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=0,[SOUND]
cs-410_8_5_2,cs-410,8,5,Topic,"00:00:06,885","00:00:11,190",2,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=6,lecture is about topic mining and
cs-410_8_5_3,cs-410,8,5,Topic,"00:00:11,190","00:00:14,270",3,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=11,We're going to talk about its
cs-410_8_5_4,cs-410,8,5,Topic,"00:00:17,780","00:00:22,630",4,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=17,In this lecture we're going to talk
cs-410_8_5_5,cs-410,8,5,Topic,"00:00:23,770","00:00:28,310",5,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=23,"As you see on this road map,"
cs-410_8_5_6,cs-410,8,5,Topic,"00:00:28,310","00:00:33,190",6,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=28,"mining knowledge about language,"
cs-410_8_5_7,cs-410,8,5,Topic,"00:00:33,190","00:00:37,987",7,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=33,word associations such as paradigmatic and
cs-410_8_5_8,cs-410,8,5,Topic,"00:00:39,190","00:00:43,100",8,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=39,"Now, starting from this lecture, we're"
cs-410_8_5_9,cs-410,8,5,Topic,"00:00:43,100","00:00:47,570",9,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=43,"knowledge, which is content mining, and"
cs-410_8_5_10,cs-410,8,5,Topic,"00:00:47,570","00:00:55,031",10,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=47,trying to discover knowledge about
cs-410_8_5_11,cs-410,8,5,Topic,"00:00:56,140","00:00:58,810",11,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=56,And we call that topic mining and
cs-410_8_5_12,cs-410,8,5,Topic,"00:00:59,920","00:01:04,350",12,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=59,"In this lecture, we're going to talk about"
cs-410_8_5_13,cs-410,8,5,Topic,"00:01:04,350","00:01:08,260",13,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=64,"So first of all,"
cs-410_8_5_14,cs-410,8,5,Topic,"00:01:08,260","00:01:12,600",14,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=68,So topic is something that we
cs-410_8_5_15,cs-410,8,5,Topic,"00:01:12,600","00:01:15,840",15,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=72,it's actually not that
cs-410_8_5_16,cs-410,8,5,Topic,"00:01:15,840","00:01:20,420",16,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=75,"Roughly speaking, topic is the main"
cs-410_8_5_17,cs-410,8,5,Topic,"00:01:20,420","00:01:25,860",17,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=80,And you can think of this as a theme or
cs-410_8_5_18,cs-410,8,5,Topic,"00:01:25,860","00:01:28,420",18,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=85,It can also have different granularities.
cs-410_8_5_19,cs-410,8,5,Topic,"00:01:28,420","00:01:31,240",19,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=88,"For example,"
cs-410_8_5_20,cs-410,8,5,Topic,"00:01:31,240","00:01:34,800",20,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=91,"A topic of article,"
cs-410_8_5_21,cs-410,8,5,Topic,"00:01:34,800","00:01:40,540",21,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=94,the topic of all the research articles
cs-410_8_5_22,cs-410,8,5,Topic,"00:01:40,540","00:01:45,629",22,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=100,so different grand narratives of topics
cs-410_8_5_23,cs-410,8,5,Topic,"00:01:46,760","00:01:51,628",23,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=106,"Indeed, there are many applications that"
cs-410_8_5_24,cs-410,8,5,Topic,"00:01:51,628","00:01:52,980",24,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=111,they're analyzed then.
cs-410_8_5_25,cs-410,8,5,Topic,"00:01:52,980","00:01:54,300",25,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=112,Here are some examples.
cs-410_8_5_26,cs-410,8,5,Topic,"00:01:54,300","00:01:58,280",26,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=114,"For example, we might be interested"
cs-410_8_5_27,cs-410,8,5,Topic,"00:01:58,280","00:02:00,470",27,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=118,users are talking about today?
cs-410_8_5_28,cs-410,8,5,Topic,"00:02:00,470","00:02:03,600",28,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=120,"Are they talking about NBA sports, or"
cs-410_8_5_29,cs-410,8,5,Topic,"00:02:03,600","00:02:08,540",29,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=123,are they talking about some
cs-410_8_5_30,cs-410,8,5,Topic,"00:02:08,540","00:02:12,970",30,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=128,Or we are interested in
cs-410_8_5_31,cs-410,8,5,Topic,"00:02:12,970","00:02:17,090",31,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=132,"For example, one might be interested in"
cs-410_8_5_32,cs-410,8,5,Topic,"00:02:17,090","00:02:21,840",32,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=137,"topics in data mining, and how are they"
cs-410_8_5_33,cs-410,8,5,Topic,"00:02:21,840","00:02:26,820",33,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=141,Now this involves discovery of topics
cs-410_8_5_34,cs-410,8,5,Topic,"00:02:26,820","00:02:32,910",34,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=146,also we want to discover topics in
cs-410_8_5_35,cs-410,8,5,Topic,"00:02:32,910","00:02:34,690",35,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=152,And then we can make a comparison.
cs-410_8_5_36,cs-410,8,5,Topic,"00:02:34,690","00:02:38,400",36,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=154,We might also be also interested in
cs-410_8_5_37,cs-410,8,5,Topic,"00:02:38,400","00:02:43,710",37,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=158,"some products like the iPhone 6,"
cs-410_8_5_38,cs-410,8,5,Topic,"00:02:43,710","00:02:48,360",38,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=163,And this involves discovering
cs-410_8_5_39,cs-410,8,5,Topic,"00:02:48,360","00:02:52,470",39,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=168,iPhone 6 and
cs-410_8_5_40,cs-410,8,5,Topic,"00:02:52,470","00:02:56,810",40,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=172,Or perhaps we're interested in knowing
cs-410_8_5_41,cs-410,8,5,Topic,"00:02:56,810","00:02:58,110",41,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=176,presidential election?
cs-410_8_5_42,cs-410,8,5,Topic,"00:02:59,780","00:03:04,800",42,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=179,And all these have to do with discovering
cs-410_8_5_43,cs-410,8,5,Topic,"00:03:04,800","00:03:08,680",43,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=184,and we're going to talk about a lot
cs-410_8_5_44,cs-410,8,5,Topic,"00:03:08,680","00:03:12,920",44,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=188,In general we can view a topic as
cs-410_8_5_45,cs-410,8,5,Topic,"00:03:12,920","00:03:17,830",45,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=192,So from text data we expect to
cs-410_8_5_46,cs-410,8,5,Topic,"00:03:17,830","00:03:22,650",46,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=197,then these topics generally provide
cs-410_8_5_47,cs-410,8,5,Topic,"00:03:22,650","00:03:25,690",47,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=202,And it tells us something about the world.
cs-410_8_5_48,cs-410,8,5,Topic,"00:03:25,690","00:03:28,230",48,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=205,"About a product, about a person etc."
cs-410_8_5_49,cs-410,8,5,Topic,"00:03:29,350","00:03:32,390",49,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=209,"Now when we have some non-text data,"
cs-410_8_5_50,cs-410,8,5,Topic,"00:03:32,390","00:03:36,420",50,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=212,then we can have more context for
cs-410_8_5_51,cs-410,8,5,Topic,"00:03:36,420","00:03:41,620",51,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=216,"For example, we might know the time"
cs-410_8_5_52,cs-410,8,5,Topic,"00:03:41,620","00:03:47,110",52,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=221,locations where the text
cs-410_8_5_53,cs-410,8,5,Topic,"00:03:47,110","00:03:52,945",53,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=227,"or the authors of the text, or"
cs-410_8_5_54,cs-410,8,5,Topic,"00:03:52,945","00:03:54,400",54,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=232,"All such meta data, or"
cs-410_8_5_55,cs-410,8,5,Topic,"00:03:54,400","00:03:59,610",55,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=234,context variables can be associated
cs-410_8_5_56,cs-410,8,5,Topic,"00:03:59,610","00:04:05,340",56,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=239,then we can use these context variables
cs-410_8_5_57,cs-410,8,5,Topic,"00:04:05,340","00:04:09,320",57,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=245,"For example, looking at topics over time,"
cs-410_8_5_58,cs-410,8,5,Topic,"00:04:09,320","00:04:14,290",58,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=249,"whether there's a trending topic, or"
cs-410_8_5_59,cs-410,8,5,Topic,"00:04:15,620","00:04:18,950",59,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=255,Soon you are looking at topics
cs-410_8_5_60,cs-410,8,5,Topic,"00:04:18,950","00:04:24,185",60,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=258,We might know some insights about
cs-410_8_5_61,cs-410,8,5,Topic,"00:04:26,150","00:04:29,900",61,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=266,So that's why mining
cs-410_8_5_62,cs-410,8,5,Topic,"00:04:29,900","00:04:34,540",62,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=269,"Now, let's look at the tasks"
cs-410_8_5_63,cs-410,8,5,Topic,"00:04:34,540","00:04:39,380",63,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=274,"In general, it would involve first"
cs-410_8_5_64,cs-410,8,5,Topic,"00:04:39,380","00:04:40,810",64,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=279,k topics.
cs-410_8_5_65,cs-410,8,5,Topic,"00:04:40,810","00:04:45,430",65,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=280,"And then we also would like to know, which"
cs-410_8_5_66,cs-410,8,5,Topic,"00:04:45,430","00:04:46,600",66,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=285,to what extent.
cs-410_8_5_67,cs-410,8,5,Topic,"00:04:46,600","00:04:52,970",67,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=286,"So for example, in document one, we"
cs-410_8_5_68,cs-410,8,5,Topic,"00:04:52,970","00:04:57,390",68,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=292,Topic 2 and
cs-410_8_5_69,cs-410,8,5,Topic,"00:04:58,890","00:05:00,712",69,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=298,"And other topics,"
cs-410_8_5_70,cs-410,8,5,Topic,"00:05:00,712","00:05:06,778",70,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=300,"Document two, on the other hand,"
cs-410_8_5_71,cs-410,8,5,Topic,"00:05:06,778","00:05:10,553",71,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=306,"but it did not cover Topic 1 at all, and"
cs-410_8_5_72,cs-410,8,5,Topic,"00:05:10,553","00:05:15,873",72,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=310,"it also covers Topic k to some extent,"
cs-410_8_5_73,cs-410,8,5,Topic,"00:05:15,873","00:05:19,995",73,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=315,So now you can see there
cs-410_8_5_74,cs-410,8,5,Topic,"00:05:19,995","00:05:25,760",74,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=319,"sub-tasks, the first is to discover k"
cs-410_8_5_75,cs-410,8,5,Topic,"00:05:25,760","00:05:27,140",75,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=325,What are these k topics?
cs-410_8_5_76,cs-410,8,5,Topic,"00:05:27,140","00:05:28,920",76,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=327,"Okay, major topics in the text they are."
cs-410_8_5_77,cs-410,8,5,Topic,"00:05:28,920","00:05:33,180",77,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=328,The second task is to figure out
cs-410_8_5_78,cs-410,8,5,Topic,"00:05:33,180","00:05:34,430",78,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=333,to what extent.
cs-410_8_5_79,cs-410,8,5,Topic,"00:05:34,430","00:05:37,810",79,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=334,"So more formally,"
cs-410_8_5_80,cs-410,8,5,Topic,"00:05:37,810","00:05:42,365",80,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=337,"First, we have, as input,"
cs-410_8_5_81,cs-410,8,5,Topic,"00:05:42,365","00:05:47,050",81,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=342,Here we can denote the text
cs-410_8_5_82,cs-410,8,5,Topic,"00:05:47,050","00:05:51,740",82,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=347,denote text article as d i.
cs-410_8_5_83,cs-410,8,5,Topic,"00:05:51,740","00:05:56,700",83,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=351,"And, we generally also need to have"
cs-410_8_5_84,cs-410,8,5,Topic,"00:05:56,700","00:06:01,730",84,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=356,But there may be techniques that can
cs-410_8_5_85,cs-410,8,5,Topic,"00:06:01,730","00:06:06,735",85,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=361,But in the techniques that we will
cs-410_8_5_86,cs-410,8,5,Topic,"00:06:06,735","00:06:12,340",86,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=366,"techniques, we often need to"
cs-410_8_5_87,cs-410,8,5,Topic,"00:06:14,580","00:06:19,860",87,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=374,Now the output would then be the k
cs-410_8_5_88,cs-410,8,5,Topic,"00:06:19,860","00:06:23,210",88,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=379,in order as theta sub
cs-410_8_5_89,cs-410,8,5,Topic,"00:06:24,540","00:06:29,820",89,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=384,Also we want to generate the coverage of
cs-410_8_5_90,cs-410,8,5,Topic,"00:06:29,820","00:06:32,518",90,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=389,this is denoted by pi sub i j.
cs-410_8_5_91,cs-410,8,5,Topic,"00:06:33,562","00:06:38,073",91,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=393,And pi sub ij is the probability
cs-410_8_5_92,cs-410,8,5,Topic,"00:06:38,073","00:06:41,290",92,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=398,covering topic theta sub j.
cs-410_8_5_93,cs-410,8,5,Topic,"00:06:41,290","00:06:45,450",93,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=401,"So obviously for each document, we have"
cs-410_8_5_94,cs-410,8,5,Topic,"00:06:45,450","00:06:47,830",94,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=405,"what extent the document covers,"
cs-410_8_5_95,cs-410,8,5,Topic,"00:06:48,930","00:06:53,610",95,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=408,And we can assume that these
cs-410_8_5_96,cs-410,8,5,Topic,"00:06:53,610","00:06:57,000",96,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=413,Because a document won't be able to cover
cs-410_8_5_97,cs-410,8,5,Topic,"00:06:57,000","00:07:02,520",97,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=417,other topics outside of the topics
cs-410_8_5_98,cs-410,8,5,Topic,"00:07:02,520","00:07:08,170",98,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=422,"So now, the question is, how do we define"
cs-410_8_5_99,cs-410,8,5,Topic,"00:07:08,170","00:07:11,500",99,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=428,Now this problem has not
cs-410_8_5_100,cs-410,8,5,Topic,"00:07:11,500","00:07:15,180",100,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=431,until we define what is exactly theta.
cs-410_8_5_101,cs-410,8,5,Topic,"00:07:16,970","00:07:19,381",101,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=436,"So in the next few lectures,"
cs-410_8_5_102,cs-410,8,5,Topic,"00:07:19,381","00:07:24,211",102,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=439,we're going to talk about
cs-410_8_5_103,cs-410,8,5,Topic,"00:07:24,211","00:07:34,211",103,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=444,[MUSIC]
cs-410_8_6_1,cs-410,8,6,Topic,"00:00:00,000","00:00:02,974",1,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=0,[MUSIC]
cs-410_8_6_2,cs-410,8,6,Topic,"00:00:07,749","00:00:11,320",2,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=7,This lecture is about topic mining and
cs-410_8_6_3,cs-410,8,6,Topic,"00:00:12,760","00:00:17,130",3,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=12,We're going to talk about
cs-410_8_6_4,cs-410,8,6,Topic,"00:00:17,130","00:00:20,700",4,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=17,This is a slide that you have
cs-410_8_6_5,cs-410,8,6,Topic,"00:00:20,700","00:00:25,120",5,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=20,where we define the task of
cs-410_8_6_6,cs-410,8,6,Topic,"00:00:25,120","00:00:30,700",6,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=25,"We also raised the question, how do"
cs-410_8_6_7,cs-410,8,6,Topic,"00:00:31,780","00:00:36,020",7,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=31,"So in this lecture, we're going to"
cs-410_8_6_8,cs-410,8,6,Topic,"00:00:36,020","00:00:37,780",8,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=36,that's our initial idea.
cs-410_8_6_9,cs-410,8,6,Topic,"00:00:37,780","00:00:40,980",9,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=37,Our idea here is defining
cs-410_8_6_10,cs-410,8,6,Topic,"00:00:42,020","00:00:44,200",10,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=42,A term can be a word or a phrase.
cs-410_8_6_11,cs-410,8,6,Topic,"00:00:45,240","00:00:49,500",11,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=45,"And in general,"
cs-410_8_6_12,cs-410,8,6,Topic,"00:00:49,500","00:00:54,200",12,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=49,So our first thought is just
cs-410_8_6_13,cs-410,8,6,Topic,"00:00:54,200","00:00:58,820",13,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=54,"For example, we might have terms"
cs-410_8_6_14,cs-410,8,6,Topic,"00:00:58,820","00:00:59,440",14,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=58,as you see here.
cs-410_8_6_15,cs-410,8,6,Topic,"00:00:59,440","00:01:02,603",15,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=59,"Now if we define a topic in this way,"
cs-410_8_6_16,cs-410,8,6,Topic,"00:01:02,603","00:01:09,200",16,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=62,we can then analyze the coverage
cs-410_8_6_17,cs-410,8,6,Topic,"00:01:09,200","00:01:10,510",17,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=69,"Here for example,"
cs-410_8_6_18,cs-410,8,6,Topic,"00:01:10,510","00:01:15,560",18,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=70,we might want to discover to what
cs-410_8_6_19,cs-410,8,6,Topic,"00:01:15,560","00:01:21,260",19,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=75,And we found that 30% of the content
cs-410_8_6_20,cs-410,8,6,Topic,"00:01:21,260","00:01:23,730",20,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=81,"And 12% is about the travel, etc."
cs-410_8_6_21,cs-410,8,6,Topic,"00:01:23,730","00:01:28,880",21,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=83,We might also discover document
cs-410_8_6_22,cs-410,8,6,Topic,"00:01:28,880","00:01:31,240",22,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=88,"So the coverage is zero, etc."
cs-410_8_6_23,cs-410,8,6,Topic,"00:01:32,630","00:01:39,040",23,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=92,"So now, of course,"
cs-410_8_6_24,cs-410,8,6,Topic,"00:01:39,040","00:01:42,900",24,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=99,"topic mining and analysis,"
cs-410_8_6_25,cs-410,8,6,Topic,"00:01:42,900","00:01:44,960",25,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=102,One is to discover the topics.
cs-410_8_6_26,cs-410,8,6,Topic,"00:01:44,960","00:01:48,110",26,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=104,And the second is to analyze coverage.
cs-410_8_6_27,cs-410,8,6,Topic,"00:01:48,110","00:01:51,550",27,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=108,So let's first think
cs-410_8_6_28,cs-410,8,6,Topic,"00:01:51,550","00:01:55,080",28,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=111,topics if we represent
cs-410_8_6_29,cs-410,8,6,Topic,"00:01:55,080","00:01:59,390",29,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=115,So that means we need to mine k
cs-410_8_6_30,cs-410,8,6,Topic,"00:02:01,050","00:02:04,080",30,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=121,"Now there are, of course,"
cs-410_8_6_31,cs-410,8,6,Topic,"00:02:05,670","00:02:08,617",31,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=125,And we're going to talk about
cs-410_8_6_32,cs-410,8,6,Topic,"00:02:08,617","00:02:10,750",32,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=128,which is also likely effective.
cs-410_8_6_33,cs-410,8,6,Topic,"00:02:10,750","00:02:11,641",33,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=130,"So first of all,"
cs-410_8_6_34,cs-410,8,6,Topic,"00:02:11,641","00:02:16,655",34,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=131,we're going to parse the text data in
cs-410_8_6_35,cs-410,8,6,Topic,"00:02:16,655","00:02:20,665",35,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=136,Here candidate terms can be words or
cs-410_8_6_36,cs-410,8,6,Topic,"00:02:20,665","00:02:25,475",36,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=140,Let's say the simplest solution is
cs-410_8_6_37,cs-410,8,6,Topic,"00:02:25,475","00:02:29,145",37,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=145,These words then become candidate topics.
cs-410_8_6_38,cs-410,8,6,Topic,"00:02:29,145","00:02:32,790",38,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=149,Then we're going to design a scoring
cs-410_8_6_39,cs-410,8,6,Topic,"00:02:32,790","00:02:33,650",39,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=152,is as a topic.
cs-410_8_6_40,cs-410,8,6,Topic,"00:02:35,460","00:02:37,150",40,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=155,So how can we design such a function?
cs-410_8_6_41,cs-410,8,6,Topic,"00:02:37,150","00:02:40,140",41,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=157,Well there are many things
cs-410_8_6_42,cs-410,8,6,Topic,"00:02:40,140","00:02:44,180",42,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=160,"For example, we can use pure statistics"
cs-410_8_6_43,cs-410,8,6,Topic,"00:02:45,550","00:02:48,820",43,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=165,"Intuitively, we would like to"
cs-410_8_6_44,cs-410,8,6,Topic,"00:02:48,820","00:02:53,950",44,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=168,meaning terms that can represent
cs-410_8_6_45,cs-410,8,6,Topic,"00:02:53,950","00:02:58,610",45,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=173,So that would mean we want
cs-410_8_6_46,cs-410,8,6,Topic,"00:02:58,610","00:03:03,990",46,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=178,"However, if we simply use the frequency"
cs-410_8_6_47,cs-410,8,6,Topic,"00:03:03,990","00:03:07,982",47,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=183,then the highest scored terms
cs-410_8_6_48,cs-410,8,6,Topic,"00:03:07,982","00:03:10,876",48,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=187,"functional terms like the, etc."
cs-410_8_6_49,cs-410,8,6,Topic,"00:03:10,876","00:03:13,510",49,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=190,Those terms occur very frequently English.
cs-410_8_6_50,cs-410,8,6,Topic,"00:03:14,650","00:03:19,340",50,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=194,So we also want to avoid having
cs-410_8_6_51,cs-410,8,6,Topic,"00:03:19,340","00:03:22,150",51,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=199,we want to penalize such words.
cs-410_8_6_52,cs-410,8,6,Topic,"00:03:22,150","00:03:26,480",52,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=202,"But in general, we would like to favor"
cs-410_8_6_53,cs-410,8,6,Topic,"00:03:26,480","00:03:28,020",53,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=206,not so frequent.
cs-410_8_6_54,cs-410,8,6,Topic,"00:03:28,020","00:03:34,030",54,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=208,So a particular approach could be based
cs-410_8_6_55,cs-410,8,6,Topic,"00:03:35,140","00:03:37,230",55,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=215,And TF stands for term frequency.
cs-410_8_6_56,cs-410,8,6,Topic,"00:03:37,230","00:03:40,420",56,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=217,IDF stands for inverse document frequency.
cs-410_8_6_57,cs-410,8,6,Topic,"00:03:40,420","00:03:43,310",57,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=220,We talked about some of these
cs-410_8_6_58,cs-410,8,6,Topic,"00:03:43,310","00:03:48,090",58,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=223,ideas in the lectures about
cs-410_8_6_59,cs-410,8,6,Topic,"00:03:48,090","00:03:50,766",59,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=228,"So these are statistical methods,"
cs-410_8_6_60,cs-410,8,6,Topic,"00:03:50,766","00:03:56,280",60,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=230,meaning that the function is
cs-410_8_6_61,cs-410,8,6,Topic,"00:03:56,280","00:03:59,080",61,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=236,So the scoring function
cs-410_8_6_62,cs-410,8,6,Topic,"00:03:59,080","00:04:02,890",62,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=239,"It can be applied to any language,"
cs-410_8_6_63,cs-410,8,6,Topic,"00:04:02,890","00:04:06,650",63,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=242,But when we apply such a approach
cs-410_8_6_64,cs-410,8,6,Topic,"00:04:06,650","00:04:12,020",64,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=246,we might also be able to leverage
cs-410_8_6_65,cs-410,8,6,Topic,"00:04:12,020","00:04:16,815",65,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=252,"For example, in news we might favor"
cs-410_8_6_66,cs-410,8,6,Topic,"00:04:16,815","00:04:21,340",66,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=256,We might want to favor title
cs-410_8_6_67,cs-410,8,6,Topic,"00:04:21,340","00:04:26,100",67,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=261,use the title to describe
cs-410_8_6_68,cs-410,8,6,Topic,"00:04:27,750","00:04:32,480",68,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=267,"If we're dealing with tweets,"
cs-410_8_6_69,cs-410,8,6,Topic,"00:04:32,480","00:04:37,430",69,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=272,which are invented to denote topics.
cs-410_8_6_70,cs-410,8,6,Topic,"00:04:37,430","00:04:43,630",70,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=277,"So naturally, hashtags can be good"
cs-410_8_6_71,cs-410,8,6,Topic,"00:04:44,780","00:04:50,430",71,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=284,"Anyway, after we have this design"
cs-410_8_6_72,cs-410,8,6,Topic,"00:04:50,430","00:04:55,960",72,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=290,the k topical terms by simply picking
cs-410_8_6_73,cs-410,8,6,Topic,"00:04:55,960","00:04:57,240",73,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=295,"Now, of course,"
cs-410_8_6_74,cs-410,8,6,Topic,"00:04:57,240","00:05:02,040",74,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=297,we might encounter situation where the
cs-410_8_6_75,cs-410,8,6,Topic,"00:05:02,040","00:05:06,910",75,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=302,"They're semantically similar, or"
cs-410_8_6_76,cs-410,8,6,Topic,"00:05:06,910","00:05:08,860",76,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=306,So that's not desirable.
cs-410_8_6_77,cs-410,8,6,Topic,"00:05:08,860","00:05:12,280",77,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=308,So we also want to have coverage over
cs-410_8_6_78,cs-410,8,6,Topic,"00:05:12,280","00:05:15,080",78,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=312,So we would like to remove redundancy.
cs-410_8_6_79,cs-410,8,6,Topic,"00:05:15,080","00:05:19,600",79,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=315,And one way to do that is
cs-410_8_6_80,cs-410,8,6,Topic,"00:05:19,600","00:05:24,450",80,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=319,which is sometimes called a maximal
cs-410_8_6_81,cs-410,8,6,Topic,"00:05:24,450","00:05:29,330",81,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=324,"Basically, the idea is to go down"
cs-410_8_6_82,cs-410,8,6,Topic,"00:05:29,330","00:05:34,380",82,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=329,function and gradually take terms
cs-410_8_6_83,cs-410,8,6,Topic,"00:05:34,380","00:05:36,840",83,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=334,"The first term, of course, will be picked."
cs-410_8_6_84,cs-410,8,6,Topic,"00:05:36,840","00:05:40,500",84,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=336,"When we pick the next term, we're"
cs-410_8_6_85,cs-410,8,6,Topic,"00:05:40,500","00:05:45,120",85,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=340,been picked and try to avoid
cs-410_8_6_86,cs-410,8,6,Topic,"00:05:45,120","00:05:50,610",86,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=345,So while we are considering
cs-410_8_6_87,cs-410,8,6,Topic,"00:05:50,610","00:05:54,260",87,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=350,we are also considering
cs-410_8_6_88,cs-410,8,6,Topic,"00:05:54,260","00:05:56,970",88,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=354,with respect to the terms
cs-410_8_6_89,cs-410,8,6,Topic,"00:05:58,090","00:06:02,933",89,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=358,"And with some thresholding,"
cs-410_8_6_90,cs-410,8,6,Topic,"00:06:02,933","00:06:08,330",90,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=362,the redundancy removal and
cs-410_8_6_91,cs-410,8,6,Topic,"00:06:08,330","00:06:11,990",91,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=368,"Okay, so"
cs-410_8_6_92,cs-410,8,6,Topic,"00:06:11,990","00:06:17,550",92,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=371,And those can be regarded as the topics
cs-410_8_6_93,cs-410,8,6,Topic,"00:06:17,550","00:06:21,980",93,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=377,"Next, let's think about how we're going"
cs-410_8_6_94,cs-410,8,6,Topic,"00:06:23,430","00:06:26,971",94,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=383,"So looking at this picture,"
cs-410_8_6_95,cs-410,8,6,Topic,"00:06:26,971","00:06:28,130",95,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=386,these topics.
cs-410_8_6_96,cs-410,8,6,Topic,"00:06:28,130","00:06:31,190",96,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=388,And now suppose you are give a document.
cs-410_8_6_97,cs-410,8,6,Topic,"00:06:31,190","00:06:35,040",97,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=391,How should we pick out coverage
cs-410_8_6_98,cs-410,8,6,Topic,"00:06:36,660","00:06:42,690",98,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=396,"Well, one approach can be to simply"
cs-410_8_6_99,cs-410,8,6,Topic,"00:06:42,690","00:06:46,770",99,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=402,"So for example, sports might have occurred"
cs-410_8_6_100,cs-410,8,6,Topic,"00:06:46,770","00:06:49,570",100,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=406,"travel occurred twice, etc."
cs-410_8_6_101,cs-410,8,6,Topic,"00:06:49,570","00:06:54,420",101,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=409,And then we can just normalize these
cs-410_8_6_102,cs-410,8,6,Topic,"00:06:54,420","00:06:56,570",102,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=414,probability for each topic.
cs-410_8_6_103,cs-410,8,6,Topic,"00:06:56,570","00:07:01,780",103,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=416,"So in general, the formula would"
cs-410_8_6_104,cs-410,8,6,Topic,"00:07:01,780","00:07:05,240",104,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=421,all the terms that represent the topics.
cs-410_8_6_105,cs-410,8,6,Topic,"00:07:05,240","00:07:10,220",105,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=425,And then simply normalize them so
cs-410_8_6_106,cs-410,8,6,Topic,"00:07:10,220","00:07:13,480",106,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=430,topic in the document would add to one.
cs-410_8_6_107,cs-410,8,6,Topic,"00:07:15,120","00:07:21,200",107,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=435,This forms a distribution of the topics
cs-410_8_6_108,cs-410,8,6,Topic,"00:07:21,200","00:07:26,560",108,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=441,of different topics in the document.
cs-410_8_6_109,cs-410,8,6,Topic,"00:07:26,560","00:07:30,100",109,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=446,"Now, as always,"
cs-410_8_6_110,cs-410,8,6,Topic,"00:07:30,100","00:07:34,940",110,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=450,"solving problem, we have to ask"
cs-410_8_6_111,cs-410,8,6,Topic,"00:07:34,940","00:07:37,180",111,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=454,Or is this the best way
cs-410_8_6_112,cs-410,8,6,Topic,"00:07:38,690","00:07:41,110",112,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=458,So now let's examine this approach.
cs-410_8_6_113,cs-410,8,6,Topic,"00:07:41,110","00:07:44,940",113,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=461,"In general,"
cs-410_8_6_114,cs-410,8,6,Topic,"00:07:46,010","00:07:50,280",114,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=466,by using actual data sets and
cs-410_8_6_115,cs-410,8,6,Topic,"00:07:52,360","00:07:57,340",115,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=472,"Well, in this case let's take"
cs-410_8_6_116,cs-410,8,6,Topic,"00:07:57,340","00:08:03,260",116,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=477,And we have a text document that's
cs-410_8_6_117,cs-410,8,6,Topic,"00:08:04,800","00:08:07,700",117,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=484,"So in terms of the content,"
cs-410_8_6_118,cs-410,8,6,Topic,"00:08:08,950","00:08:14,600",118,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=488,But if we simply count these
cs-410_8_6_119,cs-410,8,6,Topic,"00:08:14,600","00:08:19,070",119,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=494,we will find that the word sports
cs-410_8_6_120,cs-410,8,6,Topic,"00:08:19,070","00:08:21,420",120,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=499,even though the content
cs-410_8_6_121,cs-410,8,6,Topic,"00:08:22,520","00:08:25,750",121,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=502,So the count of sports is zero.
cs-410_8_6_122,cs-410,8,6,Topic,"00:08:25,750","00:08:31,939",122,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=505,That means the coverage of sports
cs-410_8_6_123,cs-410,8,6,Topic,"00:08:31,939","00:08:36,723",123,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=511,"Now of course,"
cs-410_8_6_124,cs-410,8,6,Topic,"00:08:36,723","00:08:40,980",124,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=516,the document and
cs-410_8_6_125,cs-410,8,6,Topic,"00:08:40,980","00:08:42,230",125,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=520,And that's okay.
cs-410_8_6_126,cs-410,8,6,Topic,"00:08:42,230","00:08:47,257",126,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=522,But sports certainly is not okay because
cs-410_8_6_127,cs-410,8,6,Topic,"00:08:47,257","00:08:49,150",127,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=527,So this estimate has problem.
cs-410_8_6_128,cs-410,8,6,Topic,"00:08:50,880","00:08:56,050",128,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=530,"What's worse, the term travel"
cs-410_8_6_129,cs-410,8,6,Topic,"00:08:56,050","00:08:59,940",129,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=536,So when we estimate the coverage
cs-410_8_6_130,cs-410,8,6,Topic,"00:08:59,940","00:09:02,140",130,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=539,we have got a non-zero count.
cs-410_8_6_131,cs-410,8,6,Topic,"00:09:02,140","00:09:05,000",131,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=542,So its estimated coverage
cs-410_8_6_132,cs-410,8,6,Topic,"00:09:05,000","00:09:07,770",132,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=545,So this obviously is also not desirable.
cs-410_8_6_133,cs-410,8,6,Topic,"00:09:08,800","00:09:13,910",133,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=548,So this simple example illustrates
cs-410_8_6_134,cs-410,8,6,Topic,"00:09:13,910","00:09:17,704",134,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=553,"First, when we count what"
cs-410_8_6_135,cs-410,8,6,Topic,"00:09:17,704","00:09:20,460",135,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=557,we also need to consider related words.
cs-410_8_6_136,cs-410,8,6,Topic,"00:09:20,460","00:09:24,285",136,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=560,We can't simply just count
cs-410_8_6_137,cs-410,8,6,Topic,"00:09:24,285","00:09:26,440",137,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=564,"In this case, it did not occur at all."
cs-410_8_6_138,cs-410,8,6,Topic,"00:09:26,440","00:09:31,340",138,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=566,But there are many related words
cs-410_8_6_139,cs-410,8,6,Topic,"00:09:31,340","00:09:33,860",139,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=571,So we need to count
cs-410_8_6_140,cs-410,8,6,Topic,"00:09:33,860","00:09:38,910",140,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=573,The second problem is that a word
cs-410_8_6_141,cs-410,8,6,Topic,"00:09:38,910","00:09:42,900",141,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=578,So here it probably means
cs-410_8_6_142,cs-410,8,6,Topic,"00:09:42,900","00:09:47,600",142,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=582,we can imagine it might also
cs-410_8_6_143,cs-410,8,6,Topic,"00:09:47,600","00:09:53,120",143,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=587,"So in that case, the star might actually"
cs-410_8_6_144,cs-410,8,6,Topic,"00:09:54,210","00:09:56,360",144,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=594,So we need to deal with that as well.
cs-410_8_6_145,cs-410,8,6,Topic,"00:09:56,360","00:10:02,325",145,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=596,"Finally, a main restriction of this"
cs-410_8_6_146,cs-410,8,6,Topic,"00:10:02,325","00:10:08,520",146,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=602,"term to describe the topic, so it cannot"
cs-410_8_6_147,cs-410,8,6,Topic,"00:10:08,520","00:10:12,040",147,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=608,"For example, a very specialized"
cs-410_8_6_148,cs-410,8,6,Topic,"00:10:12,040","00:10:15,210",148,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=612,describe by using just a word or
cs-410_8_6_149,cs-410,8,6,Topic,"00:10:15,210","00:10:17,150",149,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=615,We need to use more words.
cs-410_8_6_150,cs-410,8,6,Topic,"00:10:17,150","00:10:20,760",150,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=617,So this example illustrates
cs-410_8_6_151,cs-410,8,6,Topic,"00:10:20,760","00:10:23,310",151,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=620,this approach of treating a term as topic.
cs-410_8_6_152,cs-410,8,6,Topic,"00:10:23,310","00:10:26,725",152,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=623,"First, it lacks expressive power."
cs-410_8_6_153,cs-410,8,6,Topic,"00:10:26,725","00:10:30,729",153,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=626,Meaning that it can only represent
cs-410_8_6_154,cs-410,8,6,Topic,"00:10:30,729","00:10:36,035",154,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=630,it cannot represent the complicated topics
cs-410_8_6_155,cs-410,8,6,Topic,"00:10:37,055","00:10:40,660",155,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=637,"Second, it's incomplete"
cs-410_8_6_156,cs-410,8,6,Topic,"00:10:40,660","00:10:44,930",156,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=640,meaning that the topic itself
cs-410_8_6_157,cs-410,8,6,Topic,"00:10:44,930","00:10:48,820",157,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=644,It does not suggest what other
cs-410_8_6_158,cs-410,8,6,Topic,"00:10:48,820","00:10:52,370",158,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=648,"Even if we're talking about sports,"
cs-410_8_6_159,cs-410,8,6,Topic,"00:10:52,370","00:10:57,060",159,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=652,So it does not allow us to easily
cs-410_8_6_160,cs-410,8,6,Topic,"00:10:57,060","00:10:59,200",160,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=657,conversion to coverage of this topic.
cs-410_8_6_161,cs-410,8,6,Topic,"00:10:59,200","00:11:02,410",161,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=659,"Finally, there is this problem"
cs-410_8_6_162,cs-410,8,6,Topic,"00:11:02,410","00:11:05,862",162,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=662,A topical term or
cs-410_8_6_163,cs-410,8,6,Topic,"00:11:05,862","00:11:08,540",163,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=665,"For example,"
cs-410_8_6_164,cs-410,8,6,Topic,"00:11:10,570","00:11:14,125",164,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=670,"So in the next lecture,"
cs-410_8_6_165,cs-410,8,6,Topic,"00:11:14,125","00:11:18,806",165,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=674,about how to solve
cs-410_8_6_166,cs-410,8,6,Topic,"00:11:18,806","00:11:28,806",166,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=678,[MUSIC]
cs-410_8_7_1,cs-410,8,7,Topic,"00:00:06,750","00:00:12,040",1,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=6,This lecture is about Probabilistic Topic
cs-410_8_7_2,cs-410,8,7,Topic,"00:00:13,350","00:00:14,110",2,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=13,"In this lecture,"
cs-410_8_7_3,cs-410,8,7,Topic,"00:00:14,110","00:00:16,909",3,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=14,we're going to continue talking
cs-410_8_7_4,cs-410,8,7,Topic,"00:00:18,190","00:00:20,490",4,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=18,We're going to introduce
cs-410_8_7_5,cs-410,8,7,Topic,"00:00:22,410","00:00:26,140",5,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=22,So this is a slide that
cs-410_8_7_6,cs-410,8,7,Topic,"00:00:26,140","00:00:30,640",6,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=26,where we discussed the problems
cs-410_8_7_7,cs-410,8,7,Topic,"00:00:30,640","00:00:35,370",7,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=30,"So, to solve these problems"
cs-410_8_7_8,cs-410,8,7,Topic,"00:00:35,370","00:00:37,950",8,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=35,more words to describe the topic.
cs-410_8_7_9,cs-410,8,7,Topic,"00:00:37,950","00:00:43,110",9,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=37,And this will address the problem
cs-410_8_7_10,cs-410,8,7,Topic,"00:00:43,110","00:00:45,040",10,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=43,When we have more words that we
cs-410_8_7_11,cs-410,8,7,Topic,"00:00:45,040","00:00:49,880",11,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=45,that we can describe complicated topics.
cs-410_8_7_12,cs-410,8,7,Topic,"00:00:49,880","00:00:54,030",12,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=49,To address the second problem we
cs-410_8_7_13,cs-410,8,7,Topic,"00:00:54,030","00:00:59,140",13,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=54,This is what allows you to distinguish
cs-410_8_7_14,cs-410,8,7,Topic,"00:00:59,140","00:01:04,600",14,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=59,to introduce semantically
cs-410_8_7_15,cs-410,8,7,Topic,"00:01:04,600","00:01:09,240",15,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=64,"Finally, to solve the problem of"
cs-410_8_7_16,cs-410,8,7,Topic,"00:01:09,240","00:01:14,700",16,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=69,"ambiguous word, so"
cs-410_8_7_17,cs-410,8,7,Topic,"00:01:15,720","00:01:21,060",17,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=75,It turns out that all these can be done
cs-410_8_7_18,cs-410,8,7,Topic,"00:01:21,060","00:01:25,520",18,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=81,And that's why we're going to spend a lot
cs-410_8_7_19,cs-410,8,7,Topic,"00:01:25,520","00:01:28,130",19,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=85,"So the basic idea here is that,"
cs-410_8_7_20,cs-410,8,7,Topic,"00:01:28,130","00:01:32,600",20,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=88,improve the replantation of
cs-410_8_7_21,cs-410,8,7,Topic,"00:01:32,600","00:01:35,650",21,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=92,So what you see now is
cs-410_8_7_22,cs-410,8,7,Topic,"00:01:35,650","00:01:40,730",22,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=95,"Where we replanted each topic, it was just"
cs-410_8_7_23,cs-410,8,7,Topic,"00:01:40,730","00:01:45,240",23,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=100,But now we're going to use a word
cs-410_8_7_24,cs-410,8,7,Topic,"00:01:45,240","00:01:47,110",24,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=105,So here you see that for sports.
cs-410_8_7_25,cs-410,8,7,Topic,"00:01:47,110","00:01:50,220",25,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=107,We're going to use
cs-410_8_7_26,cs-410,8,7,Topic,"00:01:50,220","00:01:53,160",26,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=110,theoretical speaking all
cs-410_8_7_27,cs-410,8,7,Topic,"00:01:54,650","00:01:59,150",27,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=114,"So for example, the high"
cs-410_8_7_28,cs-410,8,7,Topic,"00:01:59,150","00:02:03,880",28,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=119,"game, basketball,"
cs-410_8_7_29,cs-410,8,7,Topic,"00:02:03,880","00:02:06,100",29,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=123,These are sports related terms.
cs-410_8_7_30,cs-410,8,7,Topic,"00:02:06,100","00:02:10,150",30,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=126,And of course it would also give
cs-410_8_7_31,cs-410,8,7,Topic,"00:02:10,150","00:02:15,430",31,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=130,like Trouble which might be
cs-410_8_7_32,cs-410,8,7,Topic,"00:02:15,430","00:02:17,420",32,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=135,not so much related to topic.
cs-410_8_7_33,cs-410,8,7,Topic,"00:02:18,900","00:02:23,030",33,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=138,In general we can imagine a non
cs-410_8_7_34,cs-410,8,7,Topic,"00:02:23,030","00:02:27,890",34,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=143,And some words that are not read and
cs-410_8_7_35,cs-410,8,7,Topic,"00:02:27,890","00:02:29,830",35,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=147,And these probabilities will sum to one.
cs-410_8_7_36,cs-410,8,7,Topic,"00:02:31,780","00:02:34,500",36,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=151,So that it forms a distribution
cs-410_8_7_37,cs-410,8,7,Topic,"00:02:36,650","00:02:41,440",37,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=156,"Now intuitively, this distribution"
cs-410_8_7_38,cs-410,8,7,Topic,"00:02:41,440","00:02:46,780",38,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=161,"words from the distribution, we tended"
cs-410_8_7_39,cs-410,8,7,Topic,"00:02:48,470","00:02:53,236",39,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=168,"You can also see, as a very special case,"
cs-410_8_7_40,cs-410,8,7,Topic,"00:02:53,236","00:02:57,387",40,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=173,is concentrated in entirely on
cs-410_8_7_41,cs-410,8,7,Topic,"00:02:57,387","00:03:01,670",41,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=177,And this basically degenerates
cs-410_8_7_42,cs-410,8,7,Topic,"00:03:01,670","00:03:03,270",42,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=181,of a topic was just one word.
cs-410_8_7_43,cs-410,8,7,Topic,"00:03:04,640","00:03:10,420",43,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=184,"But as a distribution,"
cs-410_8_7_44,cs-410,8,7,Topic,"00:03:10,420","00:03:13,980",44,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=190,"in general,"
cs-410_8_7_45,cs-410,8,7,Topic,"00:03:13,980","00:03:17,970",45,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=193,can model several differences
cs-410_8_7_46,cs-410,8,7,Topic,"00:03:17,970","00:03:24,500",46,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=197,Similarly we can model Travel and Science
cs-410_8_7_47,cs-410,8,7,Topic,"00:03:24,500","00:03:30,120",47,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=204,In the distribution for Travel we see top
cs-410_8_7_48,cs-410,8,7,Topic,"00:03:31,670","00:03:36,110",48,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=211,"Whereas in Science we see scientist,"
cs-410_8_7_49,cs-410,8,7,Topic,"00:03:36,110","00:03:39,820",49,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=216,"genomics, and, you know,"
cs-410_8_7_50,cs-410,8,7,Topic,"00:03:39,820","00:03:43,260",50,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=219,Now that doesn't mean sports related terms
cs-410_8_7_51,cs-410,8,7,Topic,"00:03:43,260","00:03:46,330",51,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=223,will necessarily have zero
cs-410_8_7_52,cs-410,8,7,Topic,"00:03:46,330","00:03:51,860",52,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=226,In general we can imagine all of these
cs-410_8_7_53,cs-410,8,7,Topic,"00:03:51,860","00:03:55,250",53,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=231,It's just that for a particular
cs-410_8_7_54,cs-410,8,7,Topic,"00:03:55,250","00:03:56,620",54,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=235,very small probabilities.
cs-410_8_7_55,cs-410,8,7,Topic,"00:03:58,200","00:04:02,770",55,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=238,Now you can also see there are some
cs-410_8_7_56,cs-410,8,7,Topic,"00:04:02,770","00:04:07,600",56,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=242,When I say shared it just means even
cs-410_8_7_57,cs-410,8,7,Topic,"00:04:07,600","00:04:10,990",57,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=247,you can still see one word
cs-410_8_7_58,cs-410,8,7,Topic,"00:04:10,990","00:04:13,140",58,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=250,In this case I mark them in black.
cs-410_8_7_59,cs-410,8,7,Topic,"00:04:13,140","00:04:17,110",59,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=253,"So you can see travel, for example,"
cs-410_8_7_60,cs-410,8,7,Topic,"00:04:17,110","00:04:19,420",60,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=257,with different probabilities.
cs-410_8_7_61,cs-410,8,7,Topic,"00:04:19,420","00:04:23,237",61,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=259,It has the highest probability for
cs-410_8_7_62,cs-410,8,7,Topic,"00:04:23,237","00:04:29,050",62,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=263,But with much smaller probabilities for
cs-410_8_7_63,cs-410,8,7,Topic,"00:04:29,050","00:04:32,450",63,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=269,"And similarly, you can see a Star"
cs-410_8_7_64,cs-410,8,7,Topic,"00:04:32,450","00:04:35,420",64,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=272,Science with reasonably
cs-410_8_7_65,cs-410,8,7,Topic,"00:04:35,420","00:04:39,690",65,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=275,Because they might be actually
cs-410_8_7_66,cs-410,8,7,Topic,"00:04:39,690","00:04:43,420",66,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=279,So with this replantation it addresses the
cs-410_8_7_67,cs-410,8,7,Topic,"00:04:43,420","00:04:46,750",67,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=283,"First, it now uses multiple"
cs-410_8_7_68,cs-410,8,7,Topic,"00:04:46,750","00:04:50,700",68,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=286,So it allows us to describe
cs-410_8_7_69,cs-410,8,7,Topic,"00:04:50,700","00:04:53,400",69,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=290,"Second, it assigns weights to terms."
cs-410_8_7_70,cs-410,8,7,Topic,"00:04:53,400","00:04:57,060",70,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=293,So now we can model several
cs-410_8_7_71,cs-410,8,7,Topic,"00:04:57,060","00:05:02,390",71,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=297,And you can bring in related
cs-410_8_7_72,cs-410,8,7,Topic,"00:05:02,390","00:05:07,930",72,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=302,"Third, because we have probabilities for"
cs-410_8_7_73,cs-410,8,7,Topic,"00:05:07,930","00:05:12,210",73,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=307,we can disintegrate the sense of word.
cs-410_8_7_74,cs-410,8,7,Topic,"00:05:12,210","00:05:16,930",74,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=312,In the text to decode
cs-410_8_7_75,cs-410,8,7,Topic,"00:05:16,930","00:05:22,480",75,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=316,to address all these three problems with
cs-410_8_7_76,cs-410,8,7,Topic,"00:05:22,480","00:05:27,650",76,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=322,So now of course our problem definition
cs-410_8_7_77,cs-410,8,7,Topic,"00:05:27,650","00:05:32,090",77,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=327,The slight is very similar to what
cs-410_8_7_78,cs-410,8,7,Topic,"00:05:32,090","00:05:34,920",78,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=332,added refinement for what our topic is.
cs-410_8_7_79,cs-410,8,7,Topic,"00:05:34,920","00:05:41,180",79,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=334,"Now each topic is word distribution,"
cs-410_8_7_80,cs-410,8,7,Topic,"00:05:41,180","00:05:45,460",80,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=341,that all the probabilities should sum to
cs-410_8_7_81,cs-410,8,7,Topic,"00:05:45,460","00:05:47,640",81,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=345,So you see a constraint here.
cs-410_8_7_82,cs-410,8,7,Topic,"00:05:47,640","00:05:53,060",82,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=347,And we still have another constraint
cs-410_8_7_83,cs-410,8,7,Topic,"00:05:53,060","00:05:58,180",83,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=353,So all the Pi sub ij's must sum to one for
cs-410_8_7_84,cs-410,8,7,Topic,"00:05:59,620","00:06:01,250",84,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=359,So how do we solve this problem?
cs-410_8_7_85,cs-410,8,7,Topic,"00:06:01,250","00:06:05,470",85,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=361,"Well, let's look at this problem"
cs-410_8_7_86,cs-410,8,7,Topic,"00:06:05,470","00:06:07,560",86,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=365,So we clearly specify it's input and
cs-410_8_7_87,cs-410,8,7,Topic,"00:06:07,560","00:06:11,190",87,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=367,output and
cs-410_8_7_88,cs-410,8,7,Topic,"00:06:11,190","00:06:12,920",88,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=371,Input of course is our text data.
cs-410_8_7_89,cs-410,8,7,Topic,"00:06:12,920","00:06:18,620",89,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=372,C is our collection but we also generally
cs-410_8_7_90,cs-410,8,7,Topic,"00:06:18,620","00:06:22,940",90,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=378,Or we hypothesize a number and
cs-410_8_7_91,cs-410,8,7,Topic,"00:06:22,940","00:06:27,820",91,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=382,even though we don't know the exact
cs-410_8_7_92,cs-410,8,7,Topic,"00:06:27,820","00:06:32,960",92,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=387,And V is the vocabulary that has
cs-410_8_7_93,cs-410,8,7,Topic,"00:06:32,960","00:06:38,880",93,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=392,units would be treated as
cs-410_8_7_94,cs-410,8,7,Topic,"00:06:38,880","00:06:44,780",94,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=398,In most cases we'll use words
cs-410_8_7_95,cs-410,8,7,Topic,"00:06:44,780","00:06:46,429",95,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=404,And that means each word is a unique.
cs-410_8_7_96,cs-410,8,7,Topic,"00:06:47,610","00:06:53,560",96,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=407,Now the output would consist of as first
cs-410_8_7_97,cs-410,8,7,Topic,"00:06:53,560","00:06:55,280",97,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=413,Each theta I is a word distribution.
cs-410_8_7_98,cs-410,8,7,Topic,"00:06:56,430","00:07:02,860",98,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=416,And we also want to know the coverage
cs-410_8_7_99,cs-410,8,7,Topic,"00:07:02,860","00:07:03,520",99,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=422,So that's.
cs-410_8_7_100,cs-410,8,7,Topic,"00:07:03,520","00:07:06,250",100,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=423,That the same pi ijs
cs-410_8_7_101,cs-410,8,7,Topic,"00:07:07,470","00:07:13,460",101,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=427,So given a set of text data we would
cs-410_8_7_102,cs-410,8,7,Topic,"00:07:13,460","00:07:16,980",102,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=433,all these coverages as you
cs-410_8_7_103,cs-410,8,7,Topic,"00:07:18,130","00:07:21,520",103,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=438,Now of course there may be many
cs-410_8_7_104,cs-410,8,7,Topic,"00:07:21,520","00:07:24,670",104,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=441,"In theory, you can write the [INAUDIBLE]"
cs-410_8_7_105,cs-410,8,7,Topic,"00:07:24,670","00:07:27,050",105,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=444,but here we're going to introduce
cs-410_8_7_106,cs-410,8,7,Topic,"00:07:27,050","00:07:32,200",106,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=447,a general way of solving this
cs-410_8_7_107,cs-410,8,7,Topic,"00:07:32,200","00:07:35,770",107,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=452,"And this is, in fact,"
cs-410_8_7_108,cs-410,8,7,Topic,"00:07:35,770","00:07:41,390",108,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=455,it's a principle way of using statistical
cs-410_8_7_109,cs-410,8,7,Topic,"00:07:41,390","00:07:46,190",109,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=461,And here I dimmed the picture
cs-410_8_7_110,cs-410,8,7,Topic,"00:07:46,190","00:07:49,470",110,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=466,in order to show the generation process.
cs-410_8_7_111,cs-410,8,7,Topic,"00:07:49,470","00:07:55,960",111,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=469,So the idea of this approach is actually
cs-410_8_7_112,cs-410,8,7,Topic,"00:07:55,960","00:08:02,070",112,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=475,So we design a probabilistic model
cs-410_8_7_113,cs-410,8,7,Topic,"00:08:02,070","00:08:04,180",113,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=482,"Of course,"
cs-410_8_7_114,cs-410,8,7,Topic,"00:08:04,180","00:08:08,060",114,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=484,The actual data aren't
cs-410_8_7_115,cs-410,8,7,Topic,"00:08:08,060","00:08:11,930",115,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=488,So that gave us a probability
cs-410_8_7_116,cs-410,8,7,Topic,"00:08:11,930","00:08:13,980",116,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=491,that you are seeing on this slide.
cs-410_8_7_117,cs-410,8,7,Topic,"00:08:13,980","00:08:18,840",117,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=493,Given a particular model and
cs-410_8_7_118,cs-410,8,7,Topic,"00:08:18,840","00:08:22,040",118,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=498,So this template of actually consists of
cs-410_8_7_119,cs-410,8,7,Topic,"00:08:22,040","00:08:24,380",119,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=502,all the parameters that
cs-410_8_7_120,cs-410,8,7,Topic,"00:08:24,380","00:08:27,780",120,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=504,And these parameters in general
cs-410_8_7_121,cs-410,8,7,Topic,"00:08:27,780","00:08:29,370",121,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=507,the probability risk model.
cs-410_8_7_122,cs-410,8,7,Topic,"00:08:29,370","00:08:32,530",122,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=509,Meaning that if you set these
cs-410_8_7_123,cs-410,8,7,Topic,"00:08:32,530","00:08:36,820",123,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=512,it will give some data points
cs-410_8_7_124,cs-410,8,7,Topic,"00:08:36,820","00:08:39,910",124,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=516,"Now in this case of course,"
cs-410_8_7_125,cs-410,8,7,Topic,"00:08:39,910","00:08:44,100",125,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=519,more precisely topic mining problem
cs-410_8_7_126,cs-410,8,7,Topic,"00:08:44,100","00:08:49,450",126,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=524,First of all we have theta i's which
cs-410_8_7_127,cs-410,8,7,Topic,"00:08:49,450","00:08:52,070",127,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=529,a set of pis for each document.
cs-410_8_7_128,cs-410,8,7,Topic,"00:08:52,070","00:08:58,980",128,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=532,"And since we have n documents, so we have"
cs-410_8_7_129,cs-410,8,7,Topic,"00:08:58,980","00:09:01,430",129,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=538,The pi values will sum to one.
cs-410_8_7_130,cs-410,8,7,Topic,"00:09:01,430","00:09:06,370",130,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=541,So this is to say that we
cs-410_8_7_131,cs-410,8,7,Topic,"00:09:06,370","00:09:10,640",131,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=546,have these word distributions and
cs-410_8_7_132,cs-410,8,7,Topic,"00:09:10,640","00:09:18,010",132,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=550,And then we can see how we can generate
cs-410_8_7_133,cs-410,8,7,Topic,"00:09:18,010","00:09:21,950",133,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=558,So how do we model the data in this way?
cs-410_8_7_134,cs-410,8,7,Topic,"00:09:21,950","00:09:25,280",134,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=561,And we assume that the data
cs-410_8_7_135,cs-410,8,7,Topic,"00:09:25,280","00:09:29,530",135,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=565,drawn from such a model that
cs-410_8_7_136,cs-410,8,7,Topic,"00:09:29,530","00:09:31,290",136,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=569,Now one interesting question here is to
cs-410_8_7_137,cs-410,8,7,Topic,"00:09:32,320","00:09:35,080",137,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=572,think about how many
cs-410_8_7_138,cs-410,8,7,Topic,"00:09:35,080","00:09:41,360",138,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=575,Now obviously we can already see
cs-410_8_7_139,cs-410,8,7,Topic,"00:09:41,360","00:09:42,140",139,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=581,For pi's.
cs-410_8_7_140,cs-410,8,7,Topic,"00:09:42,140","00:09:44,530",140,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=582,We also see k theta i's.
cs-410_8_7_141,cs-410,8,7,Topic,"00:09:44,530","00:09:49,110",141,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=584,But each theta i is actually a set
cs-410_8_7_142,cs-410,8,7,Topic,"00:09:49,110","00:09:51,580",142,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=589,It's a distribution of words.
cs-410_8_7_143,cs-410,8,7,Topic,"00:09:51,580","00:09:54,000",143,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=591,So I leave this as an exercise for
cs-410_8_7_144,cs-410,8,7,Topic,"00:09:54,000","00:09:59,980",144,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=594,you to figure out exactly how
cs-410_8_7_145,cs-410,8,7,Topic,"00:09:59,980","00:10:04,690",145,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=599,Now once we set up the model then
cs-410_8_7_146,cs-410,8,7,Topic,"00:10:04,690","00:10:07,900",146,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=604,Meaning that we can
cs-410_8_7_147,cs-410,8,7,Topic,"00:10:07,900","00:10:11,010",147,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=607,infer the parameters based on the data.
cs-410_8_7_148,cs-410,8,7,Topic,"00:10:11,010","00:10:14,930",148,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=611,In other words we would like to
cs-410_8_7_149,cs-410,8,7,Topic,"00:10:14,930","00:10:20,330",149,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=614,Until we give our data set
cs-410_8_7_150,cs-410,8,7,Topic,"00:10:20,330","00:10:22,880",150,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=620,"I just said,"
cs-410_8_7_151,cs-410,8,7,Topic,"00:10:22,880","00:10:27,090",151,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=622,some data points will have higher
cs-410_8_7_152,cs-410,8,7,Topic,"00:10:27,090","00:10:28,620",152,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=627,"What we're interested in, here,"
cs-410_8_7_153,cs-410,8,7,Topic,"00:10:28,620","00:10:33,420",153,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=628,is what parameter values will give
cs-410_8_7_154,cs-410,8,7,Topic,"00:10:33,420","00:10:37,620",154,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=633,So I also illustrate the problem
cs-410_8_7_155,cs-410,8,7,Topic,"00:10:37,620","00:10:41,720",155,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=637,"On the X axis I just illustrate lambda,"
cs-410_8_7_156,cs-410,8,7,Topic,"00:10:41,720","00:10:44,260",156,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=641,as a one dimensional variable.
cs-410_8_7_157,cs-410,8,7,Topic,"00:10:44,260","00:10:49,360",157,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=644,"It's oversimplification, obviously,"
cs-410_8_7_158,cs-410,8,7,Topic,"00:10:49,360","00:10:53,370",158,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=649,And the Y axis shows the probability
cs-410_8_7_159,cs-410,8,7,Topic,"00:10:53,370","00:10:57,780",159,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=653,This probability obviously depends
cs-410_8_7_160,cs-410,8,7,Topic,"00:10:57,780","00:11:01,480",160,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=657,So that's why it varies as you
cs-410_8_7_161,cs-410,8,7,Topic,"00:11:01,480","00:11:04,830",161,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=661,What we're interested here
cs-410_8_7_162,cs-410,8,7,Topic,"00:11:05,880","00:11:09,259",162,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=665,That would maximize the probability
cs-410_8_7_163,cs-410,8,7,Topic,"00:11:10,440","00:11:15,470",163,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=670,"So this would be, then,"
cs-410_8_7_164,cs-410,8,7,Topic,"00:11:15,470","00:11:17,040",164,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=675,"And these parameters,"
cs-410_8_7_165,cs-410,8,7,Topic,"00:11:17,040","00:11:21,720",165,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=677,note that are precisely what we
cs-410_8_7_166,cs-410,8,7,Topic,"00:11:21,720","00:11:25,405",166,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=681,So we'd treat these parameters
cs-410_8_7_167,cs-410,8,7,Topic,"00:11:25,405","00:11:28,046",167,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=685,the output of the data mining algorithm.
cs-410_8_7_168,cs-410,8,7,Topic,"00:11:28,046","00:11:32,966",168,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=688,So this is the general idea of using
cs-410_8_7_169,cs-410,8,7,Topic,"00:11:32,966","00:11:38,231",169,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=692,a generative model for text mining.
cs-410_8_7_170,cs-410,8,7,Topic,"00:11:38,231","00:11:42,762",170,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=698,"First, we design a model with"
cs-410_8_7_171,cs-410,8,7,Topic,"00:11:42,762","00:11:44,804",171,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=702,the data as well as we can.
cs-410_8_7_172,cs-410,8,7,Topic,"00:11:44,804","00:11:47,207",172,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=704,"After we have fit the data,"
cs-410_8_7_173,cs-410,8,7,Topic,"00:11:47,207","00:11:48,827",173,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=707,We will use the specific
cs-410_8_7_174,cs-410,8,7,Topic,"00:11:48,827","00:11:50,910",174,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=708,those would be the output
cs-410_8_7_175,cs-410,8,7,Topic,"00:11:50,910","00:11:55,880",175,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=710,And we'll treat those as actually
cs-410_8_7_176,cs-410,8,7,Topic,"00:11:55,880","00:11:59,460",176,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=715,By varying the model of course we
cs-410_8_7_177,cs-410,8,7,Topic,"00:11:59,460","00:12:03,840",177,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=719,"So to summarize, we introduced"
cs-410_8_7_178,cs-410,8,7,Topic,"00:12:03,840","00:12:09,020",178,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=723,namely representing as word distribution
cs-410_8_7_179,cs-410,8,7,Topic,"00:12:09,020","00:12:14,039",179,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=729,multiple words to describe a complicated
cs-410_8_7_180,cs-410,8,7,Topic,"00:12:14,039","00:12:19,390",180,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=734,weights on words so we have more than
cs-410_8_7_181,cs-410,8,7,Topic,"00:12:19,390","00:12:23,390",181,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=739,"We talked about the task of topic mining,"
cs-410_8_7_182,cs-410,8,7,Topic,"00:12:23,390","00:12:26,430",182,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=743,When we define a topic as distribution.
cs-410_8_7_183,cs-410,8,7,Topic,"00:12:26,430","00:12:30,140",183,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=746,So the importer is a clashing of text
cs-410_8_7_184,cs-410,8,7,Topic,"00:12:30,140","00:12:33,000",184,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=750,a vocabulary set and
cs-410_8_7_185,cs-410,8,7,Topic,"00:12:33,000","00:12:35,470",185,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=753,Each is a word distribution and
cs-410_8_7_186,cs-410,8,7,Topic,"00:12:35,470","00:12:38,730",186,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=755,also the coverage of all
cs-410_8_7_187,cs-410,8,7,Topic,"00:12:38,730","00:12:43,870",187,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=758,And these are formally represented
cs-410_8_7_188,cs-410,8,7,Topic,"00:12:43,870","00:12:48,710",188,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=763,And we have two constraints here for
cs-410_8_7_189,cs-410,8,7,Topic,"00:12:48,710","00:12:53,320",189,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=768,The first is the constraints
cs-410_8_7_190,cs-410,8,7,Topic,"00:12:53,320","00:12:56,820",190,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=773,In each worded distribution
cs-410_8_7_191,cs-410,8,7,Topic,"00:12:56,820","00:12:59,400",191,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=776,"must sum to 1,"
cs-410_8_7_192,cs-410,8,7,Topic,"00:12:59,400","00:13:03,960",192,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=779,The second constraint is on
cs-410_8_7_193,cs-410,8,7,Topic,"00:13:03,960","00:13:08,600",193,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=783,A document is not allowed to recover
cs-410_8_7_194,cs-410,8,7,Topic,"00:13:08,600","00:13:10,200",194,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=788,we are discovering.
cs-410_8_7_195,cs-410,8,7,Topic,"00:13:10,200","00:13:17,220",195,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=790,"So, the coverage of each of these k"
cs-410_8_7_196,cs-410,8,7,Topic,"00:13:17,220","00:13:21,580",196,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=797,We also introduce a general idea of using
cs-410_8_7_197,cs-410,8,7,Topic,"00:13:21,580","00:13:27,920",197,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=801,"And the idea here is, first we're design"
cs-410_8_7_198,cs-410,8,7,Topic,"00:13:27,920","00:13:30,780",198,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=807,We simply assume that they
cs-410_8_7_199,cs-410,8,7,Topic,"00:13:30,780","00:13:34,730",199,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=810,And inside the model we embed some
cs-410_8_7_200,cs-410,8,7,Topic,"00:13:34,730","00:13:35,650",200,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=814,denoted by lambda.
cs-410_8_7_201,cs-410,8,7,Topic,"00:13:36,770","00:13:40,605",201,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=816,And then we can infer the most
cs-410_8_7_202,cs-410,8,7,Topic,"00:13:40,605","00:13:41,935",202,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=820,given a particular data set.
cs-410_8_7_203,cs-410,8,7,Topic,"00:13:43,095","00:13:48,975",203,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=823,And we can then take the lambda star as
cs-410_8_7_204,cs-410,8,7,Topic,"00:13:48,975","00:13:49,495",204,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=828,our problem.
cs-410_8_7_205,cs-410,8,7,Topic,"00:13:50,555","00:13:53,115",205,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=830,And we can adjust
cs-410_8_7_206,cs-410,8,7,Topic,"00:13:53,115","00:13:58,855",206,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=833,the parameters to discover various
cs-410_8_7_207,cs-410,8,7,Topic,"00:13:58,855","00:14:04,999",207,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=838,As you will see later
cs-410_8_7_208,cs-410,8,7,Topic,"00:14:04,999","00:14:14,999",208,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=844,[MUSIC]
cs-410_8_8_1,cs-410,8,8,Probabilistic,"00:00:00,025","00:00:07,147",1,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=0,[SOUND]
cs-410_8_8_2,cs-410,8,8,Probabilistic,"00:00:07,147","00:00:09,992",2,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=7,lecture is about the Overview
cs-410_8_8_3,cs-410,8,8,Probabilistic,"00:00:09,992","00:00:12,001",3,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=9,which cover proper
cs-410_8_8_4,cs-410,8,8,Probabilistic,"00:00:12,001","00:00:15,906",4,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=12,In this lecture we're going to give
cs-410_8_8_5,cs-410,8,8,Probabilistic,"00:00:15,906","00:00:21,320",5,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=15,a overview of Statical Language Models.
cs-410_8_8_6,cs-410,8,8,Probabilistic,"00:00:21,320","00:00:24,320",6,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=21,These models are general models that cover
cs-410_8_8_7,cs-410,8,8,Probabilistic,"00:00:24,320","00:00:28,120",7,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=24,probabilistic topic models
cs-410_8_8_8,cs-410,8,8,Probabilistic,"00:00:28,120","00:00:30,530",8,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=28,"So first off,"
cs-410_8_8_9,cs-410,8,8,Probabilistic,"00:00:31,780","00:00:36,070",9,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=31,A Statistical Language Model is
cs-410_8_8_10,cs-410,8,8,Probabilistic,"00:00:36,070","00:00:37,870",10,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=36,over word sequences.
cs-410_8_8_11,cs-410,8,8,Probabilistic,"00:00:37,870","00:00:41,520",11,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=37,"So, for example,"
cs-410_8_8_12,cs-410,8,8,Probabilistic,"00:00:41,520","00:00:44,480",12,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=41,today is Wednesday a probability of .001.
cs-410_8_8_13,cs-410,8,8,Probabilistic,"00:00:44,480","00:00:49,040",13,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=44,"It might give today Wednesday is, which"
cs-410_8_8_14,cs-410,8,8,Probabilistic,"00:00:49,040","00:00:53,560",14,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=49,"is a non-grammatical sentence, a very,"
cs-410_8_8_15,cs-410,8,8,Probabilistic,"00:00:54,580","00:00:56,170",15,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=54,"And similarly another sentence,"
cs-410_8_8_16,cs-410,8,8,Probabilistic,"00:00:56,170","00:01:01,430",16,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=56,the eigenvalue is positive might
cs-410_8_8_17,cs-410,8,8,Probabilistic,"00:01:01,430","00:01:06,200",17,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=61,So as you can see such a distribution
cs-410_8_8_18,cs-410,8,8,Probabilistic,"00:01:06,200","00:01:09,830",18,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=66,It depends on the Context of Discussion.
cs-410_8_8_19,cs-410,8,8,Probabilistic,"00:01:09,830","00:01:15,370",19,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=69,Some Word Sequences might have higher
cs-410_8_8_20,cs-410,8,8,Probabilistic,"00:01:15,370","00:01:19,060",20,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=75,Sequence of Words might have different
cs-410_8_8_21,cs-410,8,8,Probabilistic,"00:01:20,490","00:01:24,870",21,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=80,And so this suggests that such a
cs-410_8_8_22,cs-410,8,8,Probabilistic,"00:01:26,960","00:01:31,440",22,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=86,such a model can also be regarded
cs-410_8_8_23,cs-410,8,8,Probabilistic,"00:01:31,440","00:01:32,520",23,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=91,generating text.
cs-410_8_8_24,cs-410,8,8,Probabilistic,"00:01:33,880","00:01:42,370",24,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=93,And that just means we can view text
cs-410_8_8_25,cs-410,8,8,Probabilistic,"00:01:42,370","00:01:49,225",25,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=102,"For this reason,"
cs-410_8_8_26,cs-410,8,8,Probabilistic,"00:01:49,225","00:01:54,310",26,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=109,"So, now given a model we can then"
cs-410_8_8_27,cs-410,8,8,Probabilistic,"00:01:54,310","00:01:59,790",27,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=114,"So, for example, based on the distribution"
cs-410_8_8_28,cs-410,8,8,Probabilistic,"00:01:59,790","00:02:04,240",28,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=119,when matter it say assemble
cs-410_8_8_29,cs-410,8,8,Probabilistic,"00:02:04,240","00:02:07,130",29,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=124,because it has a relative
cs-410_8_8_30,cs-410,8,8,Probabilistic,"00:02:07,130","00:02:10,100",30,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=127,We might often get such a sequence.
cs-410_8_8_31,cs-410,8,8,Probabilistic,"00:02:10,100","00:02:14,470",31,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=130,We might also get the item
cs-410_8_8_32,cs-410,8,8,Probabilistic,"00:02:14,470","00:02:19,120",32,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=134,with a smaller probability and
cs-410_8_8_33,cs-410,8,8,Probabilistic,"00:02:19,120","00:02:22,940",33,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=139,get today is Wednesday because
cs-410_8_8_34,cs-410,8,8,Probabilistic,"00:02:24,650","00:02:28,960",34,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=144,"So in general, in order to categorize such"
cs-410_8_8_35,cs-410,8,8,Probabilistic,"00:02:28,960","00:02:33,940",35,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=148,values for
cs-410_8_8_36,cs-410,8,8,Probabilistic,"00:02:33,940","00:02:37,827",36,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=153,"Obviously, it's impossible"
cs-410_8_8_37,cs-410,8,8,Probabilistic,"00:02:37,827","00:02:42,540",37,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=157,impossible to enumerate all of
cs-410_8_8_38,cs-410,8,8,Probabilistic,"00:02:42,540","00:02:49,300",38,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=162,"So in practice, we will have to"
cs-410_8_8_39,cs-410,8,8,Probabilistic,"00:02:49,300","00:02:52,710",39,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=169,"So, the simplest language model is"
cs-410_8_8_40,cs-410,8,8,Probabilistic,"00:02:52,710","00:02:57,270",40,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=172,"In such a case, it was simply a the text"
cs-410_8_8_41,cs-410,8,8,Probabilistic,"00:02:57,270","00:03:01,830",41,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=177,is generated by generating
cs-410_8_8_42,cs-410,8,8,Probabilistic,"00:03:02,980","00:03:06,660",42,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=182,"But in general, the words may"
cs-410_8_8_43,cs-410,8,8,Probabilistic,"00:03:06,660","00:03:11,020",43,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=186,"But after we make this assumption, we can"
cs-410_8_8_44,cs-410,8,8,Probabilistic,"00:03:12,230","00:03:16,700",44,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=192,"Basically, now the probability of"
cs-410_8_8_45,cs-410,8,8,Probabilistic,"00:03:16,700","00:03:21,500",45,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=196,will be just the product of
cs-410_8_8_46,cs-410,8,8,Probabilistic,"00:03:24,850","00:03:26,210",46,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=204,"So for such a model,"
cs-410_8_8_47,cs-410,8,8,Probabilistic,"00:03:26,210","00:03:30,470",47,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=206,we have as many parameters as
cs-410_8_8_48,cs-410,8,8,Probabilistic,"00:03:30,470","00:03:35,260",48,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=210,"So here we assume we have n words,"
cs-410_8_8_49,cs-410,8,8,Probabilistic,"00:03:35,260","00:03:36,590",49,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=215,One for each word.
cs-410_8_8_50,cs-410,8,8,Probabilistic,"00:03:36,590","00:03:38,700",50,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=216,And then some to 1.
cs-410_8_8_51,cs-410,8,8,Probabilistic,"00:03:38,700","00:03:43,010",51,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=218,"So, now we assume that"
cs-410_8_8_52,cs-410,8,8,Probabilistic,"00:03:43,010","00:03:46,220",52,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=223,drawn according to this word distribution.
cs-410_8_8_53,cs-410,8,8,Probabilistic,"00:03:46,220","00:03:50,870",53,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=226,"That just means,"
cs-410_8_8_54,cs-410,8,8,Probabilistic,"00:03:50,870","00:03:52,360",54,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=230,then eventually we'll get a text.
cs-410_8_8_55,cs-410,8,8,Probabilistic,"00:03:53,690","00:03:56,163",55,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=233,"So for example, now again,"
cs-410_8_8_56,cs-410,8,8,Probabilistic,"00:03:56,163","00:04:02,050",56,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=236,we can try to assemble words
cs-410_8_8_57,cs-410,8,8,Probabilistic,"00:04:02,050","00:04:05,110",57,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=242,We might get Wednesday often or
cs-410_8_8_58,cs-410,8,8,Probabilistic,"00:04:06,610","00:04:11,910",58,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=246,And some other words like eigenvalue
cs-410_8_8_59,cs-410,8,8,Probabilistic,"00:04:11,910","00:04:19,370",59,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=251,"But with this, we actually can"
cs-410_8_8_60,cs-410,8,8,Probabilistic,"00:04:19,370","00:04:25,980",60,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=259,"every sequence, even though our model"
cs-410_8_8_61,cs-410,8,8,Probabilistic,"00:04:25,980","00:04:27,780",61,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=265,And this is because of the independence.
cs-410_8_8_62,cs-410,8,8,Probabilistic,"00:04:27,780","00:04:32,970",62,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=267,"So specifically, we can compute"
cs-410_8_8_63,cs-410,8,8,Probabilistic,"00:04:34,010","00:04:37,740",63,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=274,Because it's just a product
cs-410_8_8_64,cs-410,8,8,Probabilistic,"00:04:37,740","00:04:42,000",64,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=277,"the probability of is, and"
cs-410_8_8_65,cs-410,8,8,Probabilistic,"00:04:42,000","00:04:45,380",65,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=282,"For example,"
cs-410_8_8_66,cs-410,8,8,Probabilistic,"00:04:45,380","00:04:49,650",66,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=285,multiply these numbers together you get
cs-410_8_8_67,cs-410,8,8,Probabilistic,"00:04:49,650","00:04:55,900",67,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=289,"So as you can see, with N probabilities,"
cs-410_8_8_68,cs-410,8,8,Probabilistic,"00:04:55,900","00:05:02,670",68,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=295,can characterize the probability situation
cs-410_8_8_69,cs-410,8,8,Probabilistic,"00:05:02,670","00:05:06,100",69,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=302,"And so, this is a very simple model."
cs-410_8_8_70,cs-410,8,8,Probabilistic,"00:05:06,100","00:05:07,890",70,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=306,Ignore the word order.
cs-410_8_8_71,cs-410,8,8,Probabilistic,"00:05:07,890","00:05:12,290",71,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=307,"So it may not be, in fact, in some"
cs-410_8_8_72,cs-410,8,8,Probabilistic,"00:05:12,290","00:05:15,410",72,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=312,where you may care about
cs-410_8_8_73,cs-410,8,8,Probabilistic,"00:05:15,410","00:05:18,310",73,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=315,But it turns out to be
cs-410_8_8_74,cs-410,8,8,Probabilistic,"00:05:18,310","00:05:20,950",74,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=318,many tasks that involve topic analysis.
cs-410_8_8_75,cs-410,8,8,Probabilistic,"00:05:20,950","00:05:24,590",75,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=320,And that's also what
cs-410_8_8_76,cs-410,8,8,Probabilistic,"00:05:24,590","00:05:31,000",76,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=324,"So when we have a model, we generally have"
cs-410_8_8_77,cs-410,8,8,Probabilistic,"00:05:31,000","00:05:38,520",77,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=331,"One is, given a model, how likely are we"
cs-410_8_8_78,cs-410,8,8,Probabilistic,"00:05:38,520","00:05:41,890",78,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=338,"That is,"
cs-410_8_8_79,cs-410,8,8,Probabilistic,"00:05:41,890","00:05:44,400",79,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=341,The other is the Estimation Process.
cs-410_8_8_80,cs-410,8,8,Probabilistic,"00:05:44,400","00:05:49,940",80,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=344,"And that, is to think of"
cs-410_8_8_81,cs-410,8,8,Probabilistic,"00:05:49,940","00:05:53,510",81,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=349,some observe the data and we're
cs-410_8_8_82,cs-410,8,8,Probabilistic,"00:05:53,510","00:05:56,110",82,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=353,Let's first talk about the sampling.
cs-410_8_8_83,cs-410,8,8,Probabilistic,"00:05:56,110","00:06:02,480",83,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=356,"So, here I show two examples of Water"
cs-410_8_8_84,cs-410,8,8,Probabilistic,"00:06:02,480","00:06:04,760",84,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=362,The first one has higher probabilities for
cs-410_8_8_85,cs-410,8,8,Probabilistic,"00:06:04,760","00:06:08,530",85,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=364,"words like a text mining association,"
cs-410_8_8_86,cs-410,8,8,Probabilistic,"00:06:10,120","00:06:16,030",86,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=370,Now this signals a topic about text mining
cs-410_8_8_87,cs-410,8,8,Probabilistic,"00:06:16,030","00:06:21,970",87,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=376,"such a distribution, we tend to see words"
cs-410_8_8_88,cs-410,8,8,Probabilistic,"00:06:23,710","00:06:27,460",88,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=383,"So in this case,"
cs-410_8_8_89,cs-410,8,8,Probabilistic,"00:06:27,460","00:06:30,560",89,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=387,what is the probability of
cs-410_8_8_90,cs-410,8,8,Probabilistic,"00:06:30,560","00:06:36,610",90,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=390,"Then, we likely will see text that"
cs-410_8_8_91,cs-410,8,8,Probabilistic,"00:06:36,610","00:06:42,110",91,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=396,"Of course, the text that we"
cs-410_8_8_92,cs-410,8,8,Probabilistic,"00:06:42,110","00:06:45,150",92,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=402,This distribution is unlikely coherent.
cs-410_8_8_93,cs-410,8,8,Probabilistic,"00:06:45,150","00:06:49,079",93,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=405,"Although, the probability"
cs-410_8_8_94,cs-410,8,8,Probabilistic,"00:06:49,079","00:06:53,535",94,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=409,[INAUDIBLE] publishing
cs-410_8_8_95,cs-410,8,8,Probabilistic,"00:06:53,535","00:06:59,090",95,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=413,non-zero assuming that no word has
cs-410_8_8_96,cs-410,8,8,Probabilistic,"00:06:59,090","00:07:02,590",96,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=419,"And that just means,"
cs-410_8_8_97,cs-410,8,8,Probabilistic,"00:07:02,590","00:07:06,560",97,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=422,text documents including very
cs-410_8_8_98,cs-410,8,8,Probabilistic,"00:07:07,830","00:07:09,660",98,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=427,"Now, the second distribution show,"
cs-410_8_8_99,cs-410,8,8,Probabilistic,"00:07:09,660","00:07:14,310",99,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=429,"on the bottom, has different than"
cs-410_8_8_100,cs-410,8,8,Probabilistic,"00:07:14,310","00:07:17,940",100,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=434,"So food [INAUDIBLE] healthy [INAUDIBLE],"
cs-410_8_8_101,cs-410,8,8,Probabilistic,"00:07:17,940","00:07:20,380",101,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=437,So this clearly indicates
cs-410_8_8_102,cs-410,8,8,Probabilistic,"00:07:20,380","00:07:23,190",102,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=440,In this case it's probably about health.
cs-410_8_8_103,cs-410,8,8,Probabilistic,"00:07:23,190","00:07:26,460",103,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=443,So if we sample a word
cs-410_8_8_104,cs-410,8,8,Probabilistic,"00:07:26,460","00:07:31,390",104,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=446,then the probability of observing a text
cs-410_8_8_105,cs-410,8,8,Probabilistic,"00:07:32,830","00:07:37,020",105,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=452,"On the other hand, the probability of"
cs-410_8_8_106,cs-410,8,8,Probabilistic,"00:07:37,020","00:07:40,400",106,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=457,"nutrition paper would be high,"
cs-410_8_8_107,cs-410,8,8,Probabilistic,"00:07:41,510","00:07:48,113",107,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=461,"So that just means, given a particular"
cs-410_8_8_108,cs-410,8,8,Probabilistic,"00:07:48,113","00:07:51,830",108,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=468,Now let's look at
cs-410_8_8_109,cs-410,8,8,Probabilistic,"00:07:51,830","00:07:54,910",109,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=471,"In this case, we're going to assume"
cs-410_8_8_110,cs-410,8,8,Probabilistic,"00:07:54,910","00:07:57,410",110,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=474,I will know exactly what
cs-410_8_8_111,cs-410,8,8,Probabilistic,"00:07:57,410","00:07:59,715",111,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=477,"In this case,"
cs-410_8_8_112,cs-410,8,8,Probabilistic,"00:07:59,715","00:08:06,980",112,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=479,"In fact, it's abstract of the paper,"
cs-410_8_8_113,cs-410,8,8,Probabilistic,"00:08:06,980","00:08:10,960",113,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=486,And I've shown some counts
cs-410_8_8_114,cs-410,8,8,Probabilistic,"00:08:12,550","00:08:16,880",114,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=492,"Now, if we ask the question,"
cs-410_8_8_115,cs-410,8,8,Probabilistic,"00:08:17,950","00:08:22,440",115,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=497,Language Model that has been
cs-410_8_8_116,cs-410,8,8,Probabilistic,"00:08:22,440","00:08:26,400",116,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=502,Assuming that the text is observed
cs-410_8_8_117,cs-410,8,8,Probabilistic,"00:08:26,400","00:08:28,920",117,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=506,what's our best guess
cs-410_8_8_118,cs-410,8,8,Probabilistic,"00:08:30,740","00:08:35,510",118,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=510,"Okay, so the problem now is just to"
cs-410_8_8_119,cs-410,8,8,Probabilistic,"00:08:35,510","00:08:36,490",119,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=515,As I've shown here.
cs-410_8_8_120,cs-410,8,8,Probabilistic,"00:08:37,560","00:08:38,370",120,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=517,So what do you think?
cs-410_8_8_121,cs-410,8,8,Probabilistic,"00:08:38,370","00:08:39,610",121,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=518,What would be your guess?
cs-410_8_8_122,cs-410,8,8,Probabilistic,"00:08:40,680","00:08:45,590",122,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=520,Would you guess text has
cs-410_8_8_123,cs-410,8,8,Probabilistic,"00:08:45,590","00:08:47,180",123,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=525,a relatively large probability?
cs-410_8_8_124,cs-410,8,8,Probabilistic,"00:08:48,360","00:08:50,310",124,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=528,What about query?
cs-410_8_8_125,cs-410,8,8,Probabilistic,"00:08:50,310","00:08:53,200",125,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=530,"Well, your guess probably"
cs-410_8_8_126,cs-410,8,8,Probabilistic,"00:08:53,200","00:08:56,516",126,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=533,how many times we have observed
cs-410_8_8_127,cs-410,8,8,Probabilistic,"00:08:56,516","00:09:00,550",127,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=536,And if you think about it for a moment.
cs-410_8_8_128,cs-410,8,8,Probabilistic,"00:09:00,550","00:09:04,960",128,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=540,"And if you are like many others,"
cs-410_8_8_129,cs-410,8,8,Probabilistic,"00:09:04,960","00:09:10,140",129,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=544,"well, text has a probability of 10"
cs-410_8_8_130,cs-410,8,8,Probabilistic,"00:09:10,140","00:09:15,040",130,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=550,the text 10 times in the text
cs-410_8_8_131,cs-410,8,8,Probabilistic,"00:09:15,040","00:09:19,640",131,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=555,"And similarly, mining has 5 out of 100."
cs-410_8_8_132,cs-410,8,8,Probabilistic,"00:09:19,640","00:09:25,180",132,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=559,And query has a relatively small
cs-410_8_8_133,cs-410,8,8,Probabilistic,"00:09:25,180","00:09:27,130",133,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=565,So it's 1 out of 100.
cs-410_8_8_134,cs-410,8,8,Probabilistic,"00:09:27,130","00:09:32,220",134,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=567,"Right, so that, intuitively,"
cs-410_8_8_135,cs-410,8,8,Probabilistic,"00:09:32,220","00:09:36,440",135,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=572,"But the question is, is this our best"
cs-410_8_8_136,cs-410,8,8,Probabilistic,"00:09:37,840","00:09:40,000",136,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=577,"Of course,"
cs-410_8_8_137,cs-410,8,8,Probabilistic,"00:09:40,000","00:09:45,070",137,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=580,"we have to define what do we mean by best,"
cs-410_8_8_138,cs-410,8,8,Probabilistic,"00:09:45,070","00:09:50,540",138,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=585,it turns out that our
cs-410_8_8_139,cs-410,8,8,Probabilistic,"00:09:50,540","00:09:54,680",139,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=590,In some sense and this is called
cs-410_8_8_140,cs-410,8,8,Probabilistic,"00:09:54,680","00:10:00,789",140,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=594,"And it's the best thing that, it will give"
cs-410_8_8_141,cs-410,8,8,Probabilistic,"00:10:01,960","00:10:05,740",141,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=601,"Meaning that, if you change"
cs-410_8_8_142,cs-410,8,8,Probabilistic,"00:10:05,740","00:10:10,760",142,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=605,then the probability of the observed
cs-410_8_8_143,cs-410,8,8,Probabilistic,"00:10:10,760","00:10:13,952",143,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=610,And this is called
cs-410_8_8_144,cs-410,8,8,Probabilistic,"00:10:13,952","00:10:23,952",144,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=613,[MUSIC]
cs-410_8_9_1,cs-410,8,9,Probabilistic,"00:00:00,025","00:00:05,683",1,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=0,[SOUND] This lecture is a continued
cs-410_8_9_2,cs-410,8,9,Probabilistic,"00:00:05,683","00:00:13,370",2,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=5,discussion of probabilistic topic models.
cs-410_8_9_3,cs-410,8,9,Probabilistic,"00:00:13,370","00:00:19,990",3,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=13,"In this lecture, we're going to continue"
cs-410_8_9_4,cs-410,8,9,Probabilistic,"00:00:19,990","00:00:24,970",4,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=19,We're going to talk about
cs-410_8_9_5,cs-410,8,9,Probabilistic,"00:00:24,970","00:00:28,300",5,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=24,are interested in just mining
cs-410_8_9_6,cs-410,8,9,Probabilistic,"00:00:30,880","00:00:35,910",6,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=30,"So in this simple setup,"
cs-410_8_9_7,cs-410,8,9,Probabilistic,"00:00:35,910","00:00:41,060",7,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=35,one document and
cs-410_8_9_8,cs-410,8,9,Probabilistic,"00:00:41,060","00:00:44,810",8,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=41,So this is the simplest
cs-410_8_9_9,cs-410,8,9,Probabilistic,"00:00:44,810","00:00:49,921",9,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=44,"The input now no longer has k,"
cs-410_8_9_10,cs-410,8,9,Probabilistic,"00:00:49,921","00:00:55,670",10,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=49,know there is only one topic and the
cs-410_8_9_11,cs-410,8,9,Probabilistic,"00:00:55,670","00:01:00,738",11,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=55,"In the output,"
cs-410_8_9_12,cs-410,8,9,Probabilistic,"00:01:00,738","00:01:06,150",12,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=60,we assumed that the document
cs-410_8_9_13,cs-410,8,9,Probabilistic,"00:01:06,150","00:01:10,532",13,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=66,So the main goal is just to discover
cs-410_8_9_14,cs-410,8,9,Probabilistic,"00:01:10,532","00:01:12,930",14,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=70,"this single topic, as shown here."
cs-410_8_9_15,cs-410,8,9,Probabilistic,"00:01:14,770","00:01:19,275",15,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=74,"As always, when we think about using a"
cs-410_8_9_16,cs-410,8,9,Probabilistic,"00:01:19,275","00:01:24,280",16,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=79,we start with thinking about what
cs-410_8_9_17,cs-410,8,9,Probabilistic,"00:01:24,280","00:01:28,880",17,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=84,from what perspective we're going to
cs-410_8_9_18,cs-410,8,9,Probabilistic,"00:01:28,880","00:01:32,268",18,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=88,And then we're going to
cs-410_8_9_19,cs-410,8,9,Probabilistic,"00:01:32,268","00:01:36,520",19,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=92,"the generating of the data,"
cs-410_8_9_20,cs-410,8,9,Probabilistic,"00:01:36,520","00:01:41,310",20,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=96,Where our perspective just means we want
cs-410_8_9_21,cs-410,8,9,Probabilistic,"00:01:41,310","00:01:45,700",21,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=101,"the data, so that the model will"
cs-410_8_9_22,cs-410,8,9,Probabilistic,"00:01:45,700","00:01:48,770",22,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=105,discovering the knowledge that we want.
cs-410_8_9_23,cs-410,8,9,Probabilistic,"00:01:48,770","00:01:54,210",23,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=108,And then we'll be thinking
cs-410_8_9_24,cs-410,8,9,Probabilistic,"00:01:54,210","00:02:00,480",24,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=114,write down the microfunction to
cs-410_8_9_25,cs-410,8,9,Probabilistic,"00:02:00,480","00:02:04,860",25,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=120,a data point will be
cs-410_8_9_26,cs-410,8,9,Probabilistic,"00:02:05,900","00:02:10,370",26,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=125,And the likelihood function will have
cs-410_8_9_27,cs-410,8,9,Probabilistic,"00:02:10,370","00:02:15,780",27,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=130,And then we argue our interest in
cs-410_8_9_28,cs-410,8,9,Probabilistic,"00:02:15,780","00:02:21,680",28,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=135,by maximizing the likelihood which will
cs-410_8_9_29,cs-410,8,9,Probabilistic,"00:02:21,680","00:02:26,710",29,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=141,These estimator parameters
cs-410_8_9_30,cs-410,8,9,Probabilistic,"00:02:26,710","00:02:31,640",30,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=146,"of the mining hours,"
cs-410_8_9_31,cs-410,8,9,Probabilistic,"00:02:31,640","00:02:35,320",31,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=151,parameters as the knowledge
cs-410_8_9_32,cs-410,8,9,Probabilistic,"00:02:35,320","00:02:39,690",32,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=155,So let's look at these steps for
cs-410_8_9_33,cs-410,8,9,Probabilistic,"00:02:39,690","00:02:45,970",33,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=159,Later we'll look at this procedure for
cs-410_8_9_34,cs-410,8,9,Probabilistic,"00:02:45,970","00:02:50,170",34,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=165,"So our data, in this case is, just"
cs-410_8_9_35,cs-410,8,9,Probabilistic,"00:02:50,170","00:02:52,520",35,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=170,Each word here is denoted by x sub i.
cs-410_8_9_36,cs-410,8,9,Probabilistic,"00:02:52,520","00:02:56,800",36,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=172,Our model is a Unigram language model.
cs-410_8_9_37,cs-410,8,9,Probabilistic,"00:02:56,800","00:03:03,420",37,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=176,A word distribution that we hope to
cs-410_8_9_38,cs-410,8,9,Probabilistic,"00:03:03,420","00:03:08,950",38,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=183,So we will have as many parameters as many
cs-410_8_9_39,cs-410,8,9,Probabilistic,"00:03:09,950","00:03:14,580",39,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=189,And for convenience we're
cs-410_8_9_40,cs-410,8,9,Probabilistic,"00:03:14,580","00:03:18,270",40,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=194,denote the probability of word w sub i.
cs-410_8_9_41,cs-410,8,9,Probabilistic,"00:03:20,450","00:03:23,384",41,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=200,And obviously these theta
cs-410_8_9_42,cs-410,8,9,Probabilistic,"00:03:24,480","00:03:27,110",42,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=204,Now what does a likelihood
cs-410_8_9_43,cs-410,8,9,Probabilistic,"00:03:27,110","00:03:30,970",43,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=207,"Well, this is just the probability"
cs-410_8_9_44,cs-410,8,9,Probabilistic,"00:03:30,970","00:03:31,948",44,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=210,that given such a model.
cs-410_8_9_45,cs-410,8,9,Probabilistic,"00:03:31,948","00:03:36,920",45,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=211,Because we assume the independence in
cs-410_8_9_46,cs-410,8,9,Probabilistic,"00:03:36,920","00:03:41,010",46,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=216,the document will be just a product
cs-410_8_9_47,cs-410,8,9,Probabilistic,"00:03:42,790","00:03:46,900",47,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=222,And since some word might
cs-410_8_9_48,cs-410,8,9,Probabilistic,"00:03:46,900","00:03:51,070",48,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=226,So we can also rewrite this
cs-410_8_9_49,cs-410,8,9,Probabilistic,"00:03:52,580","00:03:58,550",49,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=232,"So in this line, we have rewritten"
cs-410_8_9_50,cs-410,8,9,Probabilistic,"00:03:58,550","00:04:05,360",50,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=238,over all the unique words in
cs-410_8_9_51,cs-410,8,9,Probabilistic,"00:04:05,360","00:04:09,170",51,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=245,Now this is different
cs-410_8_9_52,cs-410,8,9,Probabilistic,"00:04:09,170","00:04:13,990",52,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=249,"Well, the product is over different"
cs-410_8_9_53,cs-410,8,9,Probabilistic,"00:04:15,040","00:04:19,694",53,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=255,"Now when we do this transformation,"
cs-410_8_9_54,cs-410,8,9,Probabilistic,"00:04:19,694","00:04:24,120",54,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=259,introduce a counter function here.
cs-410_8_9_55,cs-410,8,9,Probabilistic,"00:04:24,120","00:04:29,395",55,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=264,This denotes the count of
cs-410_8_9_56,cs-410,8,9,Probabilistic,"00:04:29,395","00:04:33,390",56,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=269,similarly this is the count
cs-410_8_9_57,cs-410,8,9,Probabilistic,"00:04:33,390","00:04:37,890",57,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=273,because these words might
cs-410_8_9_58,cs-410,8,9,Probabilistic,"00:04:37,890","00:04:40,459",58,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=277,You can also see if a word did
cs-410_8_9_59,cs-410,8,9,Probabilistic,"00:04:41,810","00:04:46,790",59,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=281,"It will have a zero count, therefore"
cs-410_8_9_60,cs-410,8,9,Probabilistic,"00:04:46,790","00:04:50,410",60,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=286,So this is a very useful form of
cs-410_8_9_61,cs-410,8,9,Probabilistic,"00:04:50,410","00:04:55,060",61,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=290,writing down the likelihood function
cs-410_8_9_62,cs-410,8,9,Probabilistic,"00:04:55,060","00:05:01,230",62,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=295,"So I want you to pay attention to this,"
cs-410_8_9_63,cs-410,8,9,Probabilistic,"00:05:01,230","00:05:07,120",63,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=301,It's just to change the product over all
cs-410_8_9_64,cs-410,8,9,Probabilistic,"00:05:07,120","00:05:12,013",64,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=307,"So in the end, of course, we'll use"
cs-410_8_9_65,cs-410,8,9,Probabilistic,"00:05:12,013","00:05:14,512",65,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=312,function and it would look like this.
cs-410_8_9_66,cs-410,8,9,Probabilistic,"00:05:14,512","00:05:19,468",66,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=314,"Next, we're going to find"
cs-410_8_9_67,cs-410,8,9,Probabilistic,"00:05:19,468","00:05:24,530",67,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=319,of these words that would maximize
cs-410_8_9_68,cs-410,8,9,Probabilistic,"00:05:24,530","00:05:30,539",68,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=324,So now lets take a look at the maximum
cs-410_8_9_69,cs-410,8,9,Probabilistic,"00:05:32,520","00:05:35,870",69,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=332,This line is copied from
cs-410_8_9_70,cs-410,8,9,Probabilistic,"00:05:35,870","00:05:37,340",70,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=335,It's just our likelihood function.
cs-410_8_9_71,cs-410,8,9,Probabilistic,"00:05:38,590","00:05:43,950",71,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=338,So our goal is to maximize
cs-410_8_9_72,cs-410,8,9,Probabilistic,"00:05:43,950","00:05:46,210",72,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=343,We will find it often easy to
cs-410_8_9_73,cs-410,8,9,Probabilistic,"00:05:47,310","00:05:51,110",73,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=347,maximize the local likelihood
cs-410_8_9_74,cs-410,8,9,Probabilistic,"00:05:51,110","00:05:56,531",74,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=351,And this is purely for
cs-410_8_9_75,cs-410,8,9,Probabilistic,"00:05:56,531","00:06:03,698",75,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=356,the logarithm transformation our function
cs-410_8_9_76,cs-410,8,9,Probabilistic,"00:06:03,698","00:06:10,704",76,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=363,And we also have constraints
cs-410_8_9_77,cs-410,8,9,Probabilistic,"00:06:10,704","00:06:16,743",77,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=370,The sum makes it easier to take
cs-410_8_9_78,cs-410,8,9,Probabilistic,"00:06:16,743","00:06:21,022",78,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=376,finding the optimal
cs-410_8_9_79,cs-410,8,9,Probabilistic,"00:06:21,022","00:06:27,349",79,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=381,"So please take a look at this sum again,"
cs-410_8_9_80,cs-410,8,9,Probabilistic,"00:06:27,349","00:06:32,434",80,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=387,And this is a form of
cs-410_8_9_81,cs-410,8,9,Probabilistic,"00:06:32,434","00:06:38,430",81,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=392,"see later also,"
cs-410_8_9_82,cs-410,8,9,Probabilistic,"00:06:38,430","00:06:42,340",82,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=398,So it's a sum over all
cs-410_8_9_83,cs-410,8,9,Probabilistic,"00:06:42,340","00:06:48,105",83,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=402,And inside the sum there is
cs-410_8_9_84,cs-410,8,9,Probabilistic,"00:06:48,105","00:06:54,980",84,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=408,And this is macroed by
cs-410_8_9_85,cs-410,8,9,Probabilistic,"00:06:55,990","00:06:57,920",85,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=415,So let's see how we can
cs-410_8_9_86,cs-410,8,9,Probabilistic,"00:06:58,920","00:07:04,030",86,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=418,Now at this point the problem is purely a
cs-410_8_9_87,cs-410,8,9,Probabilistic,"00:07:04,030","00:07:11,360",87,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=424,to just the find the optimal solution
cs-410_8_9_88,cs-410,8,9,Probabilistic,"00:07:11,360","00:07:14,694",88,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=431,The objective function is
cs-410_8_9_89,cs-410,8,9,Probabilistic,"00:07:14,694","00:07:18,621",89,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=434,the constraint is that all these
cs-410_8_9_90,cs-410,8,9,Probabilistic,"00:07:18,621","00:07:23,234",90,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=438,"So, one way to solve the problem is"
cs-410_8_9_91,cs-410,8,9,Probabilistic,"00:07:24,520","00:07:29,040",91,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=444,Now this command is beyond
cs-410_8_9_92,cs-410,8,9,Probabilistic,"00:07:29,040","00:07:33,670",92,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=449,since Lagrange multiplier is a very
cs-410_8_9_93,cs-410,8,9,Probabilistic,"00:07:33,670","00:07:37,940",93,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=453,"to just give a brief introduction to this,"
cs-410_8_9_94,cs-410,8,9,Probabilistic,"00:07:39,720","00:07:43,857",94,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=459,So in this approach we will
cs-410_8_9_95,cs-410,8,9,Probabilistic,"00:07:43,857","00:07:49,887",95,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=463,And this function will combine
cs-410_8_9_96,cs-410,8,9,Probabilistic,"00:07:49,887","00:07:55,392",96,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=469,with another term that
cs-410_8_9_97,cs-410,8,9,Probabilistic,"00:07:55,392","00:07:59,980",97,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=475,"we introduce Lagrange multiplier here,"
cs-410_8_9_98,cs-410,8,9,Probabilistic,"00:07:59,980","00:08:04,978",98,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=479,"lambda, so it's an additional parameter."
cs-410_8_9_99,cs-410,8,9,Probabilistic,"00:08:04,978","00:08:10,432",99,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=484,"Now, the idea of this approach is just to"
cs-410_8_9_100,cs-410,8,9,Probabilistic,"00:08:10,432","00:08:14,800",100,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=490,"in some sense,"
cs-410_8_9_101,cs-410,8,9,Probabilistic,"00:08:14,800","00:08:18,318",101,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=494,Now we are just interested in
cs-410_8_9_102,cs-410,8,9,Probabilistic,"00:08:19,460","00:08:24,022",102,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=499,"As you may recall from calculus,"
cs-410_8_9_103,cs-410,8,9,Probabilistic,"00:08:24,022","00:08:29,910",103,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=504,would be achieved when
cs-410_8_9_104,cs-410,8,9,Probabilistic,"00:08:29,910","00:08:31,673",104,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=509,This is a necessary condition.
cs-410_8_9_105,cs-410,8,9,Probabilistic,"00:08:31,673","00:08:33,182",105,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=511,"It's not sufficient, though."
cs-410_8_9_106,cs-410,8,9,Probabilistic,"00:08:33,182","00:08:38,205",106,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=513,So if we do that you will
cs-410_8_9_107,cs-410,8,9,Probabilistic,"00:08:38,205","00:08:42,785",107,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=518,with respect to theta i
cs-410_8_9_108,cs-410,8,9,Probabilistic,"00:08:42,785","00:08:50,815",108,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=522,And this part comes from the derivative
cs-410_8_9_109,cs-410,8,9,Probabilistic,"00:08:50,815","00:08:55,390",109,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=530,this lambda is simply taken from here.
cs-410_8_9_110,cs-410,8,9,Probabilistic,"00:08:55,390","00:09:00,178",110,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=535,And when we set it to zero we can
cs-410_8_9_111,cs-410,8,9,Probabilistic,"00:09:00,178","00:09:05,610",111,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=540,easily see theta sub i is
cs-410_8_9_112,cs-410,8,9,Probabilistic,"00:09:06,820","00:09:09,900",112,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=546,Since we know all the theta
cs-410_8_9_113,cs-410,8,9,Probabilistic,"00:09:09,900","00:09:12,423",113,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=549,"we can plug this into this constraint,"
cs-410_8_9_114,cs-410,8,9,Probabilistic,"00:09:12,423","00:09:15,600",114,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=552,And this will allow us to solve for
cs-410_8_9_115,cs-410,8,9,Probabilistic,"00:09:16,630","00:09:20,840",115,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=556,And this is just a net
cs-410_8_9_116,cs-410,8,9,Probabilistic,"00:09:20,840","00:09:27,350",116,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=560,And this further allows us to then
cs-410_8_9_117,cs-410,8,9,Probabilistic,"00:09:27,350","00:09:31,380",117,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=567,"eventually, to find the optimal"
cs-410_8_9_118,cs-410,8,9,Probabilistic,"00:09:31,380","00:09:37,280",118,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=571,And if you look at this formula it turns
cs-410_8_9_119,cs-410,8,9,Probabilistic,"00:09:37,280","00:09:43,089",119,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=577,because this is just the normalized
cs-410_8_9_120,cs-410,8,9,Probabilistic,"00:09:43,089","00:09:47,751",120,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=583,which is also a sum of all
cs-410_8_9_121,cs-410,8,9,Probabilistic,"00:09:47,751","00:09:52,157",121,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=587,"So, after all this mess, after all,"
cs-410_8_9_122,cs-410,8,9,Probabilistic,"00:09:52,157","00:09:59,044",122,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=592,we have just obtained something
cs-410_8_9_123,cs-410,8,9,Probabilistic,"00:09:59,044","00:10:04,415",123,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=599,this will be just our
cs-410_8_9_124,cs-410,8,9,Probabilistic,"00:10:04,415","00:10:10,338",124,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=604,maximize the data by
cs-410_8_9_125,cs-410,8,9,Probabilistic,"00:10:10,338","00:10:16,419",125,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=610,mass as possible to all
cs-410_8_9_126,cs-410,8,9,Probabilistic,"00:10:16,419","00:10:21,408",126,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=616,And you might also notice that this is
cs-410_8_9_127,cs-410,8,9,Probabilistic,"00:10:21,408","00:10:23,450",127,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=621,raised estimator.
cs-410_8_9_128,cs-410,8,9,Probabilistic,"00:10:23,450","00:10:29,333",128,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=623,"In general, the estimator would be to"
cs-410_8_9_129,cs-410,8,9,Probabilistic,"00:10:29,333","00:10:35,050",129,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=629,the counts have to be done in a particular
cs-410_8_9_130,cs-410,8,9,Probabilistic,"00:10:35,050","00:10:41,730",130,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=635,So this is basically an analytical
cs-410_8_9_131,cs-410,8,9,Probabilistic,"00:10:41,730","00:10:46,303",131,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=641,"In general though, when the likelihood"
cs-410_8_9_132,cs-410,8,9,Probabilistic,"00:10:46,303","00:10:50,919",132,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=646,going to be able to solve the optimization
cs-410_8_9_133,cs-410,8,9,Probabilistic,"00:10:50,919","00:10:55,134",133,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=650,Instead we have to use some
cs-410_8_9_134,cs-410,8,9,Probabilistic,"00:10:55,134","00:10:58,787",134,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=655,"we're going to see such cases later, also."
cs-410_8_9_135,cs-410,8,9,Probabilistic,"00:10:58,787","00:11:02,385",135,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=658,So if you imagine what would we
cs-410_8_9_136,cs-410,8,9,Probabilistic,"00:11:02,385","00:11:07,146",136,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=662,likelihood estimator to estimate one
cs-410_8_9_137,cs-410,8,9,Probabilistic,"00:11:07,146","00:11:09,903",137,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=667,Let's imagine this document
cs-410_8_9_138,cs-410,8,9,Probabilistic,"00:11:09,903","00:11:16,277",138,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=669,"Now, what you might see is"
cs-410_8_9_139,cs-410,8,9,Probabilistic,"00:11:16,277","00:11:20,555",139,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=676,"On the top, you will see the high"
cs-410_8_9_140,cs-410,8,9,Probabilistic,"00:11:20,555","00:11:23,710",140,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=680,"common words,"
cs-410_8_9_141,cs-410,8,9,Probabilistic,"00:11:23,710","00:11:27,742",141,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=683,And this will be followed by
cs-410_8_9_142,cs-410,8,9,Probabilistic,"00:11:27,742","00:11:31,622",142,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=687,"characterize the topic well like text,"
cs-410_8_9_143,cs-410,8,9,Probabilistic,"00:11:31,622","00:11:36,275",143,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=691,"And then in the end,"
cs-410_8_9_144,cs-410,8,9,Probabilistic,"00:11:36,275","00:11:40,017",144,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=696,words that are not really
cs-410_8_9_145,cs-410,8,9,Probabilistic,"00:11:40,017","00:11:44,320",145,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=700,they might be extraneously
cs-410_8_9_146,cs-410,8,9,Probabilistic,"00:11:44,320","00:11:49,590",146,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=704,"As a topic representation,"
cs-410_8_9_147,cs-410,8,9,Probabilistic,"00:11:49,590","00:11:52,452",147,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=709,That because the high probability
cs-410_8_9_148,cs-410,8,9,Probabilistic,"00:11:52,452","00:11:55,310",148,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=712,they are not really
cs-410_8_9_149,cs-410,8,9,Probabilistic,"00:11:55,310","00:11:58,280",149,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=715,So my question is how can we
cs-410_8_9_150,cs-410,8,9,Probabilistic,"00:11:59,720","00:12:02,680",150,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=719,Now this is the topic of the next module.
cs-410_8_9_151,cs-410,8,9,Probabilistic,"00:12:02,680","00:12:06,913",151,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=722,We're going to talk about how to use
cs-410_8_9_152,cs-410,8,9,Probabilistic,"00:12:06,913","00:12:08,077",152,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=726,these common words.
cs-410_8_9_153,cs-410,8,9,Probabilistic,"00:12:08,077","00:12:18,077",153,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=728,[MUSIC]
cs-410_9_1_1,cs-410,9,1,Probabilistic,"00:00:00,171","00:00:04,190",1,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=0,[MUSIC]
cs-410_9_1_2,cs-410,9,1,Probabilistic,"00:00:06,708","00:00:10,470",2,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=6,This lecture is about the mixture
cs-410_9_1_3,cs-410,9,1,Probabilistic,"00:00:11,900","00:00:16,280",3,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=11,In this lecture we will continue
cs-410_9_1_4,cs-410,9,1,Probabilistic,"00:00:16,280","00:00:20,950",4,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=16,"In particular, what we introduce"
cs-410_9_1_5,cs-410,9,1,Probabilistic,"00:00:20,950","00:00:24,230",5,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=20,This is a slide that
cs-410_9_1_6,cs-410,9,1,Probabilistic,"00:00:24,230","00:00:29,189",6,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=24,Where we talked about how to
cs-410_9_1_7,cs-410,9,1,Probabilistic,"00:00:29,189","00:00:34,271",7,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=29,words that we have on top of for
cs-410_9_1_8,cs-410,9,1,Probabilistic,"00:00:36,540","00:00:38,440",8,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=36,"So if you want to solve the problem,"
cs-410_9_1_9,cs-410,9,1,Probabilistic,"00:00:38,440","00:00:44,090",9,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=38,it would be useful to think about
cs-410_9_1_10,cs-410,9,1,Probabilistic,"00:00:44,090","00:00:49,570",10,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=44,"Well, this obviously because these"
cs-410_9_1_11,cs-410,9,1,Probabilistic,"00:00:49,570","00:00:52,730",11,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=49,we are using a maximum
cs-410_9_1_12,cs-410,9,1,Probabilistic,"00:00:52,730","00:00:56,170",12,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=52,Then the estimate obviously would
cs-410_9_1_13,cs-410,9,1,Probabilistic,"00:00:56,170","00:00:59,284",13,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=56,these words in order to
cs-410_9_1_14,cs-410,9,1,Probabilistic,"00:00:59,284","00:01:03,390",14,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=59,"So, in order to get rid of them that"
cs-410_9_1_15,cs-410,9,1,Probabilistic,"00:01:03,390","00:01:04,030",15,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=63,differently here.
cs-410_9_1_16,cs-410,9,1,Probabilistic,"00:01:05,740","00:01:09,290",16,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=65,In particular we'll have
cs-410_9_1_17,cs-410,9,1,Probabilistic,"00:01:09,290","00:01:12,300",17,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=69,doesn't have to explain all
cs-410_9_1_18,cs-410,9,1,Probabilistic,"00:01:12,300","00:01:13,620",18,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=72,"What were going to say is that,"
cs-410_9_1_19,cs-410,9,1,Probabilistic,"00:01:13,620","00:01:19,760",19,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=73,these common words should not be
cs-410_9_1_20,cs-410,9,1,Probabilistic,"00:01:19,760","00:01:25,750",20,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=79,So one natural way to solve the problem is
cs-410_9_1_21,cs-410,9,1,Probabilistic,"00:01:25,750","00:01:29,350",21,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=85,to account for just these common words.
cs-410_9_1_22,cs-410,9,1,Probabilistic,"00:01:29,350","00:01:33,940",22,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=89,"This way, the two distributions can be"
cs-410_9_1_23,cs-410,9,1,Probabilistic,"00:01:33,940","00:01:38,390",23,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=93,And we'll let the other model which
cs-410_9_1_24,cs-410,9,1,Probabilistic,"00:01:38,390","00:01:40,700",24,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=98,to generate the common words.
cs-410_9_1_25,cs-410,9,1,Probabilistic,"00:01:40,700","00:01:47,040",25,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=100,This way our target topic theta
cs-410_9_1_26,cs-410,9,1,Probabilistic,"00:01:47,040","00:01:51,439",26,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=107,the common handle words that are
cs-410_9_1_27,cs-410,9,1,Probabilistic,"00:01:52,880","00:01:54,310",27,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=112,"So, how does this work?"
cs-410_9_1_28,cs-410,9,1,Probabilistic,"00:01:54,310","00:01:58,210",28,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=114,"Well, it is just a small"
cs-410_9_1_29,cs-410,9,1,Probabilistic,"00:01:58,210","00:02:01,050",29,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=118,where we have just one distribution.
cs-410_9_1_30,cs-410,9,1,Probabilistic,"00:02:01,050","00:02:02,870",30,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=121,"Since we now have two distributions,"
cs-410_9_1_31,cs-410,9,1,Probabilistic,"00:02:02,870","00:02:07,810",31,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=122,we have to decide which distribution
cs-410_9_1_32,cs-410,9,1,Probabilistic,"00:02:07,810","00:02:12,670",32,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=127,Each word will still be a sample
cs-410_9_1_33,cs-410,9,1,Probabilistic,"00:02:13,730","00:02:16,940",33,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=133,Text data is still
cs-410_9_1_34,cs-410,9,1,Probabilistic,"00:02:16,940","00:02:20,770",34,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=136,"Namely, look at the generating"
cs-410_9_1_35,cs-410,9,1,Probabilistic,"00:02:20,770","00:02:23,300",35,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=140,eventually we generate a lot of words.
cs-410_9_1_36,cs-410,9,1,Probabilistic,"00:02:23,300","00:02:24,840",36,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=143,"When we generate the word,"
cs-410_9_1_37,cs-410,9,1,Probabilistic,"00:02:24,840","00:02:29,820",37,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=144,"however, we're going to first decide"
cs-410_9_1_38,cs-410,9,1,Probabilistic,"00:02:29,820","00:02:34,910",38,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=149,And this is controlled by another
cs-410_9_1_39,cs-410,9,1,Probabilistic,"00:02:34,910","00:02:39,639",39,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=154,theta sub d and
cs-410_9_1_40,cs-410,9,1,Probabilistic,"00:02:41,850","00:02:47,170",40,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=161,So this is a probability of enacting
cs-410_9_1_41,cs-410,9,1,Probabilistic,"00:02:47,170","00:02:51,150",41,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=167,This is the probability of
cs-410_9_1_42,cs-410,9,1,Probabilistic,"00:02:52,150","00:02:54,500",42,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=172,of distribution denoted by theta sub B.
cs-410_9_1_43,cs-410,9,1,Probabilistic,"00:02:55,500","00:02:59,890",43,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=175,On this case I just give example
cs-410_9_1_44,cs-410,9,1,Probabilistic,"00:02:59,890","00:03:03,800",44,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=179,"So you're going to basically flip a coin,"
cs-410_9_1_45,cs-410,9,1,Probabilistic,"00:03:03,800","00:03:05,740",45,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=183,to decide what you want to use.
cs-410_9_1_46,cs-410,9,1,Probabilistic,"00:03:05,740","00:03:09,850",46,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=185,But in general these probabilities
cs-410_9_1_47,cs-410,9,1,Probabilistic,"00:03:09,850","00:03:15,590",47,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=189,So you might bias toward using
cs-410_9_1_48,cs-410,9,1,Probabilistic,"00:03:15,590","00:03:19,960",48,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=195,So now the process of generating a word
cs-410_9_1_49,cs-410,9,1,Probabilistic,"00:03:19,960","00:03:26,500",49,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=199,Based on these probabilities choosing
cs-410_9_1_50,cs-410,9,1,Probabilistic,"00:03:26,500","00:03:31,920",50,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=206,"shows up as head, which means we're going"
cs-410_9_1_51,cs-410,9,1,Probabilistic,"00:03:31,920","00:03:37,620",51,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=211,Then we're going to use this word
cs-410_9_1_52,cs-410,9,1,Probabilistic,"00:03:37,620","00:03:40,649",52,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=217,Otherwise we might be
cs-410_9_1_53,cs-410,9,1,Probabilistic,"00:03:41,680","00:03:45,530",53,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=221,And we're going to use the background
cs-410_9_1_54,cs-410,9,1,Probabilistic,"00:03:46,910","00:03:51,330",54,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=226,"So in such a case,"
cs-410_9_1_55,cs-410,9,1,Probabilistic,"00:03:51,330","00:03:54,630",55,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=231,associated with the use
cs-410_9_1_56,cs-410,9,1,Probabilistic,"00:03:54,630","00:03:59,420",56,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=234,But we can still think of this as
cs-410_9_1_57,cs-410,9,1,Probabilistic,"00:03:59,420","00:04:01,220",57,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=239,And such a model is
cs-410_9_1_58,cs-410,9,1,Probabilistic,"00:04:02,760","00:04:03,860",58,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=242,So now let's see.
cs-410_9_1_59,cs-410,9,1,Probabilistic,"00:04:03,860","00:04:07,020",59,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=243,"In this case, what's the probability"
cs-410_9_1_60,cs-410,9,1,Probabilistic,"00:04:07,020","00:04:10,460",60,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=247,Now here I showed some words.
cs-410_9_1_61,cs-410,9,1,Probabilistic,"00:04:10,460","00:04:12,280",61,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=250,"like ""the"" and ""text""."
cs-410_9_1_62,cs-410,9,1,Probabilistic,"00:04:12,280","00:04:13,820",62,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=252,"So as in all cases,"
cs-410_9_1_63,cs-410,9,1,Probabilistic,"00:04:13,820","00:04:17,910",63,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=253,once we setup a model we are interested
cs-410_9_1_64,cs-410,9,1,Probabilistic,"00:04:17,910","00:04:19,550",64,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=257,"The basic question is, so"
cs-410_9_1_65,cs-410,9,1,Probabilistic,"00:04:19,550","00:04:23,040",65,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=259,what's the probability of
cs-410_9_1_66,cs-410,9,1,Probabilistic,"00:04:23,040","00:04:27,870",66,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=263,Now we know that the word can be observed
cs-410_9_1_67,cs-410,9,1,Probabilistic,"00:04:27,870","00:04:29,840",67,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=267,we have to consider two cases.
cs-410_9_1_68,cs-410,9,1,Probabilistic,"00:04:29,840","00:04:32,660",68,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=269,Therefore it's a sum over these two cases.
cs-410_9_1_69,cs-410,9,1,Probabilistic,"00:04:34,410","00:04:40,040",69,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=274,The first case is to use the topic for
cs-410_9_1_70,cs-410,9,1,Probabilistic,"00:04:40,040","00:04:46,150",70,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=280,And in such a case then
cs-410_9_1_71,cs-410,9,1,Probabilistic,"00:04:46,150","00:04:48,550",71,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=286,which is the probability
cs-410_9_1_72,cs-410,9,1,Probabilistic,"00:04:48,550","00:04:53,760",72,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=288,multiplied by the probability of actually
cs-410_9_1_73,cs-410,9,1,Probabilistic,"00:04:53,760","00:04:56,970",73,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=293,Both events must happen
cs-410_9_1_74,cs-410,9,1,Probabilistic,"00:04:56,970","00:05:02,050",74,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=296,We first must have choosing
cs-410_9_1_75,cs-410,9,1,Probabilistic,"00:05:02,050","00:05:07,650",75,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=302,we also have to actually have sampled
cs-410_9_1_76,cs-410,9,1,Probabilistic,"00:05:07,650","00:05:11,100",76,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=307,"And similarly,"
cs-410_9_1_77,cs-410,9,1,Probabilistic,"00:05:11,100","00:05:13,880",77,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=311,a different way of generally
cs-410_9_1_78,cs-410,9,1,Probabilistic,"00:05:15,190","00:05:20,970",78,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=315,Now obviously the probability of
cs-410_9_1_79,cs-410,9,1,Probabilistic,"00:05:20,970","00:05:25,040",79,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=320,So we also can see the two
cs-410_9_1_80,cs-410,9,1,Probabilistic,"00:05:25,040","00:05:29,720",80,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=325,"And in each case, it's a product of the"
cs-410_9_1_81,cs-410,9,1,Probabilistic,"00:05:29,720","00:05:34,530",81,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=329,is multiplied by the probability of
cs-410_9_1_82,cs-410,9,1,Probabilistic,"00:05:35,640","00:05:38,890",82,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=335,"Now whether you will see,"
cs-410_9_1_83,cs-410,9,1,Probabilistic,"00:05:38,890","00:05:43,940",83,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=338,So might want to make sure that you have
cs-410_9_1_84,cs-410,9,1,Probabilistic,"00:05:43,940","00:05:48,130",84,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=343,And you should convince yourself that
cs-410_9_1_85,cs-410,9,1,Probabilistic,"00:05:48,130","00:05:49,940",85,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=348,obsolete text.
cs-410_9_1_86,cs-410,9,1,Probabilistic,"00:05:49,940","00:05:52,010",86,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=349,So to summarize what we observed here.
cs-410_9_1_87,cs-410,9,1,Probabilistic,"00:05:52,010","00:05:57,270",87,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=352,The probability of a word from
cs-410_9_1_88,cs-410,9,1,Probabilistic,"00:05:57,270","00:05:59,500",88,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=357,of different ways of generating the word.
cs-410_9_1_89,cs-410,9,1,Probabilistic,"00:06:00,610","00:06:01,990",89,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=360,"In each case,"
cs-410_9_1_90,cs-410,9,1,Probabilistic,"00:06:01,990","00:06:07,898",90,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=361,it's a product of the probability
cs-410_9_1_91,cs-410,9,1,Probabilistic,"00:06:07,898","00:06:12,320",91,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=367,Multiplied by the probability of
cs-410_9_1_92,cs-410,9,1,Probabilistic,"00:06:12,320","00:06:14,010",92,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=372,from that component of the model.
cs-410_9_1_93,cs-410,9,1,Probabilistic,"00:06:14,010","00:06:20,940",93,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=374,And this is something quite general and
cs-410_9_1_94,cs-410,9,1,Probabilistic,"00:06:20,940","00:06:23,825",94,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=380,So the basic idea of a mixture
cs-410_9_1_95,cs-410,9,1,Probabilistic,"00:06:23,825","00:06:28,820",95,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=383,thesetwo distributions
cs-410_9_1_96,cs-410,9,1,Probabilistic,"00:06:28,820","00:06:32,810",96,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=388,So I used a box to bring all
cs-410_9_1_97,cs-410,9,1,Probabilistic,"00:06:32,810","00:06:36,200",97,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=392,So if you view this
cs-410_9_1_98,cs-410,9,1,Probabilistic,"00:06:36,200","00:06:38,610",98,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=396,it's just like any other generative model.
cs-410_9_1_99,cs-410,9,1,Probabilistic,"00:06:38,610","00:06:41,260",99,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=398,It would just give us
cs-410_9_1_100,cs-410,9,1,Probabilistic,"00:06:42,850","00:06:47,310",100,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=402,But the way that determines this
cs-410_9_1_101,cs-410,9,1,Probabilistic,"00:06:47,310","00:06:48,840",101,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=407,when we have just one distribution.
cs-410_9_1_102,cs-410,9,1,Probabilistic,"00:06:50,050","00:06:54,710",102,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=410,And this is basically a more
cs-410_9_1_103,cs-410,9,1,Probabilistic,"00:06:54,710","00:06:57,710",103,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=414,So the more complicated is more
cs-410_9_1_104,cs-410,9,1,Probabilistic,"00:06:57,710","00:06:58,740",104,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=417,And it's called a mixture model.
cs-410_9_1_105,cs-410,9,1,Probabilistic,"00:07:00,460","00:07:04,450",105,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=420,So as I just said we can treat
cs-410_9_1_106,cs-410,9,1,Probabilistic,"00:07:04,450","00:07:08,450",106,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=424,And it's often useful to think of
cs-410_9_1_107,cs-410,9,1,Probabilistic,"00:07:08,450","00:07:10,140",107,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=428,The illustration that
cs-410_9_1_108,cs-410,9,1,Probabilistic,"00:07:10,140","00:07:14,210",108,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=430,"which is dimmer now, is just"
cs-410_9_1_109,cs-410,9,1,Probabilistic,"00:07:14,210","00:07:18,390",109,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=434,"So mathematically,"
cs-410_9_1_110,cs-410,9,1,Probabilistic,"00:07:18,390","00:07:21,690",110,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=438,to just define the following
cs-410_9_1_111,cs-410,9,1,Probabilistic,"00:07:21,690","00:07:25,820",111,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=441,Where the probability of a word is
cs-410_9_1_112,cs-410,9,1,Probabilistic,"00:07:26,840","00:07:28,830",112,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=446,of generating the word.
cs-410_9_1_113,cs-410,9,1,Probabilistic,"00:07:28,830","00:07:32,800",113,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=448,And the form you are seeing now
cs-410_9_1_114,cs-410,9,1,Probabilistic,"00:07:32,800","00:07:36,680",114,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=452,what you have seen in
cs-410_9_1_115,cs-410,9,1,Probabilistic,"00:07:36,680","00:07:41,150",115,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=456,Well I just use the symbol
cs-410_9_1_116,cs-410,9,1,Probabilistic,"00:07:41,150","00:07:46,330",116,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=461,you can still see this is
cs-410_9_1_117,cs-410,9,1,Probabilistic,"00:07:46,330","00:07:47,560",117,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=466,Right?
cs-410_9_1_118,cs-410,9,1,Probabilistic,"00:07:47,560","00:07:53,080",118,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=467,And this sum is due to the fact that the
cs-410_9_1_119,cs-410,9,1,Probabilistic,"00:07:53,080","00:07:55,070",119,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=473,two ways in this case.
cs-410_9_1_120,cs-410,9,1,Probabilistic,"00:07:55,070","00:08:00,330",120,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=475,"And inside a sum,"
cs-410_9_1_121,cs-410,9,1,Probabilistic,"00:08:00,330","00:08:05,720",121,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=480,And the two terms are first
cs-410_9_1_122,cs-410,9,1,Probabilistic,"00:08:05,720","00:08:07,280",122,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=485,"like of D Second,"
cs-410_9_1_123,cs-410,9,1,Probabilistic,"00:08:07,280","00:08:12,730",123,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=487,the probability of actually observing
cs-410_9_1_124,cs-410,9,1,Probabilistic,"00:08:12,730","00:08:18,770",124,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=492,So this is a very general description
cs-410_9_1_125,cs-410,9,1,Probabilistic,"00:08:18,770","00:08:23,020",125,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=498,I just want to make sure
cs-410_9_1_126,cs-410,9,1,Probabilistic,"00:08:23,020","00:08:27,154",126,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=503,this because this is really the basis for
cs-410_9_1_127,cs-410,9,1,Probabilistic,"00:08:28,480","00:08:31,350",127,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=508,So now once we setup model.
cs-410_9_1_128,cs-410,9,1,Probabilistic,"00:08:31,350","00:08:34,310",128,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=511,We can write down that like
cs-410_9_1_129,cs-410,9,1,Probabilistic,"00:08:34,310","00:08:37,720",129,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=514,"The next question is,"
cs-410_9_1_130,cs-410,9,1,Probabilistic,"00:08:37,720","00:08:40,080",130,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=517,or what to do with the parameters.
cs-410_9_1_131,cs-410,9,1,Probabilistic,"00:08:40,080","00:08:41,540",131,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=520,Given the data.
cs-410_9_1_132,cs-410,9,1,Probabilistic,"00:08:41,540","00:08:42,860",132,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=521,"Well, in general,"
cs-410_9_1_133,cs-410,9,1,Probabilistic,"00:08:42,860","00:08:47,410",133,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=522,we can use some of the text data
cs-410_9_1_134,cs-410,9,1,Probabilistic,"00:08:47,410","00:08:50,470",134,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=527,And this estimation would allow us to
cs-410_9_1_135,cs-410,9,1,Probabilistic,"00:08:50,470","00:08:55,350",135,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=530,discover the interesting
cs-410_9_1_136,cs-410,9,1,Probabilistic,"00:08:55,350","00:08:58,450",136,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=535,"So you, in this case, what do we discover?"
cs-410_9_1_137,cs-410,9,1,Probabilistic,"00:08:58,450","00:09:01,120",137,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=538,"Well, these are presented"
cs-410_9_1_138,cs-410,9,1,Probabilistic,"00:09:01,120","00:09:03,320",138,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=541,we will have two kinds of parameters.
cs-410_9_1_139,cs-410,9,1,Probabilistic,"00:09:03,320","00:09:07,400",139,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=543,"One is the two worded distributions,"
cs-410_9_1_140,cs-410,9,1,Probabilistic,"00:09:07,400","00:09:10,380",140,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=547,the other is the coverage
cs-410_9_1_141,cs-410,9,1,Probabilistic,"00:09:12,560","00:09:14,340",141,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=552,The coverage of each topic.
cs-410_9_1_142,cs-410,9,1,Probabilistic,"00:09:14,340","00:09:17,630",142,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=554,And this is determined by
cs-410_9_1_143,cs-410,9,1,Probabilistic,"00:09:17,630","00:09:22,310",143,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=557,"probability of theta, so this is to one."
cs-410_9_1_144,cs-410,9,1,Probabilistic,"00:09:22,310","00:09:25,040",144,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=562,"Now, what's interesting is"
cs-410_9_1_145,cs-410,9,1,Probabilistic,"00:09:25,040","00:09:29,540",145,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=565,cases like when we send one of
cs-410_9_1_146,cs-410,9,1,Probabilistic,"00:09:29,540","00:09:32,770",146,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=569,"Well with the other, with the zero right?"
cs-410_9_1_147,cs-410,9,1,Probabilistic,"00:09:32,770","00:09:35,150",147,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=572,And if you look at
cs-410_9_1_148,cs-410,9,1,Probabilistic,"00:09:36,320","00:09:40,640",148,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=576,it will then degenerate to the special
cs-410_9_1_149,cs-410,9,1,Probabilistic,"00:09:40,640","00:09:46,290",149,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=580,Okay so you can easily verify that by
cs-410_9_1_150,cs-410,9,1,Probabilistic,"00:09:46,290","00:09:47,940",150,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=586,the other is Zero.
cs-410_9_1_151,cs-410,9,1,Probabilistic,"00:09:49,130","00:09:53,290",151,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=589,"So in this sense,"
cs-410_9_1_152,cs-410,9,1,Probabilistic,"00:09:53,290","00:09:56,490",152,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=593,the previous model where we
cs-410_9_1_153,cs-410,9,1,Probabilistic,"00:09:56,490","00:09:58,740",153,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=596,It can cover that as a special case.
cs-410_9_1_154,cs-410,9,1,Probabilistic,"00:09:59,960","00:10:05,340",154,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=599,"So to summarize, we talked about the"
cs-410_9_1_155,cs-410,9,1,Probabilistic,"00:10:05,340","00:10:09,110",155,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=605,the data we're considering
cs-410_9_1_156,cs-410,9,1,Probabilistic,"00:10:09,110","00:10:13,420",156,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=609,And the model is a mixture
cs-410_9_1_157,cs-410,9,1,Probabilistic,"00:10:13,420","00:10:16,830",157,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=613,"two unigram LM models,"
cs-410_9_1_158,cs-410,9,1,Probabilistic,"00:10:16,830","00:10:22,810",158,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=616,which is intended to denote the topic of
cs-410_9_1_159,cs-410,9,1,Probabilistic,"00:10:22,810","00:10:28,500",159,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=622,representing a background topic that
cs-410_9_1_160,cs-410,9,1,Probabilistic,"00:10:28,500","00:10:32,840",160,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=628,words because common words would be
cs-410_9_1_161,cs-410,9,1,Probabilistic,"00:10:33,950","00:10:36,870",161,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=633,So the parameters can
cs-410_9_1_162,cs-410,9,1,Probabilistic,"00:10:36,870","00:10:40,380",162,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=636,Lambda which I show here you can again
cs-410_9_1_163,cs-410,9,1,Probabilistic,"00:10:41,560","00:10:45,520",163,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=641,think about the question about how many
cs-410_9_1_164,cs-410,9,1,Probabilistic,"00:10:45,520","00:10:50,920",164,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=645,This is usually a good exercise to do
cs-410_9_1_165,cs-410,9,1,Probabilistic,"00:10:50,920","00:10:56,470",165,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=650,depth and to have a complete understanding
cs-410_9_1_166,cs-410,9,1,Probabilistic,"00:10:56,470","00:10:58,700",166,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=656,"And we have mixing weights,"
cs-410_9_1_167,cs-410,9,1,Probabilistic,"00:10:59,790","00:11:02,340",167,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=659,So what does a likelihood
cs-410_9_1_168,cs-410,9,1,Probabilistic,"00:11:02,340","00:11:06,620",168,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=662,"Well, it looks very similar"
cs-410_9_1_169,cs-410,9,1,Probabilistic,"00:11:06,620","00:11:09,100",169,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=666,"So for the document,"
cs-410_9_1_170,cs-410,9,1,Probabilistic,"00:11:09,100","00:11:14,260",170,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=669,first it's a product over all the words in
cs-410_9_1_171,cs-410,9,1,Probabilistic,"00:11:14,260","00:11:20,200",171,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=674,The only difference is that inside here
cs-410_9_1_172,cs-410,9,1,Probabilistic,"00:11:20,200","00:11:24,420",172,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=680,So you might have recalled before
cs-410_9_1_173,cs-410,9,1,Probabilistic,"00:11:25,420","00:11:30,610",173,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=685,But now we have this sum
cs-410_9_1_174,cs-410,9,1,Probabilistic,"00:11:30,610","00:11:34,800",174,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=690,And because of the mixture model we
cs-410_9_1_175,cs-410,9,1,Probabilistic,"00:11:34,800","00:11:37,640",175,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=694,choosing that particular
cs-410_9_1_176,cs-410,9,1,Probabilistic,"00:11:39,530","00:11:44,470",176,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=699,And so
cs-410_9_1_177,cs-410,9,1,Probabilistic,"00:11:44,470","00:11:49,800",177,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=704,by using a product over all the unique
cs-410_9_1_178,cs-410,9,1,Probabilistic,"00:11:49,800","00:11:52,878",178,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=709,having that product over all
cs-410_9_1_179,cs-410,9,1,Probabilistic,"00:11:52,878","00:11:57,582",179,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=712,And this form where we look at
cs-410_9_1_180,cs-410,9,1,Probabilistic,"00:11:57,582","00:12:04,720",180,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=717,a commutative that formed for computing
cs-410_9_1_181,cs-410,9,1,Probabilistic,"00:12:04,720","00:12:09,965",181,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=724,"And the maximum likelihood estimator is,"
cs-410_9_1_182,cs-410,9,1,Probabilistic,"00:12:09,965","00:12:15,290",182,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=729,just to find the parameters that would
cs-410_9_1_183,cs-410,9,1,Probabilistic,"00:12:15,290","00:12:18,940",183,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=735,And the constraints here
cs-410_9_1_184,cs-410,9,1,Probabilistic,"00:12:18,940","00:12:24,125",184,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=738,One is what are probabilities in each
cs-410_9_1_185,cs-410,9,1,Probabilistic,"00:12:24,125","00:12:29,142",185,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=744,[INAUDIBLE] must sum to 1 the other is
cs-410_9_1_186,cs-410,9,1,Probabilistic,"00:12:29,142","00:12:35,343",186,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=749,the choice of each
cs-410_9_1_187,cs-410,9,1,Probabilistic,"00:12:35,343","00:12:39,799",187,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=755,[MUSIC]
cs-410_9_2_1,cs-410,9,2,Probabilistic,"00:00:06,710","00:00:11,380",1,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=6,This lecture is about
cs-410_9_2_2,cs-410,9,2,Probabilistic,"00:00:11,380","00:00:13,890",2,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=11,"In this lecture, we're"
cs-410_9_2_3,cs-410,9,2,Probabilistic,"00:00:13,890","00:00:15,990",3,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=13,discussing probabilistic
cs-410_9_2_4,cs-410,9,2,Probabilistic,"00:00:15,990","00:00:18,075",4,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=15,"In particular, we're going"
cs-410_9_2_5,cs-410,9,2,Probabilistic,"00:00:18,075","00:00:21,490",5,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=18,estimate the parameters
cs-410_9_2_6,cs-410,9,2,Probabilistic,"00:00:21,920","00:00:24,780",6,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=21,So let's first look
cs-410_9_2_7,cs-410,9,2,Probabilistic,"00:00:24,780","00:00:26,760",7,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=24,"for using a mixture model,"
cs-410_9_2_8,cs-410,9,2,Probabilistic,"00:00:26,760","00:00:29,040",8,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=26,and we hope to effect out
cs-410_9_2_9,cs-410,9,2,Probabilistic,"00:00:29,040","00:00:32,610",9,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=29,the background words from
cs-410_9_2_10,cs-410,9,2,Probabilistic,"00:00:32,610","00:00:35,565",10,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=32,So the idea is to assume that
cs-410_9_2_11,cs-410,9,2,Probabilistic,"00:00:35,565","00:00:39,510",11,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=35,the text data actually
cs-410_9_2_12,cs-410,9,2,Probabilistic,"00:00:39,510","00:00:44,910",12,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=39,One kind is from
cs-410_9_2_13,cs-410,9,2,Probabilistic,"00:00:44,910","00:00:48,555",13,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=44,"so the ""is"", ""we"" etc."
cs-410_9_2_14,cs-410,9,2,Probabilistic,"00:00:48,555","00:00:50,675",14,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=48,The other kind is from
cs-410_9_2_15,cs-410,9,2,Probabilistic,"00:00:50,675","00:00:54,900",15,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=50,our topic word distribution
cs-410_9_2_16,cs-410,9,2,Probabilistic,"00:00:55,210","00:00:58,565",16,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=55,So in order to solve
cs-410_9_2_17,cs-410,9,2,Probabilistic,"00:00:58,565","00:01:01,175",17,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=58,"factoring out background words,"
cs-410_9_2_18,cs-410,9,2,Probabilistic,"00:01:01,175","00:01:05,555",18,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=61,we can set up our mixture
cs-410_9_2_19,cs-410,9,2,Probabilistic,"00:01:05,555","00:01:07,160",19,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=65,We are going to assume that
cs-410_9_2_20,cs-410,9,2,Probabilistic,"00:01:07,160","00:01:08,735",20,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=67,we already know the parameters
cs-410_9_2_21,cs-410,9,2,Probabilistic,"00:01:08,735","00:01:12,110",21,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=68,of all the values for
cs-410_9_2_22,cs-410,9,2,Probabilistic,"00:01:12,110","00:01:15,065",22,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=72,all the parameters in
cs-410_9_2_23,cs-410,9,2,Probabilistic,"00:01:15,065","00:01:19,610",23,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=75,the word distribution of
cs-410_9_2_24,cs-410,9,2,Probabilistic,"00:01:19,610","00:01:24,020",24,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=79,So this is a case of
cs-410_9_2_25,cs-410,9,2,Probabilistic,"00:01:24,020","00:01:26,300",25,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=84,some model so that we
cs-410_9_2_26,cs-410,9,2,Probabilistic,"00:01:26,300","00:01:29,450",26,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=86,embedded the unknown variables
cs-410_9_2_27,cs-410,9,2,Probabilistic,"00:01:29,450","00:01:31,175",27,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=89,but we're going to
cs-410_9_2_28,cs-410,9,2,Probabilistic,"00:01:31,175","00:01:32,810",28,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=91,We're going to assume
cs-410_9_2_29,cs-410,9,2,Probabilistic,"00:01:32,810","00:01:34,550",29,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=92,others and this is
cs-410_9_2_30,cs-410,9,2,Probabilistic,"00:01:34,550","00:01:35,795",30,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=94,a powerful way of
cs-410_9_2_31,cs-410,9,2,Probabilistic,"00:01:35,795","00:01:39,110",31,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=95,customizing a model
cs-410_9_2_32,cs-410,9,2,Probabilistic,"00:01:39,110","00:01:41,915",32,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=99,"Now you can imagine, we"
cs-410_9_2_33,cs-410,9,2,Probabilistic,"00:01:41,915","00:01:44,915",33,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=101,we also don't know the
cs-410_9_2_34,cs-410,9,2,Probabilistic,"00:01:44,915","00:01:47,630",34,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=104,"but in this case,"
cs-410_9_2_35,cs-410,9,2,Probabilistic,"00:01:47,630","00:01:51,635",35,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=107,precisely those high probability
cs-410_9_2_36,cs-410,9,2,Probabilistic,"00:01:51,635","00:01:56,600",36,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=111,So we assume the background
cs-410_9_2_37,cs-410,9,2,Probabilistic,"00:01:56,600","00:01:58,850",37,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=116,"The problem here is,"
cs-410_9_2_38,cs-410,9,2,Probabilistic,"00:01:58,850","00:02:02,840",38,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=118,how can we adjust the Theta sub
cs-410_9_2_39,cs-410,9,2,Probabilistic,"00:02:02,840","00:02:05,270",39,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=122,the probability of
cs-410_9_2_40,cs-410,9,2,Probabilistic,"00:02:05,270","00:02:08,675",40,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=125,here and we assume all the
cs-410_9_2_41,cs-410,9,2,Probabilistic,"00:02:08,675","00:02:11,780",41,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=128,"Now, although we"
cs-410_9_2_42,cs-410,9,2,Probabilistic,"00:02:11,780","00:02:13,040",42,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=131,heuristically to try to
cs-410_9_2_43,cs-410,9,2,Probabilistic,"00:02:13,040","00:02:15,395",43,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=133,factor out these
cs-410_9_2_44,cs-410,9,2,Probabilistic,"00:02:15,395","00:02:18,470",44,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=135,it's unclear whether if
cs-410_9_2_45,cs-410,9,2,Probabilistic,"00:02:18,470","00:02:20,860",45,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=138,we use maximum
cs-410_9_2_46,cs-410,9,2,Probabilistic,"00:02:20,860","00:02:24,500",46,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=140,we will actually end up having
cs-410_9_2_47,cs-410,9,2,Probabilistic,"00:02:24,500","00:02:27,320",47,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=144,"the common words like ""the"" will"
cs-410_9_2_48,cs-410,9,2,Probabilistic,"00:02:27,320","00:02:30,470",48,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=147,be indeed having smaller
cs-410_9_2_49,cs-410,9,2,Probabilistic,"00:02:30,470","00:02:33,950",49,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=150,"So now, in this case,"
cs-410_9_2_50,cs-410,9,2,Probabilistic,"00:02:33,950","00:02:37,420",50,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=153,it turns out that
cs-410_9_2_51,cs-410,9,2,Probabilistic,"00:02:37,420","00:02:41,000",51,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=157,When we set up the probabilistic
cs-410_9_2_52,cs-410,9,2,Probabilistic,"00:02:41,000","00:02:43,009",52,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=161,when we use maximum
cs-410_9_2_53,cs-410,9,2,Probabilistic,"00:02:43,009","00:02:47,180",53,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=163,we will end up having
cs-410_9_2_54,cs-410,9,2,Probabilistic,"00:02:47,180","00:02:48,950",54,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=167,the common words
cs-410_9_2_55,cs-410,9,2,Probabilistic,"00:02:48,950","00:02:53,075",55,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=168,out by the use of
cs-410_9_2_56,cs-410,9,2,Probabilistic,"00:02:53,075","00:02:56,599",56,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=173,"So to understand why this is so,"
cs-410_9_2_57,cs-410,9,2,Probabilistic,"00:02:56,599","00:03:00,425",57,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=176,it's useful to examine
cs-410_9_2_58,cs-410,9,2,Probabilistic,"00:03:00,425","00:03:03,655",58,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=180,So we're going to look
cs-410_9_2_59,cs-410,9,2,Probabilistic,"00:03:03,655","00:03:05,165",59,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=183,In order to understand
cs-410_9_2_60,cs-410,9,2,Probabilistic,"00:03:05,165","00:03:08,450",60,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=185,some interesting behaviors
cs-410_9_2_61,cs-410,9,2,Probabilistic,"00:03:08,450","00:03:11,450",61,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=188,the observed patterns
cs-410_9_2_62,cs-410,9,2,Probabilistic,"00:03:11,450","00:03:15,005",62,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=191,generalizable to mixture
cs-410_9_2_63,cs-410,9,2,Probabilistic,"00:03:15,005","00:03:18,020",63,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=195,but it's much easier to
cs-410_9_2_64,cs-410,9,2,Probabilistic,"00:03:18,020","00:03:21,455",64,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=198,we use a very simple case
cs-410_9_2_65,cs-410,9,2,Probabilistic,"00:03:21,455","00:03:24,020",65,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=201,"So specifically in this case,"
cs-410_9_2_66,cs-410,9,2,Probabilistic,"00:03:24,020","00:03:26,105",66,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=204,let's assume that
cs-410_9_2_67,cs-410,9,2,Probabilistic,"00:03:26,105","00:03:29,345",67,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=206,choosing each of the two models
cs-410_9_2_68,cs-410,9,2,Probabilistic,"00:03:29,345","00:03:30,650",68,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=209,So we're going to flip
cs-410_9_2_69,cs-410,9,2,Probabilistic,"00:03:30,650","00:03:33,860",69,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=210,a fair coin to decide
cs-410_9_2_70,cs-410,9,2,Probabilistic,"00:03:33,860","00:03:36,530",70,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=213,"Furthermore, we are going"
cs-410_9_2_71,cs-410,9,2,Probabilistic,"00:03:36,530","00:03:39,140",71,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=216,"precisely to words,"
cs-410_9_2_72,cs-410,9,2,Probabilistic,"00:03:39,140","00:03:44,015",72,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=219,"Obviously, this is"
cs-410_9_2_73,cs-410,9,2,Probabilistic,"00:03:44,015","00:03:45,810",73,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=224,"of the actual text,"
cs-410_9_2_74,cs-410,9,2,Probabilistic,"00:03:45,810","00:03:48,000",74,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=225,"but again, it is useful"
cs-410_9_2_75,cs-410,9,2,Probabilistic,"00:03:48,000","00:03:52,865",75,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=228,to examine the behavior
cs-410_9_2_76,cs-410,9,2,Probabilistic,"00:03:52,865","00:03:55,710",76,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=232,"So we further assume that,"
cs-410_9_2_77,cs-410,9,2,Probabilistic,"00:03:55,710","00:03:58,520",77,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=235,the background model gives
cs-410_9_2_78,cs-410,9,2,Probabilistic,"00:03:58,520","00:04:02,840",78,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=238,"the word ""the"" and ""text"" 0.1."
cs-410_9_2_79,cs-410,9,2,Probabilistic,"00:04:02,840","00:04:07,070",79,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=242,"Now, let's also assume that"
cs-410_9_2_80,cs-410,9,2,Probabilistic,"00:04:07,070","00:04:09,995",80,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=247,The document has just two words
cs-410_9_2_81,cs-410,9,2,Probabilistic,"00:04:09,995","00:04:11,570",81,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=249,"So now, let's write down"
cs-410_9_2_82,cs-410,9,2,Probabilistic,"00:04:11,570","00:04:13,610",82,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=251,the likelihood function
cs-410_9_2_83,cs-410,9,2,Probabilistic,"00:04:13,610","00:04:16,220",83,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=253,"First, what's the probability"
cs-410_9_2_84,cs-410,9,2,Probabilistic,"00:04:16,220","00:04:18,995",84,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=256,"of ""text"" and what's the"
cs-410_9_2_85,cs-410,9,2,Probabilistic,"00:04:18,995","00:04:20,920",85,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=258,"I hope by this point,"
cs-410_9_2_86,cs-410,9,2,Probabilistic,"00:04:20,920","00:04:23,045",86,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=260,you will be able
cs-410_9_2_87,cs-410,9,2,Probabilistic,"00:04:23,045","00:04:26,615",87,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=263,"So the probability of ""text"" is"
cs-410_9_2_88,cs-410,9,2,Probabilistic,"00:04:26,615","00:04:30,275",88,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=266,basically a sum of
cs-410_9_2_89,cs-410,9,2,Probabilistic,"00:04:30,275","00:04:32,240",89,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=270,corresponds to each of
cs-410_9_2_90,cs-410,9,2,Probabilistic,"00:04:32,240","00:04:34,760",90,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=272,the water distribution and
cs-410_9_2_91,cs-410,9,2,Probabilistic,"00:04:34,760","00:04:38,930",91,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=274,it accounts for the two ways
cs-410_9_2_92,cs-410,9,2,Probabilistic,"00:04:38,930","00:04:41,570",92,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=278,"Inside each case, we have"
cs-410_9_2_93,cs-410,9,2,Probabilistic,"00:04:41,570","00:04:43,820",93,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=281,the probability of choosing
cs-410_9_2_94,cs-410,9,2,Probabilistic,"00:04:43,820","00:04:46,940",94,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=283,0.5 multiplied by the probability
cs-410_9_2_95,cs-410,9,2,Probabilistic,"00:04:46,940","00:04:49,895",95,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=286,"of observing ""text"""
cs-410_9_2_96,cs-410,9,2,Probabilistic,"00:04:49,895","00:04:53,450",96,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=289,"Similarly, ""the"" would"
cs-410_9_2_97,cs-410,9,2,Probabilistic,"00:04:53,450","00:04:55,250",97,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=293,the same form just as it
cs-410_9_2_98,cs-410,9,2,Probabilistic,"00:04:55,250","00:04:58,205",98,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=295,was different exactly
cs-410_9_2_99,cs-410,9,2,Probabilistic,"00:04:58,205","00:05:01,130",99,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=298,"So naturally,"
cs-410_9_2_100,cs-410,9,2,Probabilistic,"00:05:01,130","00:05:03,170",100,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=301,is just the product of the two.
cs-410_9_2_101,cs-410,9,2,Probabilistic,"00:05:03,170","00:05:07,685",101,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=303,"So it's very easy to see that,"
cs-410_9_2_102,cs-410,9,2,Probabilistic,"00:05:07,685","00:05:10,070",102,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=307,once you understand
cs-410_9_2_103,cs-410,9,2,Probabilistic,"00:05:10,070","00:05:12,470",103,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=310,each word and which
cs-410_9_2_104,cs-410,9,2,Probabilistic,"00:05:12,470","00:05:14,930",104,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=312,important to understand what's
cs-410_9_2_105,cs-410,9,2,Probabilistic,"00:05:14,930","00:05:16,505",105,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=314,exactly the probability of
cs-410_9_2_106,cs-410,9,2,Probabilistic,"00:05:16,505","00:05:19,190",106,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=316,observing each word from
cs-410_9_2_107,cs-410,9,2,Probabilistic,"00:05:19,190","00:05:21,860",107,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=319,"Now, the interesting"
cs-410_9_2_108,cs-410,9,2,Probabilistic,"00:05:21,860","00:05:25,280",108,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=321,how can we then optimize
cs-410_9_2_109,cs-410,9,2,Probabilistic,"00:05:25,280","00:05:27,575",109,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=325,"Well, you will notice that,"
cs-410_9_2_110,cs-410,9,2,Probabilistic,"00:05:27,575","00:05:29,165",110,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=327,there are only two variables.
cs-410_9_2_111,cs-410,9,2,Probabilistic,"00:05:29,165","00:05:31,385",111,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=329,They are precisely
cs-410_9_2_112,cs-410,9,2,Probabilistic,"00:05:31,385","00:05:33,800",112,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=331,of the two words
cs-410_9_2_113,cs-410,9,2,Probabilistic,"00:05:33,800","00:05:38,320",113,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=333,by Theta sub d. This is
cs-410_9_2_114,cs-410,9,2,Probabilistic,"00:05:38,320","00:05:40,430",114,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=338,all the other
cs-410_9_2_115,cs-410,9,2,Probabilistic,"00:05:40,430","00:05:45,300",115,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=340,"So now, the question is"
cs-410_9_2_116,cs-410,9,2,Probabilistic,"00:05:45,300","00:05:46,610",116,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=345,So we have a simple expression
cs-410_9_2_117,cs-410,9,2,Probabilistic,"00:05:46,610","00:05:48,830",117,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=346,with two variables and we hope
cs-410_9_2_118,cs-410,9,2,Probabilistic,"00:05:48,830","00:05:50,990",118,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=348,to choose the values of
cs-410_9_2_119,cs-410,9,2,Probabilistic,"00:05:50,990","00:05:53,785",119,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=350,these two variables to
cs-410_9_2_120,cs-410,9,2,Probabilistic,"00:05:53,785","00:05:56,300",120,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=353,It's exercises that we have
cs-410_9_2_121,cs-410,9,2,Probabilistic,"00:05:56,300","00:05:59,480",121,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=356,seen some simple
cs-410_9_2_122,cs-410,9,2,Probabilistic,"00:05:59,480","00:06:03,200",122,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=359,and note that the two
cs-410_9_2_123,cs-410,9,2,Probabilistic,"00:06:03,200","00:06:05,465",123,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=363,So there's some constraint.
cs-410_9_2_124,cs-410,9,2,Probabilistic,"00:06:05,465","00:06:08,000",124,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=365,If there were
cs-410_9_2_125,cs-410,9,2,Probabilistic,"00:06:08,000","00:06:09,935",125,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=368,we will set both probabilities to
cs-410_9_2_126,cs-410,9,2,Probabilistic,"00:06:09,935","00:06:13,250",126,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=369,their maximum value which
cs-410_9_2_127,cs-410,9,2,Probabilistic,"00:06:13,250","00:06:14,675",127,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=373,but we can't do that
cs-410_9_2_128,cs-410,9,2,Probabilistic,"00:06:14,675","00:06:17,930",128,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=374,"because ""text"" and"
cs-410_9_2_129,cs-410,9,2,Probabilistic,"00:06:17,930","00:06:21,025",129,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=377,We can't give those a
cs-410_9_2_130,cs-410,9,2,Probabilistic,"00:06:21,025","00:06:23,065",130,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=381,"So now the question is,"
cs-410_9_2_131,cs-410,9,2,Probabilistic,"00:06:23,065","00:06:25,160",131,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=383,how should we allocate
cs-410_9_2_132,cs-410,9,2,Probabilistic,"00:06:25,160","00:06:27,995",132,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=385,the mass between the two words?
cs-410_9_2_133,cs-410,9,2,Probabilistic,"00:06:27,995","00:06:30,080",133,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=387,"Now, it will be useful to look at"
cs-410_9_2_134,cs-410,9,2,Probabilistic,"00:06:30,080","00:06:33,175",134,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=390,this formula for
cs-410_9_2_135,cs-410,9,2,Probabilistic,"00:06:33,175","00:06:36,325",135,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=393,intuitively what
cs-410_9_2_136,cs-410,9,2,Probabilistic,"00:06:36,325","00:06:38,300",136,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=396,set these probabilities to
cs-410_9_2_137,cs-410,9,2,Probabilistic,"00:06:38,300","00:06:40,980",137,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=398,maximize the value
cs-410_9_2_138,cs-410,9,2,Probabilistic,"00:06:41,780","00:06:44,260",138,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=401,"If we look into this further,"
cs-410_9_2_139,cs-410,9,2,Probabilistic,"00:06:44,260","00:06:46,250",139,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=404,then we'll see
cs-410_9_2_140,cs-410,9,2,Probabilistic,"00:06:46,250","00:06:50,120",140,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=406,of the two component
cs-410_9_2_141,cs-410,9,2,Probabilistic,"00:06:50,120","00:06:53,000",141,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=410,they will be
cs-410_9_2_142,cs-410,9,2,Probabilistic,"00:06:53,000","00:06:55,070",142,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=413,the probability of
cs-410_9_2_143,cs-410,9,2,Probabilistic,"00:06:55,070","00:06:57,440",143,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=415,is dictated by the maximum
cs-410_9_2_144,cs-410,9,2,Probabilistic,"00:06:57,440","00:07:00,815",144,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=417,but they're also
cs-410_9_2_145,cs-410,9,2,Probabilistic,"00:07:00,815","00:07:03,965",145,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=420,"In particular, they"
cs-410_9_2_146,cs-410,9,2,Probabilistic,"00:07:03,965","00:07:06,650",146,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=423,the words and they
cs-410_9_2_147,cs-410,9,2,Probabilistic,"00:07:06,650","00:07:09,665",147,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=426,high probabilities on
cs-410_9_2_148,cs-410,9,2,Probabilistic,"00:07:09,665","00:07:11,585",148,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=429,this competition in some sense
cs-410_9_2_149,cs-410,9,2,Probabilistic,"00:07:11,585","00:07:14,240",149,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=431,or to gain advantage
cs-410_9_2_150,cs-410,9,2,Probabilistic,"00:07:14,240","00:07:17,150",150,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=434,"So again, looking at this"
cs-410_9_2_151,cs-410,9,2,Probabilistic,"00:07:17,150","00:07:20,790",151,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=437,a constraint on
cs-410_9_2_152,cs-410,9,2,Probabilistic,"00:07:20,790","00:07:25,530",152,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=440,now if you look at
cs-410_9_2_153,cs-410,9,2,Probabilistic,"00:07:25,530","00:07:27,605",153,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=445,you might feel that
cs-410_9_2_154,cs-410,9,2,Probabilistic,"00:07:27,605","00:07:29,330",154,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=447,"the probability of ""text"""
cs-410_9_2_155,cs-410,9,2,Probabilistic,"00:07:29,330","00:07:31,720",155,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=449,"to be somewhat larger than ""the""."
cs-410_9_2_156,cs-410,9,2,Probabilistic,"00:07:31,720","00:07:34,489",156,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=451,This intuition can
cs-410_9_2_157,cs-410,9,2,Probabilistic,"00:07:34,489","00:07:37,190",157,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=454,"by mathematical fact which is,"
cs-410_9_2_158,cs-410,9,2,Probabilistic,"00:07:37,190","00:07:40,550",158,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=457,when the sum of
cs-410_9_2_159,cs-410,9,2,Probabilistic,"00:07:40,550","00:07:42,770",159,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=460,constant then the product of
cs-410_9_2_160,cs-410,9,2,Probabilistic,"00:07:42,770","00:07:45,205",160,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=462,them which is maximum
cs-410_9_2_161,cs-410,9,2,Probabilistic,"00:07:45,205","00:07:47,645",161,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=465,and this is a fact that
cs-410_9_2_162,cs-410,9,2,Probabilistic,"00:07:47,645","00:07:49,145",162,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=467,"Now, if we plug that in,"
cs-410_9_2_163,cs-410,9,2,Probabilistic,"00:07:49,145","00:07:51,890",163,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=469,we will would mean
cs-410_9_2_164,cs-410,9,2,Probabilistic,"00:07:51,890","00:07:55,750",164,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=471,the two probabilities equal.
cs-410_9_2_165,cs-410,9,2,Probabilistic,"00:07:55,750","00:07:58,760",165,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=475,When we make them equal
cs-410_9_2_166,cs-410,9,2,Probabilistic,"00:07:58,760","00:08:01,790",166,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=478,the constraint that we can
cs-410_9_2_167,cs-410,9,2,Probabilistic,"00:08:01,790","00:08:04,730",167,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=481,and the solution is the
cs-410_9_2_168,cs-410,9,2,Probabilistic,"00:08:04,730","00:08:07,610",168,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=484,would be 0.9 and probability
cs-410_9_2_169,cs-410,9,2,Probabilistic,"00:08:07,610","00:08:09,125",169,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=487,"As you can see indeed,"
cs-410_9_2_170,cs-410,9,2,Probabilistic,"00:08:09,125","00:08:11,840",170,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=489,the probability of text
cs-410_9_2_171,cs-410,9,2,Probabilistic,"00:08:11,840","00:08:14,120",171,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=491,"probability of ""the"" and"
cs-410_9_2_172,cs-410,9,2,Probabilistic,"00:08:14,120","00:08:16,895",172,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=494,this is not the case when we
cs-410_9_2_173,cs-410,9,2,Probabilistic,"00:08:16,895","00:08:18,950",173,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=496,This is clearly because of
cs-410_9_2_174,cs-410,9,2,Probabilistic,"00:08:18,950","00:08:21,350",174,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=498,the use of the
cs-410_9_2_175,cs-410,9,2,Probabilistic,"00:08:21,350","00:08:23,645",175,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=501,assign a very high probability
cs-410_9_2_176,cs-410,9,2,Probabilistic,"00:08:23,645","00:08:26,250",176,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=503,"to ""the"" low"
cs-410_9_2_177,cs-410,9,2,Probabilistic,"00:08:26,250","00:08:28,150",177,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=506,"If you look at the equation,"
cs-410_9_2_178,cs-410,9,2,Probabilistic,"00:08:28,150","00:08:29,990",178,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=508,you will see obviously
cs-410_9_2_179,cs-410,9,2,Probabilistic,"00:08:29,990","00:08:34,180",179,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=509,some interaction of
cs-410_9_2_180,cs-410,9,2,Probabilistic,"00:08:34,180","00:08:37,100",180,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=514,"In particular, you will see in"
cs-410_9_2_181,cs-410,9,2,Probabilistic,"00:08:37,100","00:08:39,695",181,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=517,order to make them equal and then
cs-410_9_2_182,cs-410,9,2,Probabilistic,"00:08:39,695","00:08:45,050",182,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=519,the probability assigned
cs-410_9_2_183,cs-410,9,2,Probabilistic,"00:08:45,050","00:08:47,360",183,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=525,be higher for a word that has
cs-410_9_2_184,cs-410,9,2,Probabilistic,"00:08:47,360","00:08:51,980",184,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=527,a smaller probability
cs-410_9_2_185,cs-410,9,2,Probabilistic,"00:08:52,290","00:08:56,420",185,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=532,This is obvious from
cs-410_9_2_186,cs-410,9,2,Probabilistic,"00:08:56,420","00:08:58,410",186,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=536,"because ""the"" background part is"
cs-410_9_2_187,cs-410,9,2,Probabilistic,"00:08:58,410","00:09:00,500",187,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=538,"weak for ""text"" it's a small."
cs-410_9_2_188,cs-410,9,2,Probabilistic,"00:09:00,500","00:09:02,750",188,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=540,So in order to
cs-410_9_2_189,cs-410,9,2,Probabilistic,"00:09:02,750","00:09:05,620",189,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=542,we must make the probability
cs-410_9_2_190,cs-410,9,2,Probabilistic,"00:09:05,620","00:09:07,340",190,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=545,Theta sub d somewhat
cs-410_9_2_191,cs-410,9,2,Probabilistic,"00:09:07,340","00:09:10,955",191,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=547,larger so that the two sides
cs-410_9_2_192,cs-410,9,2,Probabilistic,"00:09:10,955","00:09:12,290",192,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=550,So this is in fact
cs-410_9_2_193,cs-410,9,2,Probabilistic,"00:09:12,290","00:09:17,075",193,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=552,a very general behavior
cs-410_9_2_194,cs-410,9,2,Probabilistic,"00:09:17,075","00:09:18,965",194,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=557,"That is, if one distribution"
cs-410_9_2_195,cs-410,9,2,Probabilistic,"00:09:18,965","00:09:21,575",195,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=558,assigns a high probability
cs-410_9_2_196,cs-410,9,2,Probabilistic,"00:09:21,575","00:09:23,180",196,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=561,then the other distribution
cs-410_9_2_197,cs-410,9,2,Probabilistic,"00:09:23,180","00:09:25,240",197,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=563,would tend to do the opposite.
cs-410_9_2_198,cs-410,9,2,Probabilistic,"00:09:25,240","00:09:26,835",198,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=565,"Basically, it would discourage"
cs-410_9_2_199,cs-410,9,2,Probabilistic,"00:09:26,835","00:09:28,080",199,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=566,other distributions to do the
cs-410_9_2_200,cs-410,9,2,Probabilistic,"00:09:28,080","00:09:31,655",200,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=568,same and this is to
cs-410_9_2_201,cs-410,9,2,Probabilistic,"00:09:31,655","00:09:34,340",201,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=571,we can account for all words.
cs-410_9_2_202,cs-410,9,2,Probabilistic,"00:09:34,340","00:09:36,150",202,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=574,"This also means that,"
cs-410_9_2_203,cs-410,9,2,Probabilistic,"00:09:36,150","00:09:38,780",203,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=576,by using a background
cs-410_9_2_204,cs-410,9,2,Probabilistic,"00:09:38,780","00:09:41,515",204,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=578,fixed to assign high probabilities
cs-410_9_2_205,cs-410,9,2,Probabilistic,"00:09:41,515","00:09:43,689",205,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=581,we can indeed encourage
cs-410_9_2_206,cs-410,9,2,Probabilistic,"00:09:43,689","00:09:46,175",206,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=583,the unknown topic
cs-410_9_2_207,cs-410,9,2,Probabilistic,"00:09:46,175","00:09:49,855",207,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=586,assign smaller probabilities
cs-410_9_2_208,cs-410,9,2,Probabilistic,"00:09:49,855","00:09:54,125",208,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=589,"Instead, put more probability"
cs-410_9_2_209,cs-410,9,2,Probabilistic,"00:09:54,125","00:09:56,360",209,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=594,that cannot be explained well by
cs-410_9_2_210,cs-410,9,2,Probabilistic,"00:09:56,360","00:09:58,865",210,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=596,the background
cs-410_9_2_211,cs-410,9,2,Probabilistic,"00:09:58,865","00:10:00,739",211,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=598,they have a very
cs-410_9_2_212,cs-410,9,2,Probabilistic,"00:10:00,739","00:10:04,100",212,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=600,from the background
cs-410_9_3_1,cs-410,9,3,Probabilistic,"00:00:06,170","00:00:08,340",1,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=6,This lecture is about
cs-410_9_3_2,cs-410,9,3,Probabilistic,"00:00:08,340","00:00:11,250",2,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=8,the expectation-maximization
cs-410_9_3_3,cs-410,9,3,Probabilistic,"00:00:11,250","00:00:13,380",3,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=11,also called the EM algorithm.
cs-410_9_3_4,cs-410,9,3,Probabilistic,"00:00:13,380","00:00:15,960",4,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=13,"In this lecture, we're"
cs-410_9_3_5,cs-410,9,3,Probabilistic,"00:00:15,960","00:00:18,345",5,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=15,the discussion of
cs-410_9_3_6,cs-410,9,3,Probabilistic,"00:00:18,345","00:00:22,205",6,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=18,"In particular, we're going to"
cs-410_9_3_7,cs-410,9,3,Probabilistic,"00:00:22,205","00:00:26,040",7,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=22,which is a family of
cs-410_9_3_8,cs-410,9,3,Probabilistic,"00:00:26,040","00:00:28,605",8,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=26,the maximum likelihood estimate
cs-410_9_3_9,cs-410,9,3,Probabilistic,"00:00:28,605","00:00:30,180",9,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=28,So this is now
cs-410_9_3_10,cs-410,9,3,Probabilistic,"00:00:30,180","00:00:33,330",10,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=30,familiar scenario of
cs-410_9_3_11,cs-410,9,3,Probabilistic,"00:00:33,330","00:00:34,620",11,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=33,"the mixture model, to try"
cs-410_9_3_12,cs-410,9,3,Probabilistic,"00:00:34,620","00:00:36,225",12,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=34,to factor out
cs-410_9_3_13,cs-410,9,3,Probabilistic,"00:00:36,225","00:00:40,185",13,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=36,from one topic word
cs-410_9_3_14,cs-410,9,3,Probabilistic,"00:00:40,185","00:00:46,695",14,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=40,So we're interested in
cs-410_9_3_15,cs-410,9,3,Probabilistic,"00:00:46,695","00:00:49,715",15,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=46,and we're going to try to adjust
cs-410_9_3_16,cs-410,9,3,Probabilistic,"00:00:49,715","00:00:51,710",16,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=49,these probability values to
cs-410_9_3_17,cs-410,9,3,Probabilistic,"00:00:51,710","00:00:55,615",17,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=51,maximize the probability
cs-410_9_3_18,cs-410,9,3,Probabilistic,"00:00:55,615","00:00:57,050",18,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=55,Note that we assume that all
cs-410_9_3_19,cs-410,9,3,Probabilistic,"00:00:57,050","00:00:58,520",19,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=57,the other parameters are known.
cs-410_9_3_20,cs-410,9,3,Probabilistic,"00:00:58,520","00:01:00,875",20,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=58,So the only thing unknown is
cs-410_9_3_21,cs-410,9,3,Probabilistic,"00:01:00,875","00:01:04,430",21,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=60,the word probabilities
cs-410_9_3_22,cs-410,9,3,Probabilistic,"00:01:04,430","00:01:08,090",22,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=64,"In this lecture, we're"
cs-410_9_3_23,cs-410,9,3,Probabilistic,"00:01:08,090","00:01:11,665",23,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=68,compute this maximum
cs-410_9_3_24,cs-410,9,3,Probabilistic,"00:01:11,665","00:01:15,275",24,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=71,"Now, let's start with the idea of"
cs-410_9_3_25,cs-410,9,3,Probabilistic,"00:01:15,275","00:01:19,340",25,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=75,separating the words in
cs-410_9_3_26,cs-410,9,3,Probabilistic,"00:01:19,340","00:01:23,065",26,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=79,One group would be explained
cs-410_9_3_27,cs-410,9,3,Probabilistic,"00:01:23,065","00:01:25,490",27,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=83,The other group would
cs-410_9_3_28,cs-410,9,3,Probabilistic,"00:01:25,490","00:01:28,520",28,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=85,the unknown topic
cs-410_9_3_29,cs-410,9,3,Probabilistic,"00:01:28,520","00:01:31,835",29,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=88,"After all, this is"
cs-410_9_3_30,cs-410,9,3,Probabilistic,"00:01:31,835","00:01:33,590",30,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=91,But suppose we actually
cs-410_9_3_31,cs-410,9,3,Probabilistic,"00:01:33,590","00:01:36,230",31,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=93,know which word is from
cs-410_9_3_32,cs-410,9,3,Probabilistic,"00:01:36,230","00:01:38,505",32,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=96,"So that would mean, for example,"
cs-410_9_3_33,cs-410,9,3,Probabilistic,"00:01:38,505","00:01:40,910",33,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=98,"these words the, is,"
cs-410_9_3_34,cs-410,9,3,Probabilistic,"00:01:40,910","00:01:42,740",34,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=100,and we are known to
cs-410_9_3_35,cs-410,9,3,Probabilistic,"00:01:42,740","00:01:45,275",35,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=102,be from this background
cs-410_9_3_36,cs-410,9,3,Probabilistic,"00:01:45,275","00:01:48,410",36,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=105,"On the other hand, the"
cs-410_9_3_37,cs-410,9,3,Probabilistic,"00:01:48,410","00:01:50,840",37,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=108,clustering etc are known to be
cs-410_9_3_38,cs-410,9,3,Probabilistic,"00:01:50,840","00:01:53,790",38,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=110,from the topic word distribution.
cs-410_9_3_39,cs-410,9,3,Probabilistic,"00:01:53,790","00:01:55,250",39,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=113,"If you can see the color,"
cs-410_9_3_40,cs-410,9,3,Probabilistic,"00:01:55,250","00:01:57,140",40,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=115,then these are shown in blue.
cs-410_9_3_41,cs-410,9,3,Probabilistic,"00:01:57,140","00:01:59,420",41,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=117,These blue words are then
cs-410_9_3_42,cs-410,9,3,Probabilistic,"00:01:59,420","00:02:02,585",42,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=119,assumed that to be from
cs-410_9_3_43,cs-410,9,3,Probabilistic,"00:02:02,585","00:02:07,010",43,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=122,If we already know how
cs-410_9_3_44,cs-410,9,3,Probabilistic,"00:02:07,010","00:02:08,750",44,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=127,then the problem of estimating
cs-410_9_3_45,cs-410,9,3,Probabilistic,"00:02:08,750","00:02:11,315",45,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=128,the word distribution
cs-410_9_3_46,cs-410,9,3,Probabilistic,"00:02:11,315","00:02:13,655",46,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=131,If you think about
cs-410_9_3_47,cs-410,9,3,Probabilistic,"00:02:13,655","00:02:16,205",47,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=133,"you'll realize that, well,"
cs-410_9_3_48,cs-410,9,3,Probabilistic,"00:02:16,205","00:02:20,180",48,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=136,we can simply take
cs-410_9_3_49,cs-410,9,3,Probabilistic,"00:02:20,180","00:02:21,680",49,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=140,to be from this word
cs-410_9_3_50,cs-410,9,3,Probabilistic,"00:02:21,680","00:02:24,185",50,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=141,distribution theta sub d
cs-410_9_3_51,cs-410,9,3,Probabilistic,"00:02:24,185","00:02:25,940",51,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=144,So indeed this problem would be
cs-410_9_3_52,cs-410,9,3,Probabilistic,"00:02:25,940","00:02:27,920",52,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=145,very easy to solve if we had
cs-410_9_3_53,cs-410,9,3,Probabilistic,"00:02:27,920","00:02:30,320",53,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=147,known which words are from
cs-410_9_3_54,cs-410,9,3,Probabilistic,"00:02:30,320","00:02:33,110",54,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=150,"which a distribution precisely,"
cs-410_9_3_55,cs-410,9,3,Probabilistic,"00:02:33,110","00:02:35,515",55,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=153,and this is in fact
cs-410_9_3_56,cs-410,9,3,Probabilistic,"00:02:35,515","00:02:38,525",56,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=155,making this model no
cs-410_9_3_57,cs-410,9,3,Probabilistic,"00:02:38,525","00:02:40,550",57,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=158,because we can already observe
cs-410_9_3_58,cs-410,9,3,Probabilistic,"00:02:40,550","00:02:42,560",58,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=160,which distribution has been
cs-410_9_3_59,cs-410,9,3,Probabilistic,"00:02:42,560","00:02:44,900",59,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=162,used to generate
cs-410_9_3_60,cs-410,9,3,Probabilistic,"00:02:44,900","00:02:46,730",60,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=164,So we actually go back to
cs-410_9_3_61,cs-410,9,3,Probabilistic,"00:02:46,730","00:02:50,755",61,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=166,the single word
cs-410_9_3_62,cs-410,9,3,Probabilistic,"00:02:50,755","00:02:52,290",62,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=170,In this case let's call
cs-410_9_3_63,cs-410,9,3,Probabilistic,"00:02:52,290","00:02:58,895",63,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=172,these words that are
cs-410_9_3_64,cs-410,9,3,Probabilistic,"00:02:58,895","00:03:01,115",64,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=178,"a pseudo document of d prime,"
cs-410_9_3_65,cs-410,9,3,Probabilistic,"00:03:01,115","00:03:04,880",65,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=181,and now all we need to
cs-410_9_3_66,cs-410,9,3,Probabilistic,"00:03:04,880","00:03:10,325",66,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=184,these words counts
cs-410_9_3_67,cs-410,9,3,Probabilistic,"00:03:10,325","00:03:13,420",67,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=190,That's fairly straightforward.
cs-410_9_3_68,cs-410,9,3,Probabilistic,"00:03:13,420","00:03:17,000",68,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=193,It's just dictated by the
cs-410_9_3_69,cs-410,9,3,Probabilistic,"00:03:17,000","00:03:21,470",69,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=197,"Now, this idea however"
cs-410_9_3_70,cs-410,9,3,Probabilistic,"00:03:21,470","00:03:23,630",70,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=201,we in practice don't really
cs-410_9_3_71,cs-410,9,3,Probabilistic,"00:03:23,630","00:03:26,224",71,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=203,know which word is from
cs-410_9_3_72,cs-410,9,3,Probabilistic,"00:03:26,224","00:03:29,690",72,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=206,but this gives us
cs-410_9_3_73,cs-410,9,3,Probabilistic,"00:03:29,690","00:03:33,775",73,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=209,can guess which word is
cs-410_9_3_74,cs-410,9,3,Probabilistic,"00:03:33,775","00:03:37,245",74,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=213,Specifically given
cs-410_9_3_75,cs-410,9,3,Probabilistic,"00:03:37,245","00:03:41,200",75,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=217,can we infer the distribution
cs-410_9_3_76,cs-410,9,3,Probabilistic,"00:03:41,200","00:03:44,000",76,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=221,So let's assume that we actually
cs-410_9_3_77,cs-410,9,3,Probabilistic,"00:03:44,000","00:03:47,390",77,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=224,know tentative probabilities for
cs-410_9_3_78,cs-410,9,3,Probabilistic,"00:03:47,390","00:03:49,940",78,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=227,these words in theta sub d.
cs-410_9_3_79,cs-410,9,3,Probabilistic,"00:03:49,940","00:03:51,980",79,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=229,So now all the parameters
cs-410_9_3_80,cs-410,9,3,Probabilistic,"00:03:51,980","00:03:54,305",80,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=231,"are known for this mixture model,"
cs-410_9_3_81,cs-410,9,3,Probabilistic,"00:03:54,305","00:03:58,660",81,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=234,and now let's consider
cs-410_9_3_82,cs-410,9,3,Probabilistic,"00:03:58,660","00:04:02,875",82,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=238,"So the question is, do you"
cs-410_9_3_83,cs-410,9,3,Probabilistic,"00:04:02,875","00:04:05,120",83,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=242,having been generated from
cs-410_9_3_84,cs-410,9,3,Probabilistic,"00:04:05,120","00:04:08,245",84,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=245,theta sub d or from
cs-410_9_3_85,cs-410,9,3,Probabilistic,"00:04:08,245","00:04:10,670",85,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=248,"So in other words,"
cs-410_9_3_86,cs-410,9,3,Probabilistic,"00:04:10,670","00:04:14,225",86,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=250,which distribution has been
cs-410_9_3_87,cs-410,9,3,Probabilistic,"00:04:14,225","00:04:16,940",87,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=254,"Now, this inference process is"
cs-410_9_3_88,cs-410,9,3,Probabilistic,"00:04:16,940","00:04:20,570",88,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=256,a typical Bayesian inference
cs-410_9_3_89,cs-410,9,3,Probabilistic,"00:04:20,570","00:04:24,650",89,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=260,some prior about
cs-410_9_3_90,cs-410,9,3,Probabilistic,"00:04:24,650","00:04:27,575",90,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=264,So can you see what
cs-410_9_3_91,cs-410,9,3,Probabilistic,"00:04:27,575","00:04:29,630",91,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=267,"Well, the prior here is"
cs-410_9_3_92,cs-410,9,3,Probabilistic,"00:04:29,630","00:04:33,145",92,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=269,the probability of
cs-410_9_3_93,cs-410,9,3,Probabilistic,"00:04:33,145","00:04:37,925",93,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=273,So the prior is given by
cs-410_9_3_94,cs-410,9,3,Probabilistic,"00:04:37,925","00:04:39,405",94,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=277,"In this case, the prior"
cs-410_9_3_95,cs-410,9,3,Probabilistic,"00:04:39,405","00:04:44,585",95,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=279,is saying that each model
cs-410_9_3_96,cs-410,9,3,Probabilistic,"00:04:44,585","00:04:47,900",96,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=284,but we can imagine perhaps a
cs-410_9_3_97,cs-410,9,3,Probabilistic,"00:04:47,900","00:04:52,100",97,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=287,So this is called a prior
cs-410_9_3_98,cs-410,9,3,Probabilistic,"00:04:52,100","00:04:54,170",98,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=292,which distribution has
cs-410_9_3_99,cs-410,9,3,Probabilistic,"00:04:54,170","00:04:57,740",99,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=294,a word before we even
cs-410_9_3_100,cs-410,9,3,Probabilistic,"00:04:57,740","00:05:00,895",100,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=297,So that's why we
cs-410_9_3_101,cs-410,9,3,Probabilistic,"00:05:00,895","00:05:03,600",101,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=300,"So if we don't observe the word,"
cs-410_9_3_102,cs-410,9,3,Probabilistic,"00:05:03,600","00:05:05,705",102,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=303,we don't know what word
cs-410_9_3_103,cs-410,9,3,Probabilistic,"00:05:05,705","00:05:09,905",103,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=305,Our best guess is to say
cs-410_9_3_104,cs-410,9,3,Probabilistic,"00:05:09,905","00:05:12,595",104,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=309,All right. So it's
cs-410_9_3_105,cs-410,9,3,Probabilistic,"00:05:12,595","00:05:16,015",105,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=312,Now in Bayesian inference we
cs-410_9_3_106,cs-410,9,3,Probabilistic,"00:05:16,015","00:05:18,820",106,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=316,our belief after we have
cs-410_9_3_107,cs-410,9,3,Probabilistic,"00:05:18,820","00:05:20,230",107,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=318,So what is the evidence here?
cs-410_9_3_108,cs-410,9,3,Probabilistic,"00:05:20,230","00:05:24,295",108,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=320,"Well, the evidence"
cs-410_9_3_109,cs-410,9,3,Probabilistic,"00:05:24,295","00:05:28,780",109,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=324,Now that we know we're
cs-410_9_3_110,cs-410,9,3,Probabilistic,"00:05:28,780","00:05:32,530",110,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=328,So text that can be
cs-410_9_3_111,cs-410,9,3,Probabilistic,"00:05:32,530","00:05:36,160",111,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=332,and if we use
cs-410_9_3_112,cs-410,9,3,Probabilistic,"00:05:36,160","00:05:41,424",112,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=336,Bayes rule to combine the
cs-410_9_3_113,cs-410,9,3,Probabilistic,"00:05:41,424","00:05:46,390",113,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=341,what we will end up
cs-410_9_3_114,cs-410,9,3,Probabilistic,"00:05:46,390","00:05:52,485",114,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=346,prior with the likelihood
cs-410_9_3_115,cs-410,9,3,Probabilistic,"00:05:52,485","00:05:54,650",115,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=352,which is basically
cs-410_9_3_116,cs-410,9,3,Probabilistic,"00:05:54,650","00:05:57,200",116,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=354,the word text from
cs-410_9_3_117,cs-410,9,3,Probabilistic,"00:05:57,200","00:06:00,830",117,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=357,We see that in both cases
cs-410_9_3_118,cs-410,9,3,Probabilistic,"00:06:00,830","00:06:03,880",118,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=360,Note that even in the background
cs-410_9_3_119,cs-410,9,3,Probabilistic,"00:06:03,880","00:06:06,900",119,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=363,it just has a very
cs-410_9_3_120,cs-410,9,3,Probabilistic,"00:06:06,970","00:06:12,805",120,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=366,So intuitively what would
cs-410_9_3_121,cs-410,9,3,Probabilistic,"00:06:12,805","00:06:15,195",121,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=372,"Now if you're like many others,"
cs-410_9_3_122,cs-410,9,3,Probabilistic,"00:06:15,195","00:06:18,020",122,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=375,you are guess text
cs-410_9_3_123,cs-410,9,3,Probabilistic,"00:06:18,020","00:06:22,610",123,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=378,theta sub d. It's more likely
cs-410_9_3_124,cs-410,9,3,Probabilistic,"00:06:22,610","00:06:27,440",124,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=382,You will probably see that
cs-410_9_3_125,cs-410,9,3,Probabilistic,"00:06:27,440","00:06:32,720",125,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=387,a much higher probability
cs-410_9_3_126,cs-410,9,3,Probabilistic,"00:06:32,720","00:06:36,110",126,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=392,then by the background model
cs-410_9_3_127,cs-410,9,3,Probabilistic,"00:06:36,110","00:06:38,780",127,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=396,which has a very
cs-410_9_3_128,cs-410,9,3,Probabilistic,"00:06:38,780","00:06:41,380",128,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=398,"By this we're going to say, well,"
cs-410_9_3_129,cs-410,9,3,Probabilistic,"00:06:41,380","00:06:43,670",129,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=401,text is more likely from
cs-410_9_3_130,cs-410,9,3,Probabilistic,"00:06:43,670","00:06:45,830",130,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=403,theta sub d. So you see
cs-410_9_3_131,cs-410,9,3,Probabilistic,"00:06:45,830","00:06:48,320",131,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=405,our guess of which
cs-410_9_3_132,cs-410,9,3,Probabilistic,"00:06:48,320","00:06:51,335",132,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=408,used to generate
cs-410_9_3_133,cs-410,9,3,Probabilistic,"00:06:51,335","00:06:54,170",133,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=411,how high the probability of
cs-410_9_3_134,cs-410,9,3,Probabilistic,"00:06:54,170","00:06:58,655",134,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=414,the text is in
cs-410_9_3_135,cs-410,9,3,Probabilistic,"00:06:58,655","00:07:01,190",135,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=418,"We can do, tend to guess"
cs-410_9_3_136,cs-410,9,3,Probabilistic,"00:07:01,190","00:07:02,540",136,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=421,the distribution that gives us
cs-410_9_3_137,cs-410,9,3,Probabilistic,"00:07:02,540","00:07:04,355",137,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=422,"a word a higher probability,"
cs-410_9_3_138,cs-410,9,3,Probabilistic,"00:07:04,355","00:07:08,565",138,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=424,and this is likely to
cs-410_9_3_139,cs-410,9,3,Probabilistic,"00:07:08,565","00:07:11,300",139,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=428,So we're going to choose
cs-410_9_3_140,cs-410,9,3,Probabilistic,"00:07:11,300","00:07:15,665",140,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=431,a word that has
cs-410_9_3_141,cs-410,9,3,Probabilistic,"00:07:15,665","00:07:17,825",141,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=435,"So in other words,"
cs-410_9_3_142,cs-410,9,3,Probabilistic,"00:07:17,825","00:07:21,765",142,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=437,these two probabilities of
cs-410_9_3_143,cs-410,9,3,Probabilistic,"00:07:21,765","00:07:24,340",143,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=441,the word given by
cs-410_9_3_144,cs-410,9,3,Probabilistic,"00:07:24,340","00:07:30,440",144,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=444,But our guess must also
cs-410_9_3_145,cs-410,9,3,Probabilistic,"00:07:30,440","00:07:33,760",145,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=450,So we also need to
cs-410_9_3_146,cs-410,9,3,Probabilistic,"00:07:33,760","00:07:38,660",146,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=453,Why? Because imagine if we
cs-410_9_3_147,cs-410,9,3,Probabilistic,"00:07:38,660","00:07:41,210",147,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=458,we're going to say
cs-410_9_3_148,cs-410,9,3,Probabilistic,"00:07:41,210","00:07:44,210",148,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=461,a background model is
cs-410_9_3_149,cs-410,9,3,Probabilistic,"00:07:44,210","00:07:47,450",149,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=464,"Now, if you have that kind"
cs-410_9_3_150,cs-410,9,3,Probabilistic,"00:07:47,450","00:07:49,370",150,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=467,then that would
cs-410_9_3_151,cs-410,9,3,Probabilistic,"00:07:49,370","00:07:51,665",151,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=469,"You might think,"
cs-410_9_3_152,cs-410,9,3,Probabilistic,"00:07:51,665","00:07:54,740",152,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=471,maybe text could have been
cs-410_9_3_153,cs-410,9,3,Probabilistic,"00:07:54,740","00:07:57,890",153,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=474,Although the probability
cs-410_9_3_154,cs-410,9,3,Probabilistic,"00:07:57,890","00:08:00,520",154,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=477,the prior is very high.
cs-410_9_3_155,cs-410,9,3,Probabilistic,"00:08:00,520","00:08:03,450",155,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=480,"So in the end, we have"
cs-410_9_3_156,cs-410,9,3,Probabilistic,"00:08:03,450","00:08:05,880",156,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=483,and the base formula provides us
cs-410_9_3_157,cs-410,9,3,Probabilistic,"00:08:05,880","00:08:09,135",157,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=485,a solid and principled way
cs-410_9_3_158,cs-410,9,3,Probabilistic,"00:08:09,135","00:08:12,890",158,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=489,of making this kind of
cs-410_9_3_159,cs-410,9,3,Probabilistic,"00:08:12,890","00:08:15,305",159,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=492,"So more specifically,"
cs-410_9_3_160,cs-410,9,3,Probabilistic,"00:08:15,305","00:08:17,120",160,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=495,let's think about
cs-410_9_3_161,cs-410,9,3,Probabilistic,"00:08:17,120","00:08:19,220",161,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=497,this word has been generated in
cs-410_9_3_162,cs-410,9,3,Probabilistic,"00:08:19,220","00:08:22,535",162,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=499,"fact from from theta sub d. Well,"
cs-410_9_3_163,cs-410,9,3,Probabilistic,"00:08:22,535","00:08:24,920",163,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=502,in order for texts
cs-410_9_3_164,cs-410,9,3,Probabilistic,"00:08:24,920","00:08:27,095",164,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=504,theta sub d two things
cs-410_9_3_165,cs-410,9,3,Probabilistic,"00:08:27,095","00:08:31,275",165,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=507,"First, the theta sub d"
cs-410_9_3_166,cs-410,9,3,Probabilistic,"00:08:31,275","00:08:34,250",166,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=511,so we have the selection
cs-410_9_3_167,cs-410,9,3,Probabilistic,"00:08:34,250","00:08:37,145",167,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=514,"Secondly, we also have to"
cs-410_9_3_168,cs-410,9,3,Probabilistic,"00:08:37,145","00:08:40,640",168,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=517,actually have observed text
cs-410_9_3_169,cs-410,9,3,Probabilistic,"00:08:40,640","00:08:43,160",169,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=520,So when we multiply
cs-410_9_3_170,cs-410,9,3,Probabilistic,"00:08:43,160","00:08:46,940",170,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=523,we get the probability
cs-410_9_3_171,cs-410,9,3,Probabilistic,"00:08:46,940","00:08:51,230",171,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=526,fact been generated from
cs-410_9_3_172,cs-410,9,3,Probabilistic,"00:08:51,230","00:08:54,005",172,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=531,"for the background model,"
cs-410_9_3_173,cs-410,9,3,Probabilistic,"00:08:54,005","00:08:56,630",173,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=534,the probability of generating
cs-410_9_3_174,cs-410,9,3,Probabilistic,"00:08:56,630","00:09:00,075",174,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=536,text is another product
cs-410_9_3_175,cs-410,9,3,Probabilistic,"00:09:00,075","00:09:01,700",175,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=540,"Now, we also introduced"
cs-410_9_3_176,cs-410,9,3,Probabilistic,"00:09:01,700","00:09:05,665",176,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=541,the latent variable
cs-410_9_3_177,cs-410,9,3,Probabilistic,"00:09:05,665","00:09:11,625",177,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=545,whether the word is from
cs-410_9_3_178,cs-410,9,3,Probabilistic,"00:09:11,625","00:09:13,470",178,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=551,"When z is zero,"
cs-410_9_3_179,cs-410,9,3,Probabilistic,"00:09:13,470","00:09:18,350",179,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=553,it means it's from the topic
cs-410_9_3_180,cs-410,9,3,Probabilistic,"00:09:18,350","00:09:21,515",180,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=558,it means it's from
cs-410_9_3_181,cs-410,9,3,Probabilistic,"00:09:21,515","00:09:23,030",181,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=561,So now we have
cs-410_9_3_182,cs-410,9,3,Probabilistic,"00:09:23,030","00:09:26,660",182,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=563,the probability that text
cs-410_9_3_183,cs-410,9,3,Probabilistic,"00:09:26,660","00:09:29,600",183,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=566,Then we can simply normalize
cs-410_9_3_184,cs-410,9,3,Probabilistic,"00:09:29,600","00:09:33,605",184,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=569,them to have an estimate
cs-410_9_3_185,cs-410,9,3,Probabilistic,"00:09:33,605","00:09:36,185",185,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=573,that the word text is
cs-410_9_3_186,cs-410,9,3,Probabilistic,"00:09:36,185","00:09:42,135",186,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=576,from theta sub d or
cs-410_9_3_187,cs-410,9,3,Probabilistic,"00:09:42,135","00:09:46,100",187,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=582,"Then equivalently, the"
cs-410_9_3_188,cs-410,9,3,Probabilistic,"00:09:46,100","00:09:50,905",188,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=586,zero given that
cs-410_9_3_189,cs-410,9,3,Probabilistic,"00:09:50,905","00:09:55,490",189,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=590,So this is application
cs-410_9_3_190,cs-410,9,3,Probabilistic,"00:09:55,490","00:09:59,419",190,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=595,But this step is very
cs-410_9_3_191,cs-410,9,3,Probabilistic,"00:09:59,419","00:10:04,060",191,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=599,the EM algorithm because
cs-410_9_3_192,cs-410,9,3,Probabilistic,"00:10:04,060","00:10:07,250",192,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=604,then we would be able to first
cs-410_9_3_193,cs-410,9,3,Probabilistic,"00:10:07,250","00:10:11,830",193,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=607,initialize the parameter values
cs-410_9_3_194,cs-410,9,3,Probabilistic,"00:10:11,830","00:10:17,155",194,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=611,and then we're going to take
cs-410_9_3_195,cs-410,9,3,Probabilistic,"00:10:17,155","00:10:20,825",195,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=617,Which distributing has been
cs-410_9_3_196,cs-410,9,3,Probabilistic,"00:10:20,825","00:10:23,960",196,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=620,and the initialized
cs-410_9_3_197,cs-410,9,3,Probabilistic,"00:10:23,960","00:10:25,310",197,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=623,allow us to have a complete
cs-410_9_3_198,cs-410,9,3,Probabilistic,"00:10:25,310","00:10:27,035",198,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=625,specification of
cs-410_9_3_199,cs-410,9,3,Probabilistic,"00:10:27,035","00:10:31,145",199,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=627,which further allows us to
cs-410_9_3_200,cs-410,9,3,Probabilistic,"00:10:31,145","00:10:36,815",200,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=631,which distribution is more
cs-410_9_3_201,cs-410,9,3,Probabilistic,"00:10:36,815","00:10:40,700",201,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=636,This prediction
cs-410_9_3_202,cs-410,9,3,Probabilistic,"00:10:40,700","00:10:44,690",202,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=640,to separate the words from
cs-410_9_3_203,cs-410,9,3,Probabilistic,"00:10:44,690","00:10:47,115",203,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=644,Although we can't
cs-410_9_3_204,cs-410,9,3,Probabilistic,"00:10:47,115","00:10:53,250",204,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=647,but we can separate them
cs-410_9_4_1,cs-410,9,4,Probabilistic,"00:00:00,012","00:00:08,224",1,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=0,[SOUND]
cs-410_9_4_2,cs-410,9,4,Probabilistic,"00:00:08,224","00:00:12,538",2,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=8,this is indeed a general idea of
cs-410_9_4_3,cs-410,9,4,Probabilistic,"00:00:12,538","00:00:13,310",3,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=12,Algorithm.
cs-410_9_4_4,cs-410,9,4,Probabilistic,"00:00:14,640","00:00:19,210",4,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=14,So in all the EM algorithms we
cs-410_9_4_5,cs-410,9,4,Probabilistic,"00:00:19,210","00:00:21,970",5,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=19,to help us solve the problem more easily.
cs-410_9_4_6,cs-410,9,4,Probabilistic,"00:00:21,970","00:00:25,453",6,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=21,In our case the hidden variable
cs-410_9_4_7,cs-410,9,4,Probabilistic,"00:00:25,453","00:00:27,203",7,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=25,each occurrence of a word.
cs-410_9_4_8,cs-410,9,4,Probabilistic,"00:00:27,203","00:00:32,020",8,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=27,And this binary variable would
cs-410_9_4_9,cs-410,9,4,Probabilistic,"00:00:32,020","00:00:35,144",9,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=32,been generated from 0 sub d or 0 sub p.
cs-410_9_4_10,cs-410,9,4,Probabilistic,"00:00:35,144","00:00:38,420",10,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=35,And here we show some possible
cs-410_9_4_11,cs-410,9,4,Probabilistic,"00:00:38,420","00:00:43,470",11,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=38,"For example, for the it's from background,"
cs-410_9_4_12,cs-410,9,4,Probabilistic,"00:00:43,470","00:00:45,105",12,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=43,And text on the other hand.
cs-410_9_4_13,cs-410,9,4,Probabilistic,"00:00:45,105","00:00:52,040",13,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=45,Is from the topic then it's zero for
cs-410_9_4_14,cs-410,9,4,Probabilistic,"00:00:53,260","00:00:58,915",14,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=53,"Now, of course, we don't observe these z"
cs-410_9_4_15,cs-410,9,4,Probabilistic,"00:00:58,915","00:01:01,875",15,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=58,Values of z attaching to other words.
cs-410_9_4_16,cs-410,9,4,Probabilistic,"00:01:02,905","00:01:04,975",16,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=62,And that's why we call
cs-410_9_4_17,cs-410,9,4,Probabilistic,"00:01:06,135","00:01:08,905",17,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=66,"Now, the idea that we"
cs-410_9_4_18,cs-410,9,4,Probabilistic,"00:01:08,905","00:01:12,930",18,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=68,predicting the word distribution that
cs-410_9_4_19,cs-410,9,4,Probabilistic,"00:01:12,930","00:01:18,840",19,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=72,"is it a predictor,"
cs-410_9_4_20,cs-410,9,4,Probabilistic,"00:01:18,840","00:01:25,080",20,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=78,"And, so, the EM algorithm then,"
cs-410_9_4_21,cs-410,9,4,Probabilistic,"00:01:25,080","00:01:30,060",21,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=85,"First, we'll initialize all"
cs-410_9_4_22,cs-410,9,4,Probabilistic,"00:01:30,060","00:01:34,960",22,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=90,"In our case,"
cs-410_9_4_23,cs-410,9,4,Probabilistic,"00:01:34,960","00:01:37,840",23,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=94,"of a word, given by theta sub d."
cs-410_9_4_24,cs-410,9,4,Probabilistic,"00:01:37,840","00:01:39,680",24,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=97,So this is an initial addition stage.
cs-410_9_4_25,cs-410,9,4,Probabilistic,"00:01:39,680","00:01:44,150",25,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=99,These initialized values would allow
cs-410_9_4_26,cs-410,9,4,Probabilistic,"00:01:44,150","00:01:48,510",26,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=104,"of these z values, so"
cs-410_9_4_27,cs-410,9,4,Probabilistic,"00:01:48,510","00:01:53,580",27,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=108,We can't say for sure whether
cs-410_9_4_28,cs-410,9,4,Probabilistic,"00:01:53,580","00:01:55,090",28,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=113,But we can have our guess.
cs-410_9_4_29,cs-410,9,4,Probabilistic,"00:01:55,090","00:01:57,620",29,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=115,This is given by this formula.
cs-410_9_4_30,cs-410,9,4,Probabilistic,"00:01:57,620","00:01:59,710",30,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=117,It's called an E-step.
cs-410_9_4_31,cs-410,9,4,Probabilistic,"00:01:59,710","00:02:06,520",31,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=119,And so the algorithm would then try to
cs-410_9_4_32,cs-410,9,4,Probabilistic,"00:02:06,520","00:02:12,190",32,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=126,"After that, it would then invoke"
cs-410_9_4_33,cs-410,9,4,Probabilistic,"00:02:12,190","00:02:17,490",33,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=132,In this step we simply take advantage
cs-410_9_4_34,cs-410,9,4,Probabilistic,"00:02:17,490","00:02:22,825",34,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=137,then just group words that are in
cs-410_9_4_35,cs-410,9,4,Probabilistic,"00:02:22,825","00:02:26,315",35,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=142,from that ground including this as well.
cs-410_9_4_36,cs-410,9,4,Probabilistic,"00:02:27,585","00:02:32,865",36,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=147,We can then normalize the count
cs-410_9_4_37,cs-410,9,4,Probabilistic,"00:02:32,865","00:02:35,479",37,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=152,to revise our estimate of the parameters.
cs-410_9_4_38,cs-410,9,4,Probabilistic,"00:02:36,590","00:02:42,310",38,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=156,So let me also illustrate
cs-410_9_4_39,cs-410,9,4,Probabilistic,"00:02:42,310","00:02:46,760",39,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=162,that are believed to have
cs-410_9_4_40,cs-410,9,4,Probabilistic,"00:02:46,760","00:02:50,010",40,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=166,"that's text, mining algorithm,"
cs-410_9_4_41,cs-410,9,4,Probabilistic,"00:02:51,760","00:02:55,718",41,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=171,And we group them together to help us
cs-410_9_4_42,cs-410,9,4,Probabilistic,"00:02:55,718","00:03:01,170",42,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=175,re-estimate the parameters
cs-410_9_4_43,cs-410,9,4,Probabilistic,"00:03:01,170","00:03:05,120",43,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=181,So these will help us
cs-410_9_4_44,cs-410,9,4,Probabilistic,"00:03:06,170","00:03:09,970",44,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=186,Note that before we just set
cs-410_9_4_45,cs-410,9,4,Probabilistic,"00:03:09,970","00:03:15,670",45,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=189,"But with this guess, we will have"
cs-410_9_4_46,cs-410,9,4,Probabilistic,"00:03:15,670","00:03:18,740",46,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=195,"Of course, we don't know exactly"
cs-410_9_4_47,cs-410,9,4,Probabilistic,"00:03:18,740","00:03:24,850",47,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=198,So we're not going to really
cs-410_9_4_48,cs-410,9,4,Probabilistic,"00:03:24,850","00:03:26,800",48,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=204,But rather we're going to
cs-410_9_4_49,cs-410,9,4,Probabilistic,"00:03:26,800","00:03:27,980",49,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=206,And this is what happened here.
cs-410_9_4_50,cs-410,9,4,Probabilistic,"00:03:29,150","00:03:34,420",50,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=209,So we're going to adjust the count by
cs-410_9_4_51,cs-410,9,4,Probabilistic,"00:03:34,420","00:03:38,410",51,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=214,this word has been generated
cs-410_9_4_52,cs-410,9,4,Probabilistic,"00:03:39,840","00:03:42,580",52,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=219,"And you can see this,"
cs-410_9_4_53,cs-410,9,4,Probabilistic,"00:03:42,580","00:03:46,630",53,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=222,"Well, this has come from here, right?"
cs-410_9_4_54,cs-410,9,4,Probabilistic,"00:03:46,630","00:03:48,120",54,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=226,From the E-step.
cs-410_9_4_55,cs-410,9,4,Probabilistic,"00:03:48,120","00:03:52,472",55,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=228,So the EM Algorithm would
cs-410_9_4_56,cs-410,9,4,Probabilistic,"00:03:52,472","00:03:57,375",56,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=232,estimate of parameters by using
cs-410_9_4_57,cs-410,9,4,Probabilistic,"00:03:57,375","00:04:02,458",57,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=237,The E-step is to augment the data
cs-410_9_4_58,cs-410,9,4,Probabilistic,"00:04:02,458","00:04:05,910",58,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=242,And the M-step is to take advantage
cs-410_9_4_59,cs-410,9,4,Probabilistic,"00:04:05,910","00:04:08,660",59,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=245,of the additional information
cs-410_9_4_60,cs-410,9,4,Probabilistic,"00:04:08,660","00:04:13,467",60,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=248,To split the data accounts and
cs-410_9_4_61,cs-410,9,4,Probabilistic,"00:04:13,467","00:04:17,870",61,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=253,re-estimate our parameter.
cs-410_9_4_62,cs-410,9,4,Probabilistic,"00:04:17,870","00:04:22,400",62,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=257,And then once we have a new generation of
cs-410_9_4_63,cs-410,9,4,Probabilistic,"00:04:22,400","00:04:25,150",63,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=262,We are going the E-step again.
cs-410_9_4_64,cs-410,9,4,Probabilistic,"00:04:25,150","00:04:28,520",64,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=265,To improve our estimate
cs-410_9_4_65,cs-410,9,4,Probabilistic,"00:04:28,520","00:04:33,630",65,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=268,And then that would lead to another
cs-410_9_4_66,cs-410,9,4,Probabilistic,"00:04:34,770","00:04:37,910",66,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=274,For the word distribution
cs-410_9_4_67,cs-410,9,4,Probabilistic,"00:04:39,610","00:04:44,670",67,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=279,"Okay, so, as I said,"
cs-410_9_4_68,cs-410,9,4,Probabilistic,"00:04:44,670","00:04:50,380",68,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=284,"is really the variable z, hidden variable,"
cs-410_9_4_69,cs-410,9,4,Probabilistic,"00:04:50,380","00:04:55,200",69,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=290,this water is from the top water
cs-410_9_4_70,cs-410,9,4,Probabilistic,"00:04:56,810","00:05:00,780",70,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=296,"So, this slide has a lot of content and"
cs-410_9_4_71,cs-410,9,4,Probabilistic,"00:05:00,780","00:05:03,850",71,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=300,Pause the reader to digest it.
cs-410_9_4_72,cs-410,9,4,Probabilistic,"00:05:03,850","00:05:07,300",72,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=303,But this basically captures
cs-410_9_4_73,cs-410,9,4,Probabilistic,"00:05:07,300","00:05:12,500",73,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=307,Start with initial values that
cs-410_9_4_74,cs-410,9,4,Probabilistic,"00:05:12,500","00:05:18,150",74,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=312,And then we invoke E-step followed
cs-410_9_4_75,cs-410,9,4,Probabilistic,"00:05:18,150","00:05:19,690",75,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=318,setting of parameters.
cs-410_9_4_76,cs-410,9,4,Probabilistic,"00:05:19,690","00:05:23,340",76,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=319,"And then we repeated this, so"
cs-410_9_4_77,cs-410,9,4,Probabilistic,"00:05:23,340","00:05:27,060",77,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=323,that would gradually improve
cs-410_9_4_78,cs-410,9,4,Probabilistic,"00:05:27,060","00:05:30,050",78,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=327,As I will explain later
cs-410_9_4_79,cs-410,9,4,Probabilistic,"00:05:30,050","00:05:35,340",79,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=330,reaching a local maximum of
cs-410_9_4_80,cs-410,9,4,Probabilistic,"00:05:35,340","00:05:40,180",80,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=335,So lets take a look at the computation for
cs-410_9_4_81,cs-410,9,4,Probabilistic,"00:05:40,180","00:05:41,840",81,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=340,these formulas are the EM.
cs-410_9_4_82,cs-410,9,4,Probabilistic,"00:05:41,840","00:05:48,220",82,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=341,"Formulas that you see before, and"
cs-410_9_4_83,cs-410,9,4,Probabilistic,"00:05:48,220","00:05:53,720",83,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=348,"here, like here, n,"
cs-410_9_4_84,cs-410,9,4,Probabilistic,"00:05:53,720","00:05:56,040",84,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=353,Like here for example we have n plus one.
cs-410_9_4_85,cs-410,9,4,Probabilistic,"00:05:56,040","00:05:59,728",85,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=356,That means we have improved.
cs-410_9_4_86,cs-410,9,4,Probabilistic,"00:05:59,728","00:06:04,047",86,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=359,From here to here we have an improvement.
cs-410_9_4_87,cs-410,9,4,Probabilistic,"00:06:04,047","00:06:08,106",87,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=364,So in this setting we have assumed the two
cs-410_9_4_88,cs-410,9,4,Probabilistic,"00:06:08,106","00:06:09,689",88,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=368,the background model is null.
cs-410_9_4_89,cs-410,9,4,Probabilistic,"00:06:09,689","00:06:11,872",89,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=369,So what are the relevance
cs-410_9_4_90,cs-410,9,4,Probabilistic,"00:06:11,872","00:06:13,892",90,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=371,Well these are the word counts.
cs-410_9_4_91,cs-410,9,4,Probabilistic,"00:06:13,892","00:06:18,290",91,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=373,"So assume we have just four words,"
cs-410_9_4_92,cs-410,9,4,Probabilistic,"00:06:18,290","00:06:22,680",92,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=378,And this is our background model that
cs-410_9_4_93,cs-410,9,4,Probabilistic,"00:06:22,680","00:06:23,380",93,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=382,words like the.
cs-410_9_4_94,cs-410,9,4,Probabilistic,"00:06:25,910","00:06:29,860",94,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=385,"And in the first iteration,"
cs-410_9_4_95,cs-410,9,4,Probabilistic,"00:06:29,860","00:06:32,280",95,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=389,Well first we initialize all the values.
cs-410_9_4_96,cs-410,9,4,Probabilistic,"00:06:32,280","00:06:37,360",96,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=392,"So here, this probability that we're"
cs-410_9_4_97,cs-410,9,4,Probabilistic,"00:06:37,360","00:06:38,890",97,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=397,distribution of all the words.
cs-410_9_4_98,cs-410,9,4,Probabilistic,"00:06:40,330","00:06:45,940",98,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=400,And then the E-step would give us a guess
cs-410_9_4_99,cs-410,9,4,Probabilistic,"00:06:45,940","00:06:48,470",99,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=405,That will generate each word.
cs-410_9_4_100,cs-410,9,4,Probabilistic,"00:06:48,470","00:06:51,450",100,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=408,We can see we have different
cs-410_9_4_101,cs-410,9,4,Probabilistic,"00:06:51,450","00:06:52,430",101,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=411,Why?
cs-410_9_4_102,cs-410,9,4,Probabilistic,"00:06:52,430","00:06:56,840",102,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=412,"Well, that's because these words have"
cs-410_9_4_103,cs-410,9,4,Probabilistic,"00:06:56,840","00:07:00,020",103,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=416,So even though the two
cs-410_9_4_104,cs-410,9,4,Probabilistic,"00:07:00,020","00:07:05,320",104,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=420,And then our initial audition say uniform
cs-410_9_4_105,cs-410,9,4,Probabilistic,"00:07:05,320","00:07:09,270",105,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=425,"in the background of the distribution,"
cs-410_9_4_106,cs-410,9,4,Probabilistic,"00:07:09,270","00:07:14,280",106,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=429,So these words are believed to
cs-410_9_4_107,cs-410,9,4,Probabilistic,"00:07:15,820","00:07:17,930",107,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=435,These on the other hand are less likely.
cs-410_9_4_108,cs-410,9,4,Probabilistic,"00:07:17,930","00:07:19,030",108,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=437,Probably from background.
cs-410_9_4_109,cs-410,9,4,Probabilistic,"00:07:20,620","00:07:23,040",109,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=440,"So once we have these z values,"
cs-410_9_4_110,cs-410,9,4,Probabilistic,"00:07:23,040","00:07:28,810",110,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=443,we know in the M-step these probabilities
cs-410_9_4_111,cs-410,9,4,Probabilistic,"00:07:28,810","00:07:33,670",111,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=448,So four must be multiplied by this 0.33
cs-410_9_4_112,cs-410,9,4,Probabilistic,"00:07:33,670","00:07:38,190",112,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=453,in order to get the allocated
cs-410_9_4_113,cs-410,9,4,Probabilistic,"00:07:39,550","00:07:43,770",113,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=459,And this is done by this multiplication.
cs-410_9_4_114,cs-410,9,4,Probabilistic,"00:07:43,770","00:07:49,700",114,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=463,Note that if our guess says this
cs-410_9_4_115,cs-410,9,4,Probabilistic,"00:07:52,380","00:07:58,010",115,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=472,then we just get the full count
cs-410_9_4_116,cs-410,9,4,Probabilistic,"00:07:58,010","00:08:01,200",116,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=478,In general it's not going
cs-410_9_4_117,cs-410,9,4,Probabilistic,"00:08:01,200","00:08:06,760",117,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=481,So we're just going to get some percentage
cs-410_9_4_118,cs-410,9,4,Probabilistic,"00:08:06,760","00:08:09,550",118,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=486,Then we simply normalize these counts
cs-410_9_4_119,cs-410,9,4,Probabilistic,"00:08:09,550","00:08:13,170",119,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=489,to have a new generation
cs-410_9_4_120,cs-410,9,4,Probabilistic,"00:08:13,170","00:08:16,600",120,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=493,"So you can see, compare this with"
cs-410_9_4_121,cs-410,9,4,Probabilistic,"00:08:18,330","00:08:23,060",121,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=498,So compare this with this one and
cs-410_9_4_122,cs-410,9,4,Probabilistic,"00:08:23,060","00:08:25,930",122,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=503,"Not only that, we also see some"
cs-410_9_4_123,cs-410,9,4,Probabilistic,"00:08:25,930","00:08:30,110",123,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=505,words that are believed to have come from
cs-410_9_4_124,cs-410,9,4,Probabilistic,"00:08:30,110","00:08:31,400",124,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=510,"Like this one, text."
cs-410_9_4_125,cs-410,9,4,Probabilistic,"00:08:32,530","00:08:35,930",125,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=512,"And of course, this new generation of"
cs-410_9_4_126,cs-410,9,4,Probabilistic,"00:08:35,930","00:08:42,680",126,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=515,adjust the inferred latent variable or
cs-410_9_4_127,cs-410,9,4,Probabilistic,"00:08:42,680","00:08:45,742",127,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=522,"So we have a new generation of values,"
cs-410_9_4_128,cs-410,9,4,Probabilistic,"00:08:45,742","00:08:51,115",128,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=525,because of the E-step based on
cs-410_9_4_129,cs-410,9,4,Probabilistic,"00:08:51,115","00:08:56,343",129,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=531,And these new inferred values
cs-410_9_4_130,cs-410,9,4,Probabilistic,"00:08:56,343","00:09:03,166",130,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=536,another generation of the estimate
cs-410_9_4_131,cs-410,9,4,Probabilistic,"00:09:03,166","00:09:07,990",131,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=543,And so on and so forth so this is what
cs-410_9_4_132,cs-410,9,4,Probabilistic,"00:09:07,990","00:09:11,750",132,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=547,these probabilities
cs-410_9_4_133,cs-410,9,4,Probabilistic,"00:09:11,750","00:09:16,745",133,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=551,As you can see in the last row
cs-410_9_4_134,cs-410,9,4,Probabilistic,"00:09:16,745","00:09:20,985",134,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=556,and the likelihood is increasing
cs-410_9_4_135,cs-410,9,4,Probabilistic,"00:09:20,985","00:09:25,875",135,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=560,And note that these log-likelihood is
cs-410_9_4_136,cs-410,9,4,Probabilistic,"00:09:25,875","00:09:30,070",136,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=565,"between 0 and 1 when you take a logarithm,"
cs-410_9_4_137,cs-410,9,4,Probabilistic,"00:09:30,070","00:09:33,180",137,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=570,"Now what's also interesting is,"
cs-410_9_4_138,cs-410,9,4,Probabilistic,"00:09:33,180","00:09:36,600",138,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=573,And these are the inverted word split.
cs-410_9_4_139,cs-410,9,4,Probabilistic,"00:09:36,600","00:09:42,150",139,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=576,And these are the probabilities
cs-410_9_4_140,cs-410,9,4,Probabilistic,"00:09:42,150","00:09:47,980",140,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=582,"have come from one distribution, in this"
cs-410_9_4_141,cs-410,9,4,Probabilistic,"00:09:47,980","00:09:50,580",141,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=587,And you might wonder whether
cs-410_9_4_142,cs-410,9,4,Probabilistic,"00:09:50,580","00:09:55,540",142,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=590,Because our main goal is to
cs-410_9_4_143,cs-410,9,4,Probabilistic,"00:09:55,540","00:09:57,400",143,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=595,So this is our primary goal.
cs-410_9_4_144,cs-410,9,4,Probabilistic,"00:09:57,400","00:10:00,900",144,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=597,We hope to have a more discriminative
cs-410_9_4_145,cs-410,9,4,Probabilistic,"00:10:00,900","00:10:04,400",145,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=600,But the last column is also bi-product.
cs-410_9_4_146,cs-410,9,4,Probabilistic,"00:10:04,400","00:10:07,170",146,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=604,This also can actually be very useful.
cs-410_9_4_147,cs-410,9,4,Probabilistic,"00:10:07,170","00:10:08,380",147,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=607,You can think about that.
cs-410_9_4_148,cs-410,9,4,Probabilistic,"00:10:08,380","00:10:10,220",148,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=608,"We want to use, is to for"
cs-410_9_4_149,cs-410,9,4,Probabilistic,"00:10:10,220","00:10:16,080",149,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=610,example is to estimate to what extent this
cs-410_9_4_150,cs-410,9,4,Probabilistic,"00:10:16,080","00:10:18,165",150,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=616,"And this, when we add this up or"
cs-410_9_4_151,cs-410,9,4,Probabilistic,"00:10:18,165","00:10:23,304",151,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=618,take the average we will kind of know to
cs-410_9_4_152,cs-410,9,4,Probabilistic,"00:10:23,304","00:10:27,823",152,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=623,versus content was that are not
cs-410_9_4_153,cs-410,9,4,Probabilistic,"00:10:27,823","00:10:37,823",153,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=627,[MUSIC]
cs-410_9_5_1,cs-410,9,5,Probabilistic,"00:00:07,553","00:00:12,636",1,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=7,"So, I just showed you that empirically"
cs-410_9_5_2,cs-410,9,5,Probabilistic,"00:00:12,636","00:00:17,041",2,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=12,but theoretically it can also
cs-410_9_5_3,cs-410,9,5,Probabilistic,"00:00:17,041","00:00:19,295",3,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=17,converge to a local maximum.
cs-410_9_5_4,cs-410,9,5,Probabilistic,"00:00:19,295","00:00:24,925",4,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=19,So here's just an illustration of what
cs-410_9_5_5,cs-410,9,5,Probabilistic,"00:00:24,925","00:00:29,613",5,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=24,"This required more knowledge about that,"
cs-410_9_5_6,cs-410,9,5,Probabilistic,"00:00:29,613","00:00:36,910",6,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=29,"some of that inequalities,"
cs-410_9_5_7,cs-410,9,5,Probabilistic,"00:00:39,380","00:00:45,040",7,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=39,So here what you see is on the X
cs-410_9_5_8,cs-410,9,5,Probabilistic,"00:00:45,040","00:00:46,799",8,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=45,This is a parameter that we have.
cs-410_9_5_9,cs-410,9,5,Probabilistic,"00:00:46,799","00:00:49,714",9,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=46,On the y axis we see
cs-410_9_5_10,cs-410,9,5,Probabilistic,"00:00:49,714","00:00:57,171",10,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=49,So this curve is the original
cs-410_9_5_11,cs-410,9,5,Probabilistic,"00:00:57,171","00:01:04,110",11,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=57,and this is the one that
cs-410_9_5_12,cs-410,9,5,Probabilistic,"00:01:04,110","00:01:06,630",12,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=64,And we hope to find a c0 value
cs-410_9_5_13,cs-410,9,5,Probabilistic,"00:01:06,630","00:01:11,480",13,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=66,But in the case of Mitsumoto we can
cs-410_9_5_14,cs-410,9,5,Probabilistic,"00:01:11,480","00:01:12,470",14,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=71,to the problem.
cs-410_9_5_15,cs-410,9,5,Probabilistic,"00:01:12,470","00:01:14,698",15,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=72,"So, we have to resolve"
cs-410_9_5_16,cs-410,9,5,Probabilistic,"00:01:14,698","00:01:16,457",16,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=74,the EM algorithm is such an algorithm.
cs-410_9_5_17,cs-410,9,5,Probabilistic,"00:01:16,457","00:01:17,850",17,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=76,It's a Hill-Climb algorithm.
cs-410_9_5_18,cs-410,9,5,Probabilistic,"00:01:17,850","00:01:22,490",18,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=77,That would mean you start
cs-410_9_5_19,cs-410,9,5,Probabilistic,"00:01:22,490","00:01:26,260",19,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=82,"Let's say you start from here,"
cs-410_9_5_20,cs-410,9,5,Probabilistic,"00:01:26,260","00:01:32,090",20,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=86,And then you try to improve
cs-410_9_5_21,cs-410,9,5,Probabilistic,"00:01:32,090","00:01:35,420",21,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=92,another point where you can
cs-410_9_5_22,cs-410,9,5,Probabilistic,"00:01:35,420","00:01:37,630",22,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=95,So that's the ideal hill climbing.
cs-410_9_5_23,cs-410,9,5,Probabilistic,"00:01:37,630","00:01:43,030",23,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=97,"And in the EM algorithm, the way we"
cs-410_9_5_24,cs-410,9,5,Probabilistic,"00:01:43,030","00:01:46,940",24,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=103,"First, we'll fix a lower"
cs-410_9_5_25,cs-410,9,5,Probabilistic,"00:01:46,940","00:01:48,628",25,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=106,So this is the lower bound.
cs-410_9_5_26,cs-410,9,5,Probabilistic,"00:01:48,628","00:01:49,128",26,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=108,See here.
cs-410_9_5_27,cs-410,9,5,Probabilistic,"00:01:51,010","00:01:57,560",27,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=111,"And once we fit the lower bound,"
cs-410_9_5_28,cs-410,9,5,Probabilistic,"00:01:57,560","00:01:59,420",28,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=117,"And of course, the reason why this works,"
cs-410_9_5_29,cs-410,9,5,Probabilistic,"00:01:59,420","00:02:02,850",29,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=119,is because the lower bound
cs-410_9_5_30,cs-410,9,5,Probabilistic,"00:02:02,850","00:02:05,780",30,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=122,So we know our current guess is here.
cs-410_9_5_31,cs-410,9,5,Probabilistic,"00:02:05,780","00:02:11,530",31,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=125,"And by maximizing the lower bound,"
cs-410_9_5_32,cs-410,9,5,Probabilistic,"00:02:11,530","00:02:12,030",32,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=131,To here.
cs-410_9_5_33,cs-410,9,5,Probabilistic,"00:02:13,300","00:02:14,650",33,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=133,Right?
cs-410_9_5_34,cs-410,9,5,Probabilistic,"00:02:14,650","00:02:20,150",34,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=134,And we can then map to the original
cs-410_9_5_35,cs-410,9,5,Probabilistic,"00:02:20,150","00:02:25,600",35,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=140,"Because it's a lower bound, we are"
cs-410_9_5_36,cs-410,9,5,Probabilistic,"00:02:25,600","00:02:30,570",36,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=145,Because we improve our lower bound and
cs-410_9_5_37,cs-410,9,5,Probabilistic,"00:02:30,570","00:02:35,040",37,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=150,curve which is above this lower bound
cs-410_9_5_38,cs-410,9,5,Probabilistic,"00:02:36,310","00:02:39,090",38,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=156,So we already know it's
cs-410_9_5_39,cs-410,9,5,Probabilistic,"00:02:39,090","00:02:42,440",39,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=159,So we definitely improve this
cs-410_9_5_40,cs-410,9,5,Probabilistic,"00:02:42,440","00:02:47,253",40,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=162,which is above this lower bound.
cs-410_9_5_41,cs-410,9,5,Probabilistic,"00:02:47,253","00:02:49,770",41,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=167,"So, in our example,"
cs-410_9_5_42,cs-410,9,5,Probabilistic,"00:02:49,770","00:02:53,520",42,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=169,the current guess is parameter value
cs-410_9_5_43,cs-410,9,5,Probabilistic,"00:02:53,520","00:02:57,660",43,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=173,And then the next guess is
cs-410_9_5_44,cs-410,9,5,Probabilistic,"00:02:57,660","00:03:01,110",44,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=177,From this illustration you
cs-410_9_5_45,cs-410,9,5,Probabilistic,"00:03:01,110","00:03:03,620",45,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=181,is always better than the current guess.
cs-410_9_5_46,cs-410,9,5,Probabilistic,"00:03:03,620","00:03:06,930",46,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=183,"Unless it has reached the maximum,"
cs-410_9_5_47,cs-410,9,5,Probabilistic,"00:03:06,930","00:03:08,008",47,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=186,So the two would be equal.
cs-410_9_5_48,cs-410,9,5,Probabilistic,"00:03:08,008","00:03:12,821",48,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=188,"So, the E-step is basically"
cs-410_9_5_49,cs-410,9,5,Probabilistic,"00:03:12,821","00:03:17,650",49,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=192,to compute this lower bound.
cs-410_9_5_50,cs-410,9,5,Probabilistic,"00:03:17,650","00:03:22,061",50,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=197,We don't directly just compute
cs-410_9_5_51,cs-410,9,5,Probabilistic,"00:03:22,061","00:03:25,452",51,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=202,we compute the length of
cs-410_9_5_52,cs-410,9,5,Probabilistic,"00:03:25,452","00:03:28,990",52,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=205,these are basically a part
cs-410_9_5_53,cs-410,9,5,Probabilistic,"00:03:28,990","00:03:31,150",53,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=208,This helps determine the lower bound.
cs-410_9_5_54,cs-410,9,5,Probabilistic,"00:03:31,150","00:03:34,460",54,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=211,The M-step on the other hand is
cs-410_9_5_55,cs-410,9,5,Probabilistic,"00:03:34,460","00:03:37,480",55,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=214,It allows us to move
cs-410_9_5_56,cs-410,9,5,Probabilistic,"00:03:37,480","00:03:41,460",56,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=217,And that's why EM algorithm is guaranteed
cs-410_9_5_57,cs-410,9,5,Probabilistic,"00:03:42,490","00:03:46,720",57,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=222,"Now, as you can imagine,"
cs-410_9_5_58,cs-410,9,5,Probabilistic,"00:03:46,720","00:03:50,100",58,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=226,we also have to repeat the EM
cs-410_9_5_59,cs-410,9,5,Probabilistic,"00:03:50,100","00:03:54,340",59,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=230,In order to figure out which one
cs-410_9_5_60,cs-410,9,5,Probabilistic,"00:03:54,340","00:03:59,070",60,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=234,And this actually in general is a
cs-410_9_5_61,cs-410,9,5,Probabilistic,"00:03:59,070","00:04:02,689",61,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=239,So here for
cs-410_9_5_62,cs-410,9,5,Probabilistic,"00:04:02,689","00:04:06,223",62,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=242,then we gradually just
cs-410_9_5_63,cs-410,9,5,Probabilistic,"00:04:06,223","00:04:11,227",63,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=246,"So, that's not optimal, and"
cs-410_9_5_64,cs-410,9,5,Probabilistic,"00:04:11,227","00:04:16,575",64,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=251,so the only way to climb up to this gear
cs-410_9_5_65,cs-410,9,5,Probabilistic,"00:04:16,575","00:04:22,767",65,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=256,"So, in the EM algorithm, we generally"
cs-410_9_5_66,cs-410,9,5,Probabilistic,"00:04:22,767","00:04:27,880",66,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=262,or have some other way to determine
cs-410_9_5_67,cs-410,9,5,Probabilistic,"00:04:29,840","00:04:34,320",67,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=269,To summarize in this lecture we
cs-410_9_5_68,cs-410,9,5,Probabilistic,"00:04:34,320","00:04:38,683",68,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=274,This is a general algorithm for computing
cs-410_9_5_69,cs-410,9,5,Probabilistic,"00:04:38,683","00:04:42,153",69,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=278,"kinds of models, so"
cs-410_9_5_70,cs-410,9,5,Probabilistic,"00:04:42,153","00:04:46,468",70,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=282,"And it's a hill-climbing algorithm, so it"
cs-410_9_5_71,cs-410,9,5,Probabilistic,"00:04:46,468","00:04:48,250",71,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=286,it will depend on initial points.
cs-410_9_5_72,cs-410,9,5,Probabilistic,"00:04:49,770","00:04:55,414",72,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=289,The general idea is that we will have
cs-410_9_5_73,cs-410,9,5,Probabilistic,"00:04:55,414","00:05:00,270",73,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=295,In the E-step we roughly [INAUDIBLE]
cs-410_9_5_74,cs-410,9,5,Probabilistic,"00:05:00,270","00:05:05,560",74,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=300,of useful hidden variables that we
cs-410_9_5_75,cs-410,9,5,Probabilistic,"00:05:05,560","00:05:10,056",75,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=305,"In our case, this is the distribution"
cs-410_9_5_76,cs-410,9,5,Probabilistic,"00:05:10,056","00:05:15,750",76,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=310,In the M-step then we would exploit
cs-410_9_5_77,cs-410,9,5,Probabilistic,"00:05:15,750","00:05:20,790",77,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=315,"it easier to estimate the distribution,"
cs-410_9_5_78,cs-410,9,5,Probabilistic,"00:05:20,790","00:05:24,860",78,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=320,Here improve is guaranteed in
cs-410_9_5_79,cs-410,9,5,Probabilistic,"00:05:24,860","00:05:30,240",79,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=324,Note that it's not necessary that we
cs-410_9_5_80,cs-410,9,5,Probabilistic,"00:05:30,240","00:05:35,260",80,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=330,parameter value even though the likelihood
cs-410_9_5_81,cs-410,9,5,Probabilistic,"00:05:35,260","00:05:40,370",81,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=335,There are some properties that have to
cs-410_9_5_82,cs-410,9,5,Probabilistic,"00:05:40,370","00:05:44,640",82,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=340,also to convert into some stable value.
cs-410_9_5_83,cs-410,9,5,Probabilistic,"00:05:47,500","00:05:50,790",83,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=347,Now here data augmentation
cs-410_9_5_84,cs-410,9,5,Probabilistic,"00:05:50,790","00:05:51,360",84,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=350,"That means,"
cs-410_9_5_85,cs-410,9,5,Probabilistic,"00:05:51,360","00:05:54,830",85,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=351,we're not going to just say exactly
cs-410_9_5_86,cs-410,9,5,Probabilistic,"00:05:54,830","00:05:59,390",86,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=354,But we're going to have a probability
cs-410_9_5_87,cs-410,9,5,Probabilistic,"00:05:59,390","00:06:01,140",87,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=359,these hidden variables.
cs-410_9_5_88,cs-410,9,5,Probabilistic,"00:06:01,140","00:06:05,990",88,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=361,So this causes a split of counts
cs-410_9_5_89,cs-410,9,5,Probabilistic,"00:06:07,430","00:06:12,783",89,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=367,And in our case we'll split the word
cs-410_9_5_90,cs-410,9,5,Probabilistic,"00:06:12,783","00:06:22,783",90,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=372,[MUSIC]
cs-410_9_6_1,cs-410,9,6,Probabilistic,"00:00:00,012","00:00:07,295",1,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=0,[SOUND]
cs-410_9_6_2,cs-410,9,6,Probabilistic,"00:00:07,295","00:00:11,390",2,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=7,lecture is about probabilistic and
cs-410_9_6_3,cs-410,9,6,Probabilistic,"00:00:12,710","00:00:18,000",3,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=12,In this lecture we're going to introduce
cs-410_9_6_4,cs-410,9,6,Probabilistic,"00:00:18,000","00:00:18,770",4,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=18,often called PLSA.
cs-410_9_6_5,cs-410,9,6,Probabilistic,"00:00:18,770","00:00:26,060",5,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=18,"This is the most basic topic model,"
cs-410_9_6_6,cs-410,9,6,Probabilistic,"00:00:26,060","00:00:30,890",6,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=26,Now this kind of models
cs-410_9_6_7,cs-410,9,6,Probabilistic,"00:00:30,890","00:00:34,560",7,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=30,mine multiple topics from text documents.
cs-410_9_6_8,cs-410,9,6,Probabilistic,"00:00:34,560","00:00:39,410",8,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=34,And PRSA is one of the most basic
cs-410_9_6_9,cs-410,9,6,Probabilistic,"00:00:39,410","00:00:43,800",9,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=39,So let's first examine this power
cs-410_9_6_10,cs-410,9,6,Probabilistic,"00:00:43,800","00:00:47,710",10,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=43,Here I show a sample article which is
cs-410_9_6_11,cs-410,9,6,Probabilistic,"00:00:48,830","00:00:51,100",11,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=48,And I show some simple topics.
cs-410_9_6_12,cs-410,9,6,Probabilistic,"00:00:51,100","00:00:55,870",12,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=51,"For example government response,"
cs-410_9_6_13,cs-410,9,6,Probabilistic,"00:00:55,870","00:00:57,420",13,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=55,Donation and the background.
cs-410_9_6_14,cs-410,9,6,Probabilistic,"00:00:59,260","00:01:04,070",14,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=59,You can see in the article we use
cs-410_9_6_15,cs-410,9,6,Probabilistic,"00:01:05,150","00:01:09,540",15,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=65,So we first for example see there's
cs-410_9_6_16,cs-410,9,6,Probabilistic,"00:01:09,540","00:01:14,740",16,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=69,this is followed by discussion of flooding
cs-410_9_6_17,cs-410,9,6,Probabilistic,"00:01:14,740","00:01:17,440",17,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=74,We also see background
cs-410_9_6_18,cs-410,9,6,Probabilistic,"00:01:18,840","00:01:23,740",18,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=78,So the overall of topic analysis here
cs-410_9_6_19,cs-410,9,6,Probabilistic,"00:01:23,740","00:01:28,250",19,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=83,"the text, to segment the topics,"
cs-410_9_6_20,cs-410,9,6,Probabilistic,"00:01:28,250","00:01:33,820",20,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=88,"distribution and to figure out first,"
cs-410_9_6_21,cs-410,9,6,Probabilistic,"00:01:33,820","00:01:36,420",21,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=93,How do we know there's a topic
cs-410_9_6_22,cs-410,9,6,Probabilistic,"00:01:36,420","00:01:39,020",22,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=96,There's a topic about a flood in the city.
cs-410_9_6_23,cs-410,9,6,Probabilistic,"00:01:39,020","00:01:41,850",23,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=99,So these are the tasks
cs-410_9_6_24,cs-410,9,6,Probabilistic,"00:01:42,870","00:01:46,110",24,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=102,If we had discovered these
cs-410_9_6_25,cs-410,9,6,Probabilistic,"00:01:46,110","00:01:50,030",25,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=106,"as you see here,"
cs-410_9_6_26,cs-410,9,6,Probabilistic,"00:01:50,030","00:01:54,390",26,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=110,"Then you can do a lot of things,"
cs-410_9_6_27,cs-410,9,6,Probabilistic,"00:01:54,390","00:01:59,800",27,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=114,"of the topics,"
cs-410_9_6_28,cs-410,9,6,Probabilistic,"00:01:59,800","00:02:04,220",28,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=119,So the formal definition of problem of
cs-410_9_6_29,cs-410,9,6,Probabilistic,"00:02:04,220","00:02:04,870",29,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=124,shown here.
cs-410_9_6_30,cs-410,9,6,Probabilistic,"00:02:04,870","00:02:09,270",30,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=124,And this is after a slide that you
cs-410_9_6_31,cs-410,9,6,Probabilistic,"00:02:09,270","00:02:14,100",31,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=129,"So the input is a collection, the number"
cs-410_9_6_32,cs-410,9,6,Probabilistic,"00:02:14,100","00:02:15,060",32,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=134,of course the text data.
cs-410_9_6_33,cs-410,9,6,Probabilistic,"00:02:16,300","00:02:18,760",33,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=136,And then the output is of two kinds.
cs-410_9_6_34,cs-410,9,6,Probabilistic,"00:02:18,760","00:02:21,720",34,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=138,"One is the topic category,"
cs-410_9_6_35,cs-410,9,6,Probabilistic,"00:02:21,720","00:02:22,520",35,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=141,Theta i's.
cs-410_9_6_36,cs-410,9,6,Probabilistic,"00:02:22,520","00:02:24,790",36,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=142,Each theta i is a word distribution.
cs-410_9_6_37,cs-410,9,6,Probabilistic,"00:02:24,790","00:02:28,160",37,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=144,"And second, it's the topic coverage for"
cs-410_9_6_38,cs-410,9,6,Probabilistic,"00:02:28,160","00:02:30,130",38,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=148,These are pi sub i j's.
cs-410_9_6_39,cs-410,9,6,Probabilistic,"00:02:30,130","00:02:33,490",39,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=150,And they tell us which document it covers.
cs-410_9_6_40,cs-410,9,6,Probabilistic,"00:02:33,490","00:02:35,440",40,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=153,Which topic to what extent.
cs-410_9_6_41,cs-410,9,6,Probabilistic,"00:02:35,440","00:02:37,960",41,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=155,So we hope to generate these as output.
cs-410_9_6_42,cs-410,9,6,Probabilistic,"00:02:37,960","00:02:41,350",42,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=157,Because there are many useful
cs-410_9_6_43,cs-410,9,6,Probabilistic,"00:02:42,880","00:02:47,100",43,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=162,So the idea of PLSA is
cs-410_9_6_44,cs-410,9,6,Probabilistic,"00:02:47,100","00:02:50,660",44,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=167,the two component mixture model
cs-410_9_6_45,cs-410,9,6,Probabilistic,"00:02:50,660","00:02:54,760",45,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=170,The only difference is that we
cs-410_9_6_46,cs-410,9,6,Probabilistic,"00:02:54,760","00:02:57,960",46,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=174,"Otherwise, it is essentially the same."
cs-410_9_6_47,cs-410,9,6,Probabilistic,"00:02:57,960","00:03:03,730",47,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=177,So here I illustrate how we can generate
cs-410_9_6_48,cs-410,9,6,Probabilistic,"00:03:03,730","00:03:06,490",48,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=183,naturally in all cases
cs-410_9_6_49,cs-410,9,6,Probabilistic,"00:03:06,490","00:03:11,310",49,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=186,of Probabilistic modelling would want
cs-410_9_6_50,cs-410,9,6,Probabilistic,"00:03:11,310","00:03:13,400",50,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=191,"So we would also ask the question,"
cs-410_9_6_51,cs-410,9,6,Probabilistic,"00:03:13,400","00:03:18,200",51,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=193,what's the probability of observing
cs-410_9_6_52,cs-410,9,6,Probabilistic,"00:03:18,200","00:03:19,470",52,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=198,Now if you look at this picture and
cs-410_9_6_53,cs-410,9,6,Probabilistic,"00:03:19,470","00:03:21,840",53,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=199,compare this with the picture
cs-410_9_6_54,cs-410,9,6,Probabilistic,"00:03:21,840","00:03:25,580",54,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=201,you will see the only difference is
cs-410_9_6_55,cs-410,9,6,Probabilistic,"00:03:26,940","00:03:32,900",55,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=206,"So, before we have just one topic,"
cs-410_9_6_56,cs-410,9,6,Probabilistic,"00:03:32,900","00:03:35,990",56,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=212,But now we have more topics.
cs-410_9_6_57,cs-410,9,6,Probabilistic,"00:03:35,990","00:03:38,260",57,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=215,"Specifically, we have k topics now."
cs-410_9_6_58,cs-410,9,6,Probabilistic,"00:03:38,260","00:03:43,930",58,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=218,All these are topics that we assume
cs-410_9_6_59,cs-410,9,6,Probabilistic,"00:03:43,930","00:03:49,450",59,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=223,So the consequence is that our switch for
cs-410_9_6_60,cs-410,9,6,Probabilistic,"00:03:49,450","00:03:51,210",60,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=229,Before it's just a two way switch.
cs-410_9_6_61,cs-410,9,6,Probabilistic,"00:03:51,210","00:03:53,420",61,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=231,We can think of it as flipping a coin.
cs-410_9_6_62,cs-410,9,6,Probabilistic,"00:03:53,420","00:03:55,110",62,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=233,But now we have multiple ways.
cs-410_9_6_63,cs-410,9,6,Probabilistic,"00:03:55,110","00:03:59,660",63,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=235,First we can flip a coin to decide
cs-410_9_6_64,cs-410,9,6,Probabilistic,"00:03:59,660","00:04:06,913",64,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=239,So it's the background lambda
cs-410_9_6_65,cs-410,9,6,Probabilistic,"00:04:06,913","00:04:11,490",65,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=246,1 minus lambda sub B gives
cs-410_9_6_66,cs-410,9,6,Probabilistic,"00:04:11,490","00:04:16,300",66,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=251,actually choosing a non-background topic.
cs-410_9_6_67,cs-410,9,6,Probabilistic,"00:04:16,300","00:04:17,860",67,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=256,"After we have made this decision,"
cs-410_9_6_68,cs-410,9,6,Probabilistic,"00:04:17,860","00:04:24,750",68,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=257,we have to make another decision to
cs-410_9_6_69,cs-410,9,6,Probabilistic,"00:04:24,750","00:04:26,480",69,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=264,So there are K way switch here.
cs-410_9_6_70,cs-410,9,6,Probabilistic,"00:04:26,480","00:04:30,120",70,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=266,"And this is characterized by pi,"
cs-410_9_6_71,cs-410,9,6,Probabilistic,"00:04:31,450","00:04:33,775",71,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=271,This is just the difference of designs.
cs-410_9_6_72,cs-410,9,6,Probabilistic,"00:04:33,775","00:04:36,745",72,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=273,Which is a little bit more complicated.
cs-410_9_6_73,cs-410,9,6,Probabilistic,"00:04:36,745","00:04:40,655",73,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=276,But once we decide which distribution to
cs-410_9_6_74,cs-410,9,6,Probabilistic,"00:04:40,655","00:04:45,145",74,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=280,just generate a word by using one of
cs-410_9_6_75,cs-410,9,6,Probabilistic,"00:04:46,885","00:04:50,920",75,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=286,So now lets look at the question
cs-410_9_6_76,cs-410,9,6,Probabilistic,"00:04:50,920","00:04:55,780",76,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=290,So what's the probability of observing
cs-410_9_6_77,cs-410,9,6,Probabilistic,"00:04:55,780","00:04:57,250",77,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=295,What do you think?
cs-410_9_6_78,cs-410,9,6,Probabilistic,"00:04:57,250","00:05:01,150",78,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=297,Now we've seen this
cs-410_9_6_79,cs-410,9,6,Probabilistic,"00:05:01,150","00:05:05,210",79,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=301,"if you can recall, it's generally a sum."
cs-410_9_6_80,cs-410,9,6,Probabilistic,"00:05:05,210","00:05:08,540",80,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=305,Of all the different possibilities
cs-410_9_6_81,cs-410,9,6,Probabilistic,"00:05:08,540","00:05:14,260",81,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=308,So let's first look at how the word can
cs-410_9_6_82,cs-410,9,6,Probabilistic,"00:05:14,260","00:05:18,340",82,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=314,"Well, the probability that the word is"
cs-410_9_6_83,cs-410,9,6,Probabilistic,"00:05:18,340","00:05:22,700",83,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=318,is lambda multiplied by the probability
cs-410_9_6_84,cs-410,9,6,Probabilistic,"00:05:22,700","00:05:24,200",84,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=322,"Model, right."
cs-410_9_6_85,cs-410,9,6,Probabilistic,"00:05:24,200","00:05:25,150",85,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=324,Two things must happen.
cs-410_9_6_86,cs-410,9,6,Probabilistic,"00:05:25,150","00:05:28,270",86,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=325,"First, we have to have"
cs-410_9_6_87,cs-410,9,6,Probabilistic,"00:05:28,270","00:05:31,730",87,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=328,"and that's the probability of lambda,"
cs-410_9_6_88,cs-410,9,6,Probabilistic,"00:05:31,730","00:05:36,330",88,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=331,"Then second, we must have actually"
cs-410_9_6_89,cs-410,9,6,Probabilistic,"00:05:36,330","00:05:39,161",89,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=336,and that's probability
cs-410_9_6_90,cs-410,9,6,Probabilistic,"00:05:40,220","00:05:41,790",90,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=340,"Okay, so similarly,"
cs-410_9_6_91,cs-410,9,6,Probabilistic,"00:05:41,790","00:05:46,020",91,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=341,we can figure out the probability of
cs-410_9_6_92,cs-410,9,6,Probabilistic,"00:05:46,020","00:05:48,530",92,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=346,Like the topic theta sub k.
cs-410_9_6_93,cs-410,9,6,Probabilistic,"00:05:48,530","00:05:51,890",93,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=348,Now notice that here's
cs-410_9_6_94,cs-410,9,6,Probabilistic,"00:05:51,890","00:05:57,023",94,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=351,And that's because of the choice
cs-410_9_6_95,cs-410,9,6,Probabilistic,"00:05:57,023","00:06:00,630",95,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=357,only happens if two things happen.
cs-410_9_6_96,cs-410,9,6,Probabilistic,"00:06:00,630","00:06:04,020",96,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=360,One is we decide not to
cs-410_9_6_97,cs-410,9,6,Probabilistic,"00:06:04,020","00:06:07,630",97,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=364,"So, that's a probability"
cs-410_9_6_98,cs-410,9,6,Probabilistic,"00:06:07,630","00:06:13,290",98,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=367,"Second, we also have to actually choose"
cs-410_9_6_99,cs-410,9,6,Probabilistic,"00:06:13,290","00:06:16,000",99,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=373,"So that's probability of theta sub K,"
cs-410_9_6_100,cs-410,9,6,Probabilistic,"00:06:17,900","00:06:21,460",100,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=377,"And similarly, the probability of"
cs-410_9_6_101,cs-410,9,6,Probabilistic,"00:06:21,460","00:06:26,480",101,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=381,The topic and the first topic
cs-410_9_6_102,cs-410,9,6,Probabilistic,"00:06:26,480","00:06:27,250",102,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=386,And so
cs-410_9_6_103,cs-410,9,6,Probabilistic,"00:06:27,250","00:06:32,480",103,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=387,in the end the probability of observing
cs-410_9_6_104,cs-410,9,6,Probabilistic,"00:06:32,480","00:06:38,080",104,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=392,And I have to stress again this is a very
cs-410_9_6_105,cs-410,9,6,Probabilistic,"00:06:38,080","00:06:44,150",105,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=398,really key to understanding all the topic
cs-410_9_6_106,cs-410,9,6,Probabilistic,"00:06:44,150","00:06:47,410",106,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=404,So make sure that you really
cs-410_9_6_107,cs-410,9,6,Probabilistic,"00:06:49,410","00:06:53,390",107,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=409,of w is indeed the sum of these terms.
cs-410_9_6_108,cs-410,9,6,Probabilistic,"00:06:56,540","00:07:00,620",108,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=416,"So, next,"
cs-410_9_6_109,cs-410,9,6,Probabilistic,"00:07:00,620","00:07:05,250",109,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=420,we would be interested in
cs-410_9_6_110,cs-410,9,6,Probabilistic,"00:07:05,250","00:07:07,250",110,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=425,"All right, so to estimate the parameters."
cs-410_9_6_111,cs-410,9,6,Probabilistic,"00:07:07,250","00:07:07,760",111,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=427,"But firstly,"
cs-410_9_6_112,cs-410,9,6,Probabilistic,"00:07:07,760","00:07:13,510",112,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=427,let's put all these together to have the
cs-410_9_6_113,cs-410,9,6,Probabilistic,"00:07:13,510","00:07:19,010",113,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=433,The first line shows the probability of a
cs-410_9_6_114,cs-410,9,6,Probabilistic,"00:07:19,010","00:07:20,980",114,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=439,And this is an important
cs-410_9_6_115,cs-410,9,6,Probabilistic,"00:07:22,560","00:07:24,250",115,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=442,So let's take a closer look at this.
cs-410_9_6_116,cs-410,9,6,Probabilistic,"00:07:24,250","00:07:27,430",116,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=444,This actually commands all
cs-410_9_6_117,cs-410,9,6,Probabilistic,"00:07:27,430","00:07:29,280",117,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=447,So first of all we see lambda sub b here.
cs-410_9_6_118,cs-410,9,6,Probabilistic,"00:07:29,280","00:07:31,539",118,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=449,This represents a percentage
cs-410_9_6_119,cs-410,9,6,Probabilistic,"00:07:32,610","00:07:35,560",119,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=452,that we believe exist in the text data.
cs-410_9_6_120,cs-410,9,6,Probabilistic,"00:07:35,560","00:07:39,220",120,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=455,And this can be a known value
cs-410_9_6_121,cs-410,9,6,Probabilistic,"00:07:41,180","00:07:43,380",121,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=461,"Second, we see the background"
cs-410_9_6_122,cs-410,9,6,Probabilistic,"00:07:43,380","00:07:45,210",122,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=463,typically we also assume this is known.
cs-410_9_6_123,cs-410,9,6,Probabilistic,"00:07:45,210","00:07:48,000",123,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=465,"We can use a large collection of text, or"
cs-410_9_6_124,cs-410,9,6,Probabilistic,"00:07:48,000","00:07:51,780",124,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=468,use all the text that we have available
cs-410_9_6_125,cs-410,9,6,Probabilistic,"00:07:52,890","00:07:55,008",125,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=472,Now next in the next stop this formula.
cs-410_9_6_126,cs-410,9,6,Probabilistic,"00:07:55,008","00:07:57,960",126,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=475,[COUGH] Excuse me.
cs-410_9_6_127,cs-410,9,6,Probabilistic,"00:07:57,960","00:08:00,160",127,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=477,You see two interesting
cs-410_9_6_128,cs-410,9,6,Probabilistic,"00:08:00,160","00:08:01,886",128,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=480,those are the most important parameters.
cs-410_9_6_129,cs-410,9,6,Probabilistic,"00:08:01,886","00:08:04,690",129,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=481,That we are.
cs-410_9_6_130,cs-410,9,6,Probabilistic,"00:08:04,690","00:08:06,190",130,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=484,So one is pi's.
cs-410_9_6_131,cs-410,9,6,Probabilistic,"00:08:06,190","00:08:10,060",131,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=486,And these are the coverage
cs-410_9_6_132,cs-410,9,6,Probabilistic,"00:08:11,280","00:08:15,310",132,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=491,And the other is word distributions
cs-410_9_6_133,cs-410,9,6,Probabilistic,"00:08:18,530","00:08:23,780",133,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=498,"So the next line,"
cs-410_9_6_134,cs-410,9,6,Probabilistic,"00:08:23,780","00:08:26,280",134,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=503,in to calculate
cs-410_9_6_135,cs-410,9,6,Probabilistic,"00:08:26,280","00:08:29,720",135,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=506,"This is, again, of the familiar"
cs-410_9_6_136,cs-410,9,6,Probabilistic,"00:08:29,720","00:08:32,050",136,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=509,you have a count of
cs-410_9_6_137,cs-410,9,6,Probabilistic,"00:08:32,050","00:08:35,100",137,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=512,And then log of a probability.
cs-410_9_6_138,cs-410,9,6,Probabilistic,"00:08:35,100","00:08:39,040",138,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=515,Now it's a little bit more
cs-410_9_6_139,cs-410,9,6,Probabilistic,"00:08:39,040","00:08:43,890",139,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=519,"Because now we have more components,"
cs-410_9_6_140,cs-410,9,6,Probabilistic,"00:08:43,890","00:08:47,750",140,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=523,And then this line is just
cs-410_9_6_141,cs-410,9,6,Probabilistic,"00:08:47,750","00:08:51,130",141,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=527,"And it's very similar, just accounting for"
cs-410_9_6_142,cs-410,9,6,Probabilistic,"00:08:52,470","00:08:54,060",142,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=532,So what are the unknown parameters?
cs-410_9_6_143,cs-410,9,6,Probabilistic,"00:08:54,060","00:08:55,960",143,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=534,I already said that there are two kinds.
cs-410_9_6_144,cs-410,9,6,Probabilistic,"00:08:55,960","00:08:59,150",144,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=535,"One is coverage,"
cs-410_9_6_145,cs-410,9,6,Probabilistic,"00:08:59,150","00:09:02,350",145,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=539,"Again, it's a useful exercise for"
cs-410_9_6_146,cs-410,9,6,Probabilistic,"00:09:02,350","00:09:04,730",146,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=542,Exactly how many
cs-410_9_6_147,cs-410,9,6,Probabilistic,"00:09:05,750","00:09:07,940",147,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=545,How many unknown parameters are there?
cs-410_9_6_148,cs-410,9,6,Probabilistic,"00:09:07,940","00:09:08,680",148,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=547,"Now, try and"
cs-410_9_6_149,cs-410,9,6,Probabilistic,"00:09:08,680","00:09:13,090",149,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=548,think out that question will help you
cs-410_9_6_150,cs-410,9,6,Probabilistic,"00:09:13,090","00:09:17,760",150,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=553,And will also allow you to understand
cs-410_9_6_151,cs-410,9,6,Probabilistic,"00:09:17,760","00:09:20,430",151,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=557,when use PLSA to analyze text data?
cs-410_9_6_152,cs-410,9,6,Probabilistic,"00:09:20,430","00:09:22,480",152,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=560,And these are precisely
cs-410_9_6_153,cs-410,9,6,Probabilistic,"00:09:24,480","00:09:28,200",153,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=564,So after we have obtained
cs-410_9_6_154,cs-410,9,6,Probabilistic,"00:09:28,200","00:09:30,820",154,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=568,the next is to worry about
cs-410_9_6_155,cs-410,9,6,Probabilistic,"00:09:32,050","00:09:34,770",155,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=572,"And we can do the usual think,"
cs-410_9_6_156,cs-410,9,6,Probabilistic,"00:09:34,770","00:09:40,190",156,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=574,"So again, it's a constrained optimization"
cs-410_9_6_157,cs-410,9,6,Probabilistic,"00:09:40,190","00:09:44,350",157,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=580,Only that we have a collection of text and
cs-410_9_6_158,cs-410,9,6,Probabilistic,"00:09:44,350","00:09:48,655",158,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=584,"And we still have two constraints,"
cs-410_9_6_159,cs-410,9,6,Probabilistic,"00:09:48,655","00:09:50,145",159,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=588,One is the word distributions.
cs-410_9_6_160,cs-410,9,6,Probabilistic,"00:09:51,245","00:09:56,525",160,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=591,All the words must have probabilities
cs-410_9_6_161,cs-410,9,6,Probabilistic,"00:09:56,525","00:09:59,975",161,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=596,The other is the topic
cs-410_9_6_162,cs-410,9,6,Probabilistic,"00:09:59,975","00:10:05,200",162,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=599,a document will have to cover
cs-410_9_6_163,cs-410,9,6,Probabilistic,"00:10:05,200","00:10:08,820",163,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=605,the probability of covering each
cs-410_9_6_164,cs-410,9,6,Probabilistic,"00:10:08,820","00:10:13,190",164,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=608,So at this point though it's basically
cs-410_9_6_165,cs-410,9,6,Probabilistic,"00:10:13,190","00:10:16,370",165,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=613,you just need to figure out
cs-410_9_6_166,cs-410,9,6,Probabilistic,"00:10:16,370","00:10:18,670",166,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=616,There's a function with many variables.
cs-410_9_6_167,cs-410,9,6,Probabilistic,"00:10:18,670","00:10:22,481",167,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=618,and we need to just figure
cs-410_9_6_168,cs-410,9,6,Probabilistic,"00:10:22,481","00:10:26,397",168,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=622,variables to make the function
cs-410_9_6_169,cs-410,9,6,Probabilistic,"00:10:26,397","00:10:36,397",169,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=626,>> [MUSIC]
cs-410_9_7_1,cs-410,9,7,Probabilistic,"00:00:00,025","00:00:05,631",1,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=0,[SOUND] So
cs-410_9_7_2,cs-410,9,7,Probabilistic,"00:00:05,631","00:00:10,816",2,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=5,"PLSA to of LDA and to motivate that,"
cs-410_9_7_3,cs-410,9,7,Probabilistic,"00:00:10,816","00:00:17,145",3,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=10,we need to talk about some
cs-410_9_7_4,cs-410,9,7,Probabilistic,"00:00:17,145","00:00:21,085",4,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=17,"First, it's not really a generative model"
cs-410_9_7_5,cs-410,9,7,Probabilistic,"00:00:21,085","00:00:22,335",5,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=21,a new document.
cs-410_9_7_6,cs-410,9,7,Probabilistic,"00:00:22,335","00:00:26,670",6,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=22,"You can see why, and that's because the"
cs-410_9_7_7,cs-410,9,7,Probabilistic,"00:00:26,670","00:00:31,180",7,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=26,but the pis are tied to the document
cs-410_9_7_8,cs-410,9,7,Probabilistic,"00:00:31,180","00:00:33,790",8,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=31,So we can't compute the pis for
cs-410_9_7_9,cs-410,9,7,Probabilistic,"00:00:34,810","00:00:39,030",9,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=34,"And there's some heuristic workaround,"
cs-410_9_7_10,cs-410,9,7,Probabilistic,"00:00:39,030","00:00:42,990",10,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=39,"Secondly, it has many parameters, and I've"
cs-410_9_7_11,cs-410,9,7,Probabilistic,"00:00:42,990","00:00:47,170",11,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=42,"exactly there are in PLSA, and"
cs-410_9_7_12,cs-410,9,7,Probabilistic,"00:00:47,170","00:00:49,750",12,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=47,That means that model is very complex.
cs-410_9_7_13,cs-410,9,7,Probabilistic,"00:00:49,750","00:00:53,010",13,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=49,And this also means that there
cs-410_9_7_14,cs-410,9,7,Probabilistic,"00:00:53,010","00:00:55,090",14,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=53,it's prone to overfitting.
cs-410_9_7_15,cs-410,9,7,Probabilistic,"00:00:55,090","00:01:01,569",15,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=55,And that means it's very hard to
cs-410_9_7_16,cs-410,9,7,Probabilistic,"00:01:02,630","00:01:05,830",16,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=62,And that we are representing
cs-410_9_7_17,cs-410,9,7,Probabilistic,"00:01:05,830","00:01:09,590",17,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=65,"And in terms of explaining future data,"
cs-410_9_7_18,cs-410,9,7,Probabilistic,"00:01:09,590","00:01:13,260",18,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=69,it will overfit the training data
cs-410_9_7_19,cs-410,9,7,Probabilistic,"00:01:13,260","00:01:18,010",19,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=73,The model is so flexible to fit precisely
cs-410_9_7_20,cs-410,9,7,Probabilistic,"00:01:18,010","00:01:22,980",20,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=78,And then it doesn't allow us to generalize
cs-410_9_7_21,cs-410,9,7,Probabilistic,"00:01:23,990","00:01:28,530",21,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=83,This however is not a necessary problem
cs-410_9_7_22,cs-410,9,7,Probabilistic,"00:01:28,530","00:01:32,150",22,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=88,only interested in hitting
cs-410_9_7_23,cs-410,9,7,Probabilistic,"00:01:32,150","00:01:36,980",23,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=92,We are not always interested in modern
cs-410_9_7_24,cs-410,9,7,Probabilistic,"00:01:36,980","00:01:40,490",24,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=96,"or if we would care about the generality,"
cs-410_9_7_25,cs-410,9,7,Probabilistic,"00:01:42,330","00:01:46,860",25,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=102,"So LDA is proposing to improve that,"
cs-410_9_7_26,cs-410,9,7,Probabilistic,"00:01:46,860","00:01:51,470",26,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=106,PLSA a generative model by imposing
cs-410_9_7_27,cs-410,9,7,Probabilistic,"00:01:51,470","00:01:56,130",27,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=111,Dirichlet is just a special distribution
cs-410_9_7_28,cs-410,9,7,Probabilistic,"00:01:56,130","00:02:00,120",28,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=116,"So in this sense, LDA is just"
cs-410_9_7_29,cs-410,9,7,Probabilistic,"00:02:00,120","00:02:02,290",29,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=120,the parameters are now
cs-410_9_7_30,cs-410,9,7,Probabilistic,"00:02:02,290","00:02:05,570",30,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=122,You will see there are many
cs-410_9_7_31,cs-410,9,7,Probabilistic,"00:02:05,570","00:02:09,260",31,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=125,you can achieve the same goal as PLSA for
cs-410_9_7_32,cs-410,9,7,Probabilistic,"00:02:09,260","00:02:15,130",32,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=129,It means it can compute the top coverage
cs-410_9_7_33,cs-410,9,7,Probabilistic,"00:02:15,130","00:02:17,440",33,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=135,"However, there's no."
cs-410_9_7_34,cs-410,9,7,Probabilistic,"00:02:17,440","00:02:21,660",34,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=137,Why are the parameters for
cs-410_9_7_35,cs-410,9,7,Probabilistic,"00:02:21,660","00:02:26,530",35,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=141,there are fewer parameters and
cs-410_9_7_36,cs-410,9,7,Probabilistic,"00:02:26,530","00:02:29,650",36,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=146,"word distributions,"
cs-410_9_7_37,cs-410,9,7,Probabilistic,"00:02:29,650","00:02:34,300",37,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=149,of influence of these variables because
cs-410_9_7_38,cs-410,9,7,Probabilistic,"00:02:34,300","00:02:38,190",38,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=154,So the influence part again
cs-410_9_7_39,cs-410,9,7,Probabilistic,"00:02:38,190","00:02:41,770",39,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=158,So essentially they are doing something
cs-410_9_7_40,cs-410,9,7,Probabilistic,"00:02:41,770","00:02:48,110",40,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=161,LDA is a more elegant way of looking
cs-410_9_7_41,cs-410,9,7,Probabilistic,"00:02:48,110","00:02:52,810",41,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=168,So let's see how we can
cs-410_9_7_42,cs-410,9,7,Probabilistic,"00:02:52,810","00:02:56,360",42,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=172,a standard PLSA to have LDA.
cs-410_9_7_43,cs-410,9,7,Probabilistic,"00:02:56,360","00:02:59,753",43,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=176,Now a full treatment of LDA is
cs-410_9_7_44,cs-410,9,7,Probabilistic,"00:02:59,753","00:03:03,285",44,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=179,we just don't have time to go in
cs-410_9_7_45,cs-410,9,7,Probabilistic,"00:03:03,285","00:03:07,040",45,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=183,"But here, I just want to give you"
cs-410_9_7_46,cs-410,9,7,Probabilistic,"00:03:07,040","00:03:08,590",46,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=187,"what it enables, all right."
cs-410_9_7_47,cs-410,9,7,Probabilistic,"00:03:08,590","00:03:10,831",47,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=188,So this is the picture of LDA.
cs-410_9_7_48,cs-410,9,7,Probabilistic,"00:03:10,831","00:03:14,940",48,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=190,"Now, I remove the background"
cs-410_9_7_49,cs-410,9,7,Probabilistic,"00:03:15,960","00:03:19,960",49,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=195,"Now, in this model, all these"
cs-410_9_7_50,cs-410,9,7,Probabilistic,"00:03:19,960","00:03:22,220",50,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=199,we do not impose any prior.
cs-410_9_7_51,cs-410,9,7,Probabilistic,"00:03:22,220","00:03:28,650",51,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=202,So these word distributions are now
cs-410_9_7_52,cs-410,9,7,Probabilistic,"00:03:28,650","00:03:32,490",52,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=208,"So these are word distributions, so here."
cs-410_9_7_53,cs-410,9,7,Probabilistic,"00:03:32,490","00:03:35,520",53,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=212,And the other set of parameters are pis.
cs-410_9_7_54,cs-410,9,7,Probabilistic,"00:03:35,520","00:03:37,470",54,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=215,And we would present it as a vector also.
cs-410_9_7_55,cs-410,9,7,Probabilistic,"00:03:37,470","00:03:40,760",55,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=217,And this is more convenient
cs-410_9_7_56,cs-410,9,7,Probabilistic,"00:03:40,760","00:03:44,040",56,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=220,And we have one vector for each document.
cs-410_9_7_57,cs-410,9,7,Probabilistic,"00:03:44,040","00:03:48,820",57,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=224,"And in this case, in theta,"
cs-410_9_7_58,cs-410,9,7,Probabilistic,"00:03:50,140","00:03:53,470",58,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=230,"Now, the difference between LDA and"
cs-410_9_7_59,cs-410,9,7,Probabilistic,"00:03:53,470","00:03:58,390",59,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=233,"PLSA is that in LDA, we're not going"
cs-410_9_7_60,cs-410,9,7,Probabilistic,"00:03:58,390","00:04:02,170",60,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=238,"Instead, we're going to force them to"
cs-410_9_7_61,cs-410,9,7,Probabilistic,"00:04:03,400","00:04:04,900",61,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=243,"So more specifically,"
cs-410_9_7_62,cs-410,9,7,Probabilistic,"00:04:04,900","00:04:09,760",62,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=244,they will be drawn from two Dirichlet
cs-410_9_7_63,cs-410,9,7,Probabilistic,"00:04:09,760","00:04:12,880",63,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=249,the Dirichlet distribution is
cs-410_9_7_64,cs-410,9,7,Probabilistic,"00:04:12,880","00:04:16,600",64,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=252,So it gives us a probability of
cs-410_9_7_65,cs-410,9,7,Probabilistic,"00:04:16,600","00:04:19,190",65,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=256,"Take, for example, pis, right."
cs-410_9_7_66,cs-410,9,7,Probabilistic,"00:04:19,190","00:04:25,100",66,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=259,So this Dirichlet distribution tells
cs-410_9_7_67,cs-410,9,7,Probabilistic,"00:04:25,100","00:04:29,390",67,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=265,And this distribution in itself is
cs-410_9_7_68,cs-410,9,7,Probabilistic,"00:04:29,390","00:04:30,040",68,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=269,of alphas.
cs-410_9_7_69,cs-410,9,7,Probabilistic,"00:04:31,790","00:04:35,130",69,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=271,"Depending on the alphas, we can"
cs-410_9_7_70,cs-410,9,7,Probabilistic,"00:04:35,130","00:04:39,650",70,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=275,ways but with full certain choices of
cs-410_9_7_71,cs-410,9,7,Probabilistic,"00:04:39,650","00:04:40,230",71,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=279,"For example,"
cs-410_9_7_72,cs-410,9,7,Probabilistic,"00:04:40,230","00:04:45,910",72,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=280,you might favor the choice of a relatively
cs-410_9_7_73,cs-410,9,7,Probabilistic,"00:04:45,910","00:04:51,090",73,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=285,Or you might favor generating
cs-410_9_7_74,cs-410,9,7,Probabilistic,"00:04:51,090","00:04:53,000",74,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=291,and this is controlled by alpha.
cs-410_9_7_75,cs-410,9,7,Probabilistic,"00:04:53,000","00:04:56,892",75,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=293,"And similarly here, the topic or"
cs-410_9_7_76,cs-410,9,7,Probabilistic,"00:04:56,892","00:05:01,470",76,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=296,from another Dirichlet
cs-410_9_7_77,cs-410,9,7,Probabilistic,"00:05:01,470","00:05:04,450",77,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=301,"And note that here,"
cs-410_9_7_78,cs-410,9,7,Probabilistic,"00:05:04,450","00:05:10,260",78,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=304,corresponding to our inference on
cs-410_9_7_79,cs-410,9,7,Probabilistic,"00:05:10,260","00:05:10,940",79,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=310,"Whereas here,"
cs-410_9_7_80,cs-410,9,7,Probabilistic,"00:05:10,940","00:05:16,670",80,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=310,beta has n values corresponding to
cs-410_9_7_81,cs-410,9,7,Probabilistic,"00:05:17,700","00:05:22,740",81,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=317,"Now once we impose this price, then"
cs-410_9_7_82,cs-410,9,7,Probabilistic,"00:05:22,740","00:05:27,667",82,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=322,And we start with joined pis from
cs-410_9_7_83,cs-410,9,7,Probabilistic,"00:05:27,667","00:05:32,380",83,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=327,the Dirichlet distribution and
cs-410_9_7_84,cs-410,9,7,Probabilistic,"00:05:35,370","00:05:40,990",84,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=335,"And then, we're going to use the pi"
cs-410_9_7_85,cs-410,9,7,Probabilistic,"00:05:40,990","00:05:45,750",85,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=340,"to use, and this is of course"
cs-410_9_7_86,cs-410,9,7,Probabilistic,"00:05:47,250","00:05:51,580",86,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=347,"And similar here, we're not going"
cs-410_9_7_87,cs-410,9,7,Probabilistic,"00:05:51,580","00:05:56,900",87,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=351,"Instead, we're going to draw one"
cs-410_9_7_88,cs-410,9,7,Probabilistic,"00:05:56,900","00:06:01,960",88,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=356,"And then from this,"
cs-410_9_7_89,cs-410,9,7,Probabilistic,"00:06:01,960","00:06:04,739",89,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=361,And the rest is very similar to the.
cs-410_9_7_90,cs-410,9,7,Probabilistic,"00:06:04,739","00:06:07,550",90,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=364,The likelihood function now
cs-410_9_7_91,cs-410,9,7,Probabilistic,"00:06:07,550","00:06:12,130",91,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=367,But there's a close connection between the
cs-410_9_7_92,cs-410,9,7,Probabilistic,"00:06:12,130","00:06:15,240",92,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=372,So I'm going to illustrate
cs-410_9_7_93,cs-410,9,7,Probabilistic,"00:06:15,240","00:06:16,090",93,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=375,"So in the top,"
cs-410_9_7_94,cs-410,9,7,Probabilistic,"00:06:16,090","00:06:20,730",94,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=376,you see PLSA likelihood function
cs-410_9_7_95,cs-410,9,7,Probabilistic,"00:06:20,730","00:06:22,760",95,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=380,It's copied from previous slide.
cs-410_9_7_96,cs-410,9,7,Probabilistic,"00:06:22,760","00:06:25,820",96,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=382,Only that I dropped the background for
cs-410_9_7_97,cs-410,9,7,Probabilistic,"00:06:27,160","00:06:32,100",97,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=387,So in the LDA formulas you
cs-410_9_7_98,cs-410,9,7,Probabilistic,"00:06:32,100","00:06:34,970",98,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=392,You see the first equation
cs-410_9_7_99,cs-410,9,7,Probabilistic,"00:06:34,970","00:06:39,140",99,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=394,And this is the probability of generating
cs-410_9_7_100,cs-410,9,7,Probabilistic,"00:06:40,690","00:06:45,440",100,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=400,And this formula is a sum of all
cs-410_9_7_101,cs-410,9,7,Probabilistic,"00:06:45,440","00:06:50,230",101,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=405,Inside a sum is a product of
cs-410_9_7_102,cs-410,9,7,Probabilistic,"00:06:50,230","00:06:54,080",102,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=410,multiplied by the probability of
cs-410_9_7_103,cs-410,9,7,Probabilistic,"00:06:55,180","00:06:59,100",103,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=415,"So this is a very important formula,"
cs-410_9_7_104,cs-410,9,7,Probabilistic,"00:06:59,100","00:07:02,800",104,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=419,And this is actually the core
cs-410_9_7_105,cs-410,9,7,Probabilistic,"00:07:02,800","00:07:06,760",105,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=422,And you might see other topic models
cs-410_9_7_106,cs-410,9,7,Probabilistic,"00:07:06,760","00:07:08,230",106,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=426,And they all rely on this.
cs-410_9_7_107,cs-410,9,7,Probabilistic,"00:07:08,230","00:07:11,040",107,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=428,So it's very important to understand this.
cs-410_9_7_108,cs-410,9,7,Probabilistic,"00:07:11,040","00:07:15,140",108,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=431,And this gives us a probability of
cs-410_9_7_109,cs-410,9,7,Probabilistic,"00:07:15,140","00:07:20,930",109,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=435,"Now, next in the probability of"
cs-410_9_7_110,cs-410,9,7,Probabilistic,"00:07:20,930","00:07:26,710",110,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=440,"component in the LDA formula, but the LDA"
cs-410_9_7_111,cs-410,9,7,Probabilistic,"00:07:26,710","00:07:32,930",111,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=446,And that's to account for
cs-410_9_7_112,cs-410,9,7,Probabilistic,"00:07:32,930","00:07:39,180",112,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=452,So they are drawn from the original
cs-410_9_7_113,cs-410,9,7,Probabilistic,"00:07:39,180","00:07:43,210",113,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=459,"That's why we have to take an integral,"
cs-410_9_7_114,cs-410,9,7,Probabilistic,"00:07:43,210","00:07:48,374",114,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=463,could possibly draw from
cs-410_9_7_115,cs-410,9,7,Probabilistic,"00:07:48,374","00:07:52,910",115,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=468,And similarly in the likelihood for
cs-410_9_7_116,cs-410,9,7,Probabilistic,"00:07:52,910","00:07:56,570",116,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=472,"we also see further components added,"
cs-410_9_7_117,cs-410,9,7,Probabilistic,"00:07:58,190","00:07:58,760",117,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=478,Right?
cs-410_9_7_118,cs-410,9,7,Probabilistic,"00:07:58,760","00:08:03,345",118,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=478,So basically in the area we're just
cs-410_9_7_119,cs-410,9,7,Probabilistic,"00:08:03,345","00:08:08,306",119,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=483,the uncertainties and we added of course
cs-410_9_7_120,cs-410,9,7,Probabilistic,"00:08:08,306","00:08:11,480",120,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=488,"the choice of this parameters,"
cs-410_9_7_121,cs-410,9,7,Probabilistic,"00:08:12,910","00:08:15,276",121,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=492,So this is a likelihood function for LDA.
cs-410_9_7_122,cs-410,9,7,Probabilistic,"00:08:15,276","00:08:19,760",122,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=495,"Now, next to this, let's talk about the"
cs-410_9_7_123,cs-410,9,7,Probabilistic,"00:08:19,760","00:08:23,730",123,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=499,Now the parameters can be now estimated
cs-410_9_7_124,cs-410,9,7,Probabilistic,"00:08:23,730","00:08:25,280",124,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=503,maximum likelihood estimate for LDA.
cs-410_9_7_125,cs-410,9,7,Probabilistic,"00:08:25,280","00:08:31,270",125,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=505,Now you might think about how many
cs-410_9_7_126,cs-410,9,7,Probabilistic,"00:08:31,270","00:08:35,050",126,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=511,You'll see there're a fewer parameters
cs-410_9_7_127,cs-410,9,7,Probabilistic,"00:08:35,050","00:08:37,850",127,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=515,parameters are alphas and the betas.
cs-410_9_7_128,cs-410,9,7,Probabilistic,"00:08:37,850","00:08:41,330",128,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=517,So we can use the maximum likelihood
cs-410_9_7_129,cs-410,9,7,Probabilistic,"00:08:41,330","00:08:45,510",129,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=521,"Of course, it's more complicated because"
cs-410_9_7_130,cs-410,9,7,Probabilistic,"00:08:45,510","00:08:46,890",130,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=525,more complicated.
cs-410_9_7_131,cs-410,9,7,Probabilistic,"00:08:46,890","00:08:51,740",131,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=526,But what's also important
cs-410_9_7_132,cs-410,9,7,Probabilistic,"00:08:51,740","00:08:56,350",132,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=531,parameters that we are interested
cs-410_9_7_133,cs-410,9,7,Probabilistic,"00:08:56,350","00:09:00,240",133,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=536,the coverage are no
cs-410_9_7_134,cs-410,9,7,Probabilistic,"00:09:00,240","00:09:04,110",134,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=540,In this case we have to
cs-410_9_7_135,cs-410,9,7,Probabilistic,"00:09:04,110","00:09:09,700",135,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=544,posterior inference to compute them based
cs-410_9_7_136,cs-410,9,7,Probabilistic,"00:09:09,700","00:09:13,900",136,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=549,"Unfortunately, this"
cs-410_9_7_137,cs-410,9,7,Probabilistic,"00:09:13,900","00:09:17,570",137,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=553,So we generally have to resort
cs-410_9_7_138,cs-410,9,7,Probabilistic,"00:09:18,720","00:09:24,220",138,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=558,And there are many methods available for
cs-410_9_7_139,cs-410,9,7,Probabilistic,"00:09:24,220","00:09:29,100",139,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=564,see them when you use different tool kits
cs-410_9_7_140,cs-410,9,7,Probabilistic,"00:09:30,800","00:09:35,120",140,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=570,these different extensions of LDA.
cs-410_9_7_141,cs-410,9,7,Probabilistic,"00:09:35,120","00:09:39,210",141,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=575,"Now here we, of course, can't give"
cs-410_9_7_142,cs-410,9,7,Probabilistic,"00:09:39,210","00:09:43,189",142,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=579,just know that they are computed based in
cs-410_9_7_143,cs-410,9,7,Probabilistic,"00:09:43,189","00:09:50,386",143,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=583,inference by using
cs-410_9_7_144,cs-410,9,7,Probabilistic,"00:09:50,386","00:09:53,820",144,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=590,"But our math [INAUDIBLE],"
cs-410_9_7_145,cs-410,9,7,Probabilistic,"00:09:53,820","00:09:57,900",145,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=593,"in some of our math list,"
cs-410_9_7_146,cs-410,9,7,Probabilistic,"00:09:57,900","00:10:02,720",146,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=597,"And, especially when we use"
cs-410_9_7_147,cs-410,9,7,Probabilistic,"00:10:02,720","00:10:06,260",147,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=602,then the algorithm looks very
cs-410_9_7_148,cs-410,9,7,Probabilistic,"00:10:06,260","00:10:08,800",148,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=606,"So in the end,"
cs-410_9_7_149,cs-410,9,7,Probabilistic,"00:10:10,660","00:10:14,950",149,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=610,So to summarize our discussion
cs-410_9_7_150,cs-410,9,7,Probabilistic,"00:10:14,950","00:10:17,340",150,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=614,these models provide
cs-410_9_7_151,cs-410,9,7,Probabilistic,"00:10:17,340","00:10:22,300",151,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=617,way of mining and analyzing topics
cs-410_9_7_152,cs-410,9,7,Probabilistic,"00:10:22,300","00:10:27,010",152,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=622,The best basic task setup is
cs-410_9_7_153,cs-410,9,7,Probabilistic,"00:10:27,010","00:10:29,540",153,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=627,we're going to output the k topics.
cs-410_9_7_154,cs-410,9,7,Probabilistic,"00:10:29,540","00:10:32,610",154,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=629,Each topic is characterized
cs-410_9_7_155,cs-410,9,7,Probabilistic,"00:10:32,610","00:10:36,999",155,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=632,And we're going to also output proportions
cs-410_9_7_156,cs-410,9,7,Probabilistic,"00:10:38,990","00:10:45,320",156,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=638,"And PLSA is the basic topic model, and"
cs-410_9_7_157,cs-410,9,7,Probabilistic,"00:10:45,320","00:10:48,310",157,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=645,And this is often adequate for
cs-410_9_7_158,cs-410,9,7,Probabilistic,"00:10:48,310","00:10:51,800",158,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=648,That's why we spend a lot of
cs-410_9_7_159,cs-410,9,7,Probabilistic,"00:10:53,190","00:10:57,050",159,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=653,Now LDA improves over
cs-410_9_7_160,cs-410,9,7,Probabilistic,"00:10:57,050","00:11:00,650",160,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=657,This has led to theoretically
cs-410_9_7_161,cs-410,9,7,Probabilistic,"00:11:00,650","00:11:05,740",161,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=660,"However, in practice, LDA and"
cs-410_9_7_162,cs-410,9,7,Probabilistic,"00:11:05,740","00:11:10,890",162,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=665,in practice PLSA and LDA would work
cs-410_9_7_163,cs-410,9,7,Probabilistic,"00:11:12,290","00:11:16,140",163,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=672,Now here are some suggested readings if
cs-410_9_7_164,cs-410,9,7,Probabilistic,"00:11:16,140","00:11:19,340",164,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=676,First is a nice review of
cs-410_9_7_165,cs-410,9,7,Probabilistic,"00:11:20,490","00:11:25,610",165,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=680,The second has a discussion about how
cs-410_9_7_166,cs-410,9,7,Probabilistic,"00:11:25,610","00:11:29,840",166,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=685,Now I've shown you some distributions and
cs-410_9_7_167,cs-410,9,7,Probabilistic,"00:11:29,840","00:11:31,690",167,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=689,But what exactly is a topic?
cs-410_9_7_168,cs-410,9,7,Probabilistic,"00:11:31,690","00:11:35,600",168,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=691,Can we use phrases to label the topic?
cs-410_9_7_169,cs-410,9,7,Probabilistic,"00:11:35,600","00:11:37,720",169,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=695,To make it the more easy to understand and
cs-410_9_7_170,cs-410,9,7,Probabilistic,"00:11:37,720","00:11:40,480",170,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=697,this paper is about the techniques for
cs-410_9_7_171,cs-410,9,7,Probabilistic,"00:11:40,480","00:11:45,820",171,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=700,The third one is empirical comparison
cs-410_9_7_172,cs-410,9,7,Probabilistic,"00:11:45,820","00:11:48,985",172,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=705,The conclusion is that they
cs-410_9_7_173,cs-410,9,7,Probabilistic,"00:11:48,985","00:11:58,985",173,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=708,[MUSIC]
cs-410_10_1_1,cs-410,10,1,Text,"00:00:00,000","00:00:06,538",1,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=0,[MUSIC]
cs-410_10_1_2,cs-410,10,1,Text,"00:00:06,538","00:00:10,520",2,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=6,This lecture is about
cs-410_10_1_3,cs-410,10,1,Text,"00:00:12,830","00:00:18,240",3,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=12,So far we have talked about multiple
cs-410_10_1_4,cs-410,10,1,Text,"00:00:18,240","00:00:21,430",4,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=18,how do we know which
cs-410_10_1_5,cs-410,10,1,Text,"00:00:22,950","00:00:25,540",5,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=22,So this has to do with evaluation.
cs-410_10_1_6,cs-410,10,1,Text,"00:00:25,540","00:00:28,030",6,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=25,Now to talk about evaluation one must
cs-410_10_1_7,cs-410,10,1,Text,"00:00:28,030","00:00:31,480",7,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=28,go back to the clustering bias that
cs-410_10_1_8,cs-410,10,1,Text,"00:00:32,560","00:00:36,770",8,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=32,Because two objects can be similar
cs-410_10_1_9,cs-410,10,1,Text,"00:00:37,780","00:00:41,910",9,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=37,we must clearly specify
cs-410_10_1_10,cs-410,10,1,Text,"00:00:41,910","00:00:46,200",10,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=41,"Without that, the problem of"
cs-410_10_1_11,cs-410,10,1,Text,"00:00:46,200","00:00:51,740",11,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=46,So this perspective is also
cs-410_10_1_12,cs-410,10,1,Text,"00:00:51,740","00:00:53,350",12,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=51,"If you look at this slide, and"
cs-410_10_1_13,cs-410,10,1,Text,"00:00:53,350","00:00:58,830",13,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=53,you can see we have two different
cs-410_10_1_14,cs-410,10,1,Text,"00:00:58,830","00:01:03,640",14,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=58,"if you ask a question, which one is"
cs-410_10_1_15,cs-410,10,1,Text,"00:01:03,640","00:01:07,860",15,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=63,"You actually see, there's no way to answer"
cs-410_10_1_16,cs-410,10,1,Text,"00:01:07,860","00:01:13,420",16,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=67,"we'd like to cluster based on shapes,"
cs-410_10_1_17,cs-410,10,1,Text,"00:01:13,420","00:01:17,480",17,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=73,And that's precisely why
cs-410_10_1_18,cs-410,10,1,Text,"00:01:17,480","00:01:18,680",18,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=77,crucial for evaluation.
cs-410_10_1_19,cs-410,10,1,Text,"00:01:19,710","00:01:23,530",19,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=79,"In general,"
cs-410_10_1_20,cs-410,10,1,Text,"00:01:23,530","00:01:27,580",20,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=83,"one is direct evaluation, and"
cs-410_10_1_21,cs-410,10,1,Text,"00:01:27,580","00:01:29,310",21,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=87,"So in direct evaluation,"
cs-410_10_1_22,cs-410,10,1,Text,"00:01:29,310","00:01:34,060",22,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=89,"we want to answer the following questions,"
cs-410_10_1_23,cs-410,10,1,Text,"00:01:34,060","00:01:37,150",23,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=94,clusters to the ideal clusters
cs-410_10_1_24,cs-410,10,1,Text,"00:01:38,580","00:01:43,040",24,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=98,So the closeness here can be assessed
cs-410_10_1_25,cs-410,10,1,Text,"00:01:44,420","00:01:50,330",25,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=104,from multiple perspectives and
cs-410_10_1_26,cs-410,10,1,Text,"00:01:50,330","00:01:55,320",26,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=110,"of cluster result in multiple angles,"
cs-410_10_1_27,cs-410,10,1,Text,"00:01:56,790","00:02:04,010",27,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=116,Now we also want to quantify
cs-410_10_1_28,cs-410,10,1,Text,"00:02:04,010","00:02:08,500",28,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=124,us to easily compare different measures
cs-410_10_1_29,cs-410,10,1,Text,"00:02:09,870","00:02:14,630",29,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=129,"And finally, you can see, in this case,"
cs-410_10_1_30,cs-410,10,1,Text,"00:02:15,660","00:02:21,660",30,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=135,"by using humans, basically humans"
cs-410_10_1_31,cs-410,10,1,Text,"00:02:21,660","00:02:24,260",31,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=141,desire to clustering bias.
cs-410_10_1_32,cs-410,10,1,Text,"00:02:24,260","00:02:25,610",32,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=144,"Now, how do we do that exactly?"
cs-410_10_1_33,cs-410,10,1,Text,"00:02:25,610","00:02:27,519",33,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=145,"Well, the general procedure"
cs-410_10_1_34,cs-410,10,1,Text,"00:02:28,590","00:02:33,290",34,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=148,Given a test set which consists
cs-410_10_1_35,cs-410,10,1,Text,"00:02:33,290","00:02:37,100",35,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=153,we can have humans to create
cs-410_10_1_36,cs-410,10,1,Text,"00:02:37,100","00:02:42,390",36,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=157,we're going to ask humans to partition
cs-410_10_1_37,cs-410,10,1,Text,"00:02:42,390","00:02:48,490",37,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=162,And they will use their judgments based
cs-410_10_1_38,cs-410,10,1,Text,"00:02:48,490","00:02:54,470",38,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=168,to generate what they think are the best
cs-410_10_1_39,cs-410,10,1,Text,"00:02:54,470","00:02:59,990",39,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=174,used to compare with the system generated
cs-410_10_1_40,cs-410,10,1,Text,"00:03:01,380","00:03:05,700",40,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=181,"And ideally, we want the system results"
cs-410_10_1_41,cs-410,10,1,Text,"00:03:05,700","00:03:08,650",41,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=185,"results, but in general,"
cs-410_10_1_42,cs-410,10,1,Text,"00:03:08,650","00:03:12,590",42,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=188,So we would like to then quantify the
cs-410_10_1_43,cs-410,10,1,Text,"00:03:12,590","00:03:15,410",43,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=192,clusters and the gold standard clusters.
cs-410_10_1_44,cs-410,10,1,Text,"00:03:15,410","00:03:20,110",44,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=195,And this similarity can also be measure
cs-410_10_1_45,cs-410,10,1,Text,"00:03:20,110","00:03:26,750",45,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=200,give us various meshes to quantitatively
cs-410_10_1_46,cs-410,10,1,Text,"00:03:26,750","00:03:34,140",46,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=206,And some of the commonly used measures
cs-410_10_1_47,cs-410,10,1,Text,"00:03:34,140","00:03:40,015",47,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=214,a cluster has a similar object from
cs-410_10_1_48,cs-410,10,1,Text,"00:03:40,015","00:03:45,545",48,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=220,And normalized mutual information
cs-410_10_1_49,cs-410,10,1,Text,"00:03:45,545","00:03:50,485",49,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=225,which basically measures
cs-410_10_1_50,cs-410,10,1,Text,"00:03:50,485","00:03:54,935",50,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=230,cluster of object in the system generally.
cs-410_10_1_51,cs-410,10,1,Text,"00:03:54,935","00:04:00,225",51,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=234,How well can you predict the cluster
cs-410_10_1_52,cs-410,10,1,Text,"00:04:00,225","00:04:01,440",52,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=240,vice versa?
cs-410_10_1_53,cs-410,10,1,Text,"00:04:01,440","00:04:06,315",53,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=241,"And mutual information captures, the"
cs-410_10_1_54,cs-410,10,1,Text,"00:04:06,315","00:04:11,100",54,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=246,and normalized mutual information is often
cs-410_10_1_55,cs-410,10,1,Text,"00:04:11,100","00:04:15,300",55,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=251,used for quantifying the similarity for
cs-410_10_1_56,cs-410,10,1,Text,"00:04:15,300","00:04:19,220",56,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=255,"this evaluation purpose,"
cs-410_10_1_57,cs-410,10,1,Text,"00:04:21,340","00:04:24,405",57,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=261,Now again a thorough discussion
cs-410_10_1_58,cs-410,10,1,Text,"00:04:24,405","00:04:28,810",58,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=264,these evaluation issues would be
cs-410_10_1_59,cs-410,10,1,Text,"00:04:29,820","00:04:34,730",59,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=269,I've suggested some reading in
cs-410_10_1_60,cs-410,10,1,Text,"00:04:34,730","00:04:35,840",60,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=274,at to know more about that.
cs-410_10_1_61,cs-410,10,1,Text,"00:04:36,950","00:04:39,830",61,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=276,So here I just want to
cs-410_10_1_62,cs-410,10,1,Text,"00:04:39,830","00:04:43,830",62,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=279,that would allow you to think about how
cs-410_10_1_63,cs-410,10,1,Text,"00:04:43,830","00:04:48,120",63,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=283,The second way to evaluate text
cs-410_10_1_64,cs-410,10,1,Text,"00:04:48,120","00:04:52,540",64,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=288,"So in this case the question to answer is,"
cs-410_10_1_65,cs-410,10,1,Text,"00:04:52,540","00:04:55,120",65,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=292,the intended applications?
cs-410_10_1_66,cs-410,10,1,Text,"00:04:55,120","00:04:59,730",66,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=295,Now this of course is application
cs-410_10_1_67,cs-410,10,1,Text,"00:04:59,730","00:05:05,390",67,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=299,usefulness is going to depend
cs-410_10_1_68,cs-410,10,1,Text,"00:05:07,140","00:05:11,180",68,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=307,"In this case, the clustering bias is"
cs-410_10_1_69,cs-410,10,1,Text,"00:05:11,180","00:05:12,850",69,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=311,"as well, so"
cs-410_10_1_70,cs-410,10,1,Text,"00:05:12,850","00:05:17,790",70,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=312,what counts as a best cluster result
cs-410_10_1_71,cs-410,10,1,Text,"00:05:19,098","00:05:25,120",71,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=319,Now procedure wise we also would create
cs-410_10_1_72,cs-410,10,1,Text,"00:05:25,120","00:05:29,860",72,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=325,the intended application to quantify
cs-410_10_1_73,cs-410,10,1,Text,"00:05:32,880","00:05:38,870",73,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=332,"In this case,"
cs-410_10_1_74,cs-410,10,1,Text,"00:05:38,870","00:05:43,960",74,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=338,clustering to some application so we often
cs-410_10_1_75,cs-410,10,1,Text,"00:05:45,040","00:05:47,670",75,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=345,This could be the current system for
cs-410_10_1_76,cs-410,10,1,Text,"00:05:47,670","00:05:51,820",76,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=347,then you hope to add
cs-410_10_1_77,cs-410,10,1,Text,"00:05:51,820","00:05:55,340",77,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=351,the baseline system could be using
cs-410_10_1_78,cs-410,10,1,Text,"00:05:55,340","00:05:59,940",78,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=355,And then what you are trying
cs-410_10_1_79,cs-410,10,1,Text,"00:05:59,940","00:06:03,110",79,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=359,you hope to have better
cs-410_10_1_80,cs-410,10,1,Text,"00:06:03,110","00:06:06,911",80,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=363,So in any case you have a baseline system
cs-410_10_1_81,cs-410,10,1,Text,"00:06:06,911","00:06:10,110",81,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=366,algorithm to the baseline system
cs-410_10_1_82,cs-410,10,1,Text,"00:06:11,740","00:06:14,870",82,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=371,And then we have to compare the
cs-410_10_1_83,cs-410,10,1,Text,"00:06:14,870","00:06:18,660",83,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=374,the baseline system in terms
cs-410_10_1_84,cs-410,10,1,Text,"00:06:18,660","00:06:19,940",84,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=378,that particular application.
cs-410_10_1_85,cs-410,10,1,Text,"00:06:21,070","00:06:25,620",85,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=381,So in this case we call it indirect
cs-410_10_1_86,cs-410,10,1,Text,"00:06:25,620","00:06:30,040",86,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=385,explicit assessment of
cs-410_10_1_87,cs-410,10,1,Text,"00:06:30,040","00:06:35,470",87,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=390,rather it's to assess the contribution
cs-410_10_1_88,cs-410,10,1,Text,"00:06:37,350","00:06:40,570",88,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=397,"So, to summarize text clustering,"
cs-410_10_1_89,cs-410,10,1,Text,"00:06:41,950","00:06:46,750",89,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=401,it's a very useful unsupervised
cs-410_10_1_90,cs-410,10,1,Text,"00:06:46,750","00:06:52,860",90,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=406,it's particularly useful for obtaining
cs-410_10_1_91,cs-410,10,1,Text,"00:06:52,860","00:06:56,960",91,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=412,And this is often needed
cs-410_10_1_92,cs-410,10,1,Text,"00:06:56,960","00:06:59,910",92,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=416,this is often the first step when
cs-410_10_1_93,cs-410,10,1,Text,"00:07:01,720","00:07:06,610",93,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=421,The second application or
cs-410_10_1_94,cs-410,10,1,Text,"00:07:06,610","00:07:09,800",94,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=426,discover interesting clustering
cs-410_10_1_95,cs-410,10,1,Text,"00:07:09,800","00:07:11,660",95,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=429,these structures can be very meaningful.
cs-410_10_1_96,cs-410,10,1,Text,"00:07:13,330","00:07:18,752",96,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=433,There are many approaches that can
cs-410_10_1_97,cs-410,10,1,Text,"00:07:18,752","00:07:25,590",97,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=438,we discussed model based approaches and
cs-410_10_1_98,cs-410,10,1,Text,"00:07:25,590","00:07:30,390",98,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=445,"In general, strong clusters tend to"
cs-410_10_1_99,cs-410,10,1,Text,"00:07:30,390","00:07:35,350",99,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=450,Also the effectiveness of a method
cs-410_10_1_100,cs-410,10,1,Text,"00:07:35,350","00:07:40,400",100,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=455,"clustering bias is captured appropriately,"
cs-410_10_1_101,cs-410,10,1,Text,"00:07:40,400","00:07:45,150",101,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=460,"the right generating model, the model"
cs-410_10_1_102,cs-410,10,1,Text,"00:07:45,150","00:07:49,460",102,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=465,the right similarity function
cs-410_10_1_103,cs-410,10,1,Text,"00:07:49,460","00:07:53,730",103,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=469,Deciding the optimal number of customers
cs-410_10_1_104,cs-410,10,1,Text,"00:07:53,730","00:07:59,510",104,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=473,"order cluster methods, and that's"
cs-410_10_1_105,cs-410,10,1,Text,"00:07:59,510","00:08:03,885",105,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=479,and there's no training there how to guide
cs-410_10_1_106,cs-410,10,1,Text,"00:08:05,125","00:08:08,515",106,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=485,Now sometimes you may see some methods
cs-410_10_1_107,cs-410,10,1,Text,"00:08:08,515","00:08:13,575",107,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=488,"the number of clusters, but"
cs-410_10_1_108,cs-410,10,1,Text,"00:08:13,575","00:08:18,135",108,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=493,application of clustering bias there and
cs-410_10_1_109,cs-410,10,1,Text,"00:08:18,135","00:08:23,900",109,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=498,Without clearly defining a clustering
cs-410_10_1_110,cs-410,10,1,Text,"00:08:23,900","00:08:30,860",110,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=503,"the optimal number of cluster is what,"
cs-410_10_1_111,cs-410,10,1,Text,"00:08:31,920","00:08:35,650",111,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=511,And I should also say sometimes we
cs-410_10_1_112,cs-410,10,1,Text,"00:08:35,650","00:08:39,320",112,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=515,"the number of clusters, for example,"
cs-410_10_1_113,cs-410,10,1,Text,"00:08:39,320","00:08:43,340",113,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=519,then obviously you don't want
cs-410_10_1_114,cs-410,10,1,Text,"00:08:43,340","00:08:45,870",114,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=523,the number can be dictated
cs-410_10_1_115,cs-410,10,1,Text,"00:08:46,880","00:08:53,470",115,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=526,"In other situations, we might be"
cs-410_10_1_116,cs-410,10,1,Text,"00:08:53,470","00:08:59,440",116,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=533,to assess whether we've got a good number
cs-410_10_1_117,cs-410,10,1,Text,"00:08:59,440","00:09:04,330",117,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=539,"And to do that,"
cs-410_10_1_118,cs-410,10,1,Text,"00:09:04,330","00:09:06,170",118,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=544,watch how well you can fit the data.
cs-410_10_1_119,cs-410,10,1,Text,"00:09:07,430","00:09:10,740",119,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=547,In general when you add a more components
cs-410_10_1_120,cs-410,10,1,Text,"00:09:10,740","00:09:13,100",120,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=550,"the data better because you, you don't,"
cs-410_10_1_121,cs-410,10,1,Text,"00:09:13,100","00:09:17,350",121,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=553,you can always set the probability
cs-410_10_1_122,cs-410,10,1,Text,"00:09:17,350","00:09:23,930",122,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=557,So you can't in general fit the data
cs-410_10_1_123,cs-410,10,1,Text,"00:09:23,930","00:09:28,350",123,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=563,is as you add more components would you be
cs-410_10_1_124,cs-410,10,1,Text,"00:09:28,350","00:09:33,890",124,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=568,of the data and that can be used to
cs-410_10_1_125,cs-410,10,1,Text,"00:09:33,890","00:09:36,610",125,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=573,And finally evaluation
cs-410_10_1_126,cs-410,10,1,Text,"00:09:36,610","00:09:41,560",126,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=576,this kind can be done both directly and
cs-410_10_1_127,cs-410,10,1,Text,"00:09:41,560","00:09:46,590",127,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=581,do both in order to get a good sense
cs-410_10_1_128,cs-410,10,1,Text,"00:09:46,590","00:09:52,206",128,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=586,So here's some suggested reading and
cs-410_10_1_129,cs-410,10,1,Text,"00:09:52,206","00:09:58,838",129,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=592,to better understand how the matches
cs-410_10_1_130,cs-410,10,1,Text,"00:09:58,838","00:10:08,838",130,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=598,[MUSIC]
cs-410_10_2_1,cs-410,10,2,Text,"00:00:00,192","00:00:03,512",1,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=0,[SOUND]
cs-410_10_2_2,cs-410,10,2,Text,"00:00:06,614","00:00:09,660",2,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=6,This lecture is about text categorization.
cs-410_10_2_3,cs-410,10,2,Text,"00:00:11,360","00:00:15,320",3,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=11,"In this lecture, we're going to"
cs-410_10_2_4,cs-410,10,2,Text,"00:00:16,390","00:00:21,320",4,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=16,This is a very important technique for
cs-410_10_2_5,cs-410,10,2,Text,"00:00:22,470","00:00:27,035",5,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=22,It is relevant to discovery
cs-410_10_2_6,cs-410,10,2,Text,"00:00:27,035","00:00:29,134",6,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=27,knowledge as shown here.
cs-410_10_2_7,cs-410,10,2,Text,"00:00:29,134","00:00:33,380",7,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=29,"First, it's related to topic mining and"
cs-410_10_2_8,cs-410,10,2,Text,"00:00:33,380","00:00:36,060",8,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=33,"And, that's because it has to do with"
cs-410_10_2_9,cs-410,10,2,Text,"00:00:36,060","00:00:40,970",9,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=36,analyzing text to data based
cs-410_10_2_10,cs-410,10,2,Text,"00:00:40,970","00:00:46,239",10,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=40,"Secondly, it's also related to"
cs-410_10_2_11,cs-410,10,2,Text,"00:00:46,239","00:00:51,941",11,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=46,which has to do with discovery knowledge
cs-410_10_2_12,cs-410,10,2,Text,"00:00:51,941","00:00:56,301",12,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=51,"Because we can categorize the authors,"
cs-410_10_2_13,cs-410,10,2,Text,"00:00:56,301","00:01:01,813",13,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=56,based on the content of the articles
cs-410_10_2_14,cs-410,10,2,Text,"00:01:01,813","00:01:06,611",14,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=61,"We can, in general,"
cs-410_10_2_15,cs-410,10,2,Text,"00:01:06,611","00:01:10,800",15,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=66,based on the content that they produce.
cs-410_10_2_16,cs-410,10,2,Text,"00:01:12,300","00:01:16,720",16,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=72,"Finally, it's also related"
cs-410_10_2_17,cs-410,10,2,Text,"00:01:16,720","00:01:21,760",17,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=76,"Because, we can often use text"
cs-410_10_2_18,cs-410,10,2,Text,"00:01:21,760","00:01:26,180",18,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=81,variables in the real world that
cs-410_10_2_19,cs-410,10,2,Text,"00:01:27,230","00:01:32,490",19,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=87,"And so, this is a very important"
cs-410_10_2_20,cs-410,10,2,Text,"00:01:34,820","00:01:37,860",20,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=94,This is the overall plan for
cs-410_10_2_21,cs-410,10,2,Text,"00:01:37,860","00:01:40,750",21,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=97,"First, we're going to talk about"
cs-410_10_2_22,cs-410,10,2,Text,"00:01:40,750","00:01:44,510",22,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=100,why we're interested in
cs-410_10_2_23,cs-410,10,2,Text,"00:01:44,510","00:01:47,920",23,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=104,"And now, we're going to talk about"
cs-410_10_2_24,cs-410,10,2,Text,"00:01:47,920","00:01:50,780",24,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=107,how to evaluate
cs-410_10_2_25,cs-410,10,2,Text,"00:01:50,780","00:01:56,140",25,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=110,"So, the problem of text"
cs-410_10_2_26,cs-410,10,2,Text,"00:01:56,140","00:02:03,461",26,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=116,We're given a set of predefined categories
cs-410_10_2_27,cs-410,10,2,Text,"00:02:03,461","00:02:07,462",27,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=123,"And often,"
cs-410_10_2_28,cs-410,10,2,Text,"00:02:07,462","00:02:12,519",28,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=127,training set of labeled text
cs-410_10_2_29,cs-410,10,2,Text,"00:02:12,519","00:02:17,810",29,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=132,objects have already been
cs-410_10_2_30,cs-410,10,2,Text,"00:02:17,810","00:02:23,040",30,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=137,"And then, the task is to classify"
cs-410_10_2_31,cs-410,10,2,Text,"00:02:23,040","00:02:26,320",31,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=143,more of these predefined categories.
cs-410_10_2_32,cs-410,10,2,Text,"00:02:26,320","00:02:29,139",32,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=146,"So, the picture on this"
cs-410_10_2_33,cs-410,10,2,Text,"00:02:30,270","00:02:32,120",33,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=150,"When we do text categorization,"
cs-410_10_2_34,cs-410,10,2,Text,"00:02:32,120","00:02:37,630",34,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=152,we have a lot of text objects to be
cs-410_10_2_35,cs-410,10,2,Text,"00:02:37,630","00:02:43,820",35,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=157,"the system will, in general,"
cs-410_10_2_36,cs-410,10,2,Text,"00:02:43,820","00:02:49,110",36,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=163,As shown on the right and
cs-410_10_2_37,cs-410,10,2,Text,"00:02:49,110","00:02:54,280",37,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=169,and we often assume the availability
cs-410_10_2_38,cs-410,10,2,Text,"00:02:54,280","00:02:59,060",38,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=174,these are the documents that
cs-410_10_2_39,cs-410,10,2,Text,"00:02:59,060","00:03:01,660",39,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=179,And these examples are very important for
cs-410_10_2_40,cs-410,10,2,Text,"00:03:01,660","00:03:06,110",40,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=181,helping the system to learn
cs-410_10_2_41,cs-410,10,2,Text,"00:03:06,110","00:03:10,180",41,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=186,"And, this would further help"
cs-410_10_2_42,cs-410,10,2,Text,"00:03:11,280","00:03:16,560",42,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=191,the categories of new text
cs-410_10_2_43,cs-410,10,2,Text,"00:03:16,560","00:03:20,950",43,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=196,"So, here are some specific"
cs-410_10_2_44,cs-410,10,2,Text,"00:03:20,950","00:03:26,140",44,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=200,"And in fact, there are many examples,"
cs-410_10_2_45,cs-410,10,2,Text,"00:03:27,230","00:03:33,000",45,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=207,"So first, text objects can vary,"
cs-410_10_2_46,cs-410,10,2,Text,"00:03:33,000","00:03:36,730",46,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=213,"or a passage, or a sentence,"
cs-410_10_2_47,cs-410,10,2,Text,"00:03:36,730","00:03:41,400",47,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=216,"As in the case of clustering, the units"
cs-410_10_2_48,cs-410,10,2,Text,"00:03:41,400","00:03:44,090",48,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=221,this creates a lot of possibilities.
cs-410_10_2_49,cs-410,10,2,Text,"00:03:44,090","00:03:46,690",49,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=224,"Secondly, categories can also vary."
cs-410_10_2_50,cs-410,10,2,Text,"00:03:46,690","00:03:49,880",50,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=226,"Allocate in general,"
cs-410_10_2_51,cs-410,10,2,Text,"00:03:49,880","00:03:51,560",51,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=229,One is internal categories.
cs-410_10_2_52,cs-410,10,2,Text,"00:03:51,560","00:03:55,890",52,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=231,These are categories that
cs-410_10_2_53,cs-410,10,2,Text,"00:03:55,890","00:04:00,850",53,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=235,"For example, topic categories or"
cs-410_10_2_54,cs-410,10,2,Text,"00:04:00,850","00:04:04,810",54,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=240,they generally have to do with
cs-410_10_2_55,cs-410,10,2,Text,"00:04:04,810","00:04:06,930",55,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=244,throughout the categorization
cs-410_10_2_56,cs-410,10,2,Text,"00:04:08,210","00:04:13,430",56,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=248,The other kind is external categories
cs-410_10_2_57,cs-410,10,2,Text,"00:04:13,430","00:04:16,120",57,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=253,associated with the text object.
cs-410_10_2_58,cs-410,10,2,Text,"00:04:16,120","00:04:17,630",58,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=256,"For example,"
cs-410_10_2_59,cs-410,10,2,Text,"00:04:17,630","00:04:22,810",59,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=257,authors are entities associated
cs-410_10_2_60,cs-410,10,2,Text,"00:04:22,810","00:04:28,340",60,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=262,"And so, we can use their content in"
cs-410_10_2_61,cs-410,10,2,Text,"00:04:28,340","00:04:31,670",61,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=268,"which part, for example, and"
cs-410_10_2_62,cs-410,10,2,Text,"00:04:33,540","00:04:38,048",62,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=273,"Or, we can have any"
cs-410_10_2_63,cs-410,10,2,Text,"00:04:38,048","00:04:43,147",63,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=278,associate with text data
cs-410_10_2_64,cs-410,10,2,Text,"00:04:43,147","00:04:47,788",64,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=283,connection between the entity and
cs-410_10_2_65,cs-410,10,2,Text,"00:04:47,788","00:04:54,025",65,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=287,"For example, we might collect a lot"
cs-410_10_2_66,cs-410,10,2,Text,"00:04:54,025","00:04:58,073",66,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=294,"a lot of reviews about a product,"
cs-410_10_2_67,cs-410,10,2,Text,"00:04:58,073","00:05:04,770",67,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=298,this text data can help us infer
cs-410_10_2_68,cs-410,10,2,Text,"00:05:04,770","00:05:07,770",68,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=304,"In that case, we can treat this"
cs-410_10_2_69,cs-410,10,2,Text,"00:05:07,770","00:05:09,921",69,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=307,We can categorize restaurants or
cs-410_10_2_70,cs-410,10,2,Text,"00:05:09,921","00:05:13,924",70,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=309,categorize products based on
cs-410_10_2_71,cs-410,10,2,Text,"00:05:13,924","00:05:17,245",71,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=313,"So, this is an example for"
cs-410_10_2_72,cs-410,10,2,Text,"00:05:17,245","00:05:20,400",72,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=317,Here are some specific
cs-410_10_2_73,cs-410,10,2,Text,"00:05:20,400","00:05:25,110",73,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=320,News categorization is very
cs-410_10_2_74,cs-410,10,2,Text,"00:05:25,110","00:05:30,009",74,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=325,News agencies would like
cs-410_10_2_75,cs-410,10,2,Text,"00:05:30,009","00:05:35,672",75,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=330,categories to categorize
cs-410_10_2_76,cs-410,10,2,Text,"00:05:35,672","00:05:39,824",76,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=335,"And, these virtual article"
cs-410_10_2_77,cs-410,10,2,Text,"00:05:39,824","00:05:43,650",77,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=339,"For example, in the biomedical domain,"
cs-410_10_2_78,cs-410,10,2,Text,"00:05:43,650","00:05:47,930",78,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=343,"MeSH stands for Medical Subject Heading,"
cs-410_10_2_79,cs-410,10,2,Text,"00:05:49,090","00:05:52,490",79,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=349,characterize content of
cs-410_10_2_80,cs-410,10,2,Text,"00:05:54,590","00:05:59,860",80,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=354,Another example of application is spam
cs-410_10_2_81,cs-410,10,2,Text,"00:05:59,860","00:06:04,940",81,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=359,"So, we often have a spam filter"
cs-410_10_2_82,cs-410,10,2,Text,"00:06:04,940","00:06:10,260",82,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=364,to help us distinguish spams
cs-410_10_2_83,cs-410,10,2,Text,"00:06:10,260","00:06:13,000",83,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=370,this is clearly a binary
cs-410_10_2_84,cs-410,10,2,Text,"00:06:14,500","00:06:18,460",84,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=374,Sentiment categorization of
cs-410_10_2_85,cs-410,10,2,Text,"00:06:18,460","00:06:23,120",85,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=378,another kind of applications where we
cs-410_10_2_86,cs-410,10,2,Text,"00:06:23,120","00:06:26,380",86,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=383,negative or positive and
cs-410_10_2_87,cs-410,10,2,Text,"00:06:27,460","00:06:32,820",87,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=387,"So, you can have send them to categories,"
cs-410_10_2_88,cs-410,10,2,Text,"00:06:35,520","00:06:39,480",88,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=395,Another application is automatic
cs-410_10_2_89,cs-410,10,2,Text,"00:06:39,480","00:06:43,750",89,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=399,you might want to automatically sort your
cs-410_10_2_90,cs-410,10,2,Text,"00:06:43,750","00:06:47,320",90,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=403,one application of text categorization
cs-410_10_2_91,cs-410,10,2,Text,"00:06:48,370","00:06:52,580",91,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=408,The results are another important kind
cs-410_10_2_92,cs-410,10,2,Text,"00:06:52,580","00:06:55,910",92,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=412,"to the right person to handle,"
cs-410_10_2_93,cs-410,10,2,Text,"00:06:55,910","00:07:01,890",93,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=415,email messaging is generally routed
cs-410_10_2_94,cs-410,10,2,Text,"00:07:01,890","00:07:05,820",94,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=421,Different people tend to handle
cs-410_10_2_95,cs-410,10,2,Text,"00:07:05,820","00:07:11,220",95,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=425,"And in many cases, a person would manually"
cs-410_10_2_96,cs-410,10,2,Text,"00:07:11,220","00:07:15,231",96,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=431,"But, if you can imagine,"
cs-410_10_2_97,cs-410,10,2,Text,"00:07:15,231","00:07:18,794",97,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=435,text categorization system
cs-410_10_2_98,cs-410,10,2,Text,"00:07:18,794","00:07:24,969",98,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=438,"And, this is a class file, the incoming"
cs-410_10_2_99,cs-410,10,2,Text,"00:07:24,969","00:07:31,265",99,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=444,where each category actually corresponds
cs-410_10_2_100,cs-410,10,2,Text,"00:07:31,265","00:07:35,975",100,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=451,"And finally, author attribution, as I just"
cs-410_10_2_101,cs-410,10,2,Text,"00:07:35,975","00:07:39,759",101,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=455,it's another example of using text
cs-410_10_2_102,cs-410,10,2,Text,"00:07:41,480","00:07:42,960",102,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=461,some other entities.
cs-410_10_2_103,cs-410,10,2,Text,"00:07:42,960","00:07:46,890",103,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=462,"And, there are also many variants"
cs-410_10_2_104,cs-410,10,2,Text,"00:07:46,890","00:07:50,980",104,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=466,"And so, first, we have the simplest case,"
cs-410_10_2_105,cs-410,10,2,Text,"00:07:50,980","00:07:52,990",105,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=470,where there are only two categories.
cs-410_10_2_106,cs-410,10,2,Text,"00:07:52,990","00:07:57,660",106,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=472,"And, there are many examples like that,"
cs-410_10_2_107,cs-410,10,2,Text,"00:07:59,040","00:08:03,600",107,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=479,Applications with one distinguishing
cs-410_10_2_108,cs-410,10,2,Text,"00:08:03,600","00:08:04,940",108,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=483,documents for a particular query.
cs-410_10_2_109,cs-410,10,2,Text,"00:08:06,040","00:08:12,330",109,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=486,Spam filtering just distinguishing spams
cs-410_10_2_110,cs-410,10,2,Text,"00:08:12,330","00:08:16,800",110,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=492,"Sometimes, classifications of"
cs-410_10_2_111,cs-410,10,2,Text,"00:08:16,800","00:08:17,800",111,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=496,positive and a negative.
cs-410_10_2_112,cs-410,10,2,Text,"00:08:19,120","00:08:22,650",112,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=499,A more general case would be K-category
cs-410_10_2_113,cs-410,10,2,Text,"00:08:22,650","00:08:26,755",113,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=502,"many applications like that,"
cs-410_10_2_114,cs-410,10,2,Text,"00:08:26,755","00:08:30,155",114,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=506,"So, topic categorization is often"
cs-410_10_2_115,cs-410,10,2,Text,"00:08:30,155","00:08:31,935",115,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=510,multiple topics.
cs-410_10_2_116,cs-410,10,2,Text,"00:08:31,935","00:08:36,205",116,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=511,Email routing would be another example
cs-410_10_2_117,cs-410,10,2,Text,"00:08:36,205","00:08:39,322",117,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=516,if you route the email to
cs-410_10_2_118,cs-410,10,2,Text,"00:08:39,322","00:08:44,550",118,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=519,then there are multiple
cs-410_10_2_119,cs-410,10,2,Text,"00:08:44,550","00:08:48,212",119,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=524,"So, in all these cases, there are more"
cs-410_10_2_120,cs-410,10,2,Text,"00:08:49,272","00:08:52,382",120,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=529,Another variation is to have
cs-410_10_2_121,cs-410,10,2,Text,"00:08:52,382","00:08:54,442",121,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=532,where categories form a hierarchy.
cs-410_10_2_122,cs-410,10,2,Text,"00:08:54,442","00:08:56,602",122,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=534,"Again, topical hierarchy is very common."
cs-410_10_2_123,cs-410,10,2,Text,"00:08:58,232","00:09:00,742",123,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=538,Yet another variation is
cs-410_10_2_124,cs-410,10,2,Text,"00:09:00,742","00:09:04,550",124,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=540,That's when you have multiple
cs-410_10_2_125,cs-410,10,2,Text,"00:09:04,550","00:09:08,150",125,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=544,then you hope to kind of
cs-410_10_2_126,cs-410,10,2,Text,"00:09:08,150","00:09:13,340",126,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=548,Further leverage the dependency of
cs-410_10_2_127,cs-410,10,2,Text,"00:09:13,340","00:09:15,250",127,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=553,each individual task.
cs-410_10_2_128,cs-410,10,2,Text,"00:09:15,250","00:09:19,870",128,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=555,Among all these binary categorizations
cs-410_10_2_129,cs-410,10,2,Text,"00:09:19,870","00:09:25,170",129,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=559,part of it also is because it's simple and
cs-410_10_2_130,cs-410,10,2,Text,"00:09:25,170","00:09:31,000",130,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=565,it can actually be used to perform
cs-410_10_2_131,cs-410,10,2,Text,"00:09:31,000","00:09:34,839",131,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=571,"For example, a K-category"
cs-410_10_2_132,cs-410,10,2,Text,"00:09:34,839","00:09:38,665",132,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=574,performed by using binary categorization.
cs-410_10_2_133,cs-410,10,2,Text,"00:09:40,075","00:09:43,405",133,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=580,"Basically, we can look at"
cs-410_10_2_134,cs-410,10,2,Text,"00:09:43,405","00:09:49,385",134,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=583,then the binary categorization problem
cs-410_10_2_135,cs-410,10,2,Text,"00:09:49,385","00:09:52,005",135,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=589,"not, meaning in other categories."
cs-410_10_2_136,cs-410,10,2,Text,"00:09:53,485","00:09:59,820",136,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=593,"And, the hierarchical categorization"
cs-410_10_2_137,cs-410,10,2,Text,"00:09:59,820","00:10:04,300",137,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=599,doing flat categorization at each level.
cs-410_10_2_138,cs-410,10,2,Text,"00:10:04,300","00:10:07,000",138,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=604,"So, we have, first, we categorize"
cs-410_10_2_139,cs-410,10,2,Text,"00:10:07,000","00:10:09,140",139,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=607,"a small number of high-level categories,"
cs-410_10_2_140,cs-410,10,2,Text,"00:10:09,140","00:10:13,740",140,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=609,"and inside each category, we have further"
cs-410_10_2_141,cs-410,10,2,Text,"00:10:15,000","00:10:16,728",141,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=615,"So, why is text categorization important?"
cs-410_10_2_142,cs-410,10,2,Text,"00:10:16,728","00:10:21,464",142,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=616,"Well, I already showed that you,"
cs-410_10_2_143,cs-410,10,2,Text,"00:10:21,464","00:10:23,244",143,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=621,there are several reasons.
cs-410_10_2_144,cs-410,10,2,Text,"00:10:23,244","00:10:28,891",144,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=623,One is text categorization helps enrich
cs-410_10_2_145,cs-410,10,2,Text,"00:10:28,891","00:10:34,970",145,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=628,more understanding of text data that's
cs-410_10_2_146,cs-410,10,2,Text,"00:10:34,970","00:10:38,738",146,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=634,"So, now with categorization text can"
cs-410_10_2_147,cs-410,10,2,Text,"00:10:38,738","00:10:47,310",147,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=638,The keyword conditions that's often
cs-410_10_2_148,cs-410,10,2,Text,"00:10:47,310","00:10:52,455",148,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=647,But we can now also add categories and
cs-410_10_2_149,cs-410,10,2,Text,"00:10:55,485","00:11:00,085",149,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=655,Semantic categories assigned can also
cs-410_10_2_150,cs-410,10,2,Text,"00:11:00,085","00:11:01,145",150,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=660,application.
cs-410_10_2_151,cs-410,10,2,Text,"00:11:01,145","00:11:07,869",151,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=661,"So, for example, semantic categories"
cs-410_10_2_152,cs-410,10,2,Text,"00:11:07,869","00:11:12,248",152,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=667,other attribution might
cs-410_10_2_153,cs-410,10,2,Text,"00:11:12,248","00:11:18,118",153,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=672,Another example is when semantic
cs-410_10_2_154,cs-410,10,2,Text,"00:11:18,118","00:11:24,660",154,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=678,of text content and this is another case
cs-410_10_2_155,cs-410,10,2,Text,"00:11:25,950","00:11:30,940",155,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=685,"For example, if we want to know"
cs-410_10_2_156,cs-410,10,2,Text,"00:11:32,010","00:11:37,830",156,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=692,could first categorize the opinions
cs-410_10_2_157,cs-410,10,2,Text,"00:11:37,830","00:11:42,730",157,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=697,"as positive or negative and then, that"
cs-410_10_2_158,cs-410,10,2,Text,"00:11:42,730","00:11:47,810",158,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=702,"the sentiment, and it would tell us about"
cs-410_10_2_159,cs-410,10,2,Text,"00:11:47,810","00:11:52,680",159,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=707,the 70% of the views are positive and
cs-410_10_2_160,cs-410,10,2,Text,"00:11:53,810","00:11:56,865",160,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=713,"So, without doing categorization,"
cs-410_10_2_161,cs-410,10,2,Text,"00:11:56,865","00:12:02,402",161,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=716,it will be much harder to aggregate
cs-410_10_2_162,cs-410,10,2,Text,"00:12:02,402","00:12:07,468",162,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=722,way of coding text in some sense
cs-410_10_2_163,cs-410,10,2,Text,"00:12:07,468","00:12:13,640",163,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=727,"And, sometimes you may see in some"
cs-410_10_2_164,cs-410,10,2,Text,"00:12:13,640","00:12:18,704",164,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=733,"called a text coded,"
cs-410_10_2_165,cs-410,10,2,Text,"00:12:18,704","00:12:22,316",165,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=738,The second kind of reasons is to use text
cs-410_10_2_166,cs-410,10,2,Text,"00:12:22,316","00:12:27,024",166,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=742,categorization to infer
cs-410_10_2_167,cs-410,10,2,Text,"00:12:27,024","00:12:31,950",167,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=747,and text categories allows
cs-410_10_2_168,cs-410,10,2,Text,"00:12:31,950","00:12:36,950",168,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=751,of such entities that
cs-410_10_2_169,cs-410,10,2,Text,"00:12:36,950","00:12:41,140",169,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=756,"So, this means we can"
cs-410_10_2_170,cs-410,10,2,Text,"00:12:41,140","00:12:44,090",170,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=761,to discover knowledge about the world.
cs-410_10_2_171,cs-410,10,2,Text,"00:12:44,090","00:12:48,370",171,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=764,"In general, as long as we can associate"
cs-410_10_2_172,cs-410,10,2,Text,"00:12:48,370","00:12:53,600",172,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=768,we can always the text of data to help
cs-410_10_2_173,cs-410,10,2,Text,"00:12:53,600","00:12:54,502",173,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=773,"So, it's used for"
cs-410_10_2_174,cs-410,10,2,Text,"00:12:54,502","00:12:59,380",174,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=774,single information network that will
cs-410_10_2_175,cs-410,10,2,Text,"00:12:59,380","00:13:03,750",175,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=779,The obvious entities that can be
cs-410_10_2_176,cs-410,10,2,Text,"00:13:03,750","00:13:08,340",176,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=783,"But, you can also imagine the author's"
cs-410_10_2_177,cs-410,10,2,Text,"00:13:08,340","00:13:14,090",177,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=788,other things can be actually
cs-410_10_2_178,cs-410,10,2,Text,"00:13:14,090","00:13:18,860",178,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=794,"Once we have made the connection, then we"
cs-410_10_2_179,cs-410,10,2,Text,"00:13:18,860","00:13:23,520",179,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=798,"So, this is a general way to allow"
cs-410_10_2_180,cs-410,10,2,Text,"00:13:23,520","00:13:26,890",180,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=803,the text categorization to discover
cs-410_10_2_181,cs-410,10,2,Text,"00:13:26,890","00:13:32,330",181,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=806,"Very useful, especially in big text"
cs-410_10_2_182,cs-410,10,2,Text,"00:13:32,330","00:13:38,150",182,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=812,just using text data as extra sets
cs-410_10_2_183,cs-410,10,2,Text,"00:13:38,150","00:13:43,930",183,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=818,to infer certain decision factors
cs-410_10_2_184,cs-410,10,2,Text,"00:13:43,930","00:13:45,710",184,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=823,"Specifically with text, for example,"
cs-410_10_2_185,cs-410,10,2,Text,"00:13:45,710","00:13:49,220",185,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=825,we can also think of examples of
cs-410_10_2_186,cs-410,10,2,Text,"00:13:49,220","00:13:53,190",186,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=829,"For example, discovery of"
cs-410_10_2_187,cs-410,10,2,Text,"00:13:53,190","00:13:59,460",187,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=833,"And, this can be done by categorizing"
cs-410_10_2_188,cs-410,10,2,Text,"00:14:00,680","00:14:05,566",188,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=840,Another example is to predict the party
cs-410_10_2_189,cs-410,10,2,Text,"00:14:05,566","00:14:07,314",189,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=845,on the political speech.
cs-410_10_2_190,cs-410,10,2,Text,"00:14:07,314","00:14:11,967",190,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=847,"And, this is again an example"
cs-410_10_2_191,cs-410,10,2,Text,"00:14:11,967","00:14:15,146",191,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=851,some knowledge about the real world.
cs-410_10_2_192,cs-410,10,2,Text,"00:14:15,146","00:14:19,265",192,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=855,"In nature,"
cs-410_10_2_193,cs-410,10,2,Text,"00:14:19,265","00:14:24,980",193,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=859,that's as we defined and
cs-410_10_2_194,cs-410,10,2,Text,"00:14:24,980","00:14:34,980",194,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=864,[MUSIC]
cs-410_11_1_1,cs-410,11,1,Text,"00:00:00,025","00:00:06,573",1,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=0,[SOUND] This lecture is
cs-410_11_1_2,cs-410,11,1,Text,"00:00:06,573","00:00:12,439",2,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=6,of evaluation of text categorization.
cs-410_11_1_3,cs-410,11,1,Text,"00:00:12,439","00:00:18,302",3,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=12,Earlier we have introduced measures that
cs-410_11_1_4,cs-410,11,1,Text,"00:00:18,302","00:00:19,920",4,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=18,recall.
cs-410_11_1_5,cs-410,11,1,Text,"00:00:19,920","00:00:26,090",5,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=19,For each category and each document
cs-410_11_1_6,cs-410,11,1,Text,"00:00:27,680","00:00:32,530",6,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=27,further examine how to combine the
cs-410_11_1_7,cs-410,11,1,Text,"00:00:32,530","00:00:36,980",7,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=32,"different documents how to aggregate them,"
cs-410_11_1_8,cs-410,11,1,Text,"00:00:36,980","00:00:41,220",8,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=36,You see on the title here I indicated
cs-410_11_1_9,cs-410,11,1,Text,"00:00:41,220","00:00:46,190",9,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=41,this is in contrast to micro average
cs-410_11_1_10,cs-410,11,1,Text,"00:00:47,750","00:00:53,710",10,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=47,"So, again, for each category we're going"
cs-410_11_1_11,cs-410,11,1,Text,"00:00:53,710","00:00:59,880",11,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=53,for example category c1 we have
cs-410_11_1_12,cs-410,11,1,Text,"00:00:59,880","00:01:06,380",12,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=59,And similarly we can do that for category
cs-410_11_1_13,cs-410,11,1,Text,"00:01:06,380","00:01:11,050",13,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=66,Now once we compute that and
cs-410_11_1_14,cs-410,11,1,Text,"00:01:11,050","00:01:13,840",14,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=71,example we can aggregate
cs-410_11_1_15,cs-410,11,1,Text,"00:01:13,840","00:01:17,610",15,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=73,"For all the categories, for"
cs-410_11_1_16,cs-410,11,1,Text,"00:01:17,610","00:01:24,160",16,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=77,And this is often very useful to summarize
cs-410_11_1_17,cs-410,11,1,Text,"00:01:24,160","00:01:26,780",17,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=84,And aggregation can be
cs-410_11_1_18,cs-410,11,1,Text,"00:01:26,780","00:01:32,550",18,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=86,"Again as I said, in a case when you"
cs-410_11_1_19,cs-410,11,1,Text,"00:01:32,550","00:01:36,630",19,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=92,it's always good to think about what's
cs-410_11_1_20,cs-410,11,1,Text,"00:01:36,630","00:01:41,750",20,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=96,"For example, we can consider arithmetic"
cs-410_11_1_21,cs-410,11,1,Text,"00:01:41,750","00:01:46,180",21,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=101,"you can use geometric mean,"
cs-410_11_1_22,cs-410,11,1,Text,"00:01:46,180","00:01:50,540",22,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=106,"Depending on the way you aggregate,"
cs-410_11_1_23,cs-410,11,1,Text,"00:01:50,540","00:01:54,370",23,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=110,"in terms of which method works better,"
cs-410_11_1_24,cs-410,11,1,Text,"00:01:54,370","00:02:00,860",24,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=114,differences and choosing the right one or
cs-410_11_1_25,cs-410,11,1,Text,"00:02:00,860","00:02:03,770",25,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=120,So the difference fore example
cs-410_11_1_26,cs-410,11,1,Text,"00:02:03,770","00:02:08,360",26,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=123,geometrically is that the arithmetically
cs-410_11_1_27,cs-410,11,1,Text,"00:02:08,360","00:02:12,170",27,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=128,values whereas geometrically would
cs-410_11_1_28,cs-410,11,1,Text,"00:02:12,170","00:02:16,940",28,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=132,Base and so whether you are want
cs-410_11_1_29,cs-410,11,1,Text,"00:02:16,940","00:02:22,040",29,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=136,high values would be a question
cs-410_11_1_30,cs-410,11,1,Text,"00:02:22,040","00:02:24,720",30,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=142,similar we can do that for
cs-410_11_1_31,cs-410,11,1,Text,"00:02:24,720","00:02:29,400",31,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=144,So that's how we can generate the overall
cs-410_11_1_32,cs-410,11,1,Text,"00:02:31,660","00:02:36,990",32,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=151,Now we can do the same for aggregation
cs-410_11_1_33,cs-410,11,1,Text,"00:02:36,990","00:02:40,300",33,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=156,So it's exactly the same situation for
cs-410_11_1_34,cs-410,11,1,Text,"00:02:40,300","00:02:42,340",34,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=160,"Precision, recall, and F."
cs-410_11_1_35,cs-410,11,1,Text,"00:02:42,340","00:02:47,130",35,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=162,And then after we have completed
cs-410_11_1_36,cs-410,11,1,Text,"00:02:47,130","00:02:51,590",36,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=167,we're going to aggregate them to generate
cs-410_11_1_37,cs-410,11,1,Text,"00:02:51,590","00:02:52,490",37,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=171,overall F score.
cs-410_11_1_38,cs-410,11,1,Text,"00:02:53,510","00:02:57,380",38,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=173,"These are, again, examining"
cs-410_11_1_39,cs-410,11,1,Text,"00:02:57,380","00:03:00,390",39,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=177,Which one's more useful will
cs-410_11_1_40,cs-410,11,1,Text,"00:03:00,390","00:03:06,180",40,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=180,"In general, it's beneficial to look at"
cs-410_11_1_41,cs-410,11,1,Text,"00:03:06,180","00:03:10,850",41,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=186,And especially if you compare different
cs-410_11_1_42,cs-410,11,1,Text,"00:03:10,850","00:03:16,370",42,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=190,it might reveal which method
cs-410_11_1_43,cs-410,11,1,Text,"00:03:16,370","00:03:19,830",43,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=196,in what situations and
cs-410_11_1_44,cs-410,11,1,Text,"00:03:19,830","00:03:23,070",44,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=199,Understanding the strands of a method or
cs-410_11_1_45,cs-410,11,1,Text,"00:03:23,070","00:03:25,100",45,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=203,this provides further insight for
cs-410_11_1_46,cs-410,11,1,Text,"00:03:28,260","00:03:32,180",46,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=208,"So as I mentioned,"
cs-410_11_1_47,cs-410,11,1,Text,"00:03:32,180","00:03:35,890",47,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=212,in contrast to the macro average
cs-410_11_1_48,cs-410,11,1,Text,"00:03:35,890","00:03:41,110",48,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=215,"In this case, what we do is you"
cs-410_11_1_49,cs-410,11,1,Text,"00:03:41,110","00:03:44,380",49,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=221,and then compute the precision and recall.
cs-410_11_1_50,cs-410,11,1,Text,"00:03:45,460","00:03:50,480",50,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=225,So we can compute the overall
cs-410_11_1_51,cs-410,11,1,Text,"00:03:50,480","00:03:55,832",51,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=230,"how many cases are in true positive,"
cs-410_11_1_52,cs-410,11,1,Text,"00:03:55,832","00:04:01,660",52,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=235,"etc, it's computing the values"
cs-410_11_1_53,cs-410,11,1,Text,"00:04:01,660","00:04:04,090",53,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=241,and then we can compute the precision and
cs-410_11_1_54,cs-410,11,1,Text,"00:04:06,060","00:04:10,296",54,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=246,"In contrast, in macro-averaging, we're"
cs-410_11_1_55,cs-410,11,1,Text,"00:04:10,296","00:04:16,070",55,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=250,And then aggregate over these categories
cs-410_11_1_56,cs-410,11,1,Text,"00:04:16,070","00:04:19,950",56,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=256,then aggregate all the documents but
cs-410_11_1_57,cs-410,11,1,Text,"00:04:21,130","00:04:24,660",57,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=261,Now this would be very similar to
cs-410_11_1_58,cs-410,11,1,Text,"00:04:24,660","00:04:26,390",58,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=264,"used earlier, and"
cs-410_11_1_59,cs-410,11,1,Text,"00:04:26,390","00:04:31,270",59,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=266,one problem here of course to treat all
cs-410_11_1_60,cs-410,11,1,Text,"00:04:32,400","00:04:34,990",60,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=272,And this may not be desirable.
cs-410_11_1_61,cs-410,11,1,Text,"00:04:36,310","00:04:39,160",61,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=276,But it may be a property for
cs-410_11_1_62,cs-410,11,1,Text,"00:04:39,160","00:04:45,570",62,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=279,"especially if we associate the, for"
cs-410_11_1_63,cs-410,11,1,Text,"00:04:45,570","00:04:50,090",63,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=285,"Then we can actually compute for example,"
cs-410_11_1_64,cs-410,11,1,Text,"00:04:50,090","00:04:55,140",64,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=290,Where you associate the different cost or
cs-410_11_1_65,cs-410,11,1,Text,"00:04:56,210","00:04:59,620",65,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=296,so there could be variations of these
cs-410_11_1_66,cs-410,11,1,Text,"00:04:59,620","00:05:06,398",66,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=299,But in general macro average tends to
cs-410_11_1_67,cs-410,11,1,Text,"00:05:06,398","00:05:13,889",67,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=306,just because it might reflect the need for
cs-410_11_1_68,cs-410,11,1,Text,"00:05:14,890","00:05:20,620",68,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=314,on each category or performance on each
cs-410_11_1_69,cs-410,11,1,Text,"00:05:20,620","00:05:27,210",69,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=320,"But macro averaging and micro averaging,"
cs-410_11_1_70,cs-410,11,1,Text,"00:05:27,210","00:05:32,780",70,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=327,and you might see both reported in
cs-410_11_1_71,cs-410,11,1,Text,"00:05:32,780","00:05:36,750",71,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=332,Also sometimes categorization
cs-410_11_1_72,cs-410,11,1,Text,"00:05:36,750","00:05:39,290",72,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=336,be evaluated from ranking prospective.
cs-410_11_1_73,cs-410,11,1,Text,"00:05:40,400","00:05:43,990",73,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=340,And this is because categorization
cs-410_11_1_74,cs-410,11,1,Text,"00:05:43,990","00:05:49,610",74,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=343,often indeed passed it to a human for
cs-410_11_1_75,cs-410,11,1,Text,"00:05:49,610","00:05:53,300",75,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=349,"For example, it might be passed"
cs-410_11_1_76,cs-410,11,1,Text,"00:05:53,300","00:05:58,810",76,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=353,"For example, news articles can be tempted"
cs-410_11_1_77,cs-410,11,1,Text,"00:05:58,810","00:06:01,040",77,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=358,then human editors would
cs-410_11_1_78,cs-410,11,1,Text,"00:06:02,680","00:06:07,500",78,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=362,And all the email messages might be
cs-410_11_1_79,cs-410,11,1,Text,"00:06:07,500","00:06:09,890",79,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=367,handling in the help desk.
cs-410_11_1_80,cs-410,11,1,Text,"00:06:09,890","00:06:14,090",80,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=369,And in such a case the categorizations
cs-410_11_1_81,cs-410,11,1,Text,"00:06:14,090","00:06:18,600",81,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=374,the task for
cs-410_11_1_82,cs-410,11,1,Text,"00:06:19,690","00:06:25,360",82,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=379,"So, in this case the results"
cs-410_11_1_83,cs-410,11,1,Text,"00:06:26,370","00:06:32,450",83,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=386,and if the system can't give a score
cs-410_11_1_84,cs-410,11,1,Text,"00:06:32,450","00:06:39,830",84,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=392,confidence then we can use the scores
cs-410_11_1_85,cs-410,11,1,Text,"00:06:39,830","00:06:44,660",85,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=399,"then evaluate the results as a rank list,"
cs-410_11_1_86,cs-410,11,1,Text,"00:06:44,660","00:06:47,990",86,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=404,Evaluation where you rank
cs-410_11_1_87,cs-410,11,1,Text,"00:06:49,040","00:06:53,840",87,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=409,So for example a discovery of
cs-410_11_1_88,cs-410,11,1,Text,"00:06:55,790","00:07:00,140",88,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=415,based on ranking emails for
cs-410_11_1_89,cs-410,11,1,Text,"00:07:00,140","00:07:04,660",89,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=420,And this is useful if you want people
cs-410_11_1_90,cs-410,11,1,Text,"00:07:04,660","00:07:05,770",90,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=424,"spam, right?"
cs-410_11_1_91,cs-410,11,1,Text,"00:07:05,770","00:07:10,170",91,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=425,The person would then take
cs-410_11_1_92,cs-410,11,1,Text,"00:07:10,170","00:07:14,770",92,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=430,then verify whether this is indeed a spam.
cs-410_11_1_93,cs-410,11,1,Text,"00:07:14,770","00:07:19,180",93,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=434,So to reflect the utility for
cs-410_11_1_94,cs-410,11,1,Text,"00:07:19,180","00:07:23,860",94,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=439,better to evaluate Ranking Chris and this
cs-410_11_1_95,cs-410,11,1,Text,"00:07:25,020","00:07:27,650",95,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=445,And in such a case often
cs-410_11_1_96,cs-410,11,1,Text,"00:07:27,650","00:07:31,810",96,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=447,better formulated as a ranking problem
cs-410_11_1_97,cs-410,11,1,Text,"00:07:31,810","00:07:35,545",97,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=451,"So for example, ranking documents in"
cs-410_11_1_98,cs-410,11,1,Text,"00:07:35,545","00:07:39,255",98,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=455,"as a binary categorization problem,"
cs-410_11_1_99,cs-410,11,1,Text,"00:07:39,255","00:07:43,505",99,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=459,are useful to users from those that
cs-410_11_1_100,cs-410,11,1,Text,"00:07:43,505","00:07:47,045",100,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=463,"frame this as a ranking problem,"
cs-410_11_1_101,cs-410,11,1,Text,"00:07:47,045","00:07:50,540",101,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=467,That's because people tend
cs-410_11_1_102,cs-410,11,1,Text,"00:07:52,160","00:07:56,420",102,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=472,ranking evaluation more reflects
cs-410_11_1_103,cs-410,11,1,Text,"00:07:58,180","00:08:02,230",103,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=478,"So to summarize categorization evaluation,"
cs-410_11_1_104,cs-410,11,1,Text,"00:08:02,230","00:08:05,220",104,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=482,first evaluation is always very
cs-410_11_1_105,cs-410,11,1,Text,"00:08:05,220","00:08:06,090",105,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=485,So get it right.
cs-410_11_1_106,cs-410,11,1,Text,"00:08:07,200","00:08:10,120",106,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=487,"If you don't get it right,"
cs-410_11_1_107,cs-410,11,1,Text,"00:08:10,120","00:08:14,160",107,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=490,And you might be misled to believe
cs-410_11_1_108,cs-410,11,1,Text,"00:08:14,160","00:08:15,810",108,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=494,which is in fact not true.
cs-410_11_1_109,cs-410,11,1,Text,"00:08:15,810","00:08:17,460",109,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=495,So it's very important to get it right.
cs-410_11_1_110,cs-410,11,1,Text,"00:08:18,880","00:08:22,270",110,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=498,Measures must also reflect
cs-410_11_1_111,cs-410,11,1,Text,"00:08:22,270","00:08:24,100",111,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=502,a particular application.
cs-410_11_1_112,cs-410,11,1,Text,"00:08:24,100","00:08:25,760",112,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=504,"For example, in spam filtering and"
cs-410_11_1_113,cs-410,11,1,Text,"00:08:25,760","00:08:29,670",113,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=505,news categorization the results
cs-410_11_1_114,cs-410,11,1,Text,"00:08:30,680","00:08:33,760",114,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=510,So then we would need to
cs-410_11_1_115,cs-410,11,1,Text,"00:08:33,760","00:08:35,490",115,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=513,design measures appropriately.
cs-410_11_1_116,cs-410,11,1,Text,"00:08:36,650","00:08:41,660",116,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=516,We generally need to consider how will the
cs-410_11_1_117,cs-410,11,1,Text,"00:08:41,660","00:08:43,630",117,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=521,and think from a user's perspective.
cs-410_11_1_118,cs-410,11,1,Text,"00:08:43,630","00:08:46,220",118,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=523,What quality is important?
cs-410_11_1_119,cs-410,11,1,Text,"00:08:46,220","00:08:47,880",119,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=526,What aspect of quality is important?
cs-410_11_1_120,cs-410,11,1,Text,"00:08:49,240","00:08:52,440",120,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=529,Sometimes there are trade offs between
cs-410_11_1_121,cs-410,11,1,Text,"00:08:52,440","00:08:57,610",121,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=532,recall and so we need to know for this
cs-410_11_1_122,cs-410,11,1,Text,"00:08:57,610","00:08:58,860",122,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=537,or high precision is more important.
cs-410_11_1_123,cs-410,11,1,Text,"00:08:59,910","00:09:03,570",123,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=539,Ideally we associate the different cost
cs-410_11_1_124,cs-410,11,1,Text,"00:09:03,570","00:09:06,810",124,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=543,And this of course has to be designed
cs-410_11_1_125,cs-410,11,1,Text,"00:09:08,140","00:09:12,950",125,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=548,Some commonly used measures for relative
cs-410_11_1_126,cs-410,11,1,Text,"00:09:12,950","00:09:17,268",126,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=552,"Classification accuracy, it's very"
cs-410_11_1_127,cs-410,11,1,Text,"00:09:17,268","00:09:22,230",127,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=557,[INAUDIBLE] preceding [INAUDIBLE]
cs-410_11_1_128,cs-410,11,1,Text,"00:09:22,230","00:09:27,266",128,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=562,"report characterizing performances,"
cs-410_11_1_129,cs-410,11,1,Text,"00:09:27,266","00:09:32,440",129,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=567,[INAUDIBLE] like a [INAUDIBLE] Per
cs-410_11_1_130,cs-410,11,1,Text,"00:09:32,440","00:09:37,790",130,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=572,"take a average of all of them, different"
cs-410_11_1_131,cs-410,11,1,Text,"00:09:37,790","00:09:42,910",131,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=577,"In general, you want to look at the"
cs-410_11_1_132,cs-410,11,1,Text,"00:09:42,910","00:09:46,970",132,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=582,particular applications some perspectives
cs-410_11_1_133,cs-410,11,1,Text,"00:09:46,970","00:09:50,120",133,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=586,diagnoses and
cs-410_11_1_134,cs-410,11,1,Text,"00:09:50,120","00:09:54,920",134,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=590,It's generally useful to look at
cs-410_11_1_135,cs-410,11,1,Text,"00:09:54,920","00:10:00,220",135,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=594,to see subtle differences between methods
cs-410_11_1_136,cs-410,11,1,Text,"00:10:00,220","00:10:03,100",136,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=600,from which you can obtain sight for
cs-410_11_1_137,cs-410,11,1,Text,"00:10:04,670","00:10:07,340",137,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=604,Finally sometimes ranking
cs-410_11_1_138,cs-410,11,1,Text,"00:10:07,340","00:10:11,590",138,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=607,be careful sometimes categorization has
cs-410_11_1_139,cs-410,11,1,Text,"00:10:11,590","00:10:16,390",139,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=611,and there're machine running methods for
cs-410_11_1_140,cs-410,11,1,Text,"00:10:17,480","00:10:19,990",140,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=617,So here are two suggested readings.
cs-410_11_1_141,cs-410,11,1,Text,"00:10:19,990","00:10:25,120",141,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=619,One is some chapters of this book where
cs-410_11_1_142,cs-410,11,1,Text,"00:10:25,120","00:10:27,090",142,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=625,evaluation measures.
cs-410_11_1_143,cs-410,11,1,Text,"00:10:27,090","00:10:31,916",143,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=627,The second is a paper about
cs-410_11_1_144,cs-410,11,1,Text,"00:10:31,916","00:10:33,759",144,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=631,text categorization and
cs-410_11_1_145,cs-410,11,1,Text,"00:10:33,759","00:10:39,738",145,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=633,it also has an excellent discussion of
cs-410_11_1_146,cs-410,11,1,Text,"00:10:39,738","00:10:49,738",146,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=639,[MUSIC]
cs-410_12_1_1,cs-410,12,1,Opinion,"00:00:00,160","00:00:07,187",1,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=0,[SOUND]
cs-410_12_1_2,cs-410,12,1,Opinion,"00:00:07,187","00:00:09,830",2,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=7,lecture is about
cs-410_12_1_3,cs-410,12,1,Opinion,"00:00:11,010","00:00:14,400",3,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=11,Contextual text mining
cs-410_12_1_4,cs-410,12,1,Opinion,"00:00:14,400","00:00:18,880",4,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=14,kinds of knowledge that we mine from
cs-410_12_1_5,cs-410,12,1,Opinion,"00:00:18,880","00:00:23,580",5,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=18,It's related to topic mining because you
cs-410_12_1_6,cs-410,12,1,Opinion,"00:00:23,580","00:00:25,260",6,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=23,like time or location.
cs-410_12_1_7,cs-410,12,1,Opinion,"00:00:25,260","00:00:29,720",7,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=25,"And similarly, we can make opinion"
cs-410_12_1_8,cs-410,12,1,Opinion,"00:00:29,720","00:00:32,590",8,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=29,making opinions connected to context.
cs-410_12_1_9,cs-410,12,1,Opinion,"00:00:34,090","00:00:38,320",9,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=34,It's related to text based prediction
cs-410_12_1_10,cs-410,12,1,Opinion,"00:00:38,320","00:00:43,690",10,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=38,data with text data to derive
cs-410_12_1_11,cs-410,12,1,Opinion,"00:00:43,690","00:00:45,110",11,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=43,the prediction problem.
cs-410_12_1_12,cs-410,12,1,Opinion,"00:00:45,110","00:00:49,290",12,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=45,"So more specifically, why are we"
cs-410_12_1_13,cs-410,12,1,Opinion,"00:00:49,290","00:00:54,510",13,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=49,"Well, that's first because text"
cs-410_12_1_14,cs-410,12,1,Opinion,"00:00:54,510","00:01:01,740",14,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=54,And this can include direct context such
cs-410_12_1_15,cs-410,12,1,Opinion,"00:01:01,740","00:01:06,260",15,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=61,"So, the direct context can grow"
cs-410_12_1_16,cs-410,12,1,Opinion,"00:01:06,260","00:01:10,030",16,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=66,"location, authors, and"
cs-410_12_1_17,cs-410,12,1,Opinion,"00:01:10,030","00:01:12,710",17,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=70,And they're almost always available to us.
cs-410_12_1_18,cs-410,12,1,Opinion,"00:01:14,280","00:01:19,690",18,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=74,Indirect context refers to additional
cs-410_12_1_19,cs-410,12,1,Opinion,"00:01:19,690","00:01:24,440",19,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=79,"So for example, from office,"
cs-410_12_1_20,cs-410,12,1,Opinion,"00:01:24,440","00:01:29,280",20,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=84,context such as social network of
cs-410_12_1_21,cs-410,12,1,Opinion,"00:01:30,300","00:01:34,028",21,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=90,Such information is not in general
cs-410_12_1_22,cs-410,12,1,Opinion,"00:01:34,028","00:01:37,350",22,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=94,"through the process, we can connect them."
cs-410_12_1_23,cs-410,12,1,Opinion,"00:01:37,350","00:01:41,080",23,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=97,There could be other text
cs-410_12_1_24,cs-410,12,1,Opinion,"00:01:41,080","00:01:46,760",24,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=101,as this one through the other text can
cs-410_12_1_25,cs-410,12,1,Opinion,"00:01:46,760","00:01:51,400",25,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=106,"So in general, any related data"
cs-410_12_1_26,cs-410,12,1,Opinion,"00:01:51,400","00:01:53,830",26,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=111,So there could be removed or
cs-410_12_1_27,cs-410,12,1,Opinion,"00:01:55,460","00:01:56,870",27,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=115,And so what's the use?
cs-410_12_1_28,cs-410,12,1,Opinion,"00:01:56,870","00:01:59,100",28,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=116,What is text context used for?
cs-410_12_1_29,cs-410,12,1,Opinion,"00:01:59,100","00:02:05,880",29,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=119,"Well, context can be used to partition"
cs-410_12_1_30,cs-410,12,1,Opinion,"00:02:05,880","00:02:11,490",30,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=125,It can almost allow us to partition
cs-410_12_1_31,cs-410,12,1,Opinion,"00:02:11,490","00:02:14,180",31,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=131,And this is very important
cs-410_12_1_32,cs-410,12,1,Opinion,"00:02:14,180","00:02:18,060",32,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=134,us to do interesting comparative analyses.
cs-410_12_1_33,cs-410,12,1,Opinion,"00:02:18,060","00:02:21,622",33,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=138,"It also in general,"
cs-410_12_1_34,cs-410,12,1,Opinion,"00:02:21,622","00:02:23,770",34,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=141,if we associate the text with context.
cs-410_12_1_35,cs-410,12,1,Opinion,"00:02:25,290","00:02:30,540",35,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=145,So here's illustration of how context
cs-410_12_1_36,cs-410,12,1,Opinion,"00:02:30,540","00:02:35,520",36,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=150,can be regarded as interesting
cs-410_12_1_37,cs-410,12,1,Opinion,"00:02:35,520","00:02:39,449",37,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=155,So here I just showed some research
cs-410_12_1_38,cs-410,12,1,Opinion,"00:02:41,740","00:02:43,122",38,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=161,"On different venues,"
cs-410_12_1_39,cs-410,12,1,Opinion,"00:02:43,122","00:02:48,150",39,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=163,different conference names here listed on
cs-410_12_1_40,cs-410,12,1,Opinion,"00:02:49,640","00:02:53,080",40,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=169,Now such text data can be partitioned
cs-410_12_1_41,cs-410,12,1,Opinion,"00:02:53,080","00:02:55,480",41,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=173,in many interesting ways
cs-410_12_1_42,cs-410,12,1,Opinion,"00:02:56,860","00:03:01,620",42,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=176,So the context here just includes time and
cs-410_12_1_43,cs-410,12,1,Opinion,"00:03:01,620","00:03:04,930",43,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=181,But perhaps we can include
cs-410_12_1_44,cs-410,12,1,Opinion,"00:03:06,480","00:03:08,890",44,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=186,But let's see how we can partition
cs-410_12_1_45,cs-410,12,1,Opinion,"00:03:08,890","00:03:12,840",45,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=188,"First, we can treat each"
cs-410_12_1_46,cs-410,12,1,Opinion,"00:03:12,840","00:03:17,987",46,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=192,"So in this case, a paper ID and the,"
cs-410_12_1_47,cs-410,12,1,Opinion,"00:03:17,987","00:03:22,575",47,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=197,It's independent.
cs-410_12_1_48,cs-410,12,1,Opinion,"00:03:22,575","00:03:27,825",48,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=202,But we can also treat all the papers
cs-410_12_1_49,cs-410,12,1,Opinion,"00:03:27,825","00:03:32,530",49,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=207,this is only possible because
cs-410_12_1_50,cs-410,12,1,Opinion,"00:03:32,530","00:03:34,605",50,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=212,And we can partition data in this way.
cs-410_12_1_51,cs-410,12,1,Opinion,"00:03:34,605","00:03:38,012",51,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=214,This would allow us to compare topics for
cs-410_12_1_52,cs-410,12,1,Opinion,"00:03:39,840","00:03:42,745",52,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=219,"Similarly, we can partition"
cs-410_12_1_53,cs-410,12,1,Opinion,"00:03:42,745","00:03:47,740",53,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=222,We can get all the SIGIR papers and
cs-410_12_1_54,cs-410,12,1,Opinion,"00:03:47,740","00:03:51,039",54,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=227,"Or compare SIGIR papers with KDD papers,"
cs-410_12_1_55,cs-410,12,1,Opinion,"00:03:52,700","00:03:58,740",55,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=232,We can also partition the data to obtain
cs-410_12_1_56,cs-410,12,1,Opinion,"00:03:58,740","00:04:03,340",56,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=238,"and that of course,"
cs-410_12_1_57,cs-410,12,1,Opinion,"00:04:03,340","00:04:08,240",57,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=243,And this would allow us to then
cs-410_12_1_58,cs-410,12,1,Opinion,"00:04:08,240","00:04:12,390",58,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=248,another set of papers written
cs-410_12_1_59,cs-410,12,1,Opinion,"00:04:13,910","00:04:17,810",59,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=253,Or we can obtain a set of
cs-410_12_1_60,cs-410,12,1,Opinion,"00:04:17,810","00:04:21,890",60,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=257,this can be compared with
cs-410_12_1_61,cs-410,12,1,Opinion,"00:04:21,890","00:04:25,310",61,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=261,And note that these
cs-410_12_1_62,cs-410,12,1,Opinion,"00:04:25,310","00:04:28,860",62,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=265,intersected with each other to generate
cs-410_12_1_63,cs-410,12,1,Opinion,"00:04:29,890","00:04:34,280",63,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=269,"And so in general, this enables"
cs-410_12_1_64,cs-410,12,1,Opinion,"00:04:34,280","00:04:35,890",64,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=274,different context as needed.
cs-410_12_1_65,cs-410,12,1,Opinion,"00:04:37,150","00:04:40,300",65,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=277,"And in particular,"
cs-410_12_1_66,cs-410,12,1,Opinion,"00:04:40,300","00:04:43,620",66,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=280,And this often gives us
cs-410_12_1_67,cs-410,12,1,Opinion,"00:04:43,620","00:04:49,350",67,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=283,"For example, comparing topics over time,"
cs-410_12_1_68,cs-410,12,1,Opinion,"00:04:49,350","00:04:53,130",68,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=289,Comparing topics in different
cs-410_12_1_69,cs-410,12,1,Opinion,"00:04:53,130","00:04:55,230",69,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=293,about the two contexts.
cs-410_12_1_70,cs-410,12,1,Opinion,"00:04:55,230","00:04:59,680",70,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=295,So there are many interesting questions
cs-410_12_1_71,cs-410,12,1,Opinion,"00:04:59,680","00:05:01,780",71,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=299,Here I list some very specific ones.
cs-410_12_1_72,cs-410,12,1,Opinion,"00:05:01,780","00:05:05,300",72,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=301,"For example, what topics have"
cs-410_12_1_73,cs-410,12,1,Opinion,"00:05:05,300","00:05:07,420",73,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=305,recently in data mining research?
cs-410_12_1_74,cs-410,12,1,Opinion,"00:05:07,420","00:05:08,570",74,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=307,"Now to answer this question,"
cs-410_12_1_75,cs-410,12,1,Opinion,"00:05:08,570","00:05:11,724",75,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=308,obviously we need to analyze
cs-410_12_1_76,cs-410,12,1,Opinion,"00:05:13,815","00:05:17,455",76,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=313,So time is context in this case.
cs-410_12_1_77,cs-410,12,1,Opinion,"00:05:17,455","00:05:20,675",77,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=317,Is there any difference in the responses
cs-410_12_1_78,cs-410,12,1,Opinion,"00:05:20,675","00:05:22,885",78,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=320,"to the event, to any event?"
cs-410_12_1_79,cs-410,12,1,Opinion,"00:05:22,885","00:05:25,515",79,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=322,So this is a very broad
cs-410_12_1_80,cs-410,12,1,Opinion,"00:05:25,515","00:05:28,635",80,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=325,"In this case of course,"
cs-410_12_1_81,cs-410,12,1,Opinion,"00:05:28,635","00:05:31,260",81,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=328,What are the common research
cs-410_12_1_82,cs-410,12,1,Opinion,"00:05:31,260","00:05:34,110",82,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=331,"In this case, authors can be the context."
cs-410_12_1_83,cs-410,12,1,Opinion,"00:05:34,110","00:05:38,250",83,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=334,Is there any difference in the research
cs-410_12_1_84,cs-410,12,1,Opinion,"00:05:38,250","00:05:39,920",84,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=338,those outside?
cs-410_12_1_85,cs-410,12,1,Opinion,"00:05:39,920","00:05:43,990",85,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=339,"Now in this case,"
cs-410_12_1_86,cs-410,12,1,Opinion,"00:05:43,990","00:05:46,030",86,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=343,their affiliation and location.
cs-410_12_1_87,cs-410,12,1,Opinion,"00:05:47,810","00:05:51,700",87,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=347,So this goes beyond just
cs-410_12_1_88,cs-410,12,1,Opinion,"00:05:51,700","00:05:55,520",88,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=351,We need to look at the additional
cs-410_12_1_89,cs-410,12,1,Opinion,"00:05:55,520","00:05:58,580",89,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=355,Is there any difference in the opinions
cs-410_12_1_90,cs-410,12,1,Opinion,"00:05:58,580","00:06:00,720",90,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=358,one social network and another?
cs-410_12_1_91,cs-410,12,1,Opinion,"00:06:00,720","00:06:04,870",91,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=360,"In this case, the social network of"
cs-410_12_1_92,cs-410,12,1,Opinion,"00:06:06,128","00:06:10,250",92,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=366,Other topics in news data that
cs-410_12_1_93,cs-410,12,1,Opinion,"00:06:10,250","00:06:11,470",93,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=370,stock prices.
cs-410_12_1_94,cs-410,12,1,Opinion,"00:06:11,470","00:06:16,060",94,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=371,"In this case, we can use a time series"
cs-410_12_1_95,cs-410,12,1,Opinion,"00:06:17,230","00:06:20,780",95,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=377,What issues mattered in the 2012
cs-410_12_1_96,cs-410,12,1,Opinion,"00:06:20,780","00:06:21,410",96,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=380,presidential election?
cs-410_12_1_97,cs-410,12,1,Opinion,"00:06:21,410","00:06:26,350",97,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=381,"Now in this case,"
cs-410_12_1_98,cs-410,12,1,Opinion,"00:06:26,350","00:06:29,005",98,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=386,"So, as you can see,"
cs-410_12_1_99,cs-410,12,1,Opinion,"00:06:29,005","00:06:34,911",99,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=389,"Basically, contextual text mining"
cs-410_12_1_100,cs-410,12,1,Opinion,"00:06:34,911","00:06:44,911",100,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=394,[MUSIC]
cs-410_12_2_1,cs-410,12,2,Opinion,"00:00:06,910","00:00:09,570",1,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=6,This lecture is a summary
cs-410_12_2_2,cs-410,12,2,Opinion,"00:00:10,810","00:00:14,512",2,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=10,"First, let's revisit the topics"
cs-410_12_2_3,cs-410,12,2,Opinion,"00:00:14,512","00:00:18,662",3,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=14,"In the beginning, we talked about"
cs-410_12_2_4,cs-410,12,2,Opinion,"00:00:18,662","00:00:21,025",4,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=18,how it can enrich text representation.
cs-410_12_2_5,cs-410,12,2,Opinion,"00:00:21,025","00:00:26,207",5,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=21,We then talked about how to mine
cs-410_12_2_6,cs-410,12,2,Opinion,"00:00:26,207","00:00:29,434",6,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=26,"natural language used to express the,"
cs-410_12_2_7,cs-410,12,2,Opinion,"00:00:29,434","00:00:33,270",7,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=29,what's observing the world in text and
cs-410_12_2_8,cs-410,12,2,Opinion,"00:00:34,320","00:00:38,410",8,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=34,"In particular, we talked about"
cs-410_12_2_9,cs-410,12,2,Opinion,"00:00:38,410","00:00:42,300",9,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=38,We then talked about how
cs-410_12_2_10,cs-410,12,2,Opinion,"00:00:42,300","00:00:45,130",10,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=42,How to discover topics and analyze them.
cs-410_12_2_11,cs-410,12,2,Opinion,"00:00:47,580","00:00:51,314",11,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=47,This can be regarded as
cs-410_12_2_12,cs-410,12,2,Opinion,"00:00:51,314","00:00:55,747",12,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=51,and then we talked about how to mine
cs-410_12_2_13,cs-410,12,2,Opinion,"00:00:55,747","00:01:00,988",13,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=55,"particularly talk about the, how to"
cs-410_12_2_14,cs-410,12,2,Opinion,"00:01:00,988","00:01:06,048",14,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=60,"And finally, we will talk about"
cs-410_12_2_15,cs-410,12,2,Opinion,"00:01:06,048","00:01:11,204",15,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=66,do with predicting values of other real
cs-410_12_2_16,cs-410,12,2,Opinion,"00:01:11,204","00:01:16,270",16,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=71,"And in discussing this, we will also"
cs-410_12_2_17,cs-410,12,2,Opinion,"00:01:16,270","00:01:21,421",17,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=76,which can contribute additional
cs-410_12_2_18,cs-410,12,2,Opinion,"00:01:21,421","00:01:25,425",18,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=81,and also can provide context for
cs-410_12_2_19,cs-410,12,2,Opinion,"00:01:25,425","00:01:30,110",19,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=85,in particular we talked about how
cs-410_12_2_20,cs-410,12,2,Opinion,"00:01:33,240","00:01:39,078",20,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=93,So here are the key high-level
cs-410_12_2_21,cs-410,12,2,Opinion,"00:01:39,078","00:01:41,670",21,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=99,I going to go over these major topics and
cs-410_12_2_22,cs-410,12,2,Opinion,"00:01:41,670","00:01:46,400",22,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=101,point out what are the key take-away
cs-410_12_2_23,cs-410,12,2,Opinion,"00:01:47,560","00:01:50,630",23,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=107,First the NLP and text representation.
cs-410_12_2_24,cs-410,12,2,Opinion,"00:01:53,530","00:01:56,840",24,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=113,You should realize that NLP
cs-410_12_2_25,cs-410,12,2,Opinion,"00:01:56,840","00:02:01,510",25,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=116,any text replication because it
cs-410_12_2_26,cs-410,12,2,Opinion,"00:02:01,510","00:02:05,060",26,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=121,The more NLP the better text
cs-410_12_2_27,cs-410,12,2,Opinion,"00:02:05,060","00:02:08,500",27,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=125,And this further enables more
cs-410_12_2_28,cs-410,12,2,Opinion,"00:02:08,500","00:02:11,710",28,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=128,"to discover deeper knowledge,"
cs-410_12_2_29,cs-410,12,2,Opinion,"00:02:12,950","00:02:17,510",29,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=132,"However, the current estate of art"
cs-410_12_2_30,cs-410,12,2,Opinion,"00:02:17,510","00:02:19,130",30,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=137,still not robust enough.
cs-410_12_2_31,cs-410,12,2,Opinion,"00:02:19,130","00:02:23,586",31,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=139,"So, as an result,"
cs-410_12_2_32,cs-410,12,2,Opinion,"00:02:23,586","00:02:26,960",32,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=143,tend to be based on world [INAUDIBLE].
cs-410_12_2_33,cs-410,12,2,Opinion,"00:02:26,960","00:02:30,710",33,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=146,And tend to rely a lot
cs-410_12_2_34,cs-410,12,2,Opinion,"00:02:30,710","00:02:33,520",34,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=150,as we've discussed in this course.
cs-410_12_2_35,cs-410,12,2,Opinion,"00:02:33,520","00:02:39,700",35,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=153,And you may recall we've mostly
cs-410_12_2_36,cs-410,12,2,Opinion,"00:02:39,700","00:02:42,478",36,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=159,And we've relied a lot on
cs-410_12_2_37,cs-410,12,2,Opinion,"00:02:42,478","00:02:45,202",37,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=162,statistical learning
cs-410_12_2_38,cs-410,12,2,Opinion,"00:02:47,790","00:02:52,771",38,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=167,In word-association mining and
cs-410_12_2_39,cs-410,12,2,Opinion,"00:02:52,771","00:02:56,282",39,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=172,we are introduced the two concepts for
cs-410_12_2_40,cs-410,12,2,Opinion,"00:02:56,282","00:03:02,835",40,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=176,"complementary relations of words,"
cs-410_12_2_41,cs-410,12,2,Opinion,"00:03:02,835","00:03:08,130",41,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=182,These are actually very general
cs-410_12_2_42,cs-410,12,2,Opinion,"00:03:08,130","00:03:14,330",42,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=188,If you take it as meaning
cs-410_12_2_43,cs-410,12,2,Opinion,"00:03:14,330","00:03:18,840",43,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=194,context in the sequence and elements
cs-410_12_2_44,cs-410,12,2,Opinion,"00:03:18,840","00:03:24,090",44,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=198,And these relations might be also
cs-410_12_2_45,cs-410,12,2,Opinion,"00:03:25,810","00:03:29,810",45,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=205,We also talked a lot about
cs-410_12_2_46,cs-410,12,2,Opinion,"00:03:29,810","00:03:34,350",46,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=209,discuss how to discover
cs-410_12_2_47,cs-410,12,2,Opinion,"00:03:34,350","00:03:38,390",47,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=214,the context of words discover
cs-410_12_2_48,cs-410,12,2,Opinion,"00:03:38,390","00:03:39,858",48,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=218,"At that point level,"
cs-410_12_2_49,cs-410,12,2,Opinion,"00:03:39,858","00:03:44,437",49,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=219,we talked about representing text
cs-410_12_2_50,cs-410,12,2,Opinion,"00:03:44,437","00:03:48,638",50,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=224,And we talked about some retrieval
cs-410_12_2_51,cs-410,12,2,Opinion,"00:03:48,638","00:03:52,995",51,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=228,measuring similarity of text and
cs-410_12_2_52,cs-410,12,2,Opinion,"00:03:52,995","00:03:55,193",52,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=232,"tf-idf weighting, et cetera."
cs-410_12_2_53,cs-410,12,2,Opinion,"00:03:55,193","00:03:59,480",53,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=235,And this part is well-connected
cs-410_12_2_54,cs-410,12,2,Opinion,"00:03:59,480","00:04:02,330",54,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=239,There are other techniques that
cs-410_12_2_55,cs-410,12,2,Opinion,"00:04:03,890","00:04:08,650",55,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=243,The next point is about
cs-410_12_2_56,cs-410,12,2,Opinion,"00:04:08,650","00:04:12,770",56,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=248,we introduce some information
cs-410_12_2_57,cs-410,12,2,Opinion,"00:04:12,770","00:04:15,170",57,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=252,"conditional entropy,"
cs-410_12_2_58,cs-410,12,2,Opinion,"00:04:15,170","00:04:18,293",58,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=255,These are not only very useful for
cs-410_12_2_59,cs-410,12,2,Opinion,"00:04:18,293","00:04:23,680",59,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=258,"measuring the co-occurrences of words,"
cs-410_12_2_60,cs-410,12,2,Opinion,"00:04:23,680","00:04:26,940",60,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=263,"analyzing other kind of data, and"
cs-410_12_2_61,cs-410,12,2,Opinion,"00:04:26,940","00:04:29,600",61,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=266,feature selection in text
cs-410_12_2_62,cs-410,12,2,Opinion,"00:04:30,920","00:04:34,460",62,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=270,"So this is another important concept,"
cs-410_12_2_63,cs-410,12,2,Opinion,"00:04:35,480","00:04:38,640",63,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=275,And then we talked about
cs-410_12_2_64,cs-410,12,2,Opinion,"00:04:38,640","00:04:41,570",64,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=278,that's where we introduce in
cs-410_12_2_65,cs-410,12,2,Opinion,"00:04:41,570","00:04:45,960",65,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=281,We spent a lot of time to
cs-410_12_2_66,cs-410,12,2,Opinion,"00:04:45,960","00:04:52,930",66,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=285,"PLSA in detail and this is, those are the"
cs-410_12_2_67,cs-410,12,2,Opinion,"00:04:52,930","00:04:56,190",67,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=292,"Theoretically, a more opinion model, but"
cs-410_12_2_68,cs-410,12,2,Opinion,"00:04:56,190","00:05:01,460",68,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=296,we did not have enough time to really
cs-410_12_2_69,cs-410,12,2,Opinion,"00:05:02,960","00:05:06,600",69,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=302,"But in practice,"
cs-410_12_2_70,cs-410,12,2,Opinion,"00:05:06,600","00:05:09,520",70,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=306,it's simpler to implement and
cs-410_12_2_71,cs-410,12,2,Opinion,"00:05:11,520","00:05:15,930",71,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=311,In this part of Wilson videos is some
cs-410_12_2_72,cs-410,12,2,Opinion,"00:05:15,930","00:05:20,410",72,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=315,"know, one is generative model,"
cs-410_12_2_73,cs-410,12,2,Opinion,"00:05:20,410","00:05:23,630",73,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=320,modeling text data and
cs-410_12_2_74,cs-410,12,2,Opinion,"00:05:24,740","00:05:30,250",74,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=324,And we talked about the maximum life
cs-410_12_2_75,cs-410,12,2,Opinion,"00:05:30,250","00:05:35,290",75,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=330,solving the problem of
cs-410_12_2_76,cs-410,12,2,Opinion,"00:05:35,290","00:05:38,720",76,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=335,"So, these are all general techniques"
cs-410_12_2_77,cs-410,12,2,Opinion,"00:05:38,720","00:05:39,840",77,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=338,in other scenarios as well.
cs-410_12_2_78,cs-410,12,2,Opinion,"00:05:40,940","00:05:45,020",78,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=340,Then we talked about the text
cs-410_12_2_79,cs-410,12,2,Opinion,"00:05:45,020","00:05:50,450",79,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=345,Those are two important building blocks
cs-410_12_2_80,cs-410,12,2,Opinion,"00:05:50,450","00:05:56,110",80,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=350,In text with clustering we talked
cs-410_12_2_81,cs-410,12,2,Opinion,"00:05:56,110","00:06:02,400",81,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=356,using a slightly different mixture module
cs-410_12_2_82,cs-410,12,2,Opinion,"00:06:02,400","00:06:07,060",82,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=362,and we then also prefer to
cs-410_12_2_83,cs-410,12,2,Opinion,"00:06:07,060","00:06:10,000",83,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=367,approaches to test for cuss word.
cs-410_12_2_84,cs-410,12,2,Opinion,"00:06:11,340","00:06:15,350",84,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=371,In categorization we also talk
cs-410_12_2_85,cs-410,12,2,Opinion,"00:06:15,350","00:06:19,390",85,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=375,One is generative classifies
cs-410_12_2_86,cs-410,12,2,Opinion,"00:06:20,690","00:06:24,870",86,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=380,infer the condition of or
cs-410_12_2_87,cs-410,12,2,Opinion,"00:06:24,870","00:06:28,250",87,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=384,in deeper we'll introduce you should
cs-410_12_2_88,cs-410,12,2,Opinion,"00:06:29,280","00:06:36,160",88,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=389,"This is the practical use for technique,"
cs-410_12_2_89,cs-410,12,2,Opinion,"00:06:37,210","00:06:41,010",89,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=397,We also introduce the some
cs-410_12_2_90,cs-410,12,2,Opinion,"00:06:41,010","00:06:45,300",90,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=401,"particularly logistical regression,"
cs-410_12_2_91,cs-410,12,2,Opinion,"00:06:45,300","00:06:49,030",91,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=405,"They also very important, they are very"
cs-410_12_2_92,cs-410,12,2,Opinion,"00:06:49,030","00:06:50,490",92,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=409,text capitalization as well.
cs-410_12_2_93,cs-410,12,2,Opinion,"00:06:52,370","00:06:57,100",93,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=412,"In both parts, we'll also discuss"
cs-410_12_2_94,cs-410,12,2,Opinion,"00:06:57,100","00:07:03,110",94,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=417,Evaluation is quite important because if
cs-410_12_2_95,cs-410,12,2,Opinion,"00:07:03,110","00:07:07,430",95,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=423,reflect the volatility of the method then
cs-410_12_2_96,cs-410,12,2,Opinion,"00:07:07,430","00:07:10,530",96,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=427,its very important to
cs-410_12_2_97,cs-410,12,2,Opinion,"00:07:10,530","00:07:15,420",97,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=430,And we talked about variation of
cs-410_12_2_98,cs-410,12,2,Opinion,"00:07:15,420","00:07:16,550",98,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=435,specific measures.
cs-410_12_2_99,cs-410,12,2,Opinion,"00:07:18,530","00:07:21,725",99,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=438,Then we talked about the sentiment
cs-410_12_2_100,cs-410,12,2,Opinion,"00:07:21,725","00:07:25,053",100,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=441,that's where we introduced
cs-410_12_2_101,cs-410,12,2,Opinion,"00:07:25,053","00:07:29,681",101,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=445,And although it's a special
cs-410_12_2_102,cs-410,12,2,Opinion,"00:07:29,681","00:07:34,932",102,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=449,we talked about how to extend or
cs-410_12_2_103,cs-410,12,2,Opinion,"00:07:34,932","00:07:41,261",103,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=454,by using more sophisticated features that
cs-410_12_2_104,cs-410,12,2,Opinion,"00:07:41,261","00:07:46,240",104,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=461,We did a review of some common use for
cs-410_12_2_105,cs-410,12,2,Opinion,"00:07:46,240","00:07:50,836",105,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=466,then we also talked about how to
cs-410_12_2_106,cs-410,12,2,Opinion,"00:07:50,836","00:07:55,511",106,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=470,"in sentiment classification, and"
cs-410_12_2_107,cs-410,12,2,Opinion,"00:07:55,511","00:08:00,822",107,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=475,logistical regression then we also talked
cs-410_12_2_108,cs-410,12,2,Opinion,"00:08:00,822","00:08:05,104",108,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=480,This is an unsupervised way of using
cs-410_12_2_109,cs-410,12,2,Opinion,"00:08:05,104","00:08:07,280",109,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=485,review data in more detail.
cs-410_12_2_110,cs-410,12,2,Opinion,"00:08:07,280","00:08:12,650",110,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=487,"In particular, it allows us to"
cs-410_12_2_111,cs-410,12,2,Opinion,"00:08:14,580","00:08:18,490",111,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=494,a reviewer on different
cs-410_12_2_112,cs-410,12,2,Opinion,"00:08:18,490","00:08:20,998",112,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=498,So given text reviews
cs-410_12_2_113,cs-410,12,2,Opinion,"00:08:20,998","00:08:24,503",113,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=500,the method allows even further
cs-410_12_2_114,cs-410,12,2,Opinion,"00:08:24,503","00:08:26,781",114,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=504,"And it also allows us to infer,"
cs-410_12_2_115,cs-410,12,2,Opinion,"00:08:26,781","00:08:30,638",115,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=506,the viewers laying their
cs-410_12_2_116,cs-410,12,2,Opinion,"00:08:30,638","00:08:35,740",116,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=510,which aspects are more important to
cs-410_12_2_117,cs-410,12,2,Opinion,"00:08:35,740","00:08:39,140",117,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=515,And this enables a lot of
cs-410_12_2_118,cs-410,12,2,Opinion,"00:08:41,330","00:08:46,260",118,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=521,"Finally, in the discussion of prediction,"
cs-410_12_2_119,cs-410,12,2,Opinion,"00:08:46,260","00:08:50,340",119,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=526,"of text and non text data, as they"
cs-410_12_2_120,cs-410,12,2,Opinion,"00:08:51,960","00:08:57,070",120,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=531,We particularly talked about how text data
cs-410_12_2_121,cs-410,12,2,Opinion,"00:08:58,100","00:09:01,863",121,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=538,In the case of using non-text
cs-410_12_2_122,cs-410,12,2,Opinion,"00:09:01,863","00:09:04,565",122,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=541,we talked about
cs-410_12_2_123,cs-410,12,2,Opinion,"00:09:04,565","00:09:08,921",123,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=544,We introduced the contextual PLSA as a
cs-410_12_2_124,cs-410,12,2,Opinion,"00:09:08,921","00:09:13,354",124,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=548,to allows us to incorporate the context
cs-410_12_2_125,cs-410,12,2,Opinion,"00:09:13,354","00:09:18,328",125,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=553,And this is a general way to allow us
cs-410_12_2_126,cs-410,12,2,Opinion,"00:09:18,328","00:09:20,248",126,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=558,of patterns in text data.
cs-410_12_2_127,cs-410,12,2,Opinion,"00:09:20,248","00:09:24,750",127,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=560,"We also introduced the net PLSA,"
cs-410_12_2_128,cs-410,12,2,Opinion,"00:09:24,750","00:09:30,550",128,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=564,network in general of text
cs-410_12_2_129,cs-410,12,2,Opinion,"00:09:31,950","00:09:36,520",129,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=571,And finally we talk about how
cs-410_12_2_130,cs-410,12,2,Opinion,"00:09:36,520","00:09:40,560",130,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=576,mine potentially causal
cs-410_12_2_131,cs-410,12,2,Opinion,"00:09:43,110","00:09:46,560",131,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=583,"Now, in the other way of using text to"
cs-410_12_2_132,cs-410,12,2,Opinion,"00:09:47,990","00:09:51,470",132,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=587,help interpret patterns
cs-410_12_2_133,cs-410,12,2,Opinion,"00:09:51,470","00:09:57,300",133,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=591,we did not really discuss anything in
cs-410_12_2_134,cs-410,12,2,Opinion,"00:09:57,300","00:10:02,670",134,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=597,I should stress that that's after a very
cs-410_12_2_135,cs-410,12,2,Opinion,"00:10:02,670","00:10:06,700",135,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=602,if you want to build a practical
cs-410_12_2_136,cs-410,12,2,Opinion,"00:10:06,700","00:10:10,730",136,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=606,because understanding and
cs-410_12_2_137,cs-410,12,2,Opinion,"00:10:13,870","00:10:18,560",137,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=613,So this is a summary of the key
cs-410_12_2_138,cs-410,12,2,Opinion,"00:10:18,560","00:10:22,710",138,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=618,I hope these will be very
cs-410_12_2_139,cs-410,12,2,Opinion,"00:10:22,710","00:10:27,010",139,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=622,text mining applications or to you for
cs-410_12_2_140,cs-410,12,2,Opinion,"00:10:27,010","00:10:31,100",140,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=627,And this should provide a good basis for
cs-410_12_2_141,cs-410,12,2,Opinion,"00:10:31,100","00:10:33,580",141,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=631,to know more about more of allowance for
cs-410_12_2_142,cs-410,12,2,Opinion,"00:10:33,580","00:10:37,320",142,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=633,other organisms or
cs-410_12_2_143,cs-410,12,2,Opinion,"00:10:40,320","00:10:43,760",143,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=640,"So to know more about this topic,"
cs-410_12_2_144,cs-410,12,2,Opinion,"00:10:43,760","00:10:47,519",144,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=643,I would suggest you to look
cs-410_12_2_145,cs-410,12,2,Opinion,"00:10:48,550","00:10:51,820",145,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=648,And during this short period
cs-410_12_2_146,cs-410,12,2,Opinion,"00:10:51,820","00:10:57,830",146,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=651,"we could only touch the basic concepts,"
cs-410_12_2_147,cs-410,12,2,Opinion,"00:10:57,830","00:11:03,390",147,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=657,we emphasize the coverage
cs-410_12_2_148,cs-410,12,2,Opinion,"00:11:03,390","00:11:09,128",148,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=663,And this is after the cost
cs-410_12_2_149,cs-410,12,2,Opinion,"00:11:09,128","00:11:15,062",149,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=669,in many cases we omit the discussion
cs-410_12_2_150,cs-410,12,2,Opinion,"00:11:15,062","00:11:19,240",150,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=675,So to learn more about the subject
cs-410_12_2_151,cs-410,12,2,Opinion,"00:11:19,240","00:11:22,120",151,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=679,about the natural language process
cs-410_12_2_152,cs-410,12,2,Opinion,"00:11:22,120","00:11:24,200",152,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=682,all text based applications.
cs-410_12_2_153,cs-410,12,2,Opinion,"00:11:24,200","00:11:28,790",153,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=684,"The more NLP you can do, the better"
cs-410_12_2_154,cs-410,12,2,Opinion,"00:11:28,790","00:11:32,520",154,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=688,then the deeper knowledge
cs-410_12_2_155,cs-410,12,2,Opinion,"00:11:32,520","00:11:34,010",155,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=692,So this is very important.
cs-410_12_2_156,cs-410,12,2,Opinion,"00:11:37,010","00:11:39,910",156,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=697,The second area you should look into
cs-410_12_2_157,cs-410,12,2,Opinion,"00:11:41,120","00:11:45,090",157,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=701,And these techniques are now
cs-410_12_2_158,cs-410,12,2,Opinion,"00:11:46,160","00:11:49,970",158,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=706,not just text analysis applications but
cs-410_12_2_159,cs-410,12,2,Opinion,"00:11:49,970","00:11:55,310",159,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=709,A lot of NLP techniques are nowadays
cs-410_12_2_160,cs-410,12,2,Opinion,"00:11:56,900","00:12:00,790",160,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=716,"So, they are very important"
cs-410_12_2_161,cs-410,12,2,Opinion,"00:12:00,790","00:12:04,570",161,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=720,to also understanding some
cs-410_12_2_162,cs-410,12,2,Opinion,"00:12:04,570","00:12:08,220",162,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=724,naturally they will provide more tools for
cs-410_12_2_163,cs-410,12,2,Opinion,"00:12:09,770","00:12:13,930",163,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=729,"Now, a particularly interesting area,"
cs-410_12_2_164,cs-410,12,2,Opinion,"00:12:13,930","00:12:17,640",164,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=733,called deep learning has attracted
cs-410_12_2_165,cs-410,12,2,Opinion,"00:12:17,640","00:12:21,110",165,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=737,It has also shown promise
cs-410_12_2_166,cs-410,12,2,Opinion,"00:12:21,110","00:12:26,660",166,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=741,"especially in speech and vision, and"
cs-410_12_2_167,cs-410,12,2,Opinion,"00:12:26,660","00:12:30,820",167,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=746,"So, for example, recently there has"
cs-410_12_2_168,cs-410,12,2,Opinion,"00:12:30,820","00:12:34,330",168,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=750,segment analysis to
cs-410_12_2_169,cs-410,12,2,Opinion,"00:12:34,330","00:12:38,320",169,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=754,So that's one example of [INAUDIBLE]
cs-410_12_2_170,cs-410,12,2,Opinion,"00:12:38,320","00:12:40,050",170,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=758,but that's also very important.
cs-410_12_2_171,cs-410,12,2,Opinion,"00:12:41,390","00:12:45,400",171,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=761,And the other area that has emerged
cs-410_12_2_172,cs-410,12,2,Opinion,"00:12:45,400","00:12:50,720",172,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=765,"baring technique, where they can"
cs-410_12_2_173,cs-410,12,2,Opinion,"00:12:50,720","00:12:55,210",173,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=770,And then these better recognitions will
cs-410_12_2_174,cs-410,12,2,Opinion,"00:12:55,210","00:12:55,820",174,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=775,"As you can see,"
cs-410_12_2_175,cs-410,12,2,Opinion,"00:12:55,820","00:13:01,230",175,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=775,this provides directly a way to discover
cs-410_12_2_176,cs-410,12,2,Opinion,"00:13:01,230","00:13:06,600",176,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=781,"And results that people have got,"
cs-410_12_2_177,cs-410,12,2,Opinion,"00:13:06,600","00:13:10,360",177,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=786,That's another promising technique
cs-410_12_2_178,cs-410,12,2,Opinion,"00:13:12,510","00:13:16,290",178,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=792,"but, of course,"
cs-410_12_2_179,cs-410,12,2,Opinion,"00:13:16,290","00:13:20,970",179,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=796,would lead to practical useful techniques
cs-410_12_2_180,cs-410,12,2,Opinion,"00:13:20,970","00:13:25,172",180,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=800,technologies is still an open
cs-410_12_2_181,cs-410,12,2,Opinion,"00:13:25,172","00:13:28,000",181,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=805,And no serious evaluation
cs-410_12_2_182,cs-410,12,2,Opinion,"00:13:28,000","00:13:32,310",182,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=808,"In, for example, examining"
cs-410_12_2_183,cs-410,12,2,Opinion,"00:13:32,310","00:13:34,990",183,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=812,other than word similarity and
cs-410_12_2_184,cs-410,12,2,Opinion,"00:13:36,710","00:13:39,650",184,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=816,"But nevertheless,"
cs-410_12_2_185,cs-410,12,2,Opinion,"00:13:39,650","00:13:43,520",185,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=819,that surely will make impact
cs-410_12_2_186,cs-410,12,2,Opinion,"00:13:43,520","00:13:46,860",186,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=823,So its very important to
cs-410_12_2_187,cs-410,12,2,Opinion,"00:13:46,860","00:13:50,780",187,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=826,Statistical learning is also the key to
cs-410_12_2_188,cs-410,12,2,Opinion,"00:13:50,780","00:13:55,180",188,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=830,for many big data applications and we did
cs-410_12_2_189,cs-410,12,2,Opinion,"00:13:55,180","00:13:59,994",189,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=835,component but this is mostly about
cs-410_12_2_190,cs-410,12,2,Opinion,"00:13:59,994","00:14:05,050",190,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=839,techniques and this is another reason
cs-410_12_2_191,cs-410,12,2,Opinion,"00:14:07,350","00:14:11,730",191,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=847,We also suggest that you learn more about
cs-410_12_2_192,cs-410,12,2,Opinion,"00:14:11,730","00:14:16,610",192,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=851,general data mining algorithms can always
cs-410_12_2_193,cs-410,12,2,Opinion,"00:14:16,610","00:14:21,660",193,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=856,regarded as as special
cs-410_12_2_194,cs-410,12,2,Opinion,"00:14:23,520","00:14:26,030",194,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=863,So there are many applications
cs-410_12_2_195,cs-410,12,2,Opinion,"00:14:26,030","00:14:30,510",195,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=866,"In particular for example, pattern"
cs-410_12_2_196,cs-410,12,2,Opinion,"00:14:30,510","00:14:35,860",196,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=870,the interesting features for test analysis
cs-410_12_2_197,cs-410,12,2,Opinion,"00:14:35,860","00:14:40,940",197,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=875,that mining techniques can also be used
cs-410_12_2_198,cs-410,12,2,Opinion,"00:14:42,360","00:14:44,980",198,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=882,So these are all good to know.
cs-410_12_2_199,cs-410,12,2,Opinion,"00:14:44,980","00:14:49,050",199,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=884,In order to develop effective
cs-410_12_2_200,cs-410,12,2,Opinion,"00:14:49,050","00:14:52,860",200,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=889,"And finally, we also recommend you to"
cs-410_12_2_201,cs-410,12,2,Opinion,"00:14:52,860","00:14:55,930",201,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=892,"information retrieval, of search engines."
cs-410_12_2_202,cs-410,12,2,Opinion,"00:14:55,930","00:15:00,403",202,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=895,This is especially important if you
cs-410_12_2_203,cs-410,12,2,Opinion,"00:15:00,403","00:15:02,750",203,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=900,application systems.
cs-410_12_2_204,cs-410,12,2,Opinion,"00:15:02,750","00:15:05,950",204,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=902,And a search ending would
cs-410_12_2_205,cs-410,12,2,Opinion,"00:15:05,950","00:15:08,632",205,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=905,component in any text-based applications.
cs-410_12_2_206,cs-410,12,2,Opinion,"00:15:08,632","00:15:13,910",206,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=908,And that's because texts data
cs-410_12_2_207,cs-410,12,2,Opinion,"00:15:13,910","00:15:19,330",207,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=913,So humans are at the best position
cs-410_12_2_208,cs-410,12,2,Opinion,"00:15:19,330","00:15:24,910",208,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=919,it's important to have human in the loop
cs-410_12_2_209,cs-410,12,2,Opinion,"00:15:24,910","00:15:29,870",209,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=924,it can in particular help text
cs-410_12_2_210,cs-410,12,2,Opinion,"00:15:29,870","00:15:35,099",210,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=929,One is through effectively reduce
cs-410_12_2_211,cs-410,12,2,Opinion,"00:15:35,099","00:15:40,158",211,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=935,a small collection with the most
cs-410_12_2_212,cs-410,12,2,Opinion,"00:15:40,158","00:15:42,627",212,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=940,the particular interpretation.
cs-410_12_2_213,cs-410,12,2,Opinion,"00:15:42,627","00:15:47,901",213,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=942,So the other is to provide a way to
cs-410_12_2_214,cs-410,12,2,Opinion,"00:15:47,901","00:15:51,521",214,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=947,and this has to do with
cs-410_12_2_215,cs-410,12,2,Opinion,"00:15:51,521","00:15:54,853",215,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=951,"Once we discover some knowledge,"
cs-410_12_2_216,cs-410,12,2,Opinion,"00:15:54,853","00:15:57,370",216,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=954,not the discovery is really reliable.
cs-410_12_2_217,cs-410,12,2,Opinion,"00:15:57,370","00:16:00,000",217,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=957,So we need to go back to
cs-410_12_2_218,cs-410,12,2,Opinion,"00:16:00,000","00:16:02,380",218,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=960,And that is why the search
cs-410_12_2_219,cs-410,12,2,Opinion,"00:16:04,070","00:16:08,040",219,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=964,"Moreover, some techniques"
cs-410_12_2_220,cs-410,12,2,Opinion,"00:16:08,040","00:16:13,380",220,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=968,"for example BM25, vector space and"
cs-410_12_2_221,cs-410,12,2,Opinion,"00:16:13,380","00:16:16,400",221,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=973,"We only mention some of them,"
cs-410_12_2_222,cs-410,12,2,Opinion,"00:16:16,400","00:16:20,500",222,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=976,text retrieval you'll see that there
cs-410_12_2_223,cs-410,12,2,Opinion,"00:16:20,500","00:16:25,030",223,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=980,Another technique that it's used for
cs-410_12_2_224,cs-410,12,2,Opinion,"00:16:25,030","00:16:28,450",224,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=985,response of search engine to a user's
cs-410_12_2_225,cs-410,12,2,Opinion,"00:16:28,450","00:16:32,150",225,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=988,very useful for building efficient
cs-410_12_2_226,cs-410,12,2,Opinion,"00:16:35,160","00:16:39,830",226,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=995,"So, finally, I want to remind"
cs-410_12_2_227,cs-410,12,2,Opinion,"00:16:39,830","00:16:43,900",227,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=999,harnessing big text data that I showed
cs-410_12_2_228,cs-410,12,2,Opinion,"00:16:45,350","00:16:48,970",228,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1005,"So in general, to deal with"
cs-410_12_2_229,cs-410,12,2,Opinion,"00:16:48,970","00:16:51,760",229,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1008,"we need two kinds text,"
cs-410_12_2_230,cs-410,12,2,Opinion,"00:16:53,380","00:16:58,040",230,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1013,"And text retrieval, as I explained,"
cs-410_12_2_231,cs-410,12,2,Opinion,"00:16:58,040","00:17:02,930",231,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1018,a small amount of most relevant data for
cs-410_12_2_232,cs-410,12,2,Opinion,"00:17:02,930","00:17:07,240",232,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1022,"providing knowledge provenance,"
cs-410_12_2_233,cs-410,12,2,Opinion,"00:17:07,240","00:17:12,060",233,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1027,Text mining has to do with further
cs-410_12_2_234,cs-410,12,2,Opinion,"00:17:12,060","00:17:16,460",234,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1032,the actionable knowledge that can be
cs-410_12_2_235,cs-410,12,2,Opinion,"00:17:16,460","00:17:18,510",235,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1036,many other tasks.
cs-410_12_2_236,cs-410,12,2,Opinion,"00:17:18,510","00:17:20,530",236,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1038,So this course covers text mining.
cs-410_12_2_237,cs-410,12,2,Opinion,"00:17:20,530","00:17:24,020",237,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1040,And there's a companion course
cs-410_12_2_238,cs-410,12,2,Opinion,"00:17:24,020","00:17:27,130",238,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1044,Search Engines that covers text retrieval.
cs-410_12_2_239,cs-410,12,2,Opinion,"00:17:27,130","00:17:32,040",239,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1047,"If you haven't taken that course,"
cs-410_12_2_240,cs-410,12,2,Opinion,"00:17:32,040","00:17:37,490",240,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1052,especially if you are interested
cs-410_12_2_241,cs-410,12,2,Opinion,"00:17:37,490","00:17:42,138",241,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1057,And taking both courses will give you
cs-410_12_2_242,cs-410,12,2,Opinion,"00:17:42,138","00:17:43,708",242,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1062,building such a system.
cs-410_12_2_243,cs-410,12,2,Opinion,"00:17:43,708","00:17:49,250",243,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1063,So in [INAUDIBLE]
cs-410_12_2_244,cs-410,12,2,Opinion,"00:17:49,250","00:17:51,050",244,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1069,taking this course.
cs-410_12_2_245,cs-410,12,2,Opinion,"00:17:51,050","00:17:57,915",245,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1071,I hope you have learned useful knowledge
cs-410_12_2_246,cs-410,12,2,Opinion,"00:17:57,915","00:18:02,185",246,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1077,As you see from our discussions
cs-410_12_2_247,cs-410,12,2,Opinion,"00:18:02,185","00:18:06,235",247,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1082,this kind of techniques and
cs-410_12_2_248,cs-410,12,2,Opinion,"00:18:06,235","00:18:10,910",248,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1086,So I hope you can use what you have
cs-410_12_2_249,cs-410,12,2,Opinion,"00:18:10,910","00:18:15,550",249,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1090,applications will benefit society and
cs-410_12_2_250,cs-410,12,2,Opinion,"00:18:15,550","00:18:20,759",250,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1095,the research community to discover new
cs-410_12_2_251,cs-410,12,2,Opinion,"00:18:20,759","00:18:21,259",251,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1100,Thank you.
cs-410_12_2_252,cs-410,12,2,Opinion,"00:18:21,259","00:18:31,259",252,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1101,[MUSIC]
