key,course_id,week_nbr,video_id,video_title,timeline_start,timeline_end,segment_nbr,segment_link,segment_txt,pic_name
cs-410_1_1_1,cs-410,1,1, Natural Language Content Analysis,"00:00:00,008","00:00:04,018",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND],pic_cs-410_1_1_0.jpg
cs-410_1_1_2,cs-410,1,1, Natural Language Content Analysis,"00:00:09,625","00:00:12,226",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,>> This lecture is about Natural Language,pic_cs-410_1_1_0.jpg
cs-410_1_1_3,cs-410,1,1, Natural Language Content Analysis,"00:00:12,226","00:00:13,732",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,of Content Analysis.,pic_cs-410_1_1_0.jpg
cs-410_1_1_4,cs-410,1,1, Natural Language Content Analysis,"00:00:13,732","00:00:15,569",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"As you see from this picture,",pic_cs-410_1_1_0.jpg
cs-410_1_1_5,cs-410,1,1, Natural Language Content Analysis,"00:00:15,569","00:00:19,540",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,this is really the first step,pic_cs-410_1_1_0.jpg
cs-410_1_1_6,cs-410,1,1, Natural Language Content Analysis,"00:00:19,540","00:00:22,060",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,Text data are in natural languages.,pic_cs-410_1_1_0.jpg
cs-410_1_1_7,cs-410,1,1, Natural Language Content Analysis,"00:00:22,060","00:00:26,820",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,So computers have to understand,pic_cs-410_1_1_0.jpg
cs-410_1_1_8,cs-410,1,1, Natural Language Content Analysis,"00:00:26,820","00:00:29,380",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,in order to make use of the data.,pic_cs-410_1_1_0.jpg
cs-410_1_1_9,cs-410,1,1, Natural Language Content Analysis,"00:00:29,380","00:00:32,000",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,So that's the topic of this lecture.,pic_cs-410_1_1_0.jpg
cs-410_1_1_10,cs-410,1,1, Natural Language Content Analysis,"00:00:32,000","00:00:33,910",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,We're going to cover three things.,pic_cs-410_1_1_0.jpg
cs-410_1_1_11,cs-410,1,1, Natural Language Content Analysis,"00:00:33,910","00:00:36,430",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"First, what is natural",pic_cs-410_1_1_0.jpg
cs-410_1_1_12,cs-410,1,1, Natural Language Content Analysis,"00:00:36,430","00:00:41,740",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,which is the main technique for processing,pic_cs-410_1_1_0.jpg
cs-410_1_1_13,cs-410,1,1, Natural Language Content Analysis,"00:00:43,150","00:00:46,420",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,The second is the state of,pic_cs-410_1_1_0.jpg
cs-410_1_1_14,cs-410,1,1, Natural Language Content Analysis,"00:00:46,420","00:00:48,350",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,natural language processing.,pic_cs-410_1_1_0.jpg
cs-410_1_1_15,cs-410,1,1, Natural Language Content Analysis,"00:00:49,540","00:00:53,430",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,Finally we're going to cover the relation,pic_cs-410_1_1_0.jpg
cs-410_1_1_16,cs-410,1,1, Natural Language Content Analysis,"00:00:53,430","00:00:54,900",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,text retrieval.,pic_cs-410_1_1_0.jpg
cs-410_1_1_17,cs-410,1,1, Natural Language Content Analysis,"00:00:54,900","00:00:57,280",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,"First, what is NLP?",pic_cs-410_1_1_0.jpg
cs-410_1_1_18,cs-410,1,1, Natural Language Content Analysis,"00:00:57,280","00:01:02,240",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,Well the best way to explain it,pic_cs-410_1_1_0.jpg
cs-410_1_1_19,cs-410,1,1, Natural Language Content Analysis,"00:01:02,240","00:01:05,860",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,a text in a foreign language,pic_cs-410_1_1_60.jpg
cs-410_1_1_20,cs-410,1,1, Natural Language Content Analysis,"00:01:06,980","00:01:10,907",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,Now what do you have to do in,pic_cs-410_1_1_60.jpg
cs-410_1_1_21,cs-410,1,1, Natural Language Content Analysis,"00:01:10,907","00:01:13,172",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,This is basically what,pic_cs-410_1_1_60.jpg
cs-410_1_1_22,cs-410,1,1, Natural Language Content Analysis,"00:01:13,172","00:01:17,580",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,So looking at the simple sentence like,pic_cs-410_1_1_60.jpg
cs-410_1_1_23,cs-410,1,1, Natural Language Content Analysis,"00:01:18,730","00:01:22,250",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,We don't have any problems,pic_cs-410_1_1_60.jpg
cs-410_1_1_24,cs-410,1,1, Natural Language Content Analysis,"00:01:22,250","00:01:25,930",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,But imagine what the computer would,pic_cs-410_1_1_60.jpg
cs-410_1_1_25,cs-410,1,1, Natural Language Content Analysis,"00:01:25,930","00:01:27,830",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,"Well in general,",pic_cs-410_1_1_60.jpg
cs-410_1_1_26,cs-410,1,1, Natural Language Content Analysis,"00:01:27,830","00:01:34,310",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"First, it would have to know dog",pic_cs-410_1_1_60.jpg
cs-410_1_1_27,cs-410,1,1, Natural Language Content Analysis,"00:01:34,310","00:01:38,410",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"So this is called lexical analysis,",pic_cs-410_1_1_60.jpg
cs-410_1_1_28,cs-410,1,1, Natural Language Content Analysis,"00:01:38,410","00:01:42,230",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,we need to figure out the syntactic,pic_cs-410_1_1_60.jpg
cs-410_1_1_29,cs-410,1,1, Natural Language Content Analysis,"00:01:42,230","00:01:43,930",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,So that's the first step.,pic_cs-410_1_1_60.jpg
cs-410_1_1_30,cs-410,1,1, Natural Language Content Analysis,"00:01:43,930","00:01:48,060",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"After that, we're going to figure",pic_cs-410_1_1_60.jpg
cs-410_1_1_31,cs-410,1,1, Natural Language Content Analysis,"00:01:48,060","00:01:50,370",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,"So for example, here it shows that A and",pic_cs-410_1_1_60.jpg
cs-410_1_1_32,cs-410,1,1, Natural Language Content Analysis,"00:01:50,370","00:01:54,260",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,the dog would go together,pic_cs-410_1_1_60.jpg
cs-410_1_1_33,cs-410,1,1, Natural Language Content Analysis,"00:01:55,730","00:01:59,500",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,And we won't have dog and is to go first.,pic_cs-410_1_1_60.jpg
cs-410_1_1_34,cs-410,1,1, Natural Language Content Analysis,"00:01:59,500","00:02:02,969",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,And there are some structures,pic_cs-410_1_1_60.jpg
cs-410_1_1_35,cs-410,1,1, Natural Language Content Analysis,"00:02:04,470","00:02:09,650",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,But this structure shows what we might,pic_cs-410_1_1_120.jpg
cs-410_1_1_36,cs-410,1,1, Natural Language Content Analysis,"00:02:09,650","00:02:11,850",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,try to interpret the sentence.,pic_cs-410_1_1_120.jpg
cs-410_1_1_37,cs-410,1,1, Natural Language Content Analysis,"00:02:11,850","00:02:13,960",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,"Some words would go together first, and",pic_cs-410_1_1_120.jpg
cs-410_1_1_38,cs-410,1,1, Natural Language Content Analysis,"00:02:13,960","00:02:15,640",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,then they will go together,pic_cs-410_1_1_120.jpg
cs-410_1_1_39,cs-410,1,1, Natural Language Content Analysis,"00:02:16,640","00:02:20,200",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,So here we show we have noun phrases,pic_cs-410_1_1_120.jpg
cs-410_1_1_40,cs-410,1,1, Natural Language Content Analysis,"00:02:20,200","00:02:21,500",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,then verbal phrases.,pic_cs-410_1_1_120.jpg
cs-410_1_1_41,cs-410,1,1, Natural Language Content Analysis,"00:02:21,500","00:02:23,670",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,Finally we have a sentence.,pic_cs-410_1_1_120.jpg
cs-410_1_1_42,cs-410,1,1, Natural Language Content Analysis,"00:02:23,670","00:02:25,430",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,And you get this structure.,pic_cs-410_1_1_120.jpg
cs-410_1_1_43,cs-410,1,1, Natural Language Content Analysis,"00:02:25,430","00:02:29,400",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,We need to do something called,pic_cs-410_1_1_120.jpg
cs-410_1_1_44,cs-410,1,1, Natural Language Content Analysis,"00:02:29,400","00:02:31,610",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,And we may have a parser,pic_cs-410_1_1_120.jpg
cs-410_1_1_45,cs-410,1,1, Natural Language Content Analysis,"00:02:31,610","00:02:34,880",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,that would automatically,pic_cs-410_1_1_120.jpg
cs-410_1_1_46,cs-410,1,1, Natural Language Content Analysis,"00:02:34,880","00:02:38,220",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,At this point you would know,pic_cs-410_1_1_120.jpg
cs-410_1_1_47,cs-410,1,1, Natural Language Content Analysis,"00:02:38,220","00:02:40,440",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,still you don't know,pic_cs-410_1_1_120.jpg
cs-410_1_1_48,cs-410,1,1, Natural Language Content Analysis,"00:02:40,440","00:02:44,060",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,So we have to go further,pic_cs-410_1_1_120.jpg
cs-410_1_1_49,cs-410,1,1, Natural Language Content Analysis,"00:02:44,060","00:02:47,120",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,In our mind we usually can map,pic_cs-410_1_1_120.jpg
cs-410_1_1_50,cs-410,1,1, Natural Language Content Analysis,"00:02:47,120","00:02:51,330",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,such a sentence to what we already,pic_cs-410_1_1_120.jpg
cs-410_1_1_51,cs-410,1,1, Natural Language Content Analysis,"00:02:51,330","00:02:53,970",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,"For example, you might imagine",pic_cs-410_1_1_120.jpg
cs-410_1_1_52,cs-410,1,1, Natural Language Content Analysis,"00:02:53,970","00:02:56,800",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,There's a boy and,pic_cs-410_1_1_120.jpg
cs-410_1_1_53,cs-410,1,1, Natural Language Content Analysis,"00:02:56,800","00:02:59,860",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,But for a computer would have,pic_cs-410_1_1_120.jpg
cs-410_1_1_54,cs-410,1,1, Natural Language Content Analysis,"00:03:00,890","00:03:05,232",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,We'd use a symbol (d1) to denote a dog.,pic_cs-410_1_1_180.jpg
cs-410_1_1_55,cs-410,1,1, Natural Language Content Analysis,"00:03:05,232","00:03:10,430",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,And (b)1 can denote a boy and,pic_cs-410_1_1_180.jpg
cs-410_1_1_56,cs-410,1,1, Natural Language Content Analysis,"00:03:12,650","00:03:15,440",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,Now there is also a chasing,pic_cs-410_1_1_180.jpg
cs-410_1_1_57,cs-410,1,1, Natural Language Content Analysis,"00:03:15,440","00:03:19,130",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,we have a relationship chasing,pic_cs-410_1_1_180.jpg
cs-410_1_1_58,cs-410,1,1, Natural Language Content Analysis,"00:03:19,130","00:03:23,909",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,So this is how a computer would obtain,pic_cs-410_1_1_180.jpg
cs-410_1_1_59,cs-410,1,1, Natural Language Content Analysis,"00:03:25,920","00:03:31,590",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,Now from this representation we could,pic_cs-410_1_1_180.jpg
cs-410_1_1_60,cs-410,1,1, Natural Language Content Analysis,"00:03:31,590","00:03:35,960",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,and we might indeed naturally think of,pic_cs-410_1_1_180.jpg
cs-410_1_1_61,cs-410,1,1, Natural Language Content Analysis,"00:03:35,960","00:03:37,470",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,this is called inference.,pic_cs-410_1_1_180.jpg
cs-410_1_1_62,cs-410,1,1, Natural Language Content Analysis,"00:03:37,470","00:03:42,490",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"So for example, if you believe",pic_cs-410_1_1_180.jpg
cs-410_1_1_63,cs-410,1,1, Natural Language Content Analysis,"00:03:42,490","00:03:46,180",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,"this person might be scared,",pic_cs-410_1_1_180.jpg
cs-410_1_1_64,cs-410,1,1, Natural Language Content Analysis,"00:03:46,180","00:03:50,880",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,you can see computers could also,pic_cs-410_1_1_180.jpg
cs-410_1_1_65,cs-410,1,1, Natural Language Content Analysis,"00:03:50,880","00:03:54,080",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,So this is some extra knowledge,pic_cs-410_1_1_180.jpg
cs-410_1_1_66,cs-410,1,1, Natural Language Content Analysis,"00:03:54,080","00:03:56,430",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,some understanding of the text.,pic_cs-410_1_1_180.jpg
cs-410_1_1_67,cs-410,1,1, Natural Language Content Analysis,"00:03:56,430","00:04:02,280",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,You can even go further to understand,pic_cs-410_1_1_180.jpg
cs-410_1_1_68,cs-410,1,1, Natural Language Content Analysis,"00:04:02,280","00:04:05,000",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,So this has to do as a use of language.,pic_cs-410_1_1_240.jpg
cs-410_1_1_69,cs-410,1,1, Natural Language Content Analysis,"00:04:05,000","00:04:08,740",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,This is called pragmatic analysis.,pic_cs-410_1_1_240.jpg
cs-410_1_1_70,cs-410,1,1, Natural Language Content Analysis,"00:04:08,740","00:04:13,910",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,In order to understand the speak,pic_cs-410_1_1_240.jpg
cs-410_1_1_71,cs-410,1,1, Natural Language Content Analysis,"00:04:13,910","00:04:18,370",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,We say something to,pic_cs-410_1_1_240.jpg
cs-410_1_1_72,cs-410,1,1, Natural Language Content Analysis,"00:04:18,370","00:04:19,440",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,There's some purpose there.,pic_cs-410_1_1_240.jpg
cs-410_1_1_73,cs-410,1,1, Natural Language Content Analysis,"00:04:19,440","00:04:22,100",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,And this has to do with,pic_cs-410_1_1_240.jpg
cs-410_1_1_74,cs-410,1,1, Natural Language Content Analysis,"00:04:22,100","00:04:24,750",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,In this case the person who said,pic_cs-410_1_1_240.jpg
cs-410_1_1_75,cs-410,1,1, Natural Language Content Analysis,"00:04:24,750","00:04:29,200",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,this sentence might be reminding,pic_cs-410_1_1_240.jpg
cs-410_1_1_76,cs-410,1,1, Natural Language Content Analysis,"00:04:29,200","00:04:31,410",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,That could be one possible intent.,pic_cs-410_1_1_240.jpg
cs-410_1_1_77,cs-410,1,1, Natural Language Content Analysis,"00:04:33,020","00:04:36,500",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,To reach this level of,pic_cs-410_1_1_240.jpg
cs-410_1_1_78,cs-410,1,1, Natural Language Content Analysis,"00:04:36,500","00:04:41,390",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,all of these steps and,pic_cs-410_1_1_240.jpg
cs-410_1_1_79,cs-410,1,1, Natural Language Content Analysis,"00:04:41,390","00:04:46,940",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,these steps in order to completely,pic_cs-410_1_1_240.jpg
cs-410_1_1_80,cs-410,1,1, Natural Language Content Analysis,"00:04:46,940","00:04:49,560",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,Yet we humans have no trouble,pic_cs-410_1_1_240.jpg
cs-410_1_1_81,cs-410,1,1, Natural Language Content Analysis,"00:04:49,560","00:04:51,430",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,we instantly would get everything.,pic_cs-410_1_1_240.jpg
cs-410_1_1_82,cs-410,1,1, Natural Language Content Analysis,"00:04:52,790","00:04:53,760",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,There is a reason for that.,pic_cs-410_1_1_240.jpg
cs-410_1_1_83,cs-410,1,1, Natural Language Content Analysis,"00:04:53,760","00:04:57,430",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,That's because we have a large,pic_cs-410_1_1_240.jpg
cs-410_1_1_84,cs-410,1,1, Natural Language Content Analysis,"00:04:57,430","00:05:01,890",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,we can use common sense knowledge,pic_cs-410_1_1_240.jpg
cs-410_1_1_85,cs-410,1,1, Natural Language Content Analysis,"00:05:01,890","00:05:06,330",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,Computers unfortunately are hard,pic_cs-410_1_1_300.jpg
cs-410_1_1_86,cs-410,1,1, Natural Language Content Analysis,"00:05:06,330","00:05:08,430",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,They don't have such a knowledge base.,pic_cs-410_1_1_300.jpg
cs-410_1_1_87,cs-410,1,1, Natural Language Content Analysis,"00:05:08,430","00:05:12,520",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,They are still incapable of doing,pic_cs-410_1_1_300.jpg
cs-410_1_1_88,cs-410,1,1, Natural Language Content Analysis,"00:05:14,290","00:05:18,430",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,so that makes natural language,pic_cs-410_1_1_300.jpg
cs-410_1_1_89,cs-410,1,1, Natural Language Content Analysis,"00:05:18,430","00:05:21,540",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,But the fundamental reason why natural,pic_cs-410_1_1_300.jpg
cs-410_1_1_90,cs-410,1,1, Natural Language Content Analysis,"00:05:21,540","00:05:25,430",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,computers is simply because natural,pic_cs-410_1_1_300.jpg
cs-410_1_1_91,cs-410,1,1, Natural Language Content Analysis,"00:05:25,430","00:05:26,430",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,computers.,pic_cs-410_1_1_300.jpg
cs-410_1_1_92,cs-410,1,1, Natural Language Content Analysis,"00:05:26,430","00:05:30,960",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,Natural languages are designed for,pic_cs-410_1_1_300.jpg
cs-410_1_1_93,cs-410,1,1, Natural Language Content Analysis,"00:05:30,960","00:05:33,480",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,There are other languages designed for,pic_cs-410_1_1_300.jpg
cs-410_1_1_94,cs-410,1,1, Natural Language Content Analysis,"00:05:33,480","00:05:36,220",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,"For example, programming languages.",pic_cs-410_1_1_300.jpg
cs-410_1_1_95,cs-410,1,1, Natural Language Content Analysis,"00:05:36,220","00:05:38,780",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,"Those are harder for us, right?",pic_cs-410_1_1_300.jpg
cs-410_1_1_96,cs-410,1,1, Natural Language Content Analysis,"00:05:38,780","00:05:43,690",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,So natural languages is designed to,pic_cs-410_1_1_300.jpg
cs-410_1_1_97,cs-410,1,1, Natural Language Content Analysis,"00:05:43,690","00:05:46,770",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"As a result,",pic_cs-410_1_1_300.jpg
cs-410_1_1_98,cs-410,1,1, Natural Language Content Analysis,"00:05:46,770","00:05:49,540",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,because we assume everyone,pic_cs-410_1_1_300.jpg
cs-410_1_1_99,cs-410,1,1, Natural Language Content Analysis,"00:05:49,540","00:05:56,250",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,We also keep a lot of ambiguities because,pic_cs-410_1_1_300.jpg
cs-410_1_1_100,cs-410,1,1, Natural Language Content Analysis,"00:05:56,250","00:06:02,020",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,know how to decipher an ambiguous word,pic_cs-410_1_1_300.jpg
cs-410_1_1_101,cs-410,1,1, Natural Language Content Analysis,"00:06:02,020","00:06:05,320",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,There's no need to demand different,pic_cs-410_1_1_360.jpg
cs-410_1_1_102,cs-410,1,1, Natural Language Content Analysis,"00:06:05,320","00:06:08,820",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,We could overload the same word with,pic_cs-410_1_1_360.jpg
cs-410_1_1_103,cs-410,1,1, Natural Language Content Analysis,"00:06:10,460","00:06:14,350",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,Because of these reasons this makes every,pic_cs-410_1_1_360.jpg
cs-410_1_1_104,cs-410,1,1, Natural Language Content Analysis,"00:06:14,350","00:06:17,520",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"difficult for computers,",pic_cs-410_1_1_360.jpg
cs-410_1_1_105,cs-410,1,1, Natural Language Content Analysis,"00:06:18,780","00:06:22,060",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,And common sense and reasoning is,pic_cs-410_1_1_360.jpg
cs-410_1_1_106,cs-410,1,1, Natural Language Content Analysis,"00:06:23,800","00:06:26,300",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,So let me give you some,pic_cs-410_1_1_360.jpg
cs-410_1_1_107,cs-410,1,1, Natural Language Content Analysis,"00:06:27,505","00:06:29,350",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,Consider the word level ambiguity.,pic_cs-410_1_1_360.jpg
cs-410_1_1_108,cs-410,1,1, Natural Language Content Analysis,"00:06:30,730","00:06:34,510",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,The same word can have,pic_cs-410_1_1_360.jpg
cs-410_1_1_109,cs-410,1,1, Natural Language Content Analysis,"00:06:34,510","00:06:36,780",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,For example design can be a noun or,pic_cs-410_1_1_360.jpg
cs-410_1_1_110,cs-410,1,1, Natural Language Content Analysis,"00:06:39,270","00:06:42,160",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,The word of root may,pic_cs-410_1_1_360.jpg
cs-410_1_1_111,cs-410,1,1, Natural Language Content Analysis,"00:06:42,160","00:06:45,120",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So square root in math sense or,pic_cs-410_1_1_360.jpg
cs-410_1_1_112,cs-410,1,1, Natural Language Content Analysis,"00:06:46,450","00:06:49,464",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,You might be able to think,pic_cs-410_1_1_360.jpg
cs-410_1_1_113,cs-410,1,1, Natural Language Content Analysis,"00:06:49,464","00:06:52,609",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,There are also syntactical ambiguities.,pic_cs-410_1_1_360.jpg
cs-410_1_1_114,cs-410,1,1, Natural Language Content Analysis,"00:06:52,609","00:06:56,932",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"For example, the main topic of this",pic_cs-410_1_1_360.jpg
cs-410_1_1_115,cs-410,1,1, Natural Language Content Analysis,"00:06:56,932","00:07:01,480",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,can actually be interpreted in two,pic_cs-410_1_1_360.jpg
cs-410_1_1_116,cs-410,1,1, Natural Language Content Analysis,"00:07:01,480","00:07:03,900",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,Think for a moment and,pic_cs-410_1_1_420.jpg
cs-410_1_1_117,cs-410,1,1, Natural Language Content Analysis,"00:07:03,900","00:07:09,560",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,We usually think of this as,pic_cs-410_1_1_420.jpg
cs-410_1_1_118,cs-410,1,1, Natural Language Content Analysis,"00:07:09,560","00:07:13,991",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,but you could also think of this as do,pic_cs-410_1_1_420.jpg
cs-410_1_1_119,cs-410,1,1, Natural Language Content Analysis,"00:07:16,130","00:07:20,440",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,So this is an example,pic_cs-410_1_1_420.jpg
cs-410_1_1_120,cs-410,1,1, Natural Language Content Analysis,"00:07:20,440","00:07:23,190",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,What we have different is,pic_cs-410_1_1_420.jpg
cs-410_1_1_121,cs-410,1,1, Natural Language Content Analysis,"00:07:24,510","00:07:27,480",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,applied to the same sequence of words.,pic_cs-410_1_1_420.jpg
cs-410_1_1_122,cs-410,1,1, Natural Language Content Analysis,"00:07:27,480","00:07:31,730",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,Another common example of an ambiguous,pic_cs-410_1_1_420.jpg
cs-410_1_1_123,cs-410,1,1, Natural Language Content Analysis,"00:07:31,730","00:07:34,480",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,A man saw a boy with a telescope.,pic_cs-410_1_1_420.jpg
cs-410_1_1_124,cs-410,1,1, Natural Language Content Analysis,"00:07:34,480","00:07:37,810",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,"Now in this case the question is,",pic_cs-410_1_1_420.jpg
cs-410_1_1_125,cs-410,1,1, Natural Language Content Analysis,"00:07:38,820","00:07:42,700",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,This is called a prepositional,pic_cs-410_1_1_420.jpg
cs-410_1_1_126,cs-410,1,1, Natural Language Content Analysis,"00:07:42,700","00:07:45,030",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,PP attachment ambiguity.,pic_cs-410_1_1_420.jpg
cs-410_1_1_127,cs-410,1,1, Natural Language Content Analysis,"00:07:45,030","00:07:50,000",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,Now we generally don't have a problem with,pic_cs-410_1_1_420.jpg
cs-410_1_1_128,cs-410,1,1, Natural Language Content Analysis,"00:07:50,000","00:07:54,340",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,background knowledge to help,pic_cs-410_1_1_420.jpg
cs-410_1_1_129,cs-410,1,1, Natural Language Content Analysis,"00:07:55,380","00:07:57,961",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,Another example of difficulty,pic_cs-410_1_1_420.jpg
cs-410_1_1_130,cs-410,1,1, Natural Language Content Analysis,"00:07:57,961","00:08:03,290",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,So think about the sentence John,pic_cs-410_1_1_420.jpg
cs-410_1_1_131,cs-410,1,1, Natural Language Content Analysis,"00:08:03,290","00:08:07,632",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,The question here is does,pic_cs-410_1_1_480.jpg
cs-410_1_1_132,cs-410,1,1, Natural Language Content Analysis,"00:08:07,632","00:08:10,803",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So again this is something that,pic_cs-410_1_1_480.jpg
cs-410_1_1_133,cs-410,1,1, Natural Language Content Analysis,"00:08:10,803","00:08:12,540",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,the context to figure out.,pic_cs-410_1_1_480.jpg
cs-410_1_1_134,cs-410,1,1, Natural Language Content Analysis,"00:08:12,540","00:08:15,470",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,"Finally, presupposition",pic_cs-410_1_1_480.jpg
cs-410_1_1_135,cs-410,1,1, Natural Language Content Analysis,"00:08:15,470","00:08:18,110",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,"Consider the sentence,",pic_cs-410_1_1_480.jpg
cs-410_1_1_136,cs-410,1,1, Natural Language Content Analysis,"00:08:18,110","00:08:20,710",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,Now this obviously implies,pic_cs-410_1_1_480.jpg
cs-410_1_1_137,cs-410,1,1, Natural Language Content Analysis,"00:08:22,430","00:08:27,000",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,So imagine a computer wants to understand,pic_cs-410_1_1_480.jpg
cs-410_1_1_138,cs-410,1,1, Natural Language Content Analysis,"00:08:27,000","00:08:30,750",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,It would have to use a lot of,pic_cs-410_1_1_480.jpg
cs-410_1_1_139,cs-410,1,1, Natural Language Content Analysis,"00:08:30,750","00:08:35,890",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,It also would have to maintain a large,pic_cs-410_1_1_480.jpg
cs-410_1_1_140,cs-410,1,1, Natural Language Content Analysis,"00:08:35,890","00:08:41,940",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,words and how they are connected to our,pic_cs-410_1_1_480.jpg
cs-410_1_1_141,cs-410,1,1, Natural Language Content Analysis,"00:08:41,940","00:08:44,130",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,So this is why it's very difficult.,pic_cs-410_1_1_480.jpg
cs-410_1_1_142,cs-410,1,1, Natural Language Content Analysis,"00:08:45,530","00:08:49,110",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,"So as a result, we are steep not perfect,",pic_cs-410_1_1_480.jpg
cs-410_1_1_143,cs-410,1,1, Natural Language Content Analysis,"00:08:49,110","00:08:54,240",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,in fact far from perfect in understanding,pic_cs-410_1_1_480.jpg
cs-410_1_1_144,cs-410,1,1, Natural Language Content Analysis,"00:08:54,240","00:09:00,200",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,So this slide sort of gains a simplified,pic_cs-410_1_1_480.jpg
cs-410_1_1_145,cs-410,1,1, Natural Language Content Analysis,"00:09:01,580","00:09:06,640",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,We can do part of speech,pic_cs-410_1_1_540.jpg
cs-410_1_1_146,cs-410,1,1, Natural Language Content Analysis,"00:09:06,640","00:09:09,610",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,I showed 97% accuracy here.,pic_cs-410_1_1_540.jpg
cs-410_1_1_147,cs-410,1,1, Natural Language Content Analysis,"00:09:09,610","00:09:13,830",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,Now this number is obviously,pic_cs-410_1_1_540.jpg
cs-410_1_1_148,cs-410,1,1, Natural Language Content Analysis,"00:09:13,830","00:09:15,680",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,don't take this literally.,pic_cs-410_1_1_540.jpg
cs-410_1_1_149,cs-410,1,1, Natural Language Content Analysis,"00:09:15,680","00:09:18,210",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,This just shows that we,pic_cs-410_1_1_540.jpg
cs-410_1_1_150,cs-410,1,1, Natural Language Content Analysis,"00:09:18,210","00:09:20,320",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,But it's still not perfect.,pic_cs-410_1_1_540.jpg
cs-410_1_1_151,cs-410,1,1, Natural Language Content Analysis,"00:09:20,320","00:09:23,620",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"In terms of parsing,",pic_cs-410_1_1_540.jpg
cs-410_1_1_152,cs-410,1,1, Natural Language Content Analysis,"00:09:23,620","00:09:27,800",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,That means we can get noun phrase,pic_cs-410_1_1_540.jpg
cs-410_1_1_153,cs-410,1,1, Natural Language Content Analysis,"00:09:27,800","00:09:31,106",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"or some segment of the sentence, and",pic_cs-410_1_1_540.jpg
cs-410_1_1_154,cs-410,1,1, Natural Language Content Analysis,"00:09:31,106","00:09:33,439",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,this dude correct them in,pic_cs-410_1_1_540.jpg
cs-410_1_1_155,cs-410,1,1, Natural Language Content Analysis,"00:09:34,470","00:09:39,310",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"And in some evaluation results,",pic_cs-410_1_1_540.jpg
cs-410_1_1_156,cs-410,1,1, Natural Language Content Analysis,"00:09:39,310","00:09:43,140",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,accuracy in terms of partial,pic_cs-410_1_1_540.jpg
cs-410_1_1_157,cs-410,1,1, Natural Language Content Analysis,"00:09:43,140","00:09:46,910",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"Again, I have to say these numbers",pic_cs-410_1_1_540.jpg
cs-410_1_1_158,cs-410,1,1, Natural Language Content Analysis,"00:09:46,910","00:09:50,300",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,"In some other datasets,",pic_cs-410_1_1_540.jpg
cs-410_1_1_159,cs-410,1,1, Natural Language Content Analysis,"00:09:50,300","00:09:54,230",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,Most of the existing work has been,pic_cs-410_1_1_540.jpg
cs-410_1_1_160,cs-410,1,1, Natural Language Content Analysis,"00:09:54,230","00:09:59,800",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,And so a lot of these numbers are more or,pic_cs-410_1_1_540.jpg
cs-410_1_1_161,cs-410,1,1, Natural Language Content Analysis,"00:09:59,800","00:10:02,980",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"Think about social media data,",pic_cs-410_1_1_540.jpg
cs-410_1_1_162,cs-410,1,1, Natural Language Content Analysis,"00:10:05,460","00:10:07,860",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,"In terms of a semantical analysis,",pic_cs-410_1_1_600.jpg
cs-410_1_1_163,cs-410,1,1, Natural Language Content Analysis,"00:10:07,860","00:10:13,730",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,we are far from being able to do,pic_cs-410_1_1_600.jpg
cs-410_1_1_164,cs-410,1,1, Natural Language Content Analysis,"00:10:13,730","00:10:16,430",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,But we have some techniques,pic_cs-410_1_1_600.jpg
cs-410_1_1_165,cs-410,1,1, Natural Language Content Analysis,"00:10:16,430","00:10:18,880",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,do partial understanding of the sentence.,pic_cs-410_1_1_600.jpg
cs-410_1_1_166,cs-410,1,1, Natural Language Content Analysis,"00:10:18,880","00:10:22,360",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,So I could mention some of them.,pic_cs-410_1_1_600.jpg
cs-410_1_1_167,cs-410,1,1, Natural Language Content Analysis,"00:10:22,360","00:10:27,190",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,"For example, we have techniques that can",pic_cs-410_1_1_600.jpg
cs-410_1_1_168,cs-410,1,1, Natural Language Content Analysis,"00:10:27,190","00:10:30,310",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,relations mentioned in text articles.,pic_cs-410_1_1_600.jpg
cs-410_1_1_169,cs-410,1,1, Natural Language Content Analysis,"00:10:30,310","00:10:34,766",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,"For example,",pic_cs-410_1_1_600.jpg
cs-410_1_1_170,cs-410,1,1, Natural Language Content Analysis,"00:10:34,766","00:10:38,606",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"locations, organizations, etc in text.",pic_cs-410_1_1_600.jpg
cs-410_1_1_171,cs-410,1,1, Natural Language Content Analysis,"00:10:38,606","00:10:40,930",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,So this is called entity extraction.,pic_cs-410_1_1_600.jpg
cs-410_1_1_172,cs-410,1,1, Natural Language Content Analysis,"00:10:40,930","00:10:42,950",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,We may be able to recognize the relations.,pic_cs-410_1_1_600.jpg
cs-410_1_1_173,cs-410,1,1, Natural Language Content Analysis,"00:10:42,950","00:10:46,140",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,"For example,",pic_cs-410_1_1_600.jpg
cs-410_1_1_174,cs-410,1,1, Natural Language Content Analysis,"00:10:46,140","00:10:51,340",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,this person met that person or,pic_cs-410_1_1_600.jpg
cs-410_1_1_175,cs-410,1,1, Natural Language Content Analysis,"00:10:51,340","00:10:54,350",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,Such relations can be extracted by using,pic_cs-410_1_1_600.jpg
cs-410_1_1_176,cs-410,1,1, Natural Language Content Analysis,"00:10:54,350","00:10:57,230",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,the computer current,pic_cs-410_1_1_600.jpg
cs-410_1_1_177,cs-410,1,1, Natural Language Content Analysis,"00:10:57,230","00:11:00,170",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,They're not perfect but,pic_cs-410_1_1_600.jpg
cs-410_1_1_178,cs-410,1,1, Natural Language Content Analysis,"00:11:00,170","00:11:02,015",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,Some entities are harder than others.,pic_cs-410_1_1_660.jpg
cs-410_1_1_179,cs-410,1,1, Natural Language Content Analysis,"00:11:03,040","00:11:05,907",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,We can also do word sense,pic_cs-410_1_1_660.jpg
cs-410_1_1_180,cs-410,1,1, Natural Language Content Analysis,"00:11:05,907","00:11:10,446",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,We have to figure out whether this word in,pic_cs-410_1_1_660.jpg
cs-410_1_1_181,cs-410,1,1, Natural Language Content Analysis,"00:11:10,446","00:11:15,250",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,in another context the computer could,pic_cs-410_1_1_660.jpg
cs-410_1_1_182,cs-410,1,1, Natural Language Content Analysis,"00:11:15,250","00:11:18,200",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,"Again, it's not perfect, but",pic_cs-410_1_1_660.jpg
cs-410_1_1_183,cs-410,1,1, Natural Language Content Analysis,"00:11:19,530","00:11:21,240",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,"We can also do sentiment analysis,",pic_cs-410_1_1_660.jpg
cs-410_1_1_184,cs-410,1,1, Natural Language Content Analysis,"00:11:21,240","00:11:25,830",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,"meaning, to figure out whether",pic_cs-410_1_1_660.jpg
cs-410_1_1_185,cs-410,1,1, Natural Language Content Analysis,"00:11:25,830","00:11:28,940",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,This is especially useful for,pic_cs-410_1_1_660.jpg
cs-410_1_1_186,cs-410,1,1, Natural Language Content Analysis,"00:11:30,410","00:11:33,150",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,So these are examples,pic_cs-410_1_1_660.jpg
cs-410_1_1_187,cs-410,1,1, Natural Language Content Analysis,"00:11:33,150","00:11:37,570",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,And they help us to obtain partial,pic_cs-410_1_1_660.jpg
cs-410_1_1_188,cs-410,1,1, Natural Language Content Analysis,"00:11:38,850","00:11:43,410",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,It's not giving us a complete,pic_cs-410_1_1_660.jpg
cs-410_1_1_189,cs-410,1,1, Natural Language Content Analysis,"00:11:43,410","00:11:44,380",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=703,this sentence.,pic_cs-410_1_1_660.jpg
cs-410_1_1_190,cs-410,1,1, Natural Language Content Analysis,"00:11:44,380","00:11:48,150",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,But it would still help us gain,pic_cs-410_1_1_660.jpg
cs-410_1_1_191,cs-410,1,1, Natural Language Content Analysis,"00:11:48,150","00:11:49,580",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,And these can be useful.,pic_cs-410_1_1_660.jpg
cs-410_1_1_192,cs-410,1,1, Natural Language Content Analysis,"00:11:51,620","00:11:54,730",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,"In terms of inference,",pic_cs-410_1_1_660.jpg
cs-410_1_1_193,cs-410,1,1, Natural Language Content Analysis,"00:11:54,730","00:12:00,050",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,probably because of the general difficulty,pic_cs-410_1_1_660.jpg
cs-410_1_1_194,cs-410,1,1, Natural Language Content Analysis,"00:12:00,050","00:12:03,390",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=720,This is a general challenge,pic_cs-410_1_1_720.jpg
cs-410_1_1_195,cs-410,1,1, Natural Language Content Analysis,"00:12:03,390","00:12:07,468",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,Now that's probably also because,pic_cs-410_1_1_720.jpg
cs-410_1_1_196,cs-410,1,1, Natural Language Content Analysis,"00:12:07,468","00:12:10,172",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,representation for,pic_cs-410_1_1_720.jpg
cs-410_1_1_197,cs-410,1,1, Natural Language Content Analysis,"00:12:10,172","00:12:11,320",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,So this is hard.,pic_cs-410_1_1_720.jpg
cs-410_1_1_198,cs-410,1,1, Natural Language Content Analysis,"00:12:11,320","00:12:16,540",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,"Yet in some domains perhaps,",pic_cs-410_1_1_720.jpg
cs-410_1_1_199,cs-410,1,1, Natural Language Content Analysis,"00:12:16,540","00:12:23,340",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,"restrictions on the word uses, you may be",pic_cs-410_1_1_720.jpg
cs-410_1_1_200,cs-410,1,1, Natural Language Content Analysis,"00:12:23,340","00:12:28,050",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,But in general we can not,pic_cs-410_1_1_720.jpg
cs-410_1_1_201,cs-410,1,1, Natural Language Content Analysis,"00:12:28,050","00:12:31,650",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,Speech act analysis is also,pic_cs-410_1_1_720.jpg
cs-410_1_1_202,cs-410,1,1, Natural Language Content Analysis,"00:12:31,650","00:12:36,600",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=751,we can only do that analysis for,pic_cs-410_1_1_720.jpg
cs-410_1_1_203,cs-410,1,1, Natural Language Content Analysis,"00:12:36,600","00:12:41,193",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,So this roughly gives you some,pic_cs-410_1_1_720.jpg
cs-410_1_1_204,cs-410,1,1, Natural Language Content Analysis,"00:12:41,193","00:12:46,356",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,And then we also talk a little,pic_cs-410_1_1_720.jpg
cs-410_1_1_205,cs-410,1,1, Natural Language Content Analysis,"00:12:46,356","00:12:51,780",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,and so we can't even do 100%,pic_cs-410_1_1_720.jpg
cs-410_1_1_206,cs-410,1,1, Natural Language Content Analysis,"00:12:51,780","00:12:54,700",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=771,"Now this looks like a simple task, but",pic_cs-410_1_1_720.jpg
cs-410_1_1_207,cs-410,1,1, Natural Language Content Analysis,"00:12:54,700","00:12:59,800",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,"think about the example here,",pic_cs-410_1_1_720.jpg
cs-410_1_1_208,cs-410,1,1, Natural Language Content Analysis,"00:12:59,800","00:13:04,840",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=779,have different syntactic categories if you,pic_cs-410_1_1_720.jpg
cs-410_1_1_209,cs-410,1,1, Natural Language Content Analysis,"00:13:04,840","00:13:07,600",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,It's not that easy to figure,pic_cs-410_1_1_780.jpg
cs-410_1_1_210,cs-410,1,1, Natural Language Content Analysis,"00:13:10,000","00:13:12,900",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=790,It's also hard to do,pic_cs-410_1_1_780.jpg
cs-410_1_1_211,cs-410,1,1, Natural Language Content Analysis,"00:13:12,900","00:13:16,940",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,"And again, the same sentence",pic_cs-410_1_1_780.jpg
cs-410_1_1_212,cs-410,1,1, Natural Language Content Analysis,"00:13:18,010","00:13:23,330",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,This ambiguity can be very hard to,pic_cs-410_1_1_780.jpg
cs-410_1_1_213,cs-410,1,1, Natural Language Content Analysis,"00:13:23,330","00:13:27,940",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,where you have to use a lot of knowledge,pic_cs-410_1_1_780.jpg
cs-410_1_1_214,cs-410,1,1, Natural Language Content Analysis,"00:13:27,940","00:13:33,310",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,"from the background, in order to figure",pic_cs-410_1_1_780.jpg
cs-410_1_1_215,cs-410,1,1, Natural Language Content Analysis,"00:13:33,310","00:13:37,730",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,So although the sentence looks very,pic_cs-410_1_1_780.jpg
cs-410_1_1_216,cs-410,1,1, Natural Language Content Analysis,"00:13:37,730","00:13:42,380",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,And in cases when the sentence is,pic_cs-410_1_1_780.jpg
cs-410_1_1_217,cs-410,1,1, Natural Language Content Analysis,"00:13:42,380","00:13:46,760",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,"five prepositional phrases, and there",pic_cs-410_1_1_780.jpg
cs-410_1_1_218,cs-410,1,1, Natural Language Content Analysis,"00:13:48,580","00:13:51,650",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,It's also harder to do precise,pic_cs-410_1_1_780.jpg
cs-410_1_1_219,cs-410,1,1, Natural Language Content Analysis,"00:13:51,650","00:13:53,410",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=831,So here's an example.,pic_cs-410_1_1_780.jpg
cs-410_1_1_220,cs-410,1,1, Natural Language Content Analysis,"00:13:53,410","00:14:00,108",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=833,"In the sentence ""John owns a restaurant.""",pic_cs-410_1_1_780.jpg
cs-410_1_1_221,cs-410,1,1, Natural Language Content Analysis,"00:14:00,108","00:14:05,340",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,"The word own,",pic_cs-410_1_1_840.jpg
cs-410_1_1_222,cs-410,1,1, Natural Language Content Analysis,"00:14:05,340","00:14:10,210",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=845,it's very hard to precisely describe,pic_cs-410_1_1_840.jpg
cs-410_1_1_223,cs-410,1,1, Natural Language Content Analysis,"00:14:11,430","00:14:16,467",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,So as a result we have a robust and,pic_cs-410_1_1_840.jpg
cs-410_1_1_224,cs-410,1,1, Natural Language Content Analysis,"00:14:16,467","00:14:20,860",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=856,Natural Language Processing techniques,pic_cs-410_1_1_840.jpg
cs-410_1_1_225,cs-410,1,1, Natural Language Content Analysis,"00:14:22,490","00:14:25,640",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,"In a shallow way,",pic_cs-410_1_1_840.jpg
cs-410_1_1_226,cs-410,1,1, Natural Language Content Analysis,"00:14:25,640","00:14:33,600",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,"For example, parts of speech tagging or a",pic_cs-410_1_1_840.jpg
cs-410_1_1_227,cs-410,1,1, Natural Language Content Analysis,"00:14:33,600","00:14:35,520",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,"And those are not deep understanding,",pic_cs-410_1_1_840.jpg
cs-410_1_1_228,cs-410,1,1, Natural Language Content Analysis,"00:14:35,520","00:14:39,419",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,because we're not really understanding,pic_cs-410_1_1_840.jpg
cs-410_1_1_229,cs-410,1,1, Natural Language Content Analysis,"00:14:41,270","00:14:45,170",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=881,On the other hand of the deep,pic_cs-410_1_1_840.jpg
cs-410_1_1_230,cs-410,1,1, Natural Language Content Analysis,"00:14:45,170","00:14:50,840",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=885,"up well, meaning that they would",pic_cs-410_1_1_840.jpg
cs-410_1_1_231,cs-410,1,1, Natural Language Content Analysis,"00:14:50,840","00:14:54,850",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,And if you don't restrict,pic_cs-410_1_1_840.jpg
cs-410_1_1_232,cs-410,1,1, Natural Language Content Analysis,"00:14:54,850","00:14:59,750",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,"the use of words, then these",pic_cs-410_1_1_840.jpg
cs-410_1_1_233,cs-410,1,1, Natural Language Content Analysis,"00:14:59,750","00:15:04,310",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,They may work well based on machine,pic_cs-410_1_1_840.jpg
cs-410_1_1_234,cs-410,1,1, Natural Language Content Analysis,"00:15:04,310","00:15:08,520",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,that are similar to the training data,pic_cs-410_1_1_900.jpg
cs-410_1_1_235,cs-410,1,1, Natural Language Content Analysis,"00:15:08,520","00:15:13,090",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=908,But they generally wouldn't work well on,pic_cs-410_1_1_900.jpg
cs-410_1_1_236,cs-410,1,1, Natural Language Content Analysis,"00:15:13,090","00:15:14,290",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,the training data.,pic_cs-410_1_1_900.jpg
cs-410_1_1_237,cs-410,1,1, Natural Language Content Analysis,"00:15:14,290","00:15:19,150",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=914,So this pretty much summarizes the state,pic_cs-410_1_1_900.jpg
cs-410_1_1_238,cs-410,1,1, Natural Language Content Analysis,"00:15:19,150","00:15:23,590",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=919,"Of course, within such a short amount",pic_cs-410_1_1_900.jpg
cs-410_1_1_239,cs-410,1,1, Natural Language Content Analysis,"00:15:23,590","00:15:27,120",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,"a complete view of NLP,",pic_cs-410_1_1_900.jpg
cs-410_1_1_240,cs-410,1,1, Natural Language Content Analysis,"00:15:27,120","00:15:35,896",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=927,And I'd expect to see multiple courses on,pic_cs-410_1_1_900.jpg
cs-410_1_1_241,cs-410,1,1, Natural Language Content Analysis,"00:15:35,896","00:15:40,960",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=935,But because of its relevance to the topic,pic_cs-410_1_1_900.jpg
cs-410_1_1_242,cs-410,1,1, Natural Language Content Analysis,"00:15:40,960","00:15:45,410",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=940,you to know the background in case,pic_cs-410_1_1_900.jpg
cs-410_1_1_243,cs-410,1,1, Natural Language Content Analysis,"00:15:45,410","00:15:47,340",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=945,So what does that mean for Text Retrieval?,pic_cs-410_1_1_900.jpg
cs-410_1_1_244,cs-410,1,1, Natural Language Content Analysis,"00:15:48,980","00:15:53,254",244,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,"Well, in Text Retrieval we",pic_cs-410_1_1_900.jpg
cs-410_1_1_245,cs-410,1,1, Natural Language Content Analysis,"00:15:53,254","00:15:56,470",245,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,It's very hard to restrict,pic_cs-410_1_1_900.jpg
cs-410_1_1_246,cs-410,1,1, Natural Language Content Analysis,"00:15:56,470","00:16:00,092",246,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=956,And we also are often dealing,pic_cs-410_1_1_900.jpg
cs-410_1_1_247,cs-410,1,1, Natural Language Content Analysis,"00:16:00,092","00:16:06,730",247,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=960,So that means The NLP techniques must,pic_cs-410_1_1_960.jpg
cs-410_1_1_248,cs-410,1,1, Natural Language Content Analysis,"00:16:06,730","00:16:12,060",248,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=966,And that just implies today we can only,pic_cs-410_1_1_960.jpg
cs-410_1_1_249,cs-410,1,1, Natural Language Content Analysis,"00:16:12,060","00:16:13,550",249,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=972,text retrieval.,pic_cs-410_1_1_960.jpg
cs-410_1_1_250,cs-410,1,1, Natural Language Content Analysis,"00:16:13,550","00:16:14,780",250,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=973,"In fact,",pic_cs-410_1_1_960.jpg
cs-410_1_1_251,cs-410,1,1, Natural Language Content Analysis,"00:16:14,780","00:16:19,070",251,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=974,most search engines today use something,pic_cs-410_1_1_960.jpg
cs-410_1_1_252,cs-410,1,1, Natural Language Content Analysis,"00:16:20,740","00:16:25,450",252,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,"Now, this is probably the simplest",pic_cs-410_1_1_960.jpg
cs-410_1_1_253,cs-410,1,1, Natural Language Content Analysis,"00:16:25,450","00:16:29,250",253,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=985,That is to turn text data,pic_cs-410_1_1_960.jpg
cs-410_1_1_254,cs-410,1,1, Natural Language Content Analysis,"00:16:29,250","00:16:33,930",254,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=989,"Meaning we'll keep individual words, but",pic_cs-410_1_1_960.jpg
cs-410_1_1_255,cs-410,1,1, Natural Language Content Analysis,"00:16:33,930","00:16:37,660",255,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=993,And we'll keep duplicated,pic_cs-410_1_1_960.jpg
cs-410_1_1_256,cs-410,1,1, Natural Language Content Analysis,"00:16:37,660","00:16:39,950",256,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=997,So this is called a bag,pic_cs-410_1_1_960.jpg
cs-410_1_1_257,cs-410,1,1, Natural Language Content Analysis,"00:16:39,950","00:16:45,990",257,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=999,"When you represent text in this way,",pic_cs-410_1_1_960.jpg
cs-410_1_1_258,cs-410,1,1, Natural Language Content Analysis,"00:16:45,990","00:16:51,020",258,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,That just makes it harder to understand,pic_cs-410_1_1_960.jpg
cs-410_1_1_259,cs-410,1,1, Natural Language Content Analysis,"00:16:51,020","00:16:52,440",259,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1011,because we've lost the order.,pic_cs-410_1_1_960.jpg
cs-410_1_1_260,cs-410,1,1, Natural Language Content Analysis,"00:16:53,870","00:16:57,320",260,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,But yet this representation tends,pic_cs-410_1_1_960.jpg
cs-410_1_1_261,cs-410,1,1, Natural Language Content Analysis,"00:16:57,320","00:16:59,150",261,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1017,most search tasks.,pic_cs-410_1_1_960.jpg
cs-410_1_1_262,cs-410,1,1, Natural Language Content Analysis,"00:16:59,150","00:17:03,450",262,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1019,And this was partly because the search,pic_cs-410_1_1_960.jpg
cs-410_1_1_263,cs-410,1,1, Natural Language Content Analysis,"00:17:03,450","00:17:08,230",263,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1023,If you see matching of some of,pic_cs-410_1_1_1020.jpg
cs-410_1_1_264,cs-410,1,1, Natural Language Content Analysis,"00:17:08,230","00:17:12,560",264,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1028,chances are that that document is about,pic_cs-410_1_1_1020.jpg
cs-410_1_1_265,cs-410,1,1, Natural Language Content Analysis,"00:17:13,670","00:17:15,775",265,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1033,"So in comparison of some other tasks, for",pic_cs-410_1_1_1020.jpg
cs-410_1_1_266,cs-410,1,1, Natural Language Content Analysis,"00:17:15,775","00:17:20,490",266,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,"example, machine translation would require",pic_cs-410_1_1_1020.jpg
cs-410_1_1_267,cs-410,1,1, Natural Language Content Analysis,"00:17:20,490","00:17:22,680",267,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1040,Otherwise the translation would be wrong.,pic_cs-410_1_1_1020.jpg
cs-410_1_1_268,cs-410,1,1, Natural Language Content Analysis,"00:17:22,680","00:17:25,780",268,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1042,So in comparison such tasks,pic_cs-410_1_1_1020.jpg
cs-410_1_1_269,cs-410,1,1, Natural Language Content Analysis,"00:17:25,780","00:17:30,670",269,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1045,Such a representation is often sufficient,pic_cs-410_1_1_1020.jpg
cs-410_1_1_270,cs-410,1,1, Natural Language Content Analysis,"00:17:30,670","00:17:34,050",270,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1050,"the major search engines today,",pic_cs-410_1_1_1020.jpg
cs-410_1_1_271,cs-410,1,1, Natural Language Content Analysis,"00:17:35,770","00:17:40,240",271,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1055,"Of course, I put in parentheses but",pic_cs-410_1_1_1020.jpg
cs-410_1_1_272,cs-410,1,1, Natural Language Content Analysis,"00:17:40,240","00:17:42,750",272,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1060,that are not answered well by,pic_cs-410_1_1_1020.jpg
cs-410_1_1_273,cs-410,1,1, Natural Language Content Analysis,"00:17:42,750","00:17:48,320",273,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1062,they do require the replantation that,pic_cs-410_1_1_1020.jpg
cs-410_1_1_274,cs-410,1,1, Natural Language Content Analysis,"00:17:48,320","00:17:51,900",274,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1068,That would require more natural,pic_cs-410_1_1_1020.jpg
cs-410_1_1_275,cs-410,1,1, Natural Language Content Analysis,"00:17:52,950","00:17:56,600",275,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1072,There was another reason why we,pic_cs-410_1_1_1020.jpg
cs-410_1_1_276,cs-410,1,1, Natural Language Content Analysis,"00:17:56,600","00:17:59,100",276,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1076,NLP techniques in modern search engines.,pic_cs-410_1_1_1020.jpg
cs-410_1_1_277,cs-410,1,1, Natural Language Content Analysis,"00:17:59,100","00:18:02,460",277,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1079,And that's because some,pic_cs-410_1_1_1020.jpg
cs-410_1_1_278,cs-410,1,1, Natural Language Content Analysis,"00:18:02,460","00:18:05,400",278,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1082,naturally solved the problem of NLP.,pic_cs-410_1_1_1080.jpg
cs-410_1_1_279,cs-410,1,1, Natural Language Content Analysis,"00:18:05,400","00:18:09,240",279,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1085,So one example is word,pic_cs-410_1_1_1080.jpg
cs-410_1_1_280,cs-410,1,1, Natural Language Content Analysis,"00:18:09,240","00:18:11,060",280,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1089,Think about a word like Java.,pic_cs-410_1_1_1080.jpg
cs-410_1_1_281,cs-410,1,1, Natural Language Content Analysis,"00:18:11,060","00:18:13,900",281,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1091,It could mean coffee or,pic_cs-410_1_1_1080.jpg
cs-410_1_1_282,cs-410,1,1, Natural Language Content Analysis,"00:18:15,090","00:18:18,230",282,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1095,"If you look at the word anome,",pic_cs-410_1_1_1080.jpg
cs-410_1_1_283,cs-410,1,1, Natural Language Content Analysis,"00:18:18,230","00:18:23,050",283,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1098,"when the user uses the word in the query,",pic_cs-410_1_1_1080.jpg
cs-410_1_1_284,cs-410,1,1, Natural Language Content Analysis,"00:18:23,050","00:18:26,240",284,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1103,"For example, I'm looking for",pic_cs-410_1_1_1080.jpg
cs-410_1_1_285,cs-410,1,1, Natural Language Content Analysis,"00:18:26,240","00:18:31,990",285,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1106,"When I have applet there,",pic_cs-410_1_1_1080.jpg
cs-410_1_1_286,cs-410,1,1, Natural Language Content Analysis,"00:18:31,990","00:18:36,360",286,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1111,And that contest can help us,pic_cs-410_1_1_1080.jpg
cs-410_1_1_287,cs-410,1,1, Natural Language Content Analysis,"00:18:36,360","00:18:39,690",287,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1116,which Java is referring,pic_cs-410_1_1_1080.jpg
cs-410_1_1_288,cs-410,1,1, Natural Language Content Analysis,"00:18:39,690","00:18:43,710",288,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1119,Because those documents would,pic_cs-410_1_1_1080.jpg
cs-410_1_1_289,cs-410,1,1, Natural Language Content Analysis,"00:18:43,710","00:18:48,560",289,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,If Java occurs in that,pic_cs-410_1_1_1080.jpg
cs-410_1_1_290,cs-410,1,1, Natural Language Content Analysis,"00:18:48,560","00:18:52,960",290,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1128,then you would never match applet or,pic_cs-410_1_1_1080.jpg
cs-410_1_1_291,cs-410,1,1, Natural Language Content Analysis,"00:18:52,960","00:18:56,250",291,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1132,So this is the case when,pic_cs-410_1_1_1080.jpg
cs-410_1_1_292,cs-410,1,1, Natural Language Content Analysis,"00:18:56,250","00:18:58,580",292,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1136,naturally achieve the goal of word.,pic_cs-410_1_1_1080.jpg
cs-410_1_1_293,cs-410,1,1, Natural Language Content Analysis,"00:19:01,530","00:19:05,920",293,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1141,Another example is some technique called,pic_cs-410_1_1_1140.jpg
cs-410_1_1_294,cs-410,1,1, Natural Language Content Analysis,"00:19:05,920","00:19:11,360",294,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1145,feedback which we will talk about,pic_cs-410_1_1_1140.jpg
cs-410_1_1_295,cs-410,1,1, Natural Language Content Analysis,"00:19:11,360","00:19:16,938",295,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1151,This technique would allow us to add,pic_cs-410_1_1_1140.jpg
cs-410_1_1_296,cs-410,1,1, Natural Language Content Analysis,"00:19:16,938","00:19:21,859",296,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1156,those additional words could,pic_cs-410_1_1_1140.jpg
cs-410_1_1_297,cs-410,1,1, Natural Language Content Analysis,"00:19:21,859","00:19:26,155",297,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1161,And these words can help matching,pic_cs-410_1_1_1140.jpg
cs-410_1_1_298,cs-410,1,1, Natural Language Content Analysis,"00:19:26,155","00:19:27,680",298,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1166,have not occurred.,pic_cs-410_1_1_1140.jpg
cs-410_1_1_299,cs-410,1,1, Natural Language Content Analysis,"00:19:27,680","00:19:32,500",299,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1167,"So this achieves, to some extent,",pic_cs-410_1_1_1140.jpg
cs-410_1_1_300,cs-410,1,1, Natural Language Content Analysis,"00:19:32,500","00:19:35,350",300,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1172,So those techniques also helped us,pic_cs-410_1_1_1140.jpg
cs-410_1_1_301,cs-410,1,1, Natural Language Content Analysis,"00:19:35,350","00:19:38,890",301,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1175,bypass some of the difficulties,pic_cs-410_1_1_1140.jpg
cs-410_1_1_302,cs-410,1,1, Natural Language Content Analysis,"00:19:40,530","00:19:43,920",302,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1180,"However, in the long run we still need",pic_cs-410_1_1_1140.jpg
cs-410_1_1_303,cs-410,1,1, Natural Language Content Analysis,"00:19:43,920","00:19:47,280",303,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1183,techniques in order to improve the,pic_cs-410_1_1_1140.jpg
cs-410_1_1_304,cs-410,1,1, Natural Language Content Analysis,"00:19:47,280","00:19:50,939",304,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1187,And it's particularly needed for,pic_cs-410_1_1_1140.jpg
cs-410_1_1_305,cs-410,1,1, Natural Language Content Analysis,"00:19:52,160","00:19:53,390",305,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1192,Or for question and answering.,pic_cs-410_1_1_1140.jpg
cs-410_1_1_306,cs-410,1,1, Natural Language Content Analysis,"00:19:55,310","00:20:00,540",306,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1195,Google has recently launched a knowledge,pic_cs-410_1_1_1140.jpg
cs-410_1_1_307,cs-410,1,1, Natural Language Content Analysis,"00:20:00,540","00:20:05,220",307,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1200,"that goal, because knowledge graph would",pic_cs-410_1_1_1200.jpg
cs-410_1_1_308,cs-410,1,1, Natural Language Content Analysis,"00:20:05,220","00:20:09,170",308,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1205,And this goes beyond the simple,pic_cs-410_1_1_1200.jpg
cs-410_1_1_309,cs-410,1,1, Natural Language Content Analysis,"00:20:09,170","00:20:12,950",309,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1209,And such technique should help us,pic_cs-410_1_1_1200.jpg
cs-410_1_1_310,cs-410,1,1, Natural Language Content Analysis,"00:20:14,180","00:20:19,220",310,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1214,"significantly, although this is the open",pic_cs-410_1_1_1200.jpg
cs-410_1_1_311,cs-410,1,1, Natural Language Content Analysis,"00:20:19,220","00:20:24,990",311,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1219,"In sum, in this lecture we",pic_cs-410_1_1_1200.jpg
cs-410_1_1_312,cs-410,1,1, Natural Language Content Analysis,"00:20:24,990","00:20:27,820",312,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1224,we've talked about the state,pic_cs-410_1_1_1200.jpg
cs-410_1_1_313,cs-410,1,1, Natural Language Content Analysis,"00:20:27,820","00:20:30,550",313,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1227,"What we can do, what we cannot do.",pic_cs-410_1_1_1200.jpg
cs-410_1_1_314,cs-410,1,1, Natural Language Content Analysis,"00:20:30,550","00:20:34,510",314,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1230,"And finally, we also explain why",pic_cs-410_1_1_1200.jpg
cs-410_1_1_315,cs-410,1,1, Natural Language Content Analysis,"00:20:34,510","00:20:38,290",315,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1234,remains the dominant replantation,pic_cs-410_1_1_1200.jpg
cs-410_1_1_316,cs-410,1,1, Natural Language Content Analysis,"00:20:38,290","00:20:43,258",316,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1238,even though deeper NLP would be needed for,pic_cs-410_1_1_1200.jpg
cs-410_1_1_317,cs-410,1,1, Natural Language Content Analysis,"00:20:43,258","00:20:46,470",317,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1243,"If you want to know more, you can take",pic_cs-410_1_1_1200.jpg
cs-410_1_1_318,cs-410,1,1, Natural Language Content Analysis,"00:20:46,470","00:20:49,070",318,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1246,I only cited one here and,pic_cs-410_1_1_1200.jpg
cs-410_1_1_319,cs-410,1,1, Natural Language Content Analysis,"00:20:49,070","00:20:52,976",319,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1249,Thanks.,pic_cs-410_1_1_1200.jpg
cs-410_1_1_320,cs-410,1,1, Natural Language Content Analysis,"00:20:52,976","00:21:02,976",320,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1252,[MUSIC],pic_cs-410_1_1_1200.jpg
cs-410_1_2_1,cs-410,1,2, Text Access,"00:00:00,012","00:00:09,434",1,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=0,[SOUND],pic_cs-410_1_2_0.jpg
cs-410_1_2_2,cs-410,1,2, Text Access,"00:00:09,434","00:00:12,223",2,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=9,"In this lecture,",pic_cs-410_1_2_0.jpg
cs-410_1_2_3,cs-410,1,2, Text Access,"00:00:14,279","00:00:18,349",3,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=14,"In the previous lecture, we talked about",pic_cs-410_1_2_0.jpg
cs-410_1_2_4,cs-410,1,2, Text Access,"00:00:19,360","00:00:23,970",4,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=19,We explained that the state of the are,pic_cs-410_1_2_0.jpg
cs-410_1_2_5,cs-410,1,2, Text Access,"00:00:23,970","00:00:28,970",5,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=23,are still not good enough to process,pic_cs-410_1_2_0.jpg
cs-410_1_2_6,cs-410,1,2, Text Access,"00:00:28,970","00:00:30,550",6,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=28,in a robust manner.,pic_cs-410_1_2_0.jpg
cs-410_1_2_7,cs-410,1,2, Text Access,"00:00:30,550","00:00:31,360",7,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=30,"As a result,",pic_cs-410_1_2_0.jpg
cs-410_1_2_8,cs-410,1,2, Text Access,"00:00:31,360","00:00:37,250",8,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=31,bag of words remains very popular in,pic_cs-410_1_2_0.jpg
cs-410_1_2_9,cs-410,1,2, Text Access,"00:00:39,140","00:00:44,100",9,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=39,"In this lecture, we're going to talk",pic_cs-410_1_2_0.jpg
cs-410_1_2_10,cs-410,1,2, Text Access,"00:00:44,100","00:00:48,120",10,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=44,help users get access to the text data.,pic_cs-410_1_2_0.jpg
cs-410_1_2_11,cs-410,1,2, Text Access,"00:00:48,120","00:00:55,000",11,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=48,This is also important step to convert,pic_cs-410_1_2_0.jpg
cs-410_1_2_12,cs-410,1,2, Text Access,"00:00:55,000","00:00:57,610",12,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=55,That are actually needed,pic_cs-410_1_2_0.jpg
cs-410_1_2_13,cs-410,1,2, Text Access,"00:00:57,610","00:01:02,510",13,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=57,"So the main question we'll address here,",pic_cs-410_1_2_0.jpg
cs-410_1_2_14,cs-410,1,2, Text Access,"00:01:02,510","00:01:07,450",14,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=62,"can a text information system, help users",pic_cs-410_1_2_60.jpg
cs-410_1_2_15,cs-410,1,2, Text Access,"00:01:07,450","00:01:11,550",15,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=67,We're going to cover two complimentary,pic_cs-410_1_2_60.jpg
cs-410_1_2_16,cs-410,1,2, Text Access,"00:01:12,610","00:01:17,700",16,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=72,And then we're going to talk about,pic_cs-410_1_2_60.jpg
cs-410_1_2_17,cs-410,1,2, Text Access,"00:01:17,700","00:01:19,080",17,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=77,querying versus browsing.,pic_cs-410_1_2_60.jpg
cs-410_1_2_18,cs-410,1,2, Text Access,"00:01:20,770","00:01:22,860",18,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=80,So first push versus pull.,pic_cs-410_1_2_60.jpg
cs-410_1_2_19,cs-410,1,2, Text Access,"00:01:24,500","00:01:29,250",19,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=84,These are two different ways connect,pic_cs-410_1_2_60.jpg
cs-410_1_2_20,cs-410,1,2, Text Access,"00:01:29,250","00:01:29,900",20,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=89,at the right time.,pic_cs-410_1_2_60.jpg
cs-410_1_2_21,cs-410,1,2, Text Access,"00:01:31,190","00:01:35,900",21,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=91,The difference is which,pic_cs-410_1_2_60.jpg
cs-410_1_2_22,cs-410,1,2, Text Access,"00:01:37,230","00:01:38,740",22,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=97,which party takes the initiative.,pic_cs-410_1_2_60.jpg
cs-410_1_2_23,cs-410,1,2, Text Access,"00:01:40,290","00:01:41,380",23,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=100,"In the pull mode,",pic_cs-410_1_2_60.jpg
cs-410_1_2_24,cs-410,1,2, Text Access,"00:01:41,380","00:01:46,439",24,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=101,the users take the initiative to,pic_cs-410_1_2_60.jpg
cs-410_1_2_25,cs-410,1,2, Text Access,"00:01:47,700","00:01:53,420",25,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=107,"And in this case, a user typically would",pic_cs-410_1_2_60.jpg
cs-410_1_2_26,cs-410,1,2, Text Access,"00:01:53,420","00:01:56,100",26,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=113,"For example,",pic_cs-410_1_2_60.jpg
cs-410_1_2_27,cs-410,1,2, Text Access,"00:01:56,100","00:02:01,640",27,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=116,then browse the results to,pic_cs-410_1_2_60.jpg
cs-410_1_2_28,cs-410,1,2, Text Access,"00:02:02,790","00:02:06,280",28,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=122,So this is usually appropriate for,pic_cs-410_1_2_120.jpg
cs-410_1_2_29,cs-410,1,2, Text Access,"00:02:06,280","00:02:09,340",29,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=126,satisfying a user's ad,pic_cs-410_1_2_120.jpg
cs-410_1_2_30,cs-410,1,2, Text Access,"00:02:10,580","00:02:14,280",30,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=130,An ad hoc information need is,pic_cs-410_1_2_120.jpg
cs-410_1_2_31,cs-410,1,2, Text Access,"00:02:14,280","00:02:17,870",31,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=134,"For example, you want to buy a product so",pic_cs-410_1_2_120.jpg
cs-410_1_2_32,cs-410,1,2, Text Access,"00:02:17,870","00:02:22,550",32,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=137,you suddenly have a need to read,pic_cs-410_1_2_120.jpg
cs-410_1_2_33,cs-410,1,2, Text Access,"00:02:22,550","00:02:26,620",33,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=142,"But after you have cracked information,",pic_cs-410_1_2_120.jpg
cs-410_1_2_34,cs-410,1,2, Text Access,"00:02:26,620","00:02:28,880",34,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=146,You generally no longer,pic_cs-410_1_2_120.jpg
cs-410_1_2_35,cs-410,1,2, Text Access,"00:02:28,880","00:02:30,200",35,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=148,it's a temporary information need.,pic_cs-410_1_2_120.jpg
cs-410_1_2_36,cs-410,1,2, Text Access,"00:02:31,360","00:02:35,230",36,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=151,"In such a case, it's very hard for",pic_cs-410_1_2_120.jpg
cs-410_1_2_37,cs-410,1,2, Text Access,"00:02:35,230","00:02:39,480",37,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=155,it's more proper for,pic_cs-410_1_2_120.jpg
cs-410_1_2_38,cs-410,1,2, Text Access,"00:02:39,480","00:02:42,260",38,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=159,that's why search engines are very useful.,pic_cs-410_1_2_120.jpg
cs-410_1_2_39,cs-410,1,2, Text Access,"00:02:42,260","00:02:48,370",39,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=162,Today because many people have many,pic_cs-410_1_2_120.jpg
cs-410_1_2_40,cs-410,1,2, Text Access,"00:02:48,370","00:02:52,620",40,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=168,So as we're speaking Google is probably,pic_cs-410_1_2_120.jpg
cs-410_1_2_41,cs-410,1,2, Text Access,"00:02:52,620","00:02:55,720",41,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=172,"And those are all, or mostly adequate.",pic_cs-410_1_2_120.jpg
cs-410_1_2_42,cs-410,1,2, Text Access,"00:02:55,720","00:02:56,590",42,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=175,Information needs.,pic_cs-410_1_2_120.jpg
cs-410_1_2_43,cs-410,1,2, Text Access,"00:02:57,950","00:02:59,680",43,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=177,So this is a pull mode.,pic_cs-410_1_2_120.jpg
cs-410_1_2_44,cs-410,1,2, Text Access,"00:02:59,680","00:03:03,570",44,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=179,In contrast in the push mode in,pic_cs-410_1_2_120.jpg
cs-410_1_2_45,cs-410,1,2, Text Access,"00:03:03,570","00:03:07,510",45,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=183,to push the information to the user or,pic_cs-410_1_2_180.jpg
cs-410_1_2_46,cs-410,1,2, Text Access,"00:03:07,510","00:03:11,090",46,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=187,So in this case this is usually,pic_cs-410_1_2_180.jpg
cs-410_1_2_47,cs-410,1,2, Text Access,"00:03:13,100","00:03:15,190",47,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=193,Now this would be appropriate if.,pic_cs-410_1_2_180.jpg
cs-410_1_2_48,cs-410,1,2, Text Access,"00:03:15,190","00:03:16,900",48,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=195,The user has a stable information.,pic_cs-410_1_2_180.jpg
cs-410_1_2_49,cs-410,1,2, Text Access,"00:03:17,900","00:03:22,040",49,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=197,For example you may have a research,pic_cs-410_1_2_180.jpg
cs-410_1_2_50,cs-410,1,2, Text Access,"00:03:22,040","00:03:24,980",50,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=202,that interest tends to stay for a while.,pic_cs-410_1_2_180.jpg
cs-410_1_2_51,cs-410,1,2, Text Access,"00:03:24,980","00:03:26,930",51,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=204,"So, it's rather stable.",pic_cs-410_1_2_180.jpg
cs-410_1_2_52,cs-410,1,2, Text Access,"00:03:26,930","00:03:29,240",52,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=206,Your hobby is another example of.,pic_cs-410_1_2_180.jpg
cs-410_1_2_53,cs-410,1,2, Text Access,"00:03:29,240","00:03:34,100",53,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=209,A stable information need is such a case,pic_cs-410_1_2_180.jpg
cs-410_1_2_54,cs-410,1,2, Text Access,"00:03:34,100","00:03:38,860",54,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=214,"can learn your interest, and",pic_cs-410_1_2_180.jpg
cs-410_1_2_55,cs-410,1,2, Text Access,"00:03:38,860","00:03:43,710",55,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=218,If the system hasn't seen any,pic_cs-410_1_2_180.jpg
cs-410_1_2_56,cs-410,1,2, Text Access,"00:03:43,710","00:03:47,900",56,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=223,the system could then take the initiative,pic_cs-410_1_2_180.jpg
cs-410_1_2_57,cs-410,1,2, Text Access,"00:03:47,900","00:03:49,940",57,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=227,"So, for example, a news filter or",pic_cs-410_1_2_180.jpg
cs-410_1_2_58,cs-410,1,2, Text Access,"00:03:49,940","00:03:53,020",58,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=229,news recommended system could,pic_cs-410_1_2_180.jpg
cs-410_1_2_59,cs-410,1,2, Text Access,"00:03:53,020","00:03:56,870",59,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=233,identify interesting news to you and,pic_cs-410_1_2_180.jpg
cs-410_1_2_60,cs-410,1,2, Text Access,"00:03:59,130","00:04:03,960",60,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=239,This mode of information access may be,pic_cs-410_1_2_180.jpg
cs-410_1_2_61,cs-410,1,2, Text Access,"00:04:03,960","00:04:08,790",61,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=243,has good knowledge about the users need,pic_cs-410_1_2_240.jpg
cs-410_1_2_62,cs-410,1,2, Text Access,"00:04:08,790","00:04:11,850",62,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=248,"So for example, when you search for",pic_cs-410_1_2_240.jpg
cs-410_1_2_63,cs-410,1,2, Text Access,"00:04:11,850","00:04:16,130",63,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=251,a search engine might infer you might be,pic_cs-410_1_2_240.jpg
cs-410_1_2_64,cs-410,1,2, Text Access,"00:04:16,130","00:04:17,530",64,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=256,Formation.,pic_cs-410_1_2_240.jpg
cs-410_1_2_65,cs-410,1,2, Text Access,"00:04:17,530","00:04:20,950",65,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=257,And they would recommend the information,pic_cs-410_1_2_240.jpg
cs-410_1_2_66,cs-410,1,2, Text Access,"00:04:20,950","00:04:24,780",66,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=260,"example, of an advertisement",pic_cs-410_1_2_240.jpg
cs-410_1_2_67,cs-410,1,2, Text Access,"00:04:27,790","00:04:34,540",67,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=267,So this is about the two high level,pic_cs-410_1_2_240.jpg
cs-410_1_2_68,cs-410,1,2, Text Access,"00:04:35,720","00:04:38,440",68,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=275,Now let's look at the pull,pic_cs-410_1_2_240.jpg
cs-410_1_2_69,cs-410,1,2, Text Access,"00:04:39,900","00:04:43,740",69,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=279,"In the pull mode, we can further",pic_cs-410_1_2_240.jpg
cs-410_1_2_70,cs-410,1,2, Text Access,"00:04:43,740","00:04:46,010",70,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=283,Querying versus browsing.,pic_cs-410_1_2_240.jpg
cs-410_1_2_71,cs-410,1,2, Text Access,"00:04:46,010","00:04:48,790",71,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=286,"In querying,",pic_cs-410_1_2_240.jpg
cs-410_1_2_72,cs-410,1,2, Text Access,"00:04:48,790","00:04:50,560",72,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=288,"Typical the keyword query, and",pic_cs-410_1_2_240.jpg
cs-410_1_2_73,cs-410,1,2, Text Access,"00:04:50,560","00:04:53,430",73,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=290,the search engine system would,pic_cs-410_1_2_240.jpg
cs-410_1_2_74,cs-410,1,2, Text Access,"00:04:54,500","00:05:00,730",74,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=294,And this works well when the user knows,pic_cs-410_1_2_240.jpg
cs-410_1_2_75,cs-410,1,2, Text Access,"00:05:00,730","00:05:02,450",75,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=300,So if you know exactly,pic_cs-410_1_2_300.jpg
cs-410_1_2_76,cs-410,1,2, Text Access,"00:05:02,450","00:05:04,540",76,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=302,you tend to know the right keywords.,pic_cs-410_1_2_300.jpg
cs-410_1_2_77,cs-410,1,2, Text Access,"00:05:04,540","00:05:07,880",77,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=304,"And then query works very well,",pic_cs-410_1_2_300.jpg
cs-410_1_2_78,cs-410,1,2, Text Access,"00:05:09,290","00:05:12,740",78,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=309,But we also know that sometimes,pic_cs-410_1_2_300.jpg
cs-410_1_2_79,cs-410,1,2, Text Access,"00:05:12,740","00:05:16,970",79,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=312,When you don't know the right,pic_cs-410_1_2_300.jpg
cs-410_1_2_80,cs-410,1,2, Text Access,"00:05:16,970","00:05:21,760",80,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=316,you want to browse information,pic_cs-410_1_2_300.jpg
cs-410_1_2_81,cs-410,1,2, Text Access,"00:05:21,760","00:05:24,780",81,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=321,You use because browsing,pic_cs-410_1_2_300.jpg
cs-410_1_2_82,cs-410,1,2, Text Access,"00:05:24,780","00:05:29,890",82,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=324,"So in this case, in the case of browsing,",pic_cs-410_1_2_300.jpg
cs-410_1_2_83,cs-410,1,2, Text Access,"00:05:29,890","00:05:33,330",83,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=329,into the relevant information,pic_cs-410_1_2_300.jpg
cs-410_1_2_84,cs-410,1,2, Text Access,"00:05:34,740","00:05:39,850",84,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=334,supported by the structures of documents.,pic_cs-410_1_2_300.jpg
cs-410_1_2_85,cs-410,1,2, Text Access,"00:05:39,850","00:05:42,690",85,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=339,So the system would maintain,pic_cs-410_1_2_300.jpg
cs-410_1_2_86,cs-410,1,2, Text Access,"00:05:42,690","00:05:45,190",86,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=342,then the user could follow,pic_cs-410_1_2_300.jpg
cs-410_1_2_87,cs-410,1,2, Text Access,"00:05:47,370","00:05:53,850",87,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=347,So this really works well when the user,pic_cs-410_1_2_300.jpg
cs-410_1_2_88,cs-410,1,2, Text Access,"00:05:53,850","00:05:59,750",88,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=353,or the user doesn't know what,pic_cs-410_1_2_300.jpg
cs-410_1_2_89,cs-410,1,2, Text Access,"00:05:59,750","00:06:05,070",89,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=359,Or simply because the user finds it,pic_cs-410_1_2_300.jpg
cs-410_1_2_90,cs-410,1,2, Text Access,"00:06:05,070","00:06:10,450",90,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=365,So even if a user knows what query to,pic_cs-410_1_2_360.jpg
cs-410_1_2_91,cs-410,1,2, Text Access,"00:06:10,450","00:06:12,370",91,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=370,to search for information.,pic_cs-410_1_2_360.jpg
cs-410_1_2_92,cs-410,1,2, Text Access,"00:06:12,370","00:06:14,760",92,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=372,It's still harder to enter the query.,pic_cs-410_1_2_360.jpg
cs-410_1_2_93,cs-410,1,2, Text Access,"00:06:14,760","00:06:18,840",93,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=374,"In such a case, again,",pic_cs-410_1_2_360.jpg
cs-410_1_2_94,cs-410,1,2, Text Access,"00:06:18,840","00:06:23,060",94,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=378,The relationship between browsing and,pic_cs-410_1_2_360.jpg
cs-410_1_2_95,cs-410,1,2, Text Access,"00:06:23,060","00:06:24,130",95,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=383,imagine you're site seeing.,pic_cs-410_1_2_360.jpg
cs-410_1_2_96,cs-410,1,2, Text Access,"00:06:25,230","00:06:27,080",96,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=385,Imagine if you're touring a city.,pic_cs-410_1_2_360.jpg
cs-410_1_2_97,cs-410,1,2, Text Access,"00:06:27,080","00:06:29,800",97,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=387,Now if you know the exact,pic_cs-410_1_2_360.jpg
cs-410_1_2_98,cs-410,1,2, Text Access,"00:06:31,670","00:06:34,900",98,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=391,Taking a taxi there is,pic_cs-410_1_2_360.jpg
cs-410_1_2_99,cs-410,1,2, Text Access,"00:06:34,900","00:06:36,860",99,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=394,You can go directly to the site.,pic_cs-410_1_2_360.jpg
cs-410_1_2_100,cs-410,1,2, Text Access,"00:06:36,860","00:06:40,440",100,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=396,"But if you don't know the exact address,",pic_cs-410_1_2_360.jpg
cs-410_1_2_101,cs-410,1,2, Text Access,"00:06:40,440","00:06:43,579",101,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=400,Or you can take a taxi to a nearby,pic_cs-410_1_2_360.jpg
cs-410_1_2_102,cs-410,1,2, Text Access,"00:06:44,670","00:06:48,160",102,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=404,It turns out that we do exactly,pic_cs-410_1_2_360.jpg
cs-410_1_2_103,cs-410,1,2, Text Access,"00:06:48,160","00:06:51,480",103,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=408,If you know exactly what you,pic_cs-410_1_2_360.jpg
cs-410_1_2_104,cs-410,1,2, Text Access,"00:06:51,480","00:06:55,360",104,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=411,use the right keywords in your query,pic_cs-410_1_2_360.jpg
cs-410_1_2_105,cs-410,1,2, Text Access,"00:06:55,360","00:06:58,150",105,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=415,"That's usually the fastest way to do,",pic_cs-410_1_2_360.jpg
cs-410_1_2_106,cs-410,1,2, Text Access,"00:06:59,480","00:07:02,180",106,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=419,But what if you don't know,pic_cs-410_1_2_360.jpg
cs-410_1_2_107,cs-410,1,2, Text Access,"00:07:02,180","00:07:04,369",107,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=422,"Well, you clearly probably won't so well.",pic_cs-410_1_2_420.jpg
cs-410_1_2_108,cs-410,1,2, Text Access,"00:07:04,369","00:07:06,150",108,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=424,You will not related pages.,pic_cs-410_1_2_420.jpg
cs-410_1_2_109,cs-410,1,2, Text Access,"00:07:06,150","00:07:10,160",109,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=426,"And then, you need to also walk",pic_cs-410_1_2_420.jpg
cs-410_1_2_110,cs-410,1,2, Text Access,"00:07:10,160","00:07:14,110",110,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=430,meaning by following the links or,pic_cs-410_1_2_420.jpg
cs-410_1_2_111,cs-410,1,2, Text Access,"00:07:14,110","00:07:16,430",111,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=434,You can then finally get,pic_cs-410_1_2_420.jpg
cs-410_1_2_112,cs-410,1,2, Text Access,"00:07:17,580","00:07:20,720",112,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=437,If you want to learn about again.,pic_cs-410_1_2_420.jpg
cs-410_1_2_113,cs-410,1,2, Text Access,"00:07:20,720","00:07:24,610",113,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=440,You will likely do a lot of browsing so,pic_cs-410_1_2_420.jpg
cs-410_1_2_114,cs-410,1,2, Text Access,"00:07:24,610","00:07:29,914",114,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=444,just like you are looking around in,pic_cs-410_1_2_420.jpg
cs-410_1_2_115,cs-410,1,2, Text Access,"00:07:29,914","00:07:36,405",115,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=449,interesting attractions,pic_cs-410_1_2_420.jpg
cs-410_1_2_116,cs-410,1,2, Text Access,"00:07:36,405","00:07:39,200",116,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=456,[INAUDIBLE].,pic_cs-410_1_2_420.jpg
cs-410_1_2_117,cs-410,1,2, Text Access,"00:07:39,200","00:07:45,330",117,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=459,So this analogy also tells us that,pic_cs-410_1_2_420.jpg
cs-410_1_2_118,cs-410,1,2, Text Access,"00:07:45,330","00:07:50,600",118,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=465,"query, but we don't really have",pic_cs-410_1_2_420.jpg
cs-410_1_2_119,cs-410,1,2, Text Access,"00:07:50,600","00:07:54,470",119,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=470,And this is because in order,pic_cs-410_1_2_420.jpg
cs-410_1_2_120,cs-410,1,2, Text Access,"00:07:54,470","00:07:57,840",120,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=474,"we need a map to guide us,",pic_cs-410_1_2_420.jpg
cs-410_1_2_121,cs-410,1,2, Text Access,"00:07:57,840","00:07:58,410",121,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=477,"Of Chicago,",pic_cs-410_1_2_420.jpg
cs-410_1_2_122,cs-410,1,2, Text Access,"00:07:58,410","00:08:04,060",122,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=478,"through the city of Chicago, you need a",pic_cs-410_1_2_420.jpg
cs-410_1_2_123,cs-410,1,2, Text Access,"00:08:04,060","00:08:08,190",123,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=484,So how to construct such a topical,pic_cs-410_1_2_480.jpg
cs-410_1_2_124,cs-410,1,2, Text Access,"00:08:08,190","00:08:12,730",124,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=488,research question that might bring us,pic_cs-410_1_2_480.jpg
cs-410_1_2_125,cs-410,1,2, Text Access,"00:08:12,730","00:08:16,950",125,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=492,more interesting browsing experience,pic_cs-410_1_2_480.jpg
cs-410_1_2_126,cs-410,1,2, Text Access,"00:08:19,170","00:08:21,280",126,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=499,"So, to summarize this lecture,",pic_cs-410_1_2_480.jpg
cs-410_1_2_127,cs-410,1,2, Text Access,"00:08:21,280","00:08:26,550",127,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=501,we've talked about the two high level,pic_cs-410_1_2_480.jpg
cs-410_1_2_128,cs-410,1,2, Text Access,"00:08:26,550","00:08:29,130",128,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=506,Push tends to be supported by,pic_cs-410_1_2_480.jpg
cs-410_1_2_129,cs-410,1,2, Text Access,"00:08:29,130","00:08:31,770",129,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=509,Pull tends to be supported,pic_cs-410_1_2_480.jpg
cs-410_1_2_130,cs-410,1,2, Text Access,"00:08:31,770","00:08:35,710",130,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=511,"Of course, in the sophisticated",pic_cs-410_1_2_480.jpg
cs-410_1_2_131,cs-410,1,2, Text Access,"00:08:35,710","00:08:36,780",131,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=515,we should combine the two.,pic_cs-410_1_2_480.jpg
cs-410_1_2_132,cs-410,1,2, Text Access,"00:08:38,590","00:08:41,830",132,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=518,"In the pull mode, we can further this",pic_cs-410_1_2_480.jpg
cs-410_1_2_133,cs-410,1,2, Text Access,"00:08:41,830","00:08:47,140",133,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=521,Again we generally want to combine,pic_cs-410_1_2_480.jpg
cs-410_1_2_134,cs-410,1,2, Text Access,"00:08:47,140","00:08:50,080",134,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=527,so that you can support,pic_cs-410_1_2_480.jpg
cs-410_1_2_135,cs-410,1,2, Text Access,"00:08:51,220","00:08:55,420",135,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=531,If you want to know more about,pic_cs-410_1_2_480.jpg
cs-410_1_2_136,cs-410,1,2, Text Access,"00:08:55,420","00:08:58,600",136,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=535,"push, you can read this article.",pic_cs-410_1_2_480.jpg
cs-410_1_2_137,cs-410,1,2, Text Access,"00:08:58,600","00:09:03,560",137,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=538,This give excellent discussion of the,pic_cs-410_1_2_480.jpg
cs-410_1_2_138,cs-410,1,2, Text Access,"00:09:03,560","00:09:05,330",138,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=543,information retrieval.,pic_cs-410_1_2_540.jpg
cs-410_1_2_139,cs-410,1,2, Text Access,"00:09:05,330","00:09:10,271",139,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=545,Here informational filtering is similar,pic_cs-410_1_2_540.jpg
cs-410_1_2_140,cs-410,1,2, Text Access,"00:09:10,271","00:09:12,749",140,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=550,the push mode of information access.,pic_cs-410_1_2_540.jpg
cs-410_1_2_141,cs-410,1,2, Text Access,"00:09:12,749","00:09:22,749",141,https://www.coursera.org/learn/cs-410/lecture/OvxTu?t=552,[MUSIC],pic_cs-410_1_2_540.jpg
cs-410_1_3_1,cs-410,1,3, Text Retrieval Problem,"00:00:00,168","00:00:07,728",1,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=0,[MUSIC],pic_cs-410_1_3_0.jpg
cs-410_1_3_2,cs-410,1,3, Text Retrieval Problem,"00:00:07,728","00:00:10,250",2,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=7,This lecture is about,pic_cs-410_1_3_0.jpg
cs-410_1_3_3,cs-410,1,3, Text Retrieval Problem,"00:00:12,820","00:00:15,710",3,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=12,This picture shows our overall plan for,pic_cs-410_1_3_0.jpg
cs-410_1_3_4,cs-410,1,3, Text Retrieval Problem,"00:00:16,780","00:00:21,780",4,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=16,"In the last lecture, we talked about",pic_cs-410_1_3_0.jpg
cs-410_1_3_5,cs-410,1,3, Text Retrieval Problem,"00:00:21,780","00:00:24,150",5,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=21,We talked about push versus pull.,pic_cs-410_1_3_0.jpg
cs-410_1_3_6,cs-410,1,3, Text Retrieval Problem,"00:00:25,350","00:00:30,720",6,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=25,Such engines are the main tools for,pic_cs-410_1_3_0.jpg
cs-410_1_3_7,cs-410,1,3, Text Retrieval Problem,"00:00:30,720","00:00:32,690",7,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=30,"Starting from this lecture,",pic_cs-410_1_3_0.jpg
cs-410_1_3_8,cs-410,1,3, Text Retrieval Problem,"00:00:32,690","00:00:36,270",8,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=32,we're going to talk about the how,pic_cs-410_1_3_0.jpg
cs-410_1_3_9,cs-410,1,3, Text Retrieval Problem,"00:00:38,110","00:00:40,770",9,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=38,So first it's about,pic_cs-410_1_3_0.jpg
cs-410_1_3_10,cs-410,1,3, Text Retrieval Problem,"00:00:42,660","00:00:46,120",10,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=42,We're going to talk about,pic_cs-410_1_3_0.jpg
cs-410_1_3_11,cs-410,1,3, Text Retrieval Problem,"00:00:46,120","00:00:49,650",11,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=46,"First, we define Text Retrieval.",pic_cs-410_1_3_0.jpg
cs-410_1_3_12,cs-410,1,3, Text Retrieval Problem,"00:00:49,650","00:00:54,200",12,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=49,Second we're going to make a comparison,pic_cs-410_1_3_0.jpg
cs-410_1_3_13,cs-410,1,3, Text Retrieval Problem,"00:00:54,200","00:00:56,280",13,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=54,the related task Database Retrieval.,pic_cs-410_1_3_0.jpg
cs-410_1_3_14,cs-410,1,3, Text Retrieval Problem,"00:00:58,240","00:01:02,190",14,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=58,"Finally, we're going to talk about",pic_cs-410_1_3_0.jpg
cs-410_1_3_15,cs-410,1,3, Text Retrieval Problem,"00:01:02,190","00:01:06,508",15,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=62,Document Ranking as two strategies for,pic_cs-410_1_3_60.jpg
cs-410_1_3_16,cs-410,1,3, Text Retrieval Problem,"00:01:09,728","00:01:11,120",16,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=69,So what is Text Retrieval?,pic_cs-410_1_3_60.jpg
cs-410_1_3_17,cs-410,1,3, Text Retrieval Problem,"00:01:12,850","00:01:14,840",17,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=72,It should be a task that's familiar for,pic_cs-410_1_3_60.jpg
cs-410_1_3_18,cs-410,1,3, Text Retrieval Problem,"00:01:14,840","00:01:18,740",18,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=74,the most of us because we're using,pic_cs-410_1_3_60.jpg
cs-410_1_3_19,cs-410,1,3, Text Retrieval Problem,"00:01:19,920","00:01:24,190",19,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=79,So text retrieval is basically a task,pic_cs-410_1_3_60.jpg
cs-410_1_3_20,cs-410,1,3, Text Retrieval Problem,"00:01:24,190","00:01:29,900",20,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=84,where the system would respond to,pic_cs-410_1_3_60.jpg
cs-410_1_3_21,cs-410,1,3, Text Retrieval Problem,"00:01:29,900","00:01:31,540",21,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=89,"Basically, it's for supporting a query",pic_cs-410_1_3_60.jpg
cs-410_1_3_22,cs-410,1,3, Text Retrieval Problem,"00:01:32,730","00:01:37,300",22,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=92,as one way to implement the poll,pic_cs-410_1_3_60.jpg
cs-410_1_3_23,cs-410,1,3, Text Retrieval Problem,"00:01:39,250","00:01:40,940",23,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=99,So the situation is the following.,pic_cs-410_1_3_60.jpg
cs-410_1_3_24,cs-410,1,3, Text Retrieval Problem,"00:01:40,940","00:01:43,590",24,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=100,You have a collection of,pic_cs-410_1_3_60.jpg
cs-410_1_3_25,cs-410,1,3, Text Retrieval Problem,"00:01:43,590","00:01:47,364",25,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=103,These documents could be all,pic_cs-410_1_3_60.jpg
cs-410_1_3_26,cs-410,1,3, Text Retrieval Problem,"00:01:47,364","00:01:50,988",26,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=107,all the literature articles,pic_cs-410_1_3_60.jpg
cs-410_1_3_27,cs-410,1,3, Text Retrieval Problem,"00:01:50,988","00:01:56,528",27,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=110,Or maybe all the text,pic_cs-410_1_3_60.jpg
cs-410_1_3_28,cs-410,1,3, Text Retrieval Problem,"00:01:58,528","00:02:04,340",28,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=118,A user will typically give a query to,pic_cs-410_1_3_60.jpg
cs-410_1_3_29,cs-410,1,3, Text Retrieval Problem,"00:02:04,340","00:02:09,480",29,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=124,"And then, the system would return",pic_cs-410_1_3_120.jpg
cs-410_1_3_30,cs-410,1,3, Text Retrieval Problem,"00:02:09,480","00:02:14,040",30,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=129,Relevant documents refer to those,pic_cs-410_1_3_120.jpg
cs-410_1_3_31,cs-410,1,3, Text Retrieval Problem,"00:02:14,040","00:02:15,500",31,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=134,the user who typed in the query.,pic_cs-410_1_3_120.jpg
cs-410_1_3_32,cs-410,1,3, Text Retrieval Problem,"00:02:16,910","00:02:19,510",32,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=136,All this task is a phone call,pic_cs-410_1_3_120.jpg
cs-410_1_3_33,cs-410,1,3, Text Retrieval Problem,"00:02:21,170","00:02:25,585",33,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=141,But literally information retrieval would,pic_cs-410_1_3_120.jpg
cs-410_1_3_34,cs-410,1,3, Text Retrieval Problem,"00:02:25,585","00:02:30,660",34,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=145,"non-textual information as well,",pic_cs-410_1_3_120.jpg
cs-410_1_3_35,cs-410,1,3, Text Retrieval Problem,"00:02:30,660","00:02:35,960",35,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=150,It's worth noting that,pic_cs-410_1_3_120.jpg
cs-410_1_3_36,cs-410,1,3, Text Retrieval Problem,"00:02:35,960","00:02:41,610",36,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=155,of information retrieval in,pic_cs-410_1_3_120.jpg
cs-410_1_3_37,cs-410,1,3, Text Retrieval Problem,"00:02:41,610","00:02:47,010",37,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=161,video can be retrieved by,pic_cs-410_1_3_120.jpg
cs-410_1_3_38,cs-410,1,3, Text Retrieval Problem,"00:02:47,010","00:02:52,270",38,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=167,"So for example,",pic_cs-410_1_3_120.jpg
cs-410_1_3_39,cs-410,1,3, Text Retrieval Problem,"00:02:52,270","00:02:57,390",39,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=172,match a user's query was,pic_cs-410_1_3_120.jpg
cs-410_1_3_40,cs-410,1,3, Text Retrieval Problem,"00:02:59,850","00:03:03,870",40,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=179,This problem is also,pic_cs-410_1_3_120.jpg
cs-410_1_3_41,cs-410,1,3, Text Retrieval Problem,"00:03:05,550","00:03:08,680",41,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=185,And the technology is often called,pic_cs-410_1_3_180.jpg
cs-410_1_3_42,cs-410,1,3, Text Retrieval Problem,"00:03:11,190","00:03:14,540",42,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=191,If you ever take a course in databases it,pic_cs-410_1_3_180.jpg
cs-410_1_3_43,cs-410,1,3, Text Retrieval Problem,"00:03:14,540","00:03:18,400",43,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=194,will be useful to pause,pic_cs-410_1_3_180.jpg
cs-410_1_3_44,cs-410,1,3, Text Retrieval Problem,"00:03:18,400","00:03:25,200",44,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=198,think about the differences between,pic_cs-410_1_3_180.jpg
cs-410_1_3_45,cs-410,1,3, Text Retrieval Problem,"00:03:25,200","00:03:28,450",45,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=205,Now these two tasks,pic_cs-410_1_3_180.jpg
cs-410_1_3_46,cs-410,1,3, Text Retrieval Problem,"00:03:29,530","00:03:31,928",46,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=209,"But, there are some important differences.",pic_cs-410_1_3_180.jpg
cs-410_1_3_47,cs-410,1,3, Text Retrieval Problem,"00:03:33,708","00:03:38,140",47,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=213,"So, spend a moment to think about",pic_cs-410_1_3_180.jpg
cs-410_1_3_48,cs-410,1,3, Text Retrieval Problem,"00:03:38,140","00:03:43,300",48,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=218,"Think about the data, and the information",pic_cs-410_1_3_180.jpg
cs-410_1_3_49,cs-410,1,3, Text Retrieval Problem,"00:03:43,300","00:03:46,080",49,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=223,those that are managed,pic_cs-410_1_3_180.jpg
cs-410_1_3_50,cs-410,1,3, Text Retrieval Problem,"00:03:47,350","00:03:51,570",50,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=227,Think about the different between,pic_cs-410_1_3_180.jpg
cs-410_1_3_51,cs-410,1,3, Text Retrieval Problem,"00:03:51,570","00:03:57,389",51,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=231,database system versus queries that,pic_cs-410_1_3_180.jpg
cs-410_1_3_52,cs-410,1,3, Text Retrieval Problem,"00:03:59,180","00:04:00,970",52,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=239,And then finally think about the answers.,pic_cs-410_1_3_180.jpg
cs-410_1_3_53,cs-410,1,3, Text Retrieval Problem,"00:04:02,870","00:04:06,980",53,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=242,What's the difference between the two?,pic_cs-410_1_3_240.jpg
cs-410_1_3_54,cs-410,1,3, Text Retrieval Problem,"00:04:06,980","00:04:11,760",54,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=246,"Okay, so if we think about the information",pic_cs-410_1_3_240.jpg
cs-410_1_3_55,cs-410,1,3, Text Retrieval Problem,"00:04:11,760","00:04:14,890",55,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=251,we will see that in text retrieval.,pic_cs-410_1_3_240.jpg
cs-410_1_3_56,cs-410,1,3, Text Retrieval Problem,"00:04:14,890","00:04:18,100",56,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=254,"The data is unstructured, it's free text.",pic_cs-410_1_3_240.jpg
cs-410_1_3_57,cs-410,1,3, Text Retrieval Problem,"00:04:18,100","00:04:24,020",57,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=258,"But in databases, they are structured data",pic_cs-410_1_3_240.jpg
cs-410_1_3_58,cs-410,1,3, Text Retrieval Problem,"00:04:24,020","00:04:30,430",58,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=264,to tell you this column is the names,pic_cs-410_1_3_240.jpg
cs-410_1_3_59,cs-410,1,3, Text Retrieval Problem,"00:04:31,880","00:04:35,020",59,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=271,The unstructured text is not obvious,pic_cs-410_1_3_240.jpg
cs-410_1_3_60,cs-410,1,3, Text Retrieval Problem,"00:04:35,020","00:04:38,420",60,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=275,what are the names of people,pic_cs-410_1_3_240.jpg
cs-410_1_3_61,cs-410,1,3, Text Retrieval Problem,"00:04:40,440","00:04:45,930",61,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=280,"Because of this difference, we also see",pic_cs-410_1_3_240.jpg
cs-410_1_3_62,cs-410,1,3, Text Retrieval Problem,"00:04:45,930","00:04:52,900",62,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=285,ambiguous and we talk about that in the,pic_cs-410_1_3_240.jpg
cs-410_1_3_63,cs-410,1,3, Text Retrieval Problem,"00:04:52,900","00:04:55,500",63,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=292,But they don't tend to have,pic_cs-410_1_3_240.jpg
cs-410_1_3_64,cs-410,1,3, Text Retrieval Problem,"00:04:58,230","00:05:01,990",64,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=298,The results important,pic_cs-410_1_3_240.jpg
cs-410_1_3_65,cs-410,1,3, Text Retrieval Problem,"00:05:01,990","00:05:05,770",65,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=301,this is partly due to the difference,pic_cs-410_1_3_300.jpg
cs-410_1_3_66,cs-410,1,3, Text Retrieval Problem,"00:05:07,610","00:05:10,960",66,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=307,So test queries tend to be ambiguous.,pic_cs-410_1_3_300.jpg
cs-410_1_3_67,cs-410,1,3, Text Retrieval Problem,"00:05:10,960","00:05:16,290",67,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=310,"Whereas in their research,",pic_cs-410_1_3_300.jpg
cs-410_1_3_68,cs-410,1,3, Text Retrieval Problem,"00:05:16,290","00:05:22,330",68,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=316,Think about a SQL query that would clearly,pic_cs-410_1_3_300.jpg
cs-410_1_3_69,cs-410,1,3, Text Retrieval Problem,"00:05:22,330","00:05:24,690",69,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=322,So it has very well-defined semantics.,pic_cs-410_1_3_300.jpg
cs-410_1_3_70,cs-410,1,3, Text Retrieval Problem,"00:05:27,230","00:05:32,252",70,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=327,Keyword queries or electronic queries tend,pic_cs-410_1_3_300.jpg
cs-410_1_3_71,cs-410,1,3, Text Retrieval Problem,"00:05:32,252","00:05:37,952",71,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=332,"to be incomplete,",pic_cs-410_1_3_300.jpg
cs-410_1_3_72,cs-410,1,3, Text Retrieval Problem,"00:05:37,952","00:05:43,390",72,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=337,specify what documents,pic_cs-410_1_3_300.jpg
cs-410_1_3_73,cs-410,1,3, Text Retrieval Problem,"00:05:43,390","00:05:46,370",73,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=343,Whereas complete specification for,pic_cs-410_1_3_300.jpg
cs-410_1_3_74,cs-410,1,3, Text Retrieval Problem,"00:05:47,390","00:05:50,900",74,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=347,"And because of these differences,",pic_cs-410_1_3_300.jpg
cs-410_1_3_75,cs-410,1,3, Text Retrieval Problem,"00:05:50,900","00:05:56,670",75,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=350,"Being the case of text retrieval, we're",pic_cs-410_1_3_300.jpg
cs-410_1_3_76,cs-410,1,3, Text Retrieval Problem,"00:05:58,110","00:06:02,740",76,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=358,"In the database search,",pic_cs-410_1_3_300.jpg
cs-410_1_3_77,cs-410,1,3, Text Retrieval Problem,"00:06:02,740","00:06:07,260",77,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=362,match records with the sequel,pic_cs-410_1_3_360.jpg
cs-410_1_3_78,cs-410,1,3, Text Retrieval Problem,"00:06:09,110","00:06:14,550",78,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=369,"Now in the case of text retrieval,",pic_cs-410_1_3_360.jpg
cs-410_1_3_79,cs-410,1,3, Text Retrieval Problem,"00:06:14,550","00:06:19,950",79,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=374,"to the query is not very well specified,",pic_cs-410_1_3_360.jpg
cs-410_1_3_80,cs-410,1,3, Text Retrieval Problem,"00:06:21,140","00:06:25,830",80,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=381,So it's unclear what should be,pic_cs-410_1_3_360.jpg
cs-410_1_3_81,cs-410,1,3, Text Retrieval Problem,"00:06:25,830","00:06:30,510",81,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=385,"And this has very important consequences,",pic_cs-410_1_3_360.jpg
cs-410_1_3_82,cs-410,1,3, Text Retrieval Problem,"00:06:30,510","00:06:35,108",82,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=390,textual retrieval is,pic_cs-410_1_3_360.jpg
cs-410_1_3_83,cs-410,1,3, Text Retrieval Problem,"00:06:38,578","00:06:44,100",83,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=398,So this is a problem because,pic_cs-410_1_3_360.jpg
cs-410_1_3_84,cs-410,1,3, Text Retrieval Problem,"00:06:44,100","00:06:51,510",84,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=404,then we can not mathematically prove one,pic_cs-410_1_3_360.jpg
cs-410_1_3_85,cs-410,1,3, Text Retrieval Problem,"00:06:52,620","00:06:56,650",85,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=412,That also means we must rely,pic_cs-410_1_3_360.jpg
cs-410_1_3_86,cs-410,1,3, Text Retrieval Problem,"00:06:56,650","00:07:01,120",86,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=416,involving users to know,pic_cs-410_1_3_360.jpg
cs-410_1_3_87,cs-410,1,3, Text Retrieval Problem,"00:07:02,460","00:07:05,080",87,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=422,And that's why we have.,pic_cs-410_1_3_420.jpg
cs-410_1_3_88,cs-410,1,3, Text Retrieval Problem,"00:07:05,080","00:07:09,420",88,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=425,You need more than one lectures,pic_cs-410_1_3_420.jpg
cs-410_1_3_89,cs-410,1,3, Text Retrieval Problem,"00:07:09,420","00:07:12,820",89,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=429,Because this is very important topic for,pic_cs-410_1_3_420.jpg
cs-410_1_3_90,cs-410,1,3, Text Retrieval Problem,"00:07:13,890","00:07:18,902",90,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=433,Without knowing how to evaluate heroism,pic_cs-410_1_3_420.jpg
cs-410_1_3_91,cs-410,1,3, Text Retrieval Problem,"00:07:18,902","00:07:24,563",91,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=438,whether we have got the better or,pic_cs-410_1_3_420.jpg
cs-410_1_3_92,cs-410,1,3, Text Retrieval Problem,"00:07:28,393","00:07:31,155",92,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=448,So now let's look at,pic_cs-410_1_3_420.jpg
cs-410_1_3_93,cs-410,1,3, Text Retrieval Problem,"00:07:32,240","00:07:36,170",93,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=452,"So, this slide shows a formal formulation",pic_cs-410_1_3_420.jpg
cs-410_1_3_94,cs-410,1,3, Text Retrieval Problem,"00:07:37,460","00:07:43,460",94,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=457,"First, we have our vocabulary set, which",pic_cs-410_1_3_420.jpg
cs-410_1_3_95,cs-410,1,3, Text Retrieval Problem,"00:07:44,920","00:07:49,140",95,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=464,"Now here,",pic_cs-410_1_3_420.jpg
cs-410_1_3_96,cs-410,1,3, Text Retrieval Problem,"00:07:49,140","00:07:53,360",96,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=469,"in reality, on the web,",pic_cs-410_1_3_420.jpg
cs-410_1_3_97,cs-410,1,3, Text Retrieval Problem,"00:07:53,360","00:07:56,180",97,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=473,We have texts that are in,pic_cs-410_1_3_420.jpg
cs-410_1_3_98,cs-410,1,3, Text Retrieval Problem,"00:07:57,530","00:08:01,478",98,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=477,"But here for simplicity, we just",pic_cs-410_1_3_420.jpg
cs-410_1_3_99,cs-410,1,3, Text Retrieval Problem,"00:08:01,478","00:08:07,088",99,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=481,As the techniques used for retrieving,pic_cs-410_1_3_480.jpg
cs-410_1_3_100,cs-410,1,3, Text Retrieval Problem,"00:08:07,088","00:08:12,783",100,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=487,less similar to the techniques used for,pic_cs-410_1_3_480.jpg
cs-410_1_3_101,cs-410,1,3, Text Retrieval Problem,"00:08:12,783","00:08:18,819",101,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=492,"although there is important difference,",pic_cs-410_1_3_480.jpg
cs-410_1_3_102,cs-410,1,3, Text Retrieval Problem,"00:08:21,759","00:08:24,725",102,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=501,"Next, we have the query,",pic_cs-410_1_3_480.jpg
cs-410_1_3_103,cs-410,1,3, Text Retrieval Problem,"00:08:26,015","00:08:28,625",103,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=506,"And so here, you can see",pic_cs-410_1_3_480.jpg
cs-410_1_3_104,cs-410,1,3, Text Retrieval Problem,"00:08:31,175","00:08:36,482",104,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=511,the query is defined as,pic_cs-410_1_3_480.jpg
cs-410_1_3_105,cs-410,1,3, Text Retrieval Problem,"00:08:36,482","00:08:41,252",105,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=516,Each q sub i is a word in the vocabulary.,pic_cs-410_1_3_480.jpg
cs-410_1_3_106,cs-410,1,3, Text Retrieval Problem,"00:08:42,302","00:08:47,000",106,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=522,"A document is defined in the same way,",pic_cs-410_1_3_480.jpg
cs-410_1_3_107,cs-410,1,3, Text Retrieval Problem,"00:08:47,000","00:08:51,520",107,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=527,"And here,",pic_cs-410_1_3_480.jpg
cs-410_1_3_108,cs-410,1,3, Text Retrieval Problem,"00:08:52,920","00:08:55,900",108,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=532,"Now typically, the documents",pic_cs-410_1_3_480.jpg
cs-410_1_3_109,cs-410,1,3, Text Retrieval Problem,"00:08:57,100","00:09:01,460",109,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=537,But there are also cases where,pic_cs-410_1_3_480.jpg
cs-410_1_3_110,cs-410,1,3, Text Retrieval Problem,"00:09:04,370","00:09:08,530",110,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=544,So you can think about what,pic_cs-410_1_3_540.jpg
cs-410_1_3_111,cs-410,1,3, Text Retrieval Problem,"00:09:09,670","00:09:13,570",111,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=549,I hope you can think of Twitter search.,pic_cs-410_1_3_540.jpg
cs-410_1_3_112,cs-410,1,3, Text Retrieval Problem,"00:09:13,570","00:09:14,992",112,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=553,Tweets are very short.,pic_cs-410_1_3_540.jpg
cs-410_1_3_113,cs-410,1,3, Text Retrieval Problem,"00:09:16,557","00:09:20,560",113,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=556,"But in general,",pic_cs-410_1_3_540.jpg
cs-410_1_3_114,cs-410,1,3, Text Retrieval Problem,"00:09:22,934","00:09:27,389",114,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=562,"Now, then we have",pic_cs-410_1_3_540.jpg
cs-410_1_3_115,cs-410,1,3, Text Retrieval Problem,"00:09:27,389","00:09:31,240",115,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=567,and this collection can be very large.,pic_cs-410_1_3_540.jpg
cs-410_1_3_116,cs-410,1,3, Text Retrieval Problem,"00:09:31,240","00:09:32,370",116,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=571,So think about the web.,pic_cs-410_1_3_540.jpg
cs-410_1_3_117,cs-410,1,3, Text Retrieval Problem,"00:09:32,370","00:09:33,820",117,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=572,It could be very large.,pic_cs-410_1_3_540.jpg
cs-410_1_3_118,cs-410,1,3, Text Retrieval Problem,"00:09:36,140","00:09:40,300",118,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=576,And then the goal of text retrieval,pic_cs-410_1_3_540.jpg
cs-410_1_3_119,cs-410,1,3, Text Retrieval Problem,"00:09:40,300","00:09:46,358",119,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=580,"the documents, which we denote by R'(q),",pic_cs-410_1_3_540.jpg
cs-410_1_3_120,cs-410,1,3, Text Retrieval Problem,"00:09:46,358","00:09:50,290",120,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=586,"And this in general, a subset of all",pic_cs-410_1_3_540.jpg
cs-410_1_3_121,cs-410,1,3, Text Retrieval Problem,"00:09:52,410","00:09:57,862",121,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=592,"Unfortunately, this set of relevant",pic_cs-410_1_3_540.jpg
cs-410_1_3_122,cs-410,1,3, Text Retrieval Problem,"00:09:57,862","00:10:03,000",122,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=597,"and user-dependent in the sense that,",pic_cs-410_1_3_540.jpg
cs-410_1_3_123,cs-410,1,3, Text Retrieval Problem,"00:10:03,000","00:10:08,110",123,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=603,"in by different users, they expect",pic_cs-410_1_3_600.jpg
cs-410_1_3_124,cs-410,1,3, Text Retrieval Problem,"00:10:09,330","00:10:13,600",124,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=609,The query given to us by,pic_cs-410_1_3_600.jpg
cs-410_1_3_125,cs-410,1,3, Text Retrieval Problem,"00:10:13,600","00:10:15,660",125,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=613,on which document should be in this set.,pic_cs-410_1_3_600.jpg
cs-410_1_3_126,cs-410,1,3, Text Retrieval Problem,"00:10:17,840","00:10:24,940",126,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=617,"And indeed, the user is generally",pic_cs-410_1_3_600.jpg
cs-410_1_3_127,cs-410,1,3, Text Retrieval Problem,"00:10:24,940","00:10:28,940",127,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=624,"be in this set, especially in the case",pic_cs-410_1_3_600.jpg
cs-410_1_3_128,cs-410,1,3, Text Retrieval Problem,"00:10:28,940","00:10:32,540",128,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=628,"large, the user doesn't have complete",pic_cs-410_1_3_600.jpg
cs-410_1_3_129,cs-410,1,3, Text Retrieval Problem,"00:10:34,000","00:10:39,550",129,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=634,So the best search system,pic_cs-410_1_3_600.jpg
cs-410_1_3_130,cs-410,1,3, Text Retrieval Problem,"00:10:39,550","00:10:45,856",130,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=639,an approximation of this,pic_cs-410_1_3_600.jpg
cs-410_1_3_131,cs-410,1,3, Text Retrieval Problem,"00:10:45,856","00:10:50,168",131,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=645,So we denote it by R'(q).,pic_cs-410_1_3_600.jpg
cs-410_1_3_132,cs-410,1,3, Text Retrieval Problem,"00:10:50,168","00:10:54,835",132,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=650,"So formerly,",pic_cs-410_1_3_600.jpg
cs-410_1_3_133,cs-410,1,3, Text Retrieval Problem,"00:10:54,835","00:10:59,849",133,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=654,R'(q) approximation of,pic_cs-410_1_3_600.jpg
cs-410_1_3_134,cs-410,1,3, Text Retrieval Problem,"00:10:59,849","00:11:01,902",134,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=659,So how can we do that?,pic_cs-410_1_3_600.jpg
cs-410_1_3_135,cs-410,1,3, Text Retrieval Problem,"00:11:01,902","00:11:07,290",135,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=661,Now imagine if you are now asked,pic_cs-410_1_3_660.jpg
cs-410_1_3_136,cs-410,1,3, Text Retrieval Problem,"00:11:08,980","00:11:10,480",136,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=668,What would you do?,pic_cs-410_1_3_660.jpg
cs-410_1_3_137,cs-410,1,3, Text Retrieval Problem,"00:11:10,480","00:11:12,526",137,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=670,Now think for a moment.,pic_cs-410_1_3_660.jpg
cs-410_1_3_138,cs-410,1,3, Text Retrieval Problem,"00:11:12,526","00:11:14,433",138,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=672,"Right, so these are your input.",pic_cs-410_1_3_660.jpg
cs-410_1_3_139,cs-410,1,3, Text Retrieval Problem,"00:11:14,433","00:11:18,425",139,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=674,"The query, the documents.",pic_cs-410_1_3_660.jpg
cs-410_1_3_140,cs-410,1,3, Text Retrieval Problem,"00:11:20,399","00:11:24,021",140,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=680,And then you are to compute,pic_cs-410_1_3_660.jpg
cs-410_1_3_141,cs-410,1,3, Text Retrieval Problem,"00:11:24,021","00:11:28,060",141,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=684,which is a set of documents that,pic_cs-410_1_3_660.jpg
cs-410_1_3_142,cs-410,1,3, Text Retrieval Problem,"00:11:29,770","00:11:31,926",142,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=689,"So, how would you solve the problem?",pic_cs-410_1_3_660.jpg
cs-410_1_3_143,cs-410,1,3, Text Retrieval Problem,"00:11:31,926","00:11:37,630",143,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=691,"Now in general,",pic_cs-410_1_3_660.jpg
cs-410_1_3_144,cs-410,1,3, Text Retrieval Problem,"00:11:39,720","00:11:42,970",144,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=699,The first strategy is we do a document,pic_cs-410_1_3_660.jpg
cs-410_1_3_145,cs-410,1,3, Text Retrieval Problem,"00:11:42,970","00:11:47,740",145,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=702,going to have a binary classification,pic_cs-410_1_3_660.jpg
cs-410_1_3_146,cs-410,1,3, Text Retrieval Problem,"00:11:49,350","00:11:52,110",146,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=709,That's a function that,pic_cs-410_1_3_660.jpg
cs-410_1_3_147,cs-410,1,3, Text Retrieval Problem,"00:11:52,110","00:11:55,740",147,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=712,"query as input, and then give a zero or",pic_cs-410_1_3_660.jpg
cs-410_1_3_148,cs-410,1,3, Text Retrieval Problem,"00:11:55,740","00:12:01,170",148,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=715,one as output to indicate whether this,pic_cs-410_1_3_660.jpg
cs-410_1_3_149,cs-410,1,3, Text Retrieval Problem,"00:12:02,330","00:12:05,310",149,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=722,"So in this case, you can see the document.",pic_cs-410_1_3_720.jpg
cs-410_1_3_150,cs-410,1,3, Text Retrieval Problem,"00:12:08,700","00:12:15,130",150,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=728,"The relevant document is set,",pic_cs-410_1_3_720.jpg
cs-410_1_3_151,cs-410,1,3, Text Retrieval Problem,"00:12:15,130","00:12:22,340",151,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=735,"It basically, all the documents that",pic_cs-410_1_3_720.jpg
cs-410_1_3_152,cs-410,1,3, Text Retrieval Problem,"00:12:25,410","00:12:26,040",152,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=745,"So in this case,",pic_cs-410_1_3_720.jpg
cs-410_1_3_153,cs-410,1,3, Text Retrieval Problem,"00:12:26,040","00:12:29,930",153,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=746,you can see the system must have decide,pic_cs-410_1_3_720.jpg
cs-410_1_3_154,cs-410,1,3, Text Retrieval Problem,"00:12:29,930","00:12:33,680",154,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=749,"Basically, it has to say",pic_cs-410_1_3_720.jpg
cs-410_1_3_155,cs-410,1,3, Text Retrieval Problem,"00:12:33,680","00:12:36,330",155,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=753,And this is called absolute relevance.,pic_cs-410_1_3_720.jpg
cs-410_1_3_156,cs-410,1,3, Text Retrieval Problem,"00:12:36,330","00:12:38,940",156,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=756,"Basically, it needs to know",pic_cs-410_1_3_720.jpg
cs-410_1_3_157,cs-410,1,3, Text Retrieval Problem,"00:12:38,940","00:12:39,850",157,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=758,useful to the user.,pic_cs-410_1_3_720.jpg
cs-410_1_3_158,cs-410,1,3, Text Retrieval Problem,"00:12:41,940","00:12:44,910",158,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=761,"Alternatively, there's another",pic_cs-410_1_3_720.jpg
cs-410_1_3_159,cs-410,1,3, Text Retrieval Problem,"00:12:46,160","00:12:47,150",159,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=766,"Now in this case,",pic_cs-410_1_3_720.jpg
cs-410_1_3_160,cs-410,1,3, Text Retrieval Problem,"00:12:47,150","00:12:52,290",160,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=767,the system is not going to make a call,pic_cs-410_1_3_720.jpg
cs-410_1_3_161,cs-410,1,3, Text Retrieval Problem,"00:12:52,290","00:12:57,230",161,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=772,But rather the system is going to,pic_cs-410_1_3_720.jpg
cs-410_1_3_162,cs-410,1,3, Text Retrieval Problem,"00:12:58,440","00:13:01,540",162,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=778,That would simply give us a value,pic_cs-410_1_3_720.jpg
cs-410_1_3_163,cs-410,1,3, Text Retrieval Problem,"00:13:01,540","00:13:04,170",163,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=781,that would indicate which,pic_cs-410_1_3_780.jpg
cs-410_1_3_164,cs-410,1,3, Text Retrieval Problem,"00:13:05,740","00:13:08,590",164,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=785,So it's not going to make a call whether,pic_cs-410_1_3_780.jpg
cs-410_1_3_165,cs-410,1,3, Text Retrieval Problem,"00:13:08,590","00:13:12,696",165,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=788,But rather it would say which,pic_cs-410_1_3_780.jpg
cs-410_1_3_166,cs-410,1,3, Text Retrieval Problem,"00:13:12,696","00:13:17,669",166,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=792,So this function then can be,pic_cs-410_1_3_780.jpg
cs-410_1_3_167,cs-410,1,3, Text Retrieval Problem,"00:13:17,669","00:13:22,135",167,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=797,then we're going to let,pic_cs-410_1_3_780.jpg
cs-410_1_3_168,cs-410,1,3, Text Retrieval Problem,"00:13:22,135","00:13:25,296",168,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=802,when the user looks at the document.,pic_cs-410_1_3_780.jpg
cs-410_1_3_169,cs-410,1,3, Text Retrieval Problem,"00:13:25,296","00:13:31,410",169,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=805,So we have a threshold theta,pic_cs-410_1_3_780.jpg
cs-410_1_3_170,cs-410,1,3, Text Retrieval Problem,"00:13:31,410","00:13:37,398",170,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=811,documents should be in,pic_cs-410_1_3_780.jpg
cs-410_1_3_171,cs-410,1,3, Text Retrieval Problem,"00:13:37,398","00:13:40,802",171,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=817,And we're going to assume,pic_cs-410_1_3_780.jpg
cs-410_1_3_172,cs-410,1,3, Text Retrieval Problem,"00:13:40,802","00:13:45,312",172,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=820,are ranked above the threshold,pic_cs-410_1_3_780.jpg
cs-410_1_3_173,cs-410,1,3, Text Retrieval Problem,"00:13:45,312","00:13:49,780",173,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=825,these are the documents that,pic_cs-410_1_3_780.jpg
cs-410_1_3_174,cs-410,1,3, Text Retrieval Problem,"00:13:49,780","00:13:54,490",174,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=829,And theta is a cutoff,pic_cs-410_1_3_780.jpg
cs-410_1_3_175,cs-410,1,3, Text Retrieval Problem,"00:13:56,980","00:14:00,980",175,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=836,So here we've got some collaboration,pic_cs-410_1_3_780.jpg
cs-410_1_3_176,cs-410,1,3, Text Retrieval Problem,"00:14:00,980","00:14:03,330",176,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=840,because we don't really make a cutoff.,pic_cs-410_1_3_840.jpg
cs-410_1_3_177,cs-410,1,3, Text Retrieval Problem,"00:14:03,330","00:14:07,060",177,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=843,And the user kind of helped,pic_cs-410_1_3_840.jpg
cs-410_1_3_178,cs-410,1,3, Text Retrieval Problem,"00:14:08,120","00:14:10,950",178,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=848,"So in this case,",pic_cs-410_1_3_840.jpg
cs-410_1_3_179,cs-410,1,3, Text Retrieval Problem,"00:14:10,950","00:14:14,440",179,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=850,if one document is more,pic_cs-410_1_3_840.jpg
cs-410_1_3_180,cs-410,1,3, Text Retrieval Problem,"00:14:14,440","00:14:17,990",180,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=854,"And that is, it only needs to",pic_cs-410_1_3_840.jpg
cs-410_1_3_181,cs-410,1,3, Text Retrieval Problem,"00:14:19,050","00:14:20,770",181,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=859,as opposed to absolute relevance.,pic_cs-410_1_3_840.jpg
cs-410_1_3_182,cs-410,1,3, Text Retrieval Problem,"00:14:22,230","00:14:26,290",182,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=862,Now you can probably already sense that,pic_cs-410_1_3_840.jpg
cs-410_1_3_183,cs-410,1,3, Text Retrieval Problem,"00:14:26,290","00:14:31,560",183,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=866,relative relevance would be easier to,pic_cs-410_1_3_840.jpg
cs-410_1_3_184,cs-410,1,3, Text Retrieval Problem,"00:14:31,560","00:14:32,800",184,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=871,"Because in the first case,",pic_cs-410_1_3_840.jpg
cs-410_1_3_185,cs-410,1,3, Text Retrieval Problem,"00:14:32,800","00:14:36,420",185,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=872,we have to say exactly whether,pic_cs-410_1_3_840.jpg
cs-410_1_3_186,cs-410,1,3, Text Retrieval Problem,"00:14:37,990","00:14:45,500",186,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=877,And it turns out that ranking is indeed,pic_cs-410_1_3_840.jpg
cs-410_1_3_187,cs-410,1,3, Text Retrieval Problem,"00:14:46,710","00:14:50,240",187,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=886,So let's look at these two,pic_cs-410_1_3_840.jpg
cs-410_1_3_188,cs-410,1,3, Text Retrieval Problem,"00:14:50,240","00:14:53,960",188,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=890,So this picture shows how it works.,pic_cs-410_1_3_840.jpg
cs-410_1_3_189,cs-410,1,3, Text Retrieval Problem,"00:14:53,960","00:14:58,780",189,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=893,"So on the left side,",pic_cs-410_1_3_840.jpg
cs-410_1_3_190,cs-410,1,3, Text Retrieval Problem,"00:14:58,780","00:15:02,710",190,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=898,we use the pluses to indicate,pic_cs-410_1_3_840.jpg
cs-410_1_3_191,cs-410,1,3, Text Retrieval Problem,"00:15:02,710","00:15:09,990",191,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=902,So we can see the true relevant,pic_cs-410_1_3_900.jpg
cs-410_1_3_192,cs-410,1,3, Text Retrieval Problem,"00:15:09,990","00:15:15,210",192,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=909,"of true relevant documents, consists",pic_cs-410_1_3_900.jpg
cs-410_1_3_193,cs-410,1,3, Text Retrieval Problem,"00:15:17,450","00:15:20,972",193,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=917,"And with the document selection function,",pic_cs-410_1_3_900.jpg
cs-410_1_3_194,cs-410,1,3, Text Retrieval Problem,"00:15:20,972","00:15:25,636",194,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=920,we're going to basically,pic_cs-410_1_3_900.jpg
cs-410_1_3_195,cs-410,1,3, Text Retrieval Problem,"00:15:25,636","00:15:30,050",195,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=925,"relevant documents, and non-relevant ones.",pic_cs-410_1_3_900.jpg
cs-410_1_3_196,cs-410,1,3, Text Retrieval Problem,"00:15:30,050","00:15:34,700",196,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=930,"Of course, the classified will not",pic_cs-410_1_3_900.jpg
cs-410_1_3_197,cs-410,1,3, Text Retrieval Problem,"00:15:34,700","00:15:39,522",197,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=934,"So here we can see, in the approximation",pic_cs-410_1_3_900.jpg
cs-410_1_3_198,cs-410,1,3, Text Retrieval Problem,"00:15:39,522","00:15:41,660",198,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=939,we have got some number in the documents.,pic_cs-410_1_3_900.jpg
cs-410_1_3_199,cs-410,1,3, Text Retrieval Problem,"00:15:43,090","00:15:44,168",199,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=943,"And similarly,",pic_cs-410_1_3_900.jpg
cs-410_1_3_200,cs-410,1,3, Text Retrieval Problem,"00:15:44,168","00:15:48,868",200,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=944,there is a relevant document that's,pic_cs-410_1_3_900.jpg
cs-410_1_3_201,cs-410,1,3, Text Retrieval Problem,"00:15:48,868","00:15:53,972",201,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=948,"In the case of document ranking,",pic_cs-410_1_3_900.jpg
cs-410_1_3_202,cs-410,1,3, Text Retrieval Problem,"00:15:53,972","00:15:59,368",202,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=953,simply ranks all the documents in,pic_cs-410_1_3_900.jpg
cs-410_1_3_203,cs-410,1,3, Text Retrieval Problem,"00:15:59,368","00:16:04,580",203,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=959,"And then, we're going to let the user",pic_cs-410_1_3_900.jpg
cs-410_1_3_204,cs-410,1,3, Text Retrieval Problem,"00:16:04,580","00:16:07,510",204,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=964,If the user wants to,pic_cs-410_1_3_960.jpg
cs-410_1_3_205,cs-410,1,3, Text Retrieval Problem,"00:16:07,510","00:16:11,630",205,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=967,then the user will scroll down some,pic_cs-410_1_3_960.jpg
cs-410_1_3_206,cs-410,1,3, Text Retrieval Problem,"00:16:11,630","00:16:17,010",206,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=971,But if the user only wants to,pic_cs-410_1_3_960.jpg
cs-410_1_3_207,cs-410,1,3, Text Retrieval Problem,"00:16:17,010","00:16:20,750",207,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=977,the user might stop at the top position.,pic_cs-410_1_3_960.jpg
cs-410_1_3_208,cs-410,1,3, Text Retrieval Problem,"00:16:20,750","00:16:24,200",208,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=980,"So in this case, the user stops at d4.",pic_cs-410_1_3_960.jpg
cs-410_1_3_209,cs-410,1,3, Text Retrieval Problem,"00:16:24,200","00:16:30,950",209,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=984,"So in fact, we have delivered",pic_cs-410_1_3_960.jpg
cs-410_1_3_210,cs-410,1,3, Text Retrieval Problem,"00:16:33,940","00:16:40,300",210,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=993,So as I said ranking is generally,pic_cs-410_1_3_960.jpg
cs-410_1_3_211,cs-410,1,3, Text Retrieval Problem,"00:16:40,300","00:16:46,410",211,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1000,because the classifier in the case of,pic_cs-410_1_3_960.jpg
cs-410_1_3_212,cs-410,1,3, Text Retrieval Problem,"00:16:46,410","00:16:47,790",212,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1006,Why?,pic_cs-410_1_3_960.jpg
cs-410_1_3_213,cs-410,1,3, Text Retrieval Problem,"00:16:47,790","00:16:51,100",213,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1007,Because the only clue,pic_cs-410_1_3_960.jpg
cs-410_1_3_214,cs-410,1,3, Text Retrieval Problem,"00:16:51,100","00:16:56,550",214,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1011,But the query may not be accurate in the,pic_cs-410_1_3_960.jpg
cs-410_1_3_215,cs-410,1,3, Text Retrieval Problem,"00:16:57,660","00:17:02,860",215,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1017,"For example, you might expect relevant",pic_cs-410_1_3_960.jpg
cs-410_1_3_216,cs-410,1,3, Text Retrieval Problem,"00:17:04,460","00:17:08,050",216,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1024,topics by using specific vocabulary.,pic_cs-410_1_3_1020.jpg
cs-410_1_3_217,cs-410,1,3, Text Retrieval Problem,"00:17:08,050","00:17:13,550",217,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1028,"And as a result,",pic_cs-410_1_3_1020.jpg
cs-410_1_3_218,cs-410,1,3, Text Retrieval Problem,"00:17:13,550","00:17:15,690",218,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1033,"Because in the collection,",pic_cs-410_1_3_1020.jpg
cs-410_1_3_219,cs-410,1,3, Text Retrieval Problem,"00:17:15,690","00:17:19,990",219,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1035,no others have discussed the topic,pic_cs-410_1_3_1020.jpg
cs-410_1_3_220,cs-410,1,3, Text Retrieval Problem,"00:17:19,990","00:17:24,380",220,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1039,"So in this case,",pic_cs-410_1_3_1020.jpg
cs-410_1_3_221,cs-410,1,3, Text Retrieval Problem,"00:17:25,970","00:17:31,980",221,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1045,no relevant documents to return in,pic_cs-410_1_3_1020.jpg
cs-410_1_3_222,cs-410,1,3, Text Retrieval Problem,"00:17:33,230","00:17:37,892",222,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1053,"On the other hand,",pic_cs-410_1_3_1020.jpg
cs-410_1_3_223,cs-410,1,3, Text Retrieval Problem,"00:17:37,892","00:17:40,430",223,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1057,"for example, if the query",pic_cs-410_1_3_1020.jpg
cs-410_1_3_224,cs-410,1,3, Text Retrieval Problem,"00:17:40,430","00:17:44,610",224,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1060,does not have sufficient descriptive,pic_cs-410_1_3_1020.jpg
cs-410_1_3_225,cs-410,1,3, Text Retrieval Problem,"00:17:44,610","00:17:51,100",225,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1064,You may actually end up having of,pic_cs-410_1_3_1020.jpg
cs-410_1_3_226,cs-410,1,3, Text Retrieval Problem,"00:17:51,100","00:17:55,840",226,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1071,thought these words my be sufficient,pic_cs-410_1_3_1020.jpg
cs-410_1_3_227,cs-410,1,3, Text Retrieval Problem,"00:17:55,840","00:17:58,590",227,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1075,"But, it turns out they",pic_cs-410_1_3_1020.jpg
cs-410_1_3_228,cs-410,1,3, Text Retrieval Problem,"00:17:58,590","00:18:04,280",228,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1078,"there are many distractions,",pic_cs-410_1_3_1020.jpg
cs-410_1_3_229,cs-410,1,3, Text Retrieval Problem,"00:18:04,280","00:18:07,450",229,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1084,"And so, this is a case of over delivery.",pic_cs-410_1_3_1080.jpg
cs-410_1_3_230,cs-410,1,3, Text Retrieval Problem,"00:18:08,570","00:18:13,910",230,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1088,"Unfortunately, it's very hard to find the",pic_cs-410_1_3_1080.jpg
cs-410_1_3_231,cs-410,1,3, Text Retrieval Problem,"00:18:15,390","00:18:15,900",231,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1095,Why?,pic_cs-410_1_3_1080.jpg
cs-410_1_3_232,cs-410,1,3, Text Retrieval Problem,"00:18:15,900","00:18:19,940",232,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1095,Because whether users looking for,pic_cs-410_1_3_1080.jpg
cs-410_1_3_233,cs-410,1,3, Text Retrieval Problem,"00:18:19,940","00:18:24,520",233,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1099,not have a good knowledge about,pic_cs-410_1_3_1080.jpg
cs-410_1_3_234,cs-410,1,3, Text Retrieval Problem,"00:18:24,520","00:18:28,800",234,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1104,"And in that case, the user does not",pic_cs-410_1_3_1080.jpg
cs-410_1_3_235,cs-410,1,3, Text Retrieval Problem,"00:18:30,240","00:18:33,770",235,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1110,vocabularies will be used in,pic_cs-410_1_3_1080.jpg
cs-410_1_3_236,cs-410,1,3, Text Retrieval Problem,"00:18:33,770","00:18:36,064",236,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1113,So it's very hard for,pic_cs-410_1_3_1080.jpg
cs-410_1_3_237,cs-410,1,3, Text Retrieval Problem,"00:18:36,064","00:18:42,061",237,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1116,a user to pre-specify the right,pic_cs-410_1_3_1080.jpg
cs-410_1_3_238,cs-410,1,3, Text Retrieval Problem,"00:18:44,569","00:18:49,502",238,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1124,"Even if the classifier is accurate,",pic_cs-410_1_3_1080.jpg
cs-410_1_3_239,cs-410,1,3, Text Retrieval Problem,"00:18:49,502","00:18:54,880",239,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1129,"relevant documents, because they",pic_cs-410_1_3_1080.jpg
cs-410_1_3_240,cs-410,1,3, Text Retrieval Problem,"00:18:56,130","00:18:58,330",240,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1136,Relevance is often a matter of degree.,pic_cs-410_1_3_1080.jpg
cs-410_1_3_241,cs-410,1,3, Text Retrieval Problem,"00:18:59,560","00:19:05,250",241,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1139,So we must prioritize these documents for,pic_cs-410_1_3_1080.jpg
cs-410_1_3_242,cs-410,1,3, Text Retrieval Problem,"00:19:06,320","00:19:10,690",242,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1146,And note that this,pic_cs-410_1_3_1140.jpg
cs-410_1_3_243,cs-410,1,3, Text Retrieval Problem,"00:19:12,300","00:19:15,840",243,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1152,because a user cannot,pic_cs-410_1_3_1140.jpg
cs-410_1_3_244,cs-410,1,3, Text Retrieval Problem,"00:19:15,840","00:19:20,720",244,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1155,the user generally would have to,pic_cs-410_1_3_1140.jpg
cs-410_1_3_245,cs-410,1,3, Text Retrieval Problem,"00:19:21,750","00:19:29,220",245,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1161,"And therefore, it would make sense to",pic_cs-410_1_3_1140.jpg
cs-410_1_3_246,cs-410,1,3, Text Retrieval Problem,"00:19:29,220","00:19:32,100",246,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1169,And that's what ranking is doing.,pic_cs-410_1_3_1140.jpg
cs-410_1_3_247,cs-410,1,3, Text Retrieval Problem,"00:19:32,100","00:19:35,240",247,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1172,"So for these reasons,",pic_cs-410_1_3_1140.jpg
cs-410_1_3_248,cs-410,1,3, Text Retrieval Problem,"00:19:36,330","00:19:39,610",248,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1176,Now this preference also has,pic_cs-410_1_3_1140.jpg
cs-410_1_3_249,cs-410,1,3, Text Retrieval Problem,"00:19:39,610","00:19:42,330",249,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1179,this is given by the probability,pic_cs-410_1_3_1140.jpg
cs-410_1_3_250,cs-410,1,3, Text Retrieval Problem,"00:19:44,210","00:19:47,930",250,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1184,"In the end of this lecture,",pic_cs-410_1_3_1140.jpg
cs-410_1_3_251,cs-410,1,3, Text Retrieval Problem,"00:19:49,320","00:19:54,260",251,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1189,"This principle says, returning a ranked",pic_cs-410_1_3_1140.jpg
cs-410_1_3_252,cs-410,1,3, Text Retrieval Problem,"00:19:54,260","00:19:57,590",252,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1194,of probability that a document,pic_cs-410_1_3_1140.jpg
cs-410_1_3_253,cs-410,1,3, Text Retrieval Problem,"00:19:57,590","00:20:01,270",253,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1197,is the optimal strategy under,pic_cs-410_1_3_1140.jpg
cs-410_1_3_254,cs-410,1,3, Text Retrieval Problem,"00:20:02,620","00:20:05,690",254,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1202,"First, the utility of",pic_cs-410_1_3_1200.jpg
cs-410_1_3_255,cs-410,1,3, Text Retrieval Problem,"00:20:05,690","00:20:09,280",255,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1205,Is independent of the utility,pic_cs-410_1_3_1200.jpg
cs-410_1_3_256,cs-410,1,3, Text Retrieval Problem,"00:20:10,980","00:20:15,300",256,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1210,"Second, a user would be assumed to",pic_cs-410_1_3_1200.jpg
cs-410_1_3_257,cs-410,1,3, Text Retrieval Problem,"00:20:15,300","00:20:21,775",257,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1215,Now it's easy to understand why these,pic_cs-410_1_3_1200.jpg
cs-410_1_3_258,cs-410,1,3, Text Retrieval Problem,"00:20:21,775","00:20:27,130",258,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1221,Site for the ranking strategy.,pic_cs-410_1_3_1200.jpg
cs-410_1_3_259,cs-410,1,3, Text Retrieval Problem,"00:20:27,130","00:20:30,930",259,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1227,"Because if the documents are independent,",pic_cs-410_1_3_1200.jpg
cs-410_1_3_260,cs-410,1,3, Text Retrieval Problem,"00:20:30,930","00:20:34,270",260,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1230,then we can evaluate the utility,pic_cs-410_1_3_1200.jpg
cs-410_1_3_261,cs-410,1,3, Text Retrieval Problem,"00:20:36,350","00:20:40,270",261,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1236,And this would allow the computer,pic_cs-410_1_3_1200.jpg
cs-410_1_3_262,cs-410,1,3, Text Retrieval Problem,"00:20:40,270","00:20:43,920",262,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1240,"And then, we are going to rank these",pic_cs-410_1_3_1200.jpg
cs-410_1_3_263,cs-410,1,3, Text Retrieval Problem,"00:20:45,710","00:20:51,300",263,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1245,The second assumption is to say that the,pic_cs-410_1_3_1200.jpg
cs-410_1_3_264,cs-410,1,3, Text Retrieval Problem,"00:20:51,300","00:20:55,050",264,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1251,If the user is not going to follow,pic_cs-410_1_3_1200.jpg
cs-410_1_3_265,cs-410,1,3, Text Retrieval Problem,"00:20:55,050","00:20:59,440",265,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1255,"the documents sequentially, then obviously",pic_cs-410_1_3_1200.jpg
cs-410_1_3_266,cs-410,1,3, Text Retrieval Problem,"00:21:00,560","00:21:08,270",266,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1260,"So under these two assumptions, we can",pic_cs-410_1_3_1260.jpg
cs-410_1_3_267,cs-410,1,3, Text Retrieval Problem,"00:21:08,270","00:21:13,170",267,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1268,"is, in fact, the best that you could do.",pic_cs-410_1_3_1260.jpg
cs-410_1_3_268,cs-410,1,3, Text Retrieval Problem,"00:21:13,170","00:21:14,700",268,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1273,"Now, I've put one question here.",pic_cs-410_1_3_1260.jpg
cs-410_1_3_269,cs-410,1,3, Text Retrieval Problem,"00:21:14,700","00:21:16,330",269,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1274,Do these two assumptions hold?,pic_cs-410_1_3_1260.jpg
cs-410_1_3_270,cs-410,1,3, Text Retrieval Problem,"00:21:18,240","00:21:23,110",270,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1278,"I suggest you to pause the lecture,",pic_cs-410_1_3_1260.jpg
cs-410_1_3_271,cs-410,1,3, Text Retrieval Problem,"00:21:27,950","00:21:33,065",271,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1287,"Now, can you think of",pic_cs-410_1_3_1260.jpg
cs-410_1_3_272,cs-410,1,3, Text Retrieval Problem,"00:21:33,065","00:21:39,238",272,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1293,suggest these assumptions,pic_cs-410_1_3_1260.jpg
cs-410_1_3_273,cs-410,1,3, Text Retrieval Problem,"00:21:44,462","00:21:46,953",273,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1304,"Now, if you think for a moment,",pic_cs-410_1_3_1260.jpg
cs-410_1_3_274,cs-410,1,3, Text Retrieval Problem,"00:21:46,953","00:21:51,490",274,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1306,you may realize none of,pic_cs-410_1_3_1260.jpg
cs-410_1_3_275,cs-410,1,3, Text Retrieval Problem,"00:21:53,230","00:21:57,690",275,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1313,"For example, in the case of",pic_cs-410_1_3_1260.jpg
cs-410_1_3_276,cs-410,1,3, Text Retrieval Problem,"00:21:57,690","00:22:02,590",276,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1317,have documents that have similar or,pic_cs-410_1_3_1260.jpg
cs-410_1_3_277,cs-410,1,3, Text Retrieval Problem,"00:22:02,590","00:22:06,620",277,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1322,"If we look at each of them alone,",pic_cs-410_1_3_1320.jpg
cs-410_1_3_278,cs-410,1,3, Text Retrieval Problem,"00:22:07,790","00:22:12,510",278,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1327,But if the user has already seen,pic_cs-410_1_3_1320.jpg
cs-410_1_3_279,cs-410,1,3, Text Retrieval Problem,"00:22:12,510","00:22:17,240",279,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1332,generally not very useful for the user to,pic_cs-410_1_3_1320.jpg
cs-410_1_3_280,cs-410,1,3, Text Retrieval Problem,"00:22:19,030","00:22:22,040",280,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1339,So clearly the utility,pic_cs-410_1_3_1320.jpg
cs-410_1_3_281,cs-410,1,3, Text Retrieval Problem,"00:22:22,040","00:22:25,680",281,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1342,is dependent on other documents,pic_cs-410_1_3_1320.jpg
cs-410_1_3_282,cs-410,1,3, Text Retrieval Problem,"00:22:27,350","00:22:32,510",282,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1347,In some other cases you might see,pic_cs-410_1_3_1320.jpg
cs-410_1_3_283,cs-410,1,3, Text Retrieval Problem,"00:22:32,510","00:22:38,490",283,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1352,"be useful to the user, but when three",pic_cs-410_1_3_1320.jpg
cs-410_1_3_284,cs-410,1,3, Text Retrieval Problem,"00:22:38,490","00:22:41,070",284,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1358,They provide answers to,pic_cs-410_1_3_1320.jpg
cs-410_1_3_285,cs-410,1,3, Text Retrieval Problem,"00:22:42,140","00:22:46,883",285,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1362,So this is a collective relevance and,pic_cs-410_1_3_1320.jpg
cs-410_1_3_286,cs-410,1,3, Text Retrieval Problem,"00:22:46,883","00:22:51,542",286,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1366,the value of the document might,pic_cs-410_1_3_1320.jpg
cs-410_1_3_287,cs-410,1,3, Text Retrieval Problem,"00:22:53,329","00:22:58,100",287,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1373,Sequential browsing generally would make,pic_cs-410_1_3_1320.jpg
cs-410_1_3_288,cs-410,1,3, Text Retrieval Problem,"00:22:59,220","00:23:04,650",288,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1379,"But even if you have a rank list,",pic_cs-410_1_3_1320.jpg
cs-410_1_3_289,cs-410,1,3, Text Retrieval Problem,"00:23:04,650","00:23:10,140",289,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1384,users don't always just go strictly,pic_cs-410_1_3_1380.jpg
cs-410_1_3_290,cs-410,1,3, Text Retrieval Problem,"00:23:10,140","00:23:14,910",290,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1390,They sometimes will look at the bottom for,pic_cs-410_1_3_1380.jpg
cs-410_1_3_291,cs-410,1,3, Text Retrieval Problem,"00:23:14,910","00:23:17,810",291,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1394,And if you think about the more,pic_cs-410_1_3_1380.jpg
cs-410_1_3_292,cs-410,1,3, Text Retrieval Problem,"00:23:17,810","00:23:22,100",292,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1397,we could possibly use like,pic_cs-410_1_3_1380.jpg
cs-410_1_3_293,cs-410,1,3, Text Retrieval Problem,"00:23:22,100","00:23:26,820",293,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1402,Where you can put that additional,pic_cs-410_1_3_1380.jpg
cs-410_1_3_294,cs-410,1,3, Text Retrieval Problem,"00:23:26,820","00:23:29,379",294,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1406,sequential browsing is a very,pic_cs-410_1_3_1380.jpg
cs-410_1_3_295,cs-410,1,3, Text Retrieval Problem,"00:23:32,010","00:23:34,300",295,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1412,So the point here is that,pic_cs-410_1_3_1380.jpg
cs-410_1_3_296,cs-410,1,3, Text Retrieval Problem,"00:23:35,740","00:23:39,990",296,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1415,none of these assumptions is,pic_cs-410_1_3_1380.jpg
cs-410_1_3_297,cs-410,1,3, Text Retrieval Problem,"00:23:41,100","00:23:45,290",297,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1421,But probability ranking principle,pic_cs-410_1_3_1380.jpg
cs-410_1_3_298,cs-410,1,3, Text Retrieval Problem,"00:23:46,870","00:23:51,020",298,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1426,ranking as a primary pattern for,pic_cs-410_1_3_1380.jpg
cs-410_1_3_299,cs-410,1,3, Text Retrieval Problem,"00:23:51,020","00:23:53,180",299,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1431,And this has actually been the basis for,pic_cs-410_1_3_1380.jpg
cs-410_1_3_300,cs-410,1,3, Text Retrieval Problem,"00:23:53,180","00:23:57,090",300,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1433,a lot of research work in,pic_cs-410_1_3_1380.jpg
cs-410_1_3_301,cs-410,1,3, Text Retrieval Problem,"00:23:57,090","00:24:00,530",301,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1437,And many hours have been designed,pic_cs-410_1_3_1380.jpg
cs-410_1_3_302,cs-410,1,3, Text Retrieval Problem,"00:24:01,590","00:24:06,410",302,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1441,despite that the assumptions,pic_cs-410_1_3_1440.jpg
cs-410_1_3_303,cs-410,1,3, Text Retrieval Problem,"00:24:06,410","00:24:11,570",303,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1446,And we can address this problem,pic_cs-410_1_3_1440.jpg
cs-410_1_3_304,cs-410,1,3, Text Retrieval Problem,"00:24:11,570","00:24:15,430",304,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1451,"Of a ranked list, for example,",pic_cs-410_1_3_1440.jpg
cs-410_1_3_305,cs-410,1,3, Text Retrieval Problem,"00:24:20,260","00:24:22,500",305,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1460,"So to summarize this lecture,",pic_cs-410_1_3_1440.jpg
cs-410_1_3_306,cs-410,1,3, Text Retrieval Problem,"00:24:22,500","00:24:28,000",306,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1462,the main points that you can,pic_cs-410_1_3_1440.jpg
cs-410_1_3_307,cs-410,1,3, Text Retrieval Problem,"00:24:28,000","00:24:31,760",307,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1468,"First, text retrieval is",pic_cs-410_1_3_1440.jpg
cs-410_1_3_308,cs-410,1,3, Text Retrieval Problem,"00:24:31,760","00:24:37,951",308,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1471,And that means which algorithm is,pic_cs-410_1_3_1440.jpg
cs-410_1_3_309,cs-410,1,3, Text Retrieval Problem,"00:24:37,951","00:24:42,500",309,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1477,"Second, document ranking",pic_cs-410_1_3_1440.jpg
cs-410_1_3_310,cs-410,1,3, Text Retrieval Problem,"00:24:42,500","00:24:46,180",310,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1482,And this will help users prioritize,pic_cs-410_1_3_1440.jpg
cs-410_1_3_311,cs-410,1,3, Text Retrieval Problem,"00:24:47,410","00:24:52,693",311,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1487,And this is also to bypass the difficulty,pic_cs-410_1_3_1440.jpg
cs-410_1_3_312,cs-410,1,3, Text Retrieval Problem,"00:24:52,693","00:24:58,221",312,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1492,Because we can get some help from users,pic_cs-410_1_3_1440.jpg
cs-410_1_3_313,cs-410,1,3, Text Retrieval Problem,"00:24:58,221","00:24:59,809",313,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1498,it's more flexible.,pic_cs-410_1_3_1440.jpg
cs-410_1_3_314,cs-410,1,3, Text Retrieval Problem,"00:25:01,967","00:25:06,904",314,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1501,"So, this further suggests that the main",pic_cs-410_1_3_1500.jpg
cs-410_1_3_315,cs-410,1,3, Text Retrieval Problem,"00:25:06,904","00:25:09,950",315,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1506,engine is the design,pic_cs-410_1_3_1500.jpg
cs-410_1_3_316,cs-410,1,3, Text Retrieval Problem,"00:25:10,970","00:25:16,150",316,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1510,"In other words, we need to define",pic_cs-410_1_3_1500.jpg
cs-410_1_3_317,cs-410,1,3, Text Retrieval Problem,"00:25:16,150","00:25:19,480",317,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1516,on the query and document pair.,pic_cs-410_1_3_1500.jpg
cs-410_1_3_318,cs-410,1,3, Text Retrieval Problem,"00:25:21,360","00:25:26,151",318,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1521,How we design such a function is the main,pic_cs-410_1_3_1500.jpg
cs-410_1_3_319,cs-410,1,3, Text Retrieval Problem,"00:25:29,123","00:25:32,060",319,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1529,There are two suggested,pic_cs-410_1_3_1500.jpg
cs-410_1_3_320,cs-410,1,3, Text Retrieval Problem,"00:25:32,060","00:25:36,180",320,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1532,The first is the classical paper on,pic_cs-410_1_3_1500.jpg
cs-410_1_3_321,cs-410,1,3, Text Retrieval Problem,"00:25:37,470","00:25:42,380",321,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1537,The second one is a must-read for anyone,pic_cs-410_1_3_1500.jpg
cs-410_1_3_322,cs-410,1,3, Text Retrieval Problem,"00:25:42,380","00:25:49,220",322,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1542,"It's a classic IR book, which has",pic_cs-410_1_3_1500.jpg
cs-410_1_3_323,cs-410,1,3, Text Retrieval Problem,"00:25:49,220","00:25:55,540",323,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1549,and results in early days up to,pic_cs-410_1_3_1500.jpg
cs-410_1_3_324,cs-410,1,3, Text Retrieval Problem,"00:25:55,540","00:25:59,762",324,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1555,Chapter six of this book has,pic_cs-410_1_3_1500.jpg
cs-410_1_3_325,cs-410,1,3, Text Retrieval Problem,"00:25:59,762","00:26:06,211",325,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1559,the Probability Ranking Principle and,pic_cs-410_1_3_1500.jpg
cs-410_1_3_326,cs-410,1,3, Text Retrieval Problem,"00:26:06,211","00:26:16,211",326,https://www.coursera.org/learn/cs-410/lecture/CXoWB?t=1566,[MUSIC],pic_cs-410_1_3_1560.jpg
cs-410_1_4_1,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:00,000","00:00:07,569",1,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=0,[SOUND],pic_cs-410_1_4_0.jpg
cs-410_1_4_2,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:07,569","00:00:10,320",2,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=7,This lecture is a overview of,pic_cs-410_1_4_0.jpg
cs-410_1_4_3,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:13,630","00:00:17,820",3,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=13,"In the previous lecture, we introduced",pic_cs-410_1_4_0.jpg
cs-410_1_4_4,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:17,820","00:00:20,330",4,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=17,We explained that the main problem,pic_cs-410_1_4_0.jpg
cs-410_1_4_5,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:20,330","00:00:24,780",5,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=20,is the design of ranking function,pic_cs-410_1_4_0.jpg
cs-410_1_4_6,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:24,780","00:00:25,510",6,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=24,"In this lecture,",pic_cs-410_1_4_0.jpg
cs-410_1_4_7,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:25,510","00:00:31,040",7,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=25,we will give an overview of different,pic_cs-410_1_4_0.jpg
cs-410_1_4_8,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:33,840","00:00:35,750",8,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=33,So the problem is the following.,pic_cs-410_1_4_0.jpg
cs-410_1_4_9,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:35,750","00:00:39,310",9,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=35,We have a query that has,pic_cs-410_1_4_0.jpg
cs-410_1_4_10,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:39,310","00:00:42,710",10,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=39,the document that's also,pic_cs-410_1_4_0.jpg
cs-410_1_4_11,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:42,710","00:00:44,509",11,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=42,And we hope to define a function f,pic_cs-410_1_4_0.jpg
cs-410_1_4_12,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:45,770","00:00:49,596",12,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=45,that can compute a score based,pic_cs-410_1_4_0.jpg
cs-410_1_4_13,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:49,596","00:00:54,870",13,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=49,So the main challenge you hear is with,pic_cs-410_1_4_0.jpg
cs-410_1_4_14,cs-410,1,4, Overview of Text Retrieval Methods,"00:00:54,870","00:01:00,275",14,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=54,can rank all the relevant documents,pic_cs-410_1_4_0.jpg
cs-410_1_4_15,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:00,275","00:01:05,544",15,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=60,"Clearly, this means our function",pic_cs-410_1_4_60.jpg
cs-410_1_4_16,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:05,544","00:01:10,824",16,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=65,the likelihood that a document,pic_cs-410_1_4_60.jpg
cs-410_1_4_17,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:10,824","00:01:16,490",17,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=70,That also means we have to have,pic_cs-410_1_4_60.jpg
cs-410_1_4_18,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:16,490","00:01:19,621",18,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=76,"In particular, in order to",pic_cs-410_1_4_60.jpg
cs-410_1_4_19,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:19,621","00:01:23,380",19,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=79,we have to have a computational,pic_cs-410_1_4_60.jpg
cs-410_1_4_20,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:23,380","00:01:27,232",20,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=83,And we achieve this goal by,pic_cs-410_1_4_60.jpg
cs-410_1_4_21,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:27,232","00:01:30,370",21,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=87,which gives us,pic_cs-410_1_4_60.jpg
cs-410_1_4_22,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:32,650","00:01:34,110",22,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=92,"Now, over many decades,",pic_cs-410_1_4_60.jpg
cs-410_1_4_23,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:34,110","00:01:38,420",23,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=94,researchers have designed many,pic_cs-410_1_4_60.jpg
cs-410_1_4_24,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:38,420","00:01:40,680",24,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=98,And they fall into different categories.,pic_cs-410_1_4_60.jpg
cs-410_1_4_25,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:42,290","00:01:48,830",25,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=102,"First, one family of the models",pic_cs-410_1_4_60.jpg
cs-410_1_4_26,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:50,090","00:01:54,170",26,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=110,"Basically, we assume that if",pic_cs-410_1_4_60.jpg
cs-410_1_4_27,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:54,170","00:01:57,970",27,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=114,"the query than another document is,",pic_cs-410_1_4_60.jpg
cs-410_1_4_28,cs-410,1,4, Overview of Text Retrieval Methods,"00:01:57,970","00:02:02,310",28,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=117,then we will say the first document,pic_cs-410_1_4_60.jpg
cs-410_1_4_29,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:02,310","00:02:05,330",29,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=122,"So in this case,",pic_cs-410_1_4_120.jpg
cs-410_1_4_30,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:05,330","00:02:08,572",30,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=125,the similarity between the query and,pic_cs-410_1_4_120.jpg
cs-410_1_4_31,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:08,572","00:02:13,760",31,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=128,One well known example in this,pic_cs-410_1_4_120.jpg
cs-410_1_4_32,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:13,760","00:02:17,160",32,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=133,which we will cover more in,pic_cs-410_1_4_120.jpg
cs-410_1_4_33,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:20,370","00:02:24,010",33,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=140,A second kind of models,pic_cs-410_1_4_120.jpg
cs-410_1_4_34,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:24,010","00:02:30,600",34,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=144,"In this family of models, we follow a very",pic_cs-410_1_4_120.jpg
cs-410_1_4_35,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:30,600","00:02:35,200",35,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=150,queries and documents are all,pic_cs-410_1_4_120.jpg
cs-410_1_4_36,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:36,420","00:02:41,220",36,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=156,And we assume there is a binary,pic_cs-410_1_4_120.jpg
cs-410_1_4_37,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:42,370","00:02:45,420",37,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=162,to indicate whether a document,pic_cs-410_1_4_120.jpg
cs-410_1_4_38,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:46,530","00:02:53,090",38,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=166,We then define the score of document with,pic_cs-410_1_4_120.jpg
cs-410_1_4_39,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:53,090","00:02:59,780",39,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=173,"this random variable R is equal to 1,",pic_cs-410_1_4_120.jpg
cs-410_1_4_40,cs-410,1,4, Overview of Text Retrieval Methods,"00:02:59,780","00:03:04,363",40,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=179,There are different cases,pic_cs-410_1_4_120.jpg
cs-410_1_4_41,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:04,363","00:03:08,003",41,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=184,"One is classic probabilistic model,",pic_cs-410_1_4_180.jpg
cs-410_1_4_42,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:08,003","00:03:10,800",42,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=188,yet another is divergence,pic_cs-410_1_4_180.jpg
cs-410_1_4_43,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:12,580","00:03:17,865",43,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=192,"In a later lecture, we will talk more",pic_cs-410_1_4_180.jpg
cs-410_1_4_44,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:17,865","00:03:21,740",44,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=197,A third kind of model are based,pic_cs-410_1_4_180.jpg
cs-410_1_4_45,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:21,740","00:03:27,440",45,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=201,So here the idea is to associate,pic_cs-410_1_4_180.jpg
cs-410_1_4_46,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:27,440","00:03:31,230",46,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=207,and we can then quantify,pic_cs-410_1_4_180.jpg
cs-410_1_4_47,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:31,230","00:03:34,790",47,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=211,show that the query,pic_cs-410_1_4_180.jpg
cs-410_1_4_48,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:37,100","00:03:41,940",48,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=217,"Finally, there is also a family of models",pic_cs-410_1_4_180.jpg
cs-410_1_4_49,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:41,940","00:03:46,237",49,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=221,that are using axiomatic thinking.,pic_cs-410_1_4_180.jpg
cs-410_1_4_50,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:46,237","00:03:50,849",50,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=226,"Here, an idea is to define",pic_cs-410_1_4_180.jpg
cs-410_1_4_51,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:50,849","00:03:54,650",51,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=230,hope a good retrieval function to satisfy.,pic_cs-410_1_4_180.jpg
cs-410_1_4_52,cs-410,1,4, Overview of Text Retrieval Methods,"00:03:55,760","00:04:00,572",52,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=235,"So in this case, the problem is",pic_cs-410_1_4_180.jpg
cs-410_1_4_53,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:00,572","00:04:04,288",53,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=240,that can satisfy all,pic_cs-410_1_4_240.jpg
cs-410_1_4_54,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:05,867","00:04:12,326",54,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=245,"Interestingly, although these different",pic_cs-410_1_4_240.jpg
cs-410_1_4_55,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:12,326","00:04:17,930",55,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=252,"in the end, the retrieval function",pic_cs-410_1_4_240.jpg
cs-410_1_4_56,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:17,930","00:04:22,020",56,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=257,And these functions tend to,pic_cs-410_1_4_240.jpg
cs-410_1_4_57,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:22,020","00:04:28,010",57,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=262,So now let's take a look at the common,pic_cs-410_1_4_240.jpg
cs-410_1_4_58,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:28,010","00:04:32,760",58,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=268,and to examine some of the common,pic_cs-410_1_4_240.jpg
cs-410_1_4_59,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:33,900","00:04:38,810",59,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=273,"First, these models are all",pic_cs-410_1_4_240.jpg
cs-410_1_4_60,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:38,810","00:04:43,060",60,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=278,"of using bag of words to represent text,",pic_cs-410_1_4_240.jpg
cs-410_1_4_61,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:43,060","00:04:47,500",61,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=283,we explained this in the natural,pic_cs-410_1_4_240.jpg
cs-410_1_4_62,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:47,500","00:04:51,450",62,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=287,Bag of words representation remains,pic_cs-410_1_4_240.jpg
cs-410_1_4_63,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:51,450","00:04:52,320",63,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=291,the search engines.,pic_cs-410_1_4_240.jpg
cs-410_1_4_64,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:53,620","00:04:57,690",64,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=293,"So with this assumption,",pic_cs-410_1_4_240.jpg
cs-410_1_4_65,cs-410,1,4, Overview of Text Retrieval Methods,"00:04:57,690","00:05:03,300",65,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=297,like a presidential campaign news,pic_cs-410_1_4_240.jpg
cs-410_1_4_66,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:03,300","00:05:08,140",66,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=303,would be based on scores computed,pic_cs-410_1_4_300.jpg
cs-410_1_4_67,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:09,560","00:05:15,710",67,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=309,And that means the score would,pic_cs-410_1_4_300.jpg
cs-410_1_4_68,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:15,710","00:05:19,510",68,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=315,"such as presidential, campaign, and news.",pic_cs-410_1_4_300.jpg
cs-410_1_4_69,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:19,510","00:05:23,749",69,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=319,"Here, we can see there",pic_cs-410_1_4_300.jpg
cs-410_1_4_70,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:23,749","00:05:29,501",70,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=323,each corresponding to how well the,pic_cs-410_1_4_300.jpg
cs-410_1_4_71,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:31,475","00:05:36,729",71,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=331,"Inside of these functions,",pic_cs-410_1_4_300.jpg
cs-410_1_4_72,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:38,760","00:05:43,770",72,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=338,"So for example, one factor that",pic_cs-410_1_4_300.jpg
cs-410_1_4_73,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:43,770","00:05:48,570",73,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=343,is how many times does the word,pic_cs-410_1_4_300.jpg
cs-410_1_4_74,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:48,570","00:05:50,250",74,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=348,"This is called a term frequency, or TF.",pic_cs-410_1_4_300.jpg
cs-410_1_4_75,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:51,710","00:05:56,823",75,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=351,We might also denote as,pic_cs-410_1_4_300.jpg
cs-410_1_4_76,cs-410,1,4, Overview of Text Retrieval Methods,"00:05:56,823","00:06:03,533",76,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=356,"In general, if the word occurs",pic_cs-410_1_4_300.jpg
cs-410_1_4_77,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:03,533","00:06:08,550",77,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=363,then the value of this,pic_cs-410_1_4_360.jpg
cs-410_1_4_78,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:08,550","00:06:13,610",78,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=368,"Another factor is,",pic_cs-410_1_4_360.jpg
cs-410_1_4_79,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:13,610","00:06:18,141",79,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=373,And this is to use the document length for,pic_cs-410_1_4_360.jpg
cs-410_1_4_80,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:18,141","00:06:22,910",80,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=378,"In general, if a term occurs in a long",pic_cs-410_1_4_360.jpg
cs-410_1_4_81,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:22,910","00:06:28,430",81,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=382,"document many times,",pic_cs-410_1_4_360.jpg
cs-410_1_4_82,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:28,430","00:06:32,678",82,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=388,if it occurred the same number,pic_cs-410_1_4_360.jpg
cs-410_1_4_83,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:32,678","00:06:37,130",83,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=392,"Because in a long document, any term",pic_cs-410_1_4_360.jpg
cs-410_1_4_84,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:38,980","00:06:42,840",84,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=398,"Finally, there is this factor",pic_cs-410_1_4_360.jpg
cs-410_1_4_85,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:42,840","00:06:48,020",85,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=402,"That is, we also want to look at how",pic_cs-410_1_4_360.jpg
cs-410_1_4_86,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:48,020","00:06:55,240",86,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=408,"collection, and we call this document",pic_cs-410_1_4_360.jpg
cs-410_1_4_87,cs-410,1,4, Overview of Text Retrieval Methods,"00:06:55,240","00:07:01,200",87,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=415,"And in some other models,",pic_cs-410_1_4_360.jpg
cs-410_1_4_88,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:01,200","00:07:04,620",88,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=421,to characterize this information.,pic_cs-410_1_4_420.jpg
cs-410_1_4_89,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:05,860","00:07:09,770",89,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=425,"So here, I show the probability of",pic_cs-410_1_4_420.jpg
cs-410_1_4_90,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:10,830","00:07:14,564",90,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=430,So all these are trying to characterize,pic_cs-410_1_4_420.jpg
cs-410_1_4_91,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:14,564","00:07:15,555",91,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=434,the collection.,pic_cs-410_1_4_420.jpg
cs-410_1_4_92,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:15,555","00:07:20,418",92,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=435,"In general, matching a rare term in",pic_cs-410_1_4_420.jpg
cs-410_1_4_93,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:20,418","00:07:23,860",93,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=440,to the overall score than,pic_cs-410_1_4_420.jpg
cs-410_1_4_94,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:25,720","00:07:30,564",94,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=445,So this captures some of the main ideas,pic_cs-410_1_4_420.jpg
cs-410_1_4_95,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:30,564","00:07:32,349",95,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=450,the art original models.,pic_cs-410_1_4_420.jpg
cs-410_1_4_96,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:34,000","00:07:38,422",96,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=454,"So now, a natural question is,",pic_cs-410_1_4_420.jpg
cs-410_1_4_97,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:39,834","00:07:45,080",97,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=459,Now it turns out that many,pic_cs-410_1_4_420.jpg
cs-410_1_4_98,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:45,080","00:07:47,700",98,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=465,So here are a list of,pic_cs-410_1_4_420.jpg
cs-410_1_4_99,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:47,700","00:07:52,463",99,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=467,that are generally regarded as,pic_cs-410_1_4_420.jpg
cs-410_1_4_100,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:52,463","00:07:57,920",100,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=472,"pivoted length normalization,",pic_cs-410_1_4_420.jpg
cs-410_1_4_101,cs-410,1,4, Overview of Text Retrieval Methods,"00:07:57,920","00:08:02,110",101,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=477,"When optimized,",pic_cs-410_1_4_420.jpg
cs-410_1_4_102,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:02,110","00:08:08,508",102,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=482,And this was discussed in detail in this,pic_cs-410_1_4_480.jpg
cs-410_1_4_103,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:08,508","00:08:13,130",103,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=488,"Among all these,",pic_cs-410_1_4_480.jpg
cs-410_1_4_104,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:13,130","00:08:17,750",104,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=493,It's most likely that this has been used,pic_cs-410_1_4_480.jpg
cs-410_1_4_105,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:17,750","00:08:21,730",105,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=497,and you will also often see this,pic_cs-410_1_4_480.jpg
cs-410_1_4_106,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:22,800","00:08:27,090",106,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=502,And we'll talk more about this,pic_cs-410_1_4_480.jpg
cs-410_1_4_107,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:30,430","00:08:36,770",107,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=510,"So, to summarize, the main points made",pic_cs-410_1_4_480.jpg
cs-410_1_4_108,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:36,770","00:08:41,540",108,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=516,of a good ranking function pre-requires a,pic_cs-410_1_4_480.jpg
cs-410_1_4_109,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:41,540","00:08:45,310",109,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=521,we achieve this goal by designing,pic_cs-410_1_4_480.jpg
cs-410_1_4_110,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:47,170","00:08:52,260",110,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=527,"Second, many models are equally effective,",pic_cs-410_1_4_480.jpg
cs-410_1_4_111,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:52,260","00:08:55,760",111,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=532,Researchers are still active and,pic_cs-410_1_4_480.jpg
cs-410_1_4_112,cs-410,1,4, Overview of Text Retrieval Methods,"00:08:55,760","00:08:58,636",112,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=535,trying to find a truly,pic_cs-410_1_4_480.jpg
cs-410_1_4_113,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:00,865","00:09:03,926",113,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=540,"Finally, the state of the art",pic_cs-410_1_4_540.jpg
cs-410_1_4_114,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:03,926","00:09:05,920",114,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=543,to rely on the following ideas.,pic_cs-410_1_4_540.jpg
cs-410_1_4_115,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:05,920","00:09:08,970",115,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=545,"First, bag of words representation.",pic_cs-410_1_4_540.jpg
cs-410_1_4_116,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:08,970","00:09:14,740",116,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=548,"Second, TF and",pic_cs-410_1_4_540.jpg
cs-410_1_4_117,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:14,740","00:09:19,787",117,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=554,Such information is used in,pic_cs-410_1_4_540.jpg
cs-410_1_4_118,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:19,787","00:09:25,028",118,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=559,the overall contribution of matching,pic_cs-410_1_4_540.jpg
cs-410_1_4_119,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:25,028","00:09:29,692",119,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=565,These are often combined in interesting,pic_cs-410_1_4_540.jpg
cs-410_1_4_120,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:29,692","00:09:34,210",120,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=569,exactly they are combined to rank,pic_cs-410_1_4_540.jpg
cs-410_1_4_121,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:36,390","00:09:40,560",121,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=576,There are two suggested additional,pic_cs-410_1_4_540.jpg
cs-410_1_4_122,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:41,760","00:09:45,150",122,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=581,The first is a paper where you can,pic_cs-410_1_4_540.jpg
cs-410_1_4_123,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:45,150","00:09:48,420",123,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=585,comparison of multiple,pic_cs-410_1_4_540.jpg
cs-410_1_4_124,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:49,840","00:09:54,674",124,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=589,The second is a book with,pic_cs-410_1_4_540.jpg
cs-410_1_4_125,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:54,674","00:09:58,507",125,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=594,review of different retrieval models.,pic_cs-410_1_4_540.jpg
cs-410_1_4_126,cs-410,1,4, Overview of Text Retrieval Methods,"00:09:58,507","00:10:08,507",126,https://www.coursera.org/learn/cs-410/lecture/gxXq6?t=598,[MUSIC],pic_cs-410_1_4_540.jpg
cs-410_1_5_1,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:00,008","00:00:07,957",1,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=0,[SOUND],pic_cs-410_1_5_0.jpg
cs-410_1_5_2,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:07,957","00:00:11,940",2,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=7,This lecture is about the,pic_cs-410_1_5_0.jpg
cs-410_1_5_3,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:11,940","00:00:14,410",3,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=11,We're going to give,pic_cs-410_1_5_0.jpg
cs-410_1_5_4,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:18,800","00:00:23,730",4,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=18,"In the last lecture, we talked about",pic_cs-410_1_5_0.jpg
cs-410_1_5_5,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:23,730","00:00:29,000",5,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=23,"a retrieval model, which would give",pic_cs-410_1_5_0.jpg
cs-410_1_5_6,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:30,270","00:00:33,600",6,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=30,"In this lecture, we're going to",pic_cs-410_1_5_0.jpg
cs-410_1_5_7,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:33,600","00:00:36,640",7,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=33,designing a ramping function called,pic_cs-410_1_5_0.jpg
cs-410_1_5_8,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:37,760","00:00:41,500",8,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=37,And we're going to give a brief,pic_cs-410_1_5_0.jpg
cs-410_1_5_9,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:44,330","00:00:47,320",9,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=44,Vector space model is a special case of,pic_cs-410_1_5_0.jpg
cs-410_1_5_10,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:47,320","00:00:50,800",10,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=47,similarity based models,pic_cs-410_1_5_0.jpg
cs-410_1_5_11,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:50,800","00:00:56,049",11,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=50,Which means we assume relevance,pic_cs-410_1_5_0.jpg
cs-410_1_5_12,cs-410,1,5, Vector Space Model - Basic Idea,"00:00:56,049","00:00:59,450",12,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=56,between the document and the query.,pic_cs-410_1_5_0.jpg
cs-410_1_5_13,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:02,140","00:01:06,280",13,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=62,Now whether is this assumption,pic_cs-410_1_5_60.jpg
cs-410_1_5_14,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:06,280","00:01:09,965",14,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=66,"But in order to solve the search problem,",pic_cs-410_1_5_60.jpg
cs-410_1_5_15,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:09,965","00:01:15,860",15,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=69,we have to convert the vague notion,pic_cs-410_1_5_60.jpg
cs-410_1_5_16,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:15,860","00:01:21,459",16,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=75,definition that can be implemented,pic_cs-410_1_5_60.jpg
cs-410_1_5_17,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:21,459","00:01:26,430",17,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=81,"So in this process,",pic_cs-410_1_5_60.jpg
cs-410_1_5_18,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:26,430","00:01:31,510",18,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=86,This is the first assumption,pic_cs-410_1_5_60.jpg
cs-410_1_5_19,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:31,510","00:01:36,091",19,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=91,"Basically, we assume that if a document",pic_cs-410_1_5_60.jpg
cs-410_1_5_20,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:36,091","00:01:37,419",20,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=96,another document.,pic_cs-410_1_5_60.jpg
cs-410_1_5_21,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:37,419","00:01:42,070",21,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=97,Then the first document will be assumed it,pic_cs-410_1_5_60.jpg
cs-410_1_5_22,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:42,070","00:01:45,310",22,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=102,And this is the basis for,pic_cs-410_1_5_60.jpg
cs-410_1_5_23,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:46,800","00:01:51,970",23,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=106,"Again, it's questionable whether this is",pic_cs-410_1_5_60.jpg
cs-410_1_5_24,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:51,970","00:01:55,750",24,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=111,As we will see later there,pic_cs-410_1_5_60.jpg
cs-410_1_5_25,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:58,300","00:01:59,790",25,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=118,The basic idea of vectors for,pic_cs-410_1_5_60.jpg
cs-410_1_5_26,cs-410,1,5, Vector Space Model - Basic Idea,"00:01:59,790","00:02:03,070",26,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=119,base retrieval model is actually,pic_cs-410_1_5_60.jpg
cs-410_1_5_27,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:03,070","00:02:10,300",27,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=123,Imagine a high dimensional space where,pic_cs-410_1_5_120.jpg
cs-410_1_5_28,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:11,660","00:02:17,088",28,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=131,So here I issue a three dimensional,pic_cs-410_1_5_120.jpg
cs-410_1_5_29,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:17,088","00:02:21,120",29,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=137,"programming, library and presidential.",pic_cs-410_1_5_120.jpg
cs-410_1_5_30,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:21,120","00:02:23,260",30,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=141,So each term here defines one dimension.,pic_cs-410_1_5_120.jpg
cs-410_1_5_31,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:24,370","00:02:28,966",31,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=144,"Now we can consider vectors in this,",pic_cs-410_1_5_120.jpg
cs-410_1_5_32,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:28,966","00:02:32,275",32,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=148,And we're going to assume,pic_cs-410_1_5_120.jpg
cs-410_1_5_33,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:32,275","00:02:36,340",33,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=152,the query will be placed,pic_cs-410_1_5_120.jpg
cs-410_1_5_34,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:36,340","00:02:43,526",34,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=156,"So for example, on document might",pic_cs-410_1_5_120.jpg
cs-410_1_5_35,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:43,526","00:02:48,710",35,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=163,Now this means this document,pic_cs-410_1_5_120.jpg
cs-410_1_5_36,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:48,710","00:02:54,657",36,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=168,"presidential, but",pic_cs-410_1_5_120.jpg
cs-410_1_5_37,cs-410,1,5, Vector Space Model - Basic Idea,"00:02:54,657","00:03:00,270",37,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=174,What does this mean in terms,pic_cs-410_1_5_120.jpg
cs-410_1_5_38,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:00,270","00:03:04,370",38,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=180,That just means we're going to look at,pic_cs-410_1_5_180.jpg
cs-410_1_5_39,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:04,370","00:03:05,710",39,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=184,this vector.,pic_cs-410_1_5_180.jpg
cs-410_1_5_40,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:05,710","00:03:07,910",40,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=185,We're going to ignore everything else.,pic_cs-410_1_5_180.jpg
cs-410_1_5_41,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:07,910","00:03:12,950",41,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=187,"Basically, what we see here is only",pic_cs-410_1_5_180.jpg
cs-410_1_5_42,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:14,470","00:03:16,380",42,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=194,"Of course,",pic_cs-410_1_5_180.jpg
cs-410_1_5_43,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:16,380","00:03:20,223",43,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=196,"For example, the orders of",pic_cs-410_1_5_180.jpg
cs-410_1_5_44,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:20,223","00:03:25,038",44,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=200,that's because we assume that,pic_cs-410_1_5_180.jpg
cs-410_1_5_45,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:25,038","00:03:29,310",45,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=205,So with this presentation,pic_cs-410_1_5_180.jpg
cs-410_1_5_46,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:29,310","00:03:33,472",46,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=209,d1 simply suggests a [INAUDIBLE] library.,pic_cs-410_1_5_180.jpg
cs-410_1_5_47,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:33,472","00:03:37,949",47,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=213,Now this is different from another,pic_cs-410_1_5_180.jpg
cs-410_1_5_48,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:37,949","00:03:39,906",48,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=217,"a different vector, d2 here.",pic_cs-410_1_5_180.jpg
cs-410_1_5_49,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:39,906","00:03:44,319",49,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=219,"Now in this case, the document that",pic_cs-410_1_5_180.jpg
cs-410_1_5_50,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:44,319","00:03:46,679",50,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=224,it doesn't talk about presidential.,pic_cs-410_1_5_180.jpg
cs-410_1_5_51,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:46,679","00:03:48,830",51,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=226,So what does this remind you?,pic_cs-410_1_5_180.jpg
cs-410_1_5_52,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:48,830","00:03:54,540",52,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=228,Well you can probably guess the topic,pic_cs-410_1_5_180.jpg
cs-410_1_5_53,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:54,540","00:03:56,840",53,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=234,the library is software lab library.,pic_cs-410_1_5_180.jpg
cs-410_1_5_54,cs-410,1,5, Vector Space Model - Basic Idea,"00:03:58,366","00:04:04,110",54,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=238,So this shows that by using,pic_cs-410_1_5_180.jpg
cs-410_1_5_55,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:04,110","00:04:08,190",55,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=244,we can actually capture the differences,pic_cs-410_1_5_240.jpg
cs-410_1_5_56,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:09,610","00:04:12,190",56,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=249,Now you can also imagine,pic_cs-410_1_5_240.jpg
cs-410_1_5_57,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:12,190","00:04:15,296",57,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=252,"For example,",pic_cs-410_1_5_240.jpg
cs-410_1_5_58,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:15,296","00:04:17,632",58,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=255,that might be a presidential program.,pic_cs-410_1_5_240.jpg
cs-410_1_5_59,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:17,632","00:04:22,649",59,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=257,And in fact we can place all,pic_cs-410_1_5_240.jpg
cs-410_1_5_60,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:22,649","00:04:26,700",60,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=262,And they will be pointing,pic_cs-410_1_5_240.jpg
cs-410_1_5_61,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:26,700","00:04:27,340",61,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=266,"And similarly,",pic_cs-410_1_5_240.jpg
cs-410_1_5_62,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:27,340","00:04:31,570",62,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=267,we're going to place our query also,pic_cs-410_1_5_240.jpg
cs-410_1_5_63,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:32,630","00:04:37,226",63,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=272,And then we're going to measure the,pic_cs-410_1_5_240.jpg
cs-410_1_5_64,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:37,226","00:04:39,510",64,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=277,every document vector.,pic_cs-410_1_5_240.jpg
cs-410_1_5_65,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:39,510","00:04:40,740",65,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=279,"So in this case for example,",pic_cs-410_1_5_240.jpg
cs-410_1_5_66,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:40,740","00:04:47,200",66,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=280,we can easily see d2 seems to be,pic_cs-410_1_5_240.jpg
cs-410_1_5_67,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:47,200","00:04:50,040",67,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=287,"And therefore,",pic_cs-410_1_5_240.jpg
cs-410_1_5_68,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:51,900","00:04:56,530",68,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=291,So this is basically the main,pic_cs-410_1_5_240.jpg
cs-410_1_5_69,cs-410,1,5, Vector Space Model - Basic Idea,"00:04:58,320","00:05:02,455",69,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=298,"So to be more precise,",pic_cs-410_1_5_240.jpg
cs-410_1_5_70,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:02,455","00:05:09,000",70,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=302,vector space model is a framework.,pic_cs-410_1_5_300.jpg
cs-410_1_5_71,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:09,000","00:05:12,510",71,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=309,"In this framework,",pic_cs-410_1_5_300.jpg
cs-410_1_5_72,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:12,510","00:05:17,300",72,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=312,"First, we represent a document and",pic_cs-410_1_5_300.jpg
cs-410_1_5_73,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:18,680","00:05:21,670",73,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=318,So here a term can be any basic concept.,pic_cs-410_1_5_300.jpg
cs-410_1_5_74,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:21,670","00:05:28,920",74,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=321,"For example, a word or a phrase or",pic_cs-410_1_5_300.jpg
cs-410_1_5_75,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:28,920","00:05:32,880",75,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=328,Those are just sequence of,pic_cs-410_1_5_300.jpg
cs-410_1_5_76,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:34,460","00:05:37,400",76,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=334,Each term is assumed that will,pic_cs-410_1_5_300.jpg
cs-410_1_5_77,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:37,400","00:05:42,170",77,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=337,"Therefore n terms in our vocabulary,",pic_cs-410_1_5_300.jpg
cs-410_1_5_78,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:44,060","00:05:48,550",78,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=344,A query vector would consist,pic_cs-410_1_5_300.jpg
cs-410_1_5_79,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:49,610","00:05:53,680",79,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=349,corresponding to the weights,pic_cs-410_1_5_300.jpg
cs-410_1_5_80,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:56,250","00:05:59,540",80,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=356,Each document vector is also similar.,pic_cs-410_1_5_300.jpg
cs-410_1_5_81,cs-410,1,5, Vector Space Model - Basic Idea,"00:05:59,540","00:06:04,518",81,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=359,It has a number of elements and,pic_cs-410_1_5_300.jpg
cs-410_1_5_82,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:04,518","00:06:08,900",82,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=364,indicating the weight of,pic_cs-410_1_5_360.jpg
cs-410_1_5_83,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:08,900","00:06:12,397",83,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=368,"Here, you can see,",pic_cs-410_1_5_360.jpg
cs-410_1_5_84,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:12,397","00:06:14,445",84,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=372,"Therefore, they are N elements",pic_cs-410_1_5_360.jpg
cs-410_1_5_85,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:15,525","00:06:18,715",85,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=375,each corresponding to the weight,pic_cs-410_1_5_360.jpg
cs-410_1_5_86,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:21,385","00:06:23,860",86,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=381,So the relevance in this case,pic_cs-410_1_5_360.jpg
cs-410_1_5_87,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:23,860","00:06:28,030",87,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=383,will be assumed to be the similarity,pic_cs-410_1_5_360.jpg
cs-410_1_5_88,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:29,420","00:06:33,500",88,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=389,"Therefore, our ranking function",pic_cs-410_1_5_360.jpg
cs-410_1_5_89,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:33,500","00:06:35,720",89,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=393,between the query vector and,pic_cs-410_1_5_360.jpg
cs-410_1_5_90,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:37,570","00:06:41,780",90,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=397,Now if I ask you to write a program,pic_cs-410_1_5_360.jpg
cs-410_1_5_91,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:41,780","00:06:42,370",91,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=401,in a search engine.,pic_cs-410_1_5_360.jpg
cs-410_1_5_92,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:44,042","00:06:48,248",92,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=404,You would realize that,pic_cs-410_1_5_360.jpg
cs-410_1_5_93,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:48,248","00:06:50,750",93,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=408,"We haven't said a lot of things in detail,",pic_cs-410_1_5_360.jpg
cs-410_1_5_94,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:50,750","00:06:56,080",94,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=410,therefore it's impossible to actually,pic_cs-410_1_5_360.jpg
cs-410_1_5_95,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:56,080","00:06:58,110",95,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=416,"That's why I said, this is a framework.",pic_cs-410_1_5_360.jpg
cs-410_1_5_96,cs-410,1,5, Vector Space Model - Basic Idea,"00:06:59,370","00:07:03,310",96,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=419,And this has to be refined,pic_cs-410_1_5_360.jpg
cs-410_1_5_97,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:04,350","00:07:08,630",97,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=424,suggest a particular ranking function,pic_cs-410_1_5_420.jpg
cs-410_1_5_98,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:10,890","00:07:13,810",98,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=430,So what does this framework not say?,pic_cs-410_1_5_420.jpg
cs-410_1_5_99,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:13,810","00:07:17,800",99,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=433,"Well, it actually hasn't said many things",pic_cs-410_1_5_420.jpg
cs-410_1_5_100,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:17,800","00:07:22,520",100,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=437,that would be required in order,pic_cs-410_1_5_420.jpg
cs-410_1_5_101,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:24,420","00:07:30,340",101,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=444,"First, it did not say how we should define",pic_cs-410_1_5_420.jpg
cs-410_1_5_102,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:32,580","00:07:36,190",102,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=452,We clearly assume,pic_cs-410_1_5_420.jpg
cs-410_1_5_103,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:36,190","00:07:38,660",103,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=456,"Otherwise, there will be redundancy.",pic_cs-410_1_5_420.jpg
cs-410_1_5_104,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:38,660","00:07:45,309",104,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=458,"For example, if two synonyms or somehow",pic_cs-410_1_5_420.jpg
cs-410_1_5_105,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:45,309","00:07:50,382",105,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=465,Then they would be defining,pic_cs-410_1_5_420.jpg
cs-410_1_5_106,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:50,382","00:07:54,299",106,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=470,that would clearly cause redundancy here.,pic_cs-410_1_5_420.jpg
cs-410_1_5_107,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:54,299","00:07:59,036",107,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=474,Or all the emphasizing of,pic_cs-410_1_5_420.jpg
cs-410_1_5_108,cs-410,1,5, Vector Space Model - Basic Idea,"00:07:59,036","00:08:03,997",108,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=479,because it would be as if,pic_cs-410_1_5_420.jpg
cs-410_1_5_109,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:03,997","00:08:08,760",109,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=483,when you actually matched,pic_cs-410_1_5_480.jpg
cs-410_1_5_110,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:11,420","00:08:16,020",110,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=491,"Secondly, it did not say how we",pic_cs-410_1_5_480.jpg
cs-410_1_5_111,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:16,020","00:08:18,200",111,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=496,the query in this space.,pic_cs-410_1_5_480.jpg
cs-410_1_5_112,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:18,200","00:08:22,970",112,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=498,Basically that show you some examples,pic_cs-410_1_5_480.jpg
cs-410_1_5_113,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:22,970","00:08:27,190",113,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=502,But where exactly should the vector for,pic_cs-410_1_5_480.jpg
cs-410_1_5_114,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:29,050","00:08:33,930",114,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=509,So this is equivalent to how,pic_cs-410_1_5_480.jpg
cs-410_1_5_115,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:33,930","00:08:39,237",115,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=513,How do you compute the lose,pic_cs-410_1_5_480.jpg
cs-410_1_5_116,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:39,237","00:08:41,808",116,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=519,"This is a very important question,",pic_cs-410_1_5_480.jpg
cs-410_1_5_117,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:41,808","00:08:47,220",117,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=521,because term weight in the query vector,pic_cs-410_1_5_480.jpg
cs-410_1_5_118,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:48,820","00:08:51,460",118,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=528,"So depending on how you assign the weight,",pic_cs-410_1_5_480.jpg
cs-410_1_5_119,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:51,460","00:08:55,620",119,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=531,you might prefer some terms,pic_cs-410_1_5_480.jpg
cs-410_1_5_120,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:56,630","00:08:59,472",120,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=536,"Similarly, the total word in",pic_cs-410_1_5_480.jpg
cs-410_1_5_121,cs-410,1,5, Vector Space Model - Basic Idea,"00:08:59,472","00:09:03,559",121,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=539,It indicates how well the term,pic_cs-410_1_5_480.jpg
cs-410_1_5_122,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:03,559","00:09:08,620",122,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=543,If you got it wrong then you clearly,pic_cs-410_1_5_540.jpg
cs-410_1_5_123,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:10,150","00:09:15,343",123,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=550,"Finally, how to define the similarity",pic_cs-410_1_5_540.jpg
cs-410_1_5_124,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:15,343","00:09:20,018",124,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=555,So these questions must be addressed,pic_cs-410_1_5_540.jpg
cs-410_1_5_125,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:20,018","00:09:24,620",125,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=560,function that we can actually,pic_cs-410_1_5_540.jpg
cs-410_1_5_126,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:25,920","00:09:31,767",126,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=565,So how do we solve these problems,pic_cs-410_1_5_540.jpg
cs-410_1_5_127,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:31,767","00:09:38,702",127,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=571,is the main topic of the next lecture.,pic_cs-410_1_5_540.jpg
cs-410_1_5_128,cs-410,1,5, Vector Space Model - Basic Idea,"00:09:38,702","00:09:44,589",128,https://www.coursera.org/learn/cs-410/lecture/o8WNd?t=578,[MUSIC],pic_cs-410_1_5_540.jpg
cs-410_1_6_1,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:00,008","00:00:05,424",1,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=0,In this lecture we're going to talk,pic_cs-410_1_6_0.jpg
cs-410_1_6_2,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:05,424","00:00:12,465",2,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=5,about how to instantiate,pic_cs-410_1_6_0.jpg
cs-410_1_6_3,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:12,465","00:00:19,875",3,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=12,that we can get very,pic_cs-410_1_6_0.jpg
cs-410_1_6_4,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:22,974","00:00:27,888",4,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=22,So this is to continue the discussion,pic_cs-410_1_6_0.jpg
cs-410_1_6_5,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:27,888","00:00:32,810",5,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=27,which is one particular approach,pic_cs-410_1_6_0.jpg
cs-410_1_6_6,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:34,420","00:00:38,551",6,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=34,And we're going to talk about how,pic_cs-410_1_6_0.jpg
cs-410_1_6_7,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:38,551","00:00:42,810",7,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=38,the the vector space,pic_cs-410_1_6_0.jpg
cs-410_1_6_8,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:42,810","00:00:48,270",8,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=42,instantiate the framework to derive,pic_cs-410_1_6_0.jpg
cs-410_1_6_9,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:48,270","00:00:53,380",9,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=48,And we're going to cover the symbolist,pic_cs-410_1_6_0.jpg
cs-410_1_6_10,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:55,360","00:00:58,390",10,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=55,So as we discussed in,pic_cs-410_1_6_0.jpg
cs-410_1_6_11,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:00:58,390","00:01:00,600",11,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=58,the vector space model,pic_cs-410_1_6_0.jpg
cs-410_1_6_12,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:00,600","00:01:02,619",12,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=60,And this didn't say.,pic_cs-410_1_6_60.jpg
cs-410_1_6_13,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:05,266","00:01:11,040",13,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=65,"As we discussed in the previous lecture,",pic_cs-410_1_6_60.jpg
cs-410_1_6_14,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:11,040","00:01:13,160",14,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=71,It does not say many things.,pic_cs-410_1_6_60.jpg
cs-410_1_6_15,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:14,710","00:01:15,520",15,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=74,"So, for example,",pic_cs-410_1_6_60.jpg
cs-410_1_6_16,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:15,520","00:01:19,470",16,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=75,here it shows that it did not say,pic_cs-410_1_6_60.jpg
cs-410_1_6_17,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:20,770","00:01:25,939",17,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=80,It also did not say how we place,pic_cs-410_1_6_60.jpg
cs-410_1_6_18,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:27,130","00:01:31,470",18,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=87,It did not say how we place a query,pic_cs-410_1_6_60.jpg
cs-410_1_6_19,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:32,500","00:01:37,250",19,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=92,"And, finally, it did not say how we",pic_cs-410_1_6_60.jpg
cs-410_1_6_20,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:37,250","00:01:39,020",20,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=97,the query vector and the document vector.,pic_cs-410_1_6_60.jpg
cs-410_1_6_21,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:40,570","00:01:44,860",21,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=100,"So you can imagine,",pic_cs-410_1_6_60.jpg
cs-410_1_6_22,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:46,040","00:01:52,940",22,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=106,we have to say specifically,pic_cs-410_1_6_60.jpg
cs-410_1_6_23,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:52,940","00:01:54,830",23,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=112,What is exactly xi?,pic_cs-410_1_6_60.jpg
cs-410_1_6_24,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:54,830","00:01:56,700",24,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=114,And what is exactly yi?,pic_cs-410_1_6_60.jpg
cs-410_1_6_25,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:01:58,460","00:02:02,260",25,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=118,This will determine where,pic_cs-410_1_6_60.jpg
cs-410_1_6_26,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:02,260","00:02:04,560",26,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=122,where we place a query vector.,pic_cs-410_1_6_120.jpg
cs-410_1_6_27,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:04,560","00:02:05,230",27,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=124,"And, of course,",pic_cs-410_1_6_120.jpg
cs-410_1_6_28,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:05,230","00:02:08,869",28,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=125,we also need to say exactly what,pic_cs-410_1_6_120.jpg
cs-410_1_6_29,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:11,120","00:02:16,653",29,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=131,So if we can provide a definition,pic_cs-410_1_6_120.jpg
cs-410_1_6_30,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:16,653","00:02:22,590",30,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=136,the dimensions and these xi's or,pic_cs-410_1_6_120.jpg
cs-410_1_6_31,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:22,590","00:02:28,725",31,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=142,"queries and document, then we will be",pic_cs-410_1_6_120.jpg
cs-410_1_6_32,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:28,725","00:02:33,080",32,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=148,query vectors in this well defined space.,pic_cs-410_1_6_120.jpg
cs-410_1_6_33,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:33,080","00:02:36,414",33,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=153,"And then,",pic_cs-410_1_6_120.jpg
cs-410_1_6_34,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:36,414","00:02:39,685",34,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=156,then we'll have a well,pic_cs-410_1_6_120.jpg
cs-410_1_6_35,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:41,427","00:02:47,630",35,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=161,So let's see how we can do that and,pic_cs-410_1_6_120.jpg
cs-410_1_6_36,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:47,630","00:02:52,460",36,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=167,"Actually, I would suggest you to",pic_cs-410_1_6_120.jpg
cs-410_1_6_37,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:52,460","00:02:54,980",37,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=172,spend a couple minutes to think about.,pic_cs-410_1_6_120.jpg
cs-410_1_6_38,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:54,980","00:02:58,280",38,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=174,Suppose you are asked,pic_cs-410_1_6_120.jpg
cs-410_1_6_39,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:02:59,590","00:03:05,810",39,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=179,You have come up with the idea of vector,pic_cs-410_1_6_120.jpg
cs-410_1_6_40,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:05,810","00:03:10,310",40,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=185,"out how to compute these vectors exactly,",pic_cs-410_1_6_180.jpg
cs-410_1_6_41,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:10,310","00:03:10,810",41,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=190,What would you do?,pic_cs-410_1_6_180.jpg
cs-410_1_6_42,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:12,540","00:03:15,857",42,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=192,"So, think for a couple of minutes,",pic_cs-410_1_6_180.jpg
cs-410_1_6_43,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:20,581","00:03:26,460",43,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=200,"So, let's think about some simplest ways",pic_cs-410_1_6_180.jpg
cs-410_1_6_44,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:26,460","00:03:28,810",44,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=206,"First, how do we define the dimension?",pic_cs-410_1_6_180.jpg
cs-410_1_6_45,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:28,810","00:03:31,430",45,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=208,"Well, the obvious choice is to use",pic_cs-410_1_6_180.jpg
cs-410_1_6_46,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:31,430","00:03:34,636",46,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=211,each word in our vocabulary,pic_cs-410_1_6_180.jpg
cs-410_1_6_47,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:34,636","00:03:38,775",47,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=214,And show that there are N,pic_cs-410_1_6_180.jpg
cs-410_1_6_48,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:38,775","00:03:41,160",48,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=218,"Therefore, there are N dimensions.",pic_cs-410_1_6_180.jpg
cs-410_1_6_49,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:41,160","00:03:42,818",49,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=221,Each word defines one dimension.,pic_cs-410_1_6_180.jpg
cs-410_1_6_50,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:42,818","00:03:46,273",50,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=222,And this is basically,pic_cs-410_1_6_180.jpg
cs-410_1_6_51,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:48,965","00:03:52,395",51,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=228,Now let's look at how we,pic_cs-410_1_6_180.jpg
cs-410_1_6_52,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:54,395","00:03:57,175",52,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=234,"Again here, the simplest strategy is to",pic_cs-410_1_6_180.jpg
cs-410_1_6_53,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:03:58,700","00:04:03,650",53,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=238,use a Bit Vector to represent,pic_cs-410_1_6_180.jpg
cs-410_1_6_54,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:04,720","00:04:07,937",54,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=244,"And that means each element, xi and",pic_cs-410_1_6_240.jpg
cs-410_1_6_55,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:07,937","00:04:12,020",55,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=247,yi will be taking a value,pic_cs-410_1_6_240.jpg
cs-410_1_6_56,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:13,270","00:04:14,300",56,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=253,"When it's 1,",pic_cs-410_1_6_240.jpg
cs-410_1_6_57,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:14,300","00:04:20,750",57,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=254,it means the corresponding word is,pic_cs-410_1_6_240.jpg
cs-410_1_6_58,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:20,750","00:04:25,180",58,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=260,"When it's 0,",pic_cs-410_1_6_240.jpg
cs-410_1_6_59,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:27,070","00:04:31,210",59,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=267,So you can imagine if the user,pic_cs-410_1_6_240.jpg
cs-410_1_6_60,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:31,210","00:04:35,790",60,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=271,then the query vector will only,pic_cs-410_1_6_240.jpg
cs-410_1_6_61,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:37,630","00:04:41,450",61,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=277,"The document vector,",pic_cs-410_1_6_240.jpg
cs-410_1_6_62,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:41,450","00:04:46,700",62,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=281,But it will also have many zeros since,pic_cs-410_1_6_240.jpg
cs-410_1_6_63,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:46,700","00:04:50,720",63,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=286,Many words don't really,pic_cs-410_1_6_240.jpg
cs-410_1_6_64,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:52,110","00:04:56,570",64,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=292,Many words will only occasionally,pic_cs-410_1_6_240.jpg
cs-410_1_6_65,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:04:58,680","00:05:01,770",65,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=298,A lot of words will be absent,pic_cs-410_1_6_240.jpg
cs-410_1_6_66,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:04,390","00:05:09,770",66,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=304,So now we have placed the documents and,pic_cs-410_1_6_300.jpg
cs-410_1_6_67,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:11,450","00:05:14,240",67,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=311,Let's look at how we,pic_cs-410_1_6_300.jpg
cs-410_1_6_68,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:15,770","00:05:19,400",68,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=315,"So, a commonly used similarity",pic_cs-410_1_6_300.jpg
cs-410_1_6_69,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:20,900","00:05:25,590",69,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=320,The Dot Product of two,pic_cs-410_1_6_300.jpg
cs-410_1_6_70,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:25,590","00:05:30,590",70,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=325,the sum of the products of the,pic_cs-410_1_6_300.jpg
cs-410_1_6_71,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:30,590","00:05:38,596",71,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=330,"So, here we see that it's",pic_cs-410_1_6_300.jpg
cs-410_1_6_72,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:38,596","00:05:40,228",72,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=338,"So, here.",pic_cs-410_1_6_300.jpg
cs-410_1_6_73,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:40,228","00:05:43,420",73,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=340,"And then, x2 multiplied by y2.",pic_cs-410_1_6_300.jpg
cs-410_1_6_74,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:43,420","00:05:47,100",74,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=343,"And then, finally, xn multiplied by yn.",pic_cs-410_1_6_300.jpg
cs-410_1_6_75,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:47,100","00:05:48,810",75,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=347,"And then, we take a sum here.",pic_cs-410_1_6_300.jpg
cs-410_1_6_76,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:50,630","00:05:52,610",76,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=350,So that's a Dot Product.,pic_cs-410_1_6_300.jpg
cs-410_1_6_77,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:52,610","00:05:57,580",77,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=352,"Now, we can represent this in a more",pic_cs-410_1_6_300.jpg
cs-410_1_6_78,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:05:58,740","00:06:04,120",78,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=358,So this is only one of the many different,pic_cs-410_1_6_300.jpg
cs-410_1_6_79,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:04,120","00:06:10,640",79,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=364,"So, now we see that we have",pic_cs-410_1_6_360.jpg
cs-410_1_6_80,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:10,640","00:06:16,050",80,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=370,"we have defined the vectors, and we have",pic_cs-410_1_6_360.jpg
cs-410_1_6_81,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:16,050","00:06:21,495",81,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=376,So now we finally have the simplest,pic_cs-410_1_6_360.jpg
cs-410_1_6_82,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:21,495","00:06:26,882",82,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=381,on the bit vector [INAUDIBLE] dot product,pic_cs-410_1_6_360.jpg
cs-410_1_6_83,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:26,882","00:06:30,195",83,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=386,And the formula looks like this.,pic_cs-410_1_6_360.jpg
cs-410_1_6_84,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:30,195","00:06:32,415",84,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=390,So this is our formula.,pic_cs-410_1_6_360.jpg
cs-410_1_6_85,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:32,415","00:06:37,670",85,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=392,And that's actually a particular retrieval,pic_cs-410_1_6_360.jpg
cs-410_1_6_86,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:37,670","00:06:42,573",86,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=397,Now we can finally implement this,pic_cs-410_1_6_360.jpg
cs-410_1_6_87,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:42,573","00:06:45,350",87,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=402,and then rank the documents for query.,pic_cs-410_1_6_360.jpg
cs-410_1_6_88,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:45,350","00:06:50,110",88,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=405,"Now, at this point you should",pic_cs-410_1_6_360.jpg
cs-410_1_6_89,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:50,110","00:06:53,400",89,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=410,to think about how we can,pic_cs-410_1_6_360.jpg
cs-410_1_6_90,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:53,400","00:06:56,972",90,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=413,"So, we have gone through the process",pic_cs-410_1_6_360.jpg
cs-410_1_6_91,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:06:56,972","00:07:00,620",91,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=416,using a vector space model.,pic_cs-410_1_6_360.jpg
cs-410_1_6_92,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:00,620","00:07:05,185",92,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=420,"And then,",pic_cs-410_1_6_420.jpg
cs-410_1_6_93,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:05,185","00:07:09,780",93,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=425,"vectors in the vector space, and",pic_cs-410_1_6_420.jpg
cs-410_1_6_94,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:09,780","00:07:14,270",94,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=429,"So in the end, we've got a specific",pic_cs-410_1_6_420.jpg
cs-410_1_6_95,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:15,370","00:07:18,370",95,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=435,"Now, the next step is to think about",pic_cs-410_1_6_420.jpg
cs-410_1_6_96,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:18,370","00:07:21,160",96,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=438,"actually makes sense, right?",pic_cs-410_1_6_420.jpg
cs-410_1_6_97,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:21,160","00:07:24,140",97,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=441,Can we expect this function,pic_cs-410_1_6_420.jpg
cs-410_1_6_98,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:24,140","00:07:27,400",98,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=444,when we used it to rank documents for,pic_cs-410_1_6_420.jpg
cs-410_1_6_99,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:28,790","00:07:35,870",99,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=448,So it's worth thinking about what is,pic_cs-410_1_6_420.jpg
cs-410_1_6_100,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:35,870","00:07:38,220",100,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=455,"So, in the end, we'll get a number.",pic_cs-410_1_6_420.jpg
cs-410_1_6_101,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:38,220","00:07:40,240",101,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=458,But what does this number mean?,pic_cs-410_1_6_420.jpg
cs-410_1_6_102,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:40,240","00:07:40,990",102,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=460,Is it meaningful?,pic_cs-410_1_6_420.jpg
cs-410_1_6_103,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:42,200","00:07:44,390",103,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=462,"So, spend a couple minutes",pic_cs-410_1_6_420.jpg
cs-410_1_6_104,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:45,880","00:07:46,540",104,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=465,"And, of course,",pic_cs-410_1_6_420.jpg
cs-410_1_6_105,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:46,540","00:07:52,600",105,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=466,the general question here is do you,pic_cs-410_1_6_420.jpg
cs-410_1_6_106,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:52,600","00:07:54,680",106,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=472,Would it actually work well?,pic_cs-410_1_6_420.jpg
cs-410_1_6_107,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:54,680","00:07:58,329",107,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=474,"So, again,",pic_cs-410_1_6_420.jpg
cs-410_1_6_108,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:07:58,329","00:08:00,190",108,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=478,Is it actually meaningful?,pic_cs-410_1_6_420.jpg
cs-410_1_6_109,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:01,280","00:08:03,190",109,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=481,Does it mean something?,pic_cs-410_1_6_480.jpg
cs-410_1_6_110,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:03,190","00:08:06,560",110,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=483,This is related to how well,pic_cs-410_1_6_480.jpg
cs-410_1_6_111,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:08,260","00:08:11,530",111,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=488,"So, in order to assess",pic_cs-410_1_6_480.jpg
cs-410_1_6_112,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:11,530","00:08:15,520",112,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=491,"vector space model actually works well,",pic_cs-410_1_6_480.jpg
cs-410_1_6_113,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:17,170","00:08:22,570",113,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=497,"So, here I show some sample documents and",pic_cs-410_1_6_480.jpg
cs-410_1_6_114,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:22,570","00:08:26,390",114,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=502,The query is news about,pic_cs-410_1_6_480.jpg
cs-410_1_6_115,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:26,390","00:08:28,580",115,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=506,And we have five documents here.,pic_cs-410_1_6_480.jpg
cs-410_1_6_116,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:28,580","00:08:32,280",116,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=508,They cover different terms in the query.,pic_cs-410_1_6_480.jpg
cs-410_1_6_117,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:34,710","00:08:39,890",117,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=514,And if you look at these documents for,pic_cs-410_1_6_480.jpg
cs-410_1_6_118,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:41,880","00:08:47,070",118,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=521,"some documents are probably relevant, and",pic_cs-410_1_6_480.jpg
cs-410_1_6_119,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:48,300","00:08:54,690",119,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=528,"Now, if I asked you to rank these",pic_cs-410_1_6_480.jpg
cs-410_1_6_120,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:54,690","00:08:57,270",120,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=534,This is basically our ideal ranking.,pic_cs-410_1_6_480.jpg
cs-410_1_6_121,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:08:57,270","00:09:01,180",121,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=537,"When humans can examine the documents,",pic_cs-410_1_6_480.jpg
cs-410_1_6_122,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:03,430","00:09:06,900",122,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=543,"Now, so think for a moment,",pic_cs-410_1_6_540.jpg
cs-410_1_6_123,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:06,900","00:09:10,210",123,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=546,And perhaps by pausing the lecture.,pic_cs-410_1_6_540.jpg
cs-410_1_6_124,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:12,510","00:09:18,750",124,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=552,So I think most of you would,pic_cs-410_1_6_540.jpg
cs-410_1_6_125,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:18,750","00:09:23,353",125,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=558,better than others because they,pic_cs-410_1_6_540.jpg
cs-410_1_6_126,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:23,353","00:09:26,860",126,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=563,"They match news,",pic_cs-410_1_6_540.jpg
cs-410_1_6_127,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:27,900","00:09:33,160",127,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=567,"So, it looks like these documents",pic_cs-410_1_6_540.jpg
cs-410_1_6_128,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:33,160","00:09:37,230",128,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=573,They should be ranked on top.,pic_cs-410_1_6_540.jpg
cs-410_1_6_129,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:37,230","00:09:41,810",129,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=577,"And the other three d2, d1, and",pic_cs-410_1_6_540.jpg
cs-410_1_6_130,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:41,810","00:09:45,990",130,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=581,So we can also say d4 and,pic_cs-410_1_6_540.jpg
cs-410_1_6_131,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:45,990","00:09:50,150",131,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=585,"d1, d2 and d5 are non-relevant.",pic_cs-410_1_6_540.jpg
cs-410_1_6_132,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:50,150","00:09:55,290",132,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=590,So now let's see if our simplest,pic_cs-410_1_6_540.jpg
cs-410_1_6_133,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:55,290","00:09:57,400",133,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=595,or could do something closer.,pic_cs-410_1_6_540.jpg
cs-410_1_6_134,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:09:57,400","00:10:01,250",134,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=597,"So, let's first think about",pic_cs-410_1_6_540.jpg
cs-410_1_6_135,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:01,250","00:10:02,272",135,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=601,to score documents.,pic_cs-410_1_6_600.jpg
cs-410_1_6_136,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:02,272","00:10:04,000",136,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=602,All right.,pic_cs-410_1_6_600.jpg
cs-410_1_6_137,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:04,000","00:10:07,420",137,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=604,"Here I show two documents, d1 and d3.",pic_cs-410_1_6_600.jpg
cs-410_1_6_138,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:07,420","00:10:10,390",138,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=607,And we have the query also here.,pic_cs-410_1_6_600.jpg
cs-410_1_6_139,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:10,390","00:10:15,130",139,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=610,"In the vector space model, of course we",pic_cs-410_1_6_600.jpg
cs-410_1_6_140,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:15,130","00:10:16,830",140,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=615,these documents and the query.,pic_cs-410_1_6_600.jpg
cs-410_1_6_141,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:16,830","00:10:18,860",141,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=616,"Now, I showed the vocabulary here as well.",pic_cs-410_1_6_600.jpg
cs-410_1_6_142,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:18,860","00:10:22,850",142,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=618,So these are the end dimensions,pic_cs-410_1_6_600.jpg
cs-410_1_6_143,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:22,850","00:10:26,620",143,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=622,So what do you think is the vector for,pic_cs-410_1_6_600.jpg
cs-410_1_6_144,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:27,700","00:10:32,870",144,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=627,Note that we're assuming,pic_cs-410_1_6_600.jpg
cs-410_1_6_145,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:32,870","00:10:39,230",145,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=632,to indicate whether a term is absent or,pic_cs-410_1_6_600.jpg
cs-410_1_6_146,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:39,230","00:10:42,380",146,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=639,"So these are zero,1 bit vectors.",pic_cs-410_1_6_600.jpg
cs-410_1_6_147,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:43,880","00:10:45,790",147,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=643,So what do you think is the query vector?,pic_cs-410_1_6_600.jpg
cs-410_1_6_148,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:47,820","00:10:51,200",148,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=647,"Well, the query has four words here.",pic_cs-410_1_6_600.jpg
cs-410_1_6_149,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:51,200","00:10:54,380",149,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=651,"So for these four words,",pic_cs-410_1_6_600.jpg
cs-410_1_6_150,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:54,380","00:10:55,980",150,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=654,"And for the rest, there will be zeros.",pic_cs-410_1_6_600.jpg
cs-410_1_6_151,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:57,680","00:10:59,290",151,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=657,"Now, what about the documents?",pic_cs-410_1_6_600.jpg
cs-410_1_6_152,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:10:59,290","00:11:00,610",152,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=659,It's the same.,pic_cs-410_1_6_600.jpg
cs-410_1_6_153,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:00,610","00:11:03,430",153,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=660,"So d1 has two rows, news and about.",pic_cs-410_1_6_660.jpg
cs-410_1_6_154,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:03,430","00:11:07,367",154,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=663,"So, there are two 1's here,",pic_cs-410_1_6_660.jpg
cs-410_1_6_155,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:07,367","00:11:12,220",155,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=667,"Similarly, so now that we",pic_cs-410_1_6_660.jpg
cs-410_1_6_156,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:12,220","00:11:16,380",156,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=672,"have the two vectors,",pic_cs-410_1_6_660.jpg
cs-410_1_6_157,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:17,470","00:11:19,550",157,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=677,And we're going to use Do Product.,pic_cs-410_1_6_660.jpg
cs-410_1_6_158,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:19,550","00:11:21,610",158,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=679,"So you can see when we use Dot Product,",pic_cs-410_1_6_660.jpg
cs-410_1_6_159,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:21,610","00:11:26,030",159,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=681,we just multiply the corresponding,pic_cs-410_1_6_660.jpg
cs-410_1_6_160,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:26,030","00:11:30,894",160,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=686,"So these two will be formal product,",pic_cs-410_1_6_660.jpg
cs-410_1_6_161,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:30,894","00:11:33,920",161,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=690,and these two will,pic_cs-410_1_6_660.jpg
cs-410_1_6_162,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:33,920","00:11:38,210",162,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=693,and these two will generate yet,pic_cs-410_1_6_660.jpg
cs-410_1_6_163,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:40,020","00:11:46,320",163,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=700,"Now you can easily see if we do that,",pic_cs-410_1_6_660.jpg
cs-410_1_6_164,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:48,180","00:11:54,170",164,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=708,these zeroes because whenever we have,pic_cs-410_1_6_660.jpg
cs-410_1_6_165,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:54,170","00:11:57,538",165,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=714,So when we take a sum,pic_cs-410_1_6_660.jpg
cs-410_1_6_166,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:11:57,538","00:12:02,940",166,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=717,then the zero entries will be gone.,pic_cs-410_1_6_660.jpg
cs-410_1_6_167,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:04,400","00:12:08,010",167,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=724,"As long as you have one zero,",pic_cs-410_1_6_720.jpg
cs-410_1_6_168,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:08,010","00:12:14,710",168,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=728,"So, in the fact, we're just",pic_cs-410_1_6_720.jpg
cs-410_1_6_169,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:14,710","00:12:18,220",169,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=734,"In this case, we have seen two,",pic_cs-410_1_6_720.jpg
cs-410_1_6_170,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:18,220","00:12:20,240",170,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=738,So what does that mean?,pic_cs-410_1_6_720.jpg
cs-410_1_6_171,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:20,240","00:12:25,190",171,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=740,"Well, that means this number, or",pic_cs-410_1_6_720.jpg
cs-410_1_6_172,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:25,190","00:12:33,130",172,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=745,is simply the count of how many unique,pic_cs-410_1_6_720.jpg
cs-410_1_6_173,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:33,130","00:12:39,350",173,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=753,Because if a term is matched in the,pic_cs-410_1_6_720.jpg
cs-410_1_6_174,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:41,390","00:12:44,740",174,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=761,"If it's not, then there will",pic_cs-410_1_6_720.jpg
cs-410_1_6_175,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:46,310","00:12:50,410",175,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=766,"Similarly, if the document has a term but",pic_cs-410_1_6_720.jpg
cs-410_1_6_176,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:50,410","00:12:53,220",176,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=770,there will be a zero in the query vector.,pic_cs-410_1_6_720.jpg
cs-410_1_6_177,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:53,220","00:12:55,020",177,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=773,So those don't count.,pic_cs-410_1_6_720.jpg
cs-410_1_6_178,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:55,020","00:12:58,760",178,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=775,"So, as a result,",pic_cs-410_1_6_720.jpg
cs-410_1_6_179,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:12:58,760","00:13:03,820",179,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=778,measures how many unique query,pic_cs-410_1_6_720.jpg
cs-410_1_6_180,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:03,820","00:13:05,770",180,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=783,This is how we interpret this score.,pic_cs-410_1_6_780.jpg
cs-410_1_6_181,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:07,150","00:13:10,520",181,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=787,"Now, we can also take a look at d3.",pic_cs-410_1_6_780.jpg
cs-410_1_6_182,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:10,520","00:13:18,003",182,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=790,"In this case, you can see the result",pic_cs-410_1_6_780.jpg
cs-410_1_6_183,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:18,003","00:13:23,140",183,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=798,"distinctive query words news, presidential",pic_cs-410_1_6_780.jpg
cs-410_1_6_184,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:23,140","00:13:28,200",184,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=803,"Now in this case, this seems",pic_cs-410_1_6_780.jpg
cs-410_1_6_185,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:29,260","00:13:33,440",185,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=809,And this simplest vector,pic_cs-410_1_6_780.jpg
cs-410_1_6_186,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:33,440","00:13:35,050",186,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=813,So that looks pretty good.,pic_cs-410_1_6_780.jpg
cs-410_1_6_187,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:35,050","00:13:40,030",187,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=815,"However, if we examine this model in",pic_cs-410_1_6_780.jpg
cs-410_1_6_188,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:40,030","00:13:44,891",188,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=820,"So, here I'm going to show all",pic_cs-410_1_6_780.jpg
cs-410_1_6_189,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:44,891","00:13:49,977",189,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=824,And you can easily verify they're,pic_cs-410_1_6_780.jpg
cs-410_1_6_190,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:49,977","00:13:55,070",190,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=829,counting the number of unique query,pic_cs-410_1_6_780.jpg
cs-410_1_6_191,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:56,470","00:13:59,270",191,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=836,Now note that this measure,pic_cs-410_1_6_780.jpg
cs-410_1_6_192,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:13:59,270","00:14:03,740",192,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=839,It basically means if a document,pic_cs-410_1_6_780.jpg
cs-410_1_6_193,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:03,740","00:14:07,210",193,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=843,then the document will be,pic_cs-410_1_6_840.jpg
cs-410_1_6_194,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:07,210","00:14:09,190",194,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=847,And that seems to make sense.,pic_cs-410_1_6_840.jpg
cs-410_1_6_195,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:09,190","00:14:16,870",195,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=849,The only problem is here we can note that,pic_cs-410_1_6_840.jpg
cs-410_1_6_196,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:16,870","00:14:22,320",196,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=856,And they tied with a 3 as a score.,pic_cs-410_1_6_840.jpg
cs-410_1_6_197,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:25,050","00:14:31,000",197,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=865,"So, that's a problem because if you look",pic_cs-410_1_6_840.jpg
cs-410_1_6_198,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:31,000","00:14:36,920",198,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=871,should be ranked above d3 because,pic_cs-410_1_6_840.jpg
cs-410_1_6_199,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:36,920","00:14:42,100",199,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=876,"d3 only mentions the presidential once,",pic_cs-410_1_6_840.jpg
cs-410_1_6_200,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:42,100","00:14:47,634",200,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=882,"In the case of d3,",pic_cs-410_1_6_840.jpg
cs-410_1_6_201,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:47,634","00:14:51,360",201,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=887,But d4 is clearly above,pic_cs-410_1_6_840.jpg
cs-410_1_6_202,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:51,360","00:14:58,200",202,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=891,Another problem is that d2 and,pic_cs-410_1_6_840.jpg
cs-410_1_6_203,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:14:58,200","00:15:01,880",203,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=898,But if you look at the three words,pic_cs-410_1_6_840.jpg
cs-410_1_6_204,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:01,880","00:15:07,020",204,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=901,"it matched the news, about and campaign.",pic_cs-410_1_6_900.jpg
cs-410_1_6_205,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:07,020","00:15:11,500",205,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=907,"But in the case of d3, it matched news,",pic_cs-410_1_6_900.jpg
cs-410_1_6_206,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:12,530","00:15:17,960",206,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=912,So intuitively this reads better,pic_cs-410_1_6_900.jpg
cs-410_1_6_207,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:17,960","00:15:21,920",207,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=917,"is more important than matching about,",pic_cs-410_1_6_900.jpg
cs-410_1_6_208,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:21,920","00:15:24,910",208,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=921,even though about and,pic_cs-410_1_6_900.jpg
cs-410_1_6_209,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:26,170","00:15:30,730",209,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=926,"So intuitively,",pic_cs-410_1_6_900.jpg
cs-410_1_6_210,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:30,730","00:15:32,750",210,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=930,But this model doesn't do that.,pic_cs-410_1_6_900.jpg
cs-410_1_6_211,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:33,860","00:15:37,150",211,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=933,So that means this model,pic_cs-410_1_6_900.jpg
cs-410_1_6_212,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:37,150","00:15:39,109",212,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=937,We have to solve these problems.,pic_cs-410_1_6_900.jpg
cs-410_1_6_213,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:41,188","00:15:41,991",213,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=941,"To summarize,",pic_cs-410_1_6_900.jpg
cs-410_1_6_214,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:41,991","00:15:45,770",214,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=941,in this lecture we talked about how,pic_cs-410_1_6_900.jpg
cs-410_1_6_215,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:47,610","00:15:49,540",215,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=947,We mainly need to do three things.,pic_cs-410_1_6_900.jpg
cs-410_1_6_216,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:49,540","00:15:51,796",216,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=949,One is to define the dimension.,pic_cs-410_1_6_900.jpg
cs-410_1_6_217,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:51,796","00:15:59,896",217,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=951,The second is to decide how to place,pic_cs-410_1_6_900.jpg
cs-410_1_6_218,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:15:59,896","00:16:05,761",218,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=959,and to also place a query in,pic_cs-410_1_6_900.jpg
cs-410_1_6_219,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:07,862","00:16:11,900",219,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=967,And third is to define,pic_cs-410_1_6_960.jpg
cs-410_1_6_220,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:11,900","00:16:15,790",220,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=971,particularly the query vector and,pic_cs-410_1_6_960.jpg
cs-410_1_6_221,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:17,080","00:16:22,430",221,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=977,We also talked about various simple way,pic_cs-410_1_6_960.jpg
cs-410_1_6_222,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:22,430","00:16:27,910",222,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=982,"Indeed, that's probably the simplest",pic_cs-410_1_6_960.jpg
cs-410_1_6_223,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:27,910","00:16:31,480",223,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=987,"In this case,",pic_cs-410_1_6_960.jpg
cs-410_1_6_224,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:31,480","00:16:37,430",224,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=991,"We use a zero, 1 bit vector to",pic_cs-410_1_6_960.jpg
cs-410_1_6_225,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:37,430","00:16:42,690",225,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=997,"In this case, we basically only care",pic_cs-410_1_6_960.jpg
cs-410_1_6_226,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:42,690","00:16:43,790",226,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1002,We ignore the frequency.,pic_cs-410_1_6_960.jpg
cs-410_1_6_227,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:45,560","00:16:49,220",227,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1005,And we use the Dot Product,pic_cs-410_1_6_960.jpg
cs-410_1_6_228,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:50,360","00:16:53,304",228,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1010,"And with such a instantiation,",pic_cs-410_1_6_960.jpg
cs-410_1_6_229,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:53,304","00:16:58,870",229,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1013,we showed that the scoring,pic_cs-410_1_6_960.jpg
cs-410_1_6_230,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:16:58,870","00:17:03,260",230,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1018,a document based on the number of distinct,pic_cs-410_1_6_960.jpg
cs-410_1_6_231,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:17:04,650","00:17:09,800",231,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1024,We also showed that such a simple vector,pic_cs-410_1_6_1020.jpg
cs-410_1_6_232,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:17:09,800","00:17:10,720",232,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1029,we need to improve it.,pic_cs-410_1_6_1020.jpg
cs-410_1_6_233,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:17:12,540","00:17:18,797",233,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1032,And this is a topic that we're,pic_cs-410_1_6_1020.jpg
cs-410_1_6_234,cs-410,1,6, Vector Space Retrieval Model - Simplest Instantiation,"00:17:18,797","00:17:28,797",234,https://www.coursera.org/learn/cs-410/lecture/dM6kh?t=1038,[MUSIC],pic_cs-410_1_6_1020.jpg
cs-410_2_1_1,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:00,012","00:00:08,850",1,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=0,[SOUND],pic_cs-410_2_1_0.jpg
cs-410_2_1_2,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:08,850","00:00:12,546",2,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=8,"In this lecture, we are going to talk about how",pic_cs-410_2_1_0.jpg
cs-410_2_1_3,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:12,546","00:00:14,288",3,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=12,of the vector space model.,pic_cs-410_2_1_0.jpg
cs-410_2_1_4,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:17,448","00:00:22,110",4,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=17,This is a continued discussion,pic_cs-410_2_1_0.jpg
cs-410_2_1_5,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:22,110","00:00:26,859",5,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=22,We're going to focus on how to improve,pic_cs-410_2_1_0.jpg
cs-410_2_1_6,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:30,259","00:00:32,327",6,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=30,"In the previous lecture,",pic_cs-410_2_1_0.jpg
cs-410_2_1_7,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:32,327","00:00:38,155",7,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=32,you have seen that with simple,pic_cs-410_2_1_0.jpg
cs-410_2_1_8,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:38,155","00:00:43,889",8,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=38,we can come up with a simple scoring,pic_cs-410_2_1_0.jpg
cs-410_2_1_9,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:43,889","00:00:49,440",9,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=43,an account of how many unique query,pic_cs-410_2_1_0.jpg
cs-410_2_1_10,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:50,540","00:00:56,862",10,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=50,We also have seen that this function,pic_cs-410_2_1_0.jpg
cs-410_2_1_11,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:00:56,862","00:01:00,226",11,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=56,"In particular,",pic_cs-410_2_1_0.jpg
cs-410_2_1_12,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:00,226","00:01:05,210",12,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=60,they will all get the same score because,pic_cs-410_2_1_60.jpg
cs-410_2_1_13,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:06,322","00:01:11,330",13,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=66,But intuitively we would like,pic_cs-410_2_1_60.jpg
cs-410_2_1_14,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:11,330","00:01:13,070",14,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=71,d2 is really not relevant.,pic_cs-410_2_1_60.jpg
cs-410_2_1_15,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:14,750","00:01:22,600",15,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=74,So the problem here is that this function,pic_cs-410_2_1_60.jpg
cs-410_2_1_16,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:22,600","00:01:27,504",16,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=82,"First, we would like to give",pic_cs-410_2_1_60.jpg
cs-410_2_1_17,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:27,504","00:01:31,297",17,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=87,matched presidential more times than d3.,pic_cs-410_2_1_60.jpg
cs-410_2_1_18,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:32,520","00:01:37,657",18,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=92,"Second, intuitively, matching presidential",pic_cs-410_2_1_60.jpg
cs-410_2_1_19,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:37,657","00:01:42,808",19,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=97,"matching about, because about is a very",pic_cs-410_2_1_60.jpg
cs-410_2_1_20,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:42,808","00:01:44,970",20,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=102,It doesn't really carry that much content.,pic_cs-410_2_1_60.jpg
cs-410_2_1_21,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:47,480","00:01:48,945",21,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=107,"So in this lecture,",pic_cs-410_2_1_60.jpg
cs-410_2_1_22,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:48,945","00:01:53,868",22,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=108,let's see how we can improve the model,pic_cs-410_2_1_60.jpg
cs-410_2_1_23,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:01:53,868","00:01:59,990",23,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=113,It's worth thinking at this point,pic_cs-410_2_1_60.jpg
cs-410_2_1_24,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:01,420","00:02:06,600",24,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=121,If we look back at assumptions we have,pic_cs-410_2_1_120.jpg
cs-410_2_1_25,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:06,600","00:02:11,645",25,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=126,"space model,",pic_cs-410_2_1_120.jpg
cs-410_2_1_26,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:11,645","00:02:15,200",26,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=131,is really coming from,pic_cs-410_2_1_120.jpg
cs-410_2_1_27,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:15,200","00:02:19,391",27,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=135,"In particular, it has to do with how we",pic_cs-410_2_1_120.jpg
cs-410_2_1_28,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:22,380","00:02:25,390",28,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=142,"So then naturally,",pic_cs-410_2_1_120.jpg
cs-410_2_1_29,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:25,390","00:02:27,780",29,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=145,we have to revisit those assumptions.,pic_cs-410_2_1_120.jpg
cs-410_2_1_30,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:27,780","00:02:34,755",30,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=147,Perhaps we will have to use different ways,pic_cs-410_2_1_120.jpg
cs-410_2_1_31,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:34,755","00:02:39,130",31,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=154,"In particular, we have to place",pic_cs-410_2_1_120.jpg
cs-410_2_1_32,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:41,690","00:02:45,708",32,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=161,So let's see how we can improve this.,pic_cs-410_2_1_120.jpg
cs-410_2_1_33,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:45,708","00:02:50,248",33,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=165,One natural thought is in order to,pic_cs-410_2_1_120.jpg
cs-410_2_1_34,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:50,248","00:02:51,266",34,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=170,"the document,",pic_cs-410_2_1_120.jpg
cs-410_2_1_35,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:51,266","00:02:57,270",35,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=171,we should consider the term frequency,pic_cs-410_2_1_120.jpg
cs-410_2_1_36,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:02:57,270","00:03:02,900",36,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=177,In order to consider the difference,pic_cs-410_2_1_120.jpg
cs-410_2_1_37,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:02,900","00:03:07,620",37,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=182,term occurred multiple times and one,pic_cs-410_2_1_180.jpg
cs-410_2_1_38,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:07,620","00:03:12,010",38,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=187,"we have to consider the term frequency,",pic_cs-410_2_1_180.jpg
cs-410_2_1_39,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:13,130","00:03:18,200",39,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=193,"In the simplest model, we only modeled",pic_cs-410_2_1_180.jpg
cs-410_2_1_40,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:18,200","00:03:25,106",40,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=198,We ignored the actual number of times,pic_cs-410_2_1_180.jpg
cs-410_2_1_41,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:25,106","00:03:26,566",41,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=205,So let's add this back.,pic_cs-410_2_1_180.jpg
cs-410_2_1_42,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:26,566","00:03:30,592",42,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=206,So we're going to then,pic_cs-410_2_1_180.jpg
cs-410_2_1_43,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:30,592","00:03:34,214",43,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=210,a vector with term frequency as element.,pic_cs-410_2_1_180.jpg
cs-410_2_1_44,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:34,214","00:03:39,573",44,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=214,"So that is to say, now the elements",pic_cs-410_2_1_180.jpg
cs-410_2_1_45,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:39,573","00:03:43,489",45,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=219,the document vector will not be 0 or,pic_cs-410_2_1_180.jpg
cs-410_2_1_46,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:43,489","00:03:49,490",46,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=223,instead they will be the counts of,pic_cs-410_2_1_180.jpg
cs-410_2_1_47,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:52,140","00:03:55,340",47,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=232,So this would bring in additional,pic_cs-410_2_1_180.jpg
cs-410_2_1_48,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:03:55,340","00:04:00,650",48,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=235,this can be seen as more accurate,pic_cs-410_2_1_180.jpg
cs-410_2_1_49,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:00,650","00:04:03,849",49,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=240,So now let's see what the formula,pic_cs-410_2_1_240.jpg
cs-410_2_1_50,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:03,849","00:04:05,480",50,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=243,representation.,pic_cs-410_2_1_240.jpg
cs-410_2_1_51,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:05,480","00:04:08,920",51,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=245,"So as you'll see on this slide,",pic_cs-410_2_1_240.jpg
cs-410_2_1_52,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:10,090","00:04:14,270",52,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=250,And so the formula looks,pic_cs-410_2_1_240.jpg
cs-410_2_1_53,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:14,270","00:04:16,310",53,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=254,"In fact, it looks identical.",pic_cs-410_2_1_240.jpg
cs-410_2_1_54,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:16,310","00:04:21,178",54,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=256,"But inside the sum, of course,",pic_cs-410_2_1_240.jpg
cs-410_2_1_55,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:21,178","00:04:25,855",55,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=261,They are now the counts of word i in,pic_cs-410_2_1_240.jpg
cs-410_2_1_56,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:25,855","00:04:30,208",56,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=265,the query and in the document.,pic_cs-410_2_1_240.jpg
cs-410_2_1_57,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:30,208","00:04:35,931",57,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=270,Now at this point I also suggest you,pic_cs-410_2_1_240.jpg
cs-410_2_1_58,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:35,931","00:04:41,756",58,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=275,just to think about how we can interpret,pic_cs-410_2_1_240.jpg
cs-410_2_1_59,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:41,756","00:04:47,710",59,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=281,It's doing something very similar,pic_cs-410_2_1_240.jpg
cs-410_2_1_60,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:47,710","00:04:50,501",60,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=287,"But because of the change of the vector,",pic_cs-410_2_1_240.jpg
cs-410_2_1_61,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:50,501","00:04:54,038",61,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=290,now the new score has,pic_cs-410_2_1_240.jpg
cs-410_2_1_62,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:54,038","00:04:56,118",62,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=294,Can you see the difference?,pic_cs-410_2_1_240.jpg
cs-410_2_1_63,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:04:56,118","00:05:00,995",63,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=296,And it has to do with the consideration,pic_cs-410_2_1_240.jpg
cs-410_2_1_64,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:00,995","00:05:03,360",64,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=300,the same term in a document.,pic_cs-410_2_1_300.jpg
cs-410_2_1_65,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:03,360","00:05:06,590",65,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=303,"More importantly, we would like to know",pic_cs-410_2_1_300.jpg
cs-410_2_1_66,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:06,590","00:05:08,830",66,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=306,of the simplest vector space model.,pic_cs-410_2_1_300.jpg
cs-410_2_1_67,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:08,830","00:05:12,320",67,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=308,So let's look at this example again.,pic_cs-410_2_1_300.jpg
cs-410_2_1_68,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:12,320","00:05:16,670",68,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=312,So suppose we change the vector,pic_cs-410_2_1_300.jpg
cs-410_2_1_69,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:16,670","00:05:20,620",69,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=316,Now let's look at these,pic_cs-410_2_1_300.jpg
cs-410_2_1_70,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:20,620","00:05:24,580",70,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=320,The query vector is the same,pic_cs-410_2_1_300.jpg
cs-410_2_1_71,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:24,580","00:05:27,240",71,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=324,exactly once in the query.,pic_cs-410_2_1_300.jpg
cs-410_2_1_72,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:27,240","00:05:30,988",72,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=327,So the vector is still a 01 vector.,pic_cs-410_2_1_300.jpg
cs-410_2_1_73,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:30,988","00:05:35,472",73,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=330,"And in fact, d2 is also essentially",pic_cs-410_2_1_300.jpg
cs-410_2_1_74,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:35,472","00:05:40,120",74,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=335,because none of these words,pic_cs-410_2_1_300.jpg
cs-410_2_1_75,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:40,120","00:05:43,145",75,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=340,"As a result,",pic_cs-410_2_1_300.jpg
cs-410_2_1_76,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:45,410","00:05:49,760",76,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=345,"The same is true for d3,",pic_cs-410_2_1_300.jpg
cs-410_2_1_77,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:51,510","00:05:57,400",77,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=351,"But d4 would be different, because",pic_cs-410_2_1_300.jpg
cs-410_2_1_78,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:05:57,400","00:06:02,760",78,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=357,So the ending for presidential in the,pic_cs-410_2_1_300.jpg
cs-410_2_1_79,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:04,240","00:06:08,303",79,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=364,"As a result, now the score for",pic_cs-410_2_1_360.jpg
cs-410_2_1_80,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:08,303","00:06:09,050",80,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=368,It's a 4 now.,pic_cs-410_2_1_360.jpg
cs-410_2_1_81,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:10,130","00:06:13,380",81,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=370,"So this means by using term frequency,",pic_cs-410_2_1_360.jpg
cs-410_2_1_82,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:13,380","00:06:17,720",82,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=373,we can now rank d4 above d2 and,pic_cs-410_2_1_360.jpg
cs-410_2_1_83,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:19,250","00:06:23,725",83,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=379,So this solved the problem with d4.,pic_cs-410_2_1_360.jpg
cs-410_2_1_84,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:26,190","00:06:32,548",84,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=386,But we can also see that d2 and,pic_cs-410_2_1_360.jpg
cs-410_2_1_85,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:32,548","00:06:38,290",85,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=392,"They still have identical scores,",pic_cs-410_2_1_360.jpg
cs-410_2_1_86,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:40,420","00:06:42,434",86,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=400,So how can we fix this problem?,pic_cs-410_2_1_360.jpg
cs-410_2_1_87,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:42,434","00:06:46,261",87,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=402,"Intuitively, we would like",pic_cs-410_2_1_360.jpg
cs-410_2_1_88,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:46,261","00:06:49,736",88,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=406,matching presidential than matching about.,pic_cs-410_2_1_360.jpg
cs-410_2_1_89,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:49,736","00:06:53,028",89,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=409,But how can we solve,pic_cs-410_2_1_360.jpg
cs-410_2_1_90,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:53,028","00:06:57,651",90,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=413,Is there any way to determine,pic_cs-410_2_1_360.jpg
cs-410_2_1_91,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:06:57,651","00:07:02,478",91,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=417,more importantly and,pic_cs-410_2_1_360.jpg
cs-410_2_1_92,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:02,478","00:07:09,670",92,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=422,About is such a word which does not,pic_cs-410_2_1_420.jpg
cs-410_2_1_93,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:09,670","00:07:11,760",93,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=429,We can essentially ignore that.,pic_cs-410_2_1_420.jpg
cs-410_2_1_94,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:11,760","00:07:15,110",94,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=431,We sometimes call such,pic_cs-410_2_1_420.jpg
cs-410_2_1_95,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:15,110","00:07:18,710",95,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=435,Those are generally very frequent and,pic_cs-410_2_1_420.jpg
cs-410_2_1_96,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:18,710","00:07:21,570",96,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=438,Matching it doesn't really mean anything.,pic_cs-410_2_1_420.jpg
cs-410_2_1_97,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:21,570","00:07:23,260",97,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=441,But computationally how,pic_cs-410_2_1_420.jpg
cs-410_2_1_98,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:24,960","00:07:27,830",98,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=444,"So again, I encourage you to",pic_cs-410_2_1_420.jpg
cs-410_2_1_99,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:29,460","00:07:33,000",99,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=449,Can you came up with any statistical,pic_cs-410_2_1_420.jpg
cs-410_2_1_100,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:33,000","00:07:34,358",100,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=453,presidential from about?,pic_cs-410_2_1_420.jpg
cs-410_2_1_101,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:37,109","00:07:39,691",101,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=457,"Now if you think about it for a moment,",pic_cs-410_2_1_420.jpg
cs-410_2_1_102,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:39,691","00:07:46,170",102,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=459,you'll realize that one difference is,pic_cs-410_2_1_420.jpg
cs-410_2_1_103,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:46,170","00:07:50,764",103,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=466,So if you count the occurrence of,pic_cs-410_2_1_420.jpg
cs-410_2_1_104,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:50,764","00:07:55,852",104,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=470,then we will see that about has much,pic_cs-410_2_1_420.jpg
cs-410_2_1_105,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:07:55,852","00:07:58,990",105,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=475,which tends to occur,pic_cs-410_2_1_420.jpg
cs-410_2_1_106,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:01,000","00:08:05,887",106,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=481,So this idea suggests,pic_cs-410_2_1_480.jpg
cs-410_2_1_107,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:05,887","00:08:09,396",107,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=485,the global statistics of terms or,pic_cs-410_2_1_480.jpg
cs-410_2_1_108,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:09,396","00:08:14,660",108,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=489,some other information,pic_cs-410_2_1_480.jpg
cs-410_2_1_109,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:14,660","00:08:20,568",109,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=494,the element of about in,pic_cs-410_2_1_480.jpg
cs-410_2_1_110,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:20,568","00:08:24,754",110,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=500,"At the same time,",pic_cs-410_2_1_480.jpg
cs-410_2_1_111,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:24,754","00:08:29,278",111,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=504,the weight of presidential,pic_cs-410_2_1_480.jpg
cs-410_2_1_112,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:29,278","00:08:34,284",112,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=509,"If we can do that, then we can",pic_cs-410_2_1_480.jpg
cs-410_2_1_113,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:34,284","00:08:39,036",113,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=514,score to be less than 3 while,pic_cs-410_2_1_480.jpg
cs-410_2_1_114,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:39,036","00:08:42,996",114,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=519,Then we would be able to,pic_cs-410_2_1_480.jpg
cs-410_2_1_115,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:45,138","00:08:47,320",115,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=525,So how can we do this systematically?,pic_cs-410_2_1_480.jpg
cs-410_2_1_116,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:48,730","00:08:52,030",116,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=528,"Again, we can rely on",pic_cs-410_2_1_480.jpg
cs-410_2_1_117,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:52,030","00:08:57,218",117,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=532,"And in this case, the particular idea",pic_cs-410_2_1_480.jpg
cs-410_2_1_118,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:08:57,218","00:09:01,425",118,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=537,Now we have seen document,pic_cs-410_2_1_480.jpg
cs-410_2_1_119,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:01,425","00:09:04,030",119,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=541,the modern retrieval functions.,pic_cs-410_2_1_540.jpg
cs-410_2_1_120,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:05,800","00:09:08,500",120,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=545,We discussed this in a previous lecture.,pic_cs-410_2_1_540.jpg
cs-410_2_1_121,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:08,500","00:09:10,859",121,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=548,So here is the specific way of using it.,pic_cs-410_2_1_540.jpg
cs-410_2_1_122,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:10,859","00:09:15,910",122,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=550,Document frequency is the count of,pic_cs-410_2_1_540.jpg
cs-410_2_1_123,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:15,910","00:09:21,000",123,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=555,Here we say inverse document frequency,pic_cs-410_2_1_540.jpg
cs-410_2_1_124,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:21,000","00:09:22,700",124,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=561,that doesn't occur in many documents.,pic_cs-410_2_1_540.jpg
cs-410_2_1_125,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:24,890","00:09:30,544",125,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=564,And so the way to incorporate this,pic_cs-410_2_1_540.jpg
cs-410_2_1_126,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:30,544","00:09:35,477",126,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=570,is then to modify the frequency,pic_cs-410_2_1_540.jpg
cs-410_2_1_127,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:35,477","00:09:39,918",127,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=575,"the IDF of the corresponding word,",pic_cs-410_2_1_540.jpg
cs-410_2_1_128,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:39,918","00:09:46,044",128,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=579,"If we can do that,",pic_cs-410_2_1_540.jpg
cs-410_2_1_129,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:46,044","00:09:50,401",129,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=586,"which generally have a lower IDF, and",pic_cs-410_2_1_540.jpg
cs-410_2_1_130,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:50,401","00:09:56,138",130,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=590,"reward rare words,",pic_cs-410_2_1_540.jpg
cs-410_2_1_131,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:56,138","00:09:58,078",131,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=596,"So more specifically,",pic_cs-410_2_1_540.jpg
cs-410_2_1_132,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:09:58,078","00:10:03,025",132,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=598,the IDF can be defined as,pic_cs-410_2_1_540.jpg
cs-410_2_1_133,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:03,025","00:10:08,845",133,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=603,where M is the total number of documents,pic_cs-410_2_1_600.jpg
cs-410_2_1_134,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:08,845","00:10:15,058",134,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=608,"document frequency, the total number",pic_cs-410_2_1_600.jpg
cs-410_2_1_135,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:15,058","00:10:18,596",135,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=615,Now if you plot this,pic_cs-410_2_1_600.jpg
cs-410_2_1_136,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:18,596","00:10:23,430",136,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=618,then you would see the curve,pic_cs-410_2_1_600.jpg
cs-410_2_1_137,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:23,430","00:10:28,273",137,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=623,"In general, you can see it",pic_cs-410_2_1_600.jpg
cs-410_2_1_138,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:28,273","00:10:30,704",138,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=628,"a low DF word, a rare word.",pic_cs-410_2_1_600.jpg
cs-410_2_1_139,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:34,220","00:10:38,680",139,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=634,You can also see the maximum value,pic_cs-410_2_1_600.jpg
cs-410_2_1_140,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:40,952","00:10:45,158",140,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=640,It would be interesting for you to think,pic_cs-410_2_1_600.jpg
cs-410_2_1_141,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:45,158","00:10:46,900",141,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=645,this function.,pic_cs-410_2_1_600.jpg
cs-410_2_1_142,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:46,900","00:10:48,368",142,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=646,This could be an interesting exercise.,pic_cs-410_2_1_600.jpg
cs-410_2_1_143,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:50,918","00:10:55,238",143,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=650,Now the specific function,pic_cs-410_2_1_600.jpg
cs-410_2_1_144,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:10:55,238","00:10:59,470",144,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=655,the heuristic to simply,pic_cs-410_2_1_600.jpg
cs-410_2_1_145,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:01,528","00:11:05,800",145,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=661,But it turns out that this particular,pic_cs-410_2_1_660.jpg
cs-410_2_1_146,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:07,340","00:11:12,221",146,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=667,Now whether there's a better,pic_cs-410_2_1_660.jpg
cs-410_2_1_147,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:12,221","00:11:14,939",147,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=672,the open research question.,pic_cs-410_2_1_660.jpg
cs-410_2_1_148,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:14,939","00:11:19,665",148,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=674,But it's also clear that if,pic_cs-410_2_1_660.jpg
cs-410_2_1_149,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:19,665","00:11:22,945",149,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=679,"like what's shown here with this line,",pic_cs-410_2_1_660.jpg
cs-410_2_1_150,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:22,945","00:11:27,200",150,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=682,then it may not be as,pic_cs-410_2_1_660.jpg
cs-410_2_1_151,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:29,110","00:11:34,270",151,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=689,"In particular, you can see",pic_cs-410_2_1_660.jpg
cs-410_2_1_152,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:35,940","00:11:39,870",152,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=695,and we somehow have,pic_cs-410_2_1_660.jpg
cs-410_2_1_153,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:41,110","00:11:45,770",153,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=701,"After this point, we're going to say these",pic_cs-410_2_1_660.jpg
cs-410_2_1_154,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:45,770","00:11:48,180",154,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=705,They can be essentially ignored.,pic_cs-410_2_1_660.jpg
cs-410_2_1_155,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:48,180","00:11:52,110",155,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=708,And this makes sense when,pic_cs-410_2_1_660.jpg
cs-410_2_1_156,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:52,110","00:11:57,310",156,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=712,let's say a term occurs in more,pic_cs-410_2_1_660.jpg
cs-410_2_1_157,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:11:57,310","00:12:01,700",157,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=717,then the term is unlikely very important,pic_cs-410_2_1_660.jpg
cs-410_2_1_158,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:03,120","00:12:05,150",158,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=723,It's not very important,pic_cs-410_2_1_720.jpg
cs-410_2_1_159,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:05,150","00:12:10,145",159,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=725,So with the standard IDF you can,pic_cs-410_2_1_720.jpg
cs-410_2_1_160,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:10,145","00:12:12,285",160,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=730,they all have low weights.,pic_cs-410_2_1_720.jpg
cs-410_2_1_161,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:12,285","00:12:14,020",161,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=732,There's no difference.,pic_cs-410_2_1_720.jpg
cs-410_2_1_162,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:14,020","00:12:16,413",162,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=734,But if you look at,pic_cs-410_2_1_720.jpg
cs-410_2_1_163,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:16,413","00:12:19,206",163,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=736,at this point that there,pic_cs-410_2_1_720.jpg
cs-410_2_1_164,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:19,206","00:12:26,123",164,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=739,So intuitively we'd want to,pic_cs-410_2_1_720.jpg
cs-410_2_1_165,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:26,123","00:12:31,450",165,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=746,of low DF words rather,pic_cs-410_2_1_720.jpg
cs-410_2_1_166,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:32,990","00:12:37,972",166,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=752,"Well, of course,",pic_cs-410_2_1_720.jpg
cs-410_2_1_167,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:37,972","00:12:43,168",167,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=757,validated by using the empirically,pic_cs-410_2_1_720.jpg
cs-410_2_1_168,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:43,168","00:12:46,920",168,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=763,And we have to use users to,pic_cs-410_2_1_720.jpg
cs-410_2_1_169,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:48,580","00:12:52,948",169,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=768,So now let's see how,pic_cs-410_2_1_720.jpg
cs-410_2_1_170,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:52,948","00:12:55,000",170,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=772,So now let's look at,pic_cs-410_2_1_720.jpg
cs-410_2_1_171,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:12:56,100","00:13:00,530",171,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=776,"Now without the IDF weighting before,",pic_cs-410_2_1_720.jpg
cs-410_2_1_172,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:00,530","00:13:05,810",172,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=780,But with IDF weighting we,pic_cs-410_2_1_780.jpg
cs-410_2_1_173,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:05,810","00:13:09,520",173,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=785,by multiplying with the IDF value.,pic_cs-410_2_1_780.jpg
cs-410_2_1_174,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:09,520","00:13:14,150",174,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=789,"For example,",pic_cs-410_2_1_780.jpg
cs-410_2_1_175,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:14,150","00:13:19,680",175,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=794,in particular for about there's adjustment,pic_cs-410_2_1_780.jpg
cs-410_2_1_176,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:19,680","00:13:23,980",176,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=799,which is smaller than the IDF,pic_cs-410_2_1_780.jpg
cs-410_2_1_177,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:23,980","00:13:28,930",177,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=803,"So if you look at these,",pic_cs-410_2_1_780.jpg
cs-410_2_1_178,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:28,930","00:13:34,390",178,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=808,"As a result, adjustment here would be",pic_cs-410_2_1_780.jpg
cs-410_2_1_179,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:37,190","00:13:44,035",179,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=817,"So if we score with these new vectors,",pic_cs-410_2_1_780.jpg
cs-410_2_1_180,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:44,035","00:13:48,752",180,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=824,"of course,",pic_cs-410_2_1_780.jpg
cs-410_2_1_181,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:48,752","00:13:54,830",181,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=828,"campaign, but the matching of",pic_cs-410_2_1_780.jpg
cs-410_2_1_182,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:13:54,830","00:14:01,250",182,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=834,"So now as a result of IDF weighting,",pic_cs-410_2_1_780.jpg
cs-410_2_1_183,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:01,250","00:14:06,460",183,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=841,"because it matched a rare word,",pic_cs-410_2_1_840.jpg
cs-410_2_1_184,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:06,460","00:14:10,156",184,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=846,So this shows that the IDF,pic_cs-410_2_1_840.jpg
cs-410_2_1_185,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:12,798","00:14:19,434",185,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=852,So how effective is this model in,pic_cs-410_2_1_840.jpg
cs-410_2_1_186,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:19,434","00:14:23,438",186,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=859,"Well, let's look at all these",pic_cs-410_2_1_840.jpg
cs-410_2_1_187,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:23,438","00:14:28,100",187,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=863,These are the new scores,pic_cs-410_2_1_840.jpg
cs-410_2_1_188,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:28,100","00:14:32,580",188,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=868,But how effective is this new weighting,pic_cs-410_2_1_840.jpg
cs-410_2_1_189,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:33,770","00:14:38,520",189,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=873,So now let's see overall how effective,pic_cs-410_2_1_840.jpg
cs-410_2_1_190,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:38,520","00:14:39,490",190,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=878,with TF-IDF weighting.,pic_cs-410_2_1_840.jpg
cs-410_2_1_191,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:40,630","00:14:44,330",191,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=880,Here we show all the five documents,pic_cs-410_2_1_840.jpg
cs-410_2_1_192,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:44,330","00:14:45,720",192,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=884,these are their scores.,pic_cs-410_2_1_840.jpg
cs-410_2_1_193,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:47,000","00:14:49,760",193,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=887,Now we can see the scores for,pic_cs-410_2_1_840.jpg
cs-410_2_1_194,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:49,760","00:14:56,410",194,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=889,the first four documents here,pic_cs-410_2_1_840.jpg
cs-410_2_1_195,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:56,410","00:14:57,650",195,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=896,They are as we expected.,pic_cs-410_2_1_840.jpg
cs-410_2_1_196,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:14:58,740","00:15:05,710",196,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=898,"However, we also see a new",pic_cs-410_2_1_840.jpg
cs-410_2_1_197,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:05,710","00:15:10,490",197,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=905,which did not have a very high score,pic_cs-410_2_1_900.jpg
cs-410_2_1_198,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:10,490","00:15:13,270",198,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=910,now actually has a very high score.,pic_cs-410_2_1_900.jpg
cs-410_2_1_199,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:13,270","00:15:15,110",199,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=913,"In fact, it has the highest score here.",pic_cs-410_2_1_900.jpg
cs-410_2_1_200,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:16,850","00:15:19,002",200,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=916,So this creates a new problem.,pic_cs-410_2_1_900.jpg
cs-410_2_1_201,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:19,002","00:15:23,080",201,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=919,This is actually a common phenomenon,pic_cs-410_2_1_900.jpg
cs-410_2_1_202,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:23,080","00:15:25,570",202,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=923,"Basically, when you try",pic_cs-410_2_1_900.jpg
cs-410_2_1_203,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:25,570","00:15:27,960",203,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=925,you tend to introduce other problems.,pic_cs-410_2_1_900.jpg
cs-410_2_1_204,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:27,960","00:15:32,674",204,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=927,And that's why it's very tricky how,pic_cs-410_2_1_900.jpg
cs-410_2_1_205,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:32,674","00:15:39,658",205,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=932,And what's the best ranking function,pic_cs-410_2_1_900.jpg
cs-410_2_1_206,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:39,658","00:15:41,170",206,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=939,Researchers are still working on that.,pic_cs-410_2_1_900.jpg
cs-410_2_1_207,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:42,360","00:15:47,530",207,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=942,But in the next few lectures we're going,pic_cs-410_2_1_900.jpg
cs-410_2_1_208,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:47,530","00:15:53,030",208,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=947,ideas to further improve this model and,pic_cs-410_2_1_900.jpg
cs-410_2_1_209,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:15:55,920","00:16:00,740",209,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=955,"So to summarize this lecture, we've talked",pic_cs-410_2_1_900.jpg
cs-410_2_1_210,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:00,740","00:16:04,340",210,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=960,"model, and",pic_cs-410_2_1_960.jpg
cs-410_2_1_211,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:04,340","00:16:08,470",211,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=964,the vector space model,pic_cs-410_2_1_960.jpg
cs-410_2_1_212,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:08,470","00:16:13,573",212,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=968,So the improvement is mostly on,pic_cs-410_2_1_960.jpg
cs-410_2_1_213,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:13,573","00:16:18,673",213,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=973,give high weight to a term that,pic_cs-410_2_1_960.jpg
cs-410_2_1_214,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:18,673","00:16:21,790",214,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=978,infrequently in the whole collection.,pic_cs-410_2_1_960.jpg
cs-410_2_1_215,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:23,630","00:16:26,210",215,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=983,And we have seen that this,pic_cs-410_2_1_960.jpg
cs-410_2_1_216,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:26,210","00:16:29,440",216,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=986,looks better than the simplest,pic_cs-410_2_1_960.jpg
cs-410_2_1_217,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:29,440","00:16:33,268",217,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=989,But it also still has some problems.,pic_cs-410_2_1_960.jpg
cs-410_2_1_218,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:33,268","00:16:40,448",218,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=993,In the next lecture we're going to look at,pic_cs-410_2_1_960.jpg
cs-410_2_1_219,cs-410,2,1, Vector Space Model - Improved Instantiation,"00:16:40,448","00:16:50,448",219,https://www.coursera.org/learn/cs-410/lecture/7jqJI?t=1000,[MUSIC],pic_cs-410_2_1_960.jpg
cs-410_2_2_1,cs-410,2,2, TF Transformation,"00:00:00,000","00:00:05,293",1,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=0,[MUSIC],pic_cs-410_2_2_0.jpg
cs-410_2_2_2,cs-410,2,2, TF Transformation,"00:00:10,067","00:00:15,310",2,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=10,"In this lecture, we continue",pic_cs-410_2_2_0.jpg
cs-410_2_2_3,cs-410,2,2, TF Transformation,"00:00:15,310","00:00:18,810",3,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=15,"In particular, we're going to",pic_cs-410_2_2_0.jpg
cs-410_2_2_4,cs-410,2,2, TF Transformation,"00:00:18,810","00:00:20,100",4,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=18,"In the previous lecture,",pic_cs-410_2_2_0.jpg
cs-410_2_2_5,cs-410,2,2, TF Transformation,"00:00:20,100","00:00:25,880",5,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=20,we have derived a TF idea of weighting,pic_cs-410_2_2_0.jpg
cs-410_2_2_6,cs-410,2,2, TF Transformation,"00:00:27,100","00:00:31,760",6,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=27,And we have assumed that this model,pic_cs-410_2_2_0.jpg
cs-410_2_2_7,cs-410,2,2, TF Transformation,"00:00:31,760","00:00:37,302",7,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=31,"these examples as shown on this slide,",pic_cs-410_2_2_0.jpg
cs-410_2_2_8,cs-410,2,2, TF Transformation,"00:00:37,302","00:00:41,340",8,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=37,"d5, which has received a very high score.",pic_cs-410_2_2_0.jpg
cs-410_2_2_9,cs-410,2,2, TF Transformation,"00:00:41,340","00:00:46,510",9,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=41,"Indeed, it has received the highest",pic_cs-410_2_2_0.jpg
cs-410_2_2_10,cs-410,2,2, TF Transformation,"00:00:46,510","00:00:51,980",10,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=46,But this document is intuitive and,pic_cs-410_2_2_0.jpg
cs-410_2_2_11,cs-410,2,2, TF Transformation,"00:00:53,240","00:00:55,390",11,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=53,"In this lecture,",pic_cs-410_2_2_0.jpg
cs-410_2_2_12,cs-410,2,2, TF Transformation,"00:00:55,390","00:00:58,960",12,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=55,how we're going to use TF,pic_cs-410_2_2_0.jpg
cs-410_2_2_13,cs-410,2,2, TF Transformation,"00:01:00,410","00:01:04,870",13,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=60,"Before we discuss the details,",pic_cs-410_2_2_60.jpg
cs-410_2_2_14,cs-410,2,2, TF Transformation,"00:01:04,870","00:01:08,820",14,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=64,this simple TF-IDF,pic_cs-410_2_2_60.jpg
cs-410_2_2_15,cs-410,2,2, TF Transformation,"00:01:08,820","00:01:13,520",15,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=68,And see why this document has,pic_cs-410_2_2_60.jpg
cs-410_2_2_16,cs-410,2,2, TF Transformation,"00:01:13,520","00:01:17,510",16,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=73,"So this is the formula, and",pic_cs-410_2_2_60.jpg
cs-410_2_2_17,cs-410,2,2, TF Transformation,"00:01:17,510","00:01:21,730",17,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=77,then you will see it involves a sum,pic_cs-410_2_2_60.jpg
cs-410_2_2_18,cs-410,2,2, TF Transformation,"00:01:23,810","00:01:28,140",18,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=83,"And inside the sum, each matched",pic_cs-410_2_2_60.jpg
cs-410_2_2_19,cs-410,2,2, TF Transformation,"00:01:28,140","00:01:30,259",19,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=88,And this weight is TF-IDF weighting.,pic_cs-410_2_2_60.jpg
cs-410_2_2_20,cs-410,2,2, TF Transformation,"00:01:31,580","00:01:36,853",20,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=91,"So it has an idea of component,",pic_cs-410_2_2_60.jpg
cs-410_2_2_21,cs-410,2,2, TF Transformation,"00:01:36,853","00:01:42,586",21,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=96,One is the total number of documents,pic_cs-410_2_2_60.jpg
cs-410_2_2_22,cs-410,2,2, TF Transformation,"00:01:42,586","00:01:45,890",22,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=102,The other is the document of frequency.,pic_cs-410_2_2_60.jpg
cs-410_2_2_23,cs-410,2,2, TF Transformation,"00:01:45,890","00:01:48,220",23,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=105,This is the number of,pic_cs-410_2_2_60.jpg
cs-410_2_2_24,cs-410,2,2, TF Transformation,"00:01:48,220","00:01:49,070",24,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=108,This word w.,pic_cs-410_2_2_60.jpg
cs-410_2_2_25,cs-410,2,2, TF Transformation,"00:01:49,070","00:01:53,810",25,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=109,The other variables,pic_cs-410_2_2_60.jpg
cs-410_2_2_26,cs-410,2,2, TF Transformation,"00:01:53,810","00:01:58,350",26,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=113,involved in the formula include,pic_cs-410_2_2_60.jpg
cs-410_2_2_27,cs-410,2,2, TF Transformation,"00:02:01,440","00:02:06,100",27,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=121,"W in the query, and",pic_cs-410_2_2_120.jpg
cs-410_2_2_28,cs-410,2,2, TF Transformation,"00:02:07,650","00:02:12,150",28,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=127,"If you look at this document again,",pic_cs-410_2_2_120.jpg
cs-410_2_2_29,cs-410,2,2, TF Transformation,"00:02:12,150","00:02:16,710",29,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=132,the reason why it hasn't,pic_cs-410_2_2_120.jpg
cs-410_2_2_30,cs-410,2,2, TF Transformation,"00:02:16,710","00:02:21,035",30,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=136,it has a very high count of campaign.,pic_cs-410_2_2_120.jpg
cs-410_2_2_31,cs-410,2,2, TF Transformation,"00:02:21,035","00:02:27,170",31,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=141,So the count of campaign in this document,pic_cs-410_2_2_120.jpg
cs-410_2_2_32,cs-410,2,2, TF Transformation,"00:02:27,170","00:02:31,580",32,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=147,"the other documents, and has contributed",pic_cs-410_2_2_120.jpg
cs-410_2_2_33,cs-410,2,2, TF Transformation,"00:02:31,580","00:02:35,485",33,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=151,So in treating the amount,pic_cs-410_2_2_120.jpg
cs-410_2_2_34,cs-410,2,2, TF Transformation,"00:02:35,485","00:02:40,695",34,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=155,"this document, we need to somehow",pic_cs-410_2_2_120.jpg
cs-410_2_2_35,cs-410,2,2, TF Transformation,"00:02:40,695","00:02:44,514",35,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=160,of the matching of this,pic_cs-410_2_2_120.jpg
cs-410_2_2_36,cs-410,2,2, TF Transformation,"00:02:44,514","00:02:49,934",36,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=164,And if you think about the matching,pic_cs-410_2_2_120.jpg
cs-410_2_2_37,cs-410,2,2, TF Transformation,"00:02:49,934","00:02:52,193",37,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=169,"you actually would realize,",pic_cs-410_2_2_120.jpg
cs-410_2_2_38,cs-410,2,2, TF Transformation,"00:02:52,193","00:02:57,540",38,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=172,we probably shouldn't reward,pic_cs-410_2_2_120.jpg
cs-410_2_2_39,cs-410,2,2, TF Transformation,"00:02:57,540","00:03:02,406",39,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=177,"And by that I mean,",pic_cs-410_2_2_120.jpg
cs-410_2_2_40,cs-410,2,2, TF Transformation,"00:03:02,406","00:03:06,680",40,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=182,says a lot about,pic_cs-410_2_2_180.jpg
cs-410_2_2_41,cs-410,2,2, TF Transformation,"00:03:06,680","00:03:11,570",41,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=186,because it goes from zero,pic_cs-410_2_2_180.jpg
cs-410_2_2_42,cs-410,2,2, TF Transformation,"00:03:11,570","00:03:15,370",42,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=191,And that increase means a lot.,pic_cs-410_2_2_180.jpg
cs-410_2_2_43,cs-410,2,2, TF Transformation,"00:03:17,160","00:03:19,277",43,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=197,"Once we see a word in the document,",pic_cs-410_2_2_180.jpg
cs-410_2_2_44,cs-410,2,2, TF Transformation,"00:03:19,277","00:03:23,219",44,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=199,it's very likely that the document,pic_cs-410_2_2_180.jpg
cs-410_2_2_45,cs-410,2,2, TF Transformation,"00:03:23,219","00:03:27,934",45,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=203,If we see a extra occurrence on,pic_cs-410_2_2_180.jpg
cs-410_2_2_46,cs-410,2,2, TF Transformation,"00:03:27,934","00:03:33,493",46,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=207,"that is to go from one to two,",pic_cs-410_2_2_180.jpg
cs-410_2_2_47,cs-410,2,2, TF Transformation,"00:03:33,493","00:03:39,844",47,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=213,occurrence kind of confirmed that it's,pic_cs-410_2_2_180.jpg
cs-410_2_2_48,cs-410,2,2, TF Transformation,"00:03:39,844","00:03:44,220",48,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=219,Now we are more sure that this,pic_cs-410_2_2_180.jpg
cs-410_2_2_49,cs-410,2,2, TF Transformation,"00:03:44,220","00:03:50,430",49,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=224,"But imagine we have seen, let's say,",pic_cs-410_2_2_180.jpg
cs-410_2_2_50,cs-410,2,2, TF Transformation,"00:03:50,430","00:03:56,140",50,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=230,"Now, adding one extra occurrence is not",pic_cs-410_2_2_180.jpg
cs-410_2_2_51,cs-410,2,2, TF Transformation,"00:03:56,140","00:03:59,580",51,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=236,because we're already sure that,pic_cs-410_2_2_180.jpg
cs-410_2_2_52,cs-410,2,2, TF Transformation,"00:04:01,160","00:04:06,656",52,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=241,"So if you're thinking this way, it seems",pic_cs-410_2_2_240.jpg
cs-410_2_2_53,cs-410,2,2, TF Transformation,"00:04:06,656","00:04:12,785",53,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=246,"of a high count of a term, and",pic_cs-410_2_2_240.jpg
cs-410_2_2_54,cs-410,2,2, TF Transformation,"00:04:12,785","00:04:17,965",54,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=252,So this transformation function is,pic_cs-410_2_2_240.jpg
cs-410_2_2_55,cs-410,2,2, TF Transformation,"00:04:17,965","00:04:22,990",55,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=257,word into a term frequency weight for,pic_cs-410_2_2_240.jpg
cs-410_2_2_56,cs-410,2,2, TF Transformation,"00:04:22,990","00:04:28,420",56,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=262,"So here I show in x axis that we'll count,",pic_cs-410_2_2_240.jpg
cs-410_2_2_57,cs-410,2,2, TF Transformation,"00:04:28,420","00:04:31,470",57,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=268,y axis I show the term frequency weight.,pic_cs-410_2_2_240.jpg
cs-410_2_2_58,cs-410,2,2, TF Transformation,"00:04:33,360","00:04:36,370",58,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=273,"So in the previous breaking functions,",pic_cs-410_2_2_240.jpg
cs-410_2_2_59,cs-410,2,2, TF Transformation,"00:04:36,370","00:04:41,140",59,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=276,we actually have imprison rate,pic_cs-410_2_2_240.jpg
cs-410_2_2_60,cs-410,2,2, TF Transformation,"00:04:41,140","00:04:43,480",60,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=281,"So for example,",pic_cs-410_2_2_240.jpg
cs-410_2_2_61,cs-410,2,2, TF Transformation,"00:04:44,960","00:04:49,070",61,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=284,we actually use such a transformation,pic_cs-410_2_2_240.jpg
cs-410_2_2_62,cs-410,2,2, TF Transformation,"00:04:49,070","00:04:53,420",62,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=289,"Basically if the count is 0,",pic_cs-410_2_2_240.jpg
cs-410_2_2_63,cs-410,2,2, TF Transformation,"00:04:53,420","00:04:56,790",63,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=293,otherwise it would have a weight of 1.,pic_cs-410_2_2_240.jpg
cs-410_2_2_64,cs-410,2,2, TF Transformation,"00:04:56,790","00:04:57,940",64,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=296,It's flat.,pic_cs-410_2_2_240.jpg
cs-410_2_2_65,cs-410,2,2, TF Transformation,"00:04:59,550","00:05:04,870",65,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=299,"Now, what about using",pic_cs-410_2_2_240.jpg
cs-410_2_2_66,cs-410,2,2, TF Transformation,"00:05:04,870","00:05:10,515",66,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=304,"Well, that's a linear function, so it has",pic_cs-410_2_2_300.jpg
cs-410_2_2_67,cs-410,2,2, TF Transformation,"00:05:11,575","00:05:16,775",67,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=311,Now we have just seen that,pic_cs-410_2_2_300.jpg
cs-410_2_2_68,cs-410,2,2, TF Transformation,"00:05:18,395","00:05:20,695",68,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=318,So what we want is something like this.,pic_cs-410_2_2_300.jpg
cs-410_2_2_69,cs-410,2,2, TF Transformation,"00:05:20,695","00:05:23,160",69,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=320,"So for example,",pic_cs-410_2_2_300.jpg
cs-410_2_2_70,cs-410,2,2, TF Transformation,"00:05:23,160","00:05:26,620",70,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=323,we can't have a sublinear,pic_cs-410_2_2_300.jpg
cs-410_2_2_71,cs-410,2,2, TF Transformation,"00:05:26,620","00:05:29,850",71,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=326,And this will control the influence,pic_cs-410_2_2_300.jpg
cs-410_2_2_72,cs-410,2,2, TF Transformation,"00:05:29,850","00:05:32,270",72,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=329,because it's going to lower its inference.,pic_cs-410_2_2_300.jpg
cs-410_2_2_73,cs-410,2,2, TF Transformation,"00:05:32,270","00:05:35,060",73,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=332,"Yet, it will retain",pic_cs-410_2_2_300.jpg
cs-410_2_2_74,cs-410,2,2, TF Transformation,"00:05:36,110","00:05:41,570",74,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=336,Or we might want to even bend the curve,pic_cs-410_2_2_300.jpg
cs-410_2_2_75,cs-410,2,2, TF Transformation,"00:05:42,730","00:05:45,320",75,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=342,Now people have tried all these methods.,pic_cs-410_2_2_300.jpg
cs-410_2_2_76,cs-410,2,2, TF Transformation,"00:05:45,320","00:05:48,870",76,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=345,And they are indeed working better than,pic_cs-410_2_2_300.jpg
cs-410_2_2_77,cs-410,2,2, TF Transformation,"00:05:50,230","00:05:54,820",77,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=350,"But so far, what works the best seems",pic_cs-410_2_2_300.jpg
cs-410_2_2_78,cs-410,2,2, TF Transformation,"00:05:54,820","00:05:56,620",78,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=354,called a BM25 transformation.,pic_cs-410_2_2_300.jpg
cs-410_2_2_79,cs-410,2,2, TF Transformation,"00:05:58,070","00:05:59,480",79,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=358,BM stands for best matching.,pic_cs-410_2_2_300.jpg
cs-410_2_2_80,cs-410,2,2, TF Transformation,"00:06:01,210","00:06:04,830",80,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=361,"Now in this transformation,",pic_cs-410_2_2_360.jpg
cs-410_2_2_81,cs-410,2,2, TF Transformation,"00:06:06,460","00:06:10,910",81,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=366,And this k controls the upper,pic_cs-410_2_2_360.jpg
cs-410_2_2_82,cs-410,2,2, TF Transformation,"00:06:10,910","00:06:15,165",82,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=370,It's easy to see this,pic_cs-410_2_2_360.jpg
cs-410_2_2_83,cs-410,2,2, TF Transformation,"00:06:15,165","00:06:21,748",83,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=375,because if you look at the x divided by,pic_cs-410_2_2_360.jpg
cs-410_2_2_84,cs-410,2,2, TF Transformation,"00:06:21,748","00:06:28,060",84,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=381,then the numerator will never be able,pic_cs-410_2_2_360.jpg
cs-410_2_2_85,cs-410,2,2, TF Transformation,"00:06:28,060","00:06:29,820",85,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=388,So it's upper bounded by k+1.,pic_cs-410_2_2_360.jpg
cs-410_2_2_86,cs-410,2,2, TF Transformation,"00:06:29,820","00:06:34,540",86,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=389,"Now, this is also difference between",pic_cs-410_2_2_360.jpg
cs-410_2_2_87,cs-410,2,2, TF Transformation,"00:06:34,540","00:06:35,660",87,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=394,a logarithm transformation.,pic_cs-410_2_2_360.jpg
cs-410_2_2_88,cs-410,2,2, TF Transformation,"00:06:37,010","00:06:38,450",88,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=397,Which it doesn't have upper bound.,pic_cs-410_2_2_360.jpg
cs-410_2_2_89,cs-410,2,2, TF Transformation,"00:06:39,830","00:06:44,490",89,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=399,"Furthermore, one interesting property",pic_cs-410_2_2_360.jpg
cs-410_2_2_90,cs-410,2,2, TF Transformation,"00:06:45,610","00:06:50,310",90,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=405,we can actually simulate different,pic_cs-410_2_2_360.jpg
cs-410_2_2_91,cs-410,2,2, TF Transformation,"00:06:50,310","00:06:52,900",91,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=410,Including the two extremes,pic_cs-410_2_2_360.jpg
cs-410_2_2_92,cs-410,2,2, TF Transformation,"00:06:52,900","00:06:57,480",92,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=412,"That is, the 0/1 bit transformation and",pic_cs-410_2_2_360.jpg
cs-410_2_2_93,cs-410,2,2, TF Transformation,"00:06:57,480","00:07:01,890",93,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=417,"So for example, if we set k to 0,",pic_cs-410_2_2_360.jpg
cs-410_2_2_94,cs-410,2,2, TF Transformation,"00:07:03,630","00:07:06,710",94,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=423,the function value will be 1.,pic_cs-410_2_2_420.jpg
cs-410_2_2_95,cs-410,2,2, TF Transformation,"00:07:06,710","00:07:13,250",95,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=426,So we precisely recover,pic_cs-410_2_2_420.jpg
cs-410_2_2_96,cs-410,2,2, TF Transformation,"00:07:15,630","00:07:20,040",96,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=435,If you set k to very large,pic_cs-410_2_2_420.jpg
cs-410_2_2_97,cs-410,2,2, TF Transformation,"00:07:20,040","00:07:22,919",97,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=440,it's going to look more like,pic_cs-410_2_2_420.jpg
cs-410_2_2_98,cs-410,2,2, TF Transformation,"00:07:24,980","00:07:29,400",98,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=444,"So in this sense,",pic_cs-410_2_2_420.jpg
cs-410_2_2_99,cs-410,2,2, TF Transformation,"00:07:29,400","00:07:34,600",99,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=449,It allows us to control,pic_cs-410_2_2_420.jpg
cs-410_2_2_100,cs-410,2,2, TF Transformation,"00:07:34,600","00:07:36,780",100,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=454,It also has a nice property,pic_cs-410_2_2_420.jpg
cs-410_2_2_101,cs-410,2,2, TF Transformation,"00:07:38,020","00:07:42,390",101,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=458,And this upper bound is useful to control,pic_cs-410_2_2_420.jpg
cs-410_2_2_102,cs-410,2,2, TF Transformation,"00:07:43,860","00:07:49,718",102,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=463,And so that we can prevent a spammer,pic_cs-410_2_2_420.jpg
cs-410_2_2_103,cs-410,2,2, TF Transformation,"00:07:49,718","00:07:54,947",103,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=469,of one term to spam all queries,pic_cs-410_2_2_420.jpg
cs-410_2_2_104,cs-410,2,2, TF Transformation,"00:07:57,258","00:08:00,824",104,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=477,"In other words, this upper bound",pic_cs-410_2_2_420.jpg
cs-410_2_2_105,cs-410,2,2, TF Transformation,"00:08:00,824","00:08:05,330",105,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=480,terms would be counted when we aggregate,pic_cs-410_2_2_480.jpg
cs-410_2_2_106,cs-410,2,2, TF Transformation,"00:08:06,680","00:08:10,620",106,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=486,"As I said, this transformation",pic_cs-410_2_2_480.jpg
cs-410_2_2_107,cs-410,2,2, TF Transformation,"00:08:12,300","00:08:16,890",107,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=492,"So to summarize this lecture,",pic_cs-410_2_2_480.jpg
cs-410_2_2_108,cs-410,2,2, TF Transformation,"00:08:16,890","00:08:21,930",108,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=496,"Sublinear TF Transformation,",pic_cs-410_2_2_480.jpg
cs-410_2_2_109,cs-410,2,2, TF Transformation,"00:08:21,930","00:08:25,550",109,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=501,capture the intuition of diminishing,pic_cs-410_2_2_480.jpg
cs-410_2_2_110,cs-410,2,2, TF Transformation,"00:08:26,620","00:08:30,980",110,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=506,It's also to avoid the dominance by,pic_cs-410_2_2_480.jpg
cs-410_2_2_111,cs-410,2,2, TF Transformation,"00:08:30,980","00:08:37,050",111,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=510,This BM25 transformation that we,pic_cs-410_2_2_480.jpg
cs-410_2_2_112,cs-410,2,2, TF Transformation,"00:08:37,050","00:08:43,130",112,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=517,It's so far one of the best-performing,pic_cs-410_2_2_480.jpg
cs-410_2_2_113,cs-410,2,2, TF Transformation,"00:08:43,130","00:08:46,520",113,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=523,"It has upper bound, and so",pic_cs-410_2_2_480.jpg
cs-410_2_2_114,cs-410,2,2, TF Transformation,"00:08:47,830","00:08:54,080",114,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=527,Now if we're plugging this function into,pic_cs-410_2_2_480.jpg
cs-410_2_2_115,cs-410,2,2, TF Transformation,"00:08:54,080","00:08:57,730",115,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=534,Then we'd end up having,pic_cs-410_2_2_480.jpg
cs-410_2_2_116,cs-410,2,2, TF Transformation,"00:08:57,730","00:09:00,720",116,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=537,which has a BM25 TF component.,pic_cs-410_2_2_480.jpg
cs-410_2_2_117,cs-410,2,2, TF Transformation,"00:09:01,870","00:09:06,833",117,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=541,"Now, this is already",pic_cs-410_2_2_540.jpg
cs-410_2_2_118,cs-410,2,2, TF Transformation,"00:09:06,833","00:09:11,537",118,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=546,the odd ranking function called BM25.,pic_cs-410_2_2_540.jpg
cs-410_2_2_119,cs-410,2,2, TF Transformation,"00:09:11,537","00:09:17,890",119,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=551,And we'll discuss how we can further,pic_cs-410_2_2_540.jpg
cs-410_2_2_120,cs-410,2,2, TF Transformation,"00:09:17,890","00:09:27,890",120,https://www.coursera.org/learn/cs-410/lecture/W0NZe?t=557,[MUSIC],pic_cs-410_2_2_540.jpg
cs-410_2_3_1,cs-410,2,3, Doc Length Normalization,"00:00:00,012","00:00:03,576",1,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=0,[SOUND],pic_cs-410_2_3_0.jpg
cs-410_2_3_2,cs-410,2,3, Doc Length Normalization,"00:00:08,498","00:00:10,214",2,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=8,This lecture is about,pic_cs-410_2_3_0.jpg
cs-410_2_3_3,cs-410,2,3, Doc Length Normalization,"00:00:10,214","00:00:14,988",3,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=10,Document Length Normalization,pic_cs-410_2_3_0.jpg
cs-410_2_3_4,cs-410,2,3, Doc Length Normalization,"00:00:14,988","00:00:19,740",4,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=14,"In this lecture, we will continue",pic_cs-410_2_3_0.jpg
cs-410_2_3_5,cs-410,2,3, Doc Length Normalization,"00:00:19,740","00:00:23,990",5,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=19,"In particular, we're going to discuss the",pic_cs-410_2_3_0.jpg
cs-410_2_3_6,cs-410,2,3, Doc Length Normalization,"00:00:25,850","00:00:30,330",6,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=25,So far in the lectures about the vector,pic_cs-410_2_3_0.jpg
cs-410_2_3_7,cs-410,2,3, Doc Length Normalization,"00:00:30,330","00:00:37,480",7,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=30,signals from the document to assess,pic_cs-410_2_3_0.jpg
cs-410_2_3_8,cs-410,2,3, Doc Length Normalization,"00:00:37,480","00:00:40,000",8,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=37,"In particular,",pic_cs-410_2_3_0.jpg
cs-410_2_3_9,cs-410,2,3, Doc Length Normalization,"00:00:40,000","00:00:42,750",9,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=40,The count of a tone in a document.,pic_cs-410_2_3_0.jpg
cs-410_2_3_10,cs-410,2,3, Doc Length Normalization,"00:00:42,750","00:00:48,055",10,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=42,We have also considered it's,pic_cs-410_2_3_0.jpg
cs-410_2_3_11,cs-410,2,3, Doc Length Normalization,"00:00:48,055","00:00:50,795",11,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=48,"IDF, Inverse Document Frequency.",pic_cs-410_2_3_0.jpg
cs-410_2_3_12,cs-410,2,3, Doc Length Normalization,"00:00:50,795","00:00:53,620",12,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=50,But we have not considered,pic_cs-410_2_3_0.jpg
cs-410_2_3_13,cs-410,2,3, Doc Length Normalization,"00:00:54,855","00:01:00,899",13,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=54,"So here I show two example documents,",pic_cs-410_2_3_0.jpg
cs-410_2_3_14,cs-410,2,3, Doc Length Normalization,"00:01:01,910","00:01:05,098",14,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=61,"D6 on the other hand, has a 5000 words.",pic_cs-410_2_3_60.jpg
cs-410_2_3_15,cs-410,2,3, Doc Length Normalization,"00:01:05,098","00:01:08,882",15,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=65,If you look at the matching,pic_cs-410_2_3_60.jpg
cs-410_2_3_16,cs-410,2,3, Doc Length Normalization,"00:01:08,882","00:01:13,878",16,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=68,"we see that in d6, there are more",pic_cs-410_2_3_60.jpg
cs-410_2_3_17,cs-410,2,3, Doc Length Normalization,"00:01:13,878","00:01:18,958",17,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=73,"But one might reason that,",pic_cs-410_2_3_60.jpg
cs-410_2_3_18,cs-410,2,3, Doc Length Normalization,"00:01:18,958","00:01:23,410",18,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=78,these query words in a scattered manner.,pic_cs-410_2_3_60.jpg
cs-410_2_3_19,cs-410,2,3, Doc Length Normalization,"00:01:24,450","00:01:30,060",19,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=84,"So maybe the topic of d6, is not",pic_cs-410_2_3_60.jpg
cs-410_2_3_20,cs-410,2,3, Doc Length Normalization,"00:01:31,350","00:01:34,980",20,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=91,"So, the discussion of the campaign",pic_cs-410_2_3_60.jpg
cs-410_2_3_21,cs-410,2,3, Doc Length Normalization,"00:01:34,980","00:01:38,739",21,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=94,may have nothing to do with the managing,pic_cs-410_2_3_60.jpg
cs-410_2_3_22,cs-410,2,3, Doc Length Normalization,"00:01:40,810","00:01:44,600",22,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=100,"In general,",pic_cs-410_2_3_60.jpg
cs-410_2_3_23,cs-410,2,3, Doc Length Normalization,"00:01:44,600","00:01:47,370",23,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=104,they would have a higher chance for,pic_cs-410_2_3_60.jpg
cs-410_2_3_24,cs-410,2,3, Doc Length Normalization,"00:01:47,370","00:01:54,760",24,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=107,"In fact, if you generate a long document",pic_cs-410_2_3_60.jpg
cs-410_2_3_25,cs-410,2,3, Doc Length Normalization,"00:01:54,760","00:01:59,690",25,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=114,"a distribution of words, then eventually",pic_cs-410_2_3_60.jpg
cs-410_2_3_26,cs-410,2,3, Doc Length Normalization,"00:02:00,760","00:02:05,800",26,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=120,"So in this sense, we should penalize on",pic_cs-410_2_3_120.jpg
cs-410_2_3_27,cs-410,2,3, Doc Length Normalization,"00:02:05,800","00:02:10,400",27,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=125,"better chance matching to any query, and",pic_cs-410_2_3_120.jpg
cs-410_2_3_28,cs-410,2,3, Doc Length Normalization,"00:02:12,300","00:02:18,600",28,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=132,We also need to be careful in avoiding,pic_cs-410_2_3_120.jpg
cs-410_2_3_29,cs-410,2,3, Doc Length Normalization,"00:02:19,770","00:02:22,790",29,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=139,"On the one hand,",pic_cs-410_2_3_120.jpg
cs-410_2_3_30,cs-410,2,3, Doc Length Normalization,"00:02:22,790","00:02:27,202",30,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=142,"But on the other hand,",pic_cs-410_2_3_120.jpg
cs-410_2_3_31,cs-410,2,3, Doc Length Normalization,"00:02:27,202","00:02:30,790",31,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=147,"Now, the reasoning is because",pic_cs-410_2_3_120.jpg
cs-410_2_3_32,cs-410,2,3, Doc Length Normalization,"00:02:30,790","00:02:31,309",32,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=150,different reasons.,pic_cs-410_2_3_120.jpg
cs-410_2_3_33,cs-410,2,3, Doc Length Normalization,"00:02:32,770","00:02:36,950",33,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=152,"In one case, the document may be",pic_cs-410_2_3_120.jpg
cs-410_2_3_34,cs-410,2,3, Doc Length Normalization,"00:02:38,270","00:02:44,460",34,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=158,"So for example, think about the vortex",pic_cs-410_2_3_120.jpg
cs-410_2_3_35,cs-410,2,3, Doc Length Normalization,"00:02:44,460","00:02:47,620",35,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=164,It would use more words than,pic_cs-410_2_3_120.jpg
cs-410_2_3_36,cs-410,2,3, Doc Length Normalization,"00:02:49,560","00:02:53,140",36,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=169,"So, this is a case where we probably",pic_cs-410_2_3_120.jpg
cs-410_2_3_37,cs-410,2,3, Doc Length Normalization,"00:02:54,980","00:02:57,278",37,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=174,long documents such as a full paper.,pic_cs-410_2_3_120.jpg
cs-410_2_3_38,cs-410,2,3, Doc Length Normalization,"00:02:57,278","00:03:02,520",38,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=177,When we compare the matching,pic_cs-410_2_3_120.jpg
cs-410_2_3_39,cs-410,2,3, Doc Length Normalization,"00:03:02,520","00:03:06,410",39,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=182,document with matching of,pic_cs-410_2_3_180.jpg
cs-410_2_3_40,cs-410,2,3, Doc Length Normalization,"00:03:07,830","00:03:10,700",40,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=187,"Then long papers in general,",pic_cs-410_2_3_180.jpg
cs-410_2_3_41,cs-410,2,3, Doc Length Normalization,"00:03:10,700","00:03:15,380",41,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=190,have a higher chance of matching clearer,pic_cs-410_2_3_180.jpg
cs-410_2_3_42,cs-410,2,3, Doc Length Normalization,"00:03:15,380","00:03:18,550",42,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=195,"However, there is another case",pic_cs-410_2_3_180.jpg
cs-410_2_3_43,cs-410,2,3, Doc Length Normalization,"00:03:18,550","00:03:21,750",43,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=198,that is when the document,pic_cs-410_2_3_180.jpg
cs-410_2_3_44,cs-410,2,3, Doc Length Normalization,"00:03:21,750","00:03:24,040",44,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=201,Now consider another,pic_cs-410_2_3_180.jpg
cs-410_2_3_45,cs-410,2,3, Doc Length Normalization,"00:03:24,040","00:03:29,450",45,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=204,where we simply concatenate a lot,pic_cs-410_2_3_180.jpg
cs-410_2_3_46,cs-410,2,3, Doc Length Normalization,"00:03:29,450","00:03:34,190",46,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=209,"In such a case, obviously, we don't want",pic_cs-410_2_3_180.jpg
cs-410_2_3_47,cs-410,2,3, Doc Length Normalization,"00:03:34,190","00:03:38,270",47,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=214,"Indeed, we probably don't want to penalize",pic_cs-410_2_3_180.jpg
cs-410_2_3_48,cs-410,2,3, Doc Length Normalization,"00:03:39,700","00:03:46,490",48,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=219,"So that's why, we need to be careful about",pic_cs-410_2_3_180.jpg
cs-410_2_3_49,cs-410,2,3, Doc Length Normalization,"00:03:48,360","00:03:52,420",49,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=228,"A method of that has been working well,",pic_cs-410_2_3_180.jpg
cs-410_2_3_50,cs-410,2,3, Doc Length Normalization,"00:03:52,420","00:03:54,890",50,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=232,is called a pivoted length normalization.,pic_cs-410_2_3_180.jpg
cs-410_2_3_51,cs-410,2,3, Doc Length Normalization,"00:03:54,890","00:03:55,860",51,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=234,"And in this case,",pic_cs-410_2_3_180.jpg
cs-410_2_3_52,cs-410,2,3, Doc Length Normalization,"00:03:55,860","00:04:01,550",52,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=235,the idea is to use the average document,pic_cs-410_2_3_180.jpg
cs-410_2_3_53,cs-410,2,3, Doc Length Normalization,"00:04:01,550","00:04:05,820",53,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=241,That means we'll assume that for,pic_cs-410_2_3_240.jpg
cs-410_2_3_54,cs-410,2,3, Doc Length Normalization,"00:04:05,820","00:04:10,335",54,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=245,the score is about right so,pic_cs-410_2_3_240.jpg
cs-410_2_3_55,cs-410,2,3, Doc Length Normalization,"00:04:10,335","00:04:13,035",55,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=250,But if the document is longer,pic_cs-410_2_3_240.jpg
cs-410_2_3_56,cs-410,2,3, Doc Length Normalization,"00:04:14,125","00:04:16,275",56,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=254,then there will be some penalization.,pic_cs-410_2_3_240.jpg
cs-410_2_3_57,cs-410,2,3, Doc Length Normalization,"00:04:16,275","00:04:20,785",57,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=256,"Whereas if it's a shorter,",pic_cs-410_2_3_240.jpg
cs-410_2_3_58,cs-410,2,3, Doc Length Normalization,"00:04:20,785","00:04:26,050",58,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=260,So this is illustrated at,pic_cs-410_2_3_240.jpg
cs-410_2_3_59,cs-410,2,3, Doc Length Normalization,"00:04:26,050","00:04:28,578",59,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=266,x-axis you can see the length of document.,pic_cs-410_2_3_240.jpg
cs-410_2_3_60,cs-410,2,3, Doc Length Normalization,"00:04:28,578","00:04:33,390",60,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=268,"On the y-axis, we show the normalizer.",pic_cs-410_2_3_240.jpg
cs-410_2_3_61,cs-410,2,3, Doc Length Normalization,"00:04:33,390","00:04:39,080",61,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=273,"In this case, the Pivoted Length",pic_cs-410_2_3_240.jpg
cs-410_2_3_62,cs-410,2,3, Doc Length Normalization,"00:04:39,080","00:04:45,850",62,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=279,is seeing to be interpolation of 1 and,pic_cs-410_2_3_240.jpg
cs-410_2_3_63,cs-410,2,3, Doc Length Normalization,"00:04:45,850","00:04:50,460",63,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=285,the normalize the document in length,pic_cs-410_2_3_240.jpg
cs-410_2_3_64,cs-410,2,3, Doc Length Normalization,"00:04:53,110","00:04:58,640",64,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=293,"So you can see here,",pic_cs-410_2_3_240.jpg
cs-410_2_3_65,cs-410,2,3, Doc Length Normalization,"00:04:58,640","00:05:03,470",65,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=298,"of the document by the average documents,",pic_cs-410_2_3_240.jpg
cs-410_2_3_66,cs-410,2,3, Doc Length Normalization,"00:05:03,470","00:05:07,890",66,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=303,sense about how this document is,pic_cs-410_2_3_300.jpg
cs-410_2_3_67,cs-410,2,3, Doc Length Normalization,"00:05:07,890","00:05:16,120",67,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=307,also gives us a benefit of not,pic_cs-410_2_3_300.jpg
cs-410_2_3_68,cs-410,2,3, Doc Length Normalization,"00:05:16,120","00:05:18,990",68,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=316,We can measure the length by words or,pic_cs-410_2_3_300.jpg
cs-410_2_3_69,cs-410,2,3, Doc Length Normalization,"00:05:20,760","00:05:24,260",69,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=320,"Anyway, this normalizer",pic_cs-410_2_3_300.jpg
cs-410_2_3_70,cs-410,2,3, Doc Length Normalization,"00:05:24,260","00:05:29,660",70,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=324,"First we see that, if we set the parameter",pic_cs-410_2_3_300.jpg
cs-410_2_3_71,cs-410,2,3, Doc Length Normalization,"00:05:29,660","00:05:33,580",71,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=329,"So, there's no lens normalization at all.",pic_cs-410_2_3_300.jpg
cs-410_2_3_72,cs-410,2,3, Doc Length Normalization,"00:05:33,580","00:05:37,540",72,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=333,"So, b, in this sense,",pic_cs-410_2_3_300.jpg
cs-410_2_3_73,cs-410,2,3, Doc Length Normalization,"00:05:39,450","00:05:44,980",73,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=339,"Whereas, if we set b to a nonzero value,",pic_cs-410_2_3_300.jpg
cs-410_2_3_74,cs-410,2,3, Doc Length Normalization,"00:05:44,980","00:05:49,010",74,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=344,"All right, so",pic_cs-410_2_3_300.jpg
cs-410_2_3_75,cs-410,2,3, Doc Length Normalization,"00:05:49,010","00:05:52,179",75,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=349,documents that are longer than,pic_cs-410_2_3_300.jpg
cs-410_2_3_76,cs-410,2,3, Doc Length Normalization,"00:05:53,860","00:05:56,580",76,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=353,"Whereas, the value of",pic_cs-410_2_3_300.jpg
cs-410_2_3_77,cs-410,2,3, Doc Length Normalization,"00:05:56,580","00:05:59,460",77,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=356,would be smaller for shorter documents.,pic_cs-410_2_3_300.jpg
cs-410_2_3_78,cs-410,2,3, Doc Length Normalization,"00:05:59,460","00:06:02,720",78,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=359,"So in this sense,",pic_cs-410_2_3_300.jpg
cs-410_2_3_79,cs-410,2,3, Doc Length Normalization,"00:06:02,720","00:06:07,230",79,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=362,"long documents, and",pic_cs-410_2_3_360.jpg
cs-410_2_3_80,cs-410,2,3, Doc Length Normalization,"00:06:09,040","00:06:11,500",80,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=369,The degree of penalization,pic_cs-410_2_3_360.jpg
cs-410_2_3_81,cs-410,2,3, Doc Length Normalization,"00:06:11,500","00:06:16,750",81,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=371,"because if we set b to a larger value,",pic_cs-410_2_3_360.jpg
cs-410_2_3_82,cs-410,2,3, Doc Length Normalization,"00:06:16,750","00:06:20,580",82,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=376,There's even more penalization for,pic_cs-410_2_3_360.jpg
cs-410_2_3_83,cs-410,2,3, Doc Length Normalization,"00:06:20,580","00:06:22,380",83,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=380,the short documents.,pic_cs-410_2_3_360.jpg
cs-410_2_3_84,cs-410,2,3, Doc Length Normalization,"00:06:22,380","00:06:25,440",84,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=382,"By adjusting b, which varies from 0 to 1,",pic_cs-410_2_3_360.jpg
cs-410_2_3_85,cs-410,2,3, Doc Length Normalization,"00:06:25,440","00:06:29,450",85,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=385,we can control the degree,pic_cs-410_2_3_360.jpg
cs-410_2_3_86,cs-410,2,3, Doc Length Normalization,"00:06:29,450","00:06:35,050",86,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=389,"So, if we plug in this length",pic_cs-410_2_3_360.jpg
cs-410_2_3_87,cs-410,2,3, Doc Length Normalization,"00:06:35,050","00:06:40,490",87,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=395,"the vector space model, ranking functions",pic_cs-410_2_3_360.jpg
cs-410_2_3_88,cs-410,2,3, Doc Length Normalization,"00:06:41,510","00:06:45,270",88,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=401,Then we will end up having,pic_cs-410_2_3_360.jpg
cs-410_2_3_89,cs-410,2,3, Doc Length Normalization,"00:06:46,370","00:06:51,569",89,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=406,And these are in fact the state of,pic_cs-410_2_3_360.jpg
cs-410_2_3_90,cs-410,2,3, Doc Length Normalization,"00:06:51,569","00:06:55,290",90,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=411,Let's take a look at each of them.,pic_cs-410_2_3_360.jpg
cs-410_2_3_91,cs-410,2,3, Doc Length Normalization,"00:06:55,290","00:07:00,972",91,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=415,The first one is called a pivoted length,pic_cs-410_2_3_360.jpg
cs-410_2_3_92,cs-410,2,3, Doc Length Normalization,"00:07:00,972","00:07:04,980",92,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=420,and a reference in [INAUDIBLE],pic_cs-410_2_3_420.jpg
cs-410_2_3_93,cs-410,2,3, Doc Length Normalization,"00:07:04,980","00:07:11,836",93,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=424,"And here we see that, it's basically",pic_cs-410_2_3_420.jpg
cs-410_2_3_94,cs-410,2,3, Doc Length Normalization,"00:07:11,836","00:07:16,830",94,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=431,the idea of component should,pic_cs-410_2_3_420.jpg
cs-410_2_3_95,cs-410,2,3, Doc Length Normalization,"00:07:18,010","00:07:21,608",95,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=438,There is also a query term,pic_cs-410_2_3_420.jpg
cs-410_2_3_96,cs-410,2,3, Doc Length Normalization,"00:07:24,628","00:07:30,504",96,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=444,"And then, in the middle, there is",pic_cs-410_2_3_420.jpg
cs-410_2_3_97,cs-410,2,3, Doc Length Normalization,"00:07:30,504","00:07:35,486",97,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=450,we see we use the double logarithm,pic_cs-410_2_3_420.jpg
cs-410_2_3_98,cs-410,2,3, Doc Length Normalization,"00:07:35,486","00:07:40,460",98,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=455,this is to achieve,pic_cs-410_2_3_420.jpg
cs-410_2_3_99,cs-410,2,3, Doc Length Normalization,"00:07:40,460","00:07:45,488",99,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=460,But we also put a document,pic_cs-410_2_3_420.jpg
cs-410_2_3_100,cs-410,2,3, Doc Length Normalization,"00:07:45,488","00:07:50,596",100,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=465,"Right, so this would cause",pic_cs-410_2_3_420.jpg
cs-410_2_3_101,cs-410,2,3, Doc Length Normalization,"00:07:50,596","00:07:56,698",101,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=470,"because the larger the denominator is,",pic_cs-410_2_3_420.jpg
cs-410_2_3_102,cs-410,2,3, Doc Length Normalization,"00:07:56,698","00:07:59,660",102,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=476,And this is of course controlled,pic_cs-410_2_3_420.jpg
cs-410_2_3_103,cs-410,2,3, Doc Length Normalization,"00:08:01,420","00:08:06,130",103,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=481,"And you can see again, if b is set to 0",pic_cs-410_2_3_480.jpg
cs-410_2_3_104,cs-410,2,3, Doc Length Normalization,"00:08:08,760","00:08:16,350",104,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=488,"Okay, so this is one of the two most",pic_cs-410_2_3_480.jpg
cs-410_2_3_105,cs-410,2,3, Doc Length Normalization,"00:08:16,350","00:08:20,652",105,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=496,"The next one called a BM25 or Okapi,",pic_cs-410_2_3_480.jpg
cs-410_2_3_106,cs-410,2,3, Doc Length Normalization,"00:08:20,652","00:08:26,971",106,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=500,is also similar in that it,pic_cs-410_2_3_480.jpg
cs-410_2_3_107,cs-410,2,3, Doc Length Normalization,"00:08:26,971","00:08:30,478",107,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=506,and query IDF component here.,pic_cs-410_2_3_480.jpg
cs-410_2_3_108,cs-410,2,3, Doc Length Normalization,"00:08:32,958","00:08:36,150",108,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=512,"But in the middle,",pic_cs-410_2_3_480.jpg
cs-410_2_3_109,cs-410,2,3, Doc Length Normalization,"00:08:36,150","00:08:41,450",109,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=516,"As we explained,",pic_cs-410_2_3_480.jpg
cs-410_2_3_110,cs-410,2,3, Doc Length Normalization,"00:08:41,450","00:08:46,460",110,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=521,and that does sublinear,pic_cs-410_2_3_480.jpg
cs-410_2_3_111,cs-410,2,3, Doc Length Normalization,"00:08:48,340","00:08:53,610",111,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=528,In this case we have put the length,pic_cs-410_2_3_480.jpg
cs-410_2_3_112,cs-410,2,3, Doc Length Normalization,"00:08:53,610","00:08:58,160",112,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=533,We're adjusting k but,pic_cs-410_2_3_480.jpg
cs-410_2_3_113,cs-410,2,3, Doc Length Normalization,"00:08:58,160","00:09:02,610",113,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=538,because we put a normalizer,pic_cs-410_2_3_480.jpg
cs-410_2_3_114,cs-410,2,3, Doc Length Normalization,"00:09:02,610","00:09:08,680",114,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=542,"Therefore, again, if a document is longer",pic_cs-410_2_3_540.jpg
cs-410_2_3_115,cs-410,2,3, Doc Length Normalization,"00:09:10,110","00:09:16,070",115,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=550,So you can see after we have gone through,pic_cs-410_2_3_540.jpg
cs-410_2_3_116,cs-410,2,3, Doc Length Normalization,"00:09:16,070","00:09:24,226",116,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=556,and we have in the end reached,pic_cs-410_2_3_540.jpg
cs-410_2_3_117,cs-410,2,3, Doc Length Normalization,"00:09:24,226","00:09:28,726",117,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=564,"So, So far, we have talked about",pic_cs-410_2_3_540.jpg
cs-410_2_3_118,cs-410,2,3, Doc Length Normalization,"00:09:28,726","00:09:33,530",118,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=568,mainly how to place the document,pic_cs-410_2_3_540.jpg
cs-410_2_3_119,cs-410,2,3, Doc Length Normalization,"00:09:35,010","00:09:39,752",119,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=575,"And, this has played an important role",pic_cs-410_2_3_540.jpg
cs-410_2_3_120,cs-410,2,3, Doc Length Normalization,"00:09:39,752","00:09:41,169",120,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=579,the simple function.,pic_cs-410_2_3_540.jpg
cs-410_2_3_121,cs-410,2,3, Doc Length Normalization,"00:09:41,169","00:09:45,728",121,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=581,"But there are also other dimensions,",pic_cs-410_2_3_540.jpg
cs-410_2_3_122,cs-410,2,3, Doc Length Normalization,"00:09:45,728","00:09:50,343",122,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=585,"For example, can we further",pic_cs-410_2_3_540.jpg
cs-410_2_3_123,cs-410,2,3, Doc Length Normalization,"00:09:50,343","00:09:53,648",123,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=590,the dimension of the Vector Space Model?,pic_cs-410_2_3_540.jpg
cs-410_2_3_124,cs-410,2,3, Doc Length Normalization,"00:09:53,648","00:09:57,424",124,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=593,"Now, we've just assumed that the bag",pic_cs-410_2_3_540.jpg
cs-410_2_3_125,cs-410,2,3, Doc Length Normalization,"00:09:57,424","00:10:01,240",125,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=597,"dimension as a word but obviously,",pic_cs-410_2_3_540.jpg
cs-410_2_3_126,cs-410,2,3, Doc Length Normalization,"00:10:01,240","00:10:07,040",126,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=601,"For example, a stemmed word, those",pic_cs-410_2_3_600.jpg
cs-410_2_3_127,cs-410,2,3, Doc Length Normalization,"00:10:07,040","00:10:11,110",127,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=607,"into the same root form, so",pic_cs-410_2_3_600.jpg
cs-410_2_3_128,cs-410,2,3, Doc Length Normalization,"00:10:11,110","00:10:16,510",128,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=611,that computation and computing were all,pic_cs-410_2_3_600.jpg
cs-410_2_3_129,cs-410,2,3, Doc Length Normalization,"00:10:16,510","00:10:18,740",129,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=616,We get those stop word removal.,pic_cs-410_2_3_600.jpg
cs-410_2_3_130,cs-410,2,3, Doc Length Normalization,"00:10:18,740","00:10:25,270",130,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=618,This is to remove some very common words,pic_cs-410_2_3_600.jpg
cs-410_2_3_131,cs-410,2,3, Doc Length Normalization,"00:10:26,760","00:10:29,750",131,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=626,We get use of phrases,pic_cs-410_2_3_600.jpg
cs-410_2_3_132,cs-410,2,3, Doc Length Normalization,"00:10:29,750","00:10:33,630",132,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=629,We can even use later in,pic_cs-410_2_3_600.jpg
cs-410_2_3_133,cs-410,2,3, Doc Length Normalization,"00:10:33,630","00:10:38,540",133,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=633,some clusters of words that represent the,pic_cs-410_2_3_600.jpg
cs-410_2_3_134,cs-410,2,3, Doc Length Normalization,"00:10:39,700","00:10:44,080",134,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=639,"We can also use smaller unit,",pic_cs-410_2_3_600.jpg
cs-410_2_3_135,cs-410,2,3, Doc Length Normalization,"00:10:44,080","00:10:48,820",135,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=644,are sequences of and,pic_cs-410_2_3_600.jpg
cs-410_2_3_136,cs-410,2,3, Doc Length Normalization,"00:10:50,320","00:10:57,087",136,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=650,"However, in practice, people have found",pic_cs-410_2_3_600.jpg
cs-410_2_3_137,cs-410,2,3, Doc Length Normalization,"00:10:57,087","00:11:02,148",137,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=657,phrases is still the most effective,pic_cs-410_2_3_600.jpg
cs-410_2_3_138,cs-410,2,3, Doc Length Normalization,"00:11:02,148","00:11:08,930",138,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=662,"So, this is still so far the most",pic_cs-410_2_3_660.jpg
cs-410_2_3_139,cs-410,2,3, Doc Length Normalization,"00:11:10,120","00:11:12,560",139,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=670,And it's used in all major search engines.,pic_cs-410_2_3_660.jpg
cs-410_2_3_140,cs-410,2,3, Doc Length Normalization,"00:11:13,960","00:11:18,910",140,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=673,"I should also mention, that sometimes",pic_cs-410_2_3_660.jpg
cs-410_2_3_141,cs-410,2,3, Doc Length Normalization,"00:11:18,910","00:11:21,300",141,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=678,domain specific tokenization.,pic_cs-410_2_3_660.jpg
cs-410_2_3_142,cs-410,2,3, Doc Length Normalization,"00:11:21,300","00:11:27,991",142,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=681,"And this is actually very important, as we",pic_cs-410_2_3_660.jpg
cs-410_2_3_143,cs-410,2,3, Doc Length Normalization,"00:11:27,991","00:11:33,545",143,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=687,prevent us from matching them with each,pic_cs-410_2_3_660.jpg
cs-410_2_3_144,cs-410,2,3, Doc Length Normalization,"00:11:33,545","00:11:39,660",144,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=693,"In some languages like Chinese,",pic_cs-410_2_3_660.jpg
cs-410_2_3_145,cs-410,2,3, Doc Length Normalization,"00:11:40,860","00:11:47,290",145,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=700,text to obtain word band rates because,pic_cs-410_2_3_660.jpg
cs-410_2_3_146,cs-410,2,3, Doc Length Normalization,"00:11:47,290","00:11:51,505",146,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=707,A word might correspond to one,pic_cs-410_2_3_660.jpg
cs-410_2_3_147,cs-410,2,3, Doc Length Normalization,"00:11:51,505","00:11:53,248",147,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=711,even three characters.,pic_cs-410_2_3_660.jpg
cs-410_2_3_148,cs-410,2,3, Doc Length Normalization,"00:11:53,248","00:11:58,164",148,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=713,"So, it's easier in English when we",pic_cs-410_2_3_660.jpg
cs-410_2_3_149,cs-410,2,3, Doc Length Normalization,"00:11:58,164","00:12:02,590",149,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=718,"In some other languages, we may need",pic_cs-410_2_3_660.jpg
cs-410_2_3_150,cs-410,2,3, Doc Length Normalization,"00:12:02,590","00:12:05,098",150,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=722,figure a way out of what,pic_cs-410_2_3_720.jpg
cs-410_2_3_151,cs-410,2,3, Doc Length Normalization,"00:12:05,098","00:12:10,850",151,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=725,There is also the possibility to,pic_cs-410_2_3_720.jpg
cs-410_2_3_152,cs-410,2,3, Doc Length Normalization,"00:12:10,850","00:12:13,510",152,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=730,And so,pic_cs-410_2_3_720.jpg
cs-410_2_3_153,cs-410,2,3, Doc Length Normalization,"00:12:13,510","00:12:16,137",153,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=733,one can imagine there are other measures.,pic_cs-410_2_3_720.jpg
cs-410_2_3_154,cs-410,2,3, Doc Length Normalization,"00:12:16,137","00:12:20,550",154,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=736,"For example, we can measure the cosine",pic_cs-410_2_3_720.jpg
cs-410_2_3_155,cs-410,2,3, Doc Length Normalization,"00:12:20,550","00:12:23,740",155,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=740,Or we can use Euclidean distance measure.,pic_cs-410_2_3_720.jpg
cs-410_2_3_156,cs-410,2,3, Doc Length Normalization,"00:12:24,880","00:12:27,280",156,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=744,"And these are all possible, but",pic_cs-410_2_3_720.jpg
cs-410_2_3_157,cs-410,2,3, Doc Length Normalization,"00:12:27,280","00:12:32,680",157,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=747,dot product seems still the best and,pic_cs-410_2_3_720.jpg
cs-410_2_3_158,cs-410,2,3, Doc Length Normalization,"00:12:33,780","00:12:38,143",158,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=753,"In fact that it's sufficiently general,",pic_cs-410_2_3_720.jpg
cs-410_2_3_159,cs-410,2,3, Doc Length Normalization,"00:12:38,143","00:12:43,280",159,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=758,if you consider the possibilities,pic_cs-410_2_3_720.jpg
cs-410_2_3_160,cs-410,2,3, Doc Length Normalization,"00:12:44,280","00:12:45,390",160,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=764,"So, for example,",pic_cs-410_2_3_720.jpg
cs-410_2_3_161,cs-410,2,3, Doc Length Normalization,"00:12:45,390","00:12:50,440",161,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=765,cosine measure can be thought of as the,pic_cs-410_2_3_720.jpg
cs-410_2_3_162,cs-410,2,3, Doc Length Normalization,"00:12:50,440","00:12:54,720",162,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=770,"That means, we first normalize each factor",pic_cs-410_2_3_720.jpg
cs-410_2_3_163,cs-410,2,3, Doc Length Normalization,"00:12:54,720","00:12:57,720",163,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=774,That would be critical,pic_cs-410_2_3_720.jpg
cs-410_2_3_164,cs-410,2,3, Doc Length Normalization,"00:12:57,720","00:13:03,860",164,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=777,"I just mentioned that the BM25, seems to",pic_cs-410_2_3_720.jpg
cs-410_2_3_165,cs-410,2,3, Doc Length Normalization,"00:13:04,930","00:13:09,420",165,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=784,But there has been also further,pic_cs-410_2_3_780.jpg
cs-410_2_3_166,cs-410,2,3, Doc Length Normalization,"00:13:09,420","00:13:15,478",166,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=789,"Although, none of these words have",pic_cs-410_2_3_780.jpg
cs-410_2_3_167,cs-410,2,3, Doc Length Normalization,"00:13:15,478","00:13:20,090",167,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=795,"So in one line work,",pic_cs-410_2_3_780.jpg
cs-410_2_3_168,cs-410,2,3, Doc Length Normalization,"00:13:20,090","00:13:26,663",168,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=800,"Here, F stands for field, and this is",pic_cs-410_2_3_780.jpg
cs-410_2_3_169,cs-410,2,3, Doc Length Normalization,"00:13:26,663","00:13:30,960",169,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=806,"So for example, you might consider",pic_cs-410_2_3_780.jpg
cs-410_2_3_170,cs-410,2,3, Doc Length Normalization,"00:13:30,960","00:13:33,240",170,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=810,or body of the research article.,pic_cs-410_2_3_780.jpg
cs-410_2_3_171,cs-410,2,3, Doc Length Normalization,"00:13:33,240","00:13:39,800",171,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=813,"Or even anchor text on the web page,",pic_cs-410_2_3_780.jpg
cs-410_2_3_172,cs-410,2,3, Doc Length Normalization,"00:13:39,800","00:13:44,970",172,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=819,links to other pages and,pic_cs-410_2_3_780.jpg
cs-410_2_3_173,cs-410,2,3, Doc Length Normalization,"00:13:44,970","00:13:50,490",173,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=824,a proper way of different fields to help,pic_cs-410_2_3_780.jpg
cs-410_2_3_174,cs-410,2,3, Doc Length Normalization,"00:13:50,490","00:13:55,430",174,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=830,When we use BM25 for such a document and,pic_cs-410_2_3_780.jpg
cs-410_2_3_175,cs-410,2,3, Doc Length Normalization,"00:13:55,430","00:14:00,750",175,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=835,the obvious choice is to apply BM25 for,pic_cs-410_2_3_780.jpg
cs-410_2_3_176,cs-410,2,3, Doc Length Normalization,"00:14:00,750","00:14:06,620",176,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=840,"Basically, the idea of BM25F is",pic_cs-410_2_3_840.jpg
cs-410_2_3_177,cs-410,2,3, Doc Length Normalization,"00:14:06,620","00:14:11,670",177,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=846,"counts of terms in all the fields,",pic_cs-410_2_3_840.jpg
cs-410_2_3_178,cs-410,2,3, Doc Length Normalization,"00:14:11,670","00:14:19,430",178,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=851,"Now, this has advantage of avoiding over",pic_cs-410_2_3_840.jpg
cs-410_2_3_179,cs-410,2,3, Doc Length Normalization,"00:14:19,430","00:14:22,000",179,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=859,Remember in the sublinear,pic_cs-410_2_3_840.jpg
cs-410_2_3_180,cs-410,2,3, Doc Length Normalization,"00:14:22,000","00:14:27,800",180,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=862,the first occurrence is very important and,pic_cs-410_2_3_840.jpg
cs-410_2_3_181,cs-410,2,3, Doc Length Normalization,"00:14:27,800","00:14:29,660",181,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=867,"And if we do that for all the fields,",pic_cs-410_2_3_840.jpg
cs-410_2_3_182,cs-410,2,3, Doc Length Normalization,"00:14:29,660","00:14:35,110",182,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=869,then the same term might have gained,pic_cs-410_2_3_840.jpg
cs-410_2_3_183,cs-410,2,3, Doc Length Normalization,"00:14:35,110","00:14:38,820",183,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=875,But when we combine these,pic_cs-410_2_3_840.jpg
cs-410_2_3_184,cs-410,2,3, Doc Length Normalization,"00:14:38,820","00:14:42,110",184,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=878,we just do the transformation one time.,pic_cs-410_2_3_840.jpg
cs-410_2_3_185,cs-410,2,3, Doc Length Normalization,"00:14:42,110","00:14:42,870",185,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=882,"At that time,",pic_cs-410_2_3_840.jpg
cs-410_2_3_186,cs-410,2,3, Doc Length Normalization,"00:14:42,870","00:14:47,170",186,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=882,then the extra occurrences will not be,pic_cs-410_2_3_840.jpg
cs-410_2_3_187,cs-410,2,3, Doc Length Normalization,"00:14:48,790","00:14:54,039",187,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=888,And this method has been working very well,pic_cs-410_2_3_840.jpg
cs-410_2_3_188,cs-410,2,3, Doc Length Normalization,"00:14:55,810","00:14:59,283",188,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=895,The other line of extension,pic_cs-410_2_3_840.jpg
cs-410_2_3_189,cs-410,2,3, Doc Length Normalization,"00:14:59,283","00:15:03,810",189,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=899,"In this line,",pic_cs-410_2_3_840.jpg
cs-410_2_3_190,cs-410,2,3, Doc Length Normalization,"00:15:03,810","00:15:05,980",190,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=903,over penalization of,pic_cs-410_2_3_900.jpg
cs-410_2_3_191,cs-410,2,3, Doc Length Normalization,"00:15:08,880","00:15:13,990",191,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=908,"So to address this problem,",pic_cs-410_2_3_900.jpg
cs-410_2_3_192,cs-410,2,3, Doc Length Normalization,"00:15:13,990","00:15:18,180",192,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=913,We can simply add a small constant,pic_cs-410_2_3_900.jpg
cs-410_2_3_193,cs-410,2,3, Doc Length Normalization,"00:15:18,180","00:15:23,400",193,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=918,"But what's interesting is that,",pic_cs-410_2_3_900.jpg
cs-410_2_3_194,cs-410,2,3, Doc Length Normalization,"00:15:23,400","00:15:28,340",194,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=923,"doing such a small modification,",pic_cs-410_2_3_900.jpg
cs-410_2_3_195,cs-410,2,3, Doc Length Normalization,"00:15:28,340","00:15:33,570",195,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=928,the problem of over penalization of,pic_cs-410_2_3_900.jpg
cs-410_2_3_196,cs-410,2,3, Doc Length Normalization,"00:15:33,570","00:15:36,380",196,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=933,"So the new formula called BM25+,",pic_cs-410_2_3_900.jpg
cs-410_2_3_197,cs-410,2,3, Doc Length Normalization,"00:15:36,380","00:15:40,990",197,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=936,is empirically and,pic_cs-410_2_3_900.jpg
cs-410_2_3_198,cs-410,2,3, Doc Length Normalization,"00:15:42,590","00:15:48,432",198,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=942,So to summarize all what we have,pic_cs-410_2_3_900.jpg
cs-410_2_3_199,cs-410,2,3, Doc Length Normalization,"00:15:48,432","00:15:52,100",199,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=948,here are the major take away points.,pic_cs-410_2_3_900.jpg
cs-410_2_3_200,cs-410,2,3, Doc Length Normalization,"00:15:52,100","00:15:57,590",200,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=952,"First, in such a model,",pic_cs-410_2_3_900.jpg
cs-410_2_3_201,cs-410,2,3, Doc Length Normalization,"00:15:57,590","00:16:01,780",201,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=957,Assuming that relevance of a document,pic_cs-410_2_3_900.jpg
cs-410_2_3_202,cs-410,2,3, Doc Length Normalization,"00:16:02,820","00:16:08,030",202,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=962,basically proportional to the similarity,pic_cs-410_2_3_960.jpg
cs-410_2_3_203,cs-410,2,3, Doc Length Normalization,"00:16:08,030","00:16:10,640",203,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=968,"So naturally,",pic_cs-410_2_3_960.jpg
cs-410_2_3_204,cs-410,2,3, Doc Length Normalization,"00:16:10,640","00:16:13,830",204,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=970,document must have been,pic_cs-410_2_3_960.jpg
cs-410_2_3_205,cs-410,2,3, Doc Length Normalization,"00:16:13,830","00:16:19,050",205,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=973,"And in this case, we will present them as",pic_cs-410_2_3_960.jpg
cs-410_2_3_206,cs-410,2,3, Doc Length Normalization,"00:16:19,050","00:16:24,170",206,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=979,"Where the dimensions are defined by words,",pic_cs-410_2_3_960.jpg
cs-410_2_3_207,cs-410,2,3, Doc Length Normalization,"00:16:25,470","00:16:29,850",207,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=985,"And we generally, need to use a lot of",pic_cs-410_2_3_960.jpg
cs-410_2_3_208,cs-410,2,3, Doc Length Normalization,"00:16:29,850","00:16:34,560",208,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=989,"We use some examples, which show",pic_cs-410_2_3_960.jpg
cs-410_2_3_209,cs-410,2,3, Doc Length Normalization,"00:16:34,560","00:16:37,200",209,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=994,including Tf weighting and transformation.,pic_cs-410_2_3_960.jpg
cs-410_2_3_210,cs-410,2,3, Doc Length Normalization,"00:16:38,740","00:16:41,950",210,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=998,"And IDF weighting, and",pic_cs-410_2_3_960.jpg
cs-410_2_3_211,cs-410,2,3, Doc Length Normalization,"00:16:41,950","00:16:45,890",211,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1001,These major heuristics are the most,pic_cs-410_2_3_960.jpg
cs-410_2_3_212,cs-410,2,3, Doc Length Normalization,"00:16:45,890","00:16:51,544",212,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1005,to ensure such a general ranking function,pic_cs-410_2_3_960.jpg
cs-410_2_3_213,cs-410,2,3, Doc Length Normalization,"00:16:51,544","00:16:55,640",213,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1011,"And finally, BM25 and",pic_cs-410_2_3_960.jpg
cs-410_2_3_214,cs-410,2,3, Doc Length Normalization,"00:16:55,640","00:16:59,890",214,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1015,to be the most effective formulas,pic_cs-410_2_3_960.jpg
cs-410_2_3_215,cs-410,2,3, Doc Length Normalization,"00:16:59,890","00:17:05,100",215,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1019,"Now I have to say that, I put BM25 in",pic_cs-410_2_3_960.jpg
cs-410_2_3_216,cs-410,2,3, Doc Length Normalization,"00:17:05,100","00:17:09,759",216,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1025,"in fact, the BM25 has been derived",pic_cs-410_2_3_1020.jpg
cs-410_2_3_217,cs-410,2,3, Doc Length Normalization,"00:17:11,970","00:17:17,470",217,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1031,So the reason why I've put it in,pic_cs-410_2_3_1020.jpg
cs-410_2_3_218,cs-410,2,3, Doc Length Normalization,"00:17:17,470","00:17:22,540",218,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1037,the ranking function actually has a nice,pic_cs-410_2_3_1020.jpg
cs-410_2_3_219,cs-410,2,3, Doc Length Normalization,"00:17:22,540","00:17:23,450",219,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1042,"We can easily see,",pic_cs-410_2_3_1020.jpg
cs-410_2_3_220,cs-410,2,3, Doc Length Normalization,"00:17:23,450","00:17:27,390",220,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1043,it looks very much like a vector space,pic_cs-410_2_3_1020.jpg
cs-410_2_3_221,cs-410,2,3, Doc Length Normalization,"00:17:28,890","00:17:34,640",221,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1048,The second reason is because the original,pic_cs-410_2_3_1020.jpg
cs-410_2_3_222,cs-410,2,3, Doc Length Normalization,"00:17:36,070","00:17:39,420",222,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1056,And that form of IDF after,pic_cs-410_2_3_1020.jpg
cs-410_2_3_223,cs-410,2,3, Doc Length Normalization,"00:17:39,420","00:17:44,630",223,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1059,well as the standard IDF,pic_cs-410_2_3_1020.jpg
cs-410_2_3_224,cs-410,2,3, Doc Length Normalization,"00:17:44,630","00:17:47,910",224,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1064,"So as effective retrieval function,",pic_cs-410_2_3_1020.jpg
cs-410_2_3_225,cs-410,2,3, Doc Length Normalization,"00:17:47,910","00:17:53,360",225,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1067,BM25 should probably use a heuristic,pic_cs-410_2_3_1020.jpg
cs-410_2_3_226,cs-410,2,3, Doc Length Normalization,"00:17:53,360","00:17:55,628",226,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1073,To make them even more look,pic_cs-410_2_3_1020.jpg
cs-410_2_3_227,cs-410,2,3, Doc Length Normalization,"00:17:59,218","00:18:01,460",227,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1079,There are some additional readings.,pic_cs-410_2_3_1020.jpg
cs-410_2_3_228,cs-410,2,3, Doc Length Normalization,"00:18:01,460","00:18:05,330",228,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1081,"The first is, a paper about",pic_cs-410_2_3_1080.jpg
cs-410_2_3_229,cs-410,2,3, Doc Length Normalization,"00:18:05,330","00:18:09,224",229,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1085,It's an excellent example,pic_cs-410_2_3_1080.jpg
cs-410_2_3_230,cs-410,2,3, Doc Length Normalization,"00:18:09,224","00:18:13,650",230,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1089,analysis to suggest the need for,pic_cs-410_2_3_1080.jpg
cs-410_2_3_231,cs-410,2,3, Doc Length Normalization,"00:18:13,650","00:18:17,590",231,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1093,then further derive the length,pic_cs-410_2_3_1080.jpg
cs-410_2_3_232,cs-410,2,3, Doc Length Normalization,"00:18:17,590","00:18:22,830",232,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1097,"The second, is the original paper",pic_cs-410_2_3_1080.jpg
cs-410_2_3_233,cs-410,2,3, Doc Length Normalization,"00:18:24,180","00:18:28,452",233,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1104,"The third paper,",pic_cs-410_2_3_1080.jpg
cs-410_2_3_234,cs-410,2,3, Doc Length Normalization,"00:18:28,452","00:18:31,660",234,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1108,"its extensions, particularly BM25 F.",pic_cs-410_2_3_1080.jpg
cs-410_2_3_235,cs-410,2,3, Doc Length Normalization,"00:18:32,860","00:18:38,305",235,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1112,"And finally, in the last paper",pic_cs-410_2_3_1080.jpg
cs-410_2_3_236,cs-410,2,3, Doc Length Normalization,"00:18:38,305","00:18:43,768",236,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1118,BM25 to correct the over,pic_cs-410_2_3_1080.jpg
cs-410_2_3_237,cs-410,2,3, Doc Length Normalization,"00:18:43,768","00:18:53,768",237,https://www.coursera.org/learn/cs-410/lecture/RnXhr?t=1123,[MUSIC],pic_cs-410_2_3_1080.jpg
cs-410_2_4_1,cs-410,2,4, Implementation of TR Systems,"00:00:00,248","00:00:06,368",1,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=0,[MUSIC],pic_cs-410_2_4_0.jpg
cs-410_2_4_2,cs-410,2,4, Implementation of TR Systems,"00:00:06,368","00:00:10,308",2,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=6,This lecture is about the implementation,pic_cs-410_2_4_0.jpg
cs-410_2_4_3,cs-410,2,4, Implementation of TR Systems,"00:00:12,878","00:00:17,327",3,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=12,In this lecture we will discuss,pic_cs-410_2_4_0.jpg
cs-410_2_4_4,cs-410,2,4, Implementation of TR Systems,"00:00:17,327","00:00:20,768",4,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=17,retrieval method to build a search engine.,pic_cs-410_2_4_0.jpg
cs-410_2_4_5,cs-410,2,4, Implementation of TR Systems,"00:00:20,768","00:00:24,753",5,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=20,The main challenge is to,pic_cs-410_2_4_0.jpg
cs-410_2_4_6,cs-410,2,4, Implementation of TR Systems,"00:00:24,753","00:00:30,698",6,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=24,to enable a query to be answered very,pic_cs-410_2_4_0.jpg
cs-410_2_4_7,cs-410,2,4, Implementation of TR Systems,"00:00:30,698","00:00:34,858",7,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=30,This is a typical text,pic_cs-410_2_4_0.jpg
cs-410_2_4_8,cs-410,2,4, Implementation of TR Systems,"00:00:34,858","00:00:39,805",8,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=34,We can see the documents are first,pic_cs-410_2_4_0.jpg
cs-410_2_4_9,cs-410,2,4, Implementation of TR Systems,"00:00:39,805","00:00:43,498",9,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=39,"get tokenized units, for example, words.",pic_cs-410_2_4_0.jpg
cs-410_2_4_10,cs-410,2,4, Implementation of TR Systems,"00:00:43,498","00:00:48,058",10,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=43,"And then, these words, or",pic_cs-410_2_4_0.jpg
cs-410_2_4_11,cs-410,2,4, Implementation of TR Systems,"00:00:48,058","00:00:53,188",11,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=48,"a indexer that will create a index,",pic_cs-410_2_4_0.jpg
cs-410_2_4_12,cs-410,2,4, Implementation of TR Systems,"00:00:53,188","00:00:57,280",12,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=53,the search engine to use,pic_cs-410_2_4_0.jpg
cs-410_2_4_13,cs-410,2,4, Implementation of TR Systems,"00:00:57,280","00:01:01,830",13,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=57,And the query would be going,pic_cs-410_2_4_0.jpg
cs-410_2_4_14,cs-410,2,4, Implementation of TR Systems,"00:01:01,830","00:01:05,761",14,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=61,So the Tokenizer would be,pic_cs-410_2_4_60.jpg
cs-410_2_4_15,cs-410,2,4, Implementation of TR Systems,"00:01:05,761","00:01:09,200",15,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=65,so that the text can be,pic_cs-410_2_4_60.jpg
cs-410_2_4_16,cs-410,2,4, Implementation of TR Systems,"00:01:09,200","00:01:12,960",16,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=69,The same units would be,pic_cs-410_2_4_60.jpg
cs-410_2_4_17,cs-410,2,4, Implementation of TR Systems,"00:01:12,960","00:01:17,604",17,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=72,The query's representation would,pic_cs-410_2_4_60.jpg
cs-410_2_4_18,cs-410,2,4, Implementation of TR Systems,"00:01:17,604","00:01:22,506",18,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=77,which would use the index to quickly,pic_cs-410_2_4_60.jpg
cs-410_2_4_19,cs-410,2,4, Implementation of TR Systems,"00:01:22,506","00:01:25,268",19,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=82,the documents and then ranking them.,pic_cs-410_2_4_60.jpg
cs-410_2_4_20,cs-410,2,4, Implementation of TR Systems,"00:01:25,268","00:01:27,628",20,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=85,The results will be given to the user.,pic_cs-410_2_4_60.jpg
cs-410_2_4_21,cs-410,2,4, Implementation of TR Systems,"00:01:27,628","00:01:32,033",21,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=87,And then the user can look at the results,pic_cs-410_2_4_60.jpg
cs-410_2_4_22,cs-410,2,4, Implementation of TR Systems,"00:01:32,033","00:01:35,126",22,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=92,explicit judgements of both,pic_cs-410_2_4_60.jpg
cs-410_2_4_23,cs-410,2,4, Implementation of TR Systems,"00:01:35,126","00:01:36,603",23,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=95,which documents are bad.,pic_cs-410_2_4_60.jpg
cs-410_2_4_24,cs-410,2,4, Implementation of TR Systems,"00:01:36,603","00:01:43,353",24,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=96,Or implicit feedback such as so that,pic_cs-410_2_4_60.jpg
cs-410_2_4_25,cs-410,2,4, Implementation of TR Systems,"00:01:43,353","00:01:46,187",25,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=103,"End user will just look at the results,",pic_cs-410_2_4_60.jpg
cs-410_2_4_26,cs-410,2,4, Implementation of TR Systems,"00:01:46,187","00:01:49,193",26,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=106,"skip some, and",pic_cs-410_2_4_60.jpg
cs-410_2_4_27,cs-410,2,4, Implementation of TR Systems,"00:01:49,193","00:01:55,353",27,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=109,So these interacting signals can be used,pic_cs-410_2_4_60.jpg
cs-410_2_4_28,cs-410,2,4, Implementation of TR Systems,"00:01:55,353","00:02:01,718",28,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=115,accuracy by assuming that viewed documents,pic_cs-410_2_4_60.jpg
cs-410_2_4_29,cs-410,2,4, Implementation of TR Systems,"00:02:01,718","00:02:05,678",29,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=121,So a search engine system then,pic_cs-410_2_4_120.jpg
cs-410_2_4_30,cs-410,2,4, Implementation of TR Systems,"00:02:05,678","00:02:10,738",30,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=125,"The first part is the indexer, and",pic_cs-410_2_4_120.jpg
cs-410_2_4_31,cs-410,2,4, Implementation of TR Systems,"00:02:10,738","00:02:16,458",31,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=130,"responds to the users query, and",pic_cs-410_2_4_120.jpg
cs-410_2_4_32,cs-410,2,4, Implementation of TR Systems,"00:02:16,458","00:02:21,072",32,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=136,"Now typically, the Indexer is",pic_cs-410_2_4_120.jpg
cs-410_2_4_33,cs-410,2,4, Implementation of TR Systems,"00:02:21,072","00:02:24,179",33,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=141,you can pre-process the correct data and,pic_cs-410_2_4_120.jpg
cs-410_2_4_34,cs-410,2,4, Implementation of TR Systems,"00:02:24,179","00:02:29,168",34,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=144,"to build the inventory index,",pic_cs-410_2_4_120.jpg
cs-410_2_4_35,cs-410,2,4, Implementation of TR Systems,"00:02:29,168","00:02:34,819",35,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=149,And this data structure can then be used,pic_cs-410_2_4_120.jpg
cs-410_2_4_36,cs-410,2,4, Implementation of TR Systems,"00:02:34,819","00:02:40,668",36,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=154,to process a user's query dynamically and,pic_cs-410_2_4_120.jpg
cs-410_2_4_37,cs-410,2,4, Implementation of TR Systems,"00:02:40,668","00:02:45,368",37,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=160,The feedback mechanism can be done online,pic_cs-410_2_4_120.jpg
cs-410_2_4_38,cs-410,2,4, Implementation of TR Systems,"00:02:45,368","00:02:50,367",38,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=165,The implementation of the indexer and,pic_cs-410_2_4_120.jpg
cs-410_2_4_39,cs-410,2,4, Implementation of TR Systems,"00:02:50,367","00:02:55,378",39,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=170,and this is the main topic of this,pic_cs-410_2_4_120.jpg
cs-410_2_4_40,cs-410,2,4, Implementation of TR Systems,"00:02:55,378","00:02:59,843",40,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=175,"The feedback mechanism,",pic_cs-410_2_4_120.jpg
cs-410_2_4_41,cs-410,2,4, Implementation of TR Systems,"00:02:59,843","00:03:02,378",41,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=179,it depends on which method is used.,pic_cs-410_2_4_120.jpg
cs-410_2_4_42,cs-410,2,4, Implementation of TR Systems,"00:03:02,378","00:03:08,818",42,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=182,So that is usually done in,pic_cs-410_2_4_180.jpg
cs-410_2_4_43,cs-410,2,4, Implementation of TR Systems,"00:03:08,818","00:03:11,538",43,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=188,Let's first talk about the tokenizer.,pic_cs-410_2_4_180.jpg
cs-410_2_4_44,cs-410,2,4, Implementation of TR Systems,"00:03:11,538","00:03:16,578",44,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=191,Tokernization is a normalized lexical,pic_cs-410_2_4_180.jpg
cs-410_2_4_45,cs-410,2,4, Implementation of TR Systems,"00:03:16,578","00:03:21,368",45,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=196,so that semantically similar words,pic_cs-410_2_4_180.jpg
cs-410_2_4_46,cs-410,2,4, Implementation of TR Systems,"00:03:21,368","00:03:25,133",46,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=201,"Now, in the language like English,",pic_cs-410_2_4_180.jpg
cs-410_2_4_47,cs-410,2,4, Implementation of TR Systems,"00:03:25,133","00:03:29,548",47,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=205,this will map all the inflectional,pic_cs-410_2_4_180.jpg
cs-410_2_4_48,cs-410,2,4, Implementation of TR Systems,"00:03:29,548","00:03:33,047",48,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=209,"So for example, computer, computation, and",pic_cs-410_2_4_180.jpg
cs-410_2_4_49,cs-410,2,4, Implementation of TR Systems,"00:03:33,047","00:03:37,078",49,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=213,computing can all be matched,pic_cs-410_2_4_180.jpg
cs-410_2_4_50,cs-410,2,4, Implementation of TR Systems,"00:03:37,078","00:03:43,628",50,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=217,This way all these different forms of,pic_cs-410_2_4_180.jpg
cs-410_2_4_51,cs-410,2,4, Implementation of TR Systems,"00:03:43,628","00:03:46,433",51,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=223,"Now normally, this is a good idea,",pic_cs-410_2_4_180.jpg
cs-410_2_4_52,cs-410,2,4, Implementation of TR Systems,"00:03:46,433","00:03:52,337",52,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=226,to increase the coverage of documents,pic_cs-410_2_4_180.jpg
cs-410_2_4_53,cs-410,2,4, Implementation of TR Systems,"00:03:52,337","00:03:55,553",53,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=232,"But it's also not always beneficial,",pic_cs-410_2_4_180.jpg
cs-410_2_4_54,cs-410,2,4, Implementation of TR Systems,"00:03:55,553","00:04:00,914",54,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=235,because sometimes the subtlest,pic_cs-410_2_4_180.jpg
cs-410_2_4_55,cs-410,2,4, Implementation of TR Systems,"00:04:00,914","00:04:07,558",55,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=240,computation might still suggest the,pic_cs-410_2_4_240.jpg
cs-410_2_4_56,cs-410,2,4, Implementation of TR Systems,"00:04:07,558","00:04:13,398",56,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=247,"But in most cases,",pic_cs-410_2_4_240.jpg
cs-410_2_4_57,cs-410,2,4, Implementation of TR Systems,"00:04:13,398","00:04:19,363",57,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=253,When we tokenize the text in some other,pic_cs-410_2_4_240.jpg
cs-410_2_4_58,cs-410,2,4, Implementation of TR Systems,"00:04:19,363","00:04:25,338",58,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=259,face some special challenges in segmenting,pic_cs-410_2_4_240.jpg
cs-410_2_4_59,cs-410,2,4, Implementation of TR Systems,"00:04:25,338","00:04:29,697",59,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=265,Because it's not obvious,pic_cs-410_2_4_240.jpg
cs-410_2_4_60,cs-410,2,4, Implementation of TR Systems,"00:04:29,697","00:04:32,928",60,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=269,there's no space to separate them.,pic_cs-410_2_4_240.jpg
cs-410_2_4_61,cs-410,2,4, Implementation of TR Systems,"00:04:32,928","00:04:41,638",61,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=272,"So here of course, we have to use some",pic_cs-410_2_4_240.jpg
cs-410_2_4_62,cs-410,2,4, Implementation of TR Systems,"00:04:41,638","00:04:47,144",62,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=281,"Once we do tokenization, then we would",pic_cs-410_2_4_240.jpg
cs-410_2_4_63,cs-410,2,4, Implementation of TR Systems,"00:04:47,144","00:04:52,748",63,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=287,convert the documents and do some data,pic_cs-410_2_4_240.jpg
cs-410_2_4_64,cs-410,2,4, Implementation of TR Systems,"00:04:52,748","00:04:58,298",64,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=292,The basic idea is to precompute,pic_cs-410_2_4_240.jpg
cs-410_2_4_65,cs-410,2,4, Implementation of TR Systems,"00:04:58,298","00:05:02,848",65,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=298,So the most commonly used index,pic_cs-410_2_4_240.jpg
cs-410_2_4_66,cs-410,2,4, Implementation of TR Systems,"00:05:02,848","00:05:06,555",66,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=302,And this has been used,pic_cs-410_2_4_300.jpg
cs-410_2_4_67,cs-410,2,4, Implementation of TR Systems,"00:05:06,555","00:05:09,768",67,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=306,to support basic search algorithms.,pic_cs-410_2_4_300.jpg
cs-410_2_4_68,cs-410,2,4, Implementation of TR Systems,"00:05:09,768","00:05:13,426",68,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=309,"Sometimes the other indices, for example,",pic_cs-410_2_4_300.jpg
cs-410_2_4_69,cs-410,2,4, Implementation of TR Systems,"00:05:13,426","00:05:19,498",69,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=313,document index might be needed in order,pic_cs-410_2_4_300.jpg
cs-410_2_4_70,cs-410,2,4, Implementation of TR Systems,"00:05:19,498","00:05:24,106",70,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=319,And these kind of techniques,pic_cs-410_2_4_300.jpg
cs-410_2_4_71,cs-410,2,4, Implementation of TR Systems,"00:05:24,106","00:05:28,828",71,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=324,that they vary a lot according,pic_cs-410_2_4_300.jpg
cs-410_2_4_72,cs-410,2,4, Implementation of TR Systems,"00:05:28,828","00:05:34,549",72,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=328,To understand why we want to use,pic_cs-410_2_4_300.jpg
cs-410_2_4_73,cs-410,2,4, Implementation of TR Systems,"00:05:34,549","00:05:40,698",73,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=334,you to think about how you would,pic_cs-410_2_4_300.jpg
cs-410_2_4_74,cs-410,2,4, Implementation of TR Systems,"00:05:40,698","00:05:44,938",74,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=340,So if you want to use more time to,pic_cs-410_2_4_300.jpg
cs-410_2_4_75,cs-410,2,4, Implementation of TR Systems,"00:05:44,938","00:05:49,584",75,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=344,So think about how you can,pic_cs-410_2_4_300.jpg
cs-410_2_4_76,cs-410,2,4, Implementation of TR Systems,"00:05:49,584","00:05:54,768",76,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=349,that you can quickly respond,pic_cs-410_2_4_300.jpg
cs-410_2_4_77,cs-410,2,4, Implementation of TR Systems,"00:05:54,768","00:05:58,466",77,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=354,Where if you have thought,pic_cs-410_2_4_300.jpg
cs-410_2_4_78,cs-410,2,4, Implementation of TR Systems,"00:05:58,466","00:06:02,811",78,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=358,you might realize that where,pic_cs-410_2_4_300.jpg
cs-410_2_4_79,cs-410,2,4, Implementation of TR Systems,"00:06:02,811","00:06:07,718",79,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=362,the list of documents that match,pic_cs-410_2_4_360.jpg
cs-410_2_4_80,cs-410,2,4, Implementation of TR Systems,"00:06:07,718","00:06:11,788",80,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=367,"In this way, you can basically",pic_cs-410_2_4_360.jpg
cs-410_2_4_81,cs-410,2,4, Implementation of TR Systems,"00:06:11,788","00:06:17,503",81,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=371,So when you see a term you can simply just,pic_cs-410_2_4_360.jpg
cs-410_2_4_82,cs-410,2,4, Implementation of TR Systems,"00:06:17,503","00:06:20,508",82,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=377,that term and return the list to the user.,pic_cs-410_2_4_360.jpg
cs-410_2_4_83,cs-410,2,4, Implementation of TR Systems,"00:06:20,508","00:06:24,928",83,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=380,So that's the fastest way to,pic_cs-410_2_4_360.jpg
cs-410_2_4_84,cs-410,2,4, Implementation of TR Systems,"00:06:24,928","00:06:30,468",84,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=384,Now the idea of the invert index,pic_cs-410_2_4_360.jpg
cs-410_2_4_85,cs-410,2,4, Implementation of TR Systems,"00:06:30,468","00:06:36,017",85,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=390,We're going to do pre-constructed,pic_cs-410_2_4_360.jpg
cs-410_2_4_86,cs-410,2,4, Implementation of TR Systems,"00:06:36,017","00:06:41,388",86,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=396,us to quickly find all the documents,pic_cs-410_2_4_360.jpg
cs-410_2_4_87,cs-410,2,4, Implementation of TR Systems,"00:06:41,388","00:06:43,878",87,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=401,So let's take a look at this example.,pic_cs-410_2_4_360.jpg
cs-410_2_4_88,cs-410,2,4, Implementation of TR Systems,"00:06:43,878","00:06:45,439",88,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=403,"We have three documents here,",pic_cs-410_2_4_360.jpg
cs-410_2_4_89,cs-410,2,4, Implementation of TR Systems,"00:06:45,439","00:06:49,168",89,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=405,and these are the documents that you,pic_cs-410_2_4_360.jpg
cs-410_2_4_90,cs-410,2,4, Implementation of TR Systems,"00:06:49,168","00:06:52,916",90,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=409,Suppose that we want to create,pic_cs-410_2_4_360.jpg
cs-410_2_4_91,cs-410,2,4, Implementation of TR Systems,"00:06:52,916","00:06:57,502",91,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=412,"Then we want to maintain a dictionary, in",pic_cs-410_2_4_360.jpg
cs-410_2_4_92,cs-410,2,4, Implementation of TR Systems,"00:06:57,502","00:07:01,628",92,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=417,each term and we're going to store,pic_cs-410_2_4_360.jpg
cs-410_2_4_93,cs-410,2,4, Implementation of TR Systems,"00:07:01,628","00:07:05,960",93,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=421,"For example, the number of",pic_cs-410_2_4_420.jpg
cs-410_2_4_94,cs-410,2,4, Implementation of TR Systems,"00:07:05,960","00:07:09,458",94,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=425,the total number of code or,pic_cs-410_2_4_420.jpg
cs-410_2_4_95,cs-410,2,4, Implementation of TR Systems,"00:07:09,458","00:07:14,148",95,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=429,which means we would kind of duplicate,pic_cs-410_2_4_420.jpg
cs-410_2_4_96,cs-410,2,4, Implementation of TR Systems,"00:07:14,148","00:07:17,415",96,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=434,"And so, for example, news,",pic_cs-410_2_4_420.jpg
cs-410_2_4_97,cs-410,2,4, Implementation of TR Systems,"00:07:17,415","00:07:22,253",97,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=437,this term occur in all,pic_cs-410_2_4_420.jpg
cs-410_2_4_98,cs-410,2,4, Implementation of TR Systems,"00:07:22,253","00:07:26,198",98,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=442,so the count of documents is three.,pic_cs-410_2_4_420.jpg
cs-410_2_4_99,cs-410,2,4, Implementation of TR Systems,"00:07:26,198","00:07:32,820",99,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=446,And you might also realize we needed this,pic_cs-410_2_4_420.jpg
cs-410_2_4_100,cs-410,2,4, Implementation of TR Systems,"00:07:32,820","00:07:38,002",100,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=452,for computing some statistics to,pic_cs-410_2_4_420.jpg
cs-410_2_4_101,cs-410,2,4, Implementation of TR Systems,"00:07:38,002","00:07:42,422",101,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=458,Can you think of that?,pic_cs-410_2_4_420.jpg
cs-410_2_4_102,cs-410,2,4, Implementation of TR Systems,"00:07:42,422","00:07:49,862",102,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=462,So what weighting heuristic,pic_cs-410_2_4_420.jpg
cs-410_2_4_103,cs-410,2,4, Implementation of TR Systems,"00:07:49,862","00:07:53,622",103,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=469,"Well, that's the idea, right,",pic_cs-410_2_4_420.jpg
cs-410_2_4_104,cs-410,2,4, Implementation of TR Systems,"00:07:53,622","00:07:58,300",104,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=473,"So, IDF is the property of a term,",pic_cs-410_2_4_420.jpg
cs-410_2_4_105,cs-410,2,4, Implementation of TR Systems,"00:07:58,300","00:08:03,291",105,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=478,"So, with the document that count here,",pic_cs-410_2_4_420.jpg
cs-410_2_4_106,cs-410,2,4, Implementation of TR Systems,"00:08:03,291","00:08:06,556",106,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=483,"either at this time, or",pic_cs-410_2_4_480.jpg
cs-410_2_4_107,cs-410,2,4, Implementation of TR Systems,"00:08:06,556","00:08:10,134",107,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=486,At random time when we see a query.,pic_cs-410_2_4_480.jpg
cs-410_2_4_108,cs-410,2,4, Implementation of TR Systems,"00:08:10,134","00:08:13,641",108,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=490,"Now in addition to these basic statistics,",pic_cs-410_2_4_480.jpg
cs-410_2_4_109,cs-410,2,4, Implementation of TR Systems,"00:08:13,641","00:08:18,380",109,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=493,we'll also store all the documents,pic_cs-410_2_4_480.jpg
cs-410_2_4_110,cs-410,2,4, Implementation of TR Systems,"00:08:18,380","00:08:23,049",110,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=498,and these entries are stored,pic_cs-410_2_4_480.jpg
cs-410_2_4_111,cs-410,2,4, Implementation of TR Systems,"00:08:24,150","00:08:27,595",111,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=504,So in this case it matched,pic_cs-410_2_4_480.jpg
cs-410_2_4_112,cs-410,2,4, Implementation of TR Systems,"00:08:27,595","00:08:31,680",112,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=507,we store information about,pic_cs-410_2_4_480.jpg
cs-410_2_4_113,cs-410,2,4, Implementation of TR Systems,"00:08:31,680","00:08:38,160",113,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=511,"This is the document id,",pic_cs-410_2_4_480.jpg
cs-410_2_4_114,cs-410,2,4, Implementation of TR Systems,"00:08:38,160","00:08:45,240",114,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=518,"The tf is one for news, in the second",pic_cs-410_2_4_480.jpg
cs-410_2_4_115,cs-410,2,4, Implementation of TR Systems,"00:08:45,240","00:08:50,864",115,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=525,"So from this list, we can get all",pic_cs-410_2_4_480.jpg
cs-410_2_4_116,cs-410,2,4, Implementation of TR Systems,"00:08:50,864","00:08:55,320",116,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=530,we can also know the frequency,pic_cs-410_2_4_480.jpg
cs-410_2_4_117,cs-410,2,4, Implementation of TR Systems,"00:08:55,320","00:08:58,214",117,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=535,"So, if the query has just one word,",pic_cs-410_2_4_480.jpg
cs-410_2_4_118,cs-410,2,4, Implementation of TR Systems,"00:08:58,214","00:09:01,628",118,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=538,we have easily look up to this,pic_cs-410_2_4_480.jpg
cs-410_2_4_119,cs-410,2,4, Implementation of TR Systems,"00:09:01,628","00:09:06,780",119,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=541,go quicker into the postings to fetch,pic_cs-410_2_4_540.jpg
cs-410_2_4_120,cs-410,2,4, Implementation of TR Systems,"00:09:06,780","00:09:08,180",120,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=546,"So, let's take a look at another term.",pic_cs-410_2_4_540.jpg
cs-410_2_4_121,cs-410,2,4, Implementation of TR Systems,"00:09:09,280","00:09:12,600",121,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=549,"This time, let's take a look",pic_cs-410_2_4_540.jpg
cs-410_2_4_122,cs-410,2,4, Implementation of TR Systems,"00:09:14,130","00:09:17,950",122,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=554,"This would occur in only one document,",pic_cs-410_2_4_540.jpg
cs-410_2_4_123,cs-410,2,4, Implementation of TR Systems,"00:09:17,950","00:09:23,490",123,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=557,So the document frequency is 1 but,pic_cs-410_2_4_540.jpg
cs-410_2_4_124,cs-410,2,4, Implementation of TR Systems,"00:09:23,490","00:09:29,210",124,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=563,"So the frequency count is two, and",pic_cs-410_2_4_540.jpg
cs-410_2_4_125,cs-410,2,4, Implementation of TR Systems,"00:09:29,210","00:09:33,250",125,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=569,some other reachable method where,pic_cs-410_2_4_540.jpg
cs-410_2_4_126,cs-410,2,4, Implementation of TR Systems,"00:09:34,490","00:09:38,770",126,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=574,assess the popularity of,pic_cs-410_2_4_540.jpg
cs-410_2_4_127,cs-410,2,4, Implementation of TR Systems,"00:09:38,770","00:09:42,930",127,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=578,Similarly we'll have a pointer,pic_cs-410_2_4_540.jpg
cs-410_2_4_128,cs-410,2,4, Implementation of TR Systems,"00:09:42,930","00:09:47,490",128,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=582,"and in this case,",pic_cs-410_2_4_540.jpg
cs-410_2_4_129,cs-410,2,4, Implementation of TR Systems,"00:09:48,900","00:09:53,570",129,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=588,the term occurred in just one document and,pic_cs-410_2_4_540.jpg
cs-410_2_4_130,cs-410,2,4, Implementation of TR Systems,"00:09:53,570","00:09:57,320",130,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=593,The document id is 3 and,pic_cs-410_2_4_540.jpg
cs-410_2_4_131,cs-410,2,4, Implementation of TR Systems,"00:09:59,610","00:10:02,550",131,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=599,So this is the basic,pic_cs-410_2_4_540.jpg
cs-410_2_4_132,cs-410,2,4, Implementation of TR Systems,"00:10:02,550","00:10:04,340",132,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=602,"It's actually pretty simple, right?",pic_cs-410_2_4_600.jpg
cs-410_2_4_133,cs-410,2,4, Implementation of TR Systems,"00:10:06,580","00:10:12,370",133,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=606,With this structure we can easily fetch,pic_cs-410_2_4_600.jpg
cs-410_2_4_134,cs-410,2,4, Implementation of TR Systems,"00:10:12,370","00:10:15,760",134,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=612,And this will be the basis for,pic_cs-410_2_4_600.jpg
cs-410_2_4_135,cs-410,2,4, Implementation of TR Systems,"00:10:15,760","00:10:23,770",135,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=615,Now sometimes we also want to store,pic_cs-410_2_4_600.jpg
cs-410_2_4_136,cs-410,2,4, Implementation of TR Systems,"00:10:25,220","00:10:31,960",136,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=625,So in many of these cases the term,pic_cs-410_2_4_600.jpg
cs-410_2_4_137,cs-410,2,4, Implementation of TR Systems,"00:10:31,960","00:10:34,320",137,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=631,So there's only one position for,pic_cs-410_2_4_600.jpg
cs-410_2_4_138,cs-410,2,4, Implementation of TR Systems,"00:10:35,810","00:10:40,990",138,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=635,"But in this case, the term occurred",pic_cs-410_2_4_600.jpg
cs-410_2_4_139,cs-410,2,4, Implementation of TR Systems,"00:10:40,990","00:10:44,690",139,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=640,Now the position information is very,pic_cs-410_2_4_600.jpg
cs-410_2_4_140,cs-410,2,4, Implementation of TR Systems,"00:10:44,690","00:10:48,400",140,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=644,the matching of query terms is,pic_cs-410_2_4_600.jpg
cs-410_2_4_141,cs-410,2,4, Implementation of TR Systems,"00:10:48,400","00:10:51,360",141,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=648,"let's say, five words or ten words.",pic_cs-410_2_4_600.jpg
cs-410_2_4_142,cs-410,2,4, Implementation of TR Systems,"00:10:52,410","00:11:00,700",142,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=652,"Or, whether the matching of the two query",pic_cs-410_2_4_600.jpg
cs-410_2_4_143,cs-410,2,4, Implementation of TR Systems,"00:11:00,700","00:11:04,540",143,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=660,That this can all be checked quickly,pic_cs-410_2_4_660.jpg
cs-410_2_4_144,cs-410,2,4, Implementation of TR Systems,"00:11:05,920","00:11:10,160",144,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=665,"So, why is inverted index good for",pic_cs-410_2_4_660.jpg
cs-410_2_4_145,cs-410,2,4, Implementation of TR Systems,"00:11:10,160","00:11:16,349",145,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=670,"Well, we just talked about the possibility",pic_cs-410_2_4_660.jpg
cs-410_2_4_146,cs-410,2,4, Implementation of TR Systems,"00:11:16,349","00:11:17,990",146,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=676,And that's very easy.,pic_cs-410_2_4_660.jpg
cs-410_2_4_147,cs-410,2,4, Implementation of TR Systems,"00:11:17,990","00:11:19,910",147,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=677,What about the multiple term queries?,pic_cs-410_2_4_660.jpg
cs-410_2_4_148,cs-410,2,4, Implementation of TR Systems,"00:11:19,910","00:11:23,800",148,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=679,Well let's first look at the some,pic_cs-410_2_4_660.jpg
cs-410_2_4_149,cs-410,2,4, Implementation of TR Systems,"00:11:23,800","00:11:27,740",149,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=683,A Boolean query is basically,pic_cs-410_2_4_660.jpg
cs-410_2_4_150,cs-410,2,4, Implementation of TR Systems,"00:11:27,740","00:11:36,290",150,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=687,So I want the value in the document,pic_cs-410_2_4_660.jpg
cs-410_2_4_151,cs-410,2,4, Implementation of TR Systems,"00:11:36,290","00:11:38,770",151,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=696,So that's one conjunctive query.,pic_cs-410_2_4_660.jpg
cs-410_2_4_152,cs-410,2,4, Implementation of TR Systems,"00:11:38,770","00:11:45,440",152,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=698,Or I want the web documents,pic_cs-410_2_4_660.jpg
cs-410_2_4_153,cs-410,2,4, Implementation of TR Systems,"00:11:45,440","00:11:46,540",153,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=705,That's a disjunctive query.,pic_cs-410_2_4_660.jpg
cs-410_2_4_154,cs-410,2,4, Implementation of TR Systems,"00:11:46,540","00:11:51,070",154,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=706,But how can we answer such,pic_cs-410_2_4_660.jpg
cs-410_2_4_155,cs-410,2,4, Implementation of TR Systems,"00:11:52,090","00:11:53,860",155,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=712,"Well if you think a bit about it,",pic_cs-410_2_4_660.jpg
cs-410_2_4_156,cs-410,2,4, Implementation of TR Systems,"00:11:53,860","00:11:58,130",156,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=713,it would be obvious because,pic_cs-410_2_4_660.jpg
cs-410_2_4_157,cs-410,2,4, Implementation of TR Systems,"00:11:58,130","00:12:03,170",157,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=718,the documents that match term A and also,pic_cs-410_2_4_660.jpg
cs-410_2_4_158,cs-410,2,4, Implementation of TR Systems,"00:12:03,170","00:12:08,160",158,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=723,And then just take the intersection,pic_cs-410_2_4_720.jpg
cs-410_2_4_159,cs-410,2,4, Implementation of TR Systems,"00:12:08,160","00:12:13,050",159,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=728,Or to take the union to,pic_cs-410_2_4_720.jpg
cs-410_2_4_160,cs-410,2,4, Implementation of TR Systems,"00:12:13,050","00:12:16,020",160,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=733,So this is all very easy to answer.,pic_cs-410_2_4_720.jpg
cs-410_2_4_161,cs-410,2,4, Implementation of TR Systems,"00:12:16,020","00:12:17,780",161,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=736,It's going to be very quick.,pic_cs-410_2_4_720.jpg
cs-410_2_4_162,cs-410,2,4, Implementation of TR Systems,"00:12:17,780","00:12:20,850",162,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=737,Now what about the multi-term,pic_cs-410_2_4_720.jpg
cs-410_2_4_163,cs-410,2,4, Implementation of TR Systems,"00:12:20,850","00:12:24,390",163,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=740,We talked about the vector space model for,pic_cs-410_2_4_720.jpg
cs-410_2_4_164,cs-410,2,4, Implementation of TR Systems,"00:12:24,390","00:12:28,940",164,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=744,we will do a match such query with,pic_cs-410_2_4_720.jpg
cs-410_2_4_165,cs-410,2,4, Implementation of TR Systems,"00:12:28,940","00:12:32,330",165,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=748,And the score is based on,pic_cs-410_2_4_720.jpg
cs-410_2_4_166,cs-410,2,4, Implementation of TR Systems,"00:12:32,330","00:12:35,670",166,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=752,So in this case it's not,pic_cs-410_2_4_720.jpg
cs-410_2_4_167,cs-410,2,4, Implementation of TR Systems,"00:12:35,670","00:12:38,770",167,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=755,the scoring can be actually,pic_cs-410_2_4_720.jpg
cs-410_2_4_168,cs-410,2,4, Implementation of TR Systems,"00:12:38,770","00:12:42,430",168,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=758,Basically it's similar to,pic_cs-410_2_4_720.jpg
cs-410_2_4_169,cs-410,2,4, Implementation of TR Systems,"00:12:42,430","00:12:45,140",169,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=762,"Basically, it's like A or B.",pic_cs-410_2_4_720.jpg
cs-410_2_4_170,cs-410,2,4, Implementation of TR Systems,"00:12:45,140","00:12:50,680",170,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=765,We take the union of all the documents,pic_cs-410_2_4_720.jpg
cs-410_2_4_171,cs-410,2,4, Implementation of TR Systems,"00:12:50,680","00:12:53,320",171,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=770,then we would aggregate the term weights.,pic_cs-410_2_4_720.jpg
cs-410_2_4_172,cs-410,2,4, Implementation of TR Systems,"00:12:53,320","00:13:01,420",172,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=773,So this is a basic idea of using inverted,pic_cs-410_2_4_720.jpg
cs-410_2_4_173,cs-410,2,4, Implementation of TR Systems,"00:13:01,420","00:13:05,210",173,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=781,And we're going to talk about,pic_cs-410_2_4_780.jpg
cs-410_2_4_174,cs-410,2,4, Implementation of TR Systems,"00:13:05,210","00:13:06,000",174,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=785,"But for now,",pic_cs-410_2_4_780.jpg
cs-410_2_4_175,cs-410,2,4, Implementation of TR Systems,"00:13:06,000","00:13:12,210",175,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=786,let's just look at the question,pic_cs-410_2_4_780.jpg
cs-410_2_4_176,cs-410,2,4, Implementation of TR Systems,"00:13:12,210","00:13:17,470",176,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=792,Basically why is more efficient than,pic_cs-410_2_4_780.jpg
cs-410_2_4_177,cs-410,2,4, Implementation of TR Systems,"00:13:17,470","00:13:20,770",177,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=797,This is the obvious approach.,pic_cs-410_2_4_780.jpg
cs-410_2_4_178,cs-410,2,4, Implementation of TR Systems,"00:13:20,770","00:13:27,518",178,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=800,You can just compute a score for each,pic_cs-410_2_4_780.jpg
cs-410_2_4_179,cs-410,2,4, Implementation of TR Systems,"00:13:27,518","00:13:29,936",179,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=807,And this is a straightforward method but,pic_cs-410_2_4_780.jpg
cs-410_2_4_180,cs-410,2,4, Implementation of TR Systems,"00:13:29,936","00:13:34,496",180,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=809,this is going to be very slow imagine,pic_cs-410_2_4_780.jpg
cs-410_2_4_181,cs-410,2,4, Implementation of TR Systems,"00:13:34,496","00:13:39,620",181,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=814,If you do this then it will take,pic_cs-410_2_4_780.jpg
cs-410_2_4_182,cs-410,2,4, Implementation of TR Systems,"00:13:39,620","00:13:44,975",182,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=819,So the question now is why would,pic_cs-410_2_4_780.jpg
cs-410_2_4_183,cs-410,2,4, Implementation of TR Systems,"00:13:44,975","00:13:48,780",183,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=824,Well it has to do is the word,pic_cs-410_2_4_780.jpg
cs-410_2_4_184,cs-410,2,4, Implementation of TR Systems,"00:13:48,780","00:13:54,010",184,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=828,"So, here's some common phenomena",pic_cs-410_2_4_780.jpg
cs-410_2_4_185,cs-410,2,4, Implementation of TR Systems,"00:13:54,010","00:13:58,720",185,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=834,There are some languages independent,pic_cs-410_2_4_780.jpg
cs-410_2_4_186,cs-410,2,4, Implementation of TR Systems,"00:14:00,300","00:14:07,690",186,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=840,And these patterns are basically,pic_cs-410_2_4_840.jpg
cs-410_2_4_187,cs-410,2,4, Implementation of TR Systems,"00:14:07,690","00:14:10,830",187,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=847,A few words like the common,pic_cs-410_2_4_840.jpg
cs-410_2_4_188,cs-410,2,4, Implementation of TR Systems,"00:14:10,830","00:14:14,780",188,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=850,"we occur very, very frequently in text.",pic_cs-410_2_4_840.jpg
cs-410_2_4_189,cs-410,2,4, Implementation of TR Systems,"00:14:14,780","00:14:18,210",189,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=854,So they account for,pic_cs-410_2_4_840.jpg
cs-410_2_4_190,cs-410,2,4, Implementation of TR Systems,"00:14:19,405","00:14:22,885",190,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=859,But most words would occur just rarely.,pic_cs-410_2_4_840.jpg
cs-410_2_4_191,cs-410,2,4, Implementation of TR Systems,"00:14:22,885","00:14:25,615",191,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=862,"There are many words that occur just once,",pic_cs-410_2_4_840.jpg
cs-410_2_4_192,cs-410,2,4, Implementation of TR Systems,"00:14:25,615","00:14:29,790",192,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=865,"let's say, in a document or",pic_cs-410_2_4_840.jpg
cs-410_2_4_193,cs-410,2,4, Implementation of TR Systems,"00:14:29,790","00:14:33,306",193,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=869,And there are many such.,pic_cs-410_2_4_840.jpg
cs-410_2_4_194,cs-410,2,4, Implementation of TR Systems,"00:14:33,306","00:14:37,977",194,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=873,It's also true that the most,pic_cs-410_2_4_840.jpg
cs-410_2_4_195,cs-410,2,4, Implementation of TR Systems,"00:14:37,977","00:14:40,462",195,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=877,they have to be rare in another.,pic_cs-410_2_4_840.jpg
cs-410_2_4_196,cs-410,2,4, Implementation of TR Systems,"00:14:40,462","00:14:45,800",196,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=880,That means although the general,pic_cs-410_2_4_840.jpg
cs-410_2_4_197,cs-410,2,4, Implementation of TR Systems,"00:14:45,800","00:14:51,060",197,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=885,was observed in many cases that,pic_cs-410_2_4_840.jpg
cs-410_2_4_198,cs-410,2,4, Implementation of TR Systems,"00:14:51,060","00:14:54,770",198,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=891,may vary from context to context.,pic_cs-410_2_4_840.jpg
cs-410_2_4_199,cs-410,2,4, Implementation of TR Systems,"00:14:54,770","00:14:59,450",199,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=894,So this phenomena is characterized,pic_cs-410_2_4_840.jpg
cs-410_2_4_200,cs-410,2,4, Implementation of TR Systems,"00:14:59,450","00:15:02,210",200,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=899,This law says that the rank of a word,pic_cs-410_2_4_840.jpg
cs-410_2_4_201,cs-410,2,4, Implementation of TR Systems,"00:15:02,210","00:15:06,370",201,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=902,multiplied by the frequency of,pic_cs-410_2_4_900.jpg
cs-410_2_4_202,cs-410,2,4, Implementation of TR Systems,"00:15:07,450","00:15:13,045",202,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=907,So formally if we use F(w),pic_cs-410_2_4_900.jpg
cs-410_2_4_203,cs-410,2,4, Implementation of TR Systems,"00:15:13,045","00:15:16,310",203,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=913,r(w) to denote the rank of a word.,pic_cs-410_2_4_900.jpg
cs-410_2_4_204,cs-410,2,4, Implementation of TR Systems,"00:15:16,310","00:15:17,390",204,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=916,Then this is the formula.,pic_cs-410_2_4_900.jpg
cs-410_2_4_205,cs-410,2,4, Implementation of TR Systems,"00:15:17,390","00:15:21,300",205,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=917,"It basically says the same thing,",pic_cs-410_2_4_900.jpg
cs-410_2_4_206,cs-410,2,4, Implementation of TR Systems,"00:15:21,300","00:15:28,510",206,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=921,Where C is basically a constant and,pic_cs-410_2_4_900.jpg
cs-410_2_4_207,cs-410,2,4, Implementation of TR Systems,"00:15:28,510","00:15:34,180",207,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=928,"alpha, that might be adjusted to",pic_cs-410_2_4_900.jpg
cs-410_2_4_208,cs-410,2,4, Implementation of TR Systems,"00:15:34,180","00:15:38,128",208,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=934,So if I plot the word,pic_cs-410_2_4_900.jpg
cs-410_2_4_209,cs-410,2,4, Implementation of TR Systems,"00:15:38,128","00:15:40,980",209,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=938,then you can see this more easily.,pic_cs-410_2_4_900.jpg
cs-410_2_4_210,cs-410,2,4, Implementation of TR Systems,"00:15:40,980","00:15:43,660",210,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=940,The x axis is basically the word rank.,pic_cs-410_2_4_900.jpg
cs-410_2_4_211,cs-410,2,4, Implementation of TR Systems,"00:15:43,660","00:15:50,393",211,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=943,This is r(w) and,pic_cs-410_2_4_900.jpg
cs-410_2_4_212,cs-410,2,4, Implementation of TR Systems,"00:15:50,393","00:15:57,448",212,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=950,Now this curve shows that the product,pic_cs-410_2_4_900.jpg
cs-410_2_4_213,cs-410,2,4, Implementation of TR Systems,"00:15:57,448","00:16:02,524",213,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=957,"Now if you look at these words, we can see",pic_cs-410_2_4_900.jpg
cs-410_2_4_214,cs-410,2,4, Implementation of TR Systems,"00:16:02,524","00:16:06,870",214,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=962,"In the middle,",pic_cs-410_2_4_960.jpg
cs-410_2_4_215,cs-410,2,4, Implementation of TR Systems,"00:16:06,870","00:16:11,440",215,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=966,These words tend to occur,pic_cs-410_2_4_960.jpg
cs-410_2_4_216,cs-410,2,4, Implementation of TR Systems,"00:16:11,440","00:16:14,890",216,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=971,they are not like those,pic_cs-410_2_4_960.jpg
cs-410_2_4_217,cs-410,2,4, Implementation of TR Systems,"00:16:14,890","00:16:17,070",217,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=974,And they are also not very rare.,pic_cs-410_2_4_960.jpg
cs-410_2_4_218,cs-410,2,4, Implementation of TR Systems,"00:16:18,190","00:16:21,620",218,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=978,So they tend to be often used in,pic_cs-410_2_4_960.jpg
cs-410_2_4_219,cs-410,2,4, Implementation of TR Systems,"00:16:22,700","00:16:28,240",219,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=982,queries and they also tend,pic_cs-410_2_4_960.jpg
cs-410_2_4_220,cs-410,2,4, Implementation of TR Systems,"00:16:28,240","00:16:31,290",220,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=988,These intermediate frequency words.,pic_cs-410_2_4_960.jpg
cs-410_2_4_221,cs-410,2,4, Implementation of TR Systems,"00:16:31,290","00:16:34,480",221,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=991,But if you look at the left,pic_cs-410_2_4_960.jpg
cs-410_2_4_222,cs-410,2,4, Implementation of TR Systems,"00:16:35,820","00:16:38,330",222,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=995,these are the highest frequency words.,pic_cs-410_2_4_960.jpg
cs-410_2_4_223,cs-410,2,4, Implementation of TR Systems,"00:16:38,330","00:16:39,620",223,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=998,They are covered very frequently.,pic_cs-410_2_4_960.jpg
cs-410_2_4_224,cs-410,2,4, Implementation of TR Systems,"00:16:39,620","00:16:45,540",224,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=999,"They are usually words,",pic_cs-410_2_4_960.jpg
cs-410_2_4_225,cs-410,2,4, Implementation of TR Systems,"00:16:45,540","00:16:49,440",225,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1005,"Those words are very, very frequent and",pic_cs-410_2_4_960.jpg
cs-410_2_4_226,cs-410,2,4, Implementation of TR Systems,"00:16:49,440","00:16:54,226",226,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1009,"discriminated, and they are generally",pic_cs-410_2_4_960.jpg
cs-410_2_4_227,cs-410,2,4, Implementation of TR Systems,"00:16:54,226","00:17:01,900",227,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1014,So they are often removed and,pic_cs-410_2_4_960.jpg
cs-410_2_4_228,cs-410,2,4, Implementation of TR Systems,"00:17:01,900","00:17:06,960",228,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1021,So you can use pretty much just the kind,pic_cs-410_2_4_1020.jpg
cs-410_2_4_229,cs-410,2,4, Implementation of TR Systems,"00:17:06,960","00:17:09,620",229,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1026,infer what words might be stop words.,pic_cs-410_2_4_1020.jpg
cs-410_2_4_230,cs-410,2,4, Implementation of TR Systems,"00:17:09,620","00:17:12,690",230,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1029,Those are basically,pic_cs-410_2_4_1020.jpg
cs-410_2_4_231,cs-410,2,4, Implementation of TR Systems,"00:17:13,780","00:17:18,500",231,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1033,And they also occupy a lot of,pic_cs-410_2_4_1020.jpg
cs-410_2_4_232,cs-410,2,4, Implementation of TR Systems,"00:17:18,500","00:17:23,048",232,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1038,You can imagine the posting entries for,pic_cs-410_2_4_1020.jpg
cs-410_2_4_233,cs-410,2,4, Implementation of TR Systems,"00:17:23,048","00:17:24,370",233,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1043,"And then therefore,",pic_cs-410_2_4_1020.jpg
cs-410_2_4_234,cs-410,2,4, Implementation of TR Systems,"00:17:24,370","00:17:28,299",234,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1044,if you can remove such words you can save,pic_cs-410_2_4_1020.jpg
cs-410_2_4_235,cs-410,2,4, Implementation of TR Systems,"00:17:29,890","00:17:35,100",235,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1049,"We also show the tail part,",pic_cs-410_2_4_1020.jpg
cs-410_2_4_236,cs-410,2,4, Implementation of TR Systems,"00:17:35,100","00:17:38,470",236,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1055,"Those words don't occur very frequently,",pic_cs-410_2_4_1020.jpg
cs-410_2_4_237,cs-410,2,4, Implementation of TR Systems,"00:17:39,680","00:17:41,330",237,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1059,Those words are actually very useful for,pic_cs-410_2_4_1020.jpg
cs-410_2_4_238,cs-410,2,4, Implementation of TR Systems,"00:17:41,330","00:17:45,630",238,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1061,"search also, if a user happens to",pic_cs-410_2_4_1020.jpg
cs-410_2_4_239,cs-410,2,4, Implementation of TR Systems,"00:17:45,630","00:17:49,730",239,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1065,"But because they're rare,",pic_cs-410_2_4_1020.jpg
cs-410_2_4_240,cs-410,2,4, Implementation of TR Systems,"00:17:49,730","00:17:54,030",240,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1069,aren't necessarily,pic_cs-410_2_4_1020.jpg
cs-410_2_4_241,cs-410,2,4, Implementation of TR Systems,"00:17:54,030","00:17:58,970",241,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1074,But retain them would allow us to,pic_cs-410_2_4_1020.jpg
cs-410_2_4_242,cs-410,2,4, Implementation of TR Systems,"00:17:58,970","00:18:00,610",242,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1078,They generally have very high IDF.,pic_cs-410_2_4_1020.jpg
cs-410_2_4_243,cs-410,2,4, Implementation of TR Systems,"00:18:05,559","00:18:10,840",243,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1085,So what kind of data structures should,pic_cs-410_2_4_1080.jpg
cs-410_2_4_244,cs-410,2,4, Implementation of TR Systems,"00:18:10,840","00:18:11,970",244,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1090,"Well, it has two parts, right.",pic_cs-410_2_4_1080.jpg
cs-410_2_4_245,cs-410,2,4, Implementation of TR Systems,"00:18:11,970","00:18:16,720",245,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1091,"If you recall, we have a dictionary and",pic_cs-410_2_4_1080.jpg
cs-410_2_4_246,cs-410,2,4, Implementation of TR Systems,"00:18:16,720","00:18:21,810",246,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1096,"The dictionary has modest size, although",pic_cs-410_2_4_1080.jpg
cs-410_2_4_247,cs-410,2,4, Implementation of TR Systems,"00:18:21,810","00:18:24,810",247,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1101,large but compare it with,pic_cs-410_2_4_1080.jpg
cs-410_2_4_248,cs-410,2,4, Implementation of TR Systems,"00:18:26,220","00:18:29,710",248,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1106,And we also need to have fast,pic_cs-410_2_4_1080.jpg
cs-410_2_4_249,cs-410,2,4, Implementation of TR Systems,"00:18:29,710","00:18:32,940",249,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1109,because we're going to look up,pic_cs-410_2_4_1080.jpg
cs-410_2_4_250,cs-410,2,4, Implementation of TR Systems,"00:18:32,940","00:18:39,200",250,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1112,"So therefore, we'd prefer to keep such",pic_cs-410_2_4_1080.jpg
cs-410_2_4_251,cs-410,2,4, Implementation of TR Systems,"00:18:39,200","00:18:43,333",251,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1119,"If the collection is not very large,",pic_cs-410_2_4_1080.jpg
cs-410_2_4_252,cs-410,2,4, Implementation of TR Systems,"00:18:43,333","00:18:47,810",252,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1123,if the collection is very large,pic_cs-410_2_4_1080.jpg
cs-410_2_4_253,cs-410,2,4, Implementation of TR Systems,"00:18:47,810","00:18:52,100",253,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1127,"If the vocabulary size is very large,",pic_cs-410_2_4_1080.jpg
cs-410_2_4_254,cs-410,2,4, Implementation of TR Systems,"00:18:52,100","00:18:55,800",254,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1132,"So, in general that's how it goes.",pic_cs-410_2_4_1080.jpg
cs-410_2_4_255,cs-410,2,4, Implementation of TR Systems,"00:18:55,800","00:18:58,578",255,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1135,So the data structures,pic_cs-410_2_4_1080.jpg
cs-410_2_4_256,cs-410,2,4, Implementation of TR Systems,"00:18:58,578","00:19:01,390",256,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1138,"storing dictionary,",pic_cs-410_2_4_1080.jpg
cs-410_2_4_257,cs-410,2,4, Implementation of TR Systems,"00:19:01,390","00:19:04,375",257,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1141,"There are structures like hash table, or",pic_cs-410_2_4_1140.jpg
cs-410_2_4_258,cs-410,2,4, Implementation of TR Systems,"00:19:04,375","00:19:09,090",258,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1144,b-tree if we can't store,pic_cs-410_2_4_1140.jpg
cs-410_2_4_259,cs-410,2,4, Implementation of TR Systems,"00:19:09,090","00:19:12,760",259,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1149,And then try to build a structure that,pic_cs-410_2_4_1140.jpg
cs-410_2_4_260,cs-410,2,4, Implementation of TR Systems,"00:19:14,530","00:19:16,705",260,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1154,For postings they are huge.,pic_cs-410_2_4_1140.jpg
cs-410_2_4_261,cs-410,2,4, Implementation of TR Systems,"00:19:18,045","00:19:24,815",261,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1158,"And in general, we don't have to have",pic_cs-410_2_4_1140.jpg
cs-410_2_4_262,cs-410,2,4, Implementation of TR Systems,"00:19:24,815","00:19:29,145",262,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1164,We generally would just look up,pic_cs-410_2_4_1140.jpg
cs-410_2_4_263,cs-410,2,4, Implementation of TR Systems,"00:19:29,145","00:19:32,850",263,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1169,frequencies for all the documents,pic_cs-410_2_4_1140.jpg
cs-410_2_4_264,cs-410,2,4, Implementation of TR Systems,"00:19:33,930","00:19:36,570",264,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1173,So would read those entries sequentially.,pic_cs-410_2_4_1140.jpg
cs-410_2_4_265,cs-410,2,4, Implementation of TR Systems,"00:19:37,670","00:19:43,704",265,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1177,And therefore because it's large and,pic_cs-410_2_4_1140.jpg
cs-410_2_4_266,cs-410,2,4, Implementation of TR Systems,"00:19:43,704","00:19:49,826",266,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1183,they have to stay on disc and they would,pic_cs-410_2_4_1140.jpg
cs-410_2_4_267,cs-410,2,4, Implementation of TR Systems,"00:19:49,826","00:19:53,392",267,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1189,term frequency or,pic_cs-410_2_4_1140.jpg
cs-410_2_4_268,cs-410,2,4, Implementation of TR Systems,"00:19:53,392","00:19:58,300",268,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1193,"Now because they are very large,",pic_cs-410_2_4_1140.jpg
cs-410_2_4_269,cs-410,2,4, Implementation of TR Systems,"00:19:59,360","00:20:04,390",269,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1199,"Now this is not only to save disc space,",pic_cs-410_2_4_1140.jpg
cs-410_2_4_270,cs-410,2,4, Implementation of TR Systems,"00:20:04,390","00:20:09,080",270,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1204,"one benefit of compression, it It's",pic_cs-410_2_4_1200.jpg
cs-410_2_4_271,cs-410,2,4, Implementation of TR Systems,"00:20:09,080","00:20:11,750",271,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1209,But it's also to help improving speed.,pic_cs-410_2_4_1200.jpg
cs-410_2_4_272,cs-410,2,4, Implementation of TR Systems,"00:20:13,110","00:20:15,980",272,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1213,Can you see why?,pic_cs-410_2_4_1200.jpg
cs-410_2_4_273,cs-410,2,4, Implementation of TR Systems,"00:20:15,980","00:20:23,470",273,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1215,"Well, we know that input and",pic_cs-410_2_4_1200.jpg
cs-410_2_4_274,cs-410,2,4, Implementation of TR Systems,"00:20:23,470","00:20:28,320",274,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1223,In comparison with the time taken by CPU.,pic_cs-410_2_4_1200.jpg
cs-410_2_4_275,cs-410,2,4, Implementation of TR Systems,"00:20:28,320","00:20:33,410",275,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1228,"So, CPU is much faster but",pic_cs-410_2_4_1200.jpg
cs-410_2_4_276,cs-410,2,4, Implementation of TR Systems,"00:20:33,410","00:20:39,335",276,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1233,"so by compressing the inverter index,",pic_cs-410_2_4_1200.jpg
cs-410_2_4_277,cs-410,2,4, Implementation of TR Systems,"00:20:39,335","00:20:45,115",277,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1239,"the entries, that we have the readings,",pic_cs-410_2_4_1200.jpg
cs-410_2_4_278,cs-410,2,4, Implementation of TR Systems,"00:20:45,115","00:20:50,150",278,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1245,"would be smaller, and",pic_cs-410_2_4_1200.jpg
cs-410_2_4_279,cs-410,2,4, Implementation of TR Systems,"00:20:50,150","00:20:55,080",279,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1250,the amount of tracking IO and,pic_cs-410_2_4_1200.jpg
cs-410_2_4_280,cs-410,2,4, Implementation of TR Systems,"00:20:55,080","00:21:00,270",280,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1255,"Of course, we have to then do more",pic_cs-410_2_4_1200.jpg
cs-410_2_4_281,cs-410,2,4, Implementation of TR Systems,"00:21:00,270","00:21:03,630",281,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1260,uncompress the data in the memory.,pic_cs-410_2_4_1260.jpg
cs-410_2_4_282,cs-410,2,4, Implementation of TR Systems,"00:21:03,630","00:21:05,550",282,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1263,But as I said CPU is fast.,pic_cs-410_2_4_1260.jpg
cs-410_2_4_283,cs-410,2,4, Implementation of TR Systems,"00:21:05,550","00:21:07,019",283,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1265,So over all we can still save time.,pic_cs-410_2_4_1260.jpg
cs-410_2_4_284,cs-410,2,4, Implementation of TR Systems,"00:21:08,360","00:21:11,301",284,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1268,So compression here is both,pic_cs-410_2_4_1260.jpg
cs-410_2_4_285,cs-410,2,4, Implementation of TR Systems,"00:21:11,301","00:21:14,035",285,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1271,to speed up the loading of the index.,pic_cs-410_2_4_1260.jpg
cs-410_2_4_286,cs-410,2,4, Implementation of TR Systems,"00:21:14,035","00:21:24,035",286,https://www.coursera.org/learn/cs-410/lecture/2Cbq9?t=1274,[MUSIC],pic_cs-410_2_4_1260.jpg
cs-410_2_5_1,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:00,012","00:00:03,586",1,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=0,[SOUND],pic_cs-410_2_5_0.jpg
cs-410_2_5_2,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:07,325","00:00:10,038",2,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=7,This lecture is about the inverted index,pic_cs-410_2_5_0.jpg
cs-410_2_5_3,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:10,038","00:00:11,160",3,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=10,construction.,pic_cs-410_2_5_0.jpg
cs-410_2_5_4,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:13,840","00:00:18,520",4,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=13,"In this lecture, we will continue",pic_cs-410_2_5_0.jpg
cs-410_2_5_5,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:18,520","00:00:22,019",5,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=18,"In particular, we're going to discuss",pic_cs-410_2_5_0.jpg
cs-410_2_5_6,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:25,096","00:00:29,259",6,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=25,The construction of the inverted index,pic_cs-410_2_5_0.jpg
cs-410_2_5_7,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:29,259","00:00:30,450",7,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=29,very small.,pic_cs-410_2_5_0.jpg
cs-410_2_5_8,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:30,450","00:00:35,430",8,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=30,It's very easy to construct a dictionary,pic_cs-410_2_5_0.jpg
cs-410_2_5_9,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:36,600","00:00:42,280",9,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=36,The problem is that when our data,pic_cs-410_2_5_0.jpg
cs-410_2_5_10,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:42,280","00:00:45,180",10,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=42,then we have to use some,pic_cs-410_2_5_0.jpg
cs-410_2_5_11,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:46,500","00:00:51,900",11,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=46,"And unfortunately, in most retrieval",pic_cs-410_2_5_0.jpg
cs-410_2_5_12,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:51,900","00:00:55,700",12,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=51,And they generally cannot be,pic_cs-410_2_5_0.jpg
cs-410_2_5_13,cs-410,2,5, System Implementation - Inverted Index Construction,"00:00:56,790","00:01:01,843",13,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=56,And there are many approaches to,pic_cs-410_2_5_0.jpg
cs-410_2_5_14,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:01,843","00:01:06,710",14,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=61,method is quite common and,pic_cs-410_2_5_60.jpg
cs-410_2_5_15,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:06,710","00:01:11,480",15,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=66,"First, you collect the local termID,",pic_cs-410_2_5_60.jpg
cs-410_2_5_16,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:11,480","00:01:16,946",16,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=71,Basically you will locate the terms,pic_cs-410_2_5_60.jpg
cs-410_2_5_17,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:16,946","00:01:24,117",17,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=76,And then once you collect those accounts,pic_cs-410_2_5_60.jpg
cs-410_2_5_18,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:24,117","00:01:29,104",18,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=84,So that you will be able to local,pic_cs-410_2_5_60.jpg
cs-410_2_5_19,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:29,104","00:01:31,310",19,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=89,these are called rounds.,pic_cs-410_2_5_60.jpg
cs-410_2_5_20,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:31,310","00:01:36,930",20,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=91,And then you write them into,pic_cs-410_2_5_60.jpg
cs-410_2_5_21,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:36,930","00:01:38,940",21,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=96,then you merge in step 3.,pic_cs-410_2_5_60.jpg
cs-410_2_5_22,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:38,940","00:01:44,104",22,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=98,"Do pairwise merging of these runs, until",pic_cs-410_2_5_60.jpg
cs-410_2_5_23,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:44,104","00:01:46,460",23,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=104,generate a single inverted index.,pic_cs-410_2_5_60.jpg
cs-410_2_5_24,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:47,700","00:01:50,823",24,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=107,So this is an illustration of this method.,pic_cs-410_2_5_60.jpg
cs-410_2_5_25,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:50,823","00:01:54,265",25,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=110,On the left you see some documents and,pic_cs-410_2_5_60.jpg
cs-410_2_5_26,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:54,265","00:01:59,942",26,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=114,on the right we have a term lexicon and,pic_cs-410_2_5_60.jpg
cs-410_2_5_27,cs-410,2,5, System Implementation - Inverted Index Construction,"00:01:59,942","00:02:08,070",27,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=119,These lexicons are to map string-based,pic_cs-410_2_5_60.jpg
cs-410_2_5_28,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:08,070","00:02:12,261",28,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=128,terms into integer representations or,pic_cs-410_2_5_120.jpg
cs-410_2_5_29,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:12,261","00:02:18,112",29,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=132,map back from integers to,pic_cs-410_2_5_120.jpg
cs-410_2_5_30,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:18,112","00:02:23,010",30,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=138,The reason why we want our interest,pic_cs-410_2_5_120.jpg
cs-410_2_5_31,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:23,010","00:02:26,930",31,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=143,IDs is because integers,pic_cs-410_2_5_120.jpg
cs-410_2_5_32,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:26,930","00:02:29,770",32,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=146,"For example,",pic_cs-410_2_5_120.jpg
cs-410_2_5_33,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:29,770","00:02:33,240",33,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=149,"array, and they are also easy to compress.",pic_cs-410_2_5_120.jpg
cs-410_2_5_34,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:34,390","00:02:40,530",34,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=154,So this is one reason why we tend,pic_cs-410_2_5_120.jpg
cs-410_2_5_35,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:42,180","00:02:46,710",35,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=162,so that we don't have to,pic_cs-410_2_5_120.jpg
cs-410_2_5_36,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:46,710","00:02:48,070",36,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=166,So how does this approach work?,pic_cs-410_2_5_120.jpg
cs-410_2_5_37,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:48,070","00:02:49,822",37,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=168,"Well, it's very simple.",pic_cs-410_2_5_120.jpg
cs-410_2_5_38,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:49,822","00:02:53,260",38,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=169,We're going to scan these,pic_cs-410_2_5_120.jpg
cs-410_2_5_39,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:53,260","00:02:58,260",39,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=173,then parse the documents and,pic_cs-410_2_5_120.jpg
cs-410_2_5_40,cs-410,2,5, System Implementation - Inverted Index Construction,"00:02:58,260","00:03:03,306",40,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=178,And in this stage we generally sort,pic_cs-410_2_5_120.jpg
cs-410_2_5_41,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:03,306","00:03:06,961",41,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=183,because we process each,pic_cs-410_2_5_180.jpg
cs-410_2_5_42,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:06,961","00:03:11,310",42,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=186,So we'll first encounter all,pic_cs-410_2_5_180.jpg
cs-410_2_5_43,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:11,310","00:03:18,786",43,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=191,Therefore the document IDs,pic_cs-410_2_5_180.jpg
cs-410_2_5_44,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:18,786","00:03:25,503",44,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=198,And this will be followed by document IDs,pic_cs-410_2_5_180.jpg
cs-410_2_5_45,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:25,503","00:03:31,280",45,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=205,only just because we process,pic_cs-410_2_5_180.jpg
cs-410_2_5_46,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:31,280","00:03:34,890",46,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=211,"At some point,",pic_cs-410_2_5_180.jpg
cs-410_2_5_47,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:34,890","00:03:39,080",47,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=214,that would have to write,pic_cs-410_2_5_180.jpg
cs-410_2_5_48,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:39,080","00:03:45,830",48,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=219,Before we do that we 're going to sort,pic_cs-410_2_5_180.jpg
cs-410_2_5_49,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:45,830","00:03:51,948",49,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=225,We can sort them and then this time,pic_cs-410_2_5_180.jpg
cs-410_2_5_50,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:51,948","00:03:59,459",50,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=231,"Note that here,",pic_cs-410_2_5_180.jpg
cs-410_2_5_51,cs-410,2,5, System Implementation - Inverted Index Construction,"00:03:59,459","00:04:03,827",51,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=239,So all the entries that share the same,pic_cs-410_2_5_180.jpg
cs-410_2_5_52,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:03,827","00:04:08,557",52,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=243,"In this case,",pic_cs-410_2_5_240.jpg
cs-410_2_5_53,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:08,557","00:04:14,090",53,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=248,that match term 1 would,pic_cs-410_2_5_240.jpg
cs-410_2_5_54,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:14,090","00:04:18,850",54,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=254,And we're going to write this into,pic_cs-410_2_5_240.jpg
cs-410_2_5_55,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:18,850","00:04:22,800",55,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=258,And would that allows you to,pic_cs-410_2_5_240.jpg
cs-410_2_5_56,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:22,800","00:04:24,030",56,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=262,makes a batch of documents.,pic_cs-410_2_5_240.jpg
cs-410_2_5_57,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:24,030","00:04:26,670",57,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=264,And we're going to do that for,pic_cs-410_2_5_240.jpg
cs-410_2_5_58,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:26,670","00:04:32,546",58,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=266,So we're going to write a lot of,pic_cs-410_2_5_240.jpg
cs-410_2_5_59,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:32,546","00:04:35,400",59,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=272,And then the next stage is,pic_cs-410_2_5_240.jpg
cs-410_2_5_60,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:35,400","00:04:38,360",60,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=275,We're going to merge them and,pic_cs-410_2_5_240.jpg
cs-410_2_5_61,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:38,360","00:04:41,729",61,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=278,"Eventually, we will get",pic_cs-410_2_5_240.jpg
cs-410_2_5_62,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:41,729","00:04:45,310",62,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=281,where the entries are sorted,pic_cs-410_2_5_240.jpg
cs-410_2_5_63,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:46,960","00:04:50,870",63,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=286,"And on the top, we're going to see",pic_cs-410_2_5_240.jpg
cs-410_2_5_64,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:50,870","00:04:53,620",64,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=290,the documents that match term ID 1.,pic_cs-410_2_5_240.jpg
cs-410_2_5_65,cs-410,2,5, System Implementation - Inverted Index Construction,"00:04:53,620","00:05:00,300",65,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=293,"So this is basically, how we can do",pic_cs-410_2_5_240.jpg
cs-410_2_5_66,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:00,300","00:05:06,445",66,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=300,Even though the data cannot be,pic_cs-410_2_5_300.jpg
cs-410_2_5_67,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:06,445","00:05:12,562",67,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=306,"Now, we mention earlier that",pic_cs-410_2_5_300.jpg
cs-410_2_5_68,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:12,562","00:05:15,848",68,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=312,it's desirable to compress them.,pic_cs-410_2_5_300.jpg
cs-410_2_5_69,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:15,848","00:05:20,481",69,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=315,So let's now take a little bit,pic_cs-410_2_5_300.jpg
cs-410_2_5_70,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:20,481","00:05:24,084",70,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=320,"Well the idea of compression in general,",pic_cs-410_2_5_300.jpg
cs-410_2_5_71,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:24,084","00:05:28,310",71,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=324,leverage skewed distributions of values.,pic_cs-410_2_5_300.jpg
cs-410_2_5_72,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:28,310","00:05:31,090",72,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=328,And we generally have to use,pic_cs-410_2_5_300.jpg
cs-410_2_5_73,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:31,090","00:05:36,830",73,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=331,instead of the fixed-length,pic_cs-410_2_5_300.jpg
cs-410_2_5_74,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:36,830","00:05:41,080",74,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=336,a program manager like C++.,pic_cs-410_2_5_300.jpg
cs-410_2_5_75,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:41,080","00:05:45,840",75,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=341,And so how can we leverage,pic_cs-410_2_5_300.jpg
cs-410_2_5_76,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:45,840","00:05:48,180",76,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=345,to compress these values?,pic_cs-410_2_5_300.jpg
cs-410_2_5_77,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:48,180","00:05:53,827",77,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=348,"Well in general, we will use few",pic_cs-410_2_5_300.jpg
cs-410_2_5_78,cs-410,2,5, System Implementation - Inverted Index Construction,"00:05:53,827","00:06:00,650",78,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=353,words at the cost of using longer,pic_cs-410_2_5_300.jpg
cs-410_2_5_79,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:00,650","00:06:04,560",79,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=360,"So in our case, let's think about how",pic_cs-410_2_5_360.jpg
cs-410_2_5_80,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:05,640","00:06:09,640",80,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=365,"Now, if you can picture what",pic_cs-410_2_5_360.jpg
cs-410_2_5_81,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:09,640","00:06:13,807",81,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=369,"you will see in post things,",pic_cs-410_2_5_360.jpg
cs-410_2_5_82,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:13,807","00:06:19,089",82,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=373,Those are the frequencies of,pic_cs-410_2_5_360.jpg
cs-410_2_5_83,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:19,089","00:06:25,650",83,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=379,"Now, if you think about it, what kind",pic_cs-410_2_5_360.jpg
cs-410_2_5_84,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:25,650","00:06:29,980",84,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=385,You probably will be able to guess,pic_cs-410_2_5_360.jpg
cs-410_2_5_85,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:29,980","00:06:32,540",85,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=389,far more frequently than large numbers.,pic_cs-410_2_5_360.jpg
cs-410_2_5_86,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:32,540","00:06:33,990",86,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=392,Why?,pic_cs-410_2_5_360.jpg
cs-410_2_5_87,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:33,990","00:06:39,954",87,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=393,"Well, think about the distribution of",pic_cs-410_2_5_360.jpg
cs-410_2_5_88,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:39,954","00:06:44,810",88,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=399,and many words occur just rarely so,pic_cs-410_2_5_360.jpg
cs-410_2_5_89,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:44,810","00:06:48,419",89,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=404,"Therefore, we can use fewer bits for",pic_cs-410_2_5_360.jpg
cs-410_2_5_90,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:48,419","00:06:53,855",90,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=408,highly frequent integers and,pic_cs-410_2_5_360.jpg
cs-410_2_5_91,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:53,855","00:06:57,095",91,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=413,that's cost of using more bits for,pic_cs-410_2_5_360.jpg
cs-410_2_5_92,cs-410,2,5, System Implementation - Inverted Index Construction,"00:06:58,445","00:07:00,005",92,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=418,This is a trade off of course.,pic_cs-410_2_5_360.jpg
cs-410_2_5_93,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:00,005","00:07:05,712",93,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=420,"If the values are distributed to uniform,",pic_cs-410_2_5_420.jpg
cs-410_2_5_94,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:05,712","00:07:10,824",94,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=425,but because we tend to see many small,pic_cs-410_2_5_420.jpg
cs-410_2_5_95,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:10,824","00:07:15,769",95,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=430,We can save on average even though,pic_cs-410_2_5_420.jpg
cs-410_2_5_96,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:15,769","00:07:17,740",96,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=435,we have to use a lot of bits.,pic_cs-410_2_5_420.jpg
cs-410_2_5_97,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:19,750","00:07:23,700",97,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=439,What about the document IDs,pic_cs-410_2_5_420.jpg
cs-410_2_5_98,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:23,700","00:07:27,240",98,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=443,Well they are not distributed,pic_cs-410_2_5_420.jpg
cs-410_2_5_99,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:27,240","00:07:31,840",99,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=447,So how can we deal with that?,pic_cs-410_2_5_420.jpg
cs-410_2_5_100,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:31,840","00:07:35,488",100,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=451,Well it turns out that we can,pic_cs-410_2_5_420.jpg
cs-410_2_5_101,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:35,488","00:07:38,686",101,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=455,that is to store the difference,pic_cs-410_2_5_420.jpg
cs-410_2_5_102,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:38,686","00:07:43,495",102,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=458,And we can imagine if a term has,pic_cs-410_2_5_420.jpg
cs-410_2_5_103,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:43,495","00:07:46,640",103,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=463,there will be longest of document IDs.,pic_cs-410_2_5_420.jpg
cs-410_2_5_104,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:46,640","00:07:52,030",104,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=466,"So when we take the gap, and we take the",pic_cs-410_2_5_420.jpg
cs-410_2_5_105,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:52,030","00:07:54,340",105,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=472,those gaps will be small.,pic_cs-410_2_5_420.jpg
cs-410_2_5_106,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:54,340","00:07:57,594",106,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=474,"So again, see a lot of small numbers.",pic_cs-410_2_5_420.jpg
cs-410_2_5_107,cs-410,2,5, System Implementation - Inverted Index Construction,"00:07:57,594","00:08:00,217",107,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=477,Whereas if a term occurred,pic_cs-410_2_5_420.jpg
cs-410_2_5_108,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:00,217","00:08:04,300",108,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=480,"then the gap would be large,",pic_cs-410_2_5_480.jpg
cs-410_2_5_109,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:04,300","00:08:06,610",109,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=484,"So this creates some skewed distribution,",pic_cs-410_2_5_480.jpg
cs-410_2_5_110,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:06,610","00:08:10,669",110,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=486,that would allow us to,pic_cs-410_2_5_480.jpg
cs-410_2_5_111,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:11,850","00:08:15,621",111,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=491,This is also possible because,pic_cs-410_2_5_480.jpg
cs-410_2_5_112,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:15,621","00:08:21,249",112,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=495,"uncompress these document IDs,",pic_cs-410_2_5_480.jpg
cs-410_2_5_113,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:21,249","00:08:25,484",113,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=501,Because we stored the difference and,pic_cs-410_2_5_480.jpg
cs-410_2_5_114,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:25,484","00:08:29,574",114,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=505,document ID we have to first,pic_cs-410_2_5_480.jpg
cs-410_2_5_115,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:29,574","00:08:34,536",115,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=509,And then we can add the difference to,pic_cs-410_2_5_480.jpg
cs-410_2_5_116,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:34,536","00:08:36,365",116,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=514,the current document ID.,pic_cs-410_2_5_480.jpg
cs-410_2_5_117,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:36,365","00:08:40,834",117,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=516,Now this was possible because we only,pic_cs-410_2_5_480.jpg
cs-410_2_5_118,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:40,834","00:08:42,900",118,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=520,those document IDs.,pic_cs-410_2_5_480.jpg
cs-410_2_5_119,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:42,900","00:08:46,920",119,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=522,"Once we look up the term, we look up all",pic_cs-410_2_5_480.jpg
cs-410_2_5_120,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:46,920","00:08:48,670",120,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=526,then we sequentially process them.,pic_cs-410_2_5_480.jpg
cs-410_2_5_121,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:48,670","00:08:52,070",121,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=528,"So it's very natural,",pic_cs-410_2_5_480.jpg
cs-410_2_5_122,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:53,600","00:08:55,760",122,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=533,And there are many different methods for,pic_cs-410_2_5_480.jpg
cs-410_2_5_123,cs-410,2,5, System Implementation - Inverted Index Construction,"00:08:55,760","00:09:02,116",123,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=535,So binary code is a commonly used,pic_cs-410_2_5_480.jpg
cs-410_2_5_124,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:02,116","00:09:05,994",124,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=542,We use basically fixed glance in coding.,pic_cs-410_2_5_540.jpg
cs-410_2_5_125,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:05,994","00:09:09,276",125,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=545,"Unary code, gamma code, and",pic_cs-410_2_5_540.jpg
cs-410_2_5_126,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:09,276","00:09:11,240",126,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=549,there are many other possibilities.,pic_cs-410_2_5_540.jpg
cs-410_2_5_127,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:11,240","00:09:14,130",127,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=551,So let's look at some,pic_cs-410_2_5_540.jpg
cs-410_2_5_128,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:14,130","00:09:16,900",128,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=554,Binary coding is really,pic_cs-410_2_5_540.jpg
cs-410_2_5_129,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:16,900","00:09:20,930",129,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=556,that's a property for,pic_cs-410_2_5_540.jpg
cs-410_2_5_130,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:20,930","00:09:24,700",130,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=560,The unary coding is a variable,pic_cs-410_2_5_540.jpg
cs-410_2_5_131,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:24,700","00:09:28,891",131,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=564,"In this case, integer this 1 will be",pic_cs-410_2_5_540.jpg
cs-410_2_5_132,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:28,891","00:09:33,630",132,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=568,"encoded as x -1, 1 bit followed by 0.",pic_cs-410_2_5_540.jpg
cs-410_2_5_133,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:33,630","00:09:39,329",133,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=573,"So for example, 3 will be encoded as 2,",pic_cs-410_2_5_540.jpg
cs-410_2_5_134,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:39,329","00:09:45,042",134,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=579,"whereas 5 will be encoded as 4,",pic_cs-410_2_5_540.jpg
cs-410_2_5_135,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:45,042","00:09:51,599",135,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=585,So now you can imagine how many bits do we,pic_cs-410_2_5_540.jpg
cs-410_2_5_136,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:51,599","00:09:57,450",136,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=591,So how many bits do you have to,pic_cs-410_2_5_540.jpg
cs-410_2_5_137,cs-410,2,5, System Implementation - Inverted Index Construction,"00:09:57,450","00:10:02,549",137,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=597,"Well exactly, we have to use 100 bits.",pic_cs-410_2_5_540.jpg
cs-410_2_5_138,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:02,549","00:10:07,150",138,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=602,So it's the same number of bits,pic_cs-410_2_5_600.jpg
cs-410_2_5_139,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:07,150","00:10:12,360",139,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=607,So this is very inefficient if you,pic_cs-410_2_5_600.jpg
cs-410_2_5_140,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:12,360","00:10:17,620",140,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=612,Imagine if you occasionally see a number,pic_cs-410_2_5_600.jpg
cs-410_2_5_141,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:17,620","00:10:22,894",141,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=617,So this only works well if you,pic_cs-410_2_5_600.jpg
cs-410_2_5_142,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:22,894","00:10:28,082",142,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=622,"no large numbers, mostly very",pic_cs-410_2_5_600.jpg
cs-410_2_5_143,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:28,082","00:10:30,184",143,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=628,"Now, how do you decode this code?",pic_cs-410_2_5_600.jpg
cs-410_2_5_144,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:30,184","00:10:33,662",144,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=630,Now since these are variable,pic_cs-410_2_5_600.jpg
cs-410_2_5_145,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:33,662","00:10:37,070",145,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=633,you can't just count how many bits and,pic_cs-410_2_5_600.jpg
cs-410_2_5_146,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:38,500","00:10:44,800",146,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=638,"You can't say 8-bits or 32-bits,",pic_cs-410_2_5_600.jpg
cs-410_2_5_147,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:44,800","00:10:50,860",147,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=644,"They are variable length, so",pic_cs-410_2_5_600.jpg
cs-410_2_5_148,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:50,860","00:10:55,192",148,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=650,"In this case for unary, you can see",pic_cs-410_2_5_600.jpg
cs-410_2_5_149,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:55,192","00:10:59,161",149,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=655,Now you can easily see 0 would,pic_cs-410_2_5_600.jpg
cs-410_2_5_150,cs-410,2,5, System Implementation - Inverted Index Construction,"00:10:59,161","00:11:03,120",150,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=659,So you just count up how many 1s you,pic_cs-410_2_5_600.jpg
cs-410_2_5_151,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:03,120","00:11:06,560",151,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=663,"You have finished one number,",pic_cs-410_2_5_660.jpg
cs-410_2_5_152,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:07,960","00:11:11,266",152,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=667,Now we just saw that unary,pic_cs-410_2_5_660.jpg
cs-410_2_5_153,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:11,266","00:11:13,987",153,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=671,"In rewarding small numbers, and",pic_cs-410_2_5_660.jpg
cs-410_2_5_154,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:13,987","00:11:20,430",154,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=673,if you occasionally can see a very,pic_cs-410_2_5_660.jpg
cs-410_2_5_155,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:20,430","00:11:24,900",155,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=680,So what about some other,pic_cs-410_2_5_660.jpg
cs-410_2_5_156,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:24,900","00:11:29,200",156,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=684,Well gamma coding's one of them and,pic_cs-410_2_5_660.jpg
cs-410_2_5_157,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:29,200","00:11:34,072",157,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=689,in this method we can use unary coding for,pic_cs-410_2_5_660.jpg
cs-410_2_5_158,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:34,072","00:11:37,239",158,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=694,a transform form of that.,pic_cs-410_2_5_660.jpg
cs-410_2_5_159,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:37,239","00:11:41,210",159,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=697,So it's 1 plus the floor of log of x.,pic_cs-410_2_5_660.jpg
cs-410_2_5_160,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:41,210","00:11:47,781",160,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=701,So the magnitude of this value is,pic_cs-410_2_5_660.jpg
cs-410_2_5_161,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:47,781","00:11:52,703",161,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=707,So that's why we can afford,pic_cs-410_2_5_660.jpg
cs-410_2_5_162,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:52,703","00:11:58,728",162,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=712,And so first I have the unary code for,pic_cs-410_2_5_660.jpg
cs-410_2_5_163,cs-410,2,5, System Implementation - Inverted Index Construction,"00:11:58,728","00:12:02,327",163,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=718,And this would be followed by,pic_cs-410_2_5_660.jpg
cs-410_2_5_164,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:02,327","00:12:08,058",164,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=722,"And this basically the same uniform code,",pic_cs-410_2_5_720.jpg
cs-410_2_5_165,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:08,058","00:12:15,956",165,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=728,And we're going to use this coder to code,pic_cs-410_2_5_720.jpg
cs-410_2_5_166,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:15,956","00:12:22,178",166,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=735,And this is basically precisely,pic_cs-410_2_5_720.jpg
cs-410_2_5_167,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:25,000","00:12:30,376",167,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=745,So the unary code are basically,pic_cs-410_2_5_720.jpg
cs-410_2_5_168,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:30,376","00:12:33,029",168,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=750,well add one there and here.,pic_cs-410_2_5_720.jpg
cs-410_2_5_169,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:33,029","00:12:38,428",169,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=753,But the remaining part,pic_cs-410_2_5_720.jpg
cs-410_2_5_170,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:38,428","00:12:43,413",170,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=758,code through actually code the difference,pic_cs-410_2_5_720.jpg
cs-410_2_5_171,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:43,413","00:12:47,990",171,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=763,between the x and this 2 to the log of x.,pic_cs-410_2_5_720.jpg
cs-410_2_5_172,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:49,530","00:12:53,280",172,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=769,And it's easy to show that for this,pic_cs-410_2_5_720.jpg
cs-410_2_5_173,cs-410,2,5, System Implementation - Inverted Index Construction,"00:12:55,790","00:13:00,297",173,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=775,difference we only need to use up,pic_cs-410_2_5_720.jpg
cs-410_2_5_174,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:00,297","00:13:05,390",174,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=780,to this many bits and,pic_cs-410_2_5_780.jpg
cs-410_2_5_175,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:06,530","00:13:08,410",175,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=786,"And this is easy to understand,",pic_cs-410_2_5_780.jpg
cs-410_2_5_176,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:08,410","00:13:12,910",176,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=788,"if the difference is too large, then we",pic_cs-410_2_5_780.jpg
cs-410_2_5_177,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:14,330","00:13:19,000",177,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=794,So here are some examples for,pic_cs-410_2_5_780.jpg
cs-410_2_5_178,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:19,000","00:13:22,575",178,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=799,The first two digits are the unary code.,pic_cs-410_2_5_780.jpg
cs-410_2_5_179,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:22,575","00:13:26,706",179,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=802,"So this isn't for the value 2,",pic_cs-410_2_5_780.jpg
cs-410_2_5_180,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:26,706","00:13:30,990",180,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=806,10 encodes 2 in unary coding.,pic_cs-410_2_5_780.jpg
cs-410_2_5_181,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:32,490","00:13:37,300",181,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=812,And so that means the floor of,pic_cs-410_2_5_780.jpg
cs-410_2_5_182,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:37,300","00:13:42,398",182,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=817,"log of x is 1,",pic_cs-410_2_5_780.jpg
cs-410_2_5_183,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:42,398","00:13:45,620",183,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=822,"In code 1 plus the flow of log of x,",pic_cs-410_2_5_780.jpg
cs-410_2_5_184,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:45,620","00:13:50,145",184,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=825,since this is two then we know that,pic_cs-410_2_5_780.jpg
cs-410_2_5_185,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:52,000","00:13:55,720",185,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=832,So that 3 is still larger than 2 to the 1.,pic_cs-410_2_5_780.jpg
cs-410_2_5_186,cs-410,2,5, System Implementation - Inverted Index Construction,"00:13:55,720","00:14:00,040",186,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=835,"So the difference is 1, and",pic_cs-410_2_5_780.jpg
cs-410_2_5_187,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:01,460","00:14:04,690",187,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=841,So that's why we have 101 for 3.,pic_cs-410_2_5_840.jpg
cs-410_2_5_188,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:04,690","00:14:11,554",188,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=844,"Now similarly 5 is encoded as 110,",pic_cs-410_2_5_840.jpg
cs-410_2_5_189,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:12,970","00:14:17,981",189,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=852,And in this case the unary code in code 3.,pic_cs-410_2_5_840.jpg
cs-410_2_5_190,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:17,981","00:14:25,445",190,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=857,And so this is a unary code 110 and,pic_cs-410_2_5_840.jpg
cs-410_2_5_191,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:25,445","00:14:30,362",191,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=865,And that means we're going to,pic_cs-410_2_5_840.jpg
cs-410_2_5_192,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:30,362","00:14:32,784",192,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=870,the 2 to the 2 and that's 1.,pic_cs-410_2_5_840.jpg
cs-410_2_5_193,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:32,784","00:14:35,803",193,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=872,And so we now have again 1 at the end.,pic_cs-410_2_5_840.jpg
cs-410_2_5_194,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:35,803","00:14:39,226",194,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=875,"But this time we're going to use 2 bits,",pic_cs-410_2_5_840.jpg
cs-410_2_5_195,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:39,226","00:14:43,570",195,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=879,because with this level,pic_cs-410_2_5_840.jpg
cs-410_2_5_196,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:43,570","00:14:51,040",196,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=883,"We could have more numbers a 5, 6, 7 they",pic_cs-410_2_5_840.jpg
cs-410_2_5_197,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:51,040","00:14:53,210",197,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=891,"So in order to differentiate them,",pic_cs-410_2_5_840.jpg
cs-410_2_5_198,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:53,210","00:14:57,690",198,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=893,we have to use 2 bits in,pic_cs-410_2_5_840.jpg
cs-410_2_5_199,cs-410,2,5, System Implementation - Inverted Index Construction,"00:14:57,690","00:15:03,670",199,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=897,So you can imagine 6 would be 10 here,pic_cs-410_2_5_840.jpg
cs-410_2_5_200,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:04,710","00:15:10,615",200,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=904,It's also true that the form of,pic_cs-410_2_5_900.jpg
cs-410_2_5_201,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:10,615","00:15:15,155",201,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=910,"odd number of bits, and",pic_cs-410_2_5_900.jpg
cs-410_2_5_202,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:15,155","00:15:17,305",202,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=915,That's the end of the unary code.,pic_cs-410_2_5_900.jpg
cs-410_2_5_203,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:18,335","00:15:24,385",203,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=918,And before that or on the left side,pic_cs-410_2_5_900.jpg
cs-410_2_5_204,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:24,385","00:15:30,265",204,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=924,"And on the right side of this 0,",pic_cs-410_2_5_900.jpg
cs-410_2_5_205,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:32,550","00:15:36,540",205,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=932,So how can you decode such code?,pic_cs-410_2_5_900.jpg
cs-410_2_5_206,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:36,540","00:15:39,866",206,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=936,Well you again first do unary coding.,pic_cs-410_2_5_900.jpg
cs-410_2_5_207,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:39,866","00:15:45,371",207,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=939,"Once you hit 0, you have got the unary",pic_cs-410_2_5_900.jpg
cs-410_2_5_208,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:45,371","00:15:50,408",208,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=945,many bits you have to read further,pic_cs-410_2_5_900.jpg
cs-410_2_5_209,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:50,408","00:15:53,693",209,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=950,So this is how you can,pic_cs-410_2_5_900.jpg
cs-410_2_5_210,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:53,693","00:15:57,998",210,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=953,There is also a delta code that's,pic_cs-410_2_5_900.jpg
cs-410_2_5_211,cs-410,2,5, System Implementation - Inverted Index Construction,"00:15:57,998","00:16:01,340",211,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=957,that you replace the unary,pic_cs-410_2_5_900.jpg
cs-410_2_5_212,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:01,340","00:16:04,980",212,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=961,So that's even less,pic_cs-410_2_5_960.jpg
cs-410_2_5_213,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:04,980","00:16:08,910",213,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=964,in terms of wording the small integers.,pic_cs-410_2_5_960.jpg
cs-410_2_5_214,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:08,910","00:16:12,150",214,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=968,"So that means, it's okay if you",pic_cs-410_2_5_960.jpg
cs-410_2_5_215,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:14,100","00:16:15,190",215,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=974,It's okay with delta code.,pic_cs-410_2_5_960.jpg
cs-410_2_5_216,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:16,810","00:16:23,210",216,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=976,"It's also fine with the gamma code,",pic_cs-410_2_5_960.jpg
cs-410_2_5_217,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:23,210","00:16:26,710",217,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=983,"And they are all operating of course,",pic_cs-410_2_5_960.jpg
cs-410_2_5_218,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:26,710","00:16:32,360",218,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=986,at different degrees of favoring short or,pic_cs-410_2_5_960.jpg
cs-410_2_5_219,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:32,360","00:16:38,560",219,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=992,And that also means they would be,pic_cs-410_2_5_960.jpg
cs-410_2_5_220,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:38,560","00:16:41,720",220,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=998,But none of them is perfect for,pic_cs-410_2_5_960.jpg
cs-410_2_5_221,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:41,720","00:16:45,990",221,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1001,And which method works the best would,pic_cs-410_2_5_960.jpg
cs-410_2_5_222,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:45,990","00:16:47,610",222,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1005,in your dataset.,pic_cs-410_2_5_960.jpg
cs-410_2_5_223,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:47,610","00:16:49,660",223,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1007,"For inverted index compression,",pic_cs-410_2_5_960.jpg
cs-410_2_5_224,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:49,660","00:16:52,990",224,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1009,people have found that gamma,pic_cs-410_2_5_960.jpg
cs-410_2_5_225,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:55,114","00:16:58,340",225,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1015,So how to uncompress inverted index?,pic_cs-410_2_5_960.jpg
cs-410_2_5_226,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:58,340","00:16:59,900",226,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1018,I will just talk about this.,pic_cs-410_2_5_960.jpg
cs-410_2_5_227,cs-410,2,5, System Implementation - Inverted Index Construction,"00:16:59,900","00:17:02,920",227,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1019,"Firstly, you decode",pic_cs-410_2_5_960.jpg
cs-410_2_5_228,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:02,920","00:17:10,877",228,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1022,And we just I think discussed the how we,pic_cs-410_2_5_1020.jpg
cs-410_2_5_229,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:10,877","00:17:15,720",229,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1030,What about the document IDs that,pic_cs-410_2_5_1020.jpg
cs-410_2_5_230,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:15,720","00:17:19,320",230,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1035,"Well, we're going to do",pic_cs-410_2_5_1020.jpg
cs-410_2_5_231,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:19,320","00:17:23,800",231,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1039,"supposed the encoded I list is x1,",pic_cs-410_2_5_1020.jpg
cs-410_2_5_232,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:23,800","00:17:28,384",232,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1043,We first decode x1 to obtain,pic_cs-410_2_5_1020.jpg
cs-410_2_5_233,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:28,384","00:17:29,845",233,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1048,"Then we can decode x2,",pic_cs-410_2_5_1020.jpg
cs-410_2_5_234,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:29,845","00:17:34,610",234,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1049,which is actually the difference between,pic_cs-410_2_5_1020.jpg
cs-410_2_5_235,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:34,610","00:17:40,240",235,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1054,So we have to add the decoder,pic_cs-410_2_5_1020.jpg
cs-410_2_5_236,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:40,240","00:17:45,630",236,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1060,the value of the ID at,pic_cs-410_2_5_1020.jpg
cs-410_2_5_237,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:46,690","00:17:50,420",237,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1066,So this is where you can,pic_cs-410_2_5_1020.jpg
cs-410_2_5_238,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:50,420","00:17:52,870",238,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1070,converting document IDs to integers.,pic_cs-410_2_5_1020.jpg
cs-410_2_5_239,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:52,870","00:17:55,730",239,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1072,And that allows us to do,pic_cs-410_2_5_1020.jpg
cs-410_2_5_240,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:55,730","00:17:59,590",240,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1075,And we just repeat until we,pic_cs-410_2_5_1020.jpg
cs-410_2_5_241,cs-410,2,5, System Implementation - Inverted Index Construction,"00:17:59,590","00:18:04,004",241,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1079,Every time we use the document ID in,pic_cs-410_2_5_1020.jpg
cs-410_2_5_242,cs-410,2,5, System Implementation - Inverted Index Construction,"00:18:04,004","00:18:06,257",242,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1084,the document ID in the next position.,pic_cs-410_2_5_1080.jpg
cs-410_2_5_243,cs-410,2,5, System Implementation - Inverted Index Construction,"00:18:08,871","00:18:18,871",243,https://www.coursera.org/learn/cs-410/lecture/PgzsP?t=1088,[MUSIC],pic_cs-410_2_5_1080.jpg
cs-410_2_6_1,cs-410,2,6, System Implementation - Fast Search,"00:00:00,000","00:00:07,148",1,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=0,[SOUND],pic_cs-410_2_6_0.jpg
cs-410_2_6_2,cs-410,2,6, System Implementation - Fast Search,"00:00:07,148","00:00:12,770",2,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=7,This lecture is about how to do faster,pic_cs-410_2_6_0.jpg
cs-410_2_6_3,cs-410,2,6, System Implementation - Fast Search,"00:00:14,710","00:00:19,487",3,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=14,"In this lecture, we're going to continue",pic_cs-410_2_6_0.jpg
cs-410_2_6_4,cs-410,2,6, System Implementation - Fast Search,"00:00:19,487","00:00:20,583",4,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=19,"In particular,",pic_cs-410_2_6_0.jpg
cs-410_2_6_5,cs-410,2,6, System Implementation - Fast Search,"00:00:20,583","00:00:25,840",5,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=20,we're going to talk about how to support,pic_cs-410_2_6_0.jpg
cs-410_2_6_6,cs-410,2,6, System Implementation - Fast Search,"00:00:26,906","00:00:31,360",6,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=26,So let's think about what a general,pic_cs-410_2_6_0.jpg
cs-410_2_6_7,cs-410,2,6, System Implementation - Fast Search,"00:00:32,730","00:00:37,260",7,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=32,"Now of course, the vector space",pic_cs-410_2_6_0.jpg
cs-410_2_6_8,cs-410,2,6, System Implementation - Fast Search,"00:00:37,260","00:00:40,750",8,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=37,we can imagine many other retrieval,pic_cs-410_2_6_0.jpg
cs-410_2_6_9,cs-410,2,6, System Implementation - Fast Search,"00:00:42,390","00:00:44,970",9,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=42,So the form of this,pic_cs-410_2_6_0.jpg
cs-410_2_6_10,cs-410,2,6, System Implementation - Fast Search,"00:00:46,060","00:00:49,870",10,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=46,We see this scoring function,pic_cs-410_2_6_0.jpg
cs-410_2_6_11,cs-410,2,6, System Implementation - Fast Search,"00:00:49,870","00:00:55,260",11,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=49,a query Q is defined as,pic_cs-410_2_6_0.jpg
cs-410_2_6_12,cs-410,2,6, System Implementation - Fast Search,"00:00:55,260","00:01:00,350",12,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=55,that adjustment a function that,pic_cs-410_2_6_0.jpg
cs-410_2_6_13,cs-410,2,6, System Implementation - Fast Search,"00:01:00,350","00:01:05,425",13,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=60,"That I'll assume here at the end,",pic_cs-410_2_6_60.jpg
cs-410_2_6_14,cs-410,2,6, System Implementation - Fast Search,"00:01:05,425","00:01:09,100",14,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=65,f sub d of d and f sub q of q.,pic_cs-410_2_6_60.jpg
cs-410_2_6_15,cs-410,2,6, System Implementation - Fast Search,"00:01:09,100","00:01:14,467",15,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=69,These are adjustment factors,pic_cs-410_2_6_60.jpg
cs-410_2_6_16,cs-410,2,6, System Implementation - Fast Search,"00:01:14,467","00:01:19,387",16,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=74,so they are at the level of a document and,pic_cs-410_2_6_60.jpg
cs-410_2_6_17,cs-410,2,6, System Implementation - Fast Search,"00:01:19,387","00:01:22,719",17,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=79,"So and then inside of this function,",pic_cs-410_2_6_60.jpg
cs-410_2_6_18,cs-410,2,6, System Implementation - Fast Search,"00:01:22,719","00:01:27,127",18,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=82,we also see there's,pic_cs-410_2_6_60.jpg
cs-410_2_6_19,cs-410,2,6, System Implementation - Fast Search,"00:01:27,127","00:01:33,102",19,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=87,So this is the main part,pic_cs-410_2_6_60.jpg
cs-410_2_6_20,cs-410,2,6, System Implementation - Fast Search,"00:01:33,102","00:01:38,365",20,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=93,these as I just said of,pic_cs-410_2_6_60.jpg
cs-410_2_6_21,cs-410,2,6, System Implementation - Fast Search,"00:01:38,365","00:01:43,931",21,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=98,the level of the whole document and,pic_cs-410_2_6_60.jpg
cs-410_2_6_22,cs-410,2,6, System Implementation - Fast Search,"00:01:43,931","00:01:47,521",22,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=103,"For example, document [INAUDIBLE] and",pic_cs-410_2_6_60.jpg
cs-410_2_6_23,cs-410,2,6, System Implementation - Fast Search,"00:01:47,521","00:01:52,805",23,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=107,this aggregate punching would,pic_cs-410_2_6_60.jpg
cs-410_2_6_24,cs-410,2,6, System Implementation - Fast Search,"00:01:52,805","00:01:55,847",24,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=112,"Now inside this h function,",pic_cs-410_2_6_60.jpg
cs-410_2_6_25,cs-410,2,6, System Implementation - Fast Search,"00:01:55,847","00:02:01,300",25,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=115,there are functions that,pic_cs-410_2_6_60.jpg
cs-410_2_6_26,cs-410,2,6, System Implementation - Fast Search,"00:02:01,300","00:02:06,384",26,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=121,of the contribution of,pic_cs-410_2_6_120.jpg
cs-410_2_6_27,cs-410,2,6, System Implementation - Fast Search,"00:02:08,475","00:02:14,305",27,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=128,"So this g,",pic_cs-410_2_6_120.jpg
cs-410_2_6_28,cs-410,2,6, System Implementation - Fast Search,"00:02:14,305","00:02:19,670",28,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=134,of a matched query term ti in document d.,pic_cs-410_2_6_120.jpg
cs-410_2_6_29,cs-410,2,6, System Implementation - Fast Search,"00:02:23,710","00:02:28,365",29,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=143,And this h function would then,pic_cs-410_2_6_120.jpg
cs-410_2_6_30,cs-410,2,6, System Implementation - Fast Search,"00:02:28,365","00:02:34,390",30,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=148,"So for example,",pic_cs-410_2_6_120.jpg
cs-410_2_6_31,cs-410,2,6, System Implementation - Fast Search,"00:02:36,110","00:02:39,940",31,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=156,but it can also be a product or it could,pic_cs-410_2_6_120.jpg
cs-410_2_6_32,cs-410,2,6, System Implementation - Fast Search,"00:02:41,250","00:02:46,207",32,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=161,"And then finally, this adjustment",pic_cs-410_2_6_120.jpg
cs-410_2_6_33,cs-410,2,6, System Implementation - Fast Search,"00:02:46,207","00:02:51,162",33,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=166,the document level or query level,pic_cs-410_2_6_120.jpg
cs-410_2_6_34,cs-410,2,6, System Implementation - Fast Search,"00:02:51,162","00:02:53,697",34,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=171,"for example, document [INAUDIBLE].",pic_cs-410_2_6_120.jpg
cs-410_2_6_35,cs-410,2,6, System Implementation - Fast Search,"00:02:53,697","00:02:58,960",35,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=173,"So, this general form would cover",pic_cs-410_2_6_120.jpg
cs-410_2_6_36,cs-410,2,6, System Implementation - Fast Search,"00:02:58,960","00:03:06,610",36,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=178,Let's look at how we can score documents,pic_cs-410_2_6_120.jpg
cs-410_2_6_37,cs-410,2,6, System Implementation - Fast Search,"00:03:07,610","00:03:10,930",37,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=187,"So, here's a general algorithm",pic_cs-410_2_6_180.jpg
cs-410_2_6_38,cs-410,2,6, System Implementation - Fast Search,"00:03:10,930","00:03:14,670",38,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=190,First this query level and,pic_cs-410_2_6_180.jpg
cs-410_2_6_39,cs-410,2,6, System Implementation - Fast Search,"00:03:14,670","00:03:19,540",39,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=194,document level factors can be,pic_cs-410_2_6_180.jpg
cs-410_2_6_40,cs-410,2,6, System Implementation - Fast Search,"00:03:19,540","00:03:22,810",40,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=199,"Of course, for the query we have to",pic_cs-410_2_6_180.jpg
cs-410_2_6_41,cs-410,2,6, System Implementation - Fast Search,"00:03:22,810","00:03:28,180",41,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=202,"document, for example,",pic_cs-410_2_6_180.jpg
cs-410_2_6_42,cs-410,2,6, System Implementation - Fast Search,"00:03:28,180","00:03:32,850",42,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=208,"And then, we maintain a score accumulator",pic_cs-410_2_6_180.jpg
cs-410_2_6_43,cs-410,2,6, System Implementation - Fast Search,"00:03:34,710","00:03:39,440",43,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=214,An h is an aggregation function,pic_cs-410_2_6_180.jpg
cs-410_2_6_44,cs-410,2,6, System Implementation - Fast Search,"00:03:39,440","00:03:40,530",44,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=219,So how do we do that?,pic_cs-410_2_6_180.jpg
cs-410_2_6_45,cs-410,2,6, System Implementation - Fast Search,"00:03:40,530","00:03:45,770",45,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=220,For each period term we're going to,pic_cs-410_2_6_180.jpg
cs-410_2_6_46,cs-410,2,6, System Implementation - Fast Search,"00:03:45,770","00:03:47,130",46,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=225,from the invert index.,pic_cs-410_2_6_180.jpg
cs-410_2_6_47,cs-410,2,6, System Implementation - Fast Search,"00:03:47,130","00:03:51,290",47,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=227,This will give us all the documents,pic_cs-410_2_6_180.jpg
cs-410_2_6_48,cs-410,2,6, System Implementation - Fast Search,"00:03:52,850","00:03:57,640",48,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=232,"and that includes d1, f1 and so dn fn.",pic_cs-410_2_6_180.jpg
cs-410_2_6_49,cs-410,2,6, System Implementation - Fast Search,"00:03:57,640","00:04:03,394",49,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=237,So each pair is a document ID and,pic_cs-410_2_6_180.jpg
cs-410_2_6_50,cs-410,2,6, System Implementation - Fast Search,"00:04:03,394","00:04:08,268",50,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=243,Then for each entry d sub j and,pic_cs-410_2_6_240.jpg
cs-410_2_6_51,cs-410,2,6, System Implementation - Fast Search,"00:04:08,268","00:04:12,436",51,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=248,of the term in this,pic_cs-410_2_6_240.jpg
cs-410_2_6_52,cs-410,2,6, System Implementation - Fast Search,"00:04:12,436","00:04:17,739",52,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=252,We'll going to compute the function,pic_cs-410_2_6_240.jpg
cs-410_2_6_53,cs-410,2,6, System Implementation - Fast Search,"00:04:17,739","00:04:19,370",53,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=257,"weight of this term, so",pic_cs-410_2_6_240.jpg
cs-410_2_6_54,cs-410,2,6, System Implementation - Fast Search,"00:04:19,370","00:04:26,170",54,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=259,we're computing the weight completion of,pic_cs-410_2_6_240.jpg
cs-410_2_6_55,cs-410,2,6, System Implementation - Fast Search,"00:04:26,170","00:04:31,152",55,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=266,"And then, we're going to update",pic_cs-410_2_6_240.jpg
cs-410_2_6_56,cs-410,2,6, System Implementation - Fast Search,"00:04:31,152","00:04:35,820",56,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=271,this document and,pic_cs-410_2_6_240.jpg
cs-410_2_6_57,cs-410,2,6, System Implementation - Fast Search,"00:04:35,820","00:04:41,144",57,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=275,accumulator that would,pic_cs-410_2_6_240.jpg
cs-410_2_6_58,cs-410,2,6, System Implementation - Fast Search,"00:04:41,144","00:04:46,640",58,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=281,So this is basically a general,pic_cs-410_2_6_240.jpg
cs-410_2_6_59,cs-410,2,6, System Implementation - Fast Search,"00:04:46,640","00:04:51,288",59,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=286,functions of this form by,pic_cs-410_2_6_240.jpg
cs-410_2_6_60,cs-410,2,6, System Implementation - Fast Search,"00:04:51,288","00:04:54,621",60,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=291,Note that we don't have to,pic_cs-410_2_6_240.jpg
cs-410_2_6_61,cs-410,2,6, System Implementation - Fast Search,"00:04:54,621","00:04:56,906",61,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=294,that didn't match any query term.,pic_cs-410_2_6_240.jpg
cs-410_2_6_62,cs-410,2,6, System Implementation - Fast Search,"00:04:56,906","00:04:59,096",62,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=296,"Well, this is why it's fast,",pic_cs-410_2_6_240.jpg
cs-410_2_6_63,cs-410,2,6, System Implementation - Fast Search,"00:04:59,096","00:05:04,418",63,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=299,we only need to process the documents,pic_cs-410_2_6_240.jpg
cs-410_2_6_64,cs-410,2,6, System Implementation - Fast Search,"00:05:04,418","00:05:09,415",64,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=304,"In the end, then we're going to adjust",pic_cs-410_2_6_300.jpg
cs-410_2_6_65,cs-410,2,6, System Implementation - Fast Search,"00:05:09,415","00:05:11,600",65,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=309,sub a and then we can sort.,pic_cs-410_2_6_300.jpg
cs-410_2_6_66,cs-410,2,6, System Implementation - Fast Search,"00:05:11,600","00:05:14,270",66,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=311,So let's take a look,pic_cs-410_2_6_300.jpg
cs-410_2_6_67,cs-410,2,6, System Implementation - Fast Search,"00:05:14,270","00:05:17,880",67,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=314,"In this case, let's assume the scoring",pic_cs-410_2_6_300.jpg
cs-410_2_6_68,cs-410,2,6, System Implementation - Fast Search,"00:05:17,880","00:05:24,450",68,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=317,"it just takes the sum of t f, the role of",pic_cs-410_2_6_300.jpg
cs-410_2_6_69,cs-410,2,6, System Implementation - Fast Search,"00:05:25,830","00:05:31,340",69,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=325,This simplification would help,pic_cs-410_2_6_300.jpg
cs-410_2_6_70,cs-410,2,6, System Implementation - Fast Search,"00:05:31,340","00:05:36,640",70,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=331,It's very easy to extend the computation,pic_cs-410_2_6_300.jpg
cs-410_2_6_71,cs-410,2,6, System Implementation - Fast Search,"00:05:36,640","00:05:43,422",71,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=336,"the transformation of tf, or [INAUDIBLE]",pic_cs-410_2_6_300.jpg
cs-410_2_6_72,cs-410,2,6, System Implementation - Fast Search,"00:05:43,422","00:05:47,890",72,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=343,"So let's take a look at specific example,",pic_cs-410_2_6_300.jpg
cs-410_2_6_73,cs-410,2,6, System Implementation - Fast Search,"00:05:48,980","00:05:54,600",73,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=348,and it show some entries of,pic_cs-410_2_6_300.jpg
cs-410_2_6_74,cs-410,2,6, System Implementation - Fast Search,"00:05:54,600","00:05:56,800",74,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=354,Information occurred in four documents and,pic_cs-410_2_6_300.jpg
cs-410_2_6_75,cs-410,2,6, System Implementation - Fast Search,"00:05:56,800","00:06:01,260",75,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=356,"their frequencies are also there,",pic_cs-410_2_6_300.jpg
cs-410_2_6_76,cs-410,2,6, System Implementation - Fast Search,"00:06:01,260","00:06:07,210",76,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=361,"So let's see how the arrows works, so",pic_cs-410_2_6_360.jpg
cs-410_2_6_77,cs-410,2,6, System Implementation - Fast Search,"00:06:07,210","00:06:09,580",77,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=367,"and we fetch the first query then,",pic_cs-410_2_6_360.jpg
cs-410_2_6_78,cs-410,2,6, System Implementation - Fast Search,"00:06:09,580","00:06:12,260",78,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=369,"That's information, right?",pic_cs-410_2_6_360.jpg
cs-410_2_6_79,cs-410,2,6, System Implementation - Fast Search,"00:06:12,260","00:06:16,360",79,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=372,And imagine we have all these,pic_cs-410_2_6_360.jpg
cs-410_2_6_80,cs-410,2,6, System Implementation - Fast Search,"00:06:17,800","00:06:19,800",80,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=377,scores for these documents.,pic_cs-410_2_6_360.jpg
cs-410_2_6_81,cs-410,2,6, System Implementation - Fast Search,"00:06:19,800","00:06:21,740",81,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=379,We can imagine there will be other but,pic_cs-410_2_6_360.jpg
cs-410_2_6_82,cs-410,2,6, System Implementation - Fast Search,"00:06:21,740","00:06:24,660",82,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=381,then they will only be,pic_cs-410_2_6_360.jpg
cs-410_2_6_83,cs-410,2,6, System Implementation - Fast Search,"00:06:24,660","00:06:28,681",83,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=384,"So before we do any waiting of terms,",pic_cs-410_2_6_360.jpg
cs-410_2_6_84,cs-410,2,6, System Implementation - Fast Search,"00:06:28,681","00:06:32,979",84,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=388,we don't even need a score of.,pic_cs-410_2_6_360.jpg
cs-410_2_6_85,cs-410,2,6, System Implementation - Fast Search,"00:06:32,979","00:06:36,859",85,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=392,That comes actually we have these score,pic_cs-410_2_6_360.jpg
cs-410_2_6_86,cs-410,2,6, System Implementation - Fast Search,"00:06:38,260","00:06:43,110",86,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=398,So lets fetch the interest from,pic_cs-410_2_6_360.jpg
cs-410_2_6_87,cs-410,2,6, System Implementation - Fast Search,"00:06:43,110","00:06:45,080",87,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=403,"information, that the first one.",pic_cs-410_2_6_360.jpg
cs-410_2_6_88,cs-410,2,6, System Implementation - Fast Search,"00:06:46,260","00:06:50,809",88,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=406,So these four accumulators obviously,pic_cs-410_2_6_360.jpg
cs-410_2_6_89,cs-410,2,6, System Implementation - Fast Search,"00:06:51,830","00:06:54,418",89,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=411,"So, the first entry is d1 and 3,",pic_cs-410_2_6_360.jpg
cs-410_2_6_90,cs-410,2,6, System Implementation - Fast Search,"00:06:54,418","00:06:58,357",90,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=414,3 is occurrences of,pic_cs-410_2_6_360.jpg
cs-410_2_6_91,cs-410,2,6, System Implementation - Fast Search,"00:06:58,357","00:07:03,617",91,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=418,Since our scoring function assume that the,pic_cs-410_2_6_360.jpg
cs-410_2_6_92,cs-410,2,6, System Implementation - Fast Search,"00:07:03,617","00:07:09,178",92,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=423,We just need to add a 3 to the score,pic_cs-410_2_6_420.jpg
cs-410_2_6_93,cs-410,2,6, System Implementation - Fast Search,"00:07:09,178","00:07:16,388",93,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=429,the increase of score due to matching,pic_cs-410_2_6_420.jpg
cs-410_2_6_94,cs-410,2,6, System Implementation - Fast Search,"00:07:16,388","00:07:19,679",94,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=436,"And then, we go to the next entry,",pic_cs-410_2_6_420.jpg
cs-410_2_6_95,cs-410,2,6, System Implementation - Fast Search,"00:07:19,679","00:07:22,493",95,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=439,then we add a 4 to the score,pic_cs-410_2_6_420.jpg
cs-410_2_6_96,cs-410,2,6, System Implementation - Fast Search,"00:07:22,493","00:07:27,614",96,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=442,"Of course, at this point, that we will",pic_cs-410_2_6_420.jpg
cs-410_2_6_97,cs-410,2,6, System Implementation - Fast Search,"00:07:27,614","00:07:33,427",97,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=447,"And so at this point, we allocated",pic_cs-410_2_6_420.jpg
cs-410_2_6_98,cs-410,2,6, System Implementation - Fast Search,"00:07:33,427","00:07:39,174",98,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=453,"and we add one, we allocate another",pic_cs-410_2_6_420.jpg
cs-410_2_6_99,cs-410,2,6, System Implementation - Fast Search,"00:07:39,174","00:07:44,370",99,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=459,"And then finally,",pic_cs-410_2_6_420.jpg
cs-410_2_6_100,cs-410,2,6, System Implementation - Fast Search,"00:07:44,370","00:07:50,450",100,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=464,information occurred five,pic_cs-410_2_6_420.jpg
cs-410_2_6_101,cs-410,2,6, System Implementation - Fast Search,"00:07:50,450","00:07:55,310",101,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=470,"Okay, so this completes the processing of",pic_cs-410_2_6_420.jpg
cs-410_2_6_102,cs-410,2,6, System Implementation - Fast Search,"00:07:55,310","00:07:56,500",102,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=475,information.,pic_cs-410_2_6_420.jpg
cs-410_2_6_103,cs-410,2,6, System Implementation - Fast Search,"00:07:56,500","00:08:00,080",103,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=476,It processed all the contributions,pic_cs-410_2_6_420.jpg
cs-410_2_6_104,cs-410,2,6, System Implementation - Fast Search,"00:08:00,080","00:08:00,820",104,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=480,four documents.,pic_cs-410_2_6_480.jpg
cs-410_2_6_105,cs-410,2,6, System Implementation - Fast Search,"00:08:01,830","00:08:06,900",105,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=481,"So now, our error will go to",pic_cs-410_2_6_480.jpg
cs-410_2_6_106,cs-410,2,6, System Implementation - Fast Search,"00:08:06,900","00:08:09,810",106,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=486,"So, we're going to fetch all",pic_cs-410_2_6_480.jpg
cs-410_2_6_107,cs-410,2,6, System Implementation - Fast Search,"00:08:10,830","00:08:11,520",107,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=490,"So, in this case,",pic_cs-410_2_6_480.jpg
cs-410_2_6_108,cs-410,2,6, System Implementation - Fast Search,"00:08:11,520","00:08:15,700",108,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=491,"there are three entries, and",pic_cs-410_2_6_480.jpg
cs-410_2_6_109,cs-410,2,6, System Implementation - Fast Search,"00:08:15,700","00:08:18,410",109,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=495,The first is d2 and 3 and,pic_cs-410_2_6_480.jpg
cs-410_2_6_110,cs-410,2,6, System Implementation - Fast Search,"00:08:18,410","00:08:22,890",110,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=498,that means security occur three,pic_cs-410_2_6_480.jpg
cs-410_2_6_111,cs-410,2,6, System Implementation - Fast Search,"00:08:22,890","00:08:26,300",111,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=502,"Well, we do exactly the same,",pic_cs-410_2_6_480.jpg
cs-410_2_6_112,cs-410,2,6, System Implementation - Fast Search,"00:08:26,300","00:08:31,557",112,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=506,"So, this time we're going to change the",pic_cs-410_2_6_480.jpg
cs-410_2_6_113,cs-410,2,6, System Implementation - Fast Search,"00:08:31,557","00:08:36,390",113,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=511,allocated and,pic_cs-410_2_6_480.jpg
cs-410_2_6_114,cs-410,2,6, System Implementation - Fast Search,"00:08:36,390","00:08:40,470",114,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=516,"value which is a 4, so",pic_cs-410_2_6_480.jpg
cs-410_2_6_115,cs-410,2,6, System Implementation - Fast Search,"00:08:41,530","00:08:46,333",115,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=521,D2 score is increased because the match,pic_cs-410_2_6_480.jpg
cs-410_2_6_116,cs-410,2,6, System Implementation - Fast Search,"00:08:46,333","00:08:47,382",116,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=526,the security.,pic_cs-410_2_6_480.jpg
cs-410_2_6_117,cs-410,2,6, System Implementation - Fast Search,"00:08:47,382","00:08:53,721",117,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=527,"Go to the next entry, that's d4 and",pic_cs-410_2_6_480.jpg
cs-410_2_6_118,cs-410,2,6, System Implementation - Fast Search,"00:08:53,721","00:08:59,040",118,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=533,"d4 and again, we add 1 to d4 so",pic_cs-410_2_6_480.jpg
cs-410_2_6_119,cs-410,2,6, System Implementation - Fast Search,"00:08:59,040","00:09:02,449",119,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=539,"Finally, we process d5 and a 3.",pic_cs-410_2_6_480.jpg
cs-410_2_6_120,cs-410,2,6, System Implementation - Fast Search,"00:09:02,449","00:09:07,679",120,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=542,Since we have not yet allocated a score,pic_cs-410_2_6_540.jpg
cs-410_2_6_121,cs-410,2,6, System Implementation - Fast Search,"00:09:07,679","00:09:12,190",121,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=547,"we're going to allocate 1 for d5,",pic_cs-410_2_6_540.jpg
cs-410_2_6_122,cs-410,2,6, System Implementation - Fast Search,"00:09:12,190","00:09:19,480",122,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=552,"So, those scores, of the last rule,",pic_cs-410_2_6_540.jpg
cs-410_2_6_123,cs-410,2,6, System Implementation - Fast Search,"00:09:20,480","00:09:25,810",123,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=560,If our scoring function is just,pic_cs-410_2_6_540.jpg
cs-410_2_6_124,cs-410,2,6, System Implementation - Fast Search,"00:09:27,080","00:09:31,600",124,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=567,"Now, what if we, actually,",pic_cs-410_2_6_540.jpg
cs-410_2_6_125,cs-410,2,6, System Implementation - Fast Search,"00:09:31,600","00:09:35,130",125,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=571,"Well, we going to do the [INAUDIBLE]",pic_cs-410_2_6_540.jpg
cs-410_2_6_126,cs-410,2,6, System Implementation - Fast Search,"00:09:36,490","00:09:40,020",126,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=576,"So, to summarize this,",pic_cs-410_2_6_540.jpg
cs-410_2_6_127,cs-410,2,6, System Implementation - Fast Search,"00:09:40,020","00:09:44,660",127,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=580,we first process the information,pic_cs-410_2_6_540.jpg
cs-410_2_6_128,cs-410,2,6, System Implementation - Fast Search,"00:09:44,660","00:09:49,520",128,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=584,we processed all the entries,pic_cs-410_2_6_540.jpg
cs-410_2_6_129,cs-410,2,6, System Implementation - Fast Search,"00:09:49,520","00:09:54,775",129,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=589,"Then we process the security,",pic_cs-410_2_6_540.jpg
cs-410_2_6_130,cs-410,2,6, System Implementation - Fast Search,"00:09:54,775","00:10:00,916",130,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=594,what should be the order of processing,pic_cs-410_2_6_540.jpg
cs-410_2_6_131,cs-410,2,6, System Implementation - Fast Search,"00:10:00,916","00:10:05,677",131,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=600,It might make a difference especially,pic_cs-410_2_6_600.jpg
cs-410_2_6_132,cs-410,2,6, System Implementation - Fast Search,"00:10:05,677","00:10:07,670",132,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=605,the score accumulators.,pic_cs-410_2_6_600.jpg
cs-410_2_6_133,cs-410,2,6, System Implementation - Fast Search,"00:10:07,670","00:10:12,226",133,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=607,"Let's say, we only want to keep",pic_cs-410_2_6_600.jpg
cs-410_2_6_134,cs-410,2,6, System Implementation - Fast Search,"00:10:12,226","00:10:15,601",134,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=612,What do you think would be,pic_cs-410_2_6_600.jpg
cs-410_2_6_135,cs-410,2,6, System Implementation - Fast Search,"00:10:15,601","00:10:22,680",135,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=615,Would you process a common term first or,pic_cs-410_2_6_600.jpg
cs-410_2_6_136,cs-410,2,6, System Implementation - Fast Search,"00:10:24,460","00:10:30,597",136,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=624,The answers is we just go to who,pic_cs-410_2_6_600.jpg
cs-410_2_6_137,cs-410,2,6, System Implementation - Fast Search,"00:10:30,597","00:10:35,531",137,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=630,"A rare term would match a few documents,",pic_cs-410_2_6_600.jpg
cs-410_2_6_138,cs-410,2,6, System Implementation - Fast Search,"00:10:35,531","00:10:38,910",138,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=635,"be higher,",pic_cs-410_2_6_600.jpg
cs-410_2_6_139,cs-410,2,6, System Implementation - Fast Search,"00:10:38,910","00:10:44,933",139,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=638,"And then, it allows us to attach",pic_cs-410_2_6_600.jpg
cs-410_2_6_140,cs-410,2,6, System Implementation - Fast Search,"00:10:44,933","00:10:48,042",140,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=644,"So, it helps pruning",pic_cs-410_2_6_600.jpg
cs-410_2_6_141,cs-410,2,6, System Implementation - Fast Search,"00:10:48,042","00:10:51,901",141,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=648,if we don't need so,pic_cs-410_2_6_600.jpg
cs-410_2_6_142,cs-410,2,6, System Implementation - Fast Search,"00:10:51,901","00:10:55,474",142,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=651,So those are all heuristics for,pic_cs-410_2_6_600.jpg
cs-410_2_6_143,cs-410,2,6, System Implementation - Fast Search,"00:10:55,474","00:10:59,850",143,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=655,Here you can also see how we can,pic_cs-410_2_6_600.jpg
cs-410_2_6_144,cs-410,2,6, System Implementation - Fast Search,"00:10:59,850","00:11:03,192",144,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=659,So they can [INAUDIBLE] when we,pic_cs-410_2_6_600.jpg
cs-410_2_6_145,cs-410,2,6, System Implementation - Fast Search,"00:11:03,192","00:11:04,700",145,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=663,each query time.,pic_cs-410_2_6_660.jpg
cs-410_2_6_146,cs-410,2,6, System Implementation - Fast Search,"00:11:04,700","00:11:08,420",146,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=664,When we fetch the inverted index we,pic_cs-410_2_6_660.jpg
cs-410_2_6_147,cs-410,2,6, System Implementation - Fast Search,"00:11:08,420","00:11:09,990",147,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=668,then we can compute IDF.,pic_cs-410_2_6_660.jpg
cs-410_2_6_148,cs-410,2,6, System Implementation - Fast Search,"00:11:09,990","00:11:13,710",148,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=669,Or maybe perhaps the IDF value,pic_cs-410_2_6_660.jpg
cs-410_2_6_149,cs-410,2,6, System Implementation - Fast Search,"00:11:13,710","00:11:16,890",149,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=673,when we indexed the documents.,pic_cs-410_2_6_660.jpg
cs-410_2_6_150,cs-410,2,6, System Implementation - Fast Search,"00:11:16,890","00:11:22,780",150,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=676,"At that time, we already computed",pic_cs-410_2_6_660.jpg
cs-410_2_6_151,cs-410,2,6, System Implementation - Fast Search,"00:11:22,780","00:11:26,570",151,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=682,so all these can be done at this time.,pic_cs-410_2_6_660.jpg
cs-410_2_6_152,cs-410,2,6, System Implementation - Fast Search,"00:11:26,570","00:11:29,820",152,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=686,So that would mean when we process,pic_cs-410_2_6_660.jpg
cs-410_2_6_153,cs-410,2,6, System Implementation - Fast Search,"00:11:29,820","00:11:35,300",153,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=689,these words would be adjusted by the same,pic_cs-410_2_6_660.jpg
cs-410_2_6_154,cs-410,2,6, System Implementation - Fast Search,"00:11:36,590","00:11:39,580",154,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=696,So this is the basic idea of using,pic_cs-410_2_6_660.jpg
cs-410_2_6_155,cs-410,2,6, System Implementation - Fast Search,"00:11:39,580","00:11:44,770",155,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=699,it works well for all kinds of,pic_cs-410_2_6_660.jpg
cs-410_2_6_156,cs-410,2,6, System Implementation - Fast Search,"00:11:44,770","00:11:49,726",156,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=704,"And this generally,",pic_cs-410_2_6_660.jpg
cs-410_2_6_157,cs-410,2,6, System Implementation - Fast Search,"00:11:49,726","00:11:53,397",157,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=709,most state of art retrieval functions.,pic_cs-410_2_6_660.jpg
cs-410_2_6_158,cs-410,2,6, System Implementation - Fast Search,"00:11:53,397","00:11:58,708",158,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=713,So there are some tricks to,pic_cs-410_2_6_660.jpg
cs-410_2_6_159,cs-410,2,6, System Implementation - Fast Search,"00:11:58,708","00:12:02,988",159,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=718,some general techniques,pic_cs-410_2_6_660.jpg
cs-410_2_6_160,cs-410,2,6, System Implementation - Fast Search,"00:12:02,988","00:12:07,756",160,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=722,This is we just store some results of,pic_cs-410_2_6_720.jpg
cs-410_2_6_161,cs-410,2,6, System Implementation - Fast Search,"00:12:07,756","00:12:12,291",161,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=727,"when you see the same query,",pic_cs-410_2_6_720.jpg
cs-410_2_6_162,cs-410,2,6, System Implementation - Fast Search,"00:12:12,291","00:12:17,781",162,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=732,"Similarly, you can also slow the list",pic_cs-410_2_6_720.jpg
cs-410_2_6_163,cs-410,2,6, System Implementation - Fast Search,"00:12:17,781","00:12:19,041",163,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=737,a popular term.,pic_cs-410_2_6_720.jpg
cs-410_2_6_164,cs-410,2,6, System Implementation - Fast Search,"00:12:19,041","00:12:21,268",164,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=739,"And if the query term is popular likely,",pic_cs-410_2_6_720.jpg
cs-410_2_6_165,cs-410,2,6, System Implementation - Fast Search,"00:12:21,268","00:12:25,620",165,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=741,you will soon need to factor the inverted,pic_cs-410_2_6_720.jpg
cs-410_2_6_166,cs-410,2,6, System Implementation - Fast Search,"00:12:25,620","00:12:30,569",166,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=745,"So keeping it in the memory would help,",pic_cs-410_2_6_720.jpg
cs-410_2_6_167,cs-410,2,6, System Implementation - Fast Search,"00:12:30,569","00:12:32,206",167,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=750,improving efficiency.,pic_cs-410_2_6_720.jpg
cs-410_2_6_168,cs-410,2,6, System Implementation - Fast Search,"00:12:32,206","00:12:36,694",168,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=752,We can also keep only the most promising,pic_cs-410_2_6_720.jpg
cs-410_2_6_169,cs-410,2,6, System Implementation - Fast Search,"00:12:36,694","00:12:39,281",169,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=756,doesn't want to examine so many documents.,pic_cs-410_2_6_720.jpg
cs-410_2_6_170,cs-410,2,6, System Implementation - Fast Search,"00:12:39,281","00:12:44,092",170,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=759,We only need to return high,pic_cs-410_2_6_720.jpg
cs-410_2_6_171,cs-410,2,6, System Implementation - Fast Search,"00:12:44,092","00:12:46,410",171,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=764,likely are ranked on the top.,pic_cs-410_2_6_720.jpg
cs-410_2_6_172,cs-410,2,6, System Implementation - Fast Search,"00:12:47,900","00:12:51,860",172,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=767,"For that purpose,",pic_cs-410_2_6_720.jpg
cs-410_2_6_173,cs-410,2,6, System Implementation - Fast Search,"00:12:51,860","00:12:53,810",173,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=771,We don't have to store,pic_cs-410_2_6_720.jpg
cs-410_2_6_174,cs-410,2,6, System Implementation - Fast Search,"00:12:53,810","00:12:59,936",174,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=773,"At some point, we just keep",pic_cs-410_2_6_720.jpg
cs-410_2_6_175,cs-410,2,6, System Implementation - Fast Search,"00:12:59,936","00:13:06,257",175,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=779,Another technique is to do parallel,pic_cs-410_2_6_720.jpg
cs-410_2_6_176,cs-410,2,6, System Implementation - Fast Search,"00:13:06,257","00:13:11,731",176,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=786,really process in such a large,pic_cs-410_2_6_780.jpg
cs-410_2_6_177,cs-410,2,6, System Implementation - Fast Search,"00:13:11,731","00:13:15,869",177,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=791,And you scale up to,pic_cs-410_2_6_780.jpg
cs-410_2_6_178,cs-410,2,6, System Implementation - Fast Search,"00:13:15,869","00:13:20,628",178,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=795,the special techniques you,pic_cs-410_2_6_780.jpg
cs-410_2_6_179,cs-410,2,6, System Implementation - Fast Search,"00:13:20,628","00:13:25,609",179,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=800,to distribute the storage,pic_cs-410_2_6_780.jpg
cs-410_2_6_180,cs-410,2,6, System Implementation - Fast Search,"00:13:25,609","00:13:31,850",180,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=805,So here is a list of some text retrieval,pic_cs-410_2_6_780.jpg
cs-410_2_6_181,cs-410,2,6, System Implementation - Fast Search,"00:13:31,850","00:13:37,160",181,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=811,You can find more information,pic_cs-410_2_6_780.jpg
cs-410_2_6_182,cs-410,2,6, System Implementation - Fast Search,"00:13:37,160","00:13:42,510",182,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=817,"And here, I listed your four here,",pic_cs-410_2_6_780.jpg
cs-410_2_6_183,cs-410,2,6, System Implementation - Fast Search,"00:13:42,510","00:13:48,361",183,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=822,that can support a lot of applications and,pic_cs-410_2_6_780.jpg
cs-410_2_6_184,cs-410,2,6, System Implementation - Fast Search,"00:13:48,361","00:13:51,900",184,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=828,You can use it to build a search,pic_cs-410_2_6_780.jpg
cs-410_2_6_185,cs-410,2,6, System Implementation - Fast Search,"00:13:51,900","00:13:55,555",185,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=831,The downside is that it's not,pic_cs-410_2_6_780.jpg
cs-410_2_6_186,cs-410,2,6, System Implementation - Fast Search,"00:13:55,555","00:14:01,500",186,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=835,the algorithms implemented they are also,pic_cs-410_2_6_780.jpg
cs-410_2_6_187,cs-410,2,6, System Implementation - Fast Search,"00:14:01,500","00:14:06,294",187,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=841,Lemur or Indri is another,pic_cs-410_2_6_840.jpg
cs-410_2_6_188,cs-410,2,6, System Implementation - Fast Search,"00:14:06,294","00:14:10,068",188,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=846,a nice support web,pic_cs-410_2_6_840.jpg
cs-410_2_6_189,cs-410,2,6, System Implementation - Fast Search,"00:14:10,068","00:14:16,094",189,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=850,it has many advanced search algorithms and,pic_cs-410_2_6_840.jpg
cs-410_2_6_190,cs-410,2,6, System Implementation - Fast Search,"00:14:16,094","00:14:20,735",190,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=856,Terrier is yet another toolkit,pic_cs-410_2_6_840.jpg
cs-410_2_6_191,cs-410,2,6, System Implementation - Fast Search,"00:14:20,735","00:14:25,108",191,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=860,application capability and,pic_cs-410_2_6_840.jpg
cs-410_2_6_192,cs-410,2,6, System Implementation - Fast Search,"00:14:25,108","00:14:30,008",192,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=865,So that's maybe in between Lemur or,pic_cs-410_2_6_840.jpg
cs-410_2_6_193,cs-410,2,6, System Implementation - Fast Search,"00:14:30,008","00:14:34,663",193,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=870,maybe rather combining,pic_cs-410_2_6_840.jpg
cs-410_2_6_194,cs-410,2,6, System Implementation - Fast Search,"00:14:34,663","00:14:38,110",194,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=874,so that's also useful tool kit.,pic_cs-410_2_6_840.jpg
cs-410_2_6_195,cs-410,2,6, System Implementation - Fast Search,"00:14:38,110","00:14:41,920",195,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=878,MeTA is a toolkit that we will use for,pic_cs-410_2_6_840.jpg
cs-410_2_6_196,cs-410,2,6, System Implementation - Fast Search,"00:14:41,920","00:14:46,590",196,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=881,the problem assignment and,pic_cs-410_2_6_840.jpg
cs-410_2_6_197,cs-410,2,6, System Implementation - Fast Search,"00:14:47,690","00:14:54,250",197,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=887,a combination of both text retrieval,pic_cs-410_2_6_840.jpg
cs-410_2_6_198,cs-410,2,6, System Implementation - Fast Search,"00:14:54,250","00:15:01,390",198,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=894,And so talking models are implement they,pic_cs-410_2_6_840.jpg
cs-410_2_6_199,cs-410,2,6, System Implementation - Fast Search,"00:15:01,390","00:15:06,720",199,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=901,implemented in the toolkit as,pic_cs-410_2_6_900.jpg
cs-410_2_6_200,cs-410,2,6, System Implementation - Fast Search,"00:15:06,720","00:15:10,580",200,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=906,So to summarize all the discussion,pic_cs-410_2_6_900.jpg
cs-410_2_6_201,cs-410,2,6, System Implementation - Fast Search,"00:15:11,600","00:15:14,700",201,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=911,here are the major takeaway points.,pic_cs-410_2_6_900.jpg
cs-410_2_6_202,cs-410,2,6, System Implementation - Fast Search,"00:15:14,700","00:15:20,760",202,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=914,Inverted index is the primary data,pic_cs-410_2_6_900.jpg
cs-410_2_6_203,cs-410,2,6, System Implementation - Fast Search,"00:15:20,760","00:15:25,300",203,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=920,and that's the key to enable,pic_cs-410_2_6_900.jpg
cs-410_2_6_204,cs-410,2,6, System Implementation - Fast Search,"00:15:26,350","00:15:31,116",204,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=926,And the basic idea is to preprocess,pic_cs-410_2_6_900.jpg
cs-410_2_6_205,cs-410,2,6, System Implementation - Fast Search,"00:15:31,116","00:15:34,491",205,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=931,we want to do compression,pic_cs-410_2_6_900.jpg
cs-410_2_6_206,cs-410,2,6, System Implementation - Fast Search,"00:15:34,491","00:15:39,625",206,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=934,So that we can save disk space and,pic_cs-410_2_6_900.jpg
cs-410_2_6_207,cs-410,2,6, System Implementation - Fast Search,"00:15:39,625","00:15:43,840",207,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=939,processing of inverted index in general.,pic_cs-410_2_6_900.jpg
cs-410_2_6_208,cs-410,2,6, System Implementation - Fast Search,"00:15:43,840","00:15:48,400",208,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=943,We talked about how to construct the,pic_cs-410_2_6_900.jpg
cs-410_2_6_209,cs-410,2,6, System Implementation - Fast Search,"00:15:48,400","00:15:49,179",209,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=948,the memory.,pic_cs-410_2_6_900.jpg
cs-410_2_6_210,cs-410,2,6, System Implementation - Fast Search,"00:15:49,179","00:15:54,374",210,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=949,And then we talk about faster search using,pic_cs-410_2_6_900.jpg
cs-410_2_6_211,cs-410,2,6, System Implementation - Fast Search,"00:15:54,374","00:15:59,960",211,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=954,the invective index to accumulate a scores,pic_cs-410_2_6_900.jpg
cs-410_2_6_212,cs-410,2,6, System Implementation - Fast Search,"00:15:59,960","00:16:03,760",212,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=959,And we exploit the Zipf's law to,pic_cs-410_2_6_900.jpg
cs-410_2_6_213,cs-410,2,6, System Implementation - Fast Search,"00:16:03,760","00:16:06,052",213,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=963,that don't match any query term and,pic_cs-410_2_6_960.jpg
cs-410_2_6_214,cs-410,2,6, System Implementation - Fast Search,"00:16:06,052","00:16:11,540",214,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=966,this algorithm can actually support,pic_cs-410_2_6_960.jpg
cs-410_2_6_215,cs-410,2,6, System Implementation - Fast Search,"00:16:13,400","00:16:17,630",215,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=973,So these basic techniques,pic_cs-410_2_6_960.jpg
cs-410_2_6_216,cs-410,2,6, System Implementation - Fast Search,"00:16:17,630","00:16:23,570",216,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=977,further scaling up using distributed file,pic_cs-410_2_6_960.jpg
cs-410_2_6_217,cs-410,2,6, System Implementation - Fast Search,"00:16:23,570","00:16:28,410",217,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=983,Here are two additional readings you,pic_cs-410_2_6_960.jpg
cs-410_2_6_218,cs-410,2,6, System Implementation - Fast Search,"00:16:28,410","00:16:31,040",218,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=988,you are interested in,pic_cs-410_2_6_960.jpg
cs-410_2_6_219,cs-410,2,6, System Implementation - Fast Search,"00:16:31,040","00:16:38,156",219,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=991,The first one is a classical,pic_cs-410_2_6_960.jpg
cs-410_2_6_220,cs-410,2,6, System Implementation - Fast Search,"00:16:38,156","00:16:41,590",220,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=998,o inverted index and,pic_cs-410_2_6_960.jpg
cs-410_2_6_221,cs-410,2,6, System Implementation - Fast Search,"00:16:41,590","00:16:46,035",221,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=1001,"And how to,",pic_cs-410_2_6_960.jpg
cs-410_2_6_222,cs-410,2,6, System Implementation - Fast Search,"00:16:46,035","00:16:49,811",222,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=1006,"any inputs of the space,",pic_cs-410_2_6_960.jpg
cs-410_2_6_223,cs-410,2,6, System Implementation - Fast Search,"00:16:49,811","00:16:54,802",223,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=1009,The second one is a newer textbook that,pic_cs-410_2_6_960.jpg
cs-410_2_6_224,cs-410,2,6, System Implementation - Fast Search,"00:16:54,802","00:16:56,675",224,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=1014,evaluating search engines.,pic_cs-410_2_6_960.jpg
cs-410_2_6_225,cs-410,2,6, System Implementation - Fast Search,"00:16:58,835","00:17:08,835",225,https://www.coursera.org/learn/cs-410/lecture/QKK7y?t=1018,[MUSIC],pic_cs-410_2_6_960.jpg
cs-410_3_1_1,cs-410,3,1, Evaluation of TR Systems,"00:00:00,000","00:00:03,655",1,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=0,[MUSIC],pic_cs-410_3_1_0.jpg
cs-410_3_1_2,cs-410,3,1, Evaluation of TR Systems,"00:00:07,739","00:00:14,807",2,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=7,This lecture is about Evaluation of,pic_cs-410_3_1_0.jpg
cs-410_3_1_3,cs-410,3,1, Evaluation of TR Systems,"00:00:14,807","00:00:19,490",3,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=14,"lectures, we have talked about",pic_cs-410_3_1_0.jpg
cs-410_3_1_4,cs-410,3,1, Evaluation of TR Systems,"00:00:19,490","00:00:22,120",4,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=19,different kinds of ranking functions.,pic_cs-410_3_1_0.jpg
cs-410_3_1_5,cs-410,3,1, Evaluation of TR Systems,"00:00:23,550","00:00:27,040",5,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=23,But how do we know which,pic_cs-410_3_1_0.jpg
cs-410_3_1_6,cs-410,3,1, Evaluation of TR Systems,"00:00:27,040","00:00:28,390",6,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=27,"In order to answer this question,",pic_cs-410_3_1_0.jpg
cs-410_3_1_7,cs-410,3,1, Evaluation of TR Systems,"00:00:28,390","00:00:33,000",7,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=28,we have to compare them and that means we,pic_cs-410_3_1_0.jpg
cs-410_3_1_8,cs-410,3,1, Evaluation of TR Systems,"00:00:34,790","00:00:37,000",8,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=34,So this is the main topic of this lecture.,pic_cs-410_3_1_0.jpg
cs-410_3_1_9,cs-410,3,1, Evaluation of TR Systems,"00:00:40,462","00:00:42,730",9,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=40,"First, lets think about why",pic_cs-410_3_1_0.jpg
cs-410_3_1_10,cs-410,3,1, Evaluation of TR Systems,"00:00:42,730","00:00:44,290",10,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=42,I already give one reason.,pic_cs-410_3_1_0.jpg
cs-410_3_1_11,cs-410,3,1, Evaluation of TR Systems,"00:00:44,290","00:00:48,770",11,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=44,"That is, we have to use evaluation",pic_cs-410_3_1_0.jpg
cs-410_3_1_12,cs-410,3,1, Evaluation of TR Systems,"00:00:48,770","00:00:50,330",12,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=48,works better.,pic_cs-410_3_1_0.jpg
cs-410_3_1_13,cs-410,3,1, Evaluation of TR Systems,"00:00:50,330","00:00:54,310",13,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=50,Now this is very important for,pic_cs-410_3_1_0.jpg
cs-410_3_1_14,cs-410,3,1, Evaluation of TR Systems,"00:00:54,310","00:01:00,173",14,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=54,"Otherwise, we wouldn't know whether a new",pic_cs-410_3_1_0.jpg
cs-410_3_1_15,cs-410,3,1, Evaluation of TR Systems,"00:01:00,173","00:01:04,710",15,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=60,"In the beginning of this course, we talked",pic_cs-410_3_1_60.jpg
cs-410_3_1_16,cs-410,3,1, Evaluation of TR Systems,"00:01:04,710","00:01:07,200",16,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=64,We compare it with data base retrieval.,pic_cs-410_3_1_60.jpg
cs-410_3_1_17,cs-410,3,1, Evaluation of TR Systems,"00:01:08,440","00:01:14,390",17,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=68,There we mentioned that text retrieval,pic_cs-410_3_1_60.jpg
cs-410_3_1_18,cs-410,3,1, Evaluation of TR Systems,"00:01:14,390","00:01:18,240",18,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=74,So evaluation must rely on users.,pic_cs-410_3_1_60.jpg
cs-410_3_1_19,cs-410,3,1, Evaluation of TR Systems,"00:01:18,240","00:01:22,210",19,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=78,"Which system works better,",pic_cs-410_3_1_60.jpg
cs-410_3_1_20,cs-410,3,1, Evaluation of TR Systems,"00:01:25,020","00:01:28,850",20,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=85,"So, this becomes a very",pic_cs-410_3_1_60.jpg
cs-410_3_1_21,cs-410,3,1, Evaluation of TR Systems,"00:01:28,850","00:01:32,510",21,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=88,because how can we get users,pic_cs-410_3_1_60.jpg
cs-410_3_1_22,cs-410,3,1, Evaluation of TR Systems,"00:01:32,510","00:01:35,281",22,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=92,How can we do a fair comparison,pic_cs-410_3_1_60.jpg
cs-410_3_1_23,cs-410,3,1, Evaluation of TR Systems,"00:01:37,208","00:01:39,970",23,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=97,So just go back to the reasons for,pic_cs-410_3_1_60.jpg
cs-410_3_1_24,cs-410,3,1, Evaluation of TR Systems,"00:01:41,210","00:01:42,660",24,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=101,I listed two reasons here.,pic_cs-410_3_1_60.jpg
cs-410_3_1_25,cs-410,3,1, Evaluation of TR Systems,"00:01:42,660","00:01:47,060",25,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=102,"The second reason, is basically what I",pic_cs-410_3_1_60.jpg
cs-410_3_1_26,cs-410,3,1, Evaluation of TR Systems,"00:01:47,060","00:01:51,910",26,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=107,reason which is to assess the actual,pic_cs-410_3_1_60.jpg
cs-410_3_1_27,cs-410,3,1, Evaluation of TR Systems,"00:01:51,910","00:01:55,200",27,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=111,Imagine you're building your,pic_cs-410_3_1_60.jpg
cs-410_3_1_28,cs-410,3,1, Evaluation of TR Systems,"00:01:55,200","00:02:01,660",28,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=115,it would be interesting knowing how well,pic_cs-410_3_1_60.jpg
cs-410_3_1_29,cs-410,3,1, Evaluation of TR Systems,"00:02:01,660","00:02:02,350",29,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=121,"So in this case,",pic_cs-410_3_1_120.jpg
cs-410_3_1_30,cs-410,3,1, Evaluation of TR Systems,"00:02:02,350","00:02:07,850",30,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=122,matches must reflect the utility to,pic_cs-410_3_1_120.jpg
cs-410_3_1_31,cs-410,3,1, Evaluation of TR Systems,"00:02:07,850","00:02:11,840",31,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=127,"And typically, this has to be",pic_cs-410_3_1_120.jpg
cs-410_3_1_32,cs-410,3,1, Evaluation of TR Systems,"00:02:11,840","00:02:13,960",32,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=131,using the real search engine.,pic_cs-410_3_1_120.jpg
cs-410_3_1_33,cs-410,3,1, Evaluation of TR Systems,"00:02:16,340","00:02:18,630",33,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=136,"In the second case, or the second reason,",pic_cs-410_3_1_120.jpg
cs-410_3_1_34,cs-410,3,1, Evaluation of TR Systems,"00:02:19,970","00:02:26,130",34,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=139,the measures actually all need to collated,pic_cs-410_3_1_120.jpg
cs-410_3_1_35,cs-410,3,1, Evaluation of TR Systems,"00:02:26,130","00:02:30,120",35,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=146,"Thus, they don't have to accurately",pic_cs-410_3_1_120.jpg
cs-410_3_1_36,cs-410,3,1, Evaluation of TR Systems,"00:02:31,980","00:02:37,680",36,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=151,So the measure only needs to be good,pic_cs-410_3_1_120.jpg
cs-410_3_1_37,cs-410,3,1, Evaluation of TR Systems,"00:02:38,860","00:02:41,780",37,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=158,And this is usually done,pic_cs-410_3_1_120.jpg
cs-410_3_1_38,cs-410,3,1, Evaluation of TR Systems,"00:02:41,780","00:02:48,110",38,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=161,And this is the main idea that we'll,pic_cs-410_3_1_120.jpg
cs-410_3_1_39,cs-410,3,1, Evaluation of TR Systems,"00:02:48,110","00:02:53,520",39,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=168,This has been very important for,pic_cs-410_3_1_120.jpg
cs-410_3_1_40,cs-410,3,1, Evaluation of TR Systems,"00:02:53,520","00:02:56,910",40,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=173,for improving search,pic_cs-410_3_1_120.jpg
cs-410_3_1_41,cs-410,3,1, Evaluation of TR Systems,"00:02:58,910","00:03:01,880",41,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=178,So let's talk about what to measure.,pic_cs-410_3_1_120.jpg
cs-410_3_1_42,cs-410,3,1, Evaluation of TR Systems,"00:03:01,880","00:03:06,750",42,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=181,There are many aspects of searching,pic_cs-410_3_1_180.jpg
cs-410_3_1_43,cs-410,3,1, Evaluation of TR Systems,"00:03:06,750","00:03:09,000",43,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=186,"And here,",pic_cs-410_3_1_180.jpg
cs-410_3_1_44,cs-410,3,1, Evaluation of TR Systems,"00:03:09,000","00:03:11,190",44,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=189,"One, is effectiveness or accuracy.",pic_cs-410_3_1_180.jpg
cs-410_3_1_45,cs-410,3,1, Evaluation of TR Systems,"00:03:11,190","00:03:13,710",45,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=191,How accurate are the search results?,pic_cs-410_3_1_180.jpg
cs-410_3_1_46,cs-410,3,1, Evaluation of TR Systems,"00:03:13,710","00:03:18,110",46,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=193,"In this case, we're measuring a system's",pic_cs-410_3_1_180.jpg
cs-410_3_1_47,cs-410,3,1, Evaluation of TR Systems,"00:03:18,110","00:03:20,150",47,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=198,on top of non relevant ones.,pic_cs-410_3_1_180.jpg
cs-410_3_1_48,cs-410,3,1, Evaluation of TR Systems,"00:03:20,150","00:03:21,850",48,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=200,"The second, is efficiency.",pic_cs-410_3_1_180.jpg
cs-410_3_1_49,cs-410,3,1, Evaluation of TR Systems,"00:03:21,850","00:03:24,470",49,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=201,How quickly can you get the results?,pic_cs-410_3_1_180.jpg
cs-410_3_1_50,cs-410,3,1, Evaluation of TR Systems,"00:03:24,470","00:03:27,710",50,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=204,How much computing resources,pic_cs-410_3_1_180.jpg
cs-410_3_1_51,cs-410,3,1, Evaluation of TR Systems,"00:03:27,710","00:03:31,150",51,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=207,"In this case, we need to measure the space",pic_cs-410_3_1_180.jpg
cs-410_3_1_52,cs-410,3,1, Evaluation of TR Systems,"00:03:32,540","00:03:34,890",52,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=212,The third aspect is usability.,pic_cs-410_3_1_180.jpg
cs-410_3_1_53,cs-410,3,1, Evaluation of TR Systems,"00:03:34,890","00:03:38,950",53,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=214,"Basically the question is,",pic_cs-410_3_1_180.jpg
cs-410_3_1_54,cs-410,3,1, Evaluation of TR Systems,"00:03:38,950","00:03:40,840",54,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=218,"Here, obviously, interfaces and",pic_cs-410_3_1_180.jpg
cs-410_3_1_55,cs-410,3,1, Evaluation of TR Systems,"00:03:40,840","00:03:45,870",55,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=220,many other things also important and,pic_cs-410_3_1_180.jpg
cs-410_3_1_56,cs-410,3,1, Evaluation of TR Systems,"00:03:47,410","00:03:51,670",56,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=227,"Now in this course, we're going to",pic_cs-410_3_1_180.jpg
cs-410_3_1_57,cs-410,3,1, Evaluation of TR Systems,"00:03:51,670","00:03:52,710",57,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=231,accuracy measures.,pic_cs-410_3_1_180.jpg
cs-410_3_1_58,cs-410,3,1, Evaluation of TR Systems,"00:03:52,710","00:03:55,340",58,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=232,Because the efficiency and,pic_cs-410_3_1_180.jpg
cs-410_3_1_59,cs-410,3,1, Evaluation of TR Systems,"00:03:55,340","00:04:00,230",59,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=235,usability dimensions are not,pic_cs-410_3_1_180.jpg
cs-410_3_1_60,cs-410,3,1, Evaluation of TR Systems,"00:04:00,230","00:04:08,640",60,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=240,And so they are needed for,pic_cs-410_3_1_240.jpg
cs-410_3_1_61,cs-410,3,1, Evaluation of TR Systems,"00:04:08,640","00:04:13,347",61,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=248,And there is also good coverage,pic_cs-410_3_1_240.jpg
cs-410_3_1_62,cs-410,3,1, Evaluation of TR Systems,"00:04:13,347","00:04:18,780",62,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=253,But how to evaluate search,pic_cs-410_3_1_240.jpg
cs-410_3_1_63,cs-410,3,1, Evaluation of TR Systems,"00:04:18,780","00:04:23,110",63,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=258,something unique to text retrieval and,pic_cs-410_3_1_240.jpg
cs-410_3_1_64,cs-410,3,1, Evaluation of TR Systems,"00:04:23,110","00:04:28,428",64,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=263,The main idea that people have proposed,pic_cs-410_3_1_240.jpg
cs-410_3_1_65,cs-410,3,1, Evaluation of TR Systems,"00:04:28,428","00:04:33,850",65,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=268,the text retrieval algorithm is called,pic_cs-410_3_1_240.jpg
cs-410_3_1_66,cs-410,3,1, Evaluation of TR Systems,"00:04:33,850","00:04:40,145",66,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=273,This one actually was developed,pic_cs-410_3_1_240.jpg
cs-410_3_1_67,cs-410,3,1, Evaluation of TR Systems,"00:04:40,145","00:04:44,785",67,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=280,It's a methodology for,pic_cs-410_3_1_240.jpg
cs-410_3_1_68,cs-410,3,1, Evaluation of TR Systems,"00:04:45,985","00:04:49,305",68,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=285,Its sampling methodology that has,pic_cs-410_3_1_240.jpg
cs-410_3_1_69,cs-410,3,1, Evaluation of TR Systems,"00:04:49,305","00:04:50,880",69,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=289,search engine evaluation.,pic_cs-410_3_1_240.jpg
cs-410_3_1_70,cs-410,3,1, Evaluation of TR Systems,"00:04:50,880","00:04:55,930",70,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=290,But also for evaluating virtually,pic_cs-410_3_1_240.jpg
cs-410_3_1_71,cs-410,3,1, Evaluation of TR Systems,"00:04:55,930","00:05:01,180",71,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=295,for example in natural language processing,pic_cs-410_3_1_240.jpg
cs-410_3_1_72,cs-410,3,1, Evaluation of TR Systems,"00:05:01,180","00:05:05,620",72,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=301,"is empirical to find, we typically",pic_cs-410_3_1_300.jpg
cs-410_3_1_73,cs-410,3,1, Evaluation of TR Systems,"00:05:05,620","00:05:09,450",73,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=305,And today with the big data challenging,pic_cs-410_3_1_300.jpg
cs-410_3_1_74,cs-410,3,1, Evaluation of TR Systems,"00:05:09,450","00:05:13,590",74,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=309,with the use of machine,pic_cs-410_3_1_300.jpg
cs-410_3_1_75,cs-410,3,1, Evaluation of TR Systems,"00:05:13,590","00:05:17,430",75,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=313,"This methodology has been very popular,",pic_cs-410_3_1_300.jpg
cs-410_3_1_76,cs-410,3,1, Evaluation of TR Systems,"00:05:17,430","00:05:20,100",76,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=317,a search engine application in the 1960s.,pic_cs-410_3_1_300.jpg
cs-410_3_1_77,cs-410,3,1, Evaluation of TR Systems,"00:05:20,100","00:05:25,250",77,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=320,So the basic idea of this approach is,pic_cs-410_3_1_300.jpg
cs-410_3_1_78,cs-410,3,1, Evaluation of TR Systems,"00:05:25,250","00:05:26,200",78,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=325,define measures.,pic_cs-410_3_1_300.jpg
cs-410_3_1_79,cs-410,3,1, Evaluation of TR Systems,"00:05:27,220","00:05:30,350",79,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=327,"Once such a test collection is built,",pic_cs-410_3_1_300.jpg
cs-410_3_1_80,cs-410,3,1, Evaluation of TR Systems,"00:05:30,350","00:05:32,990",80,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=330,again to test different algorithms.,pic_cs-410_3_1_300.jpg
cs-410_3_1_81,cs-410,3,1, Evaluation of TR Systems,"00:05:32,990","00:05:36,180",81,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=332,And we're going to define measures,pic_cs-410_3_1_300.jpg
cs-410_3_1_82,cs-410,3,1, Evaluation of TR Systems,"00:05:36,180","00:05:39,660",82,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=336,performance of a system and algorithm.,pic_cs-410_3_1_300.jpg
cs-410_3_1_83,cs-410,3,1, Evaluation of TR Systems,"00:05:41,070","00:05:42,960",83,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=341,So how exactly will this work?,pic_cs-410_3_1_300.jpg
cs-410_3_1_84,cs-410,3,1, Evaluation of TR Systems,"00:05:42,960","00:05:47,090",84,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=342,Well we can do have a sample collection of,pic_cs-410_3_1_300.jpg
cs-410_3_1_85,cs-410,3,1, Evaluation of TR Systems,"00:05:47,090","00:05:49,986",85,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=347,the real document collection,pic_cs-410_3_1_300.jpg
cs-410_3_1_86,cs-410,3,1, Evaluation of TR Systems,"00:05:49,986","00:05:53,210",86,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=349,We're going to also have a sample,pic_cs-410_3_1_300.jpg
cs-410_3_1_87,cs-410,3,1, Evaluation of TR Systems,"00:05:53,210","00:05:55,240",87,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=353,This is a little simulator,pic_cs-410_3_1_300.jpg
cs-410_3_1_88,cs-410,3,1, Evaluation of TR Systems,"00:05:56,270","00:05:58,980",88,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=356,"Then, we'll have to have",pic_cs-410_3_1_300.jpg
cs-410_3_1_89,cs-410,3,1, Evaluation of TR Systems,"00:05:58,980","00:06:03,930",89,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=358,These are judgments of which documents,pic_cs-410_3_1_300.jpg
cs-410_3_1_90,cs-410,3,1, Evaluation of TR Systems,"00:06:03,930","00:06:08,250",90,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=363,"Ideally, they have to be made by",pic_cs-410_3_1_360.jpg
cs-410_3_1_91,cs-410,3,1, Evaluation of TR Systems,"00:06:08,250","00:06:12,930",91,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=368,Because those are the people that know,pic_cs-410_3_1_360.jpg
cs-410_3_1_92,cs-410,3,1, Evaluation of TR Systems,"00:06:12,930","00:06:14,690",92,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=372,"And finally, we have to have matches for",pic_cs-410_3_1_360.jpg
cs-410_3_1_93,cs-410,3,1, Evaluation of TR Systems,"00:06:14,690","00:06:19,830",93,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=374,quantify how well our system's result,pic_cs-410_3_1_360.jpg
cs-410_3_1_94,cs-410,3,1, Evaluation of TR Systems,"00:06:19,830","00:06:24,560",94,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=379,That would be constructed base,pic_cs-410_3_1_360.jpg
cs-410_3_1_95,cs-410,3,1, Evaluation of TR Systems,"00:06:24,560","00:06:30,917",95,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=384,So this methodology is very useful for,pic_cs-410_3_1_360.jpg
cs-410_3_1_96,cs-410,3,1, Evaluation of TR Systems,"00:06:30,917","00:06:36,130",96,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=390,because the test can be reused many times.,pic_cs-410_3_1_360.jpg
cs-410_3_1_97,cs-410,3,1, Evaluation of TR Systems,"00:06:36,130","00:06:41,340",97,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=396,And it will also provide a fair,pic_cs-410_3_1_360.jpg
cs-410_3_1_98,cs-410,3,1, Evaluation of TR Systems,"00:06:41,340","00:06:43,370",98,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=401,We have the same criteria or,pic_cs-410_3_1_360.jpg
cs-410_3_1_99,cs-410,3,1, Evaluation of TR Systems,"00:06:43,370","00:06:47,570",99,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=403,same dataset to be used to,pic_cs-410_3_1_360.jpg
cs-410_3_1_100,cs-410,3,1, Evaluation of TR Systems,"00:06:47,570","00:06:50,810",100,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=407,This allows us to compare,pic_cs-410_3_1_360.jpg
cs-410_3_1_101,cs-410,3,1, Evaluation of TR Systems,"00:06:50,810","00:06:55,660",101,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=410,an old algorithm that was divided many,pic_cs-410_3_1_360.jpg
cs-410_3_1_102,cs-410,3,1, Evaluation of TR Systems,"00:06:55,660","00:06:59,580",102,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=415,"So this is the illustration of this works,",pic_cs-410_3_1_360.jpg
cs-410_3_1_103,cs-410,3,1, Evaluation of TR Systems,"00:06:59,580","00:07:03,930",103,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=419,we need our queries that are showing here.,pic_cs-410_3_1_360.jpg
cs-410_3_1_104,cs-410,3,1, Evaluation of TR Systems,"00:07:03,930","00:07:05,180",104,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=423,"We have Q1, Q2 etc.",pic_cs-410_3_1_420.jpg
cs-410_3_1_105,cs-410,3,1, Evaluation of TR Systems,"00:07:05,180","00:07:08,300",105,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=425,We also need the documents and,pic_cs-410_3_1_420.jpg
cs-410_3_1_106,cs-410,3,1, Evaluation of TR Systems,"00:07:08,300","00:07:10,580",106,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=428,on the right side you will see,pic_cs-410_3_1_420.jpg
cs-410_3_1_107,cs-410,3,1, Evaluation of TR Systems,"00:07:10,580","00:07:19,150",107,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=430,These are basically the binary judgments,pic_cs-410_3_1_420.jpg
cs-410_3_1_108,cs-410,3,1, Evaluation of TR Systems,"00:07:19,150","00:07:23,790",108,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=439,"So for example,",pic_cs-410_3_1_420.jpg
cs-410_3_1_109,cs-410,3,1, Evaluation of TR Systems,"00:07:23,790","00:07:27,920",109,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=443,"D2 is judged as being relevant as well,",pic_cs-410_3_1_420.jpg
cs-410_3_1_110,cs-410,3,1, Evaluation of TR Systems,"00:07:27,920","00:07:28,980",110,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=447,And the Q1 etc.,pic_cs-410_3_1_420.jpg
cs-410_3_1_111,cs-410,3,1, Evaluation of TR Systems,"00:07:28,980","00:07:32,490",111,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=448,These will be created by users.,pic_cs-410_3_1_420.jpg
cs-410_3_1_112,cs-410,3,1, Evaluation of TR Systems,"00:07:34,190","00:07:38,460",112,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=454,"Once we have these, and",pic_cs-410_3_1_420.jpg
cs-410_3_1_113,cs-410,3,1, Evaluation of TR Systems,"00:07:38,460","00:07:43,560",113,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=458,"And then if you have two systems,",pic_cs-410_3_1_420.jpg
cs-410_3_1_114,cs-410,3,1, Evaluation of TR Systems,"00:07:43,560","00:07:47,260",114,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=463,then you can just run each,pic_cs-410_3_1_420.jpg
cs-410_3_1_115,cs-410,3,1, Evaluation of TR Systems,"00:07:47,260","00:07:50,580",115,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=467,the documents and,pic_cs-410_3_1_420.jpg
cs-410_3_1_116,cs-410,3,1, Evaluation of TR Systems,"00:07:50,580","00:07:56,347",116,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=470,Let's say if the queries Q1 and,pic_cs-410_3_1_420.jpg
cs-410_3_1_117,cs-410,3,1, Evaluation of TR Systems,"00:07:56,347","00:08:02,350",117,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=476,Here I show R sub A as,pic_cs-410_3_1_420.jpg
cs-410_3_1_118,cs-410,3,1, Evaluation of TR Systems,"00:08:02,350","00:08:05,170",118,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=482,"So this is, remember we talked about",pic_cs-410_3_1_480.jpg
cs-410_3_1_119,cs-410,3,1, Evaluation of TR Systems,"00:08:05,170","00:08:09,750",119,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=485,task of computing approximation,pic_cs-410_3_1_480.jpg
cs-410_3_1_120,cs-410,3,1, Evaluation of TR Systems,"00:08:09,750","00:08:13,910",120,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=489,R sub A is system A's approximation here.,pic_cs-410_3_1_480.jpg
cs-410_3_1_121,cs-410,3,1, Evaluation of TR Systems,"00:08:14,980","00:08:20,000",121,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=494,And R sub B is system B's,pic_cs-410_3_1_480.jpg
cs-410_3_1_122,cs-410,3,1, Evaluation of TR Systems,"00:08:21,100","00:08:22,810",122,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=501,"Now, let's take a look at these results.",pic_cs-410_3_1_480.jpg
cs-410_3_1_123,cs-410,3,1, Evaluation of TR Systems,"00:08:22,810","00:08:24,190",123,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=502,So which is better?,pic_cs-410_3_1_480.jpg
cs-410_3_1_124,cs-410,3,1, Evaluation of TR Systems,"00:08:24,190","00:08:26,810",124,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=504,"Now imagine if a user,",pic_cs-410_3_1_480.jpg
cs-410_3_1_125,cs-410,3,1, Evaluation of TR Systems,"00:08:26,810","00:08:31,145",125,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=506,Now let's take a look at the both results.,pic_cs-410_3_1_480.jpg
cs-410_3_1_126,cs-410,3,1, Evaluation of TR Systems,"00:08:31,145","00:08:33,270",126,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=511,And there are some differences and,pic_cs-410_3_1_480.jpg
cs-410_3_1_127,cs-410,3,1, Evaluation of TR Systems,"00:08:33,270","00:08:40,160",127,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=513,there are some documents that,pic_cs-410_3_1_480.jpg
cs-410_3_1_128,cs-410,3,1, Evaluation of TR Systems,"00:08:40,160","00:08:44,200",128,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=520,"But if you look at the results,",pic_cs-410_3_1_480.jpg
cs-410_3_1_129,cs-410,3,1, Evaluation of TR Systems,"00:08:44,200","00:08:48,640",129,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=524,A is better in the sense that we don't,pic_cs-410_3_1_480.jpg
cs-410_3_1_130,cs-410,3,1, Evaluation of TR Systems,"00:08:48,640","00:08:52,260",130,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=528,"And among the three documents returned,",pic_cs-410_3_1_480.jpg
cs-410_3_1_131,cs-410,3,1, Evaluation of TR Systems,"00:08:52,260","00:08:55,430",131,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=532,"So that's good, it's precise.",pic_cs-410_3_1_480.jpg
cs-410_3_1_132,cs-410,3,1, Evaluation of TR Systems,"00:08:55,430","00:08:58,770",132,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=535,On the other hand one council,pic_cs-410_3_1_480.jpg
cs-410_3_1_133,cs-410,3,1, Evaluation of TR Systems,"00:08:58,770","00:09:01,280",133,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=538,because we've got all of,pic_cs-410_3_1_480.jpg
cs-410_3_1_134,cs-410,3,1, Evaluation of TR Systems,"00:09:01,280","00:09:03,500",134,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=541,We've got three instead of two.,pic_cs-410_3_1_540.jpg
cs-410_3_1_135,cs-410,3,1, Evaluation of TR Systems,"00:09:03,500","00:09:06,690",135,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=543,So which one is better and,pic_cs-410_3_1_540.jpg
cs-410_3_1_136,cs-410,3,1, Evaluation of TR Systems,"00:09:08,820","00:09:12,670",136,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=548,"Well, obviously this question",pic_cs-410_3_1_540.jpg
cs-410_3_1_137,cs-410,3,1, Evaluation of TR Systems,"00:09:12,670","00:09:14,820",137,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=552,It depends on users as well.,pic_cs-410_3_1_540.jpg
cs-410_3_1_138,cs-410,3,1, Evaluation of TR Systems,"00:09:14,820","00:09:19,950",138,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=554,You might even imagine for,pic_cs-410_3_1_540.jpg
cs-410_3_1_139,cs-410,3,1, Evaluation of TR Systems,"00:09:19,950","00:09:23,747",139,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=559,If the user is not interested in,pic_cs-410_3_1_540.jpg
cs-410_3_1_140,cs-410,3,1, Evaluation of TR Systems,"00:09:23,747","00:09:28,582",140,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=563,"Right, in this case the user doesn't",pic_cs-410_3_1_540.jpg
cs-410_3_1_141,cs-410,3,1, Evaluation of TR Systems,"00:09:28,582","00:09:31,020",141,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=568,see most of the relevant documents.,pic_cs-410_3_1_540.jpg
cs-410_3_1_142,cs-410,3,1, Evaluation of TR Systems,"00:09:31,020","00:09:34,230",142,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=571,"On the other hand,",pic_cs-410_3_1_540.jpg
cs-410_3_1_143,cs-410,3,1, Evaluation of TR Systems,"00:09:34,230","00:09:37,130",143,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=574,to have as many random,pic_cs-410_3_1_540.jpg
cs-410_3_1_144,cs-410,3,1, Evaluation of TR Systems,"00:09:37,130","00:09:41,617",144,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=577,"For example, if you're doing a literature",pic_cs-410_3_1_540.jpg
cs-410_3_1_145,cs-410,3,1, Evaluation of TR Systems,"00:09:41,617","00:09:43,844",145,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=581,and you might find that,pic_cs-410_3_1_540.jpg
cs-410_3_1_146,cs-410,3,1, Evaluation of TR Systems,"00:09:43,844","00:09:48,985",146,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=583,"So in the case, we will have to also",pic_cs-410_3_1_540.jpg
cs-410_3_1_147,cs-410,3,1, Evaluation of TR Systems,"00:09:48,985","00:09:53,408",147,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=588,And we might need it to define multiple,pic_cs-410_3_1_540.jpg
cs-410_3_1_148,cs-410,3,1, Evaluation of TR Systems,"00:09:53,408","00:09:55,798",148,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=593,perspectives of looking at the results.,pic_cs-410_3_1_540.jpg
cs-410_3_1_149,cs-410,3,1, Evaluation of TR Systems,"00:09:58,259","00:10:08,259",149,https://www.coursera.org/learn/cs-410/lecture/YSvkh?t=598,[MUSIC],pic_cs-410_3_1_540.jpg
cs-410_3_2_1,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:00,012","00:00:06,908",1,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=0,[SOUND],pic_cs-410_3_2_0.jpg
cs-410_3_2_2,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:06,908","00:00:13,498",2,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=6,lecture is about the basic measures for,pic_cs-410_3_2_0.jpg
cs-410_3_2_3,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:13,498","00:00:18,955",3,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=13,"In this lecture,",pic_cs-410_3_2_0.jpg
cs-410_3_2_4,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:18,955","00:00:24,528",4,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=18,measures to quantitatively,pic_cs-410_3_2_0.jpg
cs-410_3_2_5,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:24,528","00:00:29,163",5,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=24,This is a slide that you have seen,pic_cs-410_3_2_0.jpg
cs-410_3_2_6,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:29,163","00:00:32,318",6,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=29,about the Granville,pic_cs-410_3_2_0.jpg
cs-410_3_2_7,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:32,318","00:00:39,122",7,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=32,We can have a test faction that consists,pic_cs-410_3_2_0.jpg
cs-410_3_2_8,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:39,122","00:00:47,930",8,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=39,We can then run two systems on these,pic_cs-410_3_2_0.jpg
cs-410_3_2_9,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:47,930","00:00:49,528",9,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=47,Their performance.,pic_cs-410_3_2_0.jpg
cs-410_3_2_10,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:49,528","00:00:54,828",10,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=49,"And we raise the question,",pic_cs-410_3_2_0.jpg
cs-410_3_2_11,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:54,828","00:00:57,928",11,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=54,Is system A better or is system B better?,pic_cs-410_3_2_0.jpg
cs-410_3_2_12,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:00:57,928","00:01:02,398",12,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=57,So let's now talk about how to,pic_cs-410_3_2_0.jpg
cs-410_3_2_13,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:02,398","00:01:07,841",13,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=62,Suppose we have a total of 10 relevant,pic_cs-410_3_2_60.jpg
cs-410_3_2_14,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:07,841","00:01:08,848",14,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=67,this query.,pic_cs-410_3_2_60.jpg
cs-410_3_2_15,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:08,848","00:01:15,162",15,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=68,"Now, the relevant judgments show on",pic_cs-410_3_2_60.jpg
cs-410_3_2_16,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:15,162","00:01:19,908",16,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=75,"And we have only seen 3 [INAUDIBLE] there,",pic_cs-410_3_2_60.jpg
cs-410_3_2_17,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:19,908","00:01:26,133",17,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=79,"But, we can imagine there are other Random",pic_cs-410_3_2_60.jpg
cs-410_3_2_18,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:26,133","00:01:30,895",18,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=86,"So now, intuitively,",pic_cs-410_3_2_60.jpg
cs-410_3_2_19,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:30,895","00:01:35,668",19,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=90,A is better because it,pic_cs-410_3_2_60.jpg
cs-410_3_2_20,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:35,668","00:01:42,019",20,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=95,And in particular we have seen,pic_cs-410_3_2_60.jpg
cs-410_3_2_21,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:42,019","00:01:46,251",21,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=102,"two of them are relevant but in system B,",pic_cs-410_3_2_60.jpg
cs-410_3_2_22,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:46,251","00:01:52,248",22,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=106,we have five results and,pic_cs-410_3_2_60.jpg
cs-410_3_2_23,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:52,248","00:01:56,418",23,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=112,So intuitively it looks like,pic_cs-410_3_2_60.jpg
cs-410_3_2_24,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:01:56,418","00:02:00,670",24,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=116,And this infusion can be captured,pic_cs-410_3_2_60.jpg
cs-410_3_2_25,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:00,670","00:02:05,866",25,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=120,where we simply compute to what extent,pic_cs-410_3_2_120.jpg
cs-410_3_2_26,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:05,866","00:02:07,788",26,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=125,"If you have 100% position,",pic_cs-410_3_2_120.jpg
cs-410_3_2_27,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:07,788","00:02:11,638",27,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=127,that would mean that all,pic_cs-410_3_2_120.jpg
cs-410_3_2_28,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:11,638","00:02:16,476",28,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=131,So in this case system A has,pic_cs-410_3_2_120.jpg
cs-410_3_2_29,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:16,476","00:02:20,606",29,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=136,three System B has some,pic_cs-410_3_2_120.jpg
cs-410_3_2_30,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:20,606","00:02:25,208",30,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=140,this shows that system,pic_cs-410_3_2_120.jpg
cs-410_3_2_31,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:25,208","00:02:30,065",31,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=145,But we also talked about System B,pic_cs-410_3_2_120.jpg
cs-410_3_2_32,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:30,065","00:02:35,220",32,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=150,would like to retrieve as many,pic_cs-410_3_2_120.jpg
cs-410_3_2_33,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:35,220","00:02:39,839",33,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=155,So in that case we'll have to compare,pic_cs-410_3_2_120.jpg
cs-410_3_2_34,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:39,839","00:02:42,940",34,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=159,retrieve and,pic_cs-410_3_2_120.jpg
cs-410_3_2_35,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:42,940","00:02:48,000",35,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=162,This method uses the completeness,pic_cs-410_3_2_120.jpg
cs-410_3_2_36,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:48,000","00:02:51,090",36,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=168,In your retrieval result.,pic_cs-410_3_2_120.jpg
cs-410_3_2_37,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:51,090","00:02:57,500",37,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=171,So we just assume that there are ten,pic_cs-410_3_2_120.jpg
cs-410_3_2_38,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:02:57,500","00:03:01,510",38,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=177,"And here we've got two of them,",pic_cs-410_3_2_120.jpg
cs-410_3_2_39,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:01,510","00:03:04,130",39,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=181,So the recall is 2 out of 10.,pic_cs-410_3_2_180.jpg
cs-410_3_2_40,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:04,130","00:03:07,630",40,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=184,"Whereas System B has called a 3,",pic_cs-410_3_2_180.jpg
cs-410_3_2_41,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:07,630","00:03:11,278",41,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=187,Now we can see by recall,pic_cs-410_3_2_180.jpg
cs-410_3_2_42,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:11,278","00:03:15,240",42,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=191,And these two measures turn out to,pic_cs-410_3_2_180.jpg
cs-410_3_2_43,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:15,240","00:03:16,978",43,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=195,evaluating search engine.,pic_cs-410_3_2_180.jpg
cs-410_3_2_44,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:16,978","00:03:21,824",44,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=196,And they are very important because,pic_cs-410_3_2_180.jpg
cs-410_3_2_45,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:21,824","00:03:24,298",45,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=201,other test evaluation problems.,pic_cs-410_3_2_180.jpg
cs-410_3_2_46,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:24,298","00:03:28,660",46,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=204,"For example, if you look at",pic_cs-410_3_2_180.jpg
cs-410_3_2_47,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:28,660","00:03:34,030",47,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=208,you tend to see precision recall numbers,pic_cs-410_3_2_180.jpg
cs-410_3_2_48,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:35,290","00:03:38,520",48,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=215,"Okay so, now let's define these",pic_cs-410_3_2_180.jpg
cs-410_3_2_49,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:38,520","00:03:44,410",49,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=218,And these measures are to evaluate a set,pic_cs-410_3_2_180.jpg
cs-410_3_2_50,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:44,410","00:03:48,950",50,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=224,we are considering that approximation,pic_cs-410_3_2_180.jpg
cs-410_3_2_51,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:50,100","00:03:53,300",51,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=230,We can distinguish 4 cases depending,pic_cs-410_3_2_180.jpg
cs-410_3_2_52,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:53,300","00:03:59,720",52,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=233,A document can be retrieved or,pic_cs-410_3_2_180.jpg
cs-410_3_2_53,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:03:59,720","00:04:01,570",53,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=239,Because we are talking,pic_cs-410_3_2_180.jpg
cs-410_3_2_54,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:02,710","00:04:05,640",54,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=242,A document can be also relevant or,pic_cs-410_3_2_240.jpg
cs-410_3_2_55,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:05,640","00:04:10,310",55,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=245,not relevant depending on whether the user,pic_cs-410_3_2_240.jpg
cs-410_3_2_56,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:11,950","00:04:16,890",56,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=251,So we can now have counts of documents in.,pic_cs-410_3_2_240.jpg
cs-410_3_2_57,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:16,890","00:04:21,610",57,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=256,Each of the four categories again,pic_cs-410_3_2_240.jpg
cs-410_3_2_58,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:21,610","00:04:24,420",58,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=261,documents that have been retrieved and,pic_cs-410_3_2_240.jpg
cs-410_3_2_59,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:24,420","00:04:30,530",59,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=264,B for documents that are not retrieved but,pic_cs-410_3_2_240.jpg
cs-410_3_2_60,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:31,750","00:04:35,550",60,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=271,No with this table then,pic_cs-410_3_2_240.jpg
cs-410_3_2_61,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:36,690","00:04:42,450",61,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=276,As the ratio of the relevant,pic_cs-410_3_2_240.jpg
cs-410_3_2_62,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:42,450","00:04:47,440",62,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=282,retrieved documents A to the total,pic_cs-410_3_2_240.jpg
cs-410_3_2_63,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:48,450","00:04:53,390",63,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=288,"So, this is just A divided",pic_cs-410_3_2_240.jpg
cs-410_3_2_64,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:53,390","00:04:55,640",64,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=293,The sum of this column.,pic_cs-410_3_2_240.jpg
cs-410_3_2_65,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:04:56,820","00:05:04,360",65,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=296,Singularly recall is defined by,pic_cs-410_3_2_240.jpg
cs-410_3_2_66,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:04,360","00:05:07,470",66,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=304,So that's again to divide a by.,pic_cs-410_3_2_300.jpg
cs-410_3_2_67,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:07,470","00:05:10,360",67,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=307,The sum of the row instead of the column.,pic_cs-410_3_2_300.jpg
cs-410_3_2_68,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:10,360","00:05:15,810",68,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=310,"All right, so we can see precision and",pic_cs-410_3_2_300.jpg
cs-410_3_2_69,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:16,930","00:05:20,000",69,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=316,that's the number of,pic_cs-410_3_2_300.jpg
cs-410_3_2_70,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:20,000","00:05:22,449",70,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=320,But we're going to use,pic_cs-410_3_2_300.jpg
cs-410_3_2_71,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:23,590","00:05:27,300",71,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=323,"Okay, so what would be an ideal result.",pic_cs-410_3_2_300.jpg
cs-410_3_2_72,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:27,300","00:05:31,330",72,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=327,"Well, you can easily see being",pic_cs-410_3_2_300.jpg
cs-410_3_2_73,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:31,330","00:05:34,060",73,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=331,recall oil to be 1.0.,pic_cs-410_3_2_300.jpg
cs-410_3_2_74,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:34,060","00:05:39,510",74,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=334,That means We have got 1% of,pic_cs-410_3_2_300.jpg
cs-410_3_2_75,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:39,510","00:05:44,770",75,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=339,"in our results, and all of the results",pic_cs-410_3_2_300.jpg
cs-410_3_2_76,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:44,770","00:05:47,540",76,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=344,At least there's no single,pic_cs-410_3_2_300.jpg
cs-410_3_2_77,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:48,680","00:05:53,920",77,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=348,"In reality, however, high recall tends",pic_cs-410_3_2_300.jpg
cs-410_3_2_78,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:53,920","00:05:56,210",78,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=353,And you can imagine why that's the case.,pic_cs-410_3_2_300.jpg
cs-410_3_2_79,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:05:56,210","00:06:00,790",79,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=356,As you go down the to try to get as,pic_cs-410_3_2_300.jpg
cs-410_3_2_80,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:00,790","00:06:05,890",80,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=360,"you tend to encounter a lot of documents,",pic_cs-410_3_2_360.jpg
cs-410_3_2_81,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:05,890","00:06:11,450",81,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=365,Note that this set can also,pic_cs-410_3_2_360.jpg
cs-410_3_2_82,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:11,450","00:06:15,490",82,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=371,"In the rest of this, that's why although",pic_cs-410_3_2_360.jpg
cs-410_3_2_83,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:15,490","00:06:20,560",83,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=375,"retrieve the documents, they are actually",pic_cs-410_3_2_360.jpg
cs-410_3_2_84,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:20,560","00:06:24,270",84,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=380,They are the fundamental measures in,pic_cs-410_3_2_360.jpg
cs-410_3_2_85,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:24,270","00:06:30,010",85,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=384,We often are interested in The precision,pic_cs-410_3_2_360.jpg
cs-410_3_2_86,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:30,010","00:06:33,400",86,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=390,This means we look at how many documents,pic_cs-410_3_2_360.jpg
cs-410_3_2_87,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:33,400","00:06:35,870",87,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=393,among the top ten results,pic_cs-410_3_2_360.jpg
cs-410_3_2_88,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:35,870","00:06:38,290",88,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=395,"Now, this is a very meaningful measure,",pic_cs-410_3_2_360.jpg
cs-410_3_2_89,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:38,290","00:06:43,780",89,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=398,because it tells us how many relevant,pic_cs-410_3_2_360.jpg
cs-410_3_2_90,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:43,780","00:06:47,790",90,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=403,On the first page of where they,pic_cs-410_3_2_360.jpg
cs-410_3_2_91,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:50,000","00:06:55,780",91,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=410,So precision and recall,pic_cs-410_3_2_360.jpg
cs-410_3_2_92,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:06:55,780","00:07:02,040",92,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=415,use them to further evaluate a search,pic_cs-410_3_2_360.jpg
cs-410_3_2_93,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:03,460","00:07:07,210",93,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=423,We just said that there tends to be,pic_cs-410_3_2_420.jpg
cs-410_3_2_94,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:07,210","00:07:10,730",94,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=427,so naturally it would be,pic_cs-410_3_2_420.jpg
cs-410_3_2_95,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:10,730","00:07:15,490",95,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=430,"And here's one method that's often used,",pic_cs-410_3_2_420.jpg
cs-410_3_2_96,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:15,490","00:07:21,020",96,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=435,it's a [INAUDIBLE] mean of precision and,pic_cs-410_3_2_420.jpg
cs-410_3_2_97,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:22,450","00:07:27,741",97,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=442,"So, you can see at first, compute the.",pic_cs-410_3_2_420.jpg
cs-410_3_2_98,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:29,210","00:07:34,360",98,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=449,"Inverse of R and P here,",pic_cs-410_3_2_420.jpg
cs-410_3_2_99,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:34,360","00:07:41,180",99,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=454,the 2 by using coefficients,pic_cs-410_3_2_420.jpg
cs-410_3_2_100,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:42,850","00:07:47,029",100,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=462,And after some transformation you can,pic_cs-410_3_2_420.jpg
cs-410_3_2_101,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:49,010","00:07:51,790",101,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=469,And in any case it just becomes,pic_cs-410_3_2_420.jpg
cs-410_3_2_102,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:51,790","00:07:56,360",102,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=471,"recall, and beta is a parameter,",pic_cs-410_3_2_420.jpg
cs-410_3_2_103,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:07:56,360","00:08:01,572",103,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=476,It can control the emphasis,pic_cs-410_3_2_420.jpg
cs-410_3_2_104,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:01,572","00:08:08,360",104,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=481,set beta to 1 We end up having a special,pic_cs-410_3_2_480.jpg
cs-410_3_2_105,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:08,360","00:08:13,460",105,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=488,This is a popular measure that's often,pic_cs-410_3_2_480.jpg
cs-410_3_2_106,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:13,460","00:08:14,780",106,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=493,And the formula looks very simple.,pic_cs-410_3_2_480.jpg
cs-410_3_2_107,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:16,170","00:08:17,948",107,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=496,"It's just this, here.",pic_cs-410_3_2_480.jpg
cs-410_3_2_108,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:20,718","00:08:24,668",108,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=500,Now it's easy to see that if,pic_cs-410_3_2_480.jpg
cs-410_3_2_109,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:24,668","00:08:28,570",109,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=504,larger recall than f,pic_cs-410_3_2_480.jpg
cs-410_3_2_110,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:28,570","00:08:32,940",110,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=508,"But, what's interesting is that",pic_cs-410_3_2_480.jpg
cs-410_3_2_111,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:32,940","00:08:36,260",111,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=512,recall is captured,pic_cs-410_3_2_480.jpg
cs-410_3_2_112,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:36,260","00:08:41,000",112,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=516,"So, in order to understand that, we",pic_cs-410_3_2_480.jpg
cs-410_3_2_113,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:42,170","00:08:48,270",113,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=522,can first look at the natural,pic_cs-410_3_2_480.jpg
cs-410_3_2_114,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:48,270","00:08:53,090",114,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=528,using the symbol arithmetically,pic_cs-410_3_2_480.jpg
cs-410_3_2_115,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:08:53,090","00:09:00,730",115,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=533,That would be likely the most natural way,pic_cs-410_3_2_480.jpg
cs-410_3_2_116,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:01,870","00:09:05,940",116,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=541,"If you want to think more,",pic_cs-410_3_2_540.jpg
cs-410_3_2_117,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:07,940","00:09:10,960",117,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=547,So why is this not as good as F1?,pic_cs-410_3_2_540.jpg
cs-410_3_2_118,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:13,550","00:09:15,038",118,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=553,Or what's the problem with this?,pic_cs-410_3_2_540.jpg
cs-410_3_2_119,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:18,121","00:09:23,270",119,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=558,"Now, if you think about",pic_cs-410_3_2_540.jpg
cs-410_3_2_120,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:23,270","00:09:28,300",120,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=563,you can see this is,pic_cs-410_3_2_540.jpg
cs-410_3_2_121,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:28,300","00:09:31,870",121,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=568,"In this case,",pic_cs-410_3_2_540.jpg
cs-410_3_2_122,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:31,870","00:09:36,580",122,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=571,"In the case of a sum, the total value",pic_cs-410_3_2_540.jpg
cs-410_3_2_123,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:36,580","00:09:42,850",123,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=576,that means if you have a very high P or,pic_cs-410_3_2_540.jpg
cs-410_3_2_124,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:42,850","00:09:47,820",124,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=582,don't care about whether the other value,pic_cs-410_3_2_540.jpg
cs-410_3_2_125,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:47,820","00:09:53,920",125,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=587,Now this is not desirable because one,pic_cs-410_3_2_540.jpg
cs-410_3_2_126,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:53,920","00:09:57,110",126,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=593,We have perfect recall easily.,pic_cs-410_3_2_540.jpg
cs-410_3_2_127,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:57,110","00:09:58,140",127,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=597,Can we imagine how?,pic_cs-410_3_2_540.jpg
cs-410_3_2_128,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:09:59,810","00:10:03,830",128,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=599,It's probably very easy to,pic_cs-410_3_2_540.jpg
cs-410_3_2_129,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:03,830","00:10:06,399",129,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=603,all the documents in the collection and,pic_cs-410_3_2_600.jpg
cs-410_3_2_130,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:07,420","00:10:11,130",130,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=607,And this will give us 0.5 as the average.,pic_cs-410_3_2_600.jpg
cs-410_3_2_131,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:11,130","00:10:15,583",131,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=611,But such results are clearly not,pic_cs-410_3_2_600.jpg
cs-410_3_2_132,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:15,583","00:10:20,350",132,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=615,though the average using this,pic_cs-410_3_2_600.jpg
cs-410_3_2_133,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:21,750","00:10:25,930",133,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=621,In contrast you can see F 1 would,pic_cs-410_3_2_600.jpg
cs-410_3_2_134,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:25,930","00:10:27,750",134,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=625,"recall are roughly That seminar, so",pic_cs-410_3_2_600.jpg
cs-410_3_2_135,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:27,750","00:10:33,360",135,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=627,it would a case where you had,pic_cs-410_3_2_600.jpg
cs-410_3_2_136,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:35,320","00:10:38,360",136,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=635,So this means f one encodes,pic_cs-410_3_2_600.jpg
cs-410_3_2_137,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:38,360","00:10:43,690",137,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=638,Now this example shows,pic_cs-410_3_2_600.jpg
cs-410_3_2_138,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:43,690","00:10:44,230",138,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=643,Methodology here.,pic_cs-410_3_2_600.jpg
cs-410_3_2_139,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:44,230","00:10:49,950",139,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=644,But when you try to solve a problem you,pic_cs-410_3_2_600.jpg
cs-410_3_2_140,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:49,950","00:10:52,120",140,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=649,let's say in this it's,pic_cs-410_3_2_600.jpg
cs-410_3_2_141,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:53,790","00:10:57,160",141,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=653,But it's important not to,pic_cs-410_3_2_600.jpg
cs-410_3_2_142,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:10:57,160","00:11:00,790",142,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=657,It's important to think whether you,pic_cs-410_3_2_600.jpg
cs-410_3_2_143,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:02,170","00:11:06,180",143,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=662,And once you think about the multiple,pic_cs-410_3_2_660.jpg
cs-410_3_2_144,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:06,180","00:11:10,930",144,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=666,"difference, and then think about",pic_cs-410_3_2_660.jpg
cs-410_3_2_145,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:10,930","00:11:13,280",145,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=670,"In this case, if you think more carefully,",pic_cs-410_3_2_660.jpg
cs-410_3_2_146,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:13,280","00:11:15,920",146,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=673,you will think that F1,pic_cs-410_3_2_660.jpg
cs-410_3_2_147,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:15,920","00:11:18,300",147,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=675,Than the simple.,pic_cs-410_3_2_660.jpg
cs-410_3_2_148,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:18,300","00:11:21,670",148,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=678,Although in other cases there,pic_cs-410_3_2_660.jpg
cs-410_3_2_149,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:21,670","00:11:25,858",149,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=681,But in this case the seems not reasonable.,pic_cs-410_3_2_660.jpg
cs-410_3_2_150,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:25,858","00:11:29,260",150,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=685,But if you don't pay attention,pic_cs-410_3_2_660.jpg
cs-410_3_2_151,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:29,260","00:11:33,780",151,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=689,you might just take a easy way to,pic_cs-410_3_2_660.jpg
cs-410_3_2_152,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:33,780","00:11:37,360",152,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=693,"And here later, you will find that,",pic_cs-410_3_2_660.jpg
cs-410_3_2_153,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:37,360","00:11:38,620",153,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=697,All right.,pic_cs-410_3_2_660.jpg
cs-410_3_2_154,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:38,620","00:11:43,760",154,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=698,So this methodology is actually very,pic_cs-410_3_2_660.jpg
cs-410_3_2_155,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:43,760","00:11:46,020",155,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=703,Try to think about the best solution.,pic_cs-410_3_2_660.jpg
cs-410_3_2_156,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:46,020","00:11:50,890",156,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=706,"Try to understand the problem very well,",pic_cs-410_3_2_660.jpg
cs-410_3_2_157,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:50,890","00:11:55,890",157,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=710,"know why you needed this measure, and why",pic_cs-410_3_2_660.jpg
cs-410_3_2_158,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:11:55,890","00:11:59,530",158,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=715,And then use that to guide you in,pic_cs-410_3_2_660.jpg
cs-410_3_2_159,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:03,320","00:12:08,510",159,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=723,"To summarize, we talked about",pic_cs-410_3_2_720.jpg
cs-410_3_2_160,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:08,510","00:12:11,530",160,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=728,are there retrievable,pic_cs-410_3_2_720.jpg
cs-410_3_2_161,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:11,530","00:12:13,690",161,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=731,We also talk about the Recall.,pic_cs-410_3_2_720.jpg
cs-410_3_2_162,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:13,690","00:12:17,260",162,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=733,"Which addresses the question, have all of",pic_cs-410_3_2_720.jpg
cs-410_3_2_163,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:17,260","00:12:21,250",163,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=737,"These two, are the two,",pic_cs-410_3_2_720.jpg
cs-410_3_2_164,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:21,250","00:12:25,270",164,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=741,They are used for,pic_cs-410_3_2_720.jpg
cs-410_3_2_165,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:25,270","00:12:28,670",165,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=745,We talk about F measure as a way to,pic_cs-410_3_2_720.jpg
cs-410_3_2_166,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:29,970","00:12:33,600",166,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=749,We also talked about the tradeoff,pic_cs-410_3_2_720.jpg
cs-410_3_2_167,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:33,600","00:12:38,140",167,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=753,And this turns out to depend,pic_cs-410_3_2_720.jpg
cs-410_3_2_168,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:38,140","00:12:42,133",168,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=758,we'll discuss this point,pic_cs-410_3_2_720.jpg
cs-410_3_2_169,cs-410,3,2, Evaluation of TR Systems - Basic Measures,"00:12:42,133","00:12:52,133",169,https://www.coursera.org/learn/cs-410/lecture/VMh3Z?t=762,[MUSIC],pic_cs-410_3_2_720.jpg
cs-410_3_3_1,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:00,199","00:00:03,699",1,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=0,[MUSIC],pic_cs-410_3_3_0.jpg
cs-410_3_3_2,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:07,099","00:00:11,070",2,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=7,"This lecture is about,",pic_cs-410_3_3_0.jpg
cs-410_3_3_3,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:13,290","00:00:17,810",3,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=13,"In this lecture, we will continue",pic_cs-410_3_3_0.jpg
cs-410_3_3_4,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:17,810","00:00:18,470",4,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=17,"In particular,",pic_cs-410_3_3_0.jpg
cs-410_3_3_5,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:18,470","00:00:21,799",5,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=18,"we are going to look at, how we can",pic_cs-410_3_3_0.jpg
cs-410_3_3_6,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:24,970","00:00:30,410",6,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=24,"In the previous lecture,",pic_cs-410_3_3_0.jpg
cs-410_3_3_7,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:30,410","00:00:33,430",7,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=30,"These are the two basic measures for,",pic_cs-410_3_3_0.jpg
cs-410_3_3_8,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:33,430","00:00:38,410",8,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=33,quantitatively measuring,pic_cs-410_3_3_0.jpg
cs-410_3_3_9,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:40,420","00:00:44,247",9,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=40,"But, as we talked about, ranking, before,",pic_cs-410_3_3_0.jpg
cs-410_3_3_10,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:44,247","00:00:49,420",10,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=44,we framed that the text of retrieval,pic_cs-410_3_3_0.jpg
cs-410_3_3_11,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:50,800","00:00:55,820",11,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=50,"So, we also need to evaluate the,",pic_cs-410_3_3_0.jpg
cs-410_3_3_12,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:00:56,910","00:01:01,097",12,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=56,How can we use precision-recall,pic_cs-410_3_3_0.jpg
cs-410_3_3_13,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:01,097","00:01:07,180",13,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=61,"Well, naturally, we have to look after the",pic_cs-410_3_3_60.jpg
cs-410_3_3_14,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:07,180","00:01:12,330",14,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=67,"Because in the end, the approximation",pic_cs-410_3_3_60.jpg
cs-410_3_3_15,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:12,330","00:01:17,640",15,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=72,"given by a ranked list, is determined",pic_cs-410_3_3_60.jpg
cs-410_3_3_16,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:17,640","00:01:21,470",16,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=77,Right?,pic_cs-410_3_3_60.jpg
cs-410_3_3_17,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:21,470","00:01:25,520",17,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=81,"the list of results, the user would,",pic_cs-410_3_3_60.jpg
cs-410_3_3_18,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:25,520","00:01:27,790",18,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=85,that point would determine the set.,pic_cs-410_3_3_60.jpg
cs-410_3_3_19,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:27,790","00:01:31,680",19,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=87,"And then,",pic_cs-410_3_3_60.jpg
cs-410_3_3_20,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:31,680","00:01:35,400",20,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=91,"that we have to consider,",pic_cs-410_3_3_60.jpg
cs-410_3_3_21,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:35,400","00:01:37,990",21,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=95,Without knowing where,pic_cs-410_3_3_60.jpg
cs-410_3_3_22,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:37,990","00:01:42,380",22,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=97,"then we have to consider, all",pic_cs-410_3_3_60.jpg
cs-410_3_3_23,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:42,380","00:01:44,720",23,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=102,"So, let's look at these positions.",pic_cs-410_3_3_60.jpg
cs-410_3_3_24,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:44,720","00:01:49,020",24,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=104,"Look at this slide, and",pic_cs-410_3_3_60.jpg
cs-410_3_3_25,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:49,020","00:01:51,718",25,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=109,"what if the user stops at the,",pic_cs-410_3_3_60.jpg
cs-410_3_3_26,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:51,718","00:01:55,140",26,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=111,What's the precision-recall at this point?,pic_cs-410_3_3_60.jpg
cs-410_3_3_27,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:55,140","00:01:55,800",27,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=115,What do you think?,pic_cs-410_3_3_60.jpg
cs-410_3_3_28,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:01:56,970","00:02:02,920",28,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=116,"Well, it's easy to see, that this document",pic_cs-410_3_3_60.jpg
cs-410_3_3_29,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:02,920","00:02:05,960",29,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=122,"We have, got one document,",pic_cs-410_3_3_120.jpg
cs-410_3_3_30,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:05,960","00:02:07,380",30,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=125,What about the recall?,pic_cs-410_3_3_120.jpg
cs-410_3_3_31,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:07,380","00:02:11,990",31,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=127,"Well, note that, we're assuming that,",pic_cs-410_3_3_120.jpg
cs-410_3_3_32,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:11,990","00:02:14,980",32,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=131,"this query in the collection,",pic_cs-410_3_3_120.jpg
cs-410_3_3_33,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:16,310","00:02:18,650",33,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=136,What if the user stops,pic_cs-410_3_3_120.jpg
cs-410_3_3_34,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:19,820","00:02:20,320",34,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=139,Top two.,pic_cs-410_3_3_120.jpg
cs-410_3_3_35,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:21,470","00:02:25,820",35,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=141,"Well, the precision is the same,",pic_cs-410_3_3_120.jpg
cs-410_3_3_36,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:25,820","00:02:27,060",36,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=145,"And, the record is two out of ten.",pic_cs-410_3_3_120.jpg
cs-410_3_3_37,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:28,600","00:02:31,630",37,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=148,What if the user stops,pic_cs-410_3_3_120.jpg
cs-410_3_3_38,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:31,630","00:02:35,980",38,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=151,"Well, this is interesting,",pic_cs-410_3_3_120.jpg
cs-410_3_3_39,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:35,980","00:02:40,030",39,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=155,"additional relevant document,",pic_cs-410_3_3_120.jpg
cs-410_3_3_40,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:41,170","00:02:45,600",40,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=161,"But the precision is lower,",pic_cs-410_3_3_120.jpg
cs-410_3_3_41,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:45,600","00:02:46,680",41,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=165,what's exactly the precision?,pic_cs-410_3_3_120.jpg
cs-410_3_3_42,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:49,110","00:02:52,020",42,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=169,"Well, it's two out of three, right?",pic_cs-410_3_3_120.jpg
cs-410_3_3_43,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:52,020","00:02:54,920",43,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=172,"And, recall is the same, two out of ten.",pic_cs-410_3_3_120.jpg
cs-410_3_3_44,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:54,920","00:02:58,930",44,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=174,"So, when would see another point,",pic_cs-410_3_3_120.jpg
cs-410_3_3_45,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:02:58,930","00:03:02,473",45,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=178,"Now, if you look down the list,",pic_cs-410_3_3_120.jpg
cs-410_3_3_46,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:02,473","00:03:06,110",46,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=182,"we have, seeing another relevant document.",pic_cs-410_3_3_180.jpg
cs-410_3_3_47,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:06,110","00:03:10,800",47,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=186,"In this case D5, at that point, the,",pic_cs-410_3_3_180.jpg
cs-410_3_3_48,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:10,800","00:03:13,840",48,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=190,"three out of ten, and,",pic_cs-410_3_3_180.jpg
cs-410_3_3_49,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:15,150","00:03:20,200",49,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=195,"So, you can see, if we keep doing this,",pic_cs-410_3_3_180.jpg
cs-410_3_3_50,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:20,200","00:03:23,780",50,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=200,"And then, we will have",pic_cs-410_3_3_180.jpg
cs-410_3_3_51,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:23,780","00:03:26,150",51,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=203,"because there are eight documents,",pic_cs-410_3_3_180.jpg
cs-410_3_3_52,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:26,150","00:03:28,200",52,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=206,"And, the recall is a four out of ten.",pic_cs-410_3_3_180.jpg
cs-410_3_3_53,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:29,540","00:03:33,500",53,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=209,"Now, when can we get,",pic_cs-410_3_3_180.jpg
cs-410_3_3_54,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:33,500","00:03:39,740",54,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=213,"Well, in this list, we don't have it,",pic_cs-410_3_3_180.jpg
cs-410_3_3_55,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:39,740","00:03:40,560",55,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=219,"We don't know, where it is?",pic_cs-410_3_3_180.jpg
cs-410_3_3_56,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:40,560","00:03:45,700",56,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=220,"But, as convenience, we often assume that,",pic_cs-410_3_3_180.jpg
cs-410_3_3_57,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:47,230","00:03:51,890",57,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=227,"at all the, the othe,",pic_cs-410_3_3_180.jpg
cs-410_3_3_58,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:51,890","00:03:56,550",58,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=231,"all the other levels of recall,",pic_cs-410_3_3_180.jpg
cs-410_3_3_59,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:56,550","00:03:59,140",59,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=236,"So, of course,",pic_cs-410_3_3_180.jpg
cs-410_3_3_60,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:03:59,140","00:04:04,040",60,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=239,"the actual position would be higher,",pic_cs-410_3_3_180.jpg
cs-410_3_3_61,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:05,230","00:04:09,390",61,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=245,"in order to, have an easy way to,",pic_cs-410_3_3_240.jpg
cs-410_3_3_62,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:09,390","00:04:12,980",62,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=249,compute another measure called Average,pic_cs-410_3_3_240.jpg
cs-410_3_3_63,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:14,300","00:04:16,560",63,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=254,"Now, I should also say, now, here you see,",pic_cs-410_3_3_240.jpg
cs-410_3_3_64,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:16,560","00:04:21,230",64,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=256,we make these assumptions that,pic_cs-410_3_3_240.jpg
cs-410_3_3_65,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:22,270","00:04:28,950",65,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=262,"But, this is okay, for",pic_cs-410_3_3_240.jpg
cs-410_3_3_66,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:28,950","00:04:34,870",66,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=268,"And, this is for the relative comparison,",pic_cs-410_3_3_240.jpg
cs-410_3_3_67,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:34,870","00:04:39,560",67,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=274,"or actual, actual number deviates",pic_cs-410_3_3_240.jpg
cs-410_3_3_68,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:39,560","00:04:41,970",68,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=279,"As long as the deviation,",pic_cs-410_3_3_240.jpg
cs-410_3_3_69,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:41,970","00:04:46,810",69,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=281,is not biased toward any particular,pic_cs-410_3_3_240.jpg
cs-410_3_3_70,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:46,810","00:04:50,560",70,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=286,"We can still,",pic_cs-410_3_3_240.jpg
cs-410_3_3_71,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:50,560","00:04:53,360",71,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=290,"And, this is important point,",pic_cs-410_3_3_240.jpg
cs-410_3_3_72,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:53,360","00:04:55,550",72,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=293,"When you compare different algorithms,",pic_cs-410_3_3_240.jpg
cs-410_3_3_73,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:55,550","00:04:58,810",73,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=295,the key's to avoid any,pic_cs-410_3_3_240.jpg
cs-410_3_3_74,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:04:58,810","00:05:02,130",74,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=298,"And, as long as, you can avoid that.",pic_cs-410_3_3_240.jpg
cs-410_3_3_75,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:02,130","00:05:06,580",75,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=302,"It's okay, for you to do transformation",pic_cs-410_3_3_300.jpg
cs-410_3_3_76,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:06,580","00:05:07,640",76,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=306,you can preserve the order.,pic_cs-410_3_3_300.jpg
cs-410_3_3_77,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:09,380","00:05:11,170",77,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=309,"Okay, so, we'll just talk about,",pic_cs-410_3_3_300.jpg
cs-410_3_3_78,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:11,170","00:05:16,030",78,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=311,we can get a lot of precision-recall,pic_cs-410_3_3_300.jpg
cs-410_3_3_79,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:16,030","00:05:19,000",79,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=316,"So, now, you can imagine,",pic_cs-410_3_3_300.jpg
cs-410_3_3_80,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:19,000","00:05:22,389",80,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=319,"And, this just shows on the,",pic_cs-410_3_3_300.jpg
cs-410_3_3_81,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:23,610","00:05:30,110",81,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=323,"And, on the y-axis, we show the precision.",pic_cs-410_3_3_300.jpg
cs-410_3_3_82,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:30,110","00:05:35,336",82,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=330,"So, the precision line was marked as .1,",pic_cs-410_3_3_300.jpg
cs-410_3_3_83,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:35,336","00:05:35,998",83,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=335,Right?,pic_cs-410_3_3_300.jpg
cs-410_3_3_84,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:35,998","00:05:38,618",84,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=335,"this is, the different, levels of recall.",pic_cs-410_3_3_300.jpg
cs-410_3_3_85,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:38,618","00:05:44,390",85,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=338,"And,, the y-axis also has,",pic_cs-410_3_3_300.jpg
cs-410_3_3_86,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:45,450","00:05:49,150",86,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=345,"So, we plot the, these, precision-recall",pic_cs-410_3_3_300.jpg
cs-410_3_3_87,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:49,150","00:05:51,360",87,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=349,as points on this picture.,pic_cs-410_3_3_300.jpg
cs-410_3_3_88,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:51,360","00:05:56,410",88,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=351,"Now, we can further, and",pic_cs-410_3_3_300.jpg
cs-410_3_3_89,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:56,410","00:05:57,290",89,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=356,"As you'll see,",pic_cs-410_3_3_300.jpg
cs-410_3_3_90,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:05:57,290","00:06:02,040",90,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=357,"we assumed all the other, precision",pic_cs-410_3_3_300.jpg
cs-410_3_3_91,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:02,040","00:06:08,232",91,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=362,"And, that's why, they are down here,",pic_cs-410_3_3_360.jpg
cs-410_3_3_92,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:08,232","00:06:14,980",92,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=368,"And this, the actual curve probably will",pic_cs-410_3_3_360.jpg
cs-410_3_3_93,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:14,980","00:06:19,410",93,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=374,"discussed, it, it doesn't matter that",pic_cs-410_3_3_360.jpg
cs-410_3_3_94,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:20,430","00:06:24,600",94,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=380,"because this would be,",pic_cs-410_3_3_360.jpg
cs-410_3_3_95,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:25,950","00:06:31,016",95,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=385,"Okay, so, now that we,",pic_cs-410_3_3_360.jpg
cs-410_3_3_96,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:31,016","00:06:34,290",96,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=391,how can we compare ranked to back list?,pic_cs-410_3_3_360.jpg
cs-410_3_3_97,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:34,290","00:06:37,049",97,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=394,"All right, so, that means,",pic_cs-410_3_3_360.jpg
cs-410_3_3_98,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:38,430","00:06:40,880",98,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=398,"And here, we show, two cases.",pic_cs-410_3_3_360.jpg
cs-410_3_3_99,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:40,880","00:06:47,260",99,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=400,"Where system A is showing red,",pic_cs-410_3_3_360.jpg
cs-410_3_3_100,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:48,610","00:06:50,820",100,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=408,"All right, so, which one is better?",pic_cs-410_3_3_360.jpg
cs-410_3_3_101,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:50,820","00:06:54,080",101,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=410,"I hope you can see,",pic_cs-410_3_3_360.jpg
cs-410_3_3_102,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:54,080","00:06:56,900",102,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=414,Why?,pic_cs-410_3_3_360.jpg
cs-410_3_3_103,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:06:58,340","00:07:01,500",103,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=418,"see same level of recall here,",pic_cs-410_3_3_360.jpg
cs-410_3_3_104,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:01,500","00:07:06,800",104,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=421,"the precision point by system A is better,",pic_cs-410_3_3_420.jpg
cs-410_3_3_105,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:06,800","00:07:08,260",105,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=426,"So, there's no question.",pic_cs-410_3_3_420.jpg
cs-410_3_3_106,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:08,260","00:07:13,360",106,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=428,"In here, you can imagine, what does the",pic_cs-410_3_3_420.jpg
cs-410_3_3_107,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:13,360","00:07:17,470",107,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=433,"Well, it has to have perfect,",pic_cs-410_3_3_420.jpg
cs-410_3_3_108,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:17,470","00:07:18,450",108,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=437,it has to be this line.,pic_cs-410_3_3_420.jpg
cs-410_3_3_109,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:18,450","00:07:21,300",109,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=438,That would be the ideal system.,pic_cs-410_3_3_420.jpg
cs-410_3_3_110,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:21,300","00:07:24,230",110,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=441,"In general, the higher the curve is,",pic_cs-410_3_3_420.jpg
cs-410_3_3_111,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:24,230","00:07:27,160",111,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=444,"The problem is that,",pic_cs-410_3_3_420.jpg
cs-410_3_3_112,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:27,160","00:07:29,110",112,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=447,This actually happens often.,pic_cs-410_3_3_420.jpg
cs-410_3_3_113,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:29,110","00:07:30,790",113,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=449,"Like, the two curves cross each other.",pic_cs-410_3_3_420.jpg
cs-410_3_3_114,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:32,430","00:07:34,240",114,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=452,"Now, in this case, which one is better?",pic_cs-410_3_3_420.jpg
cs-410_3_3_115,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:35,300","00:07:35,800",115,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=455,What do you think?,pic_cs-410_3_3_420.jpg
cs-410_3_3_116,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:38,240","00:07:41,730",116,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=458,"Now, this is a real problem,",pic_cs-410_3_3_420.jpg
cs-410_3_3_117,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:41,730","00:07:47,150",117,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=461,"Suppose, you build a search engine,",pic_cs-410_3_3_420.jpg
cs-410_3_3_118,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:47,150","00:07:50,990",118,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=467,"that's shown here in blue, or system B.",pic_cs-410_3_3_420.jpg
cs-410_3_3_119,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:50,990","00:07:53,580",119,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=470,"And, you have come up with a new idea.",pic_cs-410_3_3_420.jpg
cs-410_3_3_120,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:53,580","00:07:54,500",120,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=473,"And, you test it.",pic_cs-410_3_3_420.jpg
cs-410_3_3_121,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:54,500","00:07:58,490",121,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=474,"And, the results are shown in red,",pic_cs-410_3_3_420.jpg
cs-410_3_3_122,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:07:59,990","00:08:04,550",122,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=479,"Now, your question is, is your new",pic_cs-410_3_3_420.jpg
cs-410_3_3_123,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:05,630","00:08:10,510",123,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=485,"Or more, practically,",pic_cs-410_3_3_480.jpg
cs-410_3_3_124,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:10,510","00:08:15,410",124,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=490,"you're already using, your, in your search",pic_cs-410_3_3_480.jpg
cs-410_3_3_125,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:15,410","00:08:20,760",125,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=495,"So, should we use system,",pic_cs-410_3_3_480.jpg
cs-410_3_3_126,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:20,760","00:08:23,250",126,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=500,"This is going to be a real decision,",pic_cs-410_3_3_480.jpg
cs-410_3_3_127,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:23,250","00:08:29,430",127,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=503,"If you make the replacement, the search",pic_cs-410_3_3_480.jpg
cs-410_3_3_128,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:29,430","00:08:34,170",128,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=509,"whereas, if you don't do that,",pic_cs-410_3_3_480.jpg
cs-410_3_3_129,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:34,170","00:08:34,770",129,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=514,"So, what do you do?",pic_cs-410_3_3_480.jpg
cs-410_3_3_130,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:36,210","00:08:40,580",130,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=516,"Now, if you want to spend more time",pic_cs-410_3_3_480.jpg
cs-410_3_3_131,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:40,580","00:08:42,840",131,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=520,"And, it's actually very",pic_cs-410_3_3_480.jpg
cs-410_3_3_132,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:42,840","00:08:46,350",132,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=522,"As I said, it's a real decision that you",pic_cs-410_3_3_480.jpg
cs-410_3_3_133,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:46,350","00:08:51,329",133,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=526,"search engine, or if you're working, for",pic_cs-410_3_3_480.jpg
cs-410_3_3_134,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:52,330","00:08:54,630",134,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=532,"Now, if you have thought about this for",pic_cs-410_3_3_480.jpg
cs-410_3_3_135,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:54,630","00:08:59,630",135,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=534,"a moment, you might realize that,",pic_cs-410_3_3_480.jpg
cs-410_3_3_136,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:08:59,630","00:09:04,615",136,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=539,"Now, some users might like a system A,",pic_cs-410_3_3_480.jpg
cs-410_3_3_137,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:04,615","00:09:05,895",137,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=544,"So, what's the difference here?",pic_cs-410_3_3_540.jpg
cs-410_3_3_138,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:05,895","00:09:08,545",138,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=545,"Well, the difference is just that,",pic_cs-410_3_3_540.jpg
cs-410_3_3_139,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:08,545","00:09:14,145",139,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=548,"in the, low level of recall,",pic_cs-410_3_3_540.jpg
cs-410_3_3_140,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:14,145","00:09:15,845",140,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=554,There's a higher precision.,pic_cs-410_3_3_540.jpg
cs-410_3_3_141,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:15,845","00:09:19,520",141,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=555,"But in high recall region,",pic_cs-410_3_3_540.jpg
cs-410_3_3_142,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:20,910","00:09:24,040",142,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=560,"Now, so, that also means,",pic_cs-410_3_3_540.jpg
cs-410_3_3_143,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:24,040","00:09:28,630",143,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=564,"cares about the high recall, or",pic_cs-410_3_3_540.jpg
cs-410_3_3_144,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:28,630","00:09:32,200",144,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=568,"You can imagine, if someone is just going",pic_cs-410_3_3_540.jpg
cs-410_3_3_145,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:32,200","00:09:33,489",145,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=572,want to find out something,pic_cs-410_3_3_540.jpg
cs-410_3_3_146,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:34,750","00:09:36,530",146,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=574,"Well, which one is better?",pic_cs-410_3_3_540.jpg
cs-410_3_3_147,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:36,530","00:09:37,060",147,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=576,What do you think?,pic_cs-410_3_3_540.jpg
cs-410_3_3_148,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:38,110","00:09:41,510",148,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=578,"In this case, clearly, system B is better,",pic_cs-410_3_3_540.jpg
cs-410_3_3_149,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:41,510","00:09:44,920",149,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=581,because the user is unlikely,pic_cs-410_3_3_540.jpg
cs-410_3_3_150,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:44,920","00:09:46,500",150,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=584,The user doesn't care about high recall.,pic_cs-410_3_3_540.jpg
cs-410_3_3_151,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:47,780","00:09:50,673",151,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=587,"On the other hand,",pic_cs-410_3_3_540.jpg
cs-410_3_3_152,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:50,673","00:09:54,800",152,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=590,"where a user is doing you are,",pic_cs-410_3_3_540.jpg
cs-410_3_3_153,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:09:54,800","00:10:00,320",153,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=594,"You want to find, whether your idea ha,",pic_cs-410_3_3_540.jpg
cs-410_3_3_154,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:00,320","00:10:03,130",154,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=600,"In that case, you emphasize high recall.",pic_cs-410_3_3_600.jpg
cs-410_3_3_155,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:03,130","00:10:06,630",155,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=603,"So, you want to see,",pic_cs-410_3_3_600.jpg
cs-410_3_3_156,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:06,630","00:10:09,570",156,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=606,"Therefore, you might, favor, system A.",pic_cs-410_3_3_600.jpg
cs-410_3_3_157,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:09,570","00:10:12,090",157,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=609,"So, that means, which one is better?",pic_cs-410_3_3_600.jpg
cs-410_3_3_158,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:12,090","00:10:18,420",158,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=612,"That actually depends on users,",pic_cs-410_3_3_600.jpg
cs-410_3_3_159,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:19,520","00:10:24,040",159,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=619,"So, this means, you may not necessarily",pic_cs-410_3_3_600.jpg
cs-410_3_3_160,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:25,290","00:10:28,810",160,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=625,that would accurately,pic_cs-410_3_3_600.jpg
cs-410_3_3_161,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:29,860","00:10:31,750",161,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=629,You have to look at the overall picture.,pic_cs-410_3_3_600.jpg
cs-410_3_3_162,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:31,750","00:10:35,620",162,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=631,"Yet, as I said, when you have",pic_cs-410_3_3_600.jpg
cs-410_3_3_163,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:35,620","00:10:38,210",163,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=635,"whether you replace ours with another,",pic_cs-410_3_3_600.jpg
cs-410_3_3_164,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:38,210","00:10:44,320",164,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=638,then you may have to actually come up with,pic_cs-410_3_3_600.jpg
cs-410_3_3_165,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:44,320","00:10:49,800",165,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=644,"Or, when we compare many different",pic_cs-410_3_3_600.jpg
cs-410_3_3_166,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:49,800","00:10:54,590",166,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=649,"one number to compare, them with, so, that",pic_cs-410_3_3_600.jpg
cs-410_3_3_167,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:10:54,590","00:11:00,258",167,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=654,"So, for all these reasons, it is desirable",pic_cs-410_3_3_600.jpg
cs-410_3_3_168,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:00,258","00:11:01,510",168,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=660,"So, how do we do that?",pic_cs-410_3_3_660.jpg
cs-410_3_3_169,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:01,510","00:11:05,860",169,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=661,"And, that,",pic_cs-410_3_3_660.jpg
cs-410_3_3_170,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:05,860","00:11:09,560",170,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=665,"So, here again it's",pic_cs-410_3_3_660.jpg
cs-410_3_3_171,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:09,560","00:11:13,570",171,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=669,"And, one way to summarize",pic_cs-410_3_3_660.jpg
cs-410_3_3_172,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:13,570","00:11:18,090",172,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=673,"this whole curve,",pic_cs-410_3_3_660.jpg
cs-410_3_3_173,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:19,330","00:11:21,820",173,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=679,Right?,pic_cs-410_3_3_660.jpg
cs-410_3_3_174,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:21,820","00:11:25,209",174,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=681,"There are other ways to measure that,",pic_cs-410_3_3_660.jpg
cs-410_3_3_175,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:26,430","00:11:31,110",175,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=686,this particular way of matching,pic_cs-410_3_3_660.jpg
cs-410_3_3_176,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:31,110","00:11:36,260",176,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=691,"has been used, since a long time ago for",pic_cs-410_3_3_660.jpg
cs-410_3_3_177,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:36,260","00:11:41,140",177,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=696,"basically, in this way, and",pic_cs-410_3_3_660.jpg
cs-410_3_3_178,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:41,140","00:11:46,260",178,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=701,"Basically, we're going to take a, a look",pic_cs-410_3_3_660.jpg
cs-410_3_3_179,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:47,600","00:11:49,540",179,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=707,"And then, look out for the precision.",pic_cs-410_3_3_660.jpg
cs-410_3_3_180,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:49,540","00:11:51,930",180,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=709,"So, we know, you know,",pic_cs-410_3_3_660.jpg
cs-410_3_3_181,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:51,930","00:11:56,590",181,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=711,"And, this is another,",pic_cs-410_3_3_660.jpg
cs-410_3_3_182,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:56,590","00:11:59,362",182,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=716,"Now, this, we don't count to this one,",pic_cs-410_3_3_660.jpg
cs-410_3_3_183,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:11:59,362","00:12:04,511",183,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=719,"because the recall level is the same,",pic_cs-410_3_3_660.jpg
cs-410_3_3_184,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:04,511","00:12:10,120",184,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=724,"this number, and that's precision at",pic_cs-410_3_3_720.jpg
cs-410_3_3_185,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:10,120","00:12:13,580",185,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=730,"So, we have all these, you know, added up.",pic_cs-410_3_3_720.jpg
cs-410_3_3_186,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:13,580","00:12:16,180",186,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=733,These are the precisions,pic_cs-410_3_3_720.jpg
cs-410_3_3_187,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:16,180","00:12:21,130",187,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=736,corresponding to retrieving the first,pic_cs-410_3_3_720.jpg
cs-410_3_3_188,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:21,130","00:12:25,260",188,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=741,"then, the third, that follows, et cetera.",pic_cs-410_3_3_720.jpg
cs-410_3_3_189,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:25,260","00:12:29,265",189,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=745,"Now, we missed the many relevant",pic_cs-410_3_3_720.jpg
cs-410_3_3_190,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:29,265","00:12:32,380",190,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=749,"we just, assume,",pic_cs-410_3_3_720.jpg
cs-410_3_3_191,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:33,540","00:12:35,740",191,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=753,"And then, finally, we take the average.",pic_cs-410_3_3_720.jpg
cs-410_3_3_192,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:35,740","00:12:37,900",192,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=755,"So, we divide it by ten, and",pic_cs-410_3_3_720.jpg
cs-410_3_3_193,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:37,900","00:12:40,510",193,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=757,which is the total number of relevant,pic_cs-410_3_3_720.jpg
cs-410_3_3_194,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:41,670","00:12:46,610",194,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=761,"Note that here,",pic_cs-410_3_3_720.jpg
cs-410_3_3_195,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:46,610","00:12:49,440",195,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=766,Which is a number retrieved,pic_cs-410_3_3_720.jpg
cs-410_3_3_196,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:49,440","00:12:52,980",196,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=769,"Now, imagine, if I divide by four,",pic_cs-410_3_3_720.jpg
cs-410_3_3_197,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:54,370","00:12:55,820",197,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=774,"Now, think about this, for a moment.",pic_cs-410_3_3_720.jpg
cs-410_3_3_198,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:12:57,050","00:13:01,300",198,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=777,"It's a common mistake that people,",pic_cs-410_3_3_720.jpg
cs-410_3_3_199,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:02,720","00:13:08,208",199,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=782,"Right, so, if we, we divide this by four,",pic_cs-410_3_3_780.jpg
cs-410_3_3_200,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:08,208","00:13:13,180",200,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=788,"In fact, that you are favoring a system,",pic_cs-410_3_3_780.jpg
cs-410_3_3_201,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:13,180","00:13:18,785",201,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=793,"documents, as in that case,",pic_cs-410_3_3_780.jpg
cs-410_3_3_202,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:18,785","00:13:22,115",202,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=798,"So, this would be, not a good matching.",pic_cs-410_3_3_780.jpg
cs-410_3_3_203,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:22,115","00:13:25,862",203,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=802,"So, note that this denomina,",pic_cs-410_3_3_780.jpg
cs-410_3_3_204,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:25,862","00:13:29,170",204,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=805,the total number of relevant documents.,pic_cs-410_3_3_780.jpg
cs-410_3_3_205,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:29,170","00:13:33,620",205,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=809,"And, this will basically ,compute",pic_cs-410_3_3_780.jpg
cs-410_3_3_206,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:33,620","00:13:40,080",206,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=813,"And, this is the standard method,",pic_cs-410_3_3_780.jpg
cs-410_3_3_207,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:41,210","00:13:44,860",207,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=821,"Note that, it actually combines",pic_cs-410_3_3_780.jpg
cs-410_3_3_208,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:44,860","00:13:49,230",208,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=824,"But first, you know, we have",pic_cs-410_3_3_780.jpg
cs-410_3_3_209,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:49,230","00:13:53,240",209,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=829,"we also consider recall, because if missed",pic_cs-410_3_3_780.jpg
cs-410_3_3_210,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:53,240","00:13:57,470",210,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=833,"All right, so,",pic_cs-410_3_3_780.jpg
cs-410_3_3_211,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:13:57,470","00:14:02,190",211,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=837,"And furthermore, you can see this",pic_cs-410_3_3_780.jpg
cs-410_3_3_212,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:02,190","00:14:04,770",212,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=842,of a position of a relevant document.,pic_cs-410_3_3_840.jpg
cs-410_3_3_213,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:04,770","00:14:09,520",213,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=844,"Let's say, if I move this relevant",pic_cs-410_3_3_840.jpg
cs-410_3_3_214,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:09,520","00:14:12,670",214,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=849,"it would increase this means,",pic_cs-410_3_3_840.jpg
cs-410_3_3_215,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:12,670","00:14:17,630",215,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=852,"Whereas, if I move any relevant document,",pic_cs-410_3_3_840.jpg
cs-410_3_3_216,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:17,630","00:14:23,720",216,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=857,"document down, then it would decrease,",pic_cs-410_3_3_840.jpg
cs-410_3_3_217,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:23,720","00:14:25,266",217,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=863,"So, this is a very good,",pic_cs-410_3_3_840.jpg
cs-410_3_3_218,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:25,266","00:14:30,570",218,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=865,because it's a very sensitive to,pic_cs-410_3_3_840.jpg
cs-410_3_3_219,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:30,570","00:14:34,740",219,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=870,"It can tell, small differences",pic_cs-410_3_3_840.jpg
cs-410_3_3_220,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:34,740","00:14:35,880",220,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=874,"And, that is what we want,",pic_cs-410_3_3_840.jpg
cs-410_3_3_221,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:35,880","00:14:40,430",221,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=875,sometimes one algorithm only works,pic_cs-410_3_3_840.jpg
cs-410_3_3_222,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:40,430","00:14:42,440",222,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=880,"And, we want to see this difference.",pic_cs-410_3_3_840.jpg
cs-410_3_3_223,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:42,440","00:14:46,110",223,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=882,"In contrast, if we look at",pic_cs-410_3_3_840.jpg
cs-410_3_3_224,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:46,110","00:14:49,520",224,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=886,"If we look at this, this whole set, well,",pic_cs-410_3_3_840.jpg
cs-410_3_3_225,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:49,520","00:14:52,000",225,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=889,"what, what's the precision,",pic_cs-410_3_3_840.jpg
cs-410_3_3_226,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:52,000","00:14:54,328",226,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=892,"Well, it's easy to see,",pic_cs-410_3_3_840.jpg
cs-410_3_3_227,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:14:54,328","00:15:02,200",227,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=894,"So, that precision is very meaningful,",pic_cs-410_3_3_840.jpg
cs-410_3_3_228,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:02,200","00:15:04,580",228,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=902,"So, that's pretty useful, right?",pic_cs-410_3_3_900.jpg
cs-410_3_3_229,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:04,580","00:15:07,850",229,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=904,"So, it's a meaningful measure,",pic_cs-410_3_3_900.jpg
cs-410_3_3_230,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:07,850","00:15:11,770",230,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=907,"But, if we use this measure to",pic_cs-410_3_3_900.jpg
cs-410_3_3_231,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:11,770","00:15:16,480",231,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=911,because it wouldn't be sensitive to where,pic_cs-410_3_3_900.jpg
cs-410_3_3_232,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:16,480","00:15:21,910",232,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=916,If I move them around the precision,pic_cs-410_3_3_900.jpg
cs-410_3_3_233,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:21,910","00:15:22,570",233,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=921,Right.,pic_cs-410_3_3_900.jpg
cs-410_3_3_234,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:22,570","00:15:25,590",234,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=922,this is not a good measure for,pic_cs-410_3_3_900.jpg
cs-410_3_3_235,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:25,590","00:15:29,990",235,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=925,"In contrast, the average precision",pic_cs-410_3_3_900.jpg
cs-410_3_3_236,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:29,990","00:15:34,511",236,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=929,"It can tell the difference of, different,",pic_cs-410_3_3_900.jpg
cs-410_3_3_237,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:34,511","00:15:39,269",237,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=934,"a difference in ranked list in,",pic_cs-410_3_3_900.jpg
cs-410_3_3_238,cs-410,3,3, Evaluation of TR Systems - Evaluating Ranked Lists - Part 1,"00:15:39,269","00:15:49,269",238,https://www.coursera.org/learn/cs-410/lecture/rU7LT?t=939,[MUSIC],pic_cs-410_3_3_900.jpg
cs-410_3_4_1,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:00,012","00:00:03,467",1,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=0,[SOUND],pic_cs-410_3_4_0.jpg
cs-410_3_4_2,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:11,647","00:00:13,963",2,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=11,So average precision is computer for,pic_cs-410_3_4_0.jpg
cs-410_3_4_3,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:13,963","00:00:14,690",3,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=13,just one.,pic_cs-410_3_4_0.jpg
cs-410_3_4_4,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:14,690","00:00:18,290",4,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=14,one query.,pic_cs-410_3_4_0.jpg
cs-410_3_4_5,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:18,290","00:00:24,570",5,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=18,different queries and this is to,pic_cs-410_3_4_0.jpg
cs-410_3_4_6,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:24,570","00:00:29,530",6,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=24,Depending on the queries you use you,pic_cs-410_3_4_0.jpg
cs-410_3_4_7,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:29,530","00:00:31,870",7,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=29,"Right, so",pic_cs-410_3_4_0.jpg
cs-410_3_4_8,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:33,610","00:00:36,580",8,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=33,"If you use more queries then,",pic_cs-410_3_4_0.jpg
cs-410_3_4_9,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:36,580","00:00:39,850",9,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=36,take the average of the average,pic_cs-410_3_4_0.jpg
cs-410_3_4_10,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:41,600","00:00:42,470",10,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=41,So how can we do that?,pic_cs-410_3_4_0.jpg
cs-410_3_4_11,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:43,560","00:00:46,160",11,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=43,"Well, you can naturally.",pic_cs-410_3_4_0.jpg
cs-410_3_4_12,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:46,160","00:00:49,260",12,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=46,Think of just doing arithmetic mean as we,pic_cs-410_3_4_0.jpg
cs-410_3_4_13,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:50,670","00:00:56,000",13,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=50,"always tend to, to think in, in this way.",pic_cs-410_3_4_0.jpg
cs-410_3_4_14,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:00:56,000","00:01:02,000",14,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=56,"So, this would give us what's called",pic_cs-410_3_4_0.jpg
cs-410_3_4_15,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:02,000","00:01:02,540",15,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=62,"In this case,",pic_cs-410_3_4_60.jpg
cs-410_3_4_16,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:02,540","00:01:08,370",16,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=62,we take arithmetic mean of all the average,pic_cs-410_3_4_60.jpg
cs-410_3_4_17,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:09,930","00:01:13,840",17,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=69,But as I just mentioned in,pic_cs-410_3_4_60.jpg
cs-410_3_4_18,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:15,370","00:01:16,580",18,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=75,We call that.,pic_cs-410_3_4_60.jpg
cs-410_3_4_19,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:16,580","00:01:21,190",19,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=76,We talked about the different ways,pic_cs-410_3_4_60.jpg
cs-410_3_4_20,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:21,190","00:01:27,340",20,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=81,And we conclude that the arithmetic,pic_cs-410_3_4_60.jpg
cs-410_3_4_21,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:27,340","00:01:28,420",21,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=87,But here it's the same.,pic_cs-410_3_4_60.jpg
cs-410_3_4_22,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:28,420","00:01:32,120",22,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=88,We can also think about the alternative,pic_cs-410_3_4_60.jpg
cs-410_3_4_23,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:32,120","00:01:34,850",23,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=92,"Don't just automatically assume that,",pic_cs-410_3_4_60.jpg
cs-410_3_4_24,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:34,850","00:01:37,785",24,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=94,Let's just also take the arithmetic,pic_cs-410_3_4_60.jpg
cs-410_3_4_25,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:37,785","00:01:38,590",25,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=97,these queries.,pic_cs-410_3_4_60.jpg
cs-410_3_4_26,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:38,590","00:01:42,910",26,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=98,Let's think about what's,pic_cs-410_3_4_60.jpg
cs-410_3_4_27,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:42,910","00:01:46,660",27,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=102,"If you think about the different ways,",pic_cs-410_3_4_60.jpg
cs-410_3_4_28,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:46,660","00:01:49,880",28,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=106,probably be able to think about,pic_cs-410_3_4_60.jpg
cs-410_3_4_29,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:51,230","00:01:53,680",29,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=111,And we call this kind of average a gMAP.,pic_cs-410_3_4_60.jpg
cs-410_3_4_30,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:55,650","00:01:56,860",30,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=115,This is another way.,pic_cs-410_3_4_60.jpg
cs-410_3_4_31,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:56,860","00:01:59,520",31,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=116,"So now, once you think about",pic_cs-410_3_4_60.jpg
cs-410_3_4_32,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:01:59,520","00:02:00,820",32,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=119,Of doing the same thing.,pic_cs-410_3_4_60.jpg
cs-410_3_4_33,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:00,820","00:02:03,400",33,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=120,"The natural question to ask is,",pic_cs-410_3_4_120.jpg
cs-410_3_4_34,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:03,400","00:02:03,900",34,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=123,So.,pic_cs-410_3_4_120.jpg
cs-410_3_4_35,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:05,230","00:02:08,060",35,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=125,"So, do you use MAP or gMAP?",pic_cs-410_3_4_120.jpg
cs-410_3_4_36,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:09,830","00:02:11,200",36,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=129,"Again, that's important question.",pic_cs-410_3_4_120.jpg
cs-410_3_4_37,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:11,200","00:02:14,490",37,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=131,Imagine you are again,pic_cs-410_3_4_120.jpg
cs-410_3_4_38,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:14,490","00:02:17,109",38,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=134,by comparing the ways your old,pic_cs-410_3_4_120.jpg
cs-410_3_4_39,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:18,390","00:02:22,080",39,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=138,Now you tested multiple topics.,pic_cs-410_3_4_120.jpg
cs-410_3_4_40,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:22,080","00:02:25,150",40,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=142,Now you've got the average precision for,pic_cs-410_3_4_120.jpg
cs-410_3_4_41,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:25,150","00:02:28,470",41,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=145,Now you are thinking of looking,pic_cs-410_3_4_120.jpg
cs-410_3_4_42,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:28,470","00:02:29,470",42,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=148,You have to take the average.,pic_cs-410_3_4_120.jpg
cs-410_3_4_43,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:30,950","00:02:32,840",43,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=150,"But which, which strategy would you use?",pic_cs-410_3_4_120.jpg
cs-410_3_4_44,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:34,040","00:02:38,360",44,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=154,"Now first, you should also think about the",pic_cs-410_3_4_120.jpg
cs-410_3_4_45,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:38,360","00:02:43,100",45,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=158,Can you think of scenarios where using,pic_cs-410_3_4_120.jpg
cs-410_3_4_46,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:43,100","00:02:45,920",46,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=163,That is they would give different,pic_cs-410_3_4_120.jpg
cs-410_3_4_47,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:45,920","00:02:52,440",47,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=165,And that also means depending on,pic_cs-410_3_4_120.jpg
cs-410_3_4_48,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:52,440","00:02:54,450",48,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=172,Average of these average positions.,pic_cs-410_3_4_120.jpg
cs-410_3_4_49,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:55,600","00:02:57,460",49,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=175,You will get different conclusions.,pic_cs-410_3_4_120.jpg
cs-410_3_4_50,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:02:57,460","00:03:00,379",50,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=177,This makes the question,pic_cs-410_3_4_120.jpg
cs-410_3_4_51,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:01,620","00:03:03,480",51,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=181,Right?,pic_cs-410_3_4_180.jpg
cs-410_3_4_52,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:05,350","00:03:08,320",52,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=185,"Well again, if you look at",pic_cs-410_3_4_180.jpg
cs-410_3_4_53,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:08,320","00:03:12,750",53,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=188,Different ways of aggregating,pic_cs-410_3_4_180.jpg
cs-410_3_4_54,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:12,750","00:03:18,510",54,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=192,"You'll realize in arithmetic mean,",pic_cs-410_3_4_180.jpg
cs-410_3_4_55,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:18,510","00:03:20,250",55,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=198,So what does large value here mean?,pic_cs-410_3_4_180.jpg
cs-410_3_4_56,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:20,250","00:03:22,260",56,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=200,It means the query is relatively easy.,pic_cs-410_3_4_180.jpg
cs-410_3_4_57,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:22,260","00:03:24,750",57,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=202,"You can have a high pres,",pic_cs-410_3_4_180.jpg
cs-410_3_4_58,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:25,950","00:03:29,707",58,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=205,Whereas gMAP tends to be,pic_cs-410_3_4_180.jpg
cs-410_3_4_59,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:30,870","00:03:34,790",59,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=210,And those are the queries that,pic_cs-410_3_4_180.jpg
cs-410_3_4_60,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:34,790","00:03:36,370",60,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=214,The average precision is low.,pic_cs-410_3_4_180.jpg
cs-410_3_4_61,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:37,410","00:03:41,260",61,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=217,"So if you think about the,",pic_cs-410_3_4_180.jpg
cs-410_3_4_62,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:41,260","00:03:45,840",62,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=221,"those difficult queries,",pic_cs-410_3_4_180.jpg
cs-410_3_4_63,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:47,480","00:03:50,000",63,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=227,"On the other hand, if you just want to.",pic_cs-410_3_4_180.jpg
cs-410_3_4_64,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:50,000","00:03:50,960",64,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=230,Have improved a lot.,pic_cs-410_3_4_180.jpg
cs-410_3_4_65,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:52,060","00:03:55,750",65,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=232,Over all the kinds of queries or,pic_cs-410_3_4_180.jpg
cs-410_3_4_66,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:03:55,750","00:04:00,890",66,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=235,easy and you want to make the perfect and,pic_cs-410_3_4_180.jpg
cs-410_3_4_67,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:00,890","00:04:05,150",67,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=240,"So again, the answer depends on",pic_cs-410_3_4_240.jpg
cs-410_3_4_68,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:05,150","00:04:06,710",68,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=245,"their pref, their preferences.",pic_cs-410_3_4_240.jpg
cs-410_3_4_69,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:08,020","00:04:13,720",69,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=248,So the point that here is to think,pic_cs-410_3_4_240.jpg
cs-410_3_4_70,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:13,720","00:04:18,750",70,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=253,"the same problem, and then compare them,",pic_cs-410_3_4_240.jpg
cs-410_3_4_71,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:18,750","00:04:20,610",71,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=258,And which one makes more sense.,pic_cs-410_3_4_240.jpg
cs-410_3_4_72,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:20,610","00:04:24,970",72,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=260,"Often, when one of them might",pic_cs-410_3_4_240.jpg
cs-410_3_4_73,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:24,970","00:04:27,640",73,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=264,another might make more sense,pic_cs-410_3_4_240.jpg
cs-410_3_4_74,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:27,640","00:04:31,620",74,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=267,So it's important to pick out under,pic_cs-410_3_4_240.jpg
cs-410_3_4_75,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:35,209","00:04:38,967",75,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=275,As a special case of the mean average,pic_cs-410_3_4_240.jpg
cs-410_3_4_76,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:38,967","00:04:43,100",76,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=278,the case where there was precisely,pic_cs-410_3_4_240.jpg
cs-410_3_4_77,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:43,100","00:04:47,210",77,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=283,"And this happens often, for example,",pic_cs-410_3_4_240.jpg
cs-410_3_4_78,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:47,210","00:04:52,670",78,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=287,"Where you know a target page, let's",pic_cs-410_3_4_240.jpg
cs-410_3_4_79,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:52,670","00:04:56,140",79,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=292,"You have one relevant document there,",pic_cs-410_3_4_240.jpg
cs-410_3_4_80,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:56,140","00:04:58,250",80,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=296,"That's call a ""known item search"".",pic_cs-410_3_4_240.jpg
cs-410_3_4_81,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:04:58,250","00:05:01,330",81,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=298,"In that case,",pic_cs-410_3_4_240.jpg
cs-410_3_4_82,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:01,330","00:05:03,470",82,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=301,"Or in another application,",pic_cs-410_3_4_300.jpg
cs-410_3_4_83,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:03,470","00:05:04,640",83,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=303,maybe there's only one answer.,pic_cs-410_3_4_300.jpg
cs-410_3_4_84,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:04,640","00:05:05,250",84,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=304,Are there.,pic_cs-410_3_4_300.jpg
cs-410_3_4_85,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:05,250","00:05:07,110",85,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=305,"So if you rank the answers,",pic_cs-410_3_4_300.jpg
cs-410_3_4_86,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:07,110","00:05:12,100",86,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=307,then your goal is to rank that one,pic_cs-410_3_4_300.jpg
cs-410_3_4_87,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:12,100","00:05:16,480",87,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=312,"So in this case, you can easily",pic_cs-410_3_4_300.jpg
cs-410_3_4_88,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:16,480","00:05:21,710",88,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=316,will basically boil down,pic_cs-410_3_4_300.jpg
cs-410_3_4_89,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:21,710","00:05:28,220",89,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=321,"That is, 1 over r where r is the rank",pic_cs-410_3_4_300.jpg
cs-410_3_4_90,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:28,220","00:05:32,210",90,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=328,So if that document is ranked,pic_cs-410_3_4_300.jpg
cs-410_3_4_91,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:32,210","00:05:35,930",91,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=332,then it's 1 for reciprocal rank.,pic_cs-410_3_4_300.jpg
cs-410_3_4_92,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:35,930","00:05:39,515",92,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=335,"If it's ranked at the,",pic_cs-410_3_4_300.jpg
cs-410_3_4_93,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:39,515","00:05:40,015",93,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=339,Et cetera.,pic_cs-410_3_4_300.jpg
cs-410_3_4_94,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:41,145","00:05:45,335",94,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=341,"And then we can also take a, a average",pic_cs-410_3_4_300.jpg
cs-410_3_4_95,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:45,335","00:05:48,025",95,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=345,"reciprocal rank over a set of topics, and",pic_cs-410_3_4_300.jpg
cs-410_3_4_96,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:48,025","00:05:52,555",96,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=348,that would give us something,pic_cs-410_3_4_300.jpg
cs-410_3_4_97,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:52,555","00:05:54,830",97,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=352,It's a very popular measure.,pic_cs-410_3_4_300.jpg
cs-410_3_4_98,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:54,830","00:05:57,273",98,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=354,"For no item search or, you know,",pic_cs-410_3_4_300.jpg
cs-410_3_4_99,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:05:57,273","00:06:01,690",99,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=357,an problem where you have,pic_cs-410_3_4_300.jpg
cs-410_3_4_100,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:03,070","00:06:09,170",100,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=363,"Now again here, you can see this",pic_cs-410_3_4_360.jpg
cs-410_3_4_101,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:09,170","00:06:13,570",101,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=369,And this r is basically,pic_cs-410_3_4_360.jpg
cs-410_3_4_102,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:13,570","00:06:18,700",102,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=373,a user would have to make in order,pic_cs-410_3_4_360.jpg
cs-410_3_4_103,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:18,700","00:06:23,780",103,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=378,If it's ranked on the top it's low effort,pic_cs-410_3_4_360.jpg
cs-410_3_4_104,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:23,780","00:06:26,860",104,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=383,But if it's ranked at 100,pic_cs-410_3_4_360.jpg
cs-410_3_4_105,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:27,940","00:06:32,260",105,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=387,read presumably 100 documents,pic_cs-410_3_4_360.jpg
cs-410_3_4_106,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:32,260","00:06:37,380",106,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=392,"So, in this sense r is also a meaningful",pic_cs-410_3_4_360.jpg
cs-410_3_4_107,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:37,380","00:06:41,520",107,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=397,"take the reciprocal of r,",pic_cs-410_3_4_360.jpg
cs-410_3_4_108,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:42,750","00:06:45,895",108,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=402,So my natural question here,pic_cs-410_3_4_360.jpg
cs-410_3_4_109,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:45,895","00:06:50,550",109,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=405,I imagine if you were to design,pic_cs-410_3_4_360.jpg
cs-410_3_4_110,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:50,550","00:06:54,680",110,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=410,"of a random system,",pic_cs-410_3_4_360.jpg
cs-410_3_4_111,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:06:55,760","00:07:00,070",111,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=415,You might have thought about,pic_cs-410_3_4_360.jpg
cs-410_3_4_112,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:00,070","00:07:02,906",112,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=420,"After all,",pic_cs-410_3_4_420.jpg
cs-410_3_4_113,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:02,906","00:07:10,959",113,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=422,"But, think about if you take a average",pic_cs-410_3_4_420.jpg
cs-410_3_4_114,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:12,200","00:07:13,730",114,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=432,Again it would make a difference.,pic_cs-410_3_4_420.jpg
cs-410_3_4_115,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:13,730","00:07:16,330",115,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=433,"Right, for one single topic, using r or",pic_cs-410_3_4_420.jpg
cs-410_3_4_116,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:16,330","00:07:19,140",116,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=436,using 1 over r wouldn't,pic_cs-410_3_4_420.jpg
cs-410_3_4_117,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:19,140","00:07:21,640",117,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=439,It's the same.,pic_cs-410_3_4_420.jpg
cs-410_3_4_118,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:21,640","00:07:24,790",118,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=441,Larger r with corresponds,pic_cs-410_3_4_420.jpg
cs-410_3_4_119,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:26,400","00:07:32,700",119,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=446,"But the difference would only show when,",pic_cs-410_3_4_420.jpg
cs-410_3_4_120,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:32,700","00:07:39,290",120,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=452,"So again, think about the average of Mean",pic_cs-410_3_4_420.jpg
cs-410_3_4_121,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:39,290","00:07:39,930",121,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=459,What's the difference?,pic_cs-410_3_4_420.jpg
cs-410_3_4_122,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:39,930","00:07:41,730",122,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=459,Do you see any difference?,pic_cs-410_3_4_420.jpg
cs-410_3_4_123,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:41,730","00:07:46,050",123,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=461,"And would, would this difference",pic_cs-410_3_4_420.jpg
cs-410_3_4_124,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:46,050","00:07:46,760",124,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=466,In our conclusion.,pic_cs-410_3_4_420.jpg
cs-410_3_4_125,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:49,050","00:07:53,380",125,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=469,"And this, it turns out that,",pic_cs-410_3_4_420.jpg
cs-410_3_4_126,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:53,380","00:07:57,210",126,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=473,"if you think about it, if you want to",pic_cs-410_3_4_420.jpg
cs-410_3_4_127,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:57,210","00:07:58,230",127,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=477,then pause the video.,pic_cs-410_3_4_420.jpg
cs-410_3_4_128,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:07:59,410","00:08:04,350",128,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=479,"Basically, the difference is,",pic_cs-410_3_4_420.jpg
cs-410_3_4_129,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:04,350","00:08:07,810",129,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=484,Again it will be dominated,pic_cs-410_3_4_480.jpg
cs-410_3_4_130,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:07,810","00:08:08,840",130,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=487,So what are those values?,pic_cs-410_3_4_480.jpg
cs-410_3_4_131,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:08,840","00:08:15,240",131,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=488,Those are basically large values that,pic_cs-410_3_4_480.jpg
cs-410_3_4_132,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:15,240","00:08:20,530",132,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=495,That means the relevant items,pic_cs-410_3_4_480.jpg
cs-410_3_4_133,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:20,530","00:08:25,160",133,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=500,And the sum that's also the average,pic_cs-410_3_4_480.jpg
cs-410_3_4_134,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:25,160","00:08:28,328",134,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=505,Where those relevant documents,pic_cs-410_3_4_480.jpg
cs-410_3_4_135,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:28,328","00:08:30,774",135,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=508,in the lower portion of the ranked.,pic_cs-410_3_4_480.jpg
cs-410_3_4_136,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:30,774","00:08:35,850",136,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=510,But from a users perspective we care,pic_cs-410_3_4_480.jpg
cs-410_3_4_137,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:35,850","00:08:39,529",137,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=515,So by taking this transformation,pic_cs-410_3_4_480.jpg
cs-410_3_4_138,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:40,650","00:08:43,920",138,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=520,Here we emphasize more on,pic_cs-410_3_4_480.jpg
cs-410_3_4_139,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:43,920","00:08:48,121",139,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=523,"You know, think about",pic_cs-410_3_4_480.jpg
cs-410_3_4_140,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:48,121","00:08:52,390",140,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=528,"it would make a big difference, in 1 over",pic_cs-410_3_4_480.jpg
cs-410_3_4_141,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:52,390","00:08:57,030",141,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=532,where and when won't make much,pic_cs-410_3_4_480.jpg
cs-410_3_4_142,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:08:57,030","00:09:01,370",142,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=537,But if you use this there will,pic_cs-410_3_4_480.jpg
cs-410_3_4_143,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:01,370","00:09:03,468",143,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=541,"let's say 1,000, right.",pic_cs-410_3_4_540.jpg
cs-410_3_4_144,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:03,468","00:09:05,000",144,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=543,So this is not the desirable.,pic_cs-410_3_4_540.jpg
cs-410_3_4_145,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:06,260","00:09:09,320",145,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=546,"On the other hand, a 1 and",pic_cs-410_3_4_540.jpg
cs-410_3_4_146,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:09,320","00:09:13,150",146,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=549,So this is yet another case where there,pic_cs-410_3_4_540.jpg
cs-410_3_4_147,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:13,150","00:09:15,840",147,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=553,thing and then you need to figure,pic_cs-410_3_4_540.jpg
cs-410_3_4_148,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:17,470","00:09:22,360",148,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=557,"So to summarize,",pic_cs-410_3_4_540.jpg
cs-410_3_4_149,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:22,360","00:09:25,738",149,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=562,Can characterize the overall,pic_cs-410_3_4_540.jpg
cs-410_3_4_150,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:25,738","00:09:30,650",150,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=565,And we emphasized that the actual,pic_cs-410_3_4_540.jpg
cs-410_3_4_151,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:30,650","00:09:34,570",151,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=570,on how many top ranked results,pic_cs-410_3_4_540.jpg
cs-410_3_4_152,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:34,570","00:09:37,000",152,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=574,Some users will examine more.,pic_cs-410_3_4_540.jpg
cs-410_3_4_153,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:37,000","00:09:38,390",153,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=577,Than others.,pic_cs-410_3_4_540.jpg
cs-410_3_4_154,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:38,390","00:09:42,100",154,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=578,An average person uses a standard measure,pic_cs-410_3_4_540.jpg
cs-410_3_4_155,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:42,100","00:09:44,837",155,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=582,It combines precision and recall and,pic_cs-410_3_4_540.jpg
cs-410_3_4_156,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:44,837","00:09:48,904",156,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=584,it's sensitive to the rank,pic_cs-410_3_4_540.jpg
cs-410_3_4_157,cs-410,3,4, Evaluation of TR Systems - Evaluating Ranked Lists - Part 2,"00:09:48,904","00:09:58,904",157,https://www.coursera.org/learn/cs-410/lecture/8Q2Tw?t=588,[MUSIC],pic_cs-410_3_4_540.jpg
cs-410_3_5_1,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:00,883","00:00:05,127",1,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=0,[MUSIC],pic_cs-410_3_5_0.jpg
cs-410_3_5_2,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:07,433","00:00:12,376",2,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=7,This lecture is about how to evaluate,pic_cs-410_3_5_0.jpg
cs-410_3_5_3,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:12,376","00:00:15,560",3,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=12,multiple levels of judgements.,pic_cs-410_3_5_0.jpg
cs-410_3_5_4,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:15,560","00:00:19,994",4,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=15,"In this lecture, we will continue",pic_cs-410_3_5_0.jpg
cs-410_3_5_5,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:19,994","00:00:23,410",5,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=19,We're going to look at how to,pic_cs-410_3_5_0.jpg
cs-410_3_5_6,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:23,410","00:00:26,150",6,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=23,when we have multiple,pic_cs-410_3_5_0.jpg
cs-410_3_5_7,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:27,760","00:00:31,180",7,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=27,So far we have talked about,pic_cs-410_3_5_0.jpg
cs-410_3_5_8,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:31,180","00:00:34,169",8,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=31,that means a document is judged as,pic_cs-410_3_5_0.jpg
cs-410_3_5_9,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:35,270","00:00:40,310",9,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=35,"But earlier, we also talk about",pic_cs-410_3_5_0.jpg
cs-410_3_5_10,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:40,310","00:00:45,580",10,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=40,So we often can distinguish,pic_cs-410_3_5_0.jpg
cs-410_3_5_11,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:45,580","00:00:50,230",11,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=45,"those are very useful documents,",pic_cs-410_3_5_0.jpg
cs-410_3_5_12,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:50,230","00:00:53,000",12,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=50,"They are okay, they are useful perhaps.",pic_cs-410_3_5_0.jpg
cs-410_3_5_13,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:53,000","00:00:56,170",13,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=53,"And further from now, we're adding",pic_cs-410_3_5_0.jpg
cs-410_3_5_14,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:00:57,450","00:01:01,490",14,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=57,So imagine you can have ratings for,pic_cs-410_3_5_0.jpg
cs-410_3_5_15,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:01,490","00:01:05,390",15,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=61,"Then, you would have",pic_cs-410_3_5_60.jpg
cs-410_3_5_16,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:05,390","00:01:10,803",16,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=65,"For example, here I show example of three",pic_cs-410_3_5_60.jpg
cs-410_3_5_17,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:10,803","00:01:15,780",17,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=70,"very relevant, 2 for marginally relevant,",pic_cs-410_3_5_60.jpg
cs-410_3_5_18,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:15,780","00:01:18,990",18,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=75,"Now, how do we evaluate the search",pic_cs-410_3_5_60.jpg
cs-410_3_5_19,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:18,990","00:01:23,330",19,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=78,"Obvious that the map doesn't work, average",pic_cs-410_3_5_60.jpg
cs-410_3_5_20,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:23,330","00:01:28,190",20,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=83,"recall doesn't work,",pic_cs-410_3_5_60.jpg
cs-410_3_5_21,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:28,190","00:01:33,510",21,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=88,So let's look at some top ranked,pic_cs-410_3_5_60.jpg
cs-410_3_5_22,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:33,510","00:01:38,518",22,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=93,Imagine the user would be mostly,pic_cs-410_3_5_60.jpg
cs-410_3_5_23,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:43,122","00:01:48,165",23,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=103,"And we marked the rating levels,",pic_cs-410_3_5_60.jpg
cs-410_3_5_24,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:48,165","00:01:54,620",24,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=108,"for these documents as shown here,",pic_cs-410_3_5_60.jpg
cs-410_3_5_25,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:54,620","00:01:57,122",25,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=114,And we call these gain.,pic_cs-410_3_5_60.jpg
cs-410_3_5_26,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:01:57,122","00:02:02,345",26,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=117,And the reason why we call it,pic_cs-410_3_5_60.jpg
cs-410_3_5_27,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:02,345","00:02:08,860",27,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=122,that we are infusing is called the NDCG,pic_cs-410_3_5_120.jpg
cs-410_3_5_28,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:10,090","00:02:14,900",28,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=130,"So this gain, basically,",pic_cs-410_3_5_120.jpg
cs-410_3_5_29,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:14,900","00:02:19,790",29,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=134,information a user can obtain by,pic_cs-410_3_5_120.jpg
cs-410_3_5_30,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:19,790","00:02:24,120",30,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=139,"So looking at the first document,",pic_cs-410_3_5_120.jpg
cs-410_3_5_31,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:24,120","00:02:28,110",31,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=144,Looking at the non-relevant document,pic_cs-410_3_5_120.jpg
cs-410_3_5_32,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:29,510","00:02:32,703",32,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=149,Looking at the moderator or,pic_cs-410_3_5_120.jpg
cs-410_3_5_33,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:32,703","00:02:35,910",33,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=152,"document the user would get 2 points,",pic_cs-410_3_5_120.jpg
cs-410_3_5_34,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:35,910","00:02:40,560",34,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=155,"So, this gain to each of the measures is",pic_cs-410_3_5_120.jpg
cs-410_3_5_35,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:40,560","00:02:41,890",35,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=160,perspective.,pic_cs-410_3_5_120.jpg
cs-410_3_5_36,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:41,890","00:02:46,140",36,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=161,"Of course, if we assume the user",pic_cs-410_3_5_120.jpg
cs-410_3_5_37,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:46,140","00:02:51,060",37,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=166,"we're looking at the cutoff at 10,",pic_cs-410_3_5_120.jpg
cs-410_3_5_38,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:51,060","00:02:51,774",38,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=171,And what's that?,pic_cs-410_3_5_120.jpg
cs-410_3_5_39,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:51,774","00:02:55,825",39,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=171,"Well, that's simply the sum of these,",pic_cs-410_3_5_120.jpg
cs-410_3_5_40,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:55,825","00:02:59,275",40,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=175,"So if the user stops after the position 1,",pic_cs-410_3_5_120.jpg
cs-410_3_5_41,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:02:59,275","00:03:03,163",41,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=179,"If the user looks at another document,",pic_cs-410_3_5_120.jpg
cs-410_3_5_42,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:03,163","00:03:08,221",42,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=183,"If the user looks at the more documents,",pic_cs-410_3_5_180.jpg
cs-410_3_5_43,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:08,221","00:03:13,200",43,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=188,Of course this is at the cost of,pic_cs-410_3_5_180.jpg
cs-410_3_5_44,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:13,200","00:03:16,390",44,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=193,So cumulative gain gives,pic_cs-410_3_5_180.jpg
cs-410_3_5_45,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:16,390","00:03:21,368",45,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=196,much total gain the user would have if,pic_cs-410_3_5_180.jpg
cs-410_3_5_46,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:21,368","00:03:28,140",46,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=201,"Now, in NDCG, we also have another letter",pic_cs-410_3_5_180.jpg
cs-410_3_5_47,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:29,170","00:03:32,060",47,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=209,"So, why do we want to do discounting?",pic_cs-410_3_5_180.jpg
cs-410_3_5_48,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:32,060","00:03:35,685",48,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=212,"Well, if you look at this cumulative gain,",pic_cs-410_3_5_180.jpg
cs-410_3_5_49,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:35,685","00:03:41,975",49,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=215,which is it did not consider the rank,pic_cs-410_3_5_180.jpg
cs-410_3_5_50,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:41,975","00:03:46,115",50,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=221,"So for example, looking at this sum here,",pic_cs-410_3_5_180.jpg
cs-410_3_5_51,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:46,115","00:03:51,485",51,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=226,and we only know there is 1,pic_cs-410_3_5_180.jpg
cs-410_3_5_52,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:51,485","00:03:54,945",52,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=231,"1 marginally relevant document,",pic_cs-410_3_5_180.jpg
cs-410_3_5_53,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:54,945","00:03:57,300",53,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=234,We don't really care,pic_cs-410_3_5_180.jpg
cs-410_3_5_54,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:03:57,300","00:04:02,110",54,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=237,"Ideally, we want these two to be ranked",pic_cs-410_3_5_180.jpg
cs-410_3_5_55,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:03,120","00:04:06,420",55,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=243,But how can we capture that intuition?,pic_cs-410_3_5_240.jpg
cs-410_3_5_56,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:06,420","00:04:13,209",56,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=246,"Well we have to say, well this is 3 here",pic_cs-410_3_5_240.jpg
cs-410_3_5_57,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:13,209","00:04:18,114",57,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=253,And that means the contribution,pic_cs-410_3_5_240.jpg
cs-410_3_5_58,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:18,114","00:04:22,750",58,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=258,positions has to be,pic_cs-410_3_5_240.jpg
cs-410_3_5_59,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:22,750","00:04:24,666",59,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=262,"And this is the idea of discounting,",pic_cs-410_3_5_240.jpg
cs-410_3_5_60,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:24,666","00:04:29,530",60,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=264,"So we're going to to say, well, the first",pic_cs-410_3_5_240.jpg
cs-410_3_5_61,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:29,530","00:04:33,910",61,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=269,because the user can be assumed,pic_cs-410_3_5_240.jpg
cs-410_3_5_62,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:33,910","00:04:38,030",62,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=273,"But the second one,",pic_cs-410_3_5_240.jpg
cs-410_3_5_63,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:38,030","00:04:42,370",63,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=278,because there's a small possibility,pic_cs-410_3_5_240.jpg
cs-410_3_5_64,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:42,370","00:04:48,690",64,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=282,So we divide this gain by,pic_cs-410_3_5_240.jpg
cs-410_3_5_65,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:48,690","00:04:52,640",65,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=288,"So log of 2,",pic_cs-410_3_5_240.jpg
cs-410_3_5_66,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:52,640","00:04:57,080",66,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=292,"And when we go to the third position,",pic_cs-410_3_5_240.jpg
cs-410_3_5_67,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:04:57,080","00:05:01,270",67,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=297,"because the normalizer is log of 3,",pic_cs-410_3_5_240.jpg
cs-410_3_5_68,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:01,270","00:05:06,690",68,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=301,So when we take such a sum that a lower,pic_cs-410_3_5_300.jpg
cs-410_3_5_69,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:06,690","00:05:10,000",69,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=306,that much as a highly ranked document.,pic_cs-410_3_5_300.jpg
cs-410_3_5_70,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:10,000","00:05:15,120",70,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=310,"So that means if you, for example,",pic_cs-410_3_5_300.jpg
cs-410_3_5_71,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:15,120","00:05:20,726",71,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=315,"this position, and this one, and then",pic_cs-410_3_5_300.jpg
cs-410_3_5_72,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:20,726","00:05:27,050",72,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=320,for example very relevant,pic_cs-410_3_5_300.jpg
cs-410_3_5_73,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:27,050","00:05:31,290",73,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=327,"Imagine if you put the 3 here,",pic_cs-410_3_5_300.jpg
cs-410_3_5_74,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:31,290","00:05:34,635",74,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=331,So it's not as good as if,pic_cs-410_3_5_300.jpg
cs-410_3_5_75,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:34,635","00:05:36,650",75,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=334,So this is the idea of discounting.,pic_cs-410_3_5_300.jpg
cs-410_3_5_76,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:37,900","00:05:43,210",76,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=337,"Okay, so now at this point that we have",pic_cs-410_3_5_300.jpg
cs-410_3_5_77,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:43,210","00:05:50,000",77,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=343,measuring the utility of this ranked,pic_cs-410_3_5_300.jpg
cs-410_3_5_78,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:51,480","00:05:53,125",78,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=351,So are we happy with this?,pic_cs-410_3_5_300.jpg
cs-410_3_5_79,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:53,125","00:05:55,680",79,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=353,"Well, we can use this to rank systems.",pic_cs-410_3_5_300.jpg
cs-410_3_5_80,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:55,680","00:05:58,510",80,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=355,"Now, we still need to do a little bit more",pic_cs-410_3_5_300.jpg
cs-410_3_5_81,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:05:58,510","00:06:03,272",81,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=358,in order to make this measure,pic_cs-410_3_5_300.jpg
cs-410_3_5_82,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:03,272","00:06:10,580",82,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=363,"And this is the last step, and by the way,",pic_cs-410_3_5_360.jpg
cs-410_3_5_83,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:10,580","00:06:16,820",83,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=370,"so this is the total sum of DCG,",pic_cs-410_3_5_360.jpg
cs-410_3_5_84,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:16,820","00:06:20,880",84,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=376,"So the last step is called N,",pic_cs-410_3_5_360.jpg
cs-410_3_5_85,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:20,880","00:06:25,240",85,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=380,"And if we do that,",pic_cs-410_3_5_360.jpg
cs-410_3_5_86,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:25,240","00:06:26,463",86,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=385,So how do we do that?,pic_cs-410_3_5_360.jpg
cs-410_3_5_87,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:26,463","00:06:31,241",87,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=386,"Well, the idea here is we're",pic_cs-410_3_5_360.jpg
cs-410_3_5_88,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:31,241","00:06:35,280",88,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=391,the ideal DCG at the same cutoff.,pic_cs-410_3_5_360.jpg
cs-410_3_5_89,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:35,280","00:06:37,130",89,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=395,What is the ideal DCG?,pic_cs-410_3_5_360.jpg
cs-410_3_5_90,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:37,130","00:06:40,830",90,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=397,"Well, this is the DCG of an ideal ranking.",pic_cs-410_3_5_360.jpg
cs-410_3_5_91,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:40,830","00:06:47,690",91,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=400,So imagine if we have 9 documents in,pic_cs-410_3_5_360.jpg
cs-410_3_5_92,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:47,690","00:06:52,840",92,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=407,And that means in total we,pic_cs-410_3_5_360.jpg
cs-410_3_5_93,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:06:53,840","00:07:00,640",93,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=413,Then our ideal rank lister would have put,pic_cs-410_3_5_360.jpg
cs-410_3_5_94,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:00,640","00:07:05,730",94,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=420,So all these would have to be 3 and,pic_cs-410_3_5_420.jpg
cs-410_3_5_95,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:05,730","00:07:10,040",95,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=425,Because that's the best we could,pic_cs-410_3_5_420.jpg
cs-410_3_5_96,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:10,040","00:07:11,800",96,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=430,But all these positions would be 3.,pic_cs-410_3_5_420.jpg
cs-410_3_5_97,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:11,800","00:07:13,938",97,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=431,Right?,pic_cs-410_3_5_420.jpg
cs-410_3_5_98,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:13,938","00:07:16,090",98,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=433,So this would our ideal ranked list.,pic_cs-410_3_5_420.jpg
cs-410_3_5_99,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:18,070","00:07:21,700",99,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=438,And then we had computed the DCG for,pic_cs-410_3_5_420.jpg
cs-410_3_5_100,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:23,250","00:07:27,062",100,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=443,So this would be given by this,pic_cs-410_3_5_420.jpg
cs-410_3_5_101,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:27,062","00:07:35,723",101,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=447,And so this ideal DCG would then,pic_cs-410_3_5_420.jpg
cs-410_3_5_102,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:35,723","00:07:36,845",102,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=455,So here.,pic_cs-410_3_5_420.jpg
cs-410_3_5_103,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:36,845","00:07:40,040",103,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=456,And this idea of DCG would,pic_cs-410_3_5_420.jpg
cs-410_3_5_104,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:40,040","00:07:43,726",104,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=460,"So you can imagine now,",pic_cs-410_3_5_420.jpg
cs-410_3_5_105,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:43,726","00:07:49,590",105,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=463,compare the actual DCG with the best DCG,pic_cs-410_3_5_420.jpg
cs-410_3_5_106,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:49,590","00:07:51,146",106,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=469,Now why do we want to do this?,pic_cs-410_3_5_420.jpg
cs-410_3_5_107,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:51,146","00:07:56,590",107,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=471,"Well, by doing this we'll map the DCG",pic_cs-410_3_5_420.jpg
cs-410_3_5_108,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:07:57,900","00:08:01,650",108,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=477,"So the best value, or the highest value,",pic_cs-410_3_5_420.jpg
cs-410_3_5_109,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:01,650","00:08:07,500",109,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=481,"That's when your rank list is,",pic_cs-410_3_5_480.jpg
cs-410_3_5_110,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:07,500","00:08:12,260",110,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=487,"otherwise, in general,",pic_cs-410_3_5_480.jpg
cs-410_3_5_111,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:13,405","00:08:14,954",111,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=493,"Now, what if we don't do that?",pic_cs-410_3_5_480.jpg
cs-410_3_5_112,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:14,954","00:08:19,737",112,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=494,"Well, you can see, this transformation,",pic_cs-410_3_5_480.jpg
cs-410_3_5_113,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:19,737","00:08:24,108",113,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=499,doesn't really affect the relative,pic_cs-410_3_5_480.jpg
cs-410_3_5_114,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:24,108","00:08:29,053",114,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=504,"just one topic, because this ideal",pic_cs-410_3_5_480.jpg
cs-410_3_5_115,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:29,053","00:08:33,834",115,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=509,so the ranking of systems based on,pic_cs-410_3_5_480.jpg
cs-410_3_5_116,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:33,834","00:08:36,986",116,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=513,if you rank them based,pic_cs-410_3_5_480.jpg
cs-410_3_5_117,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:36,986","00:08:40,760",117,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=516,The difference however is,pic_cs-410_3_5_480.jpg
cs-410_3_5_118,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:40,760","00:08:42,894",118,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=520,"Because if we don't do normalization,",pic_cs-410_3_5_480.jpg
cs-410_3_5_119,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:42,894","00:08:45,730",119,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=522,different topics will have,pic_cs-410_3_5_480.jpg
cs-410_3_5_120,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:46,740","00:08:51,951",120,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=526,"For a topic like this one,",pic_cs-410_3_5_480.jpg
cs-410_3_5_121,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:51,951","00:08:56,593",121,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=531,"the DCG can get really high,",pic_cs-410_3_5_480.jpg
cs-410_3_5_122,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:08:56,593","00:09:02,794",122,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=536,there are only two very relevant documents,pic_cs-410_3_5_480.jpg
cs-410_3_5_123,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:02,794","00:09:06,124",123,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=542,Then the highest DCG that,pic_cs-410_3_5_540.jpg
cs-410_3_5_124,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:06,124","00:09:09,210",124,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=546,such a topic would not be very high.,pic_cs-410_3_5_540.jpg
cs-410_3_5_125,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:09,210","00:09:15,555",125,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=549,"So again, we face the problem of",pic_cs-410_3_5_540.jpg
cs-410_3_5_126,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:15,555","00:09:17,028",126,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=555,"When we take an average,",pic_cs-410_3_5_540.jpg
cs-410_3_5_127,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:17,028","00:09:20,826",127,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=557,we don't want the average to be,pic_cs-410_3_5_540.jpg
cs-410_3_5_128,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:20,826","00:09:23,360",128,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=560,"Those are, again, easy queries.",pic_cs-410_3_5_540.jpg
cs-410_3_5_129,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:23,360","00:09:27,220",129,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=563,"So, by doing the normalization,",pic_cs-410_3_5_540.jpg
cs-410_3_5_130,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:27,220","00:09:31,690",130,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=567,making all the queries contribute,pic_cs-410_3_5_540.jpg
cs-410_3_5_131,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:31,690","00:09:34,882",131,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=571,"So, this is a idea of NDCG, it's used for",pic_cs-410_3_5_540.jpg
cs-410_3_5_132,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:34,882","00:09:40,470",132,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=574,measuring a rank list based on multiple,pic_cs-410_3_5_540.jpg
cs-410_3_5_133,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:42,830","00:09:47,951",133,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=582,In a more general way this,pic_cs-410_3_5_540.jpg
cs-410_3_5_134,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:47,951","00:09:55,900",134,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=587,that can be applied to any ranked task,pic_cs-410_3_5_540.jpg
cs-410_3_5_135,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:09:55,900","00:10:01,111",135,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=595,And the scale of the judgements,pic_cs-410_3_5_540.jpg
cs-410_3_5_136,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:01,111","00:10:07,094",136,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=601,binary not only more than binary they,pic_cs-410_3_5_600.jpg
cs-410_3_5_137,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:07,094","00:10:11,365",137,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=607,"0, 5 or",pic_cs-410_3_5_600.jpg
cs-410_3_5_138,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:11,365","00:10:15,631",138,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=611,"And the main idea of this measure,",pic_cs-410_3_5_600.jpg
cs-410_3_5_139,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:15,631","00:10:19,920",139,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=615,is to measure the total utility,pic_cs-410_3_5_600.jpg
cs-410_3_5_140,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:19,920","00:10:24,120",140,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=619,So you always choose a cutoff and,pic_cs-410_3_5_600.jpg
cs-410_3_5_141,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:24,120","00:10:28,700",141,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=624,And it would discount the contribution,pic_cs-410_3_5_600.jpg
cs-410_3_5_142,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:28,700","00:10:31,811",142,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=628,"And then finally,",pic_cs-410_3_5_600.jpg
cs-410_3_5_143,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:31,811","00:10:37,645",143,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=631,it would do normalization to ensure,pic_cs-410_3_5_600.jpg
cs-410_3_5_144,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:37,645","00:10:43,093",144,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=637,comparability across queries.,pic_cs-410_3_5_600.jpg
cs-410_3_5_145,cs-410,3,5, Evaluation of TR Systems - Multi-Level Judgements,"00:10:43,093","00:10:48,319",145,https://www.coursera.org/learn/cs-410/lecture/uGa00?t=643,[MUSIC],pic_cs-410_3_5_600.jpg
cs-410_3_6_1,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:00,004","00:00:06,485",1,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=0,[SOUND].,pic_cs-410_3_6_0.jpg
cs-410_3_6_2,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:06,485","00:00:10,694",2,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=6,This lecture is about some practical,pic_cs-410_3_6_0.jpg
cs-410_3_6_3,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:10,694","00:00:12,939",3,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=10,evaluation of text retrieval systems.,pic_cs-410_3_6_0.jpg
cs-410_3_6_4,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:14,440","00:00:17,730",4,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=14,"In this lecture, we will continue",pic_cs-410_3_6_0.jpg
cs-410_3_6_5,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:17,730","00:00:21,250",5,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=17,We'll cover some practical,pic_cs-410_3_6_0.jpg
cs-410_3_6_6,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:21,250","00:00:24,240",6,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=21,in actual evaluation of,pic_cs-410_3_6_0.jpg
cs-410_3_6_7,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:25,500","00:00:29,060",7,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=25,"So, in order to create",pic_cs-410_3_6_0.jpg
cs-410_3_6_8,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:29,060","00:00:31,540",8,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=29,we have to create a set of queries.,pic_cs-410_3_6_0.jpg
cs-410_3_6_9,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:31,540","00:00:34,270",9,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=31,A set of documents and,pic_cs-410_3_6_0.jpg
cs-410_3_6_10,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:35,750","00:00:39,680",10,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=35,It turns out that each is,pic_cs-410_3_6_0.jpg
cs-410_3_6_11,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:39,680","00:00:43,240",11,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=39,"First, the documents and",pic_cs-410_3_6_0.jpg
cs-410_3_6_12,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:43,240","00:00:47,250",12,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=43,They must represent the real queries and,pic_cs-410_3_6_0.jpg
cs-410_3_6_13,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:48,290","00:00:50,990",13,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=48,And we also have to use many queries and,pic_cs-410_3_6_0.jpg
cs-410_3_6_14,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:50,990","00:00:55,010",14,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=50,many documents in order to,pic_cs-410_3_6_0.jpg
cs-410_3_6_15,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:00:56,470","00:01:02,560",15,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=56,For the matching of relevant,pic_cs-410_3_6_0.jpg
cs-410_3_6_16,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:02,560","00:01:10,050",16,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=62,We also need to ensure that there exists a,pic_cs-410_3_6_60.jpg
cs-410_3_6_17,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:10,050","00:01:13,900",17,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=70,"If a query has only one, that's",pic_cs-410_3_6_60.jpg
cs-410_3_6_18,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:13,900","00:01:18,300",18,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=73,It's not very informative to,pic_cs-410_3_6_60.jpg
cs-410_3_6_19,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:18,300","00:01:23,120",19,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=78,using such a query because there's not,pic_cs-410_3_6_60.jpg
cs-410_3_6_20,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:23,120","00:01:27,390",20,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=83,"So ideally, there should be more",pic_cs-410_3_6_60.jpg
cs-410_3_6_21,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:27,390","00:01:30,469",21,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=87,the queries also should represent,pic_cs-410_3_6_60.jpg
cs-410_3_6_22,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:31,470","00:01:35,240",22,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=91,"In terms of relevance judgments,",pic_cs-410_3_6_60.jpg
cs-410_3_6_23,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:35,240","00:01:38,970",23,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=95,complete judgments of all,pic_cs-410_3_6_60.jpg
cs-410_3_6_24,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:38,970","00:01:40,670",24,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=98,"Yet, minimizing human and",pic_cs-410_3_6_60.jpg
cs-410_3_6_25,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:40,670","00:01:44,980",25,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=100,"fault, because we have to use human",pic_cs-410_3_6_60.jpg
cs-410_3_6_26,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:44,980","00:01:47,690",26,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=104,It's very labor intensive.,pic_cs-410_3_6_60.jpg
cs-410_3_6_27,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:47,690","00:01:52,550",27,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=107,"And as a result, it's impossible to",pic_cs-410_3_6_60.jpg
cs-410_3_6_28,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:52,550","00:01:57,170",28,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=112,"all the queries, especially considering",pic_cs-410_3_6_60.jpg
cs-410_3_6_29,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:01:58,750","00:02:03,590",29,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=118,"So this is actually a major challenge,",pic_cs-410_3_6_60.jpg
cs-410_3_6_30,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:03,590","00:02:07,160",30,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=123,"For measures, it's also challenging,",pic_cs-410_3_6_120.jpg
cs-410_3_6_31,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:07,160","00:02:11,680",31,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=127,accurately reflect,pic_cs-410_3_6_120.jpg
cs-410_3_6_32,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:11,680","00:02:15,430",32,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=131,We have to consider carefully,pic_cs-410_3_6_120.jpg
cs-410_3_6_33,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:15,430","00:02:18,530",33,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=135,And then design measures to measure that.,pic_cs-410_3_6_120.jpg
cs-410_3_6_34,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:18,530","00:02:21,482",34,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=138,If your measure is not,pic_cs-410_3_6_120.jpg
cs-410_3_6_35,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:21,482","00:02:23,820",35,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=141,then your conclusion would be misled.,pic_cs-410_3_6_120.jpg
cs-410_3_6_36,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:23,820","00:02:25,040",36,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=143,So it's very important.,pic_cs-410_3_6_120.jpg
cs-410_3_6_37,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:26,880","00:02:29,290",37,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=146,So we're going to talk about,pic_cs-410_3_6_120.jpg
cs-410_3_6_38,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:29,290","00:02:31,360",38,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=149,One is the statistical significance test.,pic_cs-410_3_6_120.jpg
cs-410_3_6_39,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:31,360","00:02:36,350",39,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=151,And this also is a reason why,pic_cs-410_3_6_120.jpg
cs-410_3_6_40,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:36,350","00:02:41,060",40,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=156,And the question here is how sure can,pic_cs-410_3_6_120.jpg
cs-410_3_6_41,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:41,060","00:02:44,800",41,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=161,doesn't simply result from,pic_cs-410_3_6_120.jpg
cs-410_3_6_42,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:44,800","00:02:49,770",42,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=164,So here are some sample results of,pic_cs-410_3_6_120.jpg
cs-410_3_6_43,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:49,770","00:02:53,320",43,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=169,System B into different experiments.,pic_cs-410_3_6_120.jpg
cs-410_3_6_44,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:53,320","00:02:57,540",44,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=173,"And you can see in the bottom,",pic_cs-410_3_6_120.jpg
cs-410_3_6_45,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:02:57,540","00:03:02,668",45,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=177,"So the mean, if you look at the mean",pic_cs-410_3_6_120.jpg
cs-410_3_6_46,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:02,668","00:03:08,300",46,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=182,of positions are exactly the same,pic_cs-410_3_6_180.jpg
cs-410_3_6_47,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:08,300","00:03:13,250",47,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=188,"So you can see this is 0.20,",pic_cs-410_3_6_180.jpg
cs-410_3_6_48,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:13,250","00:03:18,520",48,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=193,And again here it's also 0.20 and,pic_cs-410_3_6_180.jpg
cs-410_3_6_49,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:18,520","00:03:23,440",49,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=198,"Yet, if you look at these exact average",pic_cs-410_3_6_180.jpg
cs-410_3_6_50,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:23,440","00:03:29,810",50,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=203,"If you look at these numbers in detail,",pic_cs-410_3_6_180.jpg
cs-410_3_6_51,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:29,810","00:03:35,090",51,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=209,you would feel that you can trust,pic_cs-410_3_6_180.jpg
cs-410_3_6_52,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:36,100","00:03:41,610",52,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=216,"In the another case, in the other case,",pic_cs-410_3_6_180.jpg
cs-410_3_6_53,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:41,610","00:03:48,470",53,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=221,"So, why don't you take a look at all these",pic_cs-410_3_6_180.jpg
cs-410_3_6_54,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:48,470","00:03:52,565",54,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=228,"So, if you look at the average,",pic_cs-410_3_6_180.jpg
cs-410_3_6_55,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:52,565","00:03:56,660",55,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=232,"we can easily, say that well,",pic_cs-410_3_6_180.jpg
cs-410_3_6_56,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:56,660","00:03:59,630",56,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=236,"So, after all it's 0.40 and",pic_cs-410_3_6_180.jpg
cs-410_3_6_57,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:03:59,630","00:04:05,950",57,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=239,"this is twice as much as 0.20,",pic_cs-410_3_6_180.jpg
cs-410_3_6_58,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:05,950","00:04:10,120",58,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=245,"But if you look at these two experiments,",pic_cs-410_3_6_240.jpg
cs-410_3_6_59,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:11,150","00:04:16,170",59,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=251,"You will see that, we've been more",pic_cs-410_3_6_240.jpg
cs-410_3_6_60,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:16,170","00:04:17,040",60,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=256,in experiment one.,pic_cs-410_3_6_240.jpg
cs-410_3_6_61,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:17,040","00:04:19,160",61,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=257,In this case.,pic_cs-410_3_6_240.jpg
cs-410_3_6_62,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:19,160","00:04:23,360",62,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=259,Because these numbers seem to be,pic_cs-410_3_6_240.jpg
cs-410_3_6_63,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:25,110","00:04:32,342",63,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=265,"Whereas in Experiment 2, we're not sure",pic_cs-410_3_6_240.jpg
cs-410_3_6_64,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:32,342","00:04:38,000",64,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=272,after System A is better and,pic_cs-410_3_6_240.jpg
cs-410_3_6_65,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:39,335","00:04:43,790",65,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=279,"But yet if we look at only average,",pic_cs-410_3_6_240.jpg
cs-410_3_6_66,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:45,750","00:04:47,640",66,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=285,"So, what do you think?",pic_cs-410_3_6_240.jpg
cs-410_3_6_67,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:49,170","00:04:54,150",67,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=289,"How reliable is our conclusion,",pic_cs-410_3_6_240.jpg
cs-410_3_6_68,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:04:55,940","00:04:59,430",68,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=295,"Now in this case, intuitively,",pic_cs-410_3_6_240.jpg
cs-410_3_6_69,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:01,020","00:05:04,630",69,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=301,But how can we quantitate,pic_cs-410_3_6_300.jpg
cs-410_3_6_70,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:04,630","00:05:08,430",70,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=304,And this is why we need to do,pic_cs-410_3_6_300.jpg
cs-410_3_6_71,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:09,440","00:05:13,910",71,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=309,"So, the idea of the statistical",pic_cs-410_3_6_300.jpg
cs-410_3_6_72,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:13,910","00:05:18,330",72,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=313,assess the variants across,pic_cs-410_3_6_300.jpg
cs-410_3_6_73,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:18,330","00:05:21,160",73,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=318,"If there is a big variance,",pic_cs-410_3_6_300.jpg
cs-410_3_6_74,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:21,160","00:05:25,880",74,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=321,that means the results could fluctuate,pic_cs-410_3_6_300.jpg
cs-410_3_6_75,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:25,880","00:05:30,740",75,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=325,"Then we should believe that,",pic_cs-410_3_6_300.jpg
cs-410_3_6_76,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:30,740","00:05:35,210",76,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=330,the results might change if we,pic_cs-410_3_6_300.jpg
cs-410_3_6_77,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:35,210","00:05:39,350",77,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=335,"Right, so this is then not so",pic_cs-410_3_6_300.jpg
cs-410_3_6_78,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:39,350","00:05:42,660",78,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=339,if you have c high variance,pic_cs-410_3_6_300.jpg
cs-410_3_6_79,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:43,660","00:05:49,390",79,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=343,So let's look at these results,pic_cs-410_3_6_300.jpg
cs-410_3_6_80,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:49,390","00:05:54,200",80,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=349,"So, here we show two different",pic_cs-410_3_6_300.jpg
cs-410_3_6_81,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:54,200","00:05:57,470",81,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=354,One is a sign test where,pic_cs-410_3_6_300.jpg
cs-410_3_6_82,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:05:57,470","00:06:01,260",82,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=357,"If System B is better than System A,",pic_cs-410_3_6_300.jpg
cs-410_3_6_83,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:01,260","00:06:05,400",83,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=361,When System A is better we,pic_cs-410_3_6_360.jpg
cs-410_3_6_84,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:05,400","00:06:09,600",84,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=365,"Using this case, if you see this,",pic_cs-410_3_6_360.jpg
cs-410_3_6_85,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:09,600","00:06:12,980",85,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=369,We actually have four cases,pic_cs-410_3_6_360.jpg
cs-410_3_6_86,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:12,980","00:06:16,671",86,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=372,"But three cases of System A is better,",pic_cs-410_3_6_360.jpg
cs-410_3_6_87,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:16,671","00:06:19,880",87,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=376,"this is almost like a random results,",pic_cs-410_3_6_360.jpg
cs-410_3_6_88,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:19,880","00:06:25,880",88,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=379,So if you just take a random,pic_cs-410_3_6_360.jpg
cs-410_3_6_89,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:25,880","00:06:30,090",89,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=385,if you use plus to denote the head and,pic_cs-410_3_6_360.jpg
cs-410_3_6_90,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:30,090","00:06:34,920",90,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=390,that could easily be the results of just,pic_cs-410_3_6_360.jpg
cs-410_3_6_91,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:34,920","00:06:39,700",91,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=394,"So, the fact that the average is",pic_cs-410_3_6_360.jpg
cs-410_3_6_92,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:39,700","00:06:41,330",92,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=399,We can't reliably conclude that.,pic_cs-410_3_6_360.jpg
cs-410_3_6_93,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:41,330","00:06:45,890",93,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=401,And this can be quantitatively,pic_cs-410_3_6_360.jpg
cs-410_3_6_94,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:45,890","00:06:48,380",94,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=405,And that basically means,pic_cs-410_3_6_360.jpg
cs-410_3_6_95,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:49,660","00:06:54,480",95,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=409,the probability that this result is,pic_cs-410_3_6_360.jpg
cs-410_3_6_96,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:54,480","00:06:56,140",96,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=414,"In this case, probability is 1.0.",pic_cs-410_3_6_360.jpg
cs-410_3_6_97,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:06:56,140","00:07:00,050",97,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=416,It means it surely is,pic_cs-410_3_6_360.jpg
cs-410_3_6_98,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:01,310","00:07:06,470",98,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=421,"Now in Willcoxan test,",pic_cs-410_3_6_420.jpg
cs-410_3_6_99,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:06,470","00:07:09,430",99,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=426,and we would be not only,pic_cs-410_3_6_420.jpg
cs-410_3_6_100,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:09,430","00:07:12,520",100,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=429,we'll be also looking at,pic_cs-410_3_6_420.jpg
cs-410_3_6_101,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:12,520","00:07:14,690",101,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=432,"But we can draw a similar conclusion,",pic_cs-410_3_6_420.jpg
cs-410_3_6_102,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:14,690","00:07:18,630",102,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=434,where you say it's very,pic_cs-410_3_6_420.jpg
cs-410_3_6_103,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:18,630","00:07:22,395",103,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=438,"To illustrate this, let's think",pic_cs-410_3_6_420.jpg
cs-410_3_6_104,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:22,395","00:07:23,895",104,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=442,And this is called a now distribution.,pic_cs-410_3_6_420.jpg
cs-410_3_6_105,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:23,895","00:07:26,085",105,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=443,We assume that the mean is zero here.,pic_cs-410_3_6_420.jpg
cs-410_3_6_106,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:26,085","00:07:28,705",106,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=446,Lets say we started with,pic_cs-410_3_6_420.jpg
cs-410_3_6_107,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:28,705","00:07:31,405",107,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=448,no difference between the two systems.,pic_cs-410_3_6_420.jpg
cs-410_3_6_108,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:31,405","00:07:35,230",108,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=451,But we assume that because of random,pic_cs-410_3_6_420.jpg
cs-410_3_6_109,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:35,230","00:07:37,190",109,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=455,we might observe a difference.,pic_cs-410_3_6_420.jpg
cs-410_3_6_110,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:37,190","00:07:41,300",110,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=457,So the actual difference might,pic_cs-410_3_6_420.jpg
cs-410_3_6_111,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:41,300","00:07:42,830",111,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=461,"on the right side here, right?",pic_cs-410_3_6_420.jpg
cs-410_3_6_112,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:43,920","00:07:48,102",112,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=463,"So, and this curve kind of shows",pic_cs-410_3_6_420.jpg
cs-410_3_6_113,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:48,102","00:07:52,290",113,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=468,actually observe values that,pic_cs-410_3_6_420.jpg
cs-410_3_6_114,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:07:53,770","00:07:59,440",114,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=473,"Now, so if we look at this picture then,",pic_cs-410_3_6_420.jpg
cs-410_3_6_115,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:01,070","00:08:05,530",115,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=481,"if a difference is observed here, then",pic_cs-410_3_6_480.jpg
cs-410_3_6_116,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:05,530","00:08:11,180",116,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=485,the chance is very high that this is,pic_cs-410_3_6_480.jpg
cs-410_3_6_117,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:11,180","00:08:16,150",117,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=491,We can define a region of,pic_cs-410_3_6_480.jpg
cs-410_3_6_118,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:16,150","00:08:21,890",118,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=496,random fluctuation and,pic_cs-410_3_6_480.jpg
cs-410_3_6_119,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:21,890","00:08:27,894",119,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=501,And in this then the observed may,pic_cs-410_3_6_480.jpg
cs-410_3_6_120,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:28,960","00:08:34,830",120,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=508,But if you observe a value in this,pic_cs-410_3_6_480.jpg
cs-410_3_6_121,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:34,830","00:08:39,880",121,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=514,then the difference is unlikely,pic_cs-410_3_6_480.jpg
cs-410_3_6_122,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:39,880","00:08:44,460",122,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=519,"All right, so there's a very small",pic_cs-410_3_6_480.jpg
cs-410_3_6_123,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:44,460","00:08:47,400",123,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=524,such a difference just because,pic_cs-410_3_6_480.jpg
cs-410_3_6_124,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:48,400","00:08:52,800",124,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=528,"So in that case, we can then conclude",pic_cs-410_3_6_480.jpg
cs-410_3_6_125,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:52,800","00:08:54,670",125,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=532,So System B is indeed better.,pic_cs-410_3_6_480.jpg
cs-410_3_6_126,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:56,120","00:08:59,550",126,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=536,So this is the idea of,pic_cs-410_3_6_480.jpg
cs-410_3_6_127,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:08:59,550","00:09:03,870",127,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=539,The takeaway message here is that you,pic_cs-410_3_6_480.jpg
cs-410_3_6_128,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:03,870","00:09:05,770",128,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=543,jumping into a conclusion.,pic_cs-410_3_6_540.jpg
cs-410_3_6_129,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:05,770","00:09:08,330",129,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=545,"As in this case,",pic_cs-410_3_6_540.jpg
cs-410_3_6_130,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:09,790","00:09:13,259",130,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=549,There are many different ways of doing,pic_cs-410_3_6_540.jpg
cs-410_3_6_131,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:15,260","00:09:20,270",131,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=555,"So now, let's talk about the other",pic_cs-410_3_6_540.jpg
cs-410_3_6_132,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:20,270","00:09:24,590",132,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=560,"as we said earlier,",pic_cs-410_3_6_540.jpg
cs-410_3_6_133,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:24,590","00:09:27,700",133,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=564,completely unless it's,pic_cs-410_3_6_540.jpg
cs-410_3_6_134,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:27,700","00:09:31,530",134,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=567,"So the question is,",pic_cs-410_3_6_540.jpg
cs-410_3_6_135,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:31,530","00:09:33,880",135,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=571,"in the collection,",pic_cs-410_3_6_540.jpg
cs-410_3_6_136,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:35,000","00:09:38,230",136,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=575,And the solution here is Pooling.,pic_cs-410_3_6_540.jpg
cs-410_3_6_137,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:38,230","00:09:45,640",137,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=578,And this is a strategy that has been used,pic_cs-410_3_6_540.jpg
cs-410_3_6_138,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:46,710","00:09:49,800",138,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=586,So the idea of Pooling is the following.,pic_cs-410_3_6_540.jpg
cs-410_3_6_139,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:49,800","00:09:54,410",139,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=589,We would first choose a diverse,pic_cs-410_3_6_540.jpg
cs-410_3_6_140,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:54,410","00:09:56,010",140,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=594,These are Text Retrieval systems.,pic_cs-410_3_6_540.jpg
cs-410_3_6_141,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:09:57,130","00:10:02,830",141,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=597,And we hope these methods can help us,pic_cs-410_3_6_540.jpg
cs-410_3_6_142,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:02,830","00:10:05,400",142,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=602,So the goal is to pick out,pic_cs-410_3_6_600.jpg
cs-410_3_6_143,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:05,400","00:10:08,770",143,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=605,We want to make judgements on relevant,pic_cs-410_3_6_600.jpg
cs-410_3_6_144,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:08,770","00:10:12,720",144,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=608,useful documents from users perspectives.,pic_cs-410_3_6_600.jpg
cs-410_3_6_145,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:12,720","00:10:16,339",145,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=612,So then we're going to have,pic_cs-410_3_6_600.jpg
cs-410_3_6_146,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:17,380","00:10:19,720",146,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=617,The K can vary from systems.,pic_cs-410_3_6_600.jpg
cs-410_3_6_147,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:19,720","00:10:24,370",147,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=619,But the point is to ask them to suggest,pic_cs-410_3_6_600.jpg
cs-410_3_6_148,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:25,530","00:10:29,780",148,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=625,And then we simply combine,pic_cs-410_3_6_600.jpg
cs-410_3_6_149,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:29,780","00:10:34,478",149,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=629,to form a pool of documents for,pic_cs-410_3_6_600.jpg
cs-410_3_6_150,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:34,478","00:10:41,370",150,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=634,"To judge, so imagine you have many",pic_cs-410_3_6_600.jpg
cs-410_3_6_151,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:41,370","00:10:44,498",151,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=641,"We take the top-K documents,",pic_cs-410_3_6_600.jpg
cs-410_3_6_152,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:44,498","00:10:48,060",152,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=644,"Now, of course, there are many",pic_cs-410_3_6_600.jpg
cs-410_3_6_153,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:48,060","00:10:51,860",153,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=648,many systems might have retrieved,pic_cs-410_3_6_600.jpg
cs-410_3_6_154,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:51,860","00:10:55,250",154,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=651,So there will be some duplicate documents.,pic_cs-410_3_6_600.jpg
cs-410_3_6_155,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:10:56,480","00:11:00,690",155,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=656,And there are also unique documents,pic_cs-410_3_6_600.jpg
cs-410_3_6_156,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:00,690","00:11:03,490",156,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=660,So the idea of having diverse,pic_cs-410_3_6_660.jpg
cs-410_3_6_157,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:03,490","00:11:07,470",157,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=663,set of ranking methods is to,pic_cs-410_3_6_660.jpg
cs-410_3_6_158,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:07,470","00:11:11,140",158,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=667,And can include as many possible,pic_cs-410_3_6_660.jpg
cs-410_3_6_159,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:12,360","00:11:17,180",159,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=672,"And then, the users would,",pic_cs-410_3_6_660.jpg
cs-410_3_6_160,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:17,180","00:11:21,250",160,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=677,"the judgments on this data set, this pool.",pic_cs-410_3_6_660.jpg
cs-410_3_6_161,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:21,250","00:11:26,710",161,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=681,And the other unjudged the documents are,pic_cs-410_3_6_660.jpg
cs-410_3_6_162,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:26,710","00:11:30,900",162,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=686,"Now if the pool is large enough,",pic_cs-410_3_6_660.jpg
cs-410_3_6_163,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:32,080","00:11:38,600",163,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=692,"But if the pool is not very large,",pic_cs-410_3_6_660.jpg
cs-410_3_6_164,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:38,600","00:11:41,190",164,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=698,And we might use other,pic_cs-410_3_6_660.jpg
cs-410_3_6_165,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:41,190","00:11:46,100",165,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=701,there are indeed other,pic_cs-410_3_6_660.jpg
cs-410_3_6_166,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:46,100","00:11:49,840",166,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=706,And such a strategy is generally okay for,pic_cs-410_3_6_660.jpg
cs-410_3_6_167,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:49,840","00:11:54,740",167,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=709,comparing systems that,pic_cs-410_3_6_660.jpg
cs-410_3_6_168,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:54,740","00:11:57,740",168,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=714,That means if you participate,pic_cs-410_3_6_660.jpg
cs-410_3_6_169,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:11:57,740","00:12:00,850",169,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=717,then it's unlikely that it,pic_cs-410_3_6_660.jpg
cs-410_3_6_170,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:00,850","00:12:03,100",170,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=720,because the problematic,pic_cs-410_3_6_720.jpg
cs-410_3_6_171,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:04,300","00:12:07,060",171,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=724,"However, this is problematic for",pic_cs-410_3_6_720.jpg
cs-410_3_6_172,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:07,060","00:12:11,880",172,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=727,evaluating a new system that may,pic_cs-410_3_6_720.jpg
cs-410_3_6_173,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:11,880","00:12:16,010",173,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=731,"In this case, a new system might",pic_cs-410_3_6_720.jpg
cs-410_3_6_174,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:16,010","00:12:20,850",174,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=736,nominated some read only documents,pic_cs-410_3_6_720.jpg
cs-410_3_6_175,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:20,850","00:12:24,370",175,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=740,So those documents might be,pic_cs-410_3_6_720.jpg
cs-410_3_6_176,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:24,370","00:12:26,150",176,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=744,That's unfair.,pic_cs-410_3_6_720.jpg
cs-410_3_6_177,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:26,150","00:12:32,810",177,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=746,So to summarize the whole part of textual,pic_cs-410_3_6_720.jpg
cs-410_3_6_178,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:32,810","00:12:37,150",178,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=752,Because the problem is the empirically,pic_cs-410_3_6_720.jpg
cs-410_3_6_179,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:38,450","00:12:42,470",179,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=758,"don't rely on users, there's no way to",pic_cs-410_3_6_720.jpg
cs-410_3_6_180,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:43,580","00:12:46,600",180,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=763,If we have in the property,pic_cs-410_3_6_720.jpg
cs-410_3_6_181,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:46,600","00:12:49,710",181,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=766,we might misguide our research or,pic_cs-410_3_6_720.jpg
cs-410_3_6_182,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:49,710","00:12:52,470",182,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=769,And we might just draw wrong conclusions.,pic_cs-410_3_6_720.jpg
cs-410_3_6_183,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:52,470","00:12:55,250",183,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=772,And we have seen this is,pic_cs-410_3_6_720.jpg
cs-410_3_6_184,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:12:55,250","00:12:58,190",184,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=775,So make sure to get it right for,pic_cs-410_3_6_720.jpg
cs-410_3_6_185,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:00,150","00:13:03,400",185,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=780,The main methodology is the Cranfield,pic_cs-410_3_6_780.jpg
cs-410_3_6_186,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:03,400","00:13:08,230",186,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=783,And they are the main paradigm used in,pic_cs-410_3_6_780.jpg
cs-410_3_6_187,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:08,230","00:13:10,820",187,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=788,not just a search engine variation.,pic_cs-410_3_6_780.jpg
cs-410_3_6_188,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:10,820","00:13:16,020",188,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=790,Map and nDCG are the two main,pic_cs-410_3_6_780.jpg
cs-410_3_6_189,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:16,020","00:13:19,530",189,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=796,know about and they are appropriate for,pic_cs-410_3_6_780.jpg
cs-410_3_6_190,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:19,530","00:13:22,950",190,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=799,You will see them often,pic_cs-410_3_6_780.jpg
cs-410_3_6_191,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:22,950","00:13:27,080",191,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=802,Precision at 10 documents is easier,pic_cs-410_3_6_780.jpg
cs-410_3_6_192,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:27,080","00:13:28,500",192,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=807,So that's also often useful.,pic_cs-410_3_6_780.jpg
cs-410_3_6_193,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:30,580","00:13:37,610",193,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=810,What's not covered is some other,pic_cs-410_3_6_780.jpg
cs-410_3_6_194,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:37,610","00:13:43,720",194,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=817,"Where the system would mix two,",pic_cs-410_3_6_780.jpg
cs-410_3_6_195,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:43,720","00:13:46,580",195,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=823,And then would show,pic_cs-410_3_6_780.jpg
cs-410_3_6_196,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:46,580","00:13:49,780",196,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=826,"Of course, the users don't see",pic_cs-410_3_6_780.jpg
cs-410_3_6_197,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:49,780","00:13:52,410",197,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=829,The users would judge those results or,pic_cs-410_3_6_780.jpg
cs-410_3_6_198,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:52,410","00:13:58,096",198,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=832,click on those documents in,pic_cs-410_3_6_780.jpg
cs-410_3_6_199,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:13:58,096","00:14:02,080",199,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=838,"In this case then, the search engine",pic_cs-410_3_6_780.jpg
cs-410_3_6_200,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:02,080","00:14:07,250",200,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=842,see if one method has contributed,pic_cs-410_3_6_840.jpg
cs-410_3_6_201,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:07,250","00:14:11,570",201,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=847,"If the user tends to click on one,",pic_cs-410_3_6_840.jpg
cs-410_3_6_202,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:13,050","00:14:17,730",202,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=853,then it suggests that,pic_cs-410_3_6_840.jpg
cs-410_3_6_203,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:17,730","00:14:21,008",203,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=857,So this is what leverages the real users,pic_cs-410_3_6_840.jpg
cs-410_3_6_204,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:21,008","00:14:25,640",204,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=861,It's called A-B Test and,pic_cs-410_3_6_840.jpg
cs-410_3_6_205,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:25,640","00:14:29,370",205,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=865,the modern search engines or,pic_cs-410_3_6_840.jpg
cs-410_3_6_206,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:29,370","00:14:32,590",206,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=869,Another way to evaluate IR or,pic_cs-410_3_6_840.jpg
cs-410_3_6_207,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:32,590","00:14:36,020",207,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=872,textual retrieval is user studies and,pic_cs-410_3_6_840.jpg
cs-410_3_6_208,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:36,020","00:14:39,390",208,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=876,I've put some references here,pic_cs-410_3_6_840.jpg
cs-410_3_6_209,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:39,390","00:14:40,260",209,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=879,to know more about that.,pic_cs-410_3_6_840.jpg
cs-410_3_6_210,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:41,760","00:14:44,180",210,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=881,"So, there are three",pic_cs-410_3_6_840.jpg
cs-410_3_6_211,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:44,180","00:14:49,280",211,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=884,These are three mini books about,pic_cs-410_3_6_840.jpg
cs-410_3_6_212,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:49,280","00:14:54,280",212,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=889,in covering a broad review of,pic_cs-410_3_6_840.jpg
cs-410_3_6_213,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:54,280","00:14:58,237",213,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=894,And it covers some of the things,pic_cs-410_3_6_840.jpg
cs-410_3_6_214,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:14:58,237","00:15:01,085",214,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=898,they also have a lot of others to offer.,pic_cs-410_3_6_840.jpg
cs-410_3_6_215,cs-410,3,6, Evaluation of TR Systems - Practical Issues,"00:15:02,777","00:15:12,777",215,https://www.coursera.org/learn/cs-410/lecture/thRNy?t=902,[MUSIC],pic_cs-410_3_6_900.jpg
cs-410_4_1_1,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:00,086","00:00:07,516",1,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=0,[SOUND],pic_cs-410_4_1_0.jpg
cs-410_4_1_2,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:07,516","00:00:10,282",2,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=7,lecture is about,pic_cs-410_4_1_0.jpg
cs-410_4_1_3,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:10,282","00:00:11,805",3,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=10,"In this lecture,",pic_cs-410_4_1_0.jpg
cs-410_4_1_4,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:11,805","00:00:17,806",4,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=11,we're going to continue the discussion,pic_cs-410_4_1_0.jpg
cs-410_4_1_5,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:17,806","00:00:22,942",5,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=17,We're going to look at another kind of,pic_cs-410_4_1_0.jpg
cs-410_4_1_6,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:22,942","00:00:27,584",6,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=22,functions than the Vector Space Model,pic_cs-410_4_1_0.jpg
cs-410_4_1_7,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:32,146","00:00:36,589",7,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=32,"In probabilistic models,",pic_cs-410_4_1_0.jpg
cs-410_4_1_8,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:36,589","00:00:41,822",8,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=36,based on the probability that this,pic_cs-410_4_1_0.jpg
cs-410_4_1_9,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:41,822","00:00:46,802",9,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=41,"In other words, we introduce",pic_cs-410_4_1_0.jpg
cs-410_4_1_10,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:46,802","00:00:51,400",10,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=46,This is the variable R here.,pic_cs-410_4_1_0.jpg
cs-410_4_1_11,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:51,400","00:00:54,520",11,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=51,And we also assume that the query and,pic_cs-410_4_1_0.jpg
cs-410_4_1_12,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:00:54,520","00:00:59,830",12,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=54,the documents are all observations,pic_cs-410_4_1_0.jpg
cs-410_4_1_13,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:00,920","00:01:05,810",13,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=60,"Note that in the vector-based models,",pic_cs-410_4_1_60.jpg
cs-410_4_1_14,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:05,810","00:01:11,120",14,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=65,here we assume they are the data,pic_cs-410_4_1_60.jpg
cs-410_4_1_15,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:11,120","00:01:17,940",15,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=71,"And so, the problem of retrieval becomes",pic_cs-410_4_1_60.jpg
cs-410_4_1_16,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:19,490","00:01:23,060",16,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=79,"In this category of models,",pic_cs-410_4_1_60.jpg
cs-410_4_1_17,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:23,060","00:01:27,130",17,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=83,The classic probabilistic model has,pic_cs-410_4_1_60.jpg
cs-410_4_1_18,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:27,130","00:01:30,150",18,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=87,which we discussed in in,pic_cs-410_4_1_60.jpg
cs-410_4_1_19,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:30,150","00:01:33,570",19,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=90,because its a form is actually,pic_cs-410_4_1_60.jpg
cs-410_4_1_20,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:35,260","00:01:40,180",20,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=95,"In this lecture,",pic_cs-410_4_1_60.jpg
cs-410_4_1_21,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:41,230","00:01:45,550",21,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=101,P class called a language,pic_cs-410_4_1_60.jpg
cs-410_4_1_22,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:45,550","00:01:50,150",22,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=105,"In particular, we're going to discuss",pic_cs-410_4_1_60.jpg
cs-410_4_1_23,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:51,370","00:01:55,330",23,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=111,which is one of the most effective,pic_cs-410_4_1_60.jpg
cs-410_4_1_24,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:01:57,050","00:02:01,840",24,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=117,There was also another line called,pic_cs-410_4_1_60.jpg
cs-410_4_1_25,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:01,840","00:02:04,970",25,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=121,"which has led to the PL2 function,",pic_cs-410_4_1_120.jpg
cs-410_4_1_26,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:06,440","00:02:11,070",26,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=126,it's also one of the most effective,pic_cs-410_4_1_120.jpg
cs-410_4_1_27,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:11,070","00:02:16,847",27,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=131,"In query likelihood, our assumption",pic_cs-410_4_1_120.jpg
cs-410_4_1_28,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:16,847","00:02:23,002",28,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=136,can be approximated by the probability,pic_cs-410_4_1_120.jpg
cs-410_4_1_29,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:23,002","00:02:29,656",29,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=143,"So intuitively, this probability just",pic_cs-410_4_1_120.jpg
cs-410_4_1_30,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:29,656","00:02:34,808",30,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=149,"And that is if a user likes document d,",pic_cs-410_4_1_120.jpg
cs-410_4_1_31,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:34,808","00:02:40,220",31,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=154,"the user enter query q ,in",pic_cs-410_4_1_120.jpg
cs-410_4_1_32,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:40,220","00:02:47,680",32,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=160,"So we assume that the user likes d,",pic_cs-410_4_1_120.jpg
cs-410_4_1_33,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:47,680","00:02:52,610",33,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=167,And then we ask the question about how,pic_cs-410_4_1_120.jpg
cs-410_4_1_34,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:52,610","00:02:53,250",34,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=172,from this user?,pic_cs-410_4_1_120.jpg
cs-410_4_1_35,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:54,890","00:02:56,508",35,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=174,So this is the basic idea.,pic_cs-410_4_1_120.jpg
cs-410_4_1_36,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:02:56,508","00:03:00,676",36,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=176,"Now, to understand this idea,",pic_cs-410_4_1_120.jpg
cs-410_4_1_37,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:00,676","00:03:03,741",37,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=180,the basic idea of,pic_cs-410_4_1_180.jpg
cs-410_4_1_38,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:03,741","00:03:09,150",38,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=183,"So here, I listed some imagined",pic_cs-410_4_1_180.jpg
cs-410_4_1_39,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:09,150","00:03:13,599",39,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=189,relevance judgments of queries and,pic_cs-410_4_1_180.jpg
cs-410_4_1_40,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:13,599","00:03:17,576",40,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=193,"For example, in this line,",pic_cs-410_4_1_180.jpg
cs-410_4_1_41,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:17,576","00:03:24,546",41,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=197,it shows that q1 is a query,pic_cs-410_4_1_180.jpg
cs-410_4_1_42,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:24,546","00:03:28,043",42,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=204,And d1 is a document,pic_cs-410_4_1_180.jpg
cs-410_4_1_43,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:28,043","00:03:33,036",43,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=208,And 1 means the user thinks,pic_cs-410_4_1_180.jpg
cs-410_4_1_44,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:33,036","00:03:38,685",44,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=213,So this R here can be also approximated,pic_cs-410_4_1_180.jpg
cs-410_4_1_45,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:38,685","00:03:44,810",45,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=218,engine can collect by watching how you,pic_cs-410_4_1_180.jpg
cs-410_4_1_46,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:44,810","00:03:47,990",46,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=224,"So in this case, let's say",pic_cs-410_4_1_180.jpg
cs-410_4_1_47,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:47,990","00:03:49,000",47,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=227,So there's a 1 here.,pic_cs-410_4_1_180.jpg
cs-410_4_1_48,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:50,080","00:03:56,480",48,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=230,"Similarly, the user clicked on d2 also,",pic_cs-410_4_1_180.jpg
cs-410_4_1_49,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:03:56,480","00:03:59,630",49,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=236,"In other words,",pic_cs-410_4_1_180.jpg
cs-410_4_1_50,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:00,700","00:04:05,080",50,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=240,"On the other hand,",pic_cs-410_4_1_240.jpg
cs-410_4_1_51,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:07,430","00:04:13,485",51,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=247,And d4 is non-relevant and then d5 is,pic_cs-410_4_1_240.jpg
cs-410_4_1_52,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:13,485","00:04:17,860",52,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=253,"And this part, maybe,",pic_cs-410_4_1_240.jpg
cs-410_4_1_53,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:17,860","00:04:23,009",53,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=257,So this user typed in q1 and then found,pic_cs-410_4_1_240.jpg
cs-410_4_1_54,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:23,009","00:04:26,170",54,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=263,so d1 is actually non-relevant.,pic_cs-410_4_1_240.jpg
cs-410_4_1_55,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:26,170","00:04:31,124",55,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=266,"In contrast, here we see it's relevant.",pic_cs-410_4_1_240.jpg
cs-410_4_1_56,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:31,124","00:04:38,401",56,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=271,Or this could be the same query typed,pic_cs-410_4_1_240.jpg
cs-410_4_1_57,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:38,401","00:04:42,660",57,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=278,"But d2 is also relevant, etc.",pic_cs-410_4_1_240.jpg
cs-410_4_1_58,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:42,660","00:04:47,050",58,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=282,"And then here,",pic_cs-410_4_1_240.jpg
cs-410_4_1_59,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:48,390","00:04:50,870",59,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=288,"Now, we can imagine we",pic_cs-410_4_1_240.jpg
cs-410_4_1_60,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:52,940","00:04:54,740",60,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=292,"Now we can ask the question,",pic_cs-410_4_1_240.jpg
cs-410_4_1_61,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:04:54,740","00:04:58,460",61,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=294,how can we then estimate,pic_cs-410_4_1_240.jpg
cs-410_4_1_62,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:00,390","00:05:03,690",62,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=300,So how can we compute this,pic_cs-410_4_1_300.jpg
cs-410_4_1_63,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:03,690","00:05:06,230",63,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=303,"Well, intuitively that just means",pic_cs-410_4_1_300.jpg
cs-410_4_1_64,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:06,230","00:05:10,770",64,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=306,if we look at all the entries,pic_cs-410_4_1_300.jpg
cs-410_4_1_65,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:10,770","00:05:16,010",65,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=310,"this particular q, how likely we'll",pic_cs-410_4_1_300.jpg
cs-410_4_1_66,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:16,010","00:05:18,500",66,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=316,So basically that just means that,pic_cs-410_4_1_300.jpg
cs-410_4_1_67,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:19,730","00:05:24,536",67,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=319,We can first count how many,pic_cs-410_4_1_300.jpg
cs-410_4_1_68,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:24,536","00:05:29,576",68,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=324,d as a pair in this table and,pic_cs-410_4_1_300.jpg
cs-410_4_1_69,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:29,576","00:05:34,518",69,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=329,we actually have also seen,pic_cs-410_4_1_300.jpg
cs-410_4_1_70,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:34,518","00:05:37,227",70,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=334,"And then, we just compute the ratio.",pic_cs-410_4_1_300.jpg
cs-410_4_1_71,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:39,409","00:05:42,347",71,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=339,So let's take a look at,pic_cs-410_4_1_300.jpg
cs-410_4_1_72,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:42,347","00:05:48,466",72,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=342,Suppose we are trying to compute this,pic_cs-410_4_1_300.jpg
cs-410_4_1_73,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:48,466","00:05:52,240",73,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=348,What is the estimated probability?,pic_cs-410_4_1_300.jpg
cs-410_4_1_74,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:52,240","00:05:54,760",74,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=352,"Now, think about that.",pic_cs-410_4_1_300.jpg
cs-410_4_1_75,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:54,760","00:05:58,823",75,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=354,You can pause the video if needed.,pic_cs-410_4_1_300.jpg
cs-410_4_1_76,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:05:58,823","00:06:01,560",76,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=358,Try to take a look at the table.,pic_cs-410_4_1_300.jpg
cs-410_4_1_77,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:01,560","00:06:04,606",77,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=361,And try to give your,pic_cs-410_4_1_360.jpg
cs-410_4_1_78,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:07,069","00:06:11,802",78,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=367,"Have you seen that,",pic_cs-410_4_1_360.jpg
cs-410_4_1_79,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:11,802","00:06:15,050",79,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=371,we'll be looking at these two pairs?,pic_cs-410_4_1_360.jpg
cs-410_4_1_80,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:15,050","00:06:18,020",80,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=375,"And in both cases, well,",pic_cs-410_4_1_360.jpg
cs-410_4_1_81,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:18,020","00:06:23,190",81,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=378,"actually, in one of the cases, the user",pic_cs-410_4_1_360.jpg
cs-410_4_1_82,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:23,190","00:06:26,282",82,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=383,So R = 1 in only one of the two cases.,pic_cs-410_4_1_360.jpg
cs-410_4_1_83,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:26,282","00:06:28,244",83,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=386,"In the other case, it's 0.",pic_cs-410_4_1_360.jpg
cs-410_4_1_84,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:28,244","00:06:30,846",84,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=388,So that's one out of two.,pic_cs-410_4_1_360.jpg
cs-410_4_1_85,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:30,846","00:06:34,525",85,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=390,What about the d1 and the d2?,pic_cs-410_4_1_360.jpg
cs-410_4_1_86,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:34,525","00:06:39,127",86,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=394,"Well, they are here, d1 and d2, d1 and d2,",pic_cs-410_4_1_360.jpg
cs-410_4_1_87,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:39,127","00:06:42,729",87,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=399,"in both cases, in this case, R = 1.",pic_cs-410_4_1_360.jpg
cs-410_4_1_88,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:42,729","00:06:45,700",88,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=402,So it's a two out of two and,pic_cs-410_4_1_360.jpg
cs-410_4_1_89,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:45,700","00:06:48,195",89,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=405,"So you can see with this approach,",pic_cs-410_4_1_360.jpg
cs-410_4_1_90,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:48,195","00:06:52,679",90,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=408,we can actually score these documents for,pic_cs-410_4_1_360.jpg
cs-410_4_1_91,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:52,679","00:06:56,625",91,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=412,"We now have a score for d1,",pic_cs-410_4_1_360.jpg
cs-410_4_1_92,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:06:56,625","00:07:00,334",92,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=416,And we can simply rank them,pic_cs-410_4_1_360.jpg
cs-410_4_1_93,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:00,334","00:07:04,056",93,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=420,so that's the basic idea,pic_cs-410_4_1_420.jpg
cs-410_4_1_94,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:04,056","00:07:06,971",94,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=424,"And you can see it makes a lot of sense,",pic_cs-410_4_1_420.jpg
cs-410_4_1_95,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:06,971","00:07:10,036",95,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=426,it's going to rank d2 above,pic_cs-410_4_1_420.jpg
cs-410_4_1_96,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:10,036","00:07:15,992",96,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=430,"Because in all the cases,",pic_cs-410_4_1_420.jpg
cs-410_4_1_97,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:15,992","00:07:18,314",97,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=435,The user clicked on this document.,pic_cs-410_4_1_420.jpg
cs-410_4_1_98,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:18,314","00:07:23,957",98,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=438,So this also should show that,pic_cs-410_4_1_420.jpg
cs-410_4_1_99,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:23,957","00:07:30,830",99,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=443,a search engine can learn a lot from,pic_cs-410_4_1_420.jpg
cs-410_4_1_100,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:30,830","00:07:33,580",100,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=450,This is a simple example,pic_cs-410_4_1_420.jpg
cs-410_4_1_101,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:33,580","00:07:38,760",101,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=453,with small amount of entries here we can,pic_cs-410_4_1_420.jpg
cs-410_4_1_102,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:38,760","00:07:42,160",102,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=458,These probabilities would give us,pic_cs-410_4_1_420.jpg
cs-410_4_1_103,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:42,160","00:07:46,160",103,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=462,might be more relevant or more useful,pic_cs-410_4_1_420.jpg
cs-410_4_1_104,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:47,170","00:07:51,100",104,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=467,"Now, of course, the problems that we",pic_cs-410_4_1_420.jpg
cs-410_4_1_105,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:51,100","00:07:54,048",105,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=471,all the documents and,pic_cs-410_4_1_420.jpg
cs-410_4_1_106,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:55,320","00:07:57,890",106,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=475,"There would be a lot of unseen documents,",pic_cs-410_4_1_420.jpg
cs-410_4_1_107,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:07:57,890","00:08:02,880",107,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=477,we have only collected the data from the,pic_cs-410_4_1_420.jpg
cs-410_4_1_108,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:02,880","00:08:07,370",108,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=482,And there are even more unseen queries,pic_cs-410_4_1_480.jpg
cs-410_4_1_109,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:07,370","00:08:10,060",109,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=487,queries will be typed in by users.,pic_cs-410_4_1_480.jpg
cs-410_4_1_110,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:10,060","00:08:15,090",110,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=490,"So obviously,",pic_cs-410_4_1_480.jpg
cs-410_4_1_111,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:15,090","00:08:17,190",111,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=495,it to unseen queries or unseen documents.,pic_cs-410_4_1_480.jpg
cs-410_4_1_112,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:18,635","00:08:22,278",112,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=498,"Nevertheless, this shows the basic idea",pic_cs-410_4_1_480.jpg
cs-410_4_1_113,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:22,278","00:08:23,646",113,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=502,it makes sense intuitively.,pic_cs-410_4_1_480.jpg
cs-410_4_1_114,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:23,646","00:08:28,275",114,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=503,So what do we do in such a case when,pic_cs-410_4_1_480.jpg
cs-410_4_1_115,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:28,275","00:08:29,508",115,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=508,unseen queries?,pic_cs-410_4_1_480.jpg
cs-410_4_1_116,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:29,508","00:08:32,818",116,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=509,"Well, the solutions that we have",pic_cs-410_4_1_480.jpg
cs-410_4_1_117,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:32,818","00:08:37,003",117,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=512,So in this particular case called,pic_cs-410_4_1_480.jpg
cs-410_4_1_118,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:37,003","00:08:40,784",118,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=517,we just approximate this by,pic_cs-410_4_1_480.jpg
cs-410_4_1_119,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:40,784","00:08:46,682",119,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=520,"p(q given d, R=1).",pic_cs-410_4_1_480.jpg
cs-410_4_1_120,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:46,682","00:08:51,539",120,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=526,"So in the condition part, we assume that",pic_cs-410_4_1_480.jpg
cs-410_4_1_121,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:51,539","00:08:54,640",121,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=531,have seen that the user,pic_cs-410_4_1_480.jpg
cs-410_4_1_122,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:56,190","00:08:58,777",122,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=536,And this part shows that,pic_cs-410_4_1_480.jpg
cs-410_4_1_123,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:08:58,777","00:09:01,438",123,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=538,likely the user would,pic_cs-410_4_1_480.jpg
cs-410_4_1_124,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:01,438","00:09:04,653",124,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=541,How likely we will see this,pic_cs-410_4_1_540.jpg
cs-410_4_1_125,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:04,653","00:09:08,880",125,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=544,"So note that here, we have made",pic_cs-410_4_1_540.jpg
cs-410_4_1_126,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:08,880","00:09:13,900",126,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=548,"Basically, we're going to do, assume that",pic_cs-410_4_1_540.jpg
cs-410_4_1_127,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:13,900","00:09:17,970",127,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=553,has something to do with whether,pic_cs-410_4_1_540.jpg
cs-410_4_1_128,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:17,970","00:09:20,740",128,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=557,"In other words,",pic_cs-410_4_1_540.jpg
cs-410_4_1_129,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:22,160","00:09:27,671",129,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=562,And that is a user formulates a query,pic_cs-410_4_1_540.jpg
cs-410_4_1_130,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:27,671","00:09:30,358",130,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=567,Where if you just look at this,pic_cs-410_4_1_540.jpg
cs-410_4_1_131,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:30,358","00:09:32,629",131,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=570,it's not obvious we,pic_cs-410_4_1_540.jpg
cs-410_4_1_132,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:32,629","00:09:37,941",132,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=572,So what I really meant is that,pic_cs-410_4_1_540.jpg
cs-410_4_1_133,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:37,941","00:09:43,367",133,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=577,"probability to help us score,",pic_cs-410_4_1_540.jpg
cs-410_4_1_134,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:43,367","00:09:48,794",134,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=583,probability will have to somehow,pic_cs-410_4_1_540.jpg
cs-410_4_1_135,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:48,794","00:09:54,696",135,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=588,conditional probability without,pic_cs-410_4_1_540.jpg
cs-410_4_1_136,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:54,696","00:09:59,306",136,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=594,Otherwise we would be having,pic_cs-410_4_1_540.jpg
cs-410_4_1_137,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:09:59,306","00:10:04,537",137,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=599,"by making this assumption,",pic_cs-410_4_1_540.jpg
cs-410_4_1_138,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:04,537","00:10:09,252",138,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=604,and try to just model how the user,pic_cs-410_4_1_600.jpg
cs-410_4_1_139,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:09,252","00:10:13,639",139,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=609,So this is how you can,pic_cs-410_4_1_600.jpg
cs-410_4_1_140,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:13,639","00:10:18,570",140,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=613,that we can derive a specific,pic_cs-410_4_1_600.jpg
cs-410_4_1_141,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:18,570","00:10:22,020",141,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=618,So let's look at how this model work for,pic_cs-410_4_1_600.jpg
cs-410_4_1_142,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:22,020","00:10:23,300",142,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=622,"And basically,",pic_cs-410_4_1_600.jpg
cs-410_4_1_143,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:23,300","00:10:27,420",143,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=623,what we are going to do in this case,pic_cs-410_4_1_600.jpg
cs-410_4_1_144,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:27,420","00:10:30,760",144,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=627,Which of these documents is most,pic_cs-410_4_1_600.jpg
cs-410_4_1_145,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:30,760","00:10:34,300",145,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=630,document in the user's mind when,pic_cs-410_4_1_600.jpg
cs-410_4_1_146,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:34,300","00:10:38,488",146,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=634,So we ask this question and we quantify,pic_cs-410_4_1_600.jpg
cs-410_4_1_147,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:38,488","00:10:43,443",147,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=638,a conditional probability of observing,pic_cs-410_4_1_600.jpg
cs-410_4_1_148,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:43,443","00:10:47,245",148,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=643,fact the imaginary relevant,pic_cs-410_4_1_600.jpg
cs-410_4_1_149,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:47,245","00:10:51,885",149,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=647,Here you can see we've computed all,pic_cs-410_4_1_600.jpg
cs-410_4_1_150,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:51,885","00:10:55,340",150,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=651,The likelihood of queries,pic_cs-410_4_1_600.jpg
cs-410_4_1_151,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:55,340","00:10:56,880",151,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=655,"Once we have these values,",pic_cs-410_4_1_600.jpg
cs-410_4_1_152,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:10:56,880","00:11:00,370",152,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=656,we can then rank these documents,pic_cs-410_4_1_600.jpg
cs-410_4_1_153,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:00,370","00:11:05,420",153,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=660,"So to summarize, the general idea",pic_cs-410_4_1_660.jpg
cs-410_4_1_154,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:05,420","00:11:11,740",154,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=665,risk model is to assume the we introduce,pic_cs-410_4_1_660.jpg
cs-410_4_1_155,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:11,740","00:11:12,690",155,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=671,"And then,",pic_cs-410_4_1_660.jpg
cs-410_4_1_156,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:12,690","00:11:16,740",156,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=672,let the scoring function be defined,pic_cs-410_4_1_660.jpg
cs-410_4_1_157,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:16,740","00:11:20,980",157,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=676,We also talked about approximating,pic_cs-410_4_1_660.jpg
cs-410_4_1_158,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:22,450","00:11:27,065",158,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=682,And in this case we have a ranking,pic_cs-410_4_1_660.jpg
cs-410_4_1_159,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:27,065","00:11:31,385",159,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=687,based on the probability of,pic_cs-410_4_1_660.jpg
cs-410_4_1_160,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:31,385","00:11:36,165",160,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=691,And this probability should be interpreted,pic_cs-410_4_1_660.jpg
cs-410_4_1_161,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:36,165","00:11:39,236",161,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=696,"likes document d, would pose query q.",pic_cs-410_4_1_660.jpg
cs-410_4_1_162,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:40,265","00:11:44,645",162,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=700,"Now, the question of course is, how do",pic_cs-410_4_1_660.jpg
cs-410_4_1_163,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:44,645","00:11:49,500",163,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=704,At this in general has to do with how,pic_cs-410_4_1_660.jpg
cs-410_4_1_164,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:49,500","00:11:51,980",164,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=709,because q is a text.,pic_cs-410_4_1_660.jpg
cs-410_4_1_165,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:51,980","00:11:56,560",165,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=711,And this has to do with a model,pic_cs-410_4_1_660.jpg
cs-410_4_1_166,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:11:56,560","00:12:00,580",166,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=716,And these kind of models,pic_cs-410_4_1_660.jpg
cs-410_4_1_167,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:02,190","00:12:07,440",167,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=722,"So more specifically, we will be",pic_cs-410_4_1_720.jpg
cs-410_4_1_168,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:07,440","00:12:12,050",168,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=727,conditional probability,pic_cs-410_4_1_720.jpg
cs-410_4_1_169,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:12,050","00:12:18,463",169,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=732,"If the user liked this document,",pic_cs-410_4_1_720.jpg
cs-410_4_1_170,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:18,463","00:12:21,884",170,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=738,"And in the next lecture we're going to do,",pic_cs-410_4_1_720.jpg
cs-410_4_1_171,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:21,884","00:12:27,016",171,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=741,giving introduction to language,pic_cs-410_4_1_720.jpg
cs-410_4_1_172,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:27,016","00:12:32,063",172,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=747,can model text that was a probable,pic_cs-410_4_1_720.jpg
cs-410_4_1_173,cs-410,4,1, Probabilistic Retrieval Model - Basic Idea,"00:12:32,063","00:12:42,063",173,https://www.coursera.org/learn/cs-410/lecture/nkg5n?t=752,[MUSIC],pic_cs-410_4_1_720.jpg
cs-410_4_2_1,cs-410,4,2, Statistical Language Model,"00:00:07,780","00:00:12,596",1,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=7,[SOUND] This lecture is about,pic_cs-410_4_2_0.jpg
cs-410_4_2_2,cs-410,4,2, Statistical Language Model,"00:00:12,596","00:00:13,737",2,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=12,"In this lecture,",pic_cs-410_4_2_0.jpg
cs-410_4_2_3,cs-410,4,2, Statistical Language Model,"00:00:13,737","00:00:18,445",3,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=13,we're going to give an introduction,pic_cs-410_4_2_0.jpg
cs-410_4_2_4,cs-410,4,2, Statistical Language Model,"00:00:18,445","00:00:23,305",4,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=18,This has to do with how do you model,pic_cs-410_4_2_0.jpg
cs-410_4_2_5,cs-410,4,2, Statistical Language Model,"00:00:23,305","00:00:28,272",5,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=23,So it's related to how we model,pic_cs-410_4_2_0.jpg
cs-410_4_2_6,cs-410,4,2, Statistical Language Model,"00:00:31,828","00:00:34,032",6,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=31,We're going to talk about,pic_cs-410_4_2_0.jpg
cs-410_4_2_7,cs-410,4,2, Statistical Language Model,"00:00:34,032","00:00:37,688",7,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=34,And then we're going to talk about the,pic_cs-410_4_2_0.jpg
cs-410_4_2_8,cs-410,4,2, Statistical Language Model,"00:00:37,688","00:00:42,770",8,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=37,"language model, which also happens to be",pic_cs-410_4_2_0.jpg
cs-410_4_2_9,cs-410,4,2, Statistical Language Model,"00:00:42,770","00:00:45,420",9,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=42,"And finally, what this class",pic_cs-410_4_2_0.jpg
cs-410_4_2_10,cs-410,4,2, Statistical Language Model,"00:00:47,200","00:00:48,750",10,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=47,What is a language model?,pic_cs-410_4_2_0.jpg
cs-410_4_2_11,cs-410,4,2, Statistical Language Model,"00:00:48,750","00:00:53,570",11,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=48,"Well, it's just a probability",pic_cs-410_4_2_0.jpg
cs-410_4_2_12,cs-410,4,2, Statistical Language Model,"00:00:53,570","00:00:54,540",12,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=53,"So here, I'll show one.",pic_cs-410_4_2_0.jpg
cs-410_4_2_13,cs-410,4,2, Statistical Language Model,"00:00:55,870","00:01:00,430",13,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=55,This model gives the sequence Today,pic_cs-410_4_2_0.jpg
cs-410_4_2_14,cs-410,4,2, Statistical Language Model,"00:01:00,430","00:01:03,830",14,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=60,"It give Today Wednesday is a very,",pic_cs-410_4_2_60.jpg
cs-410_4_2_15,cs-410,4,2, Statistical Language Model,"00:01:03,830","00:01:09,705",15,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=63,very small probability,pic_cs-410_4_2_60.jpg
cs-410_4_2_16,cs-410,4,2, Statistical Language Model,"00:01:11,796","00:01:15,447",16,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=71,You can see the probabilities,pic_cs-410_4_2_60.jpg
cs-410_4_2_17,cs-410,4,2, Statistical Language Model,"00:01:15,447","00:01:19,670",17,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=75,sequences of words can vary,pic_cs-410_4_2_60.jpg
cs-410_4_2_18,cs-410,4,2, Statistical Language Model,"00:01:19,670","00:01:23,256",18,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=79,"Therefore, it's clearly context dependent.",pic_cs-410_4_2_60.jpg
cs-410_4_2_19,cs-410,4,2, Statistical Language Model,"00:01:23,256","00:01:24,552",19,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=83,"In ordinary conversation,",pic_cs-410_4_2_60.jpg
cs-410_4_2_20,cs-410,4,2, Statistical Language Model,"00:01:24,552","00:01:28,510",20,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=84,probably Today is Wednesday is most,pic_cs-410_4_2_60.jpg
cs-410_4_2_21,cs-410,4,2, Statistical Language Model,"00:01:28,510","00:01:32,132",21,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=88,Imagine in the context of,pic_cs-410_4_2_60.jpg
cs-410_4_2_22,cs-410,4,2, Statistical Language Model,"00:01:32,132","00:01:36,890",22,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=92,"maybe the eigenvalue is positive,",pic_cs-410_4_2_60.jpg
cs-410_4_2_23,cs-410,4,2, Statistical Language Model,"00:01:36,890","00:01:41,080",23,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=96,This means it can be used to,pic_cs-410_4_2_60.jpg
cs-410_4_2_24,cs-410,4,2, Statistical Language Model,"00:01:42,240","00:01:45,900",24,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=102,The model can also be regarded,pic_cs-410_4_2_60.jpg
cs-410_4_2_25,cs-410,4,2, Statistical Language Model,"00:01:45,900","00:01:46,950",25,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=105,generating text.,pic_cs-410_4_2_60.jpg
cs-410_4_2_26,cs-410,4,2, Statistical Language Model,"00:01:46,950","00:01:51,660",26,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=106,And this is why it's also often,pic_cs-410_4_2_60.jpg
cs-410_4_2_27,cs-410,4,2, Statistical Language Model,"00:01:51,660","00:01:52,910",27,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=111,So what does that mean?,pic_cs-410_4_2_60.jpg
cs-410_4_2_28,cs-410,4,2, Statistical Language Model,"00:01:52,910","00:01:58,540",28,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=112,We can imagine this is a mechanism that's,pic_cs-410_4_2_60.jpg
cs-410_4_2_29,cs-410,4,2, Statistical Language Model,"00:01:58,540","00:02:05,340",29,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=118,visualised here as a stochastic system,pic_cs-410_4_2_60.jpg
cs-410_4_2_30,cs-410,4,2, Statistical Language Model,"00:02:05,340","00:02:08,608",30,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=125,"So, we can ask for a sequence,",pic_cs-410_4_2_120.jpg
cs-410_4_2_31,cs-410,4,2, Statistical Language Model,"00:02:08,608","00:02:13,548",31,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=128,"a sequence from the device if you want,",pic_cs-410_4_2_120.jpg
cs-410_4_2_32,cs-410,4,2, Statistical Language Model,"00:02:13,548","00:02:18,420",32,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=133,"Today is Wednesday, but it could",pic_cs-410_4_2_120.jpg
cs-410_4_2_33,cs-410,4,2, Statistical Language Model,"00:02:18,420","00:02:21,940",33,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=138,"So for example,",pic_cs-410_4_2_120.jpg
cs-410_4_2_34,cs-410,4,2, Statistical Language Model,"00:02:24,086","00:02:28,418",34,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=144,"So in this sense,",pic_cs-410_4_2_120.jpg
cs-410_4_2_35,cs-410,4,2, Statistical Language Model,"00:02:28,418","00:02:32,656",35,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=148,a sample observed from,pic_cs-410_4_2_120.jpg
cs-410_4_2_36,cs-410,4,2, Statistical Language Model,"00:02:32,656","00:02:33,840",36,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=152,"So, why is such a model useful?",pic_cs-410_4_2_120.jpg
cs-410_4_2_37,cs-410,4,2, Statistical Language Model,"00:02:33,840","00:02:39,720",37,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=153,"Well, it's mainly because it can quantify",pic_cs-410_4_2_120.jpg
cs-410_4_2_38,cs-410,4,2, Statistical Language Model,"00:02:39,720","00:02:41,190",38,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=159,Where do uncertainties come from?,pic_cs-410_4_2_120.jpg
cs-410_4_2_39,cs-410,4,2, Statistical Language Model,"00:02:41,190","00:02:45,690",39,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=161,"Well, one source is simply",pic_cs-410_4_2_120.jpg
cs-410_4_2_40,cs-410,4,2, Statistical Language Model,"00:02:45,690","00:02:48,870",40,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=165,that we discussed earlier in the lecture.,pic_cs-410_4_2_120.jpg
cs-410_4_2_41,cs-410,4,2, Statistical Language Model,"00:02:48,870","00:02:52,240",41,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=168,Another source is because we don't,pic_cs-410_4_2_120.jpg
cs-410_4_2_42,cs-410,4,2, Statistical Language Model,"00:02:52,240","00:02:55,300",42,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=172,we lack all the knowledge,pic_cs-410_4_2_120.jpg
cs-410_4_2_43,cs-410,4,2, Statistical Language Model,"00:02:55,300","00:02:58,420",43,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=175,"In that case,",pic_cs-410_4_2_120.jpg
cs-410_4_2_44,cs-410,4,2, Statistical Language Model,"00:02:58,420","00:03:01,800",44,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=178,So let me show some examples of questions,pic_cs-410_4_2_120.jpg
cs-410_4_2_45,cs-410,4,2, Statistical Language Model,"00:03:01,800","00:03:06,220",45,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=181,that would have interesting,pic_cs-410_4_2_180.jpg
cs-410_4_2_46,cs-410,4,2, Statistical Language Model,"00:03:06,220","00:03:11,641",46,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=186,"Given that we see John and feels,",pic_cs-410_4_2_180.jpg
cs-410_4_2_47,cs-410,4,2, Statistical Language Model,"00:03:11,641","00:03:16,866",47,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=191,as opposed to habit as the next,pic_cs-410_4_2_180.jpg
cs-410_4_2_48,cs-410,4,2, Statistical Language Model,"00:03:16,866","00:03:21,123",48,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=196,"Now, obviously, this would be very useful",pic_cs-410_4_2_180.jpg
cs-410_4_2_49,cs-410,4,2, Statistical Language Model,"00:03:21,123","00:03:25,180",49,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=201,"habit would have similar acoustic sound,",pic_cs-410_4_2_180.jpg
cs-410_4_2_50,cs-410,4,2, Statistical Language Model,"00:03:25,180","00:03:28,190",50,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=205,"But, if we look at the language model,",pic_cs-410_4_2_180.jpg
cs-410_4_2_51,cs-410,4,2, Statistical Language Model,"00:03:28,190","00:03:32,690",51,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=208,we know that John feels happy would be,pic_cs-410_4_2_180.jpg
cs-410_4_2_52,cs-410,4,2, Statistical Language Model,"00:03:35,810","00:03:39,300",52,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=215,"Another example, given that we",pic_cs-410_4_2_180.jpg
cs-410_4_2_53,cs-410,4,2, Statistical Language Model,"00:03:39,300","00:03:43,700",53,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=219,"game once in a news article,",pic_cs-410_4_2_180.jpg
cs-410_4_2_54,cs-410,4,2, Statistical Language Model,"00:03:43,700","00:03:47,430",54,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=223,This obviously is related to text,pic_cs-410_4_2_180.jpg
cs-410_4_2_55,cs-410,4,2, Statistical Language Model,"00:03:48,720","00:03:52,150",55,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=228,"Also, given that a user is",pic_cs-410_4_2_180.jpg
cs-410_4_2_56,cs-410,4,2, Statistical Language Model,"00:03:52,150","00:03:55,570",56,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=232,how likely would the user,pic_cs-410_4_2_180.jpg
cs-410_4_2_57,cs-410,4,2, Statistical Language Model,"00:03:55,570","00:03:58,530",57,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=235,"Now, this is clearly related",pic_cs-410_4_2_180.jpg
cs-410_4_2_58,cs-410,4,2, Statistical Language Model,"00:03:58,530","00:04:00,185",58,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=238,that we discussed in the previous lecture.,pic_cs-410_4_2_180.jpg
cs-410_4_2_59,cs-410,4,2, Statistical Language Model,"00:04:02,180","00:04:05,710",59,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=242,"So now,",pic_cs-410_4_2_240.jpg
cs-410_4_2_60,cs-410,4,2, Statistical Language Model,"00:04:05,710","00:04:07,910",60,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=245,called a unigram language model.,pic_cs-410_4_2_240.jpg
cs-410_4_2_61,cs-410,4,2, Statistical Language Model,"00:04:07,910","00:04:09,690",61,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=247,"In such a case,",pic_cs-410_4_2_240.jpg
cs-410_4_2_62,cs-410,4,2, Statistical Language Model,"00:04:09,690","00:04:13,550",62,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=249,we assume that we generate a text by,pic_cs-410_4_2_240.jpg
cs-410_4_2_63,cs-410,4,2, Statistical Language Model,"00:04:14,760","00:04:19,356",63,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=254,So this means the probability of,pic_cs-410_4_2_240.jpg
cs-410_4_2_64,cs-410,4,2, Statistical Language Model,"00:04:19,356","00:04:22,601",64,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=259,the product of,pic_cs-410_4_2_240.jpg
cs-410_4_2_65,cs-410,4,2, Statistical Language Model,"00:04:22,601","00:04:25,800",65,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=262,"Now normally,",pic_cs-410_4_2_240.jpg
cs-410_4_2_66,cs-410,4,2, Statistical Language Model,"00:04:25,800","00:04:30,270",66,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=265,So if you have single word in like,pic_cs-410_4_2_240.jpg
cs-410_4_2_67,cs-410,4,2, Statistical Language Model,"00:04:30,270","00:04:35,470",67,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=270,likely to observe model than if,pic_cs-410_4_2_240.jpg
cs-410_4_2_68,cs-410,4,2, Statistical Language Model,"00:04:35,470","00:04:37,780",68,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=275,So this assumption is not,pic_cs-410_4_2_240.jpg
cs-410_4_2_69,cs-410,4,2, Statistical Language Model,"00:04:37,780","00:04:39,920",69,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=277,we make this assumption,pic_cs-410_4_2_240.jpg
cs-410_4_2_70,cs-410,4,2, Statistical Language Model,"00:04:41,210","00:04:47,060",70,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=281,So now the model has precisely N,pic_cs-410_4_2_240.jpg
cs-410_4_2_71,cs-410,4,2, Statistical Language Model,"00:04:47,060","00:04:51,380",71,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=287,"We have one probability for each word, and",pic_cs-410_4_2_240.jpg
cs-410_4_2_72,cs-410,4,2, Statistical Language Model,"00:04:51,380","00:04:57,450",72,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=291,"So strictly speaking,",pic_cs-410_4_2_240.jpg
cs-410_4_2_73,cs-410,4,2, Statistical Language Model,"00:05:00,270","00:05:04,495",73,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=300,"As I said,",pic_cs-410_4_2_300.jpg
cs-410_4_2_74,cs-410,4,2, Statistical Language Model,"00:05:04,495","00:05:06,245",74,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=304,drawn from this word distribution.,pic_cs-410_4_2_300.jpg
cs-410_4_2_75,cs-410,4,2, Statistical Language Model,"00:05:08,080","00:05:11,540",75,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=308,"So for example,",pic_cs-410_4_2_300.jpg
cs-410_4_2_76,cs-410,4,2, Statistical Language Model,"00:05:11,540","00:05:18,020",76,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=311,the model to stochastically generate,pic_cs-410_4_2_300.jpg
cs-410_4_2_77,cs-410,4,2, Statistical Language Model,"00:05:18,020","00:05:19,988",77,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=318,"So instead of giving a whole sequence,",pic_cs-410_4_2_300.jpg
cs-410_4_2_78,cs-410,4,2, Statistical Language Model,"00:05:19,988","00:05:23,900",78,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=319,"like Today is Wednesday,",pic_cs-410_4_2_300.jpg
cs-410_4_2_79,cs-410,4,2, Statistical Language Model,"00:05:23,900","00:05:26,200",79,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=323,And we can get all kinds of words.,pic_cs-410_4_2_300.jpg
cs-410_4_2_80,cs-410,4,2, Statistical Language Model,"00:05:26,200","00:05:28,596",80,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=326,And we can assemble these,pic_cs-410_4_2_300.jpg
cs-410_4_2_81,cs-410,4,2, Statistical Language Model,"00:05:28,596","00:05:32,304",81,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=328,So that will still allow you,pic_cs-410_4_2_300.jpg
cs-410_4_2_82,cs-410,4,2, Statistical Language Model,"00:05:32,304","00:05:36,410",82,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=332,Today is Wednesday as the product,pic_cs-410_4_2_300.jpg
cs-410_4_2_83,cs-410,4,2, Statistical Language Model,"00:05:37,420","00:05:43,380",83,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=337,"As you can see, even though we have not",pic_cs-410_4_2_300.jpg
cs-410_4_2_84,cs-410,4,2, Statistical Language Model,"00:05:43,380","00:05:48,630",84,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=343,it actually allows us to compute,pic_cs-410_4_2_300.jpg
cs-410_4_2_85,cs-410,4,2, Statistical Language Model,"00:05:48,630","00:05:53,550",85,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=348,this model now only needs N,pic_cs-410_4_2_300.jpg
cs-410_4_2_86,cs-410,4,2, Statistical Language Model,"00:05:53,550","00:05:56,370",86,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=353,That means if we specify,pic_cs-410_4_2_300.jpg
cs-410_4_2_87,cs-410,4,2, Statistical Language Model,"00:05:56,370","00:06:01,850",87,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=356,"all the words, then the model's",pic_cs-410_4_2_300.jpg
cs-410_4_2_88,cs-410,4,2, Statistical Language Model,"00:06:01,850","00:06:06,220",88,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=361,"Whereas if we don't make this assumption,",pic_cs-410_4_2_360.jpg
cs-410_4_2_89,cs-410,4,2, Statistical Language Model,"00:06:06,220","00:06:09,720",89,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=366,all kinds of combinations,pic_cs-410_4_2_360.jpg
cs-410_4_2_90,cs-410,4,2, Statistical Language Model,"00:06:11,830","00:06:16,720",90,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=371,"So by making this assumption, it makes it",pic_cs-410_4_2_360.jpg
cs-410_4_2_91,cs-410,4,2, Statistical Language Model,"00:06:16,720","00:06:18,590",91,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=376,So let's see a specific example here.,pic_cs-410_4_2_360.jpg
cs-410_4_2_92,cs-410,4,2, Statistical Language Model,"00:06:19,810","00:06:25,450",92,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=379,Here I show two unigram language,pic_cs-410_4_2_360.jpg
cs-410_4_2_93,cs-410,4,2, Statistical Language Model,"00:06:25,450","00:06:28,050",93,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=385,And these are high probability,pic_cs-410_4_2_360.jpg
cs-410_4_2_94,cs-410,4,2, Statistical Language Model,"00:06:29,800","00:06:33,290",94,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=389,The first one clearly suggests,pic_cs-410_4_2_360.jpg
cs-410_4_2_95,cs-410,4,2, Statistical Language Model,"00:06:33,290","00:06:37,020",95,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=393,because the high probability,pic_cs-410_4_2_360.jpg
cs-410_4_2_96,cs-410,4,2, Statistical Language Model,"00:06:37,020","00:06:38,700",96,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=397,The second one is more related to health.,pic_cs-410_4_2_360.jpg
cs-410_4_2_97,cs-410,4,2, Statistical Language Model,"00:06:39,790","00:06:41,290",97,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=399,"Now we can ask the question,",pic_cs-410_4_2_360.jpg
cs-410_4_2_98,cs-410,4,2, Statistical Language Model,"00:06:41,290","00:06:46,520",98,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=401,how likely were observe a particular,pic_cs-410_4_2_360.jpg
cs-410_4_2_99,cs-410,4,2, Statistical Language Model,"00:06:46,520","00:06:49,920",99,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=406,Now suppose we sample,pic_cs-410_4_2_360.jpg
cs-410_4_2_100,cs-410,4,2, Statistical Language Model,"00:06:49,920","00:06:53,150",100,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=409,"Let's say we take the first distribution,",pic_cs-410_4_2_360.jpg
cs-410_4_2_101,cs-410,4,2, Statistical Language Model,"00:06:53,150","00:06:56,140",101,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=413,What words do you think would be,pic_cs-410_4_2_360.jpg
cs-410_4_2_102,cs-410,4,2, Statistical Language Model,"00:06:56,140","00:06:58,280",102,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=416,maybe mining maybe another word?,pic_cs-410_4_2_360.jpg
cs-410_4_2_103,cs-410,4,2, Statistical Language Model,"00:06:58,280","00:06:58,860",103,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=418,"Even food,",pic_cs-410_4_2_360.jpg
cs-410_4_2_104,cs-410,4,2, Statistical Language Model,"00:06:58,860","00:07:02,300",104,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=418,"which has a very small probability,",pic_cs-410_4_2_360.jpg
cs-410_4_2_105,cs-410,4,2, Statistical Language Model,"00:07:03,880","00:07:06,890",105,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=423,"But in general, high probability",pic_cs-410_4_2_420.jpg
cs-410_4_2_106,cs-410,4,2, Statistical Language Model,"00:07:08,130","00:07:11,200",106,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=428,So we can imagine what general text,pic_cs-410_4_2_420.jpg
cs-410_4_2_107,cs-410,4,2, Statistical Language Model,"00:07:12,230","00:07:14,630",107,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=432,"In fact, with small probability,",pic_cs-410_4_2_420.jpg
cs-410_4_2_108,cs-410,4,2, Statistical Language Model,"00:07:14,630","00:07:19,940",108,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=434,you might be able to actually generate,pic_cs-410_4_2_420.jpg
cs-410_4_2_109,cs-410,4,2, Statistical Language Model,"00:07:19,940","00:07:23,660",109,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=439,"Now, it will actually be meaningful,",pic_cs-410_4_2_420.jpg
cs-410_4_2_110,cs-410,4,2, Statistical Language Model,"00:07:23,660","00:07:24,400",110,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=443,very small.,pic_cs-410_4_2_420.jpg
cs-410_4_2_111,cs-410,4,2, Statistical Language Model,"00:07:26,100","00:07:30,220",111,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=446,"In an extreme case, you might",pic_cs-410_4_2_420.jpg
cs-410_4_2_112,cs-410,4,2, Statistical Language Model,"00:07:30,220","00:07:35,980",112,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=450,a text mining paper that would be,pic_cs-410_4_2_420.jpg
cs-410_4_2_113,cs-410,4,2, Statistical Language Model,"00:07:35,980","00:07:39,866",113,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=455,"And in that case,",pic_cs-410_4_2_420.jpg
cs-410_4_2_114,cs-410,4,2, Statistical Language Model,"00:07:39,866","00:07:42,152",114,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=459,"But it's a non-zero probability,",pic_cs-410_4_2_420.jpg
cs-410_4_2_115,cs-410,4,2, Statistical Language Model,"00:07:42,152","00:07:45,850",115,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=462,if we assume none of the words,pic_cs-410_4_2_420.jpg
cs-410_4_2_116,cs-410,4,2, Statistical Language Model,"00:07:47,430","00:07:49,380",116,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=467,"Similarly from the second topic,",pic_cs-410_4_2_420.jpg
cs-410_4_2_117,cs-410,4,2, Statistical Language Model,"00:07:49,380","00:07:52,660",117,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=469,we can imagine we can generate,pic_cs-410_4_2_420.jpg
cs-410_4_2_118,cs-410,4,2, Statistical Language Model,"00:07:52,660","00:07:58,220",118,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=472,That doesn't mean we cannot generate this,pic_cs-410_4_2_420.jpg
cs-410_4_2_119,cs-410,4,2, Statistical Language Model,"00:07:59,650","00:08:05,030",119,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=479,"We can, but the probability would be very,",pic_cs-410_4_2_420.jpg
cs-410_4_2_120,cs-410,4,2, Statistical Language Model,"00:08:05,030","00:08:09,300",120,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=485,generating a paper that can be accepted,pic_cs-410_4_2_480.jpg
cs-410_4_2_121,cs-410,4,2, Statistical Language Model,"00:08:10,400","00:08:12,470",121,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=490,So the point is that,pic_cs-410_4_2_480.jpg
cs-410_4_2_122,cs-410,4,2, Statistical Language Model,"00:08:13,590","00:08:18,410",122,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=493,we can talk about the probability of,pic_cs-410_4_2_480.jpg
cs-410_4_2_123,cs-410,4,2, Statistical Language Model,"00:08:18,410","00:08:20,790",123,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=498,Some texts will have higher,pic_cs-410_4_2_480.jpg
cs-410_4_2_124,cs-410,4,2, Statistical Language Model,"00:08:21,800","00:08:23,900",124,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=501,Now let's look at the problem,pic_cs-410_4_2_480.jpg
cs-410_4_2_125,cs-410,4,2, Statistical Language Model,"00:08:23,900","00:08:28,260",125,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=503,Suppose we now have available,pic_cs-410_4_2_480.jpg
cs-410_4_2_126,cs-410,4,2, Statistical Language Model,"00:08:28,260","00:08:31,960",126,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=508,"In this case, many of the abstract or",pic_cs-410_4_2_480.jpg
cs-410_4_2_127,cs-410,4,2, Statistical Language Model,"00:08:31,960","00:08:34,350",127,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=511,we see these word counts here.,pic_cs-410_4_2_480.jpg
cs-410_4_2_128,cs-410,4,2, Statistical Language Model,"00:08:34,350","00:08:36,846",128,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=514,The total number of words is 100.,pic_cs-410_4_2_480.jpg
cs-410_4_2_129,cs-410,4,2, Statistical Language Model,"00:08:36,846","00:08:39,530",129,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=516,Now the question you ask here,pic_cs-410_4_2_480.jpg
cs-410_4_2_130,cs-410,4,2, Statistical Language Model,"00:08:39,530","00:08:42,000",130,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=519,"We can ask the question which model,",pic_cs-410_4_2_480.jpg
cs-410_4_2_131,cs-410,4,2, Statistical Language Model,"00:08:42,000","00:08:46,340",131,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=522,which one of these distribution has,pic_cs-410_4_2_480.jpg
cs-410_4_2_132,cs-410,4,2, Statistical Language Model,"00:08:46,340","00:08:50,150",132,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=526,assuming that the text has been generated,pic_cs-410_4_2_480.jpg
cs-410_4_2_133,cs-410,4,2, Statistical Language Model,"00:08:51,970","00:08:53,100",133,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=531,So what would be your guess?,pic_cs-410_4_2_480.jpg
cs-410_4_2_134,cs-410,4,2, Statistical Language Model,"00:08:54,230","00:08:58,220",134,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=534,What we have to decide are what,pic_cs-410_4_2_480.jpg
cs-410_4_2_135,cs-410,4,2, Statistical Language Model,"00:08:58,220","00:08:58,860",135,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=538,would have.,pic_cs-410_4_2_480.jpg
cs-410_4_2_136,cs-410,4,2, Statistical Language Model,"00:09:01,971","00:09:05,260",136,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=541,"Suppose the view for a second, and",pic_cs-410_4_2_540.jpg
cs-410_4_2_137,cs-410,4,2, Statistical Language Model,"00:09:09,616","00:09:14,109",137,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=549,"If you're like a lot of people,",pic_cs-410_4_2_540.jpg
cs-410_4_2_138,cs-410,4,2, Statistical Language Model,"00:09:14,109","00:09:18,683",138,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=554,my best guess is text has a probability,pic_cs-410_4_2_540.jpg
cs-410_4_2_139,cs-410,4,2, Statistical Language Model,"00:09:18,683","00:09:23,310",139,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=558,"seen text 10 times, and",pic_cs-410_4_2_540.jpg
cs-410_4_2_140,cs-410,4,2, Statistical Language Model,"00:09:23,310","00:09:25,990",140,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=563,So we simply normalize these counts.,pic_cs-410_4_2_540.jpg
cs-410_4_2_141,cs-410,4,2, Statistical Language Model,"00:09:27,242","00:09:29,550",141,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=567,"And that's in fact the word justified, and",pic_cs-410_4_2_540.jpg
cs-410_4_2_142,cs-410,4,2, Statistical Language Model,"00:09:29,550","00:09:33,650",142,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=569,your intuition is consistent,pic_cs-410_4_2_540.jpg
cs-410_4_2_143,cs-410,4,2, Statistical Language Model,"00:09:33,650","00:09:36,170",143,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=573,And this is called the maximum,pic_cs-410_4_2_540.jpg
cs-410_4_2_144,cs-410,4,2, Statistical Language Model,"00:09:36,170","00:09:40,130",144,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=576,"In this estimator,",pic_cs-410_4_2_540.jpg
cs-410_4_2_145,cs-410,4,2, Statistical Language Model,"00:09:40,130","00:09:44,650",145,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=580,of those that would give our observe,pic_cs-410_4_2_540.jpg
cs-410_4_2_146,cs-410,4,2, Statistical Language Model,"00:09:44,650","00:09:49,050",146,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=584,That means if we change these,pic_cs-410_4_2_540.jpg
cs-410_4_2_147,cs-410,4,2, Statistical Language Model,"00:09:49,050","00:09:53,319",147,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=589,observing the particular text,pic_cs-410_4_2_540.jpg
cs-410_4_2_148,cs-410,4,2, Statistical Language Model,"00:09:55,190","00:09:58,840",148,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=595,"So you can see,",pic_cs-410_4_2_540.jpg
cs-410_4_2_149,cs-410,4,2, Statistical Language Model,"00:09:58,840","00:10:05,030",149,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=598,"Basically, we just need to look at",pic_cs-410_4_2_540.jpg
cs-410_4_2_150,cs-410,4,2, Statistical Language Model,"00:10:05,030","00:10:08,987",150,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=605,and then divide it by the total number of,pic_cs-410_4_2_600.jpg
cs-410_4_2_151,cs-410,4,2, Statistical Language Model,"00:10:08,987","00:10:11,670",151,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=608,Normalize the frequency.,pic_cs-410_4_2_600.jpg
cs-410_4_2_152,cs-410,4,2, Statistical Language Model,"00:10:11,670","00:10:13,090",152,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=611,"A consequence of this is,",pic_cs-410_4_2_600.jpg
cs-410_4_2_153,cs-410,4,2, Statistical Language Model,"00:10:13,090","00:10:18,200",153,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=613,"of course, we're going to assign",pic_cs-410_4_2_600.jpg
cs-410_4_2_154,cs-410,4,2, Statistical Language Model,"00:10:18,200","00:10:19,730",154,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=618,"If we have an observed word,",pic_cs-410_4_2_600.jpg
cs-410_4_2_155,cs-410,4,2, Statistical Language Model,"00:10:19,730","00:10:25,300",155,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=619,there will be no incentive to assign a,pic_cs-410_4_2_600.jpg
cs-410_4_2_156,cs-410,4,2, Statistical Language Model,"00:10:25,300","00:10:26,210",156,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=625,Why?,pic_cs-410_4_2_600.jpg
cs-410_4_2_157,cs-410,4,2, Statistical Language Model,"00:10:26,210","00:10:30,840",157,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=626,Because that would take away probability,pic_cs-410_4_2_600.jpg
cs-410_4_2_158,cs-410,4,2, Statistical Language Model,"00:10:30,840","00:10:33,516",158,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=630,And that obviously wouldn't maximize,pic_cs-410_4_2_600.jpg
cs-410_4_2_159,cs-410,4,2, Statistical Language Model,"00:10:33,516","00:10:37,430",159,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=633,the probability of this,pic_cs-410_4_2_600.jpg
cs-410_4_2_160,cs-410,4,2, Statistical Language Model,"00:10:37,430","00:10:42,050",160,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=637,But one has still question whether,pic_cs-410_4_2_600.jpg
cs-410_4_2_161,cs-410,4,2, Statistical Language Model,"00:10:42,050","00:10:47,820",161,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=642,"Well, the answer depends on what kind",pic_cs-410_4_2_600.jpg
cs-410_4_2_162,cs-410,4,2, Statistical Language Model,"00:10:47,820","00:10:52,320",162,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=647,This estimator gives a best model,pic_cs-410_4_2_600.jpg
cs-410_4_2_163,cs-410,4,2, Statistical Language Model,"00:10:52,320","00:10:57,400",163,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=652,But if you are interested in a model,pic_cs-410_4_2_600.jpg
cs-410_4_2_164,cs-410,4,2, Statistical Language Model,"00:10:57,400","00:11:01,910",164,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=657,"paper for this abstract, then you",pic_cs-410_4_2_600.jpg
cs-410_4_2_165,cs-410,4,2, Statistical Language Model,"00:11:01,910","00:11:07,330",165,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=661,"So for thing,",pic_cs-410_4_2_660.jpg
cs-410_4_2_166,cs-410,4,2, Statistical Language Model,"00:11:07,330","00:11:11,570",166,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=667,"of that article, so",pic_cs-410_4_2_660.jpg
cs-410_4_2_167,cs-410,4,2, Statistical Language Model,"00:11:11,570","00:11:14,390",167,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=671,even though they're not,pic_cs-410_4_2_660.jpg
cs-410_4_2_168,cs-410,4,2, Statistical Language Model,"00:11:14,390","00:11:17,750",168,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=674,So we're going to cover this,pic_cs-410_4_2_660.jpg
cs-410_4_2_169,cs-410,4,2, Statistical Language Model,"00:11:17,750","00:11:22,520",169,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=677,in this class in the query,pic_cs-410_4_2_660.jpg
cs-410_4_2_170,cs-410,4,2, Statistical Language Model,"00:11:24,350","00:11:29,520",170,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=684,So let's take a look at some possible,pic_cs-410_4_2_660.jpg
cs-410_4_2_171,cs-410,4,2, Statistical Language Model,"00:11:29,520","00:11:32,820",171,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=689,One use is simply to use,pic_cs-410_4_2_660.jpg
cs-410_4_2_172,cs-410,4,2, Statistical Language Model,"00:11:32,820","00:11:37,140",172,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=692,So here I show some general,pic_cs-410_4_2_660.jpg
cs-410_4_2_173,cs-410,4,2, Statistical Language Model,"00:11:37,140","00:11:39,830",173,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=697,We can use this text to,pic_cs-410_4_2_660.jpg
cs-410_4_2_174,cs-410,4,2, Statistical Language Model,"00:11:39,830","00:11:41,530",174,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=699,and the model might look like this.,pic_cs-410_4_2_660.jpg
cs-410_4_2_175,cs-410,4,2, Statistical Language Model,"00:11:42,720","00:11:47,845",175,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=702,"Right, so on the top, we have those",pic_cs-410_4_2_660.jpg
cs-410_4_2_176,cs-410,4,2, Statistical Language Model,"00:11:47,845","00:11:52,610",176,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=707,"etc., and then we'll see some",pic_cs-410_4_2_660.jpg
cs-410_4_2_177,cs-410,4,2, Statistical Language Model,"00:11:52,610","00:11:55,310",177,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=712,"then some very,",pic_cs-410_4_2_660.jpg
cs-410_4_2_178,cs-410,4,2, Statistical Language Model,"00:11:55,310","00:11:57,460",178,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=715,This is a background language model.,pic_cs-410_4_2_660.jpg
cs-410_4_2_179,cs-410,4,2, Statistical Language Model,"00:11:57,460","00:12:01,900",179,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=717,It represents the frequency of,pic_cs-410_4_2_660.jpg
cs-410_4_2_180,cs-410,4,2, Statistical Language Model,"00:12:01,900","00:12:04,140",180,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=721,This is the background model.,pic_cs-410_4_2_720.jpg
cs-410_4_2_181,cs-410,4,2, Statistical Language Model,"00:12:04,140","00:12:08,000",181,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=724,"Now let's look at another text,",pic_cs-410_4_2_720.jpg
cs-410_4_2_182,cs-410,4,2, Statistical Language Model,"00:12:08,000","00:12:09,979",182,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=728,we'll look at the computer,pic_cs-410_4_2_720.jpg
cs-410_4_2_183,cs-410,4,2, Statistical Language Model,"00:12:11,030","00:12:13,800",183,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=731,So we have a collection of,pic_cs-410_4_2_720.jpg
cs-410_4_2_184,cs-410,4,2, Statistical Language Model,"00:12:13,800","00:12:17,454",184,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=733,"we do as mentioned again, we can just",pic_cs-410_4_2_720.jpg
cs-410_4_2_185,cs-410,4,2, Statistical Language Model,"00:12:17,454","00:12:19,640",185,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=737,where we simply normalize the frequencies.,pic_cs-410_4_2_720.jpg
cs-410_4_2_186,cs-410,4,2, Statistical Language Model,"00:12:20,690","00:12:24,326",186,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=740,"Now in this case, we'll get",pic_cs-410_4_2_720.jpg
cs-410_4_2_187,cs-410,4,2, Statistical Language Model,"00:12:24,326","00:12:28,141",187,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=744,"On the top, it looks similar because",pic_cs-410_4_2_720.jpg
cs-410_4_2_188,cs-410,4,2, Statistical Language Model,"00:12:28,141","00:12:29,406",188,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=748,they are very common.,pic_cs-410_4_2_720.jpg
cs-410_4_2_189,cs-410,4,2, Statistical Language Model,"00:12:29,406","00:12:34,243",189,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=749,"But as we go down,",pic_cs-410_4_2_720.jpg
cs-410_4_2_190,cs-410,4,2, Statistical Language Model,"00:12:34,243","00:12:38,806",190,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=754,"computer science,",pic_cs-410_4_2_720.jpg
cs-410_4_2_191,cs-410,4,2, Statistical Language Model,"00:12:38,806","00:12:43,146",191,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=758,"And so although here, we might also see",pic_cs-410_4_2_720.jpg
cs-410_4_2_192,cs-410,4,2, Statistical Language Model,"00:12:43,146","00:12:47,490",192,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=763,we can imagine the probability here is,pic_cs-410_4_2_720.jpg
cs-410_4_2_193,cs-410,4,2, Statistical Language Model,"00:12:47,490","00:12:55,776",193,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=767,And we will see many other words here that,pic_cs-410_4_2_720.jpg
cs-410_4_2_194,cs-410,4,2, Statistical Language Model,"00:12:55,776","00:12:58,737",194,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=775,So you can see this distribution,pic_cs-410_4_2_720.jpg
cs-410_4_2_195,cs-410,4,2, Statistical Language Model,"00:12:58,737","00:13:00,830",195,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=778,the corresponding text.,pic_cs-410_4_2_720.jpg
cs-410_4_2_196,cs-410,4,2, Statistical Language Model,"00:13:00,830","00:13:02,870",196,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=780,We can look at even the smaller text.,pic_cs-410_4_2_780.jpg
cs-410_4_2_197,cs-410,4,2, Statistical Language Model,"00:13:03,970","00:13:06,870",197,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=783,"So in this case,",pic_cs-410_4_2_780.jpg
cs-410_4_2_198,cs-410,4,2, Statistical Language Model,"00:13:06,870","00:13:10,047",198,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=786,"Now if we do the same,",pic_cs-410_4_2_780.jpg
cs-410_4_2_199,cs-410,4,2, Statistical Language Model,"00:13:10,047","00:13:12,740",199,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=790,again the can be expected,pic_cs-410_4_2_780.jpg
cs-410_4_2_200,cs-410,4,2, Statistical Language Model,"00:13:12,740","00:13:16,927",200,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=792,"The sooner we see text, mining,",pic_cs-410_4_2_780.jpg
cs-410_4_2_201,cs-410,4,2, Statistical Language Model,"00:13:16,927","00:13:20,440",201,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=796,these words have relatively,pic_cs-410_4_2_780.jpg
cs-410_4_2_202,cs-410,4,2, Statistical Language Model,"00:13:20,440","00:13:27,540",202,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=800,"In contrast, in this distribution, the",pic_cs-410_4_2_780.jpg
cs-410_4_2_203,cs-410,4,2, Statistical Language Model,"00:13:27,540","00:13:32,190",203,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=807,"So this means, again,",pic_cs-410_4_2_780.jpg
cs-410_4_2_204,cs-410,4,2, Statistical Language Model,"00:13:32,190","00:13:36,266",204,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=812,"we can have a different model,",pic_cs-410_4_2_780.jpg
cs-410_4_2_205,cs-410,4,2, Statistical Language Model,"00:13:36,266","00:13:40,450",205,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=816,So we call this document,pic_cs-410_4_2_780.jpg
cs-410_4_2_206,cs-410,4,2, Statistical Language Model,"00:13:40,450","00:13:42,530",206,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=820,we call this collection language model.,pic_cs-410_4_2_780.jpg
cs-410_4_2_207,cs-410,4,2, Statistical Language Model,"00:13:42,530","00:13:46,580",207,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=822,"And later, you will see how they're",pic_cs-410_4_2_780.jpg
cs-410_4_2_208,cs-410,4,2, Statistical Language Model,"00:13:47,650","00:13:50,690",208,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=827,"But now,",pic_cs-410_4_2_780.jpg
cs-410_4_2_209,cs-410,4,2, Statistical Language Model,"00:13:50,690","00:13:55,210",209,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=830,Can we statistically find what words,pic_cs-410_4_2_780.jpg
cs-410_4_2_210,cs-410,4,2, Statistical Language Model,"00:13:56,900","00:13:58,770",210,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=836,Now how do we find such words?,pic_cs-410_4_2_780.jpg
cs-410_4_2_211,cs-410,4,2, Statistical Language Model,"00:13:58,770","00:14:04,230",211,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=838,"Well, our first thought is that let's take",pic_cs-410_4_2_780.jpg
cs-410_4_2_212,cs-410,4,2, Statistical Language Model,"00:14:04,230","00:14:08,860",212,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=844,So we can take a look at all the documents,pic_cs-410_4_2_840.jpg
cs-410_4_2_213,cs-410,4,2, Statistical Language Model,"00:14:08,860","00:14:10,930",213,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=848,Let's build a language model.,pic_cs-410_4_2_840.jpg
cs-410_4_2_214,cs-410,4,2, Statistical Language Model,"00:14:10,930","00:14:13,220",214,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=850,We can see what words we see there.,pic_cs-410_4_2_840.jpg
cs-410_4_2_215,cs-410,4,2, Statistical Language Model,"00:14:13,220","00:14:19,430",215,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=853,"Well, not surprisingly, we see these",pic_cs-410_4_2_840.jpg
cs-410_4_2_216,cs-410,4,2, Statistical Language Model,"00:14:19,430","00:14:23,490",216,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=859,"So in this case, this language model gives",pic_cs-410_4_2_840.jpg
cs-410_4_2_217,cs-410,4,2, Statistical Language Model,"00:14:23,490","00:14:26,260",217,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=863,the word in the context of computer.,pic_cs-410_4_2_840.jpg
cs-410_4_2_218,cs-410,4,2, Statistical Language Model,"00:14:26,260","00:14:29,370",218,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=866,And these common words will,pic_cs-410_4_2_840.jpg
cs-410_4_2_219,cs-410,4,2, Statistical Language Model,"00:14:29,370","00:14:31,750",219,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=869,But we also see the computer itself and,pic_cs-410_4_2_840.jpg
cs-410_4_2_220,cs-410,4,2, Statistical Language Model,"00:14:31,750","00:14:35,490",220,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=871,software will have relatively,pic_cs-410_4_2_840.jpg
cs-410_4_2_221,cs-410,4,2, Statistical Language Model,"00:14:35,490","00:14:37,320",221,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=875,"But if we just use this model,",pic_cs-410_4_2_840.jpg
cs-410_4_2_222,cs-410,4,2, Statistical Language Model,"00:14:37,320","00:14:42,037",222,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=877,we cannot just say all these words,pic_cs-410_4_2_840.jpg
cs-410_4_2_223,cs-410,4,2, Statistical Language Model,"00:14:43,210","00:14:50,700",223,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=883,"So ultimately, what we'd like to",pic_cs-410_4_2_840.jpg
cs-410_4_2_224,cs-410,4,2, Statistical Language Model,"00:14:50,700","00:14:51,420",224,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=890,How can we do that?,pic_cs-410_4_2_840.jpg
cs-410_4_2_225,cs-410,4,2, Statistical Language Model,"00:14:52,760","00:14:55,571",225,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=892,It turns out that it's possible,pic_cs-410_4_2_840.jpg
cs-410_4_2_226,cs-410,4,2, Statistical Language Model,"00:14:57,610","00:15:00,020",226,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=897,But I suggest you think about that.,pic_cs-410_4_2_840.jpg
cs-410_4_2_227,cs-410,4,2, Statistical Language Model,"00:15:00,020","00:15:03,510",227,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=900,So how can we know what,pic_cs-410_4_2_900.jpg
cs-410_4_2_228,cs-410,4,2, Statistical Language Model,"00:15:03,510","00:15:06,030",228,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=903,so that we want to kind,pic_cs-410_4_2_900.jpg
cs-410_4_2_229,cs-410,4,2, Statistical Language Model,"00:15:07,730","00:15:10,220",229,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=907,What model will tell us that?,pic_cs-410_4_2_900.jpg
cs-410_4_2_230,cs-410,4,2, Statistical Language Model,"00:15:10,220","00:15:14,180",230,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=910,"Well, maybe you can think about that.",pic_cs-410_4_2_900.jpg
cs-410_4_2_231,cs-410,4,2, Statistical Language Model,"00:15:14,180","00:15:18,170",231,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=914,So the background language model,pic_cs-410_4_2_900.jpg
cs-410_4_2_232,cs-410,4,2, Statistical Language Model,"00:15:18,170","00:15:21,240",232,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=918,It tells us what was,pic_cs-410_4_2_900.jpg
cs-410_4_2_233,cs-410,4,2, Statistical Language Model,"00:15:21,240","00:15:23,510",233,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=921,"So if we use this background model,",pic_cs-410_4_2_900.jpg
cs-410_4_2_234,cs-410,4,2, Statistical Language Model,"00:15:23,510","00:15:28,390",234,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=923,we would know that these words,pic_cs-410_4_2_900.jpg
cs-410_4_2_235,cs-410,4,2, Statistical Language Model,"00:15:28,390","00:15:31,595",235,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=928,So it's not surprising to observe,pic_cs-410_4_2_900.jpg
cs-410_4_2_236,cs-410,4,2, Statistical Language Model,"00:15:31,595","00:15:36,380",236,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=931,Whereas computer has a very,pic_cs-410_4_2_900.jpg
cs-410_4_2_237,cs-410,4,2, Statistical Language Model,"00:15:36,380","00:15:41,200",237,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=936,it's very surprising that we have seen,pic_cs-410_4_2_900.jpg
cs-410_4_2_238,cs-410,4,2, Statistical Language Model,"00:15:41,200","00:15:42,740",238,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=941,the same is true for software.,pic_cs-410_4_2_900.jpg
cs-410_4_2_239,cs-410,4,2, Statistical Language Model,"00:15:44,220","00:15:48,750",239,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=944,So then we can use these two,pic_cs-410_4_2_900.jpg
cs-410_4_2_240,cs-410,4,2, Statistical Language Model,"00:15:48,750","00:15:52,590",240,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=948,the words that are related to computer.,pic_cs-410_4_2_900.jpg
cs-410_4_2_241,cs-410,4,2, Statistical Language Model,"00:15:52,590","00:15:57,310",241,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=952,"For example, we can simply take the ratio",pic_cs-410_4_2_900.jpg
cs-410_4_2_242,cs-410,4,2, Statistical Language Model,"00:15:57,310","00:16:01,050",242,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=957,normalize the topic of language model,pic_cs-410_4_2_900.jpg
cs-410_4_2_243,cs-410,4,2, Statistical Language Model,"00:16:01,050","00:16:02,900",243,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=961,the background language model.,pic_cs-410_4_2_960.jpg
cs-410_4_2_244,cs-410,4,2, Statistical Language Model,"00:16:02,900","00:16:07,632",244,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=962,"So if we do that, we take the ratio,",pic_cs-410_4_2_960.jpg
cs-410_4_2_245,cs-410,4,2, Statistical Language Model,"00:16:07,632","00:16:11,371",245,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=967,"computer is ranked, and",pic_cs-410_4_2_960.jpg
cs-410_4_2_246,cs-410,4,2, Statistical Language Model,"00:16:11,371","00:16:14,796",246,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=971,"program, all these words",pic_cs-410_4_2_960.jpg
cs-410_4_2_247,cs-410,4,2, Statistical Language Model,"00:16:14,796","00:16:19,371",247,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=974,Because they occur very frequently in the,pic_cs-410_4_2_960.jpg
cs-410_4_2_248,cs-410,4,2, Statistical Language Model,"00:16:19,371","00:16:23,960",248,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=979,"the whole collection, whereas these common",pic_cs-410_4_2_960.jpg
cs-410_4_2_249,cs-410,4,2, Statistical Language Model,"00:16:23,960","00:16:27,850",249,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=983,"In fact,",pic_cs-410_4_2_960.jpg
cs-410_4_2_250,cs-410,4,2, Statistical Language Model,"00:16:27,850","00:16:30,780",250,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=987,because they are not really,pic_cs-410_4_2_960.jpg
cs-410_4_2_251,cs-410,4,2, Statistical Language Model,"00:16:30,780","00:16:34,920",251,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=990,By taking the sample of text,pic_cs-410_4_2_960.jpg
cs-410_4_2_252,cs-410,4,2, Statistical Language Model,"00:16:34,920","00:16:39,240",252,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=994,we don't really see more occurrences,pic_cs-410_4_2_960.jpg
cs-410_4_2_253,cs-410,4,2, Statistical Language Model,"00:16:40,250","00:16:43,310",253,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1000,So this shows that even with,pic_cs-410_4_2_960.jpg
cs-410_4_2_254,cs-410,4,2, Statistical Language Model,"00:16:43,310","00:16:46,450",254,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1003,we can do some limited,pic_cs-410_4_2_960.jpg
cs-410_4_2_255,cs-410,4,2, Statistical Language Model,"00:16:48,370","00:16:52,343",255,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1008,"So in this lecture,",pic_cs-410_4_2_960.jpg
cs-410_4_2_256,cs-410,4,2, Statistical Language Model,"00:16:52,343","00:16:56,776",256,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1012,which is basically a probability,pic_cs-410_4_2_960.jpg
cs-410_4_2_257,cs-410,4,2, Statistical Language Model,"00:16:56,776","00:17:00,067",257,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1016,We talked about the simplest language,pic_cs-410_4_2_960.jpg
cs-410_4_2_258,cs-410,4,2, Statistical Language Model,"00:17:00,067","00:17:02,720",258,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1020,which is also just a word distribution.,pic_cs-410_4_2_1020.jpg
cs-410_4_2_259,cs-410,4,2, Statistical Language Model,"00:17:02,720","00:17:05,320",259,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1022,We talked about the two,pic_cs-410_4_2_1020.jpg
cs-410_4_2_260,cs-410,4,2, Statistical Language Model,"00:17:05,320","00:17:10,360",260,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1025,One is we represent the topic in a,pic_cs-410_4_2_1020.jpg
cs-410_4_2_261,cs-410,4,2, Statistical Language Model,"00:17:10,360","00:17:12,650",261,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1030,The other is we discover,pic_cs-410_4_2_1020.jpg
cs-410_4_2_262,cs-410,4,2, Statistical Language Model,"00:17:16,456","00:17:20,089",262,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1036,"In the next lecture, we're going to talk",pic_cs-410_4_2_1020.jpg
cs-410_4_2_263,cs-410,4,2, Statistical Language Model,"00:17:20,089","00:17:21,510",263,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1040,design a retrieval function.,pic_cs-410_4_2_1020.jpg
cs-410_4_2_264,cs-410,4,2, Statistical Language Model,"00:17:23,260","00:17:24,960",264,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1043,Here are two additional readings.,pic_cs-410_4_2_1020.jpg
cs-410_4_2_265,cs-410,4,2, Statistical Language Model,"00:17:24,960","00:17:28,850",265,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1044,The first is a textbook on statistical,pic_cs-410_4_2_1020.jpg
cs-410_4_2_266,cs-410,4,2, Statistical Language Model,"00:17:30,290","00:17:35,249",266,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1050,The second is an article that,pic_cs-410_4_2_1020.jpg
cs-410_4_2_267,cs-410,4,2, Statistical Language Model,"00:17:35,249","00:17:40,326",267,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1055,language models with a lot of,pic_cs-410_4_2_1020.jpg
cs-410_4_2_268,cs-410,4,2, Statistical Language Model,"00:17:40,326","00:17:50,326",268,https://www.coursera.org/learn/cs-410/lecture/kv4Aj?t=1060,[MUSIC],pic_cs-410_4_2_1020.jpg
cs-410_4_3_1,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:00,012","00:00:03,532",1,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=0,[SOUND],pic_cs-410_4_3_0.jpg
cs-410_4_3_2,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:07,767","00:00:10,058",2,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=7,"This lecture is about query likelihood,",pic_cs-410_4_3_0.jpg
cs-410_4_3_3,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:10,058","00:00:11,960",3,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=10,probabilistic retrieval model.,pic_cs-410_4_3_0.jpg
cs-410_4_3_4,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:14,040","00:00:15,310",4,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=14,"In this lecture,",pic_cs-410_4_3_0.jpg
cs-410_4_3_5,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:15,310","00:00:19,190",5,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=15,we continue the discussion of,pic_cs-410_4_3_0.jpg
cs-410_4_3_6,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:19,190","00:00:22,830",6,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=19,"In particular, we're going to talk about",pic_cs-410_4_3_0.jpg
cs-410_4_3_7,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:25,870","00:00:31,073",7,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=25,"In the query light holder retrieval model,",pic_cs-410_4_3_0.jpg
cs-410_4_3_8,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:31,073","00:00:35,420",8,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=31,How like their user who likes a document,pic_cs-410_4_3_0.jpg
cs-410_4_3_9,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:36,990","00:00:41,462",9,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=36,"So in this case,",pic_cs-410_4_3_0.jpg
cs-410_4_3_10,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:41,462","00:00:46,663",10,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=41,particular document about,pic_cs-410_4_3_0.jpg
cs-410_4_3_11,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:46,663","00:00:50,410",11,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=46,"Now we assume,",pic_cs-410_4_3_0.jpg
cs-410_4_3_12,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:50,410","00:00:54,780",12,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=50,a basis to impose a query to try and,pic_cs-410_4_3_0.jpg
cs-410_4_3_13,cs-410,4,3, Query Likelihood Retrieval Function,"00:00:57,340","00:01:03,840",13,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=57,"So again, imagine use a process",pic_cs-410_4_3_0.jpg
cs-410_4_3_14,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:03,840","00:01:06,940",14,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=63,Where we assume that,pic_cs-410_4_3_60.jpg
cs-410_4_3_15,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:06,940","00:01:08,640",15,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=66,assembling words from the document.,pic_cs-410_4_3_60.jpg
cs-410_4_3_16,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:10,560","00:01:15,880",16,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=70,"So for example, a user might",pic_cs-410_4_3_60.jpg
cs-410_4_3_17,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:15,880","00:01:19,390",17,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=75,from this document and,pic_cs-410_4_3_60.jpg
cs-410_4_3_18,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:20,600","00:01:24,590",18,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=80,And then the user would pick,pic_cs-410_4_3_60.jpg
cs-410_4_3_19,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:24,590","00:01:25,910",19,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=84,that would be the second query word.,pic_cs-410_4_3_60.jpg
cs-410_4_3_20,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:27,420","00:01:32,400",20,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=87,Now this of course is an assumption,pic_cs-410_4_3_60.jpg
cs-410_4_3_21,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:32,400","00:01:35,008",21,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=92,how a user would pose a query.,pic_cs-410_4_3_60.jpg
cs-410_4_3_22,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:35,008","00:01:39,788",22,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=95,Whether a user actually followed this,pic_cs-410_4_3_60.jpg
cs-410_4_3_23,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:39,788","00:01:45,230",23,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=99,this assumption has allowed us to formerly,pic_cs-410_4_3_60.jpg
cs-410_4_3_24,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:46,390","00:01:50,930",24,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=106,And this allows us to also not rely on,pic_cs-410_4_3_60.jpg
cs-410_4_3_25,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:52,580","00:01:55,750",25,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=112,to use empirical data to,pic_cs-410_4_3_60.jpg
cs-410_4_3_26,cs-410,4,3, Query Likelihood Retrieval Function,"00:01:56,870","00:02:00,820",26,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=116,And this is why we can use this,pic_cs-410_4_3_60.jpg
cs-410_4_3_27,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:00,820","00:02:03,569",27,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=120,retrieval function that we can,pic_cs-410_4_3_120.jpg
cs-410_4_3_28,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:04,900","00:02:08,991",28,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=124,So as you see the assumption,pic_cs-410_4_3_120.jpg
cs-410_4_3_29,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:08,991","00:02:11,558",29,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=128,word is independent of the sample.,pic_cs-410_4_3_120.jpg
cs-410_4_3_30,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:11,558","00:02:17,880",30,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=131,And also each word is basically,pic_cs-410_4_3_120.jpg
cs-410_4_3_31,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:20,910","00:02:24,540",31,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=140,So now let's see how this works exactly.,pic_cs-410_4_3_120.jpg
cs-410_4_3_32,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:24,540","00:02:28,550",32,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=144,"Well, since we are completing",pic_cs-410_4_3_120.jpg
cs-410_4_3_33,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:29,730","00:02:34,444",33,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=149,then the probability here is just,pic_cs-410_4_3_120.jpg
cs-410_4_3_34,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:34,444","00:02:37,210",34,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=154,which is a sequence of words.,pic_cs-410_4_3_120.jpg
cs-410_4_3_35,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:37,210","00:02:42,140",35,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=157,And we make the assumption that each,pic_cs-410_4_3_120.jpg
cs-410_4_3_36,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:42,140","00:02:46,670",36,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=162,"So as a result, the probability",pic_cs-410_4_3_120.jpg
cs-410_4_3_37,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:46,670","00:02:48,920",37,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=166,of the probability of each query word.,pic_cs-410_4_3_120.jpg
cs-410_4_3_38,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:50,100","00:02:52,660",38,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=170,Now how do we compute,pic_cs-410_4_3_120.jpg
cs-410_4_3_39,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:52,660","00:02:56,740",39,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=172,"Well, based on the assumption that a word",pic_cs-410_4_3_120.jpg
cs-410_4_3_40,cs-410,4,3, Query Likelihood Retrieval Function,"00:02:56,740","00:03:01,360",40,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=176,is picked from the document,pic_cs-410_4_3_120.jpg
cs-410_4_3_41,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:01,360","00:03:05,680",41,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=181,Now we know the probability of each word,pic_cs-410_4_3_180.jpg
cs-410_4_3_42,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:05,680","00:03:08,120",42,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=185,word in the document.,pic_cs-410_4_3_180.jpg
cs-410_4_3_43,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:08,120","00:03:13,780",43,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=188,"So for example, the probability of",pic_cs-410_4_3_180.jpg
cs-410_4_3_44,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:13,780","00:03:17,520",44,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=193,Would be just the count,pic_cs-410_4_3_180.jpg
cs-410_4_3_45,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:17,520","00:03:23,060",45,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=197,divided by the total number of words,pic_cs-410_4_3_180.jpg
cs-410_4_3_46,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:23,060","00:03:28,940",46,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=203,So with these assumptions we now have,pic_cs-410_4_3_180.jpg
cs-410_4_3_47,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:28,940","00:03:30,970",47,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=208,We can use this to rank our documents.,pic_cs-410_4_3_180.jpg
cs-410_4_3_48,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:32,650","00:03:34,200",48,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=212,So does this model work?,pic_cs-410_4_3_180.jpg
cs-410_4_3_49,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:34,200","00:03:35,260",49,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=214,Let's take a look.,pic_cs-410_4_3_180.jpg
cs-410_4_3_50,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:35,260","00:03:38,670",50,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=215,Here are some example documents,pic_cs-410_4_3_180.jpg
cs-410_4_3_51,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:38,670","00:03:42,210",51,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=218,Suppose now the query is,pic_cs-410_4_3_180.jpg
cs-410_4_3_52,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:42,210","00:03:44,880",52,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=222,we see the formula here on the top.,pic_cs-410_4_3_180.jpg
cs-410_4_3_53,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:45,900","00:03:47,490",53,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=225,So how do we score this document?,pic_cs-410_4_3_180.jpg
cs-410_4_3_54,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:47,490","00:03:48,790",54,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=227,"Well, it's very simple.",pic_cs-410_4_3_180.jpg
cs-410_4_3_55,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:48,790","00:03:51,370",55,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=228,We just count how many times do,pic_cs-410_4_3_180.jpg
cs-410_4_3_56,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:51,370","00:03:54,380",56,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=231,"how many times do we have seen campaigns,",pic_cs-410_4_3_180.jpg
cs-410_4_3_57,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:54,380","00:03:57,458",57,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=234,"And we see here 44, and",pic_cs-410_4_3_180.jpg
cs-410_4_3_58,cs-410,4,3, Query Likelihood Retrieval Function,"00:03:57,458","00:04:02,297",58,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=237,So that's 2 over the length of,pic_cs-410_4_3_180.jpg
cs-410_4_3_59,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:02,297","00:04:07,710",59,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=242,the length of document 4 for,pic_cs-410_4_3_240.jpg
cs-410_4_3_60,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:07,710","00:04:11,146",60,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=247,"And similarly, we can get probabilities",pic_cs-410_4_3_240.jpg
cs-410_4_3_61,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:13,189","00:04:17,505",61,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=253,Now if you look at these numbers or,pic_cs-410_4_3_240.jpg
cs-410_4_3_62,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:17,505","00:04:22,030",62,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=257,"scoring all these documents,",pic_cs-410_4_3_240.jpg
cs-410_4_3_63,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:22,030","00:04:28,436",63,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=262,Because if we assume d3 and,pic_cs-410_4_3_240.jpg
cs-410_4_3_64,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:28,436","00:04:35,628",64,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=268,then looks like a nominal rank d4,pic_cs-410_4_3_240.jpg
cs-410_4_3_65,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:35,628","00:04:40,819",65,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=275,"And as we would expect,",pic_cs-410_4_3_240.jpg
cs-410_4_3_66,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:40,819","00:04:45,916",66,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=280,"a TF query state, and so",pic_cs-410_4_3_240.jpg
cs-410_4_3_67,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:45,916","00:04:50,096",67,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=285,"However, if we try a different",pic_cs-410_4_3_240.jpg
cs-410_4_3_68,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:50,096","00:04:54,854",68,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=290,presidential campaign update,pic_cs-410_4_3_240.jpg
cs-410_4_3_69,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:54,854","00:04:56,608",69,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=294,Well what problem?,pic_cs-410_4_3_240.jpg
cs-410_4_3_70,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:56,608","00:04:58,930",70,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=296,Well think about the update.,pic_cs-410_4_3_240.jpg
cs-410_4_3_71,cs-410,4,3, Query Likelihood Retrieval Function,"00:04:58,930","00:05:02,500",71,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=298,Now none of these documents,pic_cs-410_4_3_240.jpg
cs-410_4_3_72,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:02,500","00:05:08,420",72,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=302,So according to our assumption that a user,pic_cs-410_4_3_300.jpg
cs-410_4_3_73,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:08,420","00:05:15,003",73,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=308,"generate a query, then the probability of",pic_cs-410_4_3_300.jpg
cs-410_4_3_74,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:15,003","00:05:16,070",74,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=315,Would be 0.,pic_cs-410_4_3_300.jpg
cs-410_4_3_75,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:17,230","00:05:21,380",75,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=317,"So that causes a problem,",pic_cs-410_4_3_300.jpg
cs-410_4_3_76,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:21,380","00:05:23,710",76,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=321,to have zero probability,pic_cs-410_4_3_300.jpg
cs-410_4_3_77,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:25,330","00:05:31,127",77,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=325,Now why it's fine to have zero probability,pic_cs-410_4_3_300.jpg
cs-410_4_3_78,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:31,127","00:05:33,902",78,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=331,It's not okay to have 0 for d3 and,pic_cs-410_4_3_300.jpg
cs-410_4_3_79,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:33,902","00:05:38,600",79,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=333,d4 because now we no longer,pic_cs-410_4_3_300.jpg
cs-410_4_3_80,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:38,600","00:05:39,135",80,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=338,What's worse?,pic_cs-410_4_3_300.jpg
cs-410_4_3_81,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:39,135","00:05:41,735",81,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=339,We can't even distinguish them from d2.,pic_cs-410_4_3_300.jpg
cs-410_4_3_82,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:41,735","00:05:45,700",82,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=341,So that's obviously not desirable.,pic_cs-410_4_3_300.jpg
cs-410_4_3_83,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:45,700","00:05:48,630",83,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=345,"Now when a [INAUDIBLE] has such result,",pic_cs-410_4_3_300.jpg
cs-410_4_3_84,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:48,630","00:05:50,960",84,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=348,we should think about what,pic_cs-410_4_3_300.jpg
cs-410_4_3_85,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:52,530","00:05:56,773",85,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=352,So we have to examine what,pic_cs-410_4_3_300.jpg
cs-410_4_3_86,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:56,773","00:05:59,644",86,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=356,as we derive this ranking function.,pic_cs-410_4_3_300.jpg
cs-410_4_3_87,cs-410,4,3, Query Likelihood Retrieval Function,"00:05:59,644","00:06:03,285",87,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=359,Now is you examine those assumptions,pic_cs-410_4_3_300.jpg
cs-410_4_3_88,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:03,285","00:06:04,983",88,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=363,what has caused this problem?,pic_cs-410_4_3_360.jpg
cs-410_4_3_89,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:04,983","00:06:09,080",89,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=364,So take a moment to think about it.,pic_cs-410_4_3_360.jpg
cs-410_4_3_90,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:09,080","00:06:17,179",90,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=369,What do you think is the reason why update,pic_cs-410_4_3_360.jpg
cs-410_4_3_91,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:17,179","00:06:22,092",91,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=377,So if you think about this from the moment,pic_cs-410_4_3_360.jpg
cs-410_4_3_92,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:22,092","00:06:25,317",92,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=382,have made an assumption,pic_cs-410_4_3_360.jpg
cs-410_4_3_93,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:25,317","00:06:29,220",93,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=385,be drawn from the document,pic_cs-410_4_3_360.jpg
cs-410_4_3_94,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:29,220","00:06:33,982",94,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=389,"So in order to fix this, we have to",pic_cs-410_4_3_360.jpg
cs-410_4_3_95,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:33,982","00:06:36,912",95,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=393,a word not necessarily from the document.,pic_cs-410_4_3_360.jpg
cs-410_4_3_96,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:36,912","00:06:38,930",96,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=396,So that's the improved model.,pic_cs-410_4_3_360.jpg
cs-410_4_3_97,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:38,930","00:06:40,912",97,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=398,"An improvement here is to say that,",pic_cs-410_4_3_360.jpg
cs-410_4_3_98,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:40,912","00:06:43,687",98,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=400,well instead of drawing,pic_cs-410_4_3_360.jpg
cs-410_4_3_99,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:43,687","00:06:48,064",99,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=403,let's imagine that the user would actually,pic_cs-410_4_3_360.jpg
cs-410_4_3_100,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:48,064","00:06:50,107",100,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=408,And so I show a model here.,pic_cs-410_4_3_360.jpg
cs-410_4_3_101,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:50,107","00:06:54,479",101,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=410,And we assume that this document is,pic_cs-410_4_3_360.jpg
cs-410_4_3_102,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:54,479","00:06:55,920",102,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=414,model.,pic_cs-410_4_3_360.jpg
cs-410_4_3_103,cs-410,4,3, Query Likelihood Retrieval Function,"00:06:55,920","00:07:01,297",103,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=415,"Now, this model doesn't necessarily assign",pic_cs-410_4_3_360.jpg
cs-410_4_3_104,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:01,297","00:07:05,853",104,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=421,we can assume this model does not,pic_cs-410_4_3_420.jpg
cs-410_4_3_105,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:05,853","00:07:09,621",105,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=425,Now if we're thinking this way then,pic_cs-410_4_3_420.jpg
cs-410_4_3_106,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:09,621","00:07:10,700",106,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=429,different.,pic_cs-410_4_3_420.jpg
cs-410_4_3_107,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:10,700","00:07:14,940",107,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=430,Now the user has this model in mind,pic_cs-410_4_3_420.jpg
cs-410_4_3_108,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:14,940","00:07:18,669",108,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=434,Although the model has to be,pic_cs-410_4_3_420.jpg
cs-410_4_3_109,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:18,669","00:07:22,960",109,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=438,So the user can again generate,pic_cs-410_4_3_420.jpg
cs-410_4_3_110,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:22,960","00:07:27,680",110,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=442,"Namely, pick a word for example,",pic_cs-410_4_3_420.jpg
cs-410_4_3_111,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:29,020","00:07:32,390",111,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=449,Now the difference is that this time,pic_cs-410_4_3_420.jpg
cs-410_4_3_112,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:32,390","00:07:34,930",112,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=452,even though update doesn't,pic_cs-410_4_3_420.jpg
cs-410_4_3_113,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:34,930","00:07:38,050",113,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=454,to potentially generate,pic_cs-410_4_3_420.jpg
cs-410_4_3_114,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:38,050","00:07:43,840",114,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=458,So that a query was updated,pic_cs-410_4_3_420.jpg
cs-410_4_3_115,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:43,840","00:07:45,720",115,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=463,So this would fix our problem.,pic_cs-410_4_3_420.jpg
cs-410_4_3_116,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:45,720","00:07:50,140",116,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=465,And it's also reasonable because when our,pic_cs-410_4_3_420.jpg
cs-410_4_3_117,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:50,140","00:07:55,160",117,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=470,"in a more general way, that is unique",pic_cs-410_4_3_420.jpg
cs-410_4_3_118,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:55,160","00:07:57,830",118,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=475,So how do we compute,pic_cs-410_4_3_420.jpg
cs-410_4_3_119,cs-410,4,3, Query Likelihood Retrieval Function,"00:07:57,830","00:08:01,000",119,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=477,If we make this sum wide,pic_cs-410_4_3_420.jpg
cs-410_4_3_120,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:01,000","00:08:07,390",120,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=481,"The first one is compute this model, and",pic_cs-410_4_3_480.jpg
cs-410_4_3_121,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:07,390","00:08:15,070",121,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=487,"For example, I've shown two pulse models",pic_cs-410_4_3_480.jpg
cs-410_4_3_122,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:15,070","00:08:19,803",122,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=495,And then given a query like a data mining,pic_cs-410_4_3_480.jpg
cs-410_4_3_123,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:19,803","00:08:22,467",123,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=499,just compute the likelihood of this query.,pic_cs-410_4_3_480.jpg
cs-410_4_3_124,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:22,467","00:08:26,574",124,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=502,And by making independence,pic_cs-410_4_3_480.jpg
cs-410_4_3_125,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:26,574","00:08:30,766",125,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=506,probability as a product of,pic_cs-410_4_3_480.jpg
cs-410_4_3_126,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:30,766","00:08:34,798",126,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=510,"We do this for both documents, and",pic_cs-410_4_3_480.jpg
cs-410_4_3_127,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:34,798","00:08:35,700",127,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=514,then rank them.,pic_cs-410_4_3_480.jpg
cs-410_4_3_128,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:37,160","00:08:41,310",128,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=517,So that's the basic idea of this,pic_cs-410_4_3_480.jpg
cs-410_4_3_129,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:41,310","00:08:47,890",129,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=521,So more generally this ranking function,pic_cs-410_4_3_480.jpg
cs-410_4_3_130,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:47,890","00:08:51,800",130,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=527,"Here we assume that the query has n words,",pic_cs-410_4_3_480.jpg
cs-410_4_3_131,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:51,800","00:08:56,540",131,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=531,"w1 through wn, and",pic_cs-410_4_3_480.jpg
cs-410_4_3_132,cs-410,4,3, Query Likelihood Retrieval Function,"00:08:56,540","00:09:01,500",132,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=536,The ranking function is the probability,pic_cs-410_4_3_480.jpg
cs-410_4_3_133,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:01,500","00:09:06,080",133,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=541,given that the user is,pic_cs-410_4_3_540.jpg
cs-410_4_3_134,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:06,080","00:09:11,970",134,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=546,And this is assume it will be product of,pic_cs-410_4_3_540.jpg
cs-410_4_3_135,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:11,970","00:09:15,360",135,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=551,This is based on independent assumption.,pic_cs-410_4_3_540.jpg
cs-410_4_3_136,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:15,360","00:09:20,256",136,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=555,Now we actually often score,pic_cs-410_4_3_540.jpg
cs-410_4_3_137,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:20,256","00:09:25,250",137,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=560,using log of the query likelihood,pic_cs-410_4_3_540.jpg
cs-410_4_3_138,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:26,710","00:09:30,220",138,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=566,Now we do this to avoid,pic_cs-410_4_3_540.jpg
cs-410_4_3_139,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:30,220","00:09:35,830",139,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=570,"having a lot of small probabilities,",pic_cs-410_4_3_540.jpg
cs-410_4_3_140,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:35,830","00:09:41,060",140,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=575,And this could cause under flow and we,pic_cs-410_4_3_540.jpg
cs-410_4_3_141,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:41,060","00:09:44,100",141,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=581,the value in our algorithm function.,pic_cs-410_4_3_540.jpg
cs-410_4_3_142,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:44,100","00:09:51,079",142,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=584,We maintain the order of these documents,pic_cs-410_4_3_540.jpg
cs-410_4_3_143,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:51,079","00:09:54,935",143,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=591,And so if we take longer than,pic_cs-410_4_3_540.jpg
cs-410_4_3_144,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:54,935","00:09:59,920",144,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=594,the product would become a sum,pic_cs-410_4_3_540.jpg
cs-410_4_3_145,cs-410,4,3, Query Likelihood Retrieval Function,"00:09:59,920","00:10:03,620",145,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=599,So the sum of all the query,pic_cs-410_4_3_540.jpg
cs-410_4_3_146,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:03,620","00:10:07,670",146,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=603,that is one of the probability of,pic_cs-410_4_3_600.jpg
cs-410_4_3_147,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:09,360","00:10:13,020",147,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=609,And then we can further rewrite,pic_cs-410_4_3_600.jpg
cs-410_4_3_148,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:14,310","00:10:19,960",148,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=614,"So in the first sum here, in this sum,",pic_cs-410_4_3_600.jpg
cs-410_4_3_149,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:21,910","00:10:28,800",149,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=621,we have it over all the query words and,pic_cs-410_4_3_600.jpg
cs-410_4_3_150,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:28,800","00:10:33,030",150,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=628,And in this sum we have a sum,pic_cs-410_4_3_600.jpg
cs-410_4_3_151,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:33,030","00:10:37,050",151,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=633,But we put a counter here,pic_cs-410_4_3_600.jpg
cs-410_4_3_152,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:37,050","00:10:39,780",152,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=637,Essentially we are only considering,pic_cs-410_4_3_600.jpg
cs-410_4_3_153,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:39,780","00:10:43,570",153,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=639,"because if a word is not in the query,",pic_cs-410_4_3_600.jpg
cs-410_4_3_154,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:43,570","00:10:46,820",154,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=643,So we're still considering,pic_cs-410_4_3_600.jpg
cs-410_4_3_155,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:46,820","00:10:49,760",155,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=646,But we're using a different form as,pic_cs-410_4_3_600.jpg
cs-410_4_3_156,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:49,760","00:10:51,570",156,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=649,all the words in the vocabulary.,pic_cs-410_4_3_600.jpg
cs-410_4_3_157,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:52,960","00:10:56,435",157,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=652,"And of course, a word might occur",pic_cs-410_4_3_600.jpg
cs-410_4_3_158,cs-410,4,3, Query Likelihood Retrieval Function,"00:10:56,435","00:10:58,631",158,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=656,That's why we have a count here.,pic_cs-410_4_3_600.jpg
cs-410_4_3_159,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:00,407","00:11:04,168",159,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=660,And then this part is log of,pic_cs-410_4_3_660.jpg
cs-410_4_3_160,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:04,168","00:11:06,815",160,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=664,given by the document language model.,pic_cs-410_4_3_660.jpg
cs-410_4_3_161,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:08,647","00:11:11,497",161,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=668,"So you can see in this retrieval function,",pic_cs-410_4_3_660.jpg
cs-410_4_3_162,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:11,497","00:11:13,547",162,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=671,we actually know the count,pic_cs-410_4_3_660.jpg
cs-410_4_3_163,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:13,547","00:11:16,507",163,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=673,So the only thing that we don't know,pic_cs-410_4_3_660.jpg
cs-410_4_3_164,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:17,817","00:11:21,310",164,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=677,"Therefore, we have converted",pic_cs-410_4_3_660.jpg
cs-410_4_3_165,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:21,310","00:11:24,510",165,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=681,include the problem of estimating,pic_cs-410_4_3_660.jpg
cs-410_4_3_166,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:25,920","00:11:30,370",166,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=685,So that we can compute the probability of,pic_cs-410_4_3_660.jpg
cs-410_4_3_167,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:32,260","00:11:36,630",167,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=692,And different estimation methods would,pic_cs-410_4_3_660.jpg
cs-410_4_3_168,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:36,630","00:11:40,980",168,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=696,This is just like a different way to,pic_cs-410_4_3_660.jpg
cs-410_4_3_169,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:40,980","00:11:45,485",169,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=700,which leads to a different ranking,pic_cs-410_4_3_660.jpg
cs-410_4_3_170,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:45,485","00:11:49,353",170,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=705,Here different ways to,pic_cs-410_4_3_660.jpg
cs-410_4_3_171,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:49,353","00:11:54,065",171,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=709,a different ranking function for,pic_cs-410_4_3_660.jpg
cs-410_4_3_172,cs-410,4,3, Query Likelihood Retrieval Function,"00:11:54,065","00:12:04,065",172,https://www.coursera.org/learn/cs-410/lecture/BWexZ?t=714,[MUSIC],pic_cs-410_4_3_660.jpg
cs-410_4_4_1,cs-410,4,4, Statistical Language Model - Part 1,"00:00:00,012","00:00:07,304",1,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=0,[SOUND],pic_cs-410_4_4_0.jpg
cs-410_4_4_2,cs-410,4,4, Statistical Language Model - Part 1,"00:00:07,304","00:00:10,420",2,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=7,lecture is about smoothing,pic_cs-410_4_4_0.jpg
cs-410_4_4_3,cs-410,4,4, Statistical Language Model - Part 1,"00:00:11,700","00:00:12,390",3,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=11,"In this lecture,",pic_cs-410_4_4_0.jpg
cs-410_4_4_4,cs-410,4,4, Statistical Language Model - Part 1,"00:00:12,390","00:00:16,110",4,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=12,we're going to continue talking about,pic_cs-410_4_4_0.jpg
cs-410_4_4_5,cs-410,4,4, Statistical Language Model - Part 1,"00:00:16,110","00:00:19,630",5,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=16,"In particular,",pic_cs-410_4_4_0.jpg
cs-410_4_4_6,cs-410,4,4, Statistical Language Model - Part 1,"00:00:19,630","00:00:22,390",6,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=19,language model in the query,pic_cs-410_4_4_0.jpg
cs-410_4_4_7,cs-410,4,4, Statistical Language Model - Part 1,"00:00:23,820","00:00:27,248",7,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=23,So you have seen this slide,pic_cs-410_4_4_0.jpg
cs-410_4_4_8,cs-410,4,4, Statistical Language Model - Part 1,"00:00:27,248","00:00:30,470",8,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=27,This is the ranking function,pic_cs-410_4_4_0.jpg
cs-410_4_4_9,cs-410,4,4, Statistical Language Model - Part 1,"00:00:32,540","00:00:39,906",9,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=32,"Here, we assume that the independence of",pic_cs-410_4_4_0.jpg
cs-410_4_4_10,cs-410,4,4, Statistical Language Model - Part 1,"00:00:39,906","00:00:45,367",10,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=39,would look like the following where,pic_cs-410_4_4_0.jpg
cs-410_4_4_11,cs-410,4,4, Statistical Language Model - Part 1,"00:00:45,367","00:00:49,878",11,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=45,And inside the sum there is a log,pic_cs-410_4_4_0.jpg
cs-410_4_4_12,cs-410,4,4, Statistical Language Model - Part 1,"00:00:49,878","00:00:52,700",12,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=49,the document or document image model.,pic_cs-410_4_4_0.jpg
cs-410_4_4_13,cs-410,4,4, Statistical Language Model - Part 1,"00:00:52,700","00:00:57,750",13,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=52,So the main task now is to estimate this,pic_cs-410_4_4_0.jpg
cs-410_4_4_14,cs-410,4,4, Statistical Language Model - Part 1,"00:00:57,750","00:01:02,100",14,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=57,document language model as we,pic_cs-410_4_4_0.jpg
cs-410_4_4_15,cs-410,4,4, Statistical Language Model - Part 1,"00:01:02,100","00:01:06,530",15,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=62,estimating this model would lead,pic_cs-410_4_4_60.jpg
cs-410_4_4_16,cs-410,4,4, Statistical Language Model - Part 1,"00:01:06,530","00:01:10,810",16,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=66,"So in this lecture, we're going to",pic_cs-410_4_4_60.jpg
cs-410_4_4_17,cs-410,4,4, Statistical Language Model - Part 1,"00:01:10,810","00:01:13,110",17,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=70,So how do we estimate this language model?,pic_cs-410_4_4_60.jpg
cs-410_4_4_18,cs-410,4,4, Statistical Language Model - Part 1,"00:01:13,110","00:01:16,350",18,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=73,Well the obvious choice would be,pic_cs-410_4_4_60.jpg
cs-410_4_4_19,cs-410,4,4, Statistical Language Model - Part 1,"00:01:16,350","00:01:17,990",19,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=76,that we have seen before.,pic_cs-410_4_4_60.jpg
cs-410_4_4_20,cs-410,4,4, Statistical Language Model - Part 1,"00:01:17,990","00:01:22,200",20,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=77,And that is we're going to normalize,pic_cs-410_4_4_60.jpg
cs-410_4_4_21,cs-410,4,4, Statistical Language Model - Part 1,"00:01:24,110","00:01:26,913",21,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=84,And estimate the probability,pic_cs-410_4_4_60.jpg
cs-410_4_4_22,cs-410,4,4, Statistical Language Model - Part 1,"00:01:30,234","00:01:33,194",22,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=90,This is a step function here.,pic_cs-410_4_4_60.jpg
cs-410_4_4_23,cs-410,4,4, Statistical Language Model - Part 1,"00:01:35,934","00:01:38,543",23,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=95,Which means all of the words that have,pic_cs-410_4_4_60.jpg
cs-410_4_4_24,cs-410,4,4, Statistical Language Model - Part 1,"00:01:38,543","00:01:43,016",24,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=98,the same frequency count will,pic_cs-410_4_4_60.jpg
cs-410_4_4_25,cs-410,4,4, Statistical Language Model - Part 1,"00:01:43,016","00:01:48,570",25,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=103,"This is another freedom to count,",pic_cs-410_4_4_60.jpg
cs-410_4_4_26,cs-410,4,4, Statistical Language Model - Part 1,"00:01:48,570","00:01:51,770",26,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=108,Note that for words that have not,pic_cs-410_4_4_60.jpg
cs-410_4_4_27,cs-410,4,4, Statistical Language Model - Part 1,"00:01:52,850","00:01:55,130",27,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=112,they will have 0 probability.,pic_cs-410_4_4_60.jpg
cs-410_4_4_28,cs-410,4,4, Statistical Language Model - Part 1,"00:01:55,130","00:02:00,880",28,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=115,So we know this is just like the model,pic_cs-410_4_4_60.jpg
cs-410_4_4_29,cs-410,4,4, Statistical Language Model - Part 1,"00:02:00,880","00:02:06,730",29,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=120,Where we assume that the use of,pic_cs-410_4_4_120.jpg
cs-410_4_4_30,cs-410,4,4, Statistical Language Model - Part 1,"00:02:06,730","00:02:07,670",30,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=126,a formula to clear it.,pic_cs-410_4_4_120.jpg
cs-410_4_4_31,cs-410,4,4, Statistical Language Model - Part 1,"00:02:09,200","00:02:13,510",31,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=129,And there's no chance of assembling any,pic_cs-410_4_4_120.jpg
cs-410_4_4_32,cs-410,4,4, Statistical Language Model - Part 1,"00:02:13,510","00:02:14,360",32,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=133,we know that's not good.,pic_cs-410_4_4_120.jpg
cs-410_4_4_33,cs-410,4,4, Statistical Language Model - Part 1,"00:02:15,420","00:02:17,240",33,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=135,So how do we improve this?,pic_cs-410_4_4_120.jpg
cs-410_4_4_34,cs-410,4,4, Statistical Language Model - Part 1,"00:02:17,240","00:02:23,170",34,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=137,Well in order to assign,pic_cs-410_4_4_120.jpg
cs-410_4_4_35,cs-410,4,4, Statistical Language Model - Part 1,"00:02:23,170","00:02:28,710",35,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=143,to words that have not been observed in,pic_cs-410_4_4_120.jpg
cs-410_4_4_36,cs-410,4,4, Statistical Language Model - Part 1,"00:02:28,710","00:02:35,200",36,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=148,some probability mass from the words,pic_cs-410_4_4_120.jpg
cs-410_4_4_37,cs-410,4,4, Statistical Language Model - Part 1,"00:02:35,200","00:02:39,894",37,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=155,"So for example here, we have to take away",pic_cs-410_4_4_120.jpg
cs-410_4_4_38,cs-410,4,4, Statistical Language Model - Part 1,"00:02:39,894","00:02:45,103",38,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=159,need some extra probability mass for,pic_cs-410_4_4_120.jpg
cs-410_4_4_39,cs-410,4,4, Statistical Language Model - Part 1,"00:02:45,103","00:02:47,870",39,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=165,So all these probabilities must sum to 1.,pic_cs-410_4_4_120.jpg
cs-410_4_4_40,cs-410,4,4, Statistical Language Model - Part 1,"00:02:47,870","00:02:53,224",40,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=167,So to make this transformation and to,pic_cs-410_4_4_120.jpg
cs-410_4_4_41,cs-410,4,4, Statistical Language Model - Part 1,"00:02:53,224","00:03:00,420",41,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=173,by assigning non zero probabilities to,pic_cs-410_4_4_120.jpg
cs-410_4_4_42,cs-410,4,4, Statistical Language Model - Part 1,"00:03:01,970","00:03:06,630",42,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=181,We have to do smoothing and,pic_cs-410_4_4_180.jpg
cs-410_4_4_43,cs-410,4,4, Statistical Language Model - Part 1,"00:03:06,630","00:03:11,140",43,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=186,the estimate by considering,pic_cs-410_4_4_180.jpg
cs-410_4_4_44,cs-410,4,4, Statistical Language Model - Part 1,"00:03:13,970","00:03:17,800",44,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=193,had been asking to write more words for,pic_cs-410_4_4_180.jpg
cs-410_4_4_45,cs-410,4,4, Statistical Language Model - Part 1,"00:03:17,800","00:03:22,910",45,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=197,"the document,",pic_cs-410_4_4_180.jpg
cs-410_4_4_46,cs-410,4,4, Statistical Language Model - Part 1,"00:03:22,910","00:03:27,050",46,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=202,If you think about this factor,pic_cs-410_4_4_180.jpg
cs-410_4_4_47,cs-410,4,4, Statistical Language Model - Part 1,"00:03:27,050","00:03:30,830",47,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=207,would be a more accurate than,pic_cs-410_4_4_180.jpg
cs-410_4_4_48,cs-410,4,4, Statistical Language Model - Part 1,"00:03:30,830","00:03:35,270",48,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=210,Imagine you have seen an abstract,pic_cs-410_4_4_180.jpg
cs-410_4_4_49,cs-410,4,4, Statistical Language Model - Part 1,"00:03:35,270","00:03:37,230",49,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=215,Let's say this document is abstract.,pic_cs-410_4_4_180.jpg
cs-410_4_4_50,cs-410,4,4, Statistical Language Model - Part 1,"00:03:39,250","00:03:47,844",50,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=219,If we assume and see words in this,pic_cs-410_4_4_180.jpg
cs-410_4_4_51,cs-410,4,4, Statistical Language Model - Part 1,"00:03:47,844","00:03:51,900",51,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=227,That would mean there's,pic_cs-410_4_4_180.jpg
cs-410_4_4_52,cs-410,4,4, Statistical Language Model - Part 1,"00:03:51,900","00:03:57,170",52,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=231,a word outside the abstract,pic_cs-410_4_4_180.jpg
cs-410_4_4_53,cs-410,4,4, Statistical Language Model - Part 1,"00:03:57,170","00:04:02,193",53,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=237,But imagine a user who is interested,pic_cs-410_4_4_180.jpg
cs-410_4_4_54,cs-410,4,4, Statistical Language Model - Part 1,"00:04:02,193","00:04:06,475",54,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=242,The user might actually,pic_cs-410_4_4_240.jpg
cs-410_4_4_55,cs-410,4,4, Statistical Language Model - Part 1,"00:04:06,475","00:04:08,973",55,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=246,that chapter to use as query.,pic_cs-410_4_4_240.jpg
cs-410_4_4_56,cs-410,4,4, Statistical Language Model - Part 1,"00:04:08,973","00:04:13,916",56,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=248,"So obviously,",pic_cs-410_4_4_240.jpg
cs-410_4_4_57,cs-410,4,4, Statistical Language Model - Part 1,"00:04:13,916","00:04:18,760",57,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=253,author would have written,pic_cs-410_4_4_240.jpg
cs-410_4_4_58,cs-410,4,4, Statistical Language Model - Part 1,"00:04:18,760","00:04:23,627",58,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=258,So smoothing of the language,pic_cs-410_4_4_240.jpg
cs-410_4_4_59,cs-410,4,4, Statistical Language Model - Part 1,"00:04:23,627","00:04:27,642",59,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=263,to recover the model for,pic_cs-410_4_4_240.jpg
cs-410_4_4_60,cs-410,4,4, Statistical Language Model - Part 1,"00:04:27,642","00:04:32,346",60,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=267,"And then of course,",pic_cs-410_4_4_240.jpg
cs-410_4_4_61,cs-410,4,4, Statistical Language Model - Part 1,"00:04:32,346","00:04:36,310",61,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=272,words that are not,pic_cs-410_4_4_240.jpg
cs-410_4_4_62,cs-410,4,4, Statistical Language Model - Part 1,"00:04:36,310","00:04:39,250",62,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=276,So that's why smoothing is,pic_cs-410_4_4_240.jpg
cs-410_4_4_63,cs-410,4,4, Statistical Language Model - Part 1,"00:04:39,250","00:04:43,670",63,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=279,So let's talk a little more about,pic_cs-410_4_4_240.jpg
cs-410_4_4_64,cs-410,4,4, Statistical Language Model - Part 1,"00:04:43,670","00:04:48,500",64,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=283,"The key question here is, what probability",pic_cs-410_4_4_240.jpg
cs-410_4_4_65,cs-410,4,4, Statistical Language Model - Part 1,"00:04:50,480","00:04:52,200",65,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=290,And there are many different,pic_cs-410_4_4_240.jpg
cs-410_4_4_66,cs-410,4,4, Statistical Language Model - Part 1,"00:04:53,290","00:04:59,500",66,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=293,"One idea here, that's very useful for",pic_cs-410_4_4_240.jpg
cs-410_4_4_67,cs-410,4,4, Statistical Language Model - Part 1,"00:04:59,500","00:05:03,790",67,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=299,word be proportional to its probability,pic_cs-410_4_4_240.jpg
cs-410_4_4_68,cs-410,4,4, Statistical Language Model - Part 1,"00:05:03,790","00:05:07,785",68,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=303,That means if you don't observe,pic_cs-410_4_4_300.jpg
cs-410_4_4_69,cs-410,4,4, Statistical Language Model - Part 1,"00:05:07,785","00:05:11,583",69,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=307,We're going to assume that its,pic_cs-410_4_4_300.jpg
cs-410_4_4_70,cs-410,4,4, Statistical Language Model - Part 1,"00:05:11,583","00:05:16,310",70,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=311,by another reference language,pic_cs-410_4_4_300.jpg
cs-410_4_4_71,cs-410,4,4, Statistical Language Model - Part 1,"00:05:16,310","00:05:20,500",71,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=316,It will tell us which unseen words,pic_cs-410_4_4_300.jpg
cs-410_4_4_72,cs-410,4,4, Statistical Language Model - Part 1,"00:05:22,440","00:05:26,060",72,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=322,"In the case of retrieval,",pic_cs-410_4_4_300.jpg
cs-410_4_4_73,cs-410,4,4, Statistical Language Model - Part 1,"00:05:26,060","00:05:30,080",73,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=326,take the collection language model,pic_cs-410_4_4_300.jpg
cs-410_4_4_74,cs-410,4,4, Statistical Language Model - Part 1,"00:05:30,080","00:05:33,390",74,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=330,"That is to say, if you don't",pic_cs-410_4_4_300.jpg
cs-410_4_4_75,cs-410,4,4, Statistical Language Model - Part 1,"00:05:33,390","00:05:37,440",75,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=333,we're going to assume that,pic_cs-410_4_4_300.jpg
cs-410_4_4_76,cs-410,4,4, Statistical Language Model - Part 1,"00:05:37,440","00:05:40,658",76,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=337,would be proportional to the probability,pic_cs-410_4_4_300.jpg
cs-410_4_4_77,cs-410,4,4, Statistical Language Model - Part 1,"00:05:40,658","00:05:42,990",77,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=340,"So more formally,",pic_cs-410_4_4_300.jpg
cs-410_4_4_78,cs-410,4,4, Statistical Language Model - Part 1,"00:05:42,990","00:05:46,790",78,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=342,we'll be estimating the probability,pic_cs-410_4_4_300.jpg
cs-410_4_4_79,cs-410,4,4, Statistical Language Model - Part 1,"00:05:48,220","00:05:54,479",79,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=348,If the word is seen in,pic_cs-410_4_4_300.jpg
cs-410_4_4_80,cs-410,4,4, Statistical Language Model - Part 1,"00:05:54,479","00:06:02,251",80,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=354,would be this counted the maximum,pic_cs-410_4_4_300.jpg
cs-410_4_4_81,cs-410,4,4, Statistical Language Model - Part 1,"00:06:02,251","00:06:07,142",81,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=362,"Otherwise, if the word is not seen in the",pic_cs-410_4_4_360.jpg
cs-410_4_4_82,cs-410,4,4, Statistical Language Model - Part 1,"00:06:07,142","00:06:12,220",82,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=367,be proportional to the probability,pic_cs-410_4_4_360.jpg
cs-410_4_4_83,cs-410,4,4, Statistical Language Model - Part 1,"00:06:12,220","00:06:17,060",83,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=372,And here the coefficient that offer is to,pic_cs-410_4_4_360.jpg
cs-410_4_4_84,cs-410,4,4, Statistical Language Model - Part 1,"00:06:17,060","00:06:21,360",84,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=377,control the amount of probability,pic_cs-410_4_4_360.jpg
cs-410_4_4_85,cs-410,4,4, Statistical Language Model - Part 1,"00:06:22,450","00:06:25,390",85,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=382,"Obviously, all these",pic_cs-410_4_4_360.jpg
cs-410_4_4_86,cs-410,4,4, Statistical Language Model - Part 1,"00:06:25,390","00:06:28,300",86,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=385,alpha sub d is constrained in some way.,pic_cs-410_4_4_360.jpg
cs-410_4_4_87,cs-410,4,4, Statistical Language Model - Part 1,"00:06:29,390","00:06:33,370",87,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=389,So what if we plug in this,pic_cs-410_4_4_360.jpg
cs-410_4_4_88,cs-410,4,4, Statistical Language Model - Part 1,"00:06:33,370","00:06:35,150",88,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=393,query likelihood ranking function?,pic_cs-410_4_4_360.jpg
cs-410_4_4_89,cs-410,4,4, Statistical Language Model - Part 1,"00:06:35,150","00:06:36,290",89,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=395,This is what we will get.,pic_cs-410_4_4_360.jpg
cs-410_4_4_90,cs-410,4,4, Statistical Language Model - Part 1,"00:06:37,790","00:06:43,930",90,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=397,"In this formula, we have this",pic_cs-410_4_4_360.jpg
cs-410_4_4_91,cs-410,4,4, Statistical Language Model - Part 1,"00:06:43,930","00:06:48,900",91,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=403,as a sum over all the query words and,pic_cs-410_4_4_360.jpg
cs-410_4_4_92,cs-410,4,4, Statistical Language Model - Part 1,"00:06:48,900","00:06:54,000",92,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=408,those that we have written here as the sum,pic_cs-410_4_4_360.jpg
cs-410_4_4_93,cs-410,4,4, Statistical Language Model - Part 1,"00:06:54,000","00:06:56,780",93,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=414,This is the sum of all,pic_cs-410_4_4_360.jpg
cs-410_4_4_94,cs-410,4,4, Statistical Language Model - Part 1,"00:06:56,780","00:07:00,310",94,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=416,but not that we have a count,pic_cs-410_4_4_360.jpg
cs-410_4_4_95,cs-410,4,4, Statistical Language Model - Part 1,"00:07:00,310","00:07:04,476",95,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=420,"So in fact, we are just taking",pic_cs-410_4_4_420.jpg
cs-410_4_4_96,cs-410,4,4, Statistical Language Model - Part 1,"00:07:04,476","00:07:11,820",96,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=424,This is now a common,pic_cs-410_4_4_420.jpg
cs-410_4_4_97,cs-410,4,4, Statistical Language Model - Part 1,"00:07:11,820","00:07:16,170",97,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=431,because of its convenience,pic_cs-410_4_4_420.jpg
cs-410_4_4_98,cs-410,4,4, Statistical Language Model - Part 1,"00:07:18,710","00:07:21,949",98,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=438,"So this is as I said,",pic_cs-410_4_4_420.jpg
cs-410_4_4_99,cs-410,4,4, Statistical Language Model - Part 1,"00:07:23,130","00:07:26,950",99,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=443,"In our smoothing method,",pic_cs-410_4_4_420.jpg
cs-410_4_4_100,cs-410,4,4, Statistical Language Model - Part 1,"00:07:26,950","00:07:31,310",100,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=446,are not observed in the method would have,pic_cs-410_4_4_420.jpg
cs-410_4_4_101,cs-410,4,4, Statistical Language Model - Part 1,"00:07:31,310","00:07:33,663",101,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=451,"Name it's four, this foru.",pic_cs-410_4_4_420.jpg
cs-410_4_4_102,cs-410,4,4, Statistical Language Model - Part 1,"00:07:33,663","00:07:37,090",102,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=453,"So we're going to do then,",pic_cs-410_4_4_420.jpg
cs-410_4_4_103,cs-410,4,4, Statistical Language Model - Part 1,"00:07:38,620","00:07:44,422",103,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=458,One sum is over all the query words,pic_cs-410_4_4_420.jpg
cs-410_4_4_104,cs-410,4,4, Statistical Language Model - Part 1,"00:07:44,422","00:07:49,287",104,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=464,"That means that in this sum, all the words",pic_cs-410_4_4_420.jpg
cs-410_4_4_105,cs-410,4,4, Statistical Language Model - Part 1,"00:07:49,287","00:07:54,580",105,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=469,have a non zero probability,pic_cs-410_4_4_420.jpg
cs-410_4_4_106,cs-410,4,4, Statistical Language Model - Part 1,"00:07:54,580","00:07:59,740",106,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=474,"Sorry, it's the non zero count",pic_cs-410_4_4_420.jpg
cs-410_4_4_107,cs-410,4,4, Statistical Language Model - Part 1,"00:07:59,740","00:08:01,220",107,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=479,They all occur in the document.,pic_cs-410_4_4_420.jpg
cs-410_4_4_108,cs-410,4,4, Statistical Language Model - Part 1,"00:08:02,230","00:08:07,800",108,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=482,And they also have to of course,pic_cs-410_4_4_480.jpg
cs-410_4_4_109,cs-410,4,4, Statistical Language Model - Part 1,"00:08:07,800","00:08:13,894",109,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=487,So these are the query words,pic_cs-410_4_4_480.jpg
cs-410_4_4_110,cs-410,4,4, Statistical Language Model - Part 1,"00:08:13,894","00:08:19,153",110,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=493,"On the other hand, in this sum we",pic_cs-410_4_4_480.jpg
cs-410_4_4_111,cs-410,4,4, Statistical Language Model - Part 1,"00:08:19,153","00:08:23,630",111,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=499,that are not all query was,pic_cs-410_4_4_480.jpg
cs-410_4_4_112,cs-410,4,4, Statistical Language Model - Part 1,"00:08:25,840","00:08:31,250",112,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=505,So they occur in the query,pic_cs-410_4_4_480.jpg
cs-410_4_4_113,cs-410,4,4, Statistical Language Model - Part 1,"00:08:31,250","00:08:33,200",113,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=511,they don't occur in the document.,pic_cs-410_4_4_480.jpg
cs-410_4_4_114,cs-410,4,4, Statistical Language Model - Part 1,"00:08:33,200","00:08:33,920",114,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=513,"In this case,",pic_cs-410_4_4_480.jpg
cs-410_4_4_115,cs-410,4,4, Statistical Language Model - Part 1,"00:08:33,920","00:08:39,346",115,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=513,these words have this probability because,pic_cs-410_4_4_480.jpg
cs-410_4_4_116,cs-410,4,4, Statistical Language Model - Part 1,"00:08:39,346","00:08:44,880",116,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=519,"That here, these seen words",pic_cs-410_4_4_480.jpg
cs-410_4_4_117,cs-410,4,4, Statistical Language Model - Part 1,"00:08:47,490","00:08:51,460",117,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=527,"Now, we can go further by",pic_cs-410_4_4_480.jpg
cs-410_4_4_118,cs-410,4,4, Statistical Language Model - Part 1,"00:08:52,570","00:08:54,790",118,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=532,as a difference of two other sums.,pic_cs-410_4_4_480.jpg
cs-410_4_4_119,cs-410,4,4, Statistical Language Model - Part 1,"00:08:54,790","00:08:58,760",119,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=534,"Basically, the first sum is",pic_cs-410_4_4_480.jpg
cs-410_4_4_120,cs-410,4,4, Statistical Language Model - Part 1,"00:09:00,060","00:09:05,190",120,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=540,"Now, we know that the original sum",pic_cs-410_4_4_540.jpg
cs-410_4_4_121,cs-410,4,4, Statistical Language Model - Part 1,"00:09:05,190","00:09:10,760",121,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=545,This is over all the query words that,pic_cs-410_4_4_540.jpg
cs-410_4_4_122,cs-410,4,4, Statistical Language Model - Part 1,"00:09:12,400","00:09:19,740",122,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=552,So here we pretend that they,pic_cs-410_4_4_540.jpg
cs-410_4_4_123,cs-410,4,4, Statistical Language Model - Part 1,"00:09:19,740","00:09:21,920",123,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=559,So we take a sum over all the query words.,pic_cs-410_4_4_540.jpg
cs-410_4_4_124,cs-410,4,4, Statistical Language Model - Part 1,"00:09:21,920","00:09:28,750",124,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=561,"Obviously, this sum has extra",pic_cs-410_4_4_540.jpg
cs-410_4_4_125,cs-410,4,4, Statistical Language Model - Part 1,"00:09:30,770","00:09:33,710",125,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=570,"Because, here we're taking",pic_cs-410_4_4_540.jpg
cs-410_4_4_126,cs-410,4,4, Statistical Language Model - Part 1,"00:09:33,710","00:09:37,880",126,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=573,"There, it's not matched in the document.",pic_cs-410_4_4_540.jpg
cs-410_4_4_127,cs-410,4,4, Statistical Language Model - Part 1,"00:09:37,880","00:09:44,370",127,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=577,"So in order to make them equal, we will",pic_cs-410_4_4_540.jpg
cs-410_4_4_128,cs-410,4,4, Statistical Language Model - Part 1,"00:09:44,370","00:09:48,758",128,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=584,And this is the sum over all the query,pic_cs-410_4_4_540.jpg
cs-410_4_4_129,cs-410,4,4, Statistical Language Model - Part 1,"00:09:51,069","00:09:55,411",129,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=591,"And this makes sense, because here",pic_cs-410_4_4_540.jpg
cs-410_4_4_130,cs-410,4,4, Statistical Language Model - Part 1,"00:09:55,411","00:09:59,410",130,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=595,And then we subtract the query,pic_cs-410_4_4_540.jpg
cs-410_4_4_131,cs-410,4,4, Statistical Language Model - Part 1,"00:09:59,410","00:10:04,020",131,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=599,That would give us the query that,pic_cs-410_4_4_540.jpg
cs-410_4_4_132,cs-410,4,4, Statistical Language Model - Part 1,"00:10:05,880","00:10:11,100",132,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=605,And this is almost a reverse,pic_cs-410_4_4_600.jpg
cs-410_4_4_133,cs-410,4,4, Statistical Language Model - Part 1,"00:10:12,770","00:10:14,758",133,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=612,And you might wonder why,pic_cs-410_4_4_600.jpg
cs-410_4_4_134,cs-410,4,4, Statistical Language Model - Part 1,"00:10:14,758","00:10:19,510",134,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=614,"Well, that's because if we do this,",pic_cs-410_4_4_600.jpg
cs-410_4_4_135,cs-410,4,4, Statistical Language Model - Part 1,"00:10:19,510","00:10:25,360",135,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=619,then we have different forms,pic_cs-410_4_4_600.jpg
cs-410_4_4_136,cs-410,4,4, Statistical Language Model - Part 1,"00:10:25,360","00:10:31,370",136,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=625,"So now, you can see in this sum",pic_cs-410_4_4_600.jpg
cs-410_4_4_137,cs-410,4,4, Statistical Language Model - Part 1,"00:10:31,370","00:10:35,440",137,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=631,the query was matching the document,pic_cs-410_4_4_600.jpg
cs-410_4_4_138,cs-410,4,4, Statistical Language Model - Part 1,"00:10:36,760","00:10:45,750",138,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=636,Here we have another sum over the same set,pic_cs-410_4_4_600.jpg
cs-410_4_4_139,cs-410,4,4, Statistical Language Model - Part 1,"00:10:45,750","00:10:47,870",139,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=645,"But inside the sum, it's different.",pic_cs-410_4_4_600.jpg
cs-410_4_4_140,cs-410,4,4, Statistical Language Model - Part 1,"00:10:49,180","00:10:52,640",140,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=649,But these two sums can clearly be merged.,pic_cs-410_4_4_600.jpg
cs-410_4_4_141,cs-410,4,4, Statistical Language Model - Part 1,"00:10:54,300","00:10:57,530",141,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=654,"So if we do that, we'll get another form",pic_cs-410_4_4_600.jpg
cs-410_4_4_142,cs-410,4,4, Statistical Language Model - Part 1,"00:10:57,530","00:11:02,140",142,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=657,of the formula that looks like,pic_cs-410_4_4_600.jpg
cs-410_4_4_143,cs-410,4,4, Statistical Language Model - Part 1,"00:11:04,360","00:11:06,966",143,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=664,And note that this is,pic_cs-410_4_4_660.jpg
cs-410_4_4_144,cs-410,4,4, Statistical Language Model - Part 1,"00:11:06,966","00:11:10,796",144,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=666,Because here we combine,pic_cs-410_4_4_660.jpg
cs-410_4_4_145,cs-410,4,4, Statistical Language Model - Part 1,"00:11:10,796","00:11:16,710",145,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=670,some of the query words matching in,pic_cs-410_4_4_660.jpg
cs-410_4_4_146,cs-410,4,4, Statistical Language Model - Part 1,"00:11:19,040","00:11:24,469",146,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=679,And the other sum now is,pic_cs-410_4_4_660.jpg
cs-410_4_4_147,cs-410,4,4, Statistical Language Model - Part 1,"00:11:24,469","00:11:26,988",147,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=684,And these two parts,pic_cs-410_4_4_660.jpg
cs-410_4_4_148,cs-410,4,4, Statistical Language Model - Part 1,"00:11:26,988","00:11:30,130",148,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=686,because these are the probabilities,pic_cs-410_4_4_660.jpg
cs-410_4_4_149,cs-410,4,4, Statistical Language Model - Part 1,"00:11:31,630","00:11:36,419",149,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=691,This formula is very interesting,pic_cs-410_4_4_660.jpg
cs-410_4_4_150,cs-410,4,4, Statistical Language Model - Part 1,"00:11:37,450","00:11:39,970",150,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=697,the match the query terms.,pic_cs-410_4_4_660.jpg
cs-410_4_4_151,cs-410,4,4, Statistical Language Model - Part 1,"00:11:41,340","00:11:44,210",151,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=701,"And just like in the vector space model,",pic_cs-410_4_4_660.jpg
cs-410_4_4_152,cs-410,4,4, Statistical Language Model - Part 1,"00:11:46,030","00:11:49,930",152,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=706,of terms that are in the intersection of,pic_cs-410_4_4_660.jpg
cs-410_4_4_153,cs-410,4,4, Statistical Language Model - Part 1,"00:11:51,320","00:11:55,620",153,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=711,So it already looks a little bit,pic_cs-410_4_4_660.jpg
cs-410_4_4_154,cs-410,4,4, Statistical Language Model - Part 1,"00:11:55,620","00:12:02,573",154,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=715,"In fact, there's even more similarity",pic_cs-410_4_4_660.jpg
cs-410_4_4_155,cs-410,4,4, Statistical Language Model - Part 1,"00:12:02,573","00:12:12,573",155,https://www.coursera.org/learn/cs-410/lecture/f4CYl?t=722,[MUSIC],pic_cs-410_4_4_720.jpg
cs-410_4_5_1,cs-410,4,5, Statistical Language Model - Part 2,"00:00:00,005","00:00:03,962",1,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=0,[SOUND],pic_cs-410_4_5_0.jpg
cs-410_4_5_2,cs-410,4,5, Statistical Language Model - Part 2,"00:00:12,779","00:00:15,641",2,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=12,So I showed you how we rewrite the query,pic_cs-410_4_5_0.jpg
cs-410_4_5_3,cs-410,4,5, Statistical Language Model - Part 2,"00:00:15,641","00:00:20,830",3,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=15,like holder which is a function into,pic_cs-410_4_5_0.jpg
cs-410_4_5_4,cs-410,4,5, Statistical Language Model - Part 2,"00:00:20,830","00:00:25,840",4,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=20,of this slide after if we make,pic_cs-410_4_5_0.jpg
cs-410_4_5_5,cs-410,4,5, Statistical Language Model - Part 2,"00:00:25,840","00:00:30,426",5,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=25,the language model based on,pic_cs-410_4_5_0.jpg
cs-410_4_5_6,cs-410,4,5, Statistical Language Model - Part 2,"00:00:30,426","00:00:36,160",6,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=30,"Now if you look at this rewriting,",pic_cs-410_4_5_0.jpg
cs-410_4_5_7,cs-410,4,5, Statistical Language Model - Part 2,"00:00:36,160","00:00:42,470",7,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=36,The first benefit is it helps us better,pic_cs-410_4_5_0.jpg
cs-410_4_5_8,cs-410,4,5, Statistical Language Model - Part 2,"00:00:42,470","00:00:47,050",8,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=42,"In particular, we're going to show that",pic_cs-410_4_5_0.jpg
cs-410_4_5_9,cs-410,4,5, Statistical Language Model - Part 2,"00:00:47,050","00:00:51,340",9,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=47,with the collection language model would,pic_cs-410_4_5_0.jpg
cs-410_4_5_10,cs-410,4,5, Statistical Language Model - Part 2,"00:00:51,340","00:00:52,412",10,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=51,and length normalization.,pic_cs-410_4_5_0.jpg
cs-410_4_5_11,cs-410,4,5, Statistical Language Model - Part 2,"00:00:52,412","00:00:57,645",11,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=52,The second benefit is that,pic_cs-410_4_5_0.jpg
cs-410_4_5_12,cs-410,4,5, Statistical Language Model - Part 2,"00:00:57,645","00:01:02,940",12,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=57,the query like holder more efficiently.,pic_cs-410_4_5_0.jpg
cs-410_4_5_13,cs-410,4,5, Statistical Language Model - Part 2,"00:01:02,940","00:01:06,020",13,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=62,In particular we see that,pic_cs-410_4_5_60.jpg
cs-410_4_5_14,cs-410,4,5, Statistical Language Model - Part 2,"00:01:06,020","00:01:07,860",14,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=66,is a sum over the match,pic_cs-410_4_5_60.jpg
cs-410_4_5_15,cs-410,4,5, Statistical Language Model - Part 2,"00:01:09,670","00:01:14,910",15,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=69,So this is much better than if we,pic_cs-410_4_5_60.jpg
cs-410_4_5_16,cs-410,4,5, Statistical Language Model - Part 2,"00:01:14,910","00:01:20,257",16,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=74,After we smooth the document the damage,pic_cs-410_4_5_60.jpg
cs-410_4_5_17,cs-410,4,5, Statistical Language Model - Part 2,"00:01:20,257","00:01:21,400",17,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=80,for all the words.,pic_cs-410_4_5_60.jpg
cs-410_4_5_18,cs-410,4,5, Statistical Language Model - Part 2,"00:01:21,400","00:01:25,760",18,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=81,So this new form of the formula is,pic_cs-410_4_5_60.jpg
cs-410_4_5_19,cs-410,4,5, Statistical Language Model - Part 2,"00:01:27,580","00:01:29,850",19,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=87,It's also interesting to note that,pic_cs-410_4_5_60.jpg
cs-410_4_5_20,cs-410,4,5, Statistical Language Model - Part 2,"00:01:29,850","00:01:34,420",20,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=89,the last term here is actually,pic_cs-410_4_5_60.jpg
cs-410_4_5_21,cs-410,4,5, Statistical Language Model - Part 2,"00:01:34,420","00:01:36,610",21,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=94,Since our goal is to,pic_cs-410_4_5_60.jpg
cs-410_4_5_22,cs-410,4,5, Statistical Language Model - Part 2,"00:01:36,610","00:01:40,610",22,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=96,the same query we can ignore this term for,pic_cs-410_4_5_60.jpg
cs-410_4_5_23,cs-410,4,5, Statistical Language Model - Part 2,"00:01:40,610","00:01:43,650",23,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=100,Because it's going to be the same for,pic_cs-410_4_5_60.jpg
cs-410_4_5_24,cs-410,4,5, Statistical Language Model - Part 2,"00:01:43,650","00:01:46,630",24,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=103,Ignoring it wouldn't affect,pic_cs-410_4_5_60.jpg
cs-410_4_5_25,cs-410,4,5, Statistical Language Model - Part 2,"00:01:49,070","00:01:51,890",25,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=109,"Inside the sum, we",pic_cs-410_4_5_60.jpg
cs-410_4_5_26,cs-410,4,5, Statistical Language Model - Part 2,"00:01:52,940","00:01:57,060",26,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=112,also see that each matched query,pic_cs-410_4_5_60.jpg
cs-410_4_5_27,cs-410,4,5, Statistical Language Model - Part 2,"00:01:58,510","00:02:01,990",27,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=118,And this weight actually,pic_cs-410_4_5_60.jpg
cs-410_4_5_28,cs-410,4,5, Statistical Language Model - Part 2,"00:02:01,990","00:02:07,070",28,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=121,is very interesting because it,pic_cs-410_4_5_120.jpg
cs-410_4_5_29,cs-410,4,5, Statistical Language Model - Part 2,"00:02:07,070","00:02:11,830",29,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=127,First we can already see it has,pic_cs-410_4_5_120.jpg
cs-410_4_5_30,cs-410,4,5, Statistical Language Model - Part 2,"00:02:11,830","00:02:14,250",30,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=131,just like in the vector space model.,pic_cs-410_4_5_120.jpg
cs-410_4_5_31,cs-410,4,5, Statistical Language Model - Part 2,"00:02:14,250","00:02:16,240",31,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=134,"When we take a thought product,",pic_cs-410_4_5_120.jpg
cs-410_4_5_32,cs-410,4,5, Statistical Language Model - Part 2,"00:02:16,240","00:02:20,940",32,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=136,we see the word frequency in,pic_cs-410_4_5_120.jpg
cs-410_4_5_33,cs-410,4,5, Statistical Language Model - Part 2,"00:02:22,250","00:02:27,670",33,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=142,And so naturally this part would,pic_cs-410_4_5_120.jpg
cs-410_4_5_34,cs-410,4,5, Statistical Language Model - Part 2,"00:02:27,670","00:02:31,510",34,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=147,element from the documented vector.,pic_cs-410_4_5_120.jpg
cs-410_4_5_35,cs-410,4,5, Statistical Language Model - Part 2,"00:02:31,510","00:02:34,170",35,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=151,And here indeed we can see it actually,pic_cs-410_4_5_120.jpg
cs-410_4_5_36,cs-410,4,5, Statistical Language Model - Part 2,"00:02:35,430","00:02:39,950",36,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=155,encodes a weight that has similar,pic_cs-410_4_5_120.jpg
cs-410_4_5_37,cs-410,4,5, Statistical Language Model - Part 2,"00:02:41,160","00:02:43,660",37,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=161,"I'll let you examine it, can you see it?",pic_cs-410_4_5_120.jpg
cs-410_4_5_38,cs-410,4,5, Statistical Language Model - Part 2,"00:02:43,660","00:02:46,110",38,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=163,Can you see which part is capturing TF?,pic_cs-410_4_5_120.jpg
cs-410_4_5_39,cs-410,4,5, Statistical Language Model - Part 2,"00:02:46,110","00:02:49,870",39,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=166,And which part is,pic_cs-410_4_5_120.jpg
cs-410_4_5_40,cs-410,4,5, Statistical Language Model - Part 2,"00:02:51,680","00:02:54,630",40,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=171,So if want you can pause,pic_cs-410_4_5_120.jpg
cs-410_4_5_41,cs-410,4,5, Statistical Language Model - Part 2,"00:02:55,830","00:03:02,640",41,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=175,So have you noticed that this P sub,pic_cs-410_4_5_120.jpg
cs-410_4_5_42,cs-410,4,5, Statistical Language Model - Part 2,"00:03:02,640","00:03:08,240",42,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=182,in the sense that if a word occurs,pic_cs-410_4_5_180.jpg
cs-410_4_5_43,cs-410,4,5, Statistical Language Model - Part 2,"00:03:08,240","00:03:11,980",43,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=188,then the s made through probability,pic_cs-410_4_5_180.jpg
cs-410_4_5_44,cs-410,4,5, Statistical Language Model - Part 2,"00:03:11,980","00:03:17,694",44,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=191,So this means this term is really,pic_cs-410_4_5_180.jpg
cs-410_4_5_45,cs-410,4,5, Statistical Language Model - Part 2,"00:03:17,694","00:03:22,324",45,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=197,Now have you also noticed that,pic_cs-410_4_5_180.jpg
cs-410_4_5_46,cs-410,4,5, Statistical Language Model - Part 2,"00:03:22,324","00:03:26,090",46,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=202,is actually achieving the factor of IDF?,pic_cs-410_4_5_180.jpg
cs-410_4_5_47,cs-410,4,5, Statistical Language Model - Part 2,"00:03:26,090","00:03:29,870",47,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=206,"Why, because this is the popularity",pic_cs-410_4_5_180.jpg
cs-410_4_5_48,cs-410,4,5, Statistical Language Model - Part 2,"00:03:31,750","00:03:37,110",48,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=211,"But it's in the denominator, so if the",pic_cs-410_4_5_180.jpg
cs-410_4_5_49,cs-410,4,5, Statistical Language Model - Part 2,"00:03:37,110","00:03:39,700",49,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=217,then the weight is actually smaller.,pic_cs-410_4_5_180.jpg
cs-410_4_5_50,cs-410,4,5, Statistical Language Model - Part 2,"00:03:39,700","00:03:41,790",50,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=219,And this means a popular term.,pic_cs-410_4_5_180.jpg
cs-410_4_5_51,cs-410,4,5, Statistical Language Model - Part 2,"00:03:41,790","00:03:45,990",51,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=221,We actually have a smaller weight and this,pic_cs-410_4_5_180.jpg
cs-410_4_5_52,cs-410,4,5, Statistical Language Model - Part 2,"00:03:47,040","00:03:50,330",52,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=227,Only that we now have,pic_cs-410_4_5_180.jpg
cs-410_4_5_53,cs-410,4,5, Statistical Language Model - Part 2,"00:03:51,550","00:03:55,920",53,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=231,Remember IDF has a logarithm,pic_cs-410_4_5_180.jpg
cs-410_4_5_54,cs-410,4,5, Statistical Language Model - Part 2,"00:03:55,920","00:03:57,290",54,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=235,But here we have something different.,pic_cs-410_4_5_180.jpg
cs-410_4_5_55,cs-410,4,5, Statistical Language Model - Part 2,"00:03:58,300","00:04:02,460",55,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=238,But intuitively it,pic_cs-410_4_5_180.jpg
cs-410_4_5_56,cs-410,4,5, Statistical Language Model - Part 2,"00:04:02,460","00:04:06,550",56,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=242,"Interestingly, we also have something",pic_cs-410_4_5_240.jpg
cs-410_4_5_57,cs-410,4,5, Statistical Language Model - Part 2,"00:04:07,820","00:04:13,470",57,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=247,"Again, can you see which factor is related",pic_cs-410_4_5_240.jpg
cs-410_4_5_58,cs-410,4,5, Statistical Language Model - Part 2,"00:04:14,790","00:04:18,350",58,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=254,What I just say is that this term,pic_cs-410_4_5_240.jpg
cs-410_4_5_59,cs-410,4,5, Statistical Language Model - Part 2,"00:04:19,560","00:04:24,700",59,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=259,"This collection probability,",pic_cs-410_4_5_240.jpg
cs-410_4_5_60,cs-410,4,5, Statistical Language Model - Part 2,"00:04:24,700","00:04:29,360",60,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=264,this term here is actually related,pic_cs-410_4_5_240.jpg
cs-410_4_5_61,cs-410,4,5, Statistical Language Model - Part 2,"00:04:29,360","00:04:35,110",61,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=269,"In particular, F of sub d might",pic_cs-410_4_5_240.jpg
cs-410_4_5_62,cs-410,4,5, Statistical Language Model - Part 2,"00:04:35,110","00:04:40,480",62,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=275,So it encodes how much probability,pic_cs-410_4_5_240.jpg
cs-410_4_5_63,cs-410,4,5, Statistical Language Model - Part 2,"00:04:41,740","00:04:43,700",63,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=281,How much smoothing do we want to do?,pic_cs-410_4_5_240.jpg
cs-410_4_5_64,cs-410,4,5, Statistical Language Model - Part 2,"00:04:43,700","00:04:46,470",64,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=283,"Intuitively, if a document is long,",pic_cs-410_4_5_240.jpg
cs-410_4_5_65,cs-410,4,5, Statistical Language Model - Part 2,"00:04:46,470","00:04:50,980",65,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=286,then we need to do less smoothing because,pic_cs-410_4_5_240.jpg
cs-410_4_5_66,cs-410,4,5, Statistical Language Model - Part 2,"00:04:50,980","00:04:55,720",66,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=290,We probably have observed all the words,pic_cs-410_4_5_240.jpg
cs-410_4_5_67,cs-410,4,5, Statistical Language Model - Part 2,"00:04:55,720","00:05:00,900",67,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=295,But if the document is short then r of,pic_cs-410_4_5_240.jpg
cs-410_4_5_68,cs-410,4,5, Statistical Language Model - Part 2,"00:05:00,900","00:05:02,432",68,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=300,We need to do more smoothing.,pic_cs-410_4_5_300.jpg
cs-410_4_5_69,cs-410,4,5, Statistical Language Model - Part 2,"00:05:02,432","00:05:06,110",69,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=302,It's likey there are words that have,pic_cs-410_4_5_300.jpg
cs-410_4_5_70,cs-410,4,5, Statistical Language Model - Part 2,"00:05:06,110","00:05:12,250",70,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=306,So this term appears to paralyze,pic_cs-410_4_5_300.jpg
cs-410_4_5_71,cs-410,4,5, Statistical Language Model - Part 2,"00:05:12,250","00:05:19,100",71,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=312,other sub D would tend to be longer,pic_cs-410_4_5_300.jpg
cs-410_4_5_72,cs-410,4,5, Statistical Language Model - Part 2,"00:05:19,100","00:05:23,065",72,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=319,But note that alpha sub d,pic_cs-410_4_5_300.jpg
cs-410_4_5_73,cs-410,4,5, Statistical Language Model - Part 2,"00:05:23,065","00:05:28,570",73,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=323,this may not actually be necessary,pic_cs-410_4_5_300.jpg
cs-410_4_5_74,cs-410,4,5, Statistical Language Model - Part 2,"00:05:28,570","00:05:30,600",74,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=328,The effect is not so clear yet.,pic_cs-410_4_5_300.jpg
cs-410_4_5_75,cs-410,4,5, Statistical Language Model - Part 2,"00:05:31,930","00:05:36,570",75,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=331,"But as we will see later, when we",pic_cs-410_4_5_300.jpg
cs-410_4_5_76,cs-410,4,5, Statistical Language Model - Part 2,"00:05:36,570","00:05:40,080",76,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=336,it turns out that they do,pic_cs-410_4_5_300.jpg
cs-410_4_5_77,cs-410,4,5, Statistical Language Model - Part 2,"00:05:40,080","00:05:42,730",77,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=340,Just like in TF-IDF weighting and,pic_cs-410_4_5_300.jpg
cs-410_4_5_78,cs-410,4,5, Statistical Language Model - Part 2,"00:05:42,730","00:05:45,880",78,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=342,document length normalization,pic_cs-410_4_5_300.jpg
cs-410_4_5_79,cs-410,4,5, Statistical Language Model - Part 2,"00:05:47,490","00:05:50,670",79,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=347,"So, that's a very interesting",pic_cs-410_4_5_300.jpg
cs-410_4_5_80,cs-410,4,5, Statistical Language Model - Part 2,"00:05:50,670","00:05:54,880",80,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=350,we don't even have to think about,pic_cs-410_4_5_300.jpg
cs-410_4_5_81,cs-410,4,5, Statistical Language Model - Part 2,"00:05:54,880","00:05:59,910",81,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=354,We just need to assume that if we smooth,pic_cs-410_4_5_300.jpg
cs-410_4_5_82,cs-410,4,5, Statistical Language Model - Part 2,"00:05:59,910","00:06:05,480",82,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=359,then we would have a formula that,pic_cs-410_4_5_300.jpg
cs-410_4_5_83,cs-410,4,5, Statistical Language Model - Part 2,"00:06:05,480","00:06:06,710",83,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=365,documents length violation.,pic_cs-410_4_5_360.jpg
cs-410_4_5_84,cs-410,4,5, Statistical Language Model - Part 2,"00:06:08,210","00:06:13,150",84,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=368,What's also interesting that we have,pic_cs-410_4_5_360.jpg
cs-410_4_5_85,cs-410,4,5, Statistical Language Model - Part 2,"00:06:14,180","00:06:17,890",85,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=374,And see we have not heuristically,pic_cs-410_4_5_360.jpg
cs-410_4_5_86,cs-410,4,5, Statistical Language Model - Part 2,"00:06:19,310","00:06:23,790",86,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=379,"In fact, you can think about why",pic_cs-410_4_5_360.jpg
cs-410_4_5_87,cs-410,4,5, Statistical Language Model - Part 2,"00:06:23,790","00:06:28,651",87,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=383,You look at the assumptions that,pic_cs-410_4_5_360.jpg
cs-410_4_5_88,cs-410,4,5, Statistical Language Model - Part 2,"00:06:28,651","00:06:33,720",88,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=388,it's because we have used a logarithm,pic_cs-410_4_5_360.jpg
cs-410_4_5_89,cs-410,4,5, Statistical Language Model - Part 2,"00:06:33,720","00:06:38,090",89,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=393,And we turned the product into a sum,pic_cs-410_4_5_360.jpg
cs-410_4_5_90,cs-410,4,5, Statistical Language Model - Part 2,"00:06:38,090","00:06:39,239",90,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=398,that's why we have this logarithm.,pic_cs-410_4_5_360.jpg
cs-410_4_5_91,cs-410,4,5, Statistical Language Model - Part 2,"00:06:40,470","00:06:44,740",91,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=400,Note that if only want to heuristically,pic_cs-410_4_5_360.jpg
cs-410_4_5_92,cs-410,4,5, Statistical Language Model - Part 2,"00:06:44,740","00:06:48,830",92,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=404,"IDF weighting, we don't necessary",pic_cs-410_4_5_360.jpg
cs-410_4_5_93,cs-410,4,5, Statistical Language Model - Part 2,"00:06:48,830","00:06:53,700",93,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=408,"Imagine if we drop this logarithm,",pic_cs-410_4_5_360.jpg
cs-410_4_5_94,cs-410,4,5, Statistical Language Model - Part 2,"00:06:55,010","00:06:59,740",94,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=415,But what's nice with problem risk modeling,pic_cs-410_4_5_360.jpg
cs-410_4_5_95,cs-410,4,5, Statistical Language Model - Part 2,"00:06:59,740","00:07:01,950",95,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=419,the logarithm function here.,pic_cs-410_4_5_360.jpg
cs-410_4_5_96,cs-410,4,5, Statistical Language Model - Part 2,"00:07:01,950","00:07:07,510",96,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=421,And that's basically a fixed form,pic_cs-410_4_5_420.jpg
cs-410_4_5_97,cs-410,4,5, Statistical Language Model - Part 2,"00:07:07,510","00:07:13,010",97,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=427,"really have to heuristically design,",pic_cs-410_4_5_420.jpg
cs-410_4_5_98,cs-410,4,5, Statistical Language Model - Part 2,"00:07:13,010","00:07:18,110",98,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=433,the logarithm the model probably won't,pic_cs-410_4_5_420.jpg
cs-410_4_5_99,cs-410,4,5, Statistical Language Model - Part 2,"00:07:19,400","00:07:24,300",99,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=439,So a nice property of problem risk,pic_cs-410_4_5_420.jpg
cs-410_4_5_100,cs-410,4,5, Statistical Language Model - Part 2,"00:07:24,300","00:07:28,260",100,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=444,assumptions and the probability rules,pic_cs-410_4_5_420.jpg
cs-410_4_5_101,cs-410,4,5, Statistical Language Model - Part 2,"00:07:28,260","00:07:33,390",101,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=448,And the formula would have,pic_cs-410_4_5_420.jpg
cs-410_4_5_102,cs-410,4,5, Statistical Language Model - Part 2,"00:07:34,600","00:07:38,540",102,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=454,And if we heuristically design,pic_cs-410_4_5_420.jpg
cs-410_4_5_103,cs-410,4,5, Statistical Language Model - Part 2,"00:07:38,540","00:07:40,530",103,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=458,end up having such a specific formula.,pic_cs-410_4_5_420.jpg
cs-410_4_5_104,cs-410,4,5, Statistical Language Model - Part 2,"00:07:41,700","00:07:46,940",104,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=461,"So to summarize, we talked about the need",pic_cs-410_4_5_420.jpg
cs-410_4_5_105,cs-410,4,5, Statistical Language Model - Part 2,"00:07:46,940","00:07:52,470",105,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=466,Otherwise it would give zero probability,pic_cs-410_4_5_420.jpg
cs-410_4_5_106,cs-410,4,5, Statistical Language Model - Part 2,"00:07:52,470","00:07:57,450",106,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=472,that's not good for,pic_cs-410_4_5_420.jpg
cs-410_4_5_107,cs-410,4,5, Statistical Language Model - Part 2,"00:07:59,370","00:08:03,720",107,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=479,"It's also necessary, in general,",pic_cs-410_4_5_420.jpg
cs-410_4_5_108,cs-410,4,5, Statistical Language Model - Part 2,"00:08:03,720","00:08:08,730",108,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=483,the model represent,pic_cs-410_4_5_480.jpg
cs-410_4_5_109,cs-410,4,5, Statistical Language Model - Part 2,"00:08:08,730","00:08:16,210",109,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=488,The general idea of smoothing in retrieval,pic_cs-410_4_5_480.jpg
cs-410_4_5_110,cs-410,4,5, Statistical Language Model - Part 2,"00:08:17,800","00:08:22,760",110,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=497,to give us some clue about which unseen,pic_cs-410_4_5_480.jpg
cs-410_4_5_111,cs-410,4,5, Statistical Language Model - Part 2,"00:08:22,760","00:08:26,840",111,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=502,"That is, the probability of an unseen",pic_cs-410_4_5_480.jpg
cs-410_4_5_112,cs-410,4,5, Statistical Language Model - Part 2,"00:08:26,840","00:08:28,340",112,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=506,to its probability in the collection.,pic_cs-410_4_5_480.jpg
cs-410_4_5_113,cs-410,4,5, Statistical Language Model - Part 2,"00:08:29,610","00:08:34,330",113,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=509,"With this assumption, we've shown that we",pic_cs-410_4_5_480.jpg
cs-410_4_5_114,cs-410,4,5, Statistical Language Model - Part 2,"00:08:34,330","00:08:38,280",114,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=514,query likelihood that has,pic_cs-410_4_5_480.jpg
cs-410_4_5_115,cs-410,4,5, Statistical Language Model - Part 2,"00:08:38,280","00:08:39,970",115,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=518,document length normalization.,pic_cs-410_4_5_480.jpg
cs-410_4_5_116,cs-410,4,5, Statistical Language Model - Part 2,"00:08:39,970","00:08:42,210",116,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=519,"We also see that, through some rewriting,",pic_cs-410_4_5_480.jpg
cs-410_4_5_117,cs-410,4,5, Statistical Language Model - Part 2,"00:08:42,210","00:08:47,080",117,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=522,the scoring of such a ranking function,pic_cs-410_4_5_480.jpg
cs-410_4_5_118,cs-410,4,5, Statistical Language Model - Part 2,"00:08:47,080","00:08:50,530",118,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=527,"matched query terms,",pic_cs-410_4_5_480.jpg
cs-410_4_5_119,cs-410,4,5, Statistical Language Model - Part 2,"00:08:50,530","00:08:54,500",119,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=530,"But, the actual ranking",pic_cs-410_4_5_480.jpg
cs-410_4_5_120,cs-410,4,5, Statistical Language Model - Part 2,"00:08:54,500","00:08:59,010",120,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=534,automatically by the probability rules and,pic_cs-410_4_5_480.jpg
cs-410_4_5_121,cs-410,4,5, Statistical Language Model - Part 2,"00:08:59,010","00:09:02,210",121,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=539,And like in the vector space model,pic_cs-410_4_5_480.jpg
cs-410_4_5_122,cs-410,4,5, Statistical Language Model - Part 2,"00:09:02,210","00:09:04,580",122,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=542,think about the form of the function.,pic_cs-410_4_5_540.jpg
cs-410_4_5_123,cs-410,4,5, Statistical Language Model - Part 2,"00:09:04,580","00:09:09,234",123,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=544,"However, we still need to address",pic_cs-410_4_5_540.jpg
cs-410_4_5_124,cs-410,4,5, Statistical Language Model - Part 2,"00:09:09,234","00:09:11,652",124,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=549,smooth the document and the model.,pic_cs-410_4_5_540.jpg
cs-410_4_5_125,cs-410,4,5, Statistical Language Model - Part 2,"00:09:11,652","00:09:14,859",125,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=551,How exactly we should,pic_cs-410_4_5_540.jpg
cs-410_4_5_126,cs-410,4,5, Statistical Language Model - Part 2,"00:09:14,859","00:09:19,223",126,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=554,model based on the connection,pic_cs-410_4_5_540.jpg
cs-410_4_5_127,cs-410,4,5, Statistical Language Model - Part 2,"00:09:19,223","00:09:24,226",127,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=559,the maximum micro is made of and,pic_cs-410_4_5_540.jpg
cs-410_4_5_128,cs-410,4,5, Statistical Language Model - Part 2,"00:09:24,226","00:09:34,226",128,https://www.coursera.org/learn/cs-410/lecture/hI1vE?t=564,[MUSIC],pic_cs-410_4_5_540.jpg
cs-410_4_6_1,cs-410,4,6, Smoothing Methods - Part 1,"00:00:00,008","00:00:03,638",1,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=0,[SOUND],pic_cs-410_4_6_0.jpg
cs-410_4_6_2,cs-410,4,6, Smoothing Methods - Part 1,"00:00:07,832","00:00:09,846",2,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=7,This lecture is about the specific,pic_cs-410_4_6_0.jpg
cs-410_4_6_3,cs-410,4,6, Smoothing Methods - Part 1,"00:00:09,846","00:00:14,580",3,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=9,smoothing methods for language models,pic_cs-410_4_6_0.jpg
cs-410_4_6_4,cs-410,4,6, Smoothing Methods - Part 1,"00:00:16,560","00:00:21,030",4,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=16,"In this lecture, we will continue",pic_cs-410_4_6_0.jpg
cs-410_4_6_5,cs-410,4,6, Smoothing Methods - Part 1,"00:00:21,030","00:00:26,020",5,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=21,"information retrieval, particularly",pic_cs-410_4_6_0.jpg
cs-410_4_6_6,cs-410,4,6, Smoothing Methods - Part 1,"00:00:26,020","00:00:29,485",6,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=26,And we're going to talk about specifically,pic_cs-410_4_6_0.jpg
cs-410_4_6_7,cs-410,4,6, Smoothing Methods - Part 1,"00:00:29,485","00:00:30,856",7,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=29,such a retrieval function.,pic_cs-410_4_6_0.jpg
cs-410_4_6_8,cs-410,4,6, Smoothing Methods - Part 1,"00:00:33,591","00:00:39,021",8,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=33,So this is a slide from a previous,pic_cs-410_4_6_0.jpg
cs-410_4_6_9,cs-410,4,6, Smoothing Methods - Part 1,"00:00:39,021","00:00:44,638",9,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=39,likelihood ranking and smoothing,pic_cs-410_4_6_0.jpg
cs-410_4_6_10,cs-410,4,6, Smoothing Methods - Part 1,"00:00:44,638","00:00:50,002",10,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=44,we add up having a retrieval function,pic_cs-410_4_6_0.jpg
cs-410_4_6_11,cs-410,4,6, Smoothing Methods - Part 1,"00:00:50,002","00:00:57,370",11,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=50,So this is the retrieval function based on,pic_cs-410_4_6_0.jpg
cs-410_4_6_12,cs-410,4,6, Smoothing Methods - Part 1,"00:00:57,370","00:01:02,738",12,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=57,You can see it's a sum of all,pic_cs-410_4_6_0.jpg
cs-410_4_6_13,cs-410,4,6, Smoothing Methods - Part 1,"00:01:02,738","00:01:07,506",13,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=62,And inside its sum is the count,pic_cs-410_4_6_60.jpg
cs-410_4_6_14,cs-410,4,6, Smoothing Methods - Part 1,"00:01:07,506","00:01:11,070",14,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=67,some weight for the term in the document.,pic_cs-410_4_6_60.jpg
cs-410_4_6_15,cs-410,4,6, Smoothing Methods - Part 1,"00:01:12,240","00:01:18,170",15,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=72,"We have t of i, the f weight here, and",pic_cs-410_4_6_60.jpg
cs-410_4_6_16,cs-410,4,6, Smoothing Methods - Part 1,"00:01:20,300","00:01:24,793",16,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=80,So clearly if we want to implement this,pic_cs-410_4_6_60.jpg
cs-410_4_6_17,cs-410,4,6, Smoothing Methods - Part 1,"00:01:24,793","00:01:27,650",17,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=84,we still need to figure,pic_cs-410_4_6_60.jpg
cs-410_4_6_18,cs-410,4,6, Smoothing Methods - Part 1,"00:01:27,650","00:01:33,730",18,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=87,"In particular, we're going to need to",pic_cs-410_4_6_60.jpg
cs-410_4_6_19,cs-410,4,6, Smoothing Methods - Part 1,"00:01:33,730","00:01:39,000",19,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=93,of a word exactly and how do we set alpha.,pic_cs-410_4_6_60.jpg
cs-410_4_6_20,cs-410,4,6, Smoothing Methods - Part 1,"00:01:40,270","00:01:44,410",20,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=100,"So in order to answer this question,",pic_cs-410_4_6_60.jpg
cs-410_4_6_21,cs-410,4,6, Smoothing Methods - Part 1,"00:01:44,410","00:01:47,760",21,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=104,"smoothing methods, and",pic_cs-410_4_6_60.jpg
cs-410_4_6_22,cs-410,4,6, Smoothing Methods - Part 1,"00:01:48,900","00:01:50,512",22,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=108,We're going to talk about,pic_cs-410_4_6_60.jpg
cs-410_4_6_23,cs-410,4,6, Smoothing Methods - Part 1,"00:01:50,512","00:01:55,575",23,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=110,The first is simple linear,pic_cs-410_4_6_60.jpg
cs-410_4_6_24,cs-410,4,6, Smoothing Methods - Part 1,"00:01:55,575","00:01:59,910",24,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=115,And this is also called,pic_cs-410_4_6_60.jpg
cs-410_4_6_25,cs-410,4,6, Smoothing Methods - Part 1,"00:02:01,170","00:02:04,140",25,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=121,So the idea is actually very simple.,pic_cs-410_4_6_120.jpg
cs-410_4_6_26,cs-410,4,6, Smoothing Methods - Part 1,"00:02:04,140","00:02:09,150",26,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=124,This picture shows how,pic_cs-410_4_6_120.jpg
cs-410_4_6_27,cs-410,4,6, Smoothing Methods - Part 1,"00:02:09,150","00:02:12,440",27,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=129,language model by using,pic_cs-410_4_6_120.jpg
cs-410_4_6_28,cs-410,4,6, Smoothing Methods - Part 1,"00:02:12,440","00:02:17,950",28,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=132,That gives us word counts normalized by,pic_cs-410_4_6_120.jpg
cs-410_4_6_29,cs-410,4,6, Smoothing Methods - Part 1,"00:02:17,950","00:02:21,130",29,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=137,The idea of using this method,pic_cs-410_4_6_120.jpg
cs-410_4_6_30,cs-410,4,6, Smoothing Methods - Part 1,"00:02:22,230","00:02:26,480",30,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=142,is to maximize the probability,pic_cs-410_4_6_120.jpg
cs-410_4_6_31,cs-410,4,6, Smoothing Methods - Part 1,"00:02:26,480","00:02:31,460",31,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=146,"As a result,",pic_cs-410_4_6_120.jpg
cs-410_4_6_32,cs-410,4,6, Smoothing Methods - Part 1,"00:02:31,460","00:02:36,210",32,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=151,"in the text, it's going to get",pic_cs-410_4_6_120.jpg
cs-410_4_6_33,cs-410,4,6, Smoothing Methods - Part 1,"00:02:37,810","00:02:42,620",33,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=157,"So the idea of smoothing, then,",pic_cs-410_4_6_120.jpg
cs-410_4_6_34,cs-410,4,6, Smoothing Methods - Part 1,"00:02:42,620","00:02:47,158",34,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=162,where this word is not going to have,pic_cs-410_4_6_120.jpg
cs-410_4_6_35,cs-410,4,6, Smoothing Methods - Part 1,"00:02:47,158","00:02:50,860",35,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=167,nonzero probability should,pic_cs-410_4_6_120.jpg
cs-410_4_6_36,cs-410,4,6, Smoothing Methods - Part 1,"00:02:50,860","00:02:55,560",36,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=170,So we can note that network has,pic_cs-410_4_6_120.jpg
cs-410_4_6_37,cs-410,4,6, Smoothing Methods - Part 1,"00:02:55,560","00:03:01,367",37,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=175,So in this approach what we do is we do,pic_cs-410_4_6_120.jpg
cs-410_4_6_38,cs-410,4,6, Smoothing Methods - Part 1,"00:03:01,367","00:03:06,655",38,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=181,likelihood placement here and,pic_cs-410_4_6_180.jpg
cs-410_4_6_39,cs-410,4,6, Smoothing Methods - Part 1,"00:03:06,655","00:03:13,040",39,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=186,is computed by the smoothing parameter,pic_cs-410_4_6_180.jpg
cs-410_4_6_40,cs-410,4,6, Smoothing Methods - Part 1,"00:03:13,040","00:03:15,817",40,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=193,So this is a smoothing parameter.,pic_cs-410_4_6_180.jpg
cs-410_4_6_41,cs-410,4,6, Smoothing Methods - Part 1,"00:03:15,817","00:03:20,651",41,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=195,"The larger lambda is,",pic_cs-410_4_6_180.jpg
cs-410_4_6_42,cs-410,4,6, Smoothing Methods - Part 1,"00:03:20,651","00:03:22,828",42,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=200,"So by mixing them together,",pic_cs-410_4_6_180.jpg
cs-410_4_6_43,cs-410,4,6, Smoothing Methods - Part 1,"00:03:22,828","00:03:29,060",43,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=202,we achieve the goal of assigning nonzero,pic_cs-410_4_6_180.jpg
cs-410_4_6_44,cs-410,4,6, Smoothing Methods - Part 1,"00:03:29,060","00:03:31,400",44,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=209,So let's see how it works for,pic_cs-410_4_6_180.jpg
cs-410_4_6_45,cs-410,4,6, Smoothing Methods - Part 1,"00:03:32,430","00:03:36,790",45,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=212,"For example, if we compute",pic_cs-410_4_6_180.jpg
cs-410_4_6_46,cs-410,4,6, Smoothing Methods - Part 1,"00:03:37,940","00:03:41,080",46,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=217,Now the maximum likelihood,pic_cs-410_4_6_180.jpg
cs-410_4_6_47,cs-410,4,6, Smoothing Methods - Part 1,"00:03:41,080","00:03:43,150",47,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=221,that's going to be here.,pic_cs-410_4_6_180.jpg
cs-410_4_6_48,cs-410,4,6, Smoothing Methods - Part 1,"00:03:44,320","00:03:47,740",48,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=224,But the collection probability is this.,pic_cs-410_4_6_180.jpg
cs-410_4_6_49,cs-410,4,6, Smoothing Methods - Part 1,"00:03:47,740","00:03:50,960",49,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=227,So we'll just combine them,pic_cs-410_4_6_180.jpg
cs-410_4_6_50,cs-410,4,6, Smoothing Methods - Part 1,"00:03:53,630","00:04:00,085",50,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=233,"We can also see the word network,",pic_cs-410_4_6_180.jpg
cs-410_4_6_51,cs-410,4,6, Smoothing Methods - Part 1,"00:04:00,085","00:04:05,305",51,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=240,now is getting a non-zero,pic_cs-410_4_6_240.jpg
cs-410_4_6_52,cs-410,4,6, Smoothing Methods - Part 1,"00:04:05,305","00:04:11,992",52,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=245,And that's because the count is,pic_cs-410_4_6_240.jpg
cs-410_4_6_53,cs-410,4,6, Smoothing Methods - Part 1,"00:04:11,992","00:04:19,097",53,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=251,"But this part is nonzero, and",pic_cs-410_4_6_240.jpg
cs-410_4_6_54,cs-410,4,6, Smoothing Methods - Part 1,"00:04:19,097","00:04:24,109",54,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=259,Now if you think about this and,pic_cs-410_4_6_240.jpg
cs-410_4_6_55,cs-410,4,6, Smoothing Methods - Part 1,"00:04:24,109","00:04:29,250",55,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=264,sub d in this smoothing,pic_cs-410_4_6_240.jpg
cs-410_4_6_56,cs-410,4,6, Smoothing Methods - Part 1,"00:04:29,250","00:04:34,830",56,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=269,Because that's remember the coefficient,pic_cs-410_4_6_240.jpg
cs-410_4_6_57,cs-410,4,6, Smoothing Methods - Part 1,"00:04:34,830","00:04:40,256",57,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=274,of the word given by the collection,pic_cs-410_4_6_240.jpg
cs-410_4_6_58,cs-410,4,6, Smoothing Methods - Part 1,"00:04:40,256","00:04:43,340",58,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=280,"Okay, so",pic_cs-410_4_6_240.jpg
cs-410_4_6_59,cs-410,4,6, Smoothing Methods - Part 1,"00:04:43,340","00:04:47,903",59,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=283,The second one is similar but,pic_cs-410_4_6_240.jpg
cs-410_4_6_60,cs-410,4,6, Smoothing Methods - Part 1,"00:04:47,903","00:04:49,565",60,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=287,linear interpolation.,pic_cs-410_4_6_240.jpg
cs-410_4_6_61,cs-410,4,6, Smoothing Methods - Part 1,"00:04:49,565","00:04:52,570",61,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=289,"It's often called Dirichlet Prior,",pic_cs-410_4_6_240.jpg
cs-410_4_6_62,cs-410,4,6, Smoothing Methods - Part 1,"00:04:54,540","00:04:59,015",62,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=294,So again here we face problem,pic_cs-410_4_6_240.jpg
cs-410_4_6_63,cs-410,4,6, Smoothing Methods - Part 1,"00:04:59,015","00:05:01,565",63,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=299,an unseen word like network.,pic_cs-410_4_6_240.jpg
cs-410_4_6_64,cs-410,4,6, Smoothing Methods - Part 1,"00:05:03,765","00:05:06,957",64,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=303,Again we will use the collection,pic_cs-410_4_6_300.jpg
cs-410_4_6_65,cs-410,4,6, Smoothing Methods - Part 1,"00:05:06,957","00:05:09,707",65,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=306,we're going to combine them,pic_cs-410_4_6_300.jpg
cs-410_4_6_66,cs-410,4,6, Smoothing Methods - Part 1,"00:05:09,707","00:05:14,739",66,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=309,The formula first can be seen as,pic_cs-410_4_6_300.jpg
cs-410_4_6_67,cs-410,4,6, Smoothing Methods - Part 1,"00:05:14,739","00:05:20,258",67,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=314,likelihood estimate and,pic_cs-410_4_6_300.jpg
cs-410_4_6_68,cs-410,4,6, Smoothing Methods - Part 1,"00:05:20,258","00:05:23,580",68,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=320,as in the J-M smoothing method.,pic_cs-410_4_6_300.jpg
cs-410_4_6_69,cs-410,4,6, Smoothing Methods - Part 1,"00:05:23,580","00:05:28,388",69,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=323,Only that the coefficient now,pic_cs-410_4_6_300.jpg
cs-410_4_6_70,cs-410,4,6, Smoothing Methods - Part 1,"00:05:28,388","00:05:31,532",70,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=328,"but a dynamic coefficient in this form,",pic_cs-410_4_6_300.jpg
cs-410_4_6_71,cs-410,4,6, Smoothing Methods - Part 1,"00:05:31,532","00:05:36,760",71,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=331,"where mu is a parameter,",pic_cs-410_4_6_300.jpg
cs-410_4_6_72,cs-410,4,6, Smoothing Methods - Part 1,"00:05:36,760","00:05:40,550",72,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=336,And you can see if we,pic_cs-410_4_6_300.jpg
cs-410_4_6_73,cs-410,4,6, Smoothing Methods - Part 1,"00:05:40,550","00:05:44,690",73,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=340,the effect is that a long document would,pic_cs-410_4_6_300.jpg
cs-410_4_6_74,cs-410,4,6, Smoothing Methods - Part 1,"00:05:46,090","00:05:49,200",74,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=346,Because a long document,pic_cs-410_4_6_300.jpg
cs-410_4_6_75,cs-410,4,6, Smoothing Methods - Part 1,"00:05:49,200","00:05:53,140",75,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=349,therefore the coefficient,pic_cs-410_4_6_300.jpg
cs-410_4_6_76,cs-410,4,6, Smoothing Methods - Part 1,"00:05:53,140","00:05:59,949",76,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=353,And so a long document would have,pic_cs-410_4_6_300.jpg
cs-410_4_6_77,cs-410,4,6, Smoothing Methods - Part 1,"00:05:59,949","00:06:05,734",77,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=359,So this seems to make more sense,pic_cs-410_4_6_300.jpg
cs-410_4_6_78,cs-410,4,6, Smoothing Methods - Part 1,"00:06:05,734","00:06:08,979",78,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=365,"Of course,",pic_cs-410_4_6_360.jpg
cs-410_4_6_79,cs-410,4,6, Smoothing Methods - Part 1,"00:06:08,979","00:06:12,156",79,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=368,that the two coefficients would sum to 1.,pic_cs-410_4_6_360.jpg
cs-410_4_6_80,cs-410,4,6, Smoothing Methods - Part 1,"00:06:12,156","00:06:16,400",80,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=372,Now this is one way to,pic_cs-410_4_6_360.jpg
cs-410_4_6_81,cs-410,4,6, Smoothing Methods - Part 1,"00:06:16,400","00:06:21,080",81,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=376,"Basically, it means it's a dynamic",pic_cs-410_4_6_360.jpg
cs-410_4_6_82,cs-410,4,6, Smoothing Methods - Part 1,"00:06:22,790","00:06:27,737",82,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=382,There is another way to understand,pic_cs-410_4_6_360.jpg
cs-410_4_6_83,cs-410,4,6, Smoothing Methods - Part 1,"00:06:27,737","00:06:31,620",83,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=387,"easier to remember, and",pic_cs-410_4_6_360.jpg
cs-410_4_6_84,cs-410,4,6, Smoothing Methods - Part 1,"00:06:33,310","00:06:38,878",84,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=393,So it's easier to see how we can rewrite,pic_cs-410_4_6_360.jpg
cs-410_4_6_85,cs-410,4,6, Smoothing Methods - Part 1,"00:06:38,878","00:06:42,847",85,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=398,Now in this form we can easily,pic_cs-410_4_6_360.jpg
cs-410_4_6_86,cs-410,4,6, Smoothing Methods - Part 1,"00:06:42,847","00:06:47,060",86,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=402,"the maximum likelihood estimate,",pic_cs-410_4_6_360.jpg
cs-410_4_6_87,cs-410,4,6, Smoothing Methods - Part 1,"00:06:47,060","00:06:53,346",87,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=407,So normalize the count,pic_cs-410_4_6_360.jpg
cs-410_4_6_88,cs-410,4,6, Smoothing Methods - Part 1,"00:06:53,346","00:07:00,750",88,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=413,So in this form we can see what we did is,pic_cs-410_4_6_360.jpg
cs-410_4_6_89,cs-410,4,6, Smoothing Methods - Part 1,"00:07:01,800","00:07:03,230",89,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=421,So what does this mean?,pic_cs-410_4_6_420.jpg
cs-410_4_6_90,cs-410,4,6, Smoothing Methods - Part 1,"00:07:03,230","00:07:08,042",90,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=423,"Well, this is basically something related",pic_cs-410_4_6_420.jpg
cs-410_4_6_91,cs-410,4,6, Smoothing Methods - Part 1,"00:07:08,042","00:07:09,180",91,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=428,the collection.,pic_cs-410_4_6_420.jpg
cs-410_4_6_92,cs-410,4,6, Smoothing Methods - Part 1,"00:07:10,390","00:07:13,225",92,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=430,And we multiply that by the parameter mu.,pic_cs-410_4_6_420.jpg
cs-410_4_6_93,cs-410,4,6, Smoothing Methods - Part 1,"00:07:14,510","00:07:18,577",93,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=434,And when we combine this,pic_cs-410_4_6_420.jpg
cs-410_4_6_94,cs-410,4,6, Smoothing Methods - Part 1,"00:07:18,577","00:07:24,265",94,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=438,essentially we are adding,pic_cs-410_4_6_420.jpg
cs-410_4_6_95,cs-410,4,6, Smoothing Methods - Part 1,"00:07:24,265","00:07:31,090",95,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=444,We pretend every word has,pic_cs-410_4_6_420.jpg
cs-410_4_6_96,cs-410,4,6, Smoothing Methods - Part 1,"00:07:31,090","00:07:35,290",96,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=451,So the total count would be,pic_cs-410_4_6_420.jpg
cs-410_4_6_97,cs-410,4,6, Smoothing Methods - Part 1,"00:07:35,290","00:07:38,730",97,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=455,the actual count of,pic_cs-410_4_6_420.jpg
cs-410_4_6_98,cs-410,4,6, Smoothing Methods - Part 1,"00:07:39,950","00:07:46,020",98,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=459,"As a result, in total we would",pic_cs-410_4_6_420.jpg
cs-410_4_6_99,cs-410,4,6, Smoothing Methods - Part 1,"00:07:46,020","00:07:49,640",99,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=466,Why?,pic_cs-410_4_6_420.jpg
cs-410_4_6_100,cs-410,4,6, Smoothing Methods - Part 1,"00:07:50,770","00:07:55,480",100,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=470,"over all the words, then we'll see the",pic_cs-410_4_6_420.jpg
cs-410_4_6_101,cs-410,4,6, Smoothing Methods - Part 1,"00:07:55,480","00:07:57,380",101,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=475,and that gives us just mu.,pic_cs-410_4_6_420.jpg
cs-410_4_6_102,cs-410,4,6, Smoothing Methods - Part 1,"00:07:57,380","00:08:00,190",102,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=477,So this is the total number of,pic_cs-410_4_6_420.jpg
cs-410_4_6_103,cs-410,4,6, Smoothing Methods - Part 1,"00:08:01,550","00:08:05,270",103,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=481,And so,pic_cs-410_4_6_480.jpg
cs-410_4_6_104,cs-410,4,6, Smoothing Methods - Part 1,"00:08:05,270","00:08:12,590",104,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=485,"So in this case, we can easily",pic_cs-410_4_6_480.jpg
cs-410_4_6_105,cs-410,4,6, Smoothing Methods - Part 1,"00:08:13,920","00:08:18,130",105,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=493,add this as a pseudocount to this data.,pic_cs-410_4_6_480.jpg
cs-410_4_6_106,cs-410,4,6, Smoothing Methods - Part 1,"00:08:18,130","00:08:22,877",106,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=498,Pretend we actually augment the data,pic_cs-410_4_6_480.jpg
cs-410_4_6_107,cs-410,4,6, Smoothing Methods - Part 1,"00:08:22,877","00:08:26,022",107,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=502,defined by the collection language model.,pic_cs-410_4_6_480.jpg
cs-410_4_6_108,cs-410,4,6, Smoothing Methods - Part 1,"00:08:26,022","00:08:30,201",108,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=506,"As a result, we have more counts is that",pic_cs-410_4_6_480.jpg
cs-410_4_6_109,cs-410,4,6, Smoothing Methods - Part 1,"00:08:30,201","00:08:35,710",109,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=510,the total counts for,pic_cs-410_4_6_480.jpg
cs-410_4_6_110,cs-410,4,6, Smoothing Methods - Part 1,"00:08:35,710","00:08:41,499",110,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=515,"And as a result, even if a word has zero",pic_cs-410_4_6_480.jpg
cs-410_4_6_111,cs-410,4,6, Smoothing Methods - Part 1,"00:08:41,499","00:08:47,115",111,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=521,"count here, then it would still have",pic_cs-410_4_6_480.jpg
cs-410_4_6_112,cs-410,4,6, Smoothing Methods - Part 1,"00:08:47,115","00:08:49,750",112,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=527,So this is how this method works.,pic_cs-410_4_6_480.jpg
cs-410_4_6_113,cs-410,4,6, Smoothing Methods - Part 1,"00:08:49,750","00:08:52,650",113,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=529,Let's also take a look at,pic_cs-410_4_6_480.jpg
cs-410_4_6_114,cs-410,4,6, Smoothing Methods - Part 1,"00:08:52,650","00:08:58,580",114,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=532,So for text again we will,pic_cs-410_4_6_480.jpg
cs-410_4_6_115,cs-410,4,6, Smoothing Methods - Part 1,"00:08:58,580","00:09:03,000",115,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=538,"that we actually observe, but",pic_cs-410_4_6_480.jpg
cs-410_4_6_116,cs-410,4,6, Smoothing Methods - Part 1,"00:09:03,000","00:09:05,725",116,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=543,And so the probability of,pic_cs-410_4_6_540.jpg
cs-410_4_6_117,cs-410,4,6, Smoothing Methods - Part 1,"00:09:05,725","00:09:11,051",117,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=545,"Naturally, the probability of",pic_cs-410_4_6_540.jpg
cs-410_4_6_118,cs-410,4,6, Smoothing Methods - Part 1,"00:09:11,051","00:09:14,410",118,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=551,And so here you can also see,pic_cs-410_4_6_540.jpg
cs-410_4_6_119,cs-410,4,6, Smoothing Methods - Part 1,"00:09:15,600","00:09:16,850",119,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=555,Can you see it?,pic_cs-410_4_6_540.jpg
cs-410_4_6_120,cs-410,4,6, Smoothing Methods - Part 1,"00:09:16,850","00:09:19,020",120,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=556,"If you want to think about it,",pic_cs-410_4_6_540.jpg
cs-410_4_6_121,cs-410,4,6, Smoothing Methods - Part 1,"00:09:20,590","00:09:25,618",121,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=560,But you'll notice that this,pic_cs-410_4_6_540.jpg
cs-410_4_6_122,cs-410,4,6, Smoothing Methods - Part 1,"00:09:25,618","00:09:29,122",122,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=565,"So we can see, in this case,",pic_cs-410_4_6_540.jpg
cs-410_4_6_123,cs-410,4,6, Smoothing Methods - Part 1,"00:09:29,122","00:09:34,089",123,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=569,"alpha sub d does depend on the document,",pic_cs-410_4_6_540.jpg
cs-410_4_6_124,cs-410,4,6, Smoothing Methods - Part 1,"00:09:34,089","00:09:39,787",124,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=574,because this length,pic_cs-410_4_6_540.jpg
cs-410_4_6_125,cs-410,4,6, Smoothing Methods - Part 1,"00:09:39,787","00:09:44,609",125,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=579,"whereas in the linear interpolation,",pic_cs-410_4_6_540.jpg
cs-410_4_6_126,cs-410,4,6, Smoothing Methods - Part 1,"00:09:44,609","00:09:50,622",126,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=584,"the J-M smoothing method,",pic_cs-410_4_6_540.jpg
cs-410_4_6_127,cs-410,4,6, Smoothing Methods - Part 1,"00:09:50,622","00:09:54,919",127,https://www.coursera.org/learn/cs-410/lecture/kM6Ie?t=590,[MUSIC],pic_cs-410_4_6_540.jpg
cs-410_4_7_1,cs-410,4,7, Smoothing Methods - Part 2,"00:00:00,006","00:00:03,253",1,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=0,[SOUND],pic_cs-410_4_7_0.jpg
cs-410_4_7_2,cs-410,4,7, Smoothing Methods - Part 2,"00:00:13,295","00:00:15,326",2,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=13,So let's plug in these model masses,pic_cs-410_4_7_0.jpg
cs-410_4_7_3,cs-410,4,7, Smoothing Methods - Part 2,"00:00:15,326","00:00:18,610",3,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=15,into the ranking function to,pic_cs-410_4_7_0.jpg
cs-410_4_7_4,cs-410,4,7, Smoothing Methods - Part 2,"00:00:18,610","00:00:20,780",4,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=18,This is a general smoothing.,pic_cs-410_4_7_0.jpg
cs-410_4_7_5,cs-410,4,7, Smoothing Methods - Part 2,"00:00:20,780","00:00:24,570",5,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=20,So a general ranking function for,pic_cs-410_4_7_0.jpg
cs-410_4_7_6,cs-410,4,7, Smoothing Methods - Part 2,"00:00:24,570","00:00:26,500",6,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=24,you have seen this before.,pic_cs-410_4_7_0.jpg
cs-410_4_7_7,cs-410,4,7, Smoothing Methods - Part 2,"00:00:28,060","00:00:32,550",7,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=28,And now we have a very specific smoothing,pic_cs-410_4_7_0.jpg
cs-410_4_7_8,cs-410,4,7, Smoothing Methods - Part 2,"00:00:33,690","00:00:39,190",8,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=33,So now let's see what what's a value for,pic_cs-410_4_7_0.jpg
cs-410_4_7_9,cs-410,4,7, Smoothing Methods - Part 2,"00:00:40,450","00:00:42,900",9,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=40,And what's the value for p sub c here?,pic_cs-410_4_7_0.jpg
cs-410_4_7_10,cs-410,4,7, Smoothing Methods - Part 2,"00:00:42,900","00:00:46,930",10,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=42,"Right, so we may need to decide this",pic_cs-410_4_7_0.jpg
cs-410_4_7_11,cs-410,4,7, Smoothing Methods - Part 2,"00:00:46,930","00:00:50,470",11,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=46,in order to figure out the exact,pic_cs-410_4_7_0.jpg
cs-410_4_7_12,cs-410,4,7, Smoothing Methods - Part 2,"00:00:50,470","00:00:52,598",12,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=50,And we also need to figure,pic_cs-410_4_7_0.jpg
cs-410_4_7_13,cs-410,4,7, Smoothing Methods - Part 2,"00:00:52,598","00:00:55,910",13,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=52,So let's see.,pic_cs-410_4_7_0.jpg
cs-410_4_7_14,cs-410,4,7, Smoothing Methods - Part 2,"00:00:55,910","00:01:00,666",14,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=55,"Well this ratio is basically this,",pic_cs-410_4_7_0.jpg
cs-410_4_7_15,cs-410,4,7, Smoothing Methods - Part 2,"00:01:00,666","00:01:05,315",15,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=60,"here, this is the probability",pic_cs-410_4_7_60.jpg
cs-410_4_7_16,cs-410,4,7, Smoothing Methods - Part 2,"00:01:05,315","00:01:09,330",16,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=65,and this is the probability,pic_cs-410_4_7_60.jpg
cs-410_4_7_17,cs-410,4,7, Smoothing Methods - Part 2,"00:01:09,330","00:01:14,935",17,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=69,in other words basically 11,pic_cs-410_4_7_60.jpg
cs-410_4_7_18,cs-410,4,7, Smoothing Methods - Part 2,"00:01:14,935","00:01:18,530",18,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=74,"this, so it's easy to see that.",pic_cs-410_4_7_60.jpg
cs-410_4_7_19,cs-410,4,7, Smoothing Methods - Part 2,"00:01:18,530","00:01:21,681",19,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=78,This can be then rewritten as this.,pic_cs-410_4_7_60.jpg
cs-410_4_7_20,cs-410,4,7, Smoothing Methods - Part 2,"00:01:21,681","00:01:24,500",20,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=81,Very simple.,pic_cs-410_4_7_60.jpg
cs-410_4_7_21,cs-410,4,7, Smoothing Methods - Part 2,"00:01:24,500","00:01:26,810",21,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=84,So we can plug this into here.,pic_cs-410_4_7_60.jpg
cs-410_4_7_22,cs-410,4,7, Smoothing Methods - Part 2,"00:01:28,650","00:01:30,710",22,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=88,"And then here, what's the value for alpha?",pic_cs-410_4_7_60.jpg
cs-410_4_7_23,cs-410,4,7, Smoothing Methods - Part 2,"00:01:30,710","00:01:31,660",23,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=90,What do you think?,pic_cs-410_4_7_60.jpg
cs-410_4_7_24,cs-410,4,7, Smoothing Methods - Part 2,"00:01:31,660","00:01:35,250",24,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=91,"So it would be just lambda, right?",pic_cs-410_4_7_60.jpg
cs-410_4_7_25,cs-410,4,7, Smoothing Methods - Part 2,"00:01:38,250","00:01:43,900",25,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=98,And what would happen if we plug in,pic_cs-410_4_7_60.jpg
cs-410_4_7_26,cs-410,4,7, Smoothing Methods - Part 2,"00:01:43,900","00:01:45,350",26,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=103,What can we say about this?,pic_cs-410_4_7_60.jpg
cs-410_4_7_27,cs-410,4,7, Smoothing Methods - Part 2,"00:01:47,940","00:01:49,640",27,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=107,Does it depend on the document?,pic_cs-410_4_7_60.jpg
cs-410_4_7_28,cs-410,4,7, Smoothing Methods - Part 2,"00:01:50,660","00:01:52,170",28,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=110,"No, so it can be ignored.",pic_cs-410_4_7_60.jpg
cs-410_4_7_29,cs-410,4,7, Smoothing Methods - Part 2,"00:01:53,570","00:01:55,040",29,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=113,Right?,pic_cs-410_4_7_60.jpg
cs-410_4_7_30,cs-410,4,7, Smoothing Methods - Part 2,"00:01:55,040","00:01:58,690",30,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=115,So we'll end up having this,pic_cs-410_4_7_60.jpg
cs-410_4_7_31,cs-410,4,7, Smoothing Methods - Part 2,"00:02:00,520","00:02:02,690",31,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=120,"And in this case you can easy to see,",pic_cs-410_4_7_120.jpg
cs-410_4_7_32,cs-410,4,7, Smoothing Methods - Part 2,"00:02:02,690","00:02:07,780",32,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=122,this a precisely a vector space,pic_cs-410_4_7_120.jpg
cs-410_4_7_33,cs-410,4,7, Smoothing Methods - Part 2,"00:02:07,780","00:02:13,480",33,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=127,"a sum over all the matched query terms,",pic_cs-410_4_7_120.jpg
cs-410_4_7_34,cs-410,4,7, Smoothing Methods - Part 2,"00:02:13,480","00:02:16,140",34,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=133,What do you think is a element,pic_cs-410_4_7_120.jpg
cs-410_4_7_35,cs-410,4,7, Smoothing Methods - Part 2,"00:02:18,670","00:02:20,200",35,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=138,"Well it's this, right.",pic_cs-410_4_7_120.jpg
cs-410_4_7_36,cs-410,4,7, Smoothing Methods - Part 2,"00:02:20,200","00:02:23,210",36,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=140,So that's our document left element.,pic_cs-410_4_7_120.jpg
cs-410_4_7_37,cs-410,4,7, Smoothing Methods - Part 2,"00:02:23,210","00:02:29,210",37,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=143,And let's further examine what's,pic_cs-410_4_7_120.jpg
cs-410_4_7_38,cs-410,4,7, Smoothing Methods - Part 2,"00:02:30,370","00:02:32,440",38,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=150,Well one plus this.,pic_cs-410_4_7_120.jpg
cs-410_4_7_39,cs-410,4,7, Smoothing Methods - Part 2,"00:02:32,440","00:02:36,630",39,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=152,"So it's going to be nonnegative,",pic_cs-410_4_7_120.jpg
cs-410_4_7_40,cs-410,4,7, Smoothing Methods - Part 2,"00:02:36,630","00:02:37,850",40,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=156,"it's going to be at least 1, right?",pic_cs-410_4_7_120.jpg
cs-410_4_7_41,cs-410,4,7, Smoothing Methods - Part 2,"00:02:39,450","00:02:42,900",41,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=159,"And these, this is a parameter,",pic_cs-410_4_7_120.jpg
cs-410_4_7_42,cs-410,4,7, Smoothing Methods - Part 2,"00:02:42,900","00:02:44,340",42,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=162,And let's look at this.,pic_cs-410_4_7_120.jpg
cs-410_4_7_43,cs-410,4,7, Smoothing Methods - Part 2,"00:02:44,340","00:02:45,480",43,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=164,Now this is a TF.,pic_cs-410_4_7_120.jpg
cs-410_4_7_44,cs-410,4,7, Smoothing Methods - Part 2,"00:02:45,480","00:02:48,070",44,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=165,Now we see very clearly,pic_cs-410_4_7_120.jpg
cs-410_4_7_45,cs-410,4,7, Smoothing Methods - Part 2,"00:02:49,250","00:02:54,080",45,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=169,"And the larger the count is,",pic_cs-410_4_7_120.jpg
cs-410_4_7_46,cs-410,4,7, Smoothing Methods - Part 2,"00:02:54,080","00:02:57,080",46,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=174,"We also see IDF weighting,",pic_cs-410_4_7_120.jpg
cs-410_4_7_47,cs-410,4,7, Smoothing Methods - Part 2,"00:02:58,720","00:03:00,996",47,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=178,And we see docking the lan's,pic_cs-410_4_7_120.jpg
cs-410_4_7_48,cs-410,4,7, Smoothing Methods - Part 2,"00:03:00,996","00:03:03,270",48,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=180,So all these heuristics,pic_cs-410_4_7_180.jpg
cs-410_4_7_49,cs-410,4,7, Smoothing Methods - Part 2,"00:03:04,532","00:03:08,480",49,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=184,What's interesting that,pic_cs-410_4_7_180.jpg
cs-410_4_7_50,cs-410,4,7, Smoothing Methods - Part 2,"00:03:08,480","00:03:12,330",50,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=188,weighting function automatically,pic_cs-410_4_7_180.jpg
cs-410_4_7_51,cs-410,4,7, Smoothing Methods - Part 2,"00:03:12,330","00:03:14,270",51,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=192,"Whereas in the vector space model,",pic_cs-410_4_7_180.jpg
cs-410_4_7_52,cs-410,4,7, Smoothing Methods - Part 2,"00:03:14,270","00:03:19,330",52,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=194,we had to go through those heuristic,pic_cs-410_4_7_180.jpg
cs-410_4_7_53,cs-410,4,7, Smoothing Methods - Part 2,"00:03:19,330","00:03:21,880",53,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=199,And in this case note that,pic_cs-410_4_7_180.jpg
cs-410_4_7_54,cs-410,4,7, Smoothing Methods - Part 2,"00:03:21,880","00:03:25,120",54,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=201,And when you see whether this,pic_cs-410_4_7_180.jpg
cs-410_4_7_55,cs-410,4,7, Smoothing Methods - Part 2,"00:03:26,690","00:03:31,050",55,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=206,All right so what do you think,pic_cs-410_4_7_180.jpg
cs-410_4_7_56,cs-410,4,7, Smoothing Methods - Part 2,"00:03:31,050","00:03:33,320",56,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=211,This is a math of document.,pic_cs-410_4_7_180.jpg
cs-410_4_7_57,cs-410,4,7, Smoothing Methods - Part 2,"00:03:33,320","00:03:37,340",57,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=213,"Total number of words,",pic_cs-410_4_7_180.jpg
cs-410_4_7_58,cs-410,4,7, Smoothing Methods - Part 2,"00:03:38,400","00:03:42,727",58,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=218,"given by the collection, right?",pic_cs-410_4_7_180.jpg
cs-410_4_7_59,cs-410,4,7, Smoothing Methods - Part 2,"00:03:42,727","00:03:48,090",59,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=222,So this actually can be interpreted,pic_cs-410_4_7_180.jpg
cs-410_4_7_60,cs-410,4,7, Smoothing Methods - Part 2,"00:03:48,090","00:03:53,730",60,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=228,"If we're going to draw, a word,",pic_cs-410_4_7_180.jpg
cs-410_4_7_61,cs-410,4,7, Smoothing Methods - Part 2,"00:03:53,730","00:03:57,980",61,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=233,"And, we're going to draw as many as",pic_cs-410_4_7_180.jpg
cs-410_4_7_62,cs-410,4,7, Smoothing Methods - Part 2,"00:03:59,310","00:04:02,940",62,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=239,"If you do that,",pic_cs-410_4_7_180.jpg
cs-410_4_7_63,cs-410,4,7, Smoothing Methods - Part 2,"00:04:02,940","00:04:06,950",63,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=242,would be precisely given,pic_cs-410_4_7_240.jpg
cs-410_4_7_64,cs-410,4,7, Smoothing Methods - Part 2,"00:04:08,240","00:04:14,400",64,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=248,"So, this ratio basically,",pic_cs-410_4_7_240.jpg
cs-410_4_7_65,cs-410,4,7, Smoothing Methods - Part 2,"00:04:15,860","00:04:21,280",65,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=255,The actual count of the word in the,pic_cs-410_4_7_240.jpg
cs-410_4_7_66,cs-410,4,7, Smoothing Methods - Part 2,"00:04:21,280","00:04:29,570",66,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=261,product if the word is in fact following,pic_cs-410_4_7_240.jpg
cs-410_4_7_67,cs-410,4,7, Smoothing Methods - Part 2,"00:04:29,570","00:04:33,250",67,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=269,And if this counter is larger than,pic_cs-410_4_7_240.jpg
cs-410_4_7_68,cs-410,4,7, Smoothing Methods - Part 2,"00:04:33,250","00:04:34,789",68,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=273,this ratio would be larger than one.,pic_cs-410_4_7_240.jpg
cs-410_4_7_69,cs-410,4,7, Smoothing Methods - Part 2,"00:04:37,100","00:04:40,460",69,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=277,So that's actually a very,pic_cs-410_4_7_240.jpg
cs-410_4_7_70,cs-410,4,7, Smoothing Methods - Part 2,"00:04:40,460","00:04:43,930",70,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=280,"It's very natural and intuitive,",pic_cs-410_4_7_240.jpg
cs-410_4_7_71,cs-410,4,7, Smoothing Methods - Part 2,"00:04:45,240","00:04:49,580",71,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=285,And this is one advantage of using,pic_cs-410_4_7_240.jpg
cs-410_4_7_72,cs-410,4,7, Smoothing Methods - Part 2,"00:04:49,580","00:04:53,240",72,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=289,where we have made explicit assumptions.,pic_cs-410_4_7_240.jpg
cs-410_4_7_73,cs-410,4,7, Smoothing Methods - Part 2,"00:04:53,240","00:04:56,490",73,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=293,"And, we know precisely why",pic_cs-410_4_7_240.jpg
cs-410_4_7_74,cs-410,4,7, Smoothing Methods - Part 2,"00:04:56,490","00:04:58,800",74,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=296,"And, why we have these probabilities here.",pic_cs-410_4_7_240.jpg
cs-410_4_7_75,cs-410,4,7, Smoothing Methods - Part 2,"00:05:00,280","00:05:04,290",75,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=300,"And, we also have a formula that",pic_cs-410_4_7_300.jpg
cs-410_4_7_76,cs-410,4,7, Smoothing Methods - Part 2,"00:05:04,290","00:05:07,190",76,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=304,does TF-IDF weighting and,pic_cs-410_4_7_300.jpg
cs-410_4_7_77,cs-410,4,7, Smoothing Methods - Part 2,"00:05:09,010","00:05:11,440",77,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=309,"Let's look at the,",pic_cs-410_4_7_300.jpg
cs-410_4_7_78,cs-410,4,7, Smoothing Methods - Part 2,"00:05:11,440","00:05:16,852",78,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=311,It's very similar to,pic_cs-410_4_7_300.jpg
cs-410_4_7_79,cs-410,4,7, Smoothing Methods - Part 2,"00:05:16,852","00:05:21,540",79,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=316,"In this case,",pic_cs-410_4_7_300.jpg
cs-410_4_7_80,cs-410,4,7, Smoothing Methods - Part 2,"00:05:21,540","00:05:27,660",80,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=321,that's different from,pic_cs-410_4_7_300.jpg
cs-410_4_7_81,cs-410,4,7, Smoothing Methods - Part 2,"00:05:27,660","00:05:30,660",81,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=327,But the format looks very similar.,pic_cs-410_4_7_300.jpg
cs-410_4_7_82,cs-410,4,7, Smoothing Methods - Part 2,"00:05:30,660","00:05:32,570",82,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=330,The form of the function,pic_cs-410_4_7_300.jpg
cs-410_4_7_83,cs-410,4,7, Smoothing Methods - Part 2,"00:05:34,540","00:05:36,730",83,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=334,So we still have linear operation here.,pic_cs-410_4_7_300.jpg
cs-410_4_7_84,cs-410,4,7, Smoothing Methods - Part 2,"00:05:38,090","00:05:40,130",84,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=338,"And when we compute this ratio,",pic_cs-410_4_7_300.jpg
cs-410_4_7_85,cs-410,4,7, Smoothing Methods - Part 2,"00:05:40,130","00:05:45,460",85,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=340,one will find that is that,pic_cs-410_4_7_300.jpg
cs-410_4_7_86,cs-410,4,7, Smoothing Methods - Part 2,"00:05:46,930","00:05:51,620",86,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=346,And what's interesting here is that we,pic_cs-410_4_7_300.jpg
cs-410_4_7_87,cs-410,4,7, Smoothing Methods - Part 2,"00:05:51,620","00:05:54,440",87,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=351,We're comparing the actual count.,pic_cs-410_4_7_300.jpg
cs-410_4_7_88,cs-410,4,7, Smoothing Methods - Part 2,"00:05:54,440","00:05:59,400",88,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=354,Which is the expected account of the world,pic_cs-410_4_7_300.jpg
cs-410_4_7_89,cs-410,4,7, Smoothing Methods - Part 2,"00:05:59,400","00:06:02,660",89,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=359,the collection world probability.,pic_cs-410_4_7_300.jpg
cs-410_4_7_90,cs-410,4,7, Smoothing Methods - Part 2,"00:06:02,660","00:06:07,266",90,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=362,So note that it's interesting we don't,pic_cs-410_4_7_360.jpg
cs-410_4_7_91,cs-410,4,7, Smoothing Methods - Part 2,"00:06:07,266","00:06:08,910",91,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=367,lighter in the JMs model.,pic_cs-410_4_7_360.jpg
cs-410_4_7_92,cs-410,4,7, Smoothing Methods - Part 2,"00:06:08,910","00:06:13,880",92,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=368,All right so this of course,pic_cs-410_4_7_360.jpg
cs-410_4_7_93,cs-410,4,7, Smoothing Methods - Part 2,"00:06:15,290","00:06:18,200",93,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=375,"So you might wonder, so",pic_cs-410_4_7_360.jpg
cs-410_4_7_94,cs-410,4,7, Smoothing Methods - Part 2,"00:06:18,200","00:06:23,650",94,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=378,Interestingly the docking lens,pic_cs-410_4_7_360.jpg
cs-410_4_7_95,cs-410,4,7, Smoothing Methods - Part 2,"00:06:23,650","00:06:26,850",95,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=383,this would be plugged into this part.,pic_cs-410_4_7_360.jpg
cs-410_4_7_96,cs-410,4,7, Smoothing Methods - Part 2,"00:06:26,850","00:06:31,860",96,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=386,As a result what we get is,pic_cs-410_4_7_360.jpg
cs-410_4_7_97,cs-410,4,7, Smoothing Methods - Part 2,"00:06:31,860","00:06:35,239",97,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=391,this is again a sum over,pic_cs-410_4_7_360.jpg
cs-410_4_7_98,cs-410,4,7, Smoothing Methods - Part 2,"00:06:36,290","00:06:40,050",98,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=396,"And we're against the queer,",pic_cs-410_4_7_360.jpg
cs-410_4_7_99,cs-410,4,7, Smoothing Methods - Part 2,"00:06:41,410","00:06:45,425",99,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=401,And you can interpret this as,pic_cs-410_4_7_360.jpg
cs-410_4_7_100,cs-410,4,7, Smoothing Methods - Part 2,"00:06:45,425","00:06:48,700",100,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=405,but this is no longer,pic_cs-410_4_7_360.jpg
cs-410_4_7_101,cs-410,4,7, Smoothing Methods - Part 2,"00:06:50,100","00:06:55,165",101,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=410,"Because we have this part,",pic_cs-410_4_7_360.jpg
cs-410_4_7_102,cs-410,4,7, Smoothing Methods - Part 2,"00:06:55,165","00:06:57,810",102,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=415,right?,pic_cs-410_4_7_360.jpg
cs-410_4_7_103,cs-410,4,7, Smoothing Methods - Part 2,"00:06:57,810","00:07:01,510",103,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=417,So that just means if,pic_cs-410_4_7_360.jpg
cs-410_4_7_104,cs-410,4,7, Smoothing Methods - Part 2,"00:07:01,510","00:07:05,160",104,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=421,we have to take a sum over,pic_cs-410_4_7_420.jpg
cs-410_4_7_105,cs-410,4,7, Smoothing Methods - Part 2,"00:07:05,160","00:07:09,270",105,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=425,then do some adjustment of,pic_cs-410_4_7_420.jpg
cs-410_4_7_106,cs-410,4,7, Smoothing Methods - Part 2,"00:07:11,510","00:07:15,974",106,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=431,"But it's still, it's still clear",pic_cs-410_4_7_420.jpg
cs-410_4_7_107,cs-410,4,7, Smoothing Methods - Part 2,"00:07:15,974","00:07:19,765",107,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=435,modulation because this lens,pic_cs-410_4_7_420.jpg
cs-410_4_7_108,cs-410,4,7, Smoothing Methods - Part 2,"00:07:19,765","00:07:23,237",108,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=439,a longer document will,pic_cs-410_4_7_420.jpg
cs-410_4_7_109,cs-410,4,7, Smoothing Methods - Part 2,"00:07:23,237","00:07:27,600",109,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=443,And we can also see it has tf here and,pic_cs-410_4_7_420.jpg
cs-410_4_7_110,cs-410,4,7, Smoothing Methods - Part 2,"00:07:27,600","00:07:32,038",110,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=447,Only that this time the form of the,pic_cs-410_4_7_420.jpg
cs-410_4_7_111,cs-410,4,7, Smoothing Methods - Part 2,"00:07:32,038","00:07:34,580",111,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=452,in JMs one.,pic_cs-410_4_7_420.jpg
cs-410_4_7_112,cs-410,4,7, Smoothing Methods - Part 2,"00:07:34,580","00:07:39,780",112,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=454,But intuitively it still implements TFIDF,pic_cs-410_4_7_420.jpg
cs-410_4_7_113,cs-410,4,7, Smoothing Methods - Part 2,"00:07:39,780","00:07:44,340",113,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=459,the form of the function is dictated,pic_cs-410_4_7_420.jpg
cs-410_4_7_114,cs-410,4,7, Smoothing Methods - Part 2,"00:07:44,340","00:07:45,938",114,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=464,assumptions that we have made.,pic_cs-410_4_7_420.jpg
cs-410_4_7_115,cs-410,4,7, Smoothing Methods - Part 2,"00:07:45,938","00:07:50,420",115,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=465,Now there are also,pic_cs-410_4_7_420.jpg
cs-410_4_7_116,cs-410,4,7, Smoothing Methods - Part 2,"00:07:50,420","00:07:53,600",116,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=470,"And that is, there's no guarantee",pic_cs-410_4_7_420.jpg
cs-410_4_7_117,cs-410,4,7, Smoothing Methods - Part 2,"00:07:53,600","00:07:55,800",117,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=473,of the formula will actually work well.,pic_cs-410_4_7_420.jpg
cs-410_4_7_118,cs-410,4,7, Smoothing Methods - Part 2,"00:07:55,800","00:08:01,037",118,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=475,"So if we look about at this geo function,",pic_cs-410_4_7_420.jpg
cs-410_4_7_119,cs-410,4,7, Smoothing Methods - Part 2,"00:08:01,037","00:08:06,860",119,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=481,rendition for example it's unclear whether,pic_cs-410_4_7_480.jpg
cs-410_4_7_120,cs-410,4,7, Smoothing Methods - Part 2,"00:08:06,860","00:08:13,110",120,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=486,Unfortunately we can see here there,pic_cs-410_4_7_480.jpg
cs-410_4_7_121,cs-410,4,7, Smoothing Methods - Part 2,"00:08:13,110","00:08:17,580",121,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=493,"So we do have also the,",pic_cs-410_4_7_480.jpg
cs-410_4_7_122,cs-410,4,7, Smoothing Methods - Part 2,"00:08:17,580","00:08:20,986",122,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=497,So we do have the sublinear,pic_cs-410_4_7_480.jpg
cs-410_4_7_123,cs-410,4,7, Smoothing Methods - Part 2,"00:08:20,986","00:08:23,320",123,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=500,we do not intentionally do that.,pic_cs-410_4_7_480.jpg
cs-410_4_7_124,cs-410,4,7, Smoothing Methods - Part 2,"00:08:23,320","00:08:27,750",124,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=503,That means there's no guarantee that,pic_cs-410_4_7_480.jpg
cs-410_4_7_125,cs-410,4,7, Smoothing Methods - Part 2,"00:08:27,750","00:08:31,800",125,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=507,"Suppose we don't have logarithm,",pic_cs-410_4_7_480.jpg
cs-410_4_7_126,cs-410,4,7, Smoothing Methods - Part 2,"00:08:31,800","00:08:35,810",126,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=511,"As we discussed before, perhaps",pic_cs-410_4_7_480.jpg
cs-410_4_7_127,cs-410,4,7, Smoothing Methods - Part 2,"00:08:35,810","00:08:40,870",127,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=515,So that's an example of the gap,pic_cs-410_4_7_480.jpg
cs-410_4_7_128,cs-410,4,7, Smoothing Methods - Part 2,"00:08:40,870","00:08:43,080",128,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=520,"the relevance that we have to model,",pic_cs-410_4_7_480.jpg
cs-410_4_7_129,cs-410,4,7, Smoothing Methods - Part 2,"00:08:43,080","00:08:48,720",129,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=523,which is really a subject,pic_cs-410_4_7_480.jpg
cs-410_4_7_130,cs-410,4,7, Smoothing Methods - Part 2,"00:08:50,640","00:08:53,390",130,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=530,So it doesn't mean we cannot fix this.,pic_cs-410_4_7_480.jpg
cs-410_4_7_131,cs-410,4,7, Smoothing Methods - Part 2,"00:08:53,390","00:08:57,390",131,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=533,"For example, imagine if we did",pic_cs-410_4_7_480.jpg
cs-410_4_7_132,cs-410,4,7, Smoothing Methods - Part 2,"00:08:57,390","00:08:59,250",132,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=537,So we can take a risk and,pic_cs-410_4_7_480.jpg
cs-410_4_7_133,cs-410,4,7, Smoothing Methods - Part 2,"00:08:59,250","00:09:01,935",133,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=539,or we can even add double logarithm.,pic_cs-410_4_7_480.jpg
cs-410_4_7_134,cs-410,4,7, Smoothing Methods - Part 2,"00:09:01,935","00:09:06,200",134,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=541,"But then, it would mean that the function",pic_cs-410_4_7_540.jpg
cs-410_4_7_135,cs-410,4,7, Smoothing Methods - Part 2,"00:09:06,200","00:09:10,780",135,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=546,So the consequence of,pic_cs-410_4_7_540.jpg
cs-410_4_7_136,cs-410,4,7, Smoothing Methods - Part 2,"00:09:10,780","00:09:14,670",136,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=550,longer as predictable as,pic_cs-410_4_7_540.jpg
cs-410_4_7_137,cs-410,4,7, Smoothing Methods - Part 2,"00:09:15,810","00:09:21,410",137,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=555,"So, that's also why, for example,",pic_cs-410_4_7_540.jpg
cs-410_4_7_138,cs-410,4,7, Smoothing Methods - Part 2,"00:09:21,410","00:09:26,720",138,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=561,"still, open channel how to use",pic_cs-410_4_7_540.jpg
cs-410_4_7_139,cs-410,4,7, Smoothing Methods - Part 2,"00:09:26,720","00:09:28,690",139,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=566,better model than the PM25.,pic_cs-410_4_7_540.jpg
cs-410_4_7_140,cs-410,4,7, Smoothing Methods - Part 2,"00:09:30,420","00:09:34,500",140,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=570,In particular how do we use query,pic_cs-410_4_7_540.jpg
cs-410_4_7_141,cs-410,4,7, Smoothing Methods - Part 2,"00:09:34,500","00:09:37,650",141,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=574,that would work consistently,pic_cs-410_4_7_540.jpg
cs-410_4_7_142,cs-410,4,7, Smoothing Methods - Part 2,"00:09:37,650","00:09:39,070",142,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=577,Currently we still cannot do that.,pic_cs-410_4_7_540.jpg
cs-410_4_7_143,cs-410,4,7, Smoothing Methods - Part 2,"00:09:40,240","00:09:41,640",143,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=580,Still interesting open question.,pic_cs-410_4_7_540.jpg
cs-410_4_7_144,cs-410,4,7, Smoothing Methods - Part 2,"00:09:43,450","00:09:46,975",144,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=583,"So to summarize this part, we've talked",pic_cs-410_4_7_540.jpg
cs-410_4_7_145,cs-410,4,7, Smoothing Methods - Part 2,"00:09:46,975","00:09:52,550",145,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=586,Jelinek-Mercer which is doing the fixed,pic_cs-410_4_7_540.jpg
cs-410_4_7_146,cs-410,4,7, Smoothing Methods - Part 2,"00:09:52,550","00:09:58,430",146,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=592,Dirichlet Prior this is what add a pseudo,pic_cs-410_4_7_540.jpg
cs-410_4_7_147,cs-410,4,7, Smoothing Methods - Part 2,"00:09:58,430","00:10:04,160",147,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=598,interpolation in that the coefficient,pic_cs-410_4_7_540.jpg
cs-410_4_7_148,cs-410,4,7, Smoothing Methods - Part 2,"00:10:05,940","00:10:10,890",148,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=605,"In most cases we can see, by using these",pic_cs-410_4_7_600.jpg
cs-410_4_7_149,cs-410,4,7, Smoothing Methods - Part 2,"00:10:10,890","00:10:16,670",149,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=610,reach a retrieval function where,pic_cs-410_4_7_600.jpg
cs-410_4_7_150,cs-410,4,7, Smoothing Methods - Part 2,"00:10:16,670","00:10:17,790",150,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=616,So they are less heuristic.,pic_cs-410_4_7_600.jpg
cs-410_4_7_151,cs-410,4,7, Smoothing Methods - Part 2,"00:10:19,090","00:10:23,810",151,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=619,Explaining the results also show,pic_cs-410_4_7_600.jpg
cs-410_4_7_152,cs-410,4,7, Smoothing Methods - Part 2,"00:10:23,810","00:10:31,036",152,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=623,Also are very effective and they are,pic_cs-410_4_7_600.jpg
cs-410_4_7_153,cs-410,4,7, Smoothing Methods - Part 2,"00:10:31,036","00:10:36,260",153,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=631,So this is a major advantage,pic_cs-410_4_7_600.jpg
cs-410_4_7_154,cs-410,4,7, Smoothing Methods - Part 2,"00:10:36,260","00:10:39,480",154,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=636,where we don't have to do,pic_cs-410_4_7_600.jpg
cs-410_4_7_155,cs-410,4,7, Smoothing Methods - Part 2,"00:10:40,770","00:10:44,240",155,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=640,Yet in the end that we naturally,pic_cs-410_4_7_600.jpg
cs-410_4_7_156,cs-410,4,7, Smoothing Methods - Part 2,"00:10:44,240","00:10:45,239",156,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=644,doc length normalization.,pic_cs-410_4_7_600.jpg
cs-410_4_7_157,cs-410,4,7, Smoothing Methods - Part 2,"00:10:46,480","00:10:51,120",157,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=646,Each of these functions also has,pic_cs-410_4_7_600.jpg
cs-410_4_7_158,cs-410,4,7, Smoothing Methods - Part 2,"00:10:51,120","00:10:54,840",158,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=651,In this case of course we still need,pic_cs-410_4_7_600.jpg
cs-410_4_7_159,cs-410,4,7, Smoothing Methods - Part 2,"00:10:54,840","00:10:58,850",159,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=654,There are also methods that can be,pic_cs-410_4_7_600.jpg
cs-410_4_7_160,cs-410,4,7, Smoothing Methods - Part 2,"00:10:59,950","00:11:04,020",160,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=659,"So overall,",pic_cs-410_4_7_600.jpg
cs-410_4_7_161,cs-410,4,7, Smoothing Methods - Part 2,"00:11:04,020","00:11:08,900",161,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=664,we follow very different strategies,pic_cs-410_4_7_660.jpg
cs-410_4_7_162,cs-410,4,7, Smoothing Methods - Part 2,"00:11:08,900","00:11:12,980",162,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=668,"Yet, in the end, we end up uh,with",pic_cs-410_4_7_660.jpg
cs-410_4_7_163,cs-410,4,7, Smoothing Methods - Part 2,"00:11:12,980","00:11:15,540",163,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=672,look very similar to,pic_cs-410_4_7_660.jpg
cs-410_4_7_164,cs-410,4,7, Smoothing Methods - Part 2,"00:11:15,540","00:11:21,160",164,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=675,With some advantages in having,pic_cs-410_4_7_660.jpg
cs-410_4_7_165,cs-410,4,7, Smoothing Methods - Part 2,"00:11:21,160","00:11:24,940",165,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=681,"And then, the form dictated",pic_cs-410_4_7_660.jpg
cs-410_4_7_166,cs-410,4,7, Smoothing Methods - Part 2,"00:11:24,940","00:11:29,740",166,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=684,"Now, this also concludes our discussion of",pic_cs-410_4_7_660.jpg
cs-410_4_7_167,cs-410,4,7, Smoothing Methods - Part 2,"00:11:29,740","00:11:34,680",167,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=689,And let's recall what,pic_cs-410_4_7_660.jpg
cs-410_4_7_168,cs-410,4,7, Smoothing Methods - Part 2,"00:11:34,680","00:11:39,390",168,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=694,in order to derive the functions,pic_cs-410_4_7_660.jpg
cs-410_4_7_169,cs-410,4,7, Smoothing Methods - Part 2,"00:11:39,390","00:11:42,130",169,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=699,Well we basically have made four,pic_cs-410_4_7_660.jpg
cs-410_4_7_170,cs-410,4,7, Smoothing Methods - Part 2,"00:11:42,130","00:11:48,399",170,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=702,The first assumption is that the relevance,pic_cs-410_4_7_660.jpg
cs-410_4_7_171,cs-410,4,7, Smoothing Methods - Part 2,"00:11:49,470","00:11:53,450",171,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=709,"And the second assumption with med is, are",pic_cs-410_4_7_660.jpg
cs-410_4_7_172,cs-410,4,7, Smoothing Methods - Part 2,"00:11:53,450","00:11:57,240",172,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=713,that allows us to decompose,pic_cs-410_4_7_660.jpg
cs-410_4_7_173,cs-410,4,7, Smoothing Methods - Part 2,"00:11:57,240","00:12:01,690",173,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=717,into a product of probabilities,pic_cs-410_4_7_660.jpg
cs-410_4_7_174,cs-410,4,7, Smoothing Methods - Part 2,"00:12:03,090","00:12:07,850",174,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=723,"And then,",pic_cs-410_4_7_720.jpg
cs-410_4_7_175,cs-410,4,7, Smoothing Methods - Part 2,"00:12:07,850","00:12:10,550",175,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=727,"if a word is not seen,",pic_cs-410_4_7_720.jpg
cs-410_4_7_176,cs-410,4,7, Smoothing Methods - Part 2,"00:12:10,550","00:12:14,870",176,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=730,its probability proportional to,pic_cs-410_4_7_720.jpg
cs-410_4_7_177,cs-410,4,7, Smoothing Methods - Part 2,"00:12:14,870","00:12:17,290",177,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=734,That's a smoothing with,pic_cs-410_4_7_720.jpg
cs-410_4_7_178,cs-410,4,7, Smoothing Methods - Part 2,"00:12:17,290","00:12:20,980",178,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=737,"And finally, we made one of these",pic_cs-410_4_7_720.jpg
cs-410_4_7_179,cs-410,4,7, Smoothing Methods - Part 2,"00:12:20,980","00:12:24,940",179,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=740,So we either used JM smoothing or,pic_cs-410_4_7_720.jpg
cs-410_4_7_180,cs-410,4,7, Smoothing Methods - Part 2,"00:12:24,940","00:12:28,820",180,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=744,If we make these four assumptions,pic_cs-410_4_7_720.jpg
cs-410_4_7_181,cs-410,4,7, Smoothing Methods - Part 2,"00:12:28,820","00:12:33,430",181,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=748,to take the form of the retrieval,pic_cs-410_4_7_720.jpg
cs-410_4_7_182,cs-410,4,7, Smoothing Methods - Part 2,"00:12:33,430","00:12:37,730",182,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=753,Fortunately the function has a nice,pic_cs-410_4_7_720.jpg
cs-410_4_7_183,cs-410,4,7, Smoothing Methods - Part 2,"00:12:37,730","00:12:44,510",183,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=757,weighting and document machine and,pic_cs-410_4_7_720.jpg
cs-410_4_7_184,cs-410,4,7, Smoothing Methods - Part 2,"00:12:44,510","00:12:45,440",184,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=764,"So in that sense,",pic_cs-410_4_7_720.jpg
cs-410_4_7_185,cs-410,4,7, Smoothing Methods - Part 2,"00:12:45,440","00:12:48,920",185,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=765,these functions are less heuristic,pic_cs-410_4_7_720.jpg
cs-410_4_7_186,cs-410,4,7, Smoothing Methods - Part 2,"00:12:50,460","00:12:54,282",186,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=770,"And there are many extensions of this,",pic_cs-410_4_7_720.jpg
cs-410_4_7_187,cs-410,4,7, Smoothing Methods - Part 2,"00:12:54,282","00:12:59,336",187,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=774,you can find the discussion of them in,pic_cs-410_4_7_720.jpg
cs-410_4_7_188,cs-410,4,7, Smoothing Methods - Part 2,"00:13:04,921","00:13:14,921",188,https://www.coursera.org/learn/cs-410/lecture/gxNMo?t=784,[MUSIC],pic_cs-410_4_7_780.jpg
cs-410_5_1_1,cs-410,5,1, Feedback in Text Retrieval,"00:00:00,012","00:00:07,436",1,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=0,[SOUND],pic_cs-410_5_1_0.jpg
cs-410_5_1_2,cs-410,5,1, Feedback in Text Retrieval,"00:00:07,436","00:00:10,250",2,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=7,lecture is about the feedback,pic_cs-410_5_1_0.jpg
cs-410_5_1_3,cs-410,5,1, Feedback in Text Retrieval,"00:00:12,910","00:00:17,060",3,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=12,"So in this lecture, we will continue with",pic_cs-410_5_1_0.jpg
cs-410_5_1_4,cs-410,5,1, Feedback in Text Retrieval,"00:00:18,840","00:00:22,103",4,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=18,"In particular, we're going to talk",pic_cs-410_5_1_0.jpg
cs-410_5_1_5,cs-410,5,1, Feedback in Text Retrieval,"00:00:24,866","00:00:28,380",5,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=24,This is a diagram that shows,pic_cs-410_5_1_0.jpg
cs-410_5_1_6,cs-410,5,1, Feedback in Text Retrieval,"00:00:30,685","00:00:34,895",6,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=30,We can see the user would type in a query.,pic_cs-410_5_1_0.jpg
cs-410_5_1_7,cs-410,5,1, Feedback in Text Retrieval,"00:00:37,365","00:00:41,965",7,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=37,"And then, the query would be",pic_cs-410_5_1_0.jpg
cs-410_5_1_8,cs-410,5,1, Feedback in Text Retrieval,"00:00:41,965","00:00:46,015",8,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=41,"search engine, and",pic_cs-410_5_1_0.jpg
cs-410_5_1_9,cs-410,5,1, Feedback in Text Retrieval,"00:00:46,015","00:00:47,865",9,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=46,These results would be issued to the user.,pic_cs-410_5_1_0.jpg
cs-410_5_1_10,cs-410,5,1, Feedback in Text Retrieval,"00:00:49,475","00:00:52,760",10,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=49,"Now, after the user has",pic_cs-410_5_1_0.jpg
cs-410_5_1_11,cs-410,5,1, Feedback in Text Retrieval,"00:00:52,760","00:00:55,410",11,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=52,the user can actually make judgements.,pic_cs-410_5_1_0.jpg
cs-410_5_1_12,cs-410,5,1, Feedback in Text Retrieval,"00:00:55,410","00:00:59,009",12,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=55,"So for example, the user says,",pic_cs-410_5_1_0.jpg
cs-410_5_1_13,cs-410,5,1, Feedback in Text Retrieval,"00:00:59,009","00:01:03,097",13,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=59,this document is not very useful and,pic_cs-410_5_1_0.jpg
cs-410_5_1_14,cs-410,5,1, Feedback in Text Retrieval,"00:01:03,097","00:01:07,921",14,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=63,"Now, this is called a relevance judgment",pic_cs-410_5_1_60.jpg
cs-410_5_1_15,cs-410,5,1, Feedback in Text Retrieval,"00:01:07,921","00:01:12,510",15,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=67,got some feedback information from,pic_cs-410_5_1_60.jpg
cs-410_5_1_16,cs-410,5,1, Feedback in Text Retrieval,"00:01:12,510","00:01:14,930",16,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=72,"And this can be very useful to the system,",pic_cs-410_5_1_60.jpg
cs-410_5_1_17,cs-410,5,1, Feedback in Text Retrieval,"00:01:14,930","00:01:18,320",17,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=74,knowing what exactly is,pic_cs-410_5_1_60.jpg
cs-410_5_1_18,cs-410,5,1, Feedback in Text Retrieval,"00:01:18,320","00:01:22,790",18,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=78,So the feedback module would,pic_cs-410_5_1_60.jpg
cs-410_5_1_19,cs-410,5,1, Feedback in Text Retrieval,"00:01:22,790","00:01:26,970",19,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=82,also use the document collection,pic_cs-410_5_1_60.jpg
cs-410_5_1_20,cs-410,5,1, Feedback in Text Retrieval,"00:01:26,970","00:01:30,720",20,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=86,Typically it would involve,pic_cs-410_5_1_60.jpg
cs-410_5_1_21,cs-410,5,1, Feedback in Text Retrieval,"00:01:30,720","00:01:34,960",21,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=90,the system can now render the results,pic_cs-410_5_1_60.jpg
cs-410_5_1_22,cs-410,5,1, Feedback in Text Retrieval,"00:01:34,960","00:01:36,897",22,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=94,So this is called relevance feedback.,pic_cs-410_5_1_60.jpg
cs-410_5_1_23,cs-410,5,1, Feedback in Text Retrieval,"00:01:36,897","00:01:42,470",23,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=96,The feedback is based on relevance,pic_cs-410_5_1_60.jpg
cs-410_5_1_24,cs-410,5,1, Feedback in Text Retrieval,"00:01:42,470","00:01:44,660",24,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=102,"Now, these judgements are reliable but",pic_cs-410_5_1_60.jpg
cs-410_5_1_25,cs-410,5,1, Feedback in Text Retrieval,"00:01:44,660","00:01:50,350",25,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=104,the users generally don't want to make,pic_cs-410_5_1_60.jpg
cs-410_5_1_26,cs-410,5,1, Feedback in Text Retrieval,"00:01:50,350","00:01:54,980",26,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=110,So the down side is that it involves,pic_cs-410_5_1_60.jpg
cs-410_5_1_27,cs-410,5,1, Feedback in Text Retrieval,"00:01:57,250","00:02:00,920",27,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=117,There's another form of feedback,pic_cs-410_5_1_60.jpg
cs-410_5_1_28,cs-410,5,1, Feedback in Text Retrieval,"00:02:00,920","00:02:03,800",28,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=120,"blind feedback,",pic_cs-410_5_1_120.jpg
cs-410_5_1_29,cs-410,5,1, Feedback in Text Retrieval,"00:02:03,800","00:02:08,380",29,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=123,"In this case, we can see once",pic_cs-410_5_1_120.jpg
cs-410_5_1_30,cs-410,5,1, Feedback in Text Retrieval,"00:02:08,380","00:02:11,340",30,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=128,in fact we don't have to invoke users.,pic_cs-410_5_1_120.jpg
cs-410_5_1_31,cs-410,5,1, Feedback in Text Retrieval,"00:02:11,340","00:02:13,720",31,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=131,So you can see there's,pic_cs-410_5_1_120.jpg
cs-410_5_1_32,cs-410,5,1, Feedback in Text Retrieval,"00:02:14,730","00:02:19,846",32,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=134,And we simply assume that the top,pic_cs-410_5_1_120.jpg
cs-410_5_1_33,cs-410,5,1, Feedback in Text Retrieval,"00:02:19,846","00:02:23,940",33,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=139,Let's say we have assumed,pic_cs-410_5_1_120.jpg
cs-410_5_1_34,cs-410,5,1, Feedback in Text Retrieval,"00:02:25,250","00:02:31,000",34,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=145,"And then, we will then use this",pic_cs-410_5_1_120.jpg
cs-410_5_1_35,cs-410,5,1, Feedback in Text Retrieval,"00:02:31,000","00:02:33,110",35,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=151,and to improve the query.,pic_cs-410_5_1_120.jpg
cs-410_5_1_36,cs-410,5,1, Feedback in Text Retrieval,"00:02:34,110","00:02:35,821",36,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=154,"Now, you might wonder,",pic_cs-410_5_1_120.jpg
cs-410_5_1_37,cs-410,5,1, Feedback in Text Retrieval,"00:02:35,821","00:02:40,887",37,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=155,how could this help if we simply,pic_cs-410_5_1_120.jpg
cs-410_5_1_38,cs-410,5,1, Feedback in Text Retrieval,"00:02:40,887","00:02:46,490",38,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=160,"Well, you can imagine these top",pic_cs-410_5_1_120.jpg
cs-410_5_1_39,cs-410,5,1, Feedback in Text Retrieval,"00:02:46,490","00:02:52,070",39,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=166,similar to relevant documents,pic_cs-410_5_1_120.jpg
cs-410_5_1_40,cs-410,5,1, Feedback in Text Retrieval,"00:02:52,070","00:02:53,480",40,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=172,They look like relevant documents.,pic_cs-410_5_1_120.jpg
cs-410_5_1_41,cs-410,5,1, Feedback in Text Retrieval,"00:02:53,480","00:02:59,350",41,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=173,So it's possible to learn some related,pic_cs-410_5_1_120.jpg
cs-410_5_1_42,cs-410,5,1, Feedback in Text Retrieval,"00:02:59,350","00:03:03,610",42,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=179,"In fact, you may recall that we",pic_cs-410_5_1_120.jpg
cs-410_5_1_43,cs-410,5,1, Feedback in Text Retrieval,"00:03:03,610","00:03:08,180",43,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=183,"analyze what association, to learn",pic_cs-410_5_1_180.jpg
cs-410_5_1_44,cs-410,5,1, Feedback in Text Retrieval,"00:03:09,480","00:03:13,040",44,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=189,"And there, what we did is we",pic_cs-410_5_1_180.jpg
cs-410_5_1_45,cs-410,5,1, Feedback in Text Retrieval,"00:03:13,040","00:03:15,500",45,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=193,all the documents that contain computer.,pic_cs-410_5_1_180.jpg
cs-410_5_1_46,cs-410,5,1, Feedback in Text Retrieval,"00:03:15,500","00:03:18,761",46,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=195,So imagine now the query,pic_cs-410_5_1_180.jpg
cs-410_5_1_47,cs-410,5,1, Feedback in Text Retrieval,"00:03:18,761","00:03:23,870",47,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=198,"And then, the result will be those",pic_cs-410_5_1_180.jpg
cs-410_5_1_48,cs-410,5,1, Feedback in Text Retrieval,"00:03:23,870","00:03:29,040",48,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=203,And what we can do then is,pic_cs-410_5_1_180.jpg
cs-410_5_1_49,cs-410,5,1, Feedback in Text Retrieval,"00:03:29,040","00:03:31,860",49,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=209,They can match computer very well.,pic_cs-410_5_1_180.jpg
cs-410_5_1_50,cs-410,5,1, Feedback in Text Retrieval,"00:03:31,860","00:03:36,890",50,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=211,And we're going to count,pic_cs-410_5_1_180.jpg
cs-410_5_1_51,cs-410,5,1, Feedback in Text Retrieval,"00:03:36,890","00:03:42,126",51,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=216,"And then, we're going to then use",pic_cs-410_5_1_180.jpg
cs-410_5_1_52,cs-410,5,1, Feedback in Text Retrieval,"00:03:42,126","00:03:47,794",52,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=222,the terms that are frequent in this set,pic_cs-410_5_1_180.jpg
cs-410_5_1_53,cs-410,5,1, Feedback in Text Retrieval,"00:03:47,794","00:03:52,364",53,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=227,So if we make a contrast between,pic_cs-410_5_1_180.jpg
cs-410_5_1_54,cs-410,5,1, Feedback in Text Retrieval,"00:03:52,364","00:03:57,360",54,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=232,is that related to terms,pic_cs-410_5_1_180.jpg
cs-410_5_1_55,cs-410,5,1, Feedback in Text Retrieval,"00:03:57,360","00:03:58,528",55,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=237,As we have seen before.,pic_cs-410_5_1_180.jpg
cs-410_5_1_56,cs-410,5,1, Feedback in Text Retrieval,"00:03:58,528","00:04:04,786",56,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=238,And these related words can then be added,pic_cs-410_5_1_180.jpg
cs-410_5_1_57,cs-410,5,1, Feedback in Text Retrieval,"00:04:04,786","00:04:08,770",57,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=244,And this would help us bring the documents,pic_cs-410_5_1_240.jpg
cs-410_5_1_58,cs-410,5,1, Feedback in Text Retrieval,"00:04:08,770","00:04:11,640",58,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=248,match other words like program and,pic_cs-410_5_1_240.jpg
cs-410_5_1_59,cs-410,5,1, Feedback in Text Retrieval,"00:04:11,640","00:04:16,450",59,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=251,So this is very effective for,pic_cs-410_5_1_240.jpg
cs-410_5_1_60,cs-410,5,1, Feedback in Text Retrieval,"00:04:18,590","00:04:21,790",60,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=258,"But of course, pseudo-relevancy",pic_cs-410_5_1_240.jpg
cs-410_5_1_61,cs-410,5,1, Feedback in Text Retrieval,"00:04:21,790","00:04:24,050",61,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=261,We have to arbitrarily set a cut off.,pic_cs-410_5_1_240.jpg
cs-410_5_1_62,cs-410,5,1, Feedback in Text Retrieval,"00:04:24,050","00:04:27,010",62,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=264,So there's also something in,pic_cs-410_5_1_240.jpg
cs-410_5_1_63,cs-410,5,1, Feedback in Text Retrieval,"00:04:27,010","00:04:31,120",63,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=267,"In this case,",pic_cs-410_5_1_240.jpg
cs-410_5_1_64,cs-410,5,1, Feedback in Text Retrieval,"00:04:31,120","00:04:33,510",64,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=271,we don't have to ask,pic_cs-410_5_1_240.jpg
cs-410_5_1_65,cs-410,5,1, Feedback in Text Retrieval,"00:04:33,510","00:04:38,730",65,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=273,"Instead, we're going to observe how the",pic_cs-410_5_1_240.jpg
cs-410_5_1_66,cs-410,5,1, Feedback in Text Retrieval,"00:04:38,730","00:04:41,760",66,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=278,So in this case we'll look,pic_cs-410_5_1_240.jpg
cs-410_5_1_67,cs-410,5,1, Feedback in Text Retrieval,"00:04:41,760","00:04:43,930",67,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=281,So the user clicked on this one.,pic_cs-410_5_1_240.jpg
cs-410_5_1_68,cs-410,5,1, Feedback in Text Retrieval,"00:04:43,930","00:04:45,620",68,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=283,And the user viewed this one.,pic_cs-410_5_1_240.jpg
cs-410_5_1_69,cs-410,5,1, Feedback in Text Retrieval,"00:04:45,620","00:04:47,480",69,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=285,And the user skipped this one.,pic_cs-410_5_1_240.jpg
cs-410_5_1_70,cs-410,5,1, Feedback in Text Retrieval,"00:04:47,480","00:04:49,400",70,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=287,And the user viewed this one again.,pic_cs-410_5_1_240.jpg
cs-410_5_1_71,cs-410,5,1, Feedback in Text Retrieval,"00:04:50,410","00:04:56,880",71,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=290,"Now, this also is a clue about whether",pic_cs-410_5_1_240.jpg
cs-410_5_1_72,cs-410,5,1, Feedback in Text Retrieval,"00:04:56,880","00:05:01,540",72,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=296,And we can even assume that we're,pic_cs-410_5_1_240.jpg
cs-410_5_1_73,cs-410,5,1, Feedback in Text Retrieval,"00:05:01,540","00:05:05,930",73,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=301,"here in this document,",pic_cs-410_5_1_300.jpg
cs-410_5_1_74,cs-410,5,1, Feedback in Text Retrieval,"00:05:05,930","00:05:10,810",74,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=305,instead of the actual,pic_cs-410_5_1_300.jpg
cs-410_5_1_75,cs-410,5,1, Feedback in Text Retrieval,"00:05:10,810","00:05:15,370",75,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=310,The link they are saying web search,pic_cs-410_5_1_300.jpg
cs-410_5_1_76,cs-410,5,1, Feedback in Text Retrieval,"00:05:15,370","00:05:20,250",76,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=315,If the user tries to fetch this,pic_cs-410_5_1_300.jpg
cs-410_5_1_77,cs-410,5,1, Feedback in Text Retrieval,"00:05:20,250","00:05:25,400",77,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=320,we can assume these displayed,pic_cs-410_5_1_300.jpg
cs-410_5_1_78,cs-410,5,1, Feedback in Text Retrieval,"00:05:25,400","00:05:29,310",78,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=325,is interesting to you so,pic_cs-410_5_1_300.jpg
cs-410_5_1_79,cs-410,5,1, Feedback in Text Retrieval,"00:05:29,310","00:05:31,830",79,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=329,And this is called interesting feedback.,pic_cs-410_5_1_300.jpg
cs-410_5_1_80,cs-410,5,1, Feedback in Text Retrieval,"00:05:31,830","00:05:35,400",80,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=331,"And we can, again,",pic_cs-410_5_1_300.jpg
cs-410_5_1_81,cs-410,5,1, Feedback in Text Retrieval,"00:05:35,400","00:05:39,760",81,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=335,This is a very important,pic_cs-410_5_1_300.jpg
cs-410_5_1_82,cs-410,5,1, Feedback in Text Retrieval,"00:05:39,760","00:05:42,080",82,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=339,"Now, think about the Google and Bing and",pic_cs-410_5_1_300.jpg
cs-410_5_1_83,cs-410,5,1, Feedback in Text Retrieval,"00:05:42,080","00:05:46,990",83,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=342,they can collect a lot of user,pic_cs-410_5_1_300.jpg
cs-410_5_1_84,cs-410,5,1, Feedback in Text Retrieval,"00:05:46,990","00:05:51,320",84,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=346,So they would observe what documents,pic_cs-410_5_1_300.jpg
cs-410_5_1_85,cs-410,5,1, Feedback in Text Retrieval,"00:05:51,320","00:05:54,040",85,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=351,And this information is very valuable.,pic_cs-410_5_1_300.jpg
cs-410_5_1_86,cs-410,5,1, Feedback in Text Retrieval,"00:05:54,040","00:05:57,680",86,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=354,And they can use this to,pic_cs-410_5_1_300.jpg
cs-410_5_1_87,cs-410,5,1, Feedback in Text Retrieval,"00:05:59,040","00:06:03,625",87,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=359,"So to summarize, we talked about",pic_cs-410_5_1_300.jpg
cs-410_5_1_88,cs-410,5,1, Feedback in Text Retrieval,"00:06:03,625","00:06:07,280",88,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=363,Relevant feedback where the user,pic_cs-410_5_1_360.jpg
cs-410_5_1_89,cs-410,5,1, Feedback in Text Retrieval,"00:06:07,280","00:06:11,200",89,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=367,"It takes some user effort, but",pic_cs-410_5_1_360.jpg
cs-410_5_1_90,cs-410,5,1, Feedback in Text Retrieval,"00:06:11,200","00:06:15,931",90,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=371,We talk about the pseudo feedback where,pic_cs-410_5_1_360.jpg
cs-410_5_1_91,cs-410,5,1, Feedback in Text Retrieval,"00:06:15,931","00:06:17,310",91,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=375,will be relevant.,pic_cs-410_5_1_360.jpg
cs-410_5_1_92,cs-410,5,1, Feedback in Text Retrieval,"00:06:17,310","00:06:20,540",92,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=377,We don't have to involve the user,pic_cs-410_5_1_360.jpg
cs-410_5_1_93,cs-410,5,1, Feedback in Text Retrieval,"00:06:20,540","00:06:23,590",93,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=380,actually before we return,pic_cs-410_5_1_360.jpg
cs-410_5_1_94,cs-410,5,1, Feedback in Text Retrieval,"00:06:24,850","00:06:28,014",94,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=384,And the third is implicit feedback,pic_cs-410_5_1_360.jpg
cs-410_5_1_95,cs-410,5,1, Feedback in Text Retrieval,"00:06:29,685","00:06:31,530",95,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=389,"Where we involve the users, but",pic_cs-410_5_1_360.jpg
cs-410_5_1_96,cs-410,5,1, Feedback in Text Retrieval,"00:06:31,530","00:06:34,887",96,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=391,the user doesn't have to make,pic_cs-410_5_1_360.jpg
cs-410_5_1_97,cs-410,5,1, Feedback in Text Retrieval,"00:06:34,887","00:06:36,118",97,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=394,Make judgement.,pic_cs-410_5_1_360.jpg
cs-410_5_1_98,cs-410,5,1, Feedback in Text Retrieval,"00:06:36,118","00:06:46,118",98,https://www.coursera.org/learn/cs-410/lecture/gw3fo?t=396,[MUSIC],pic_cs-410_5_1_360.jpg
cs-410_5_2_1,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:00,012","00:00:07,558",1,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=0,[SOUND],pic_cs-410_5_2_0.jpg
cs-410_5_2_2,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:07,558","00:00:10,370",2,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=7,lecture is about the feedback,pic_cs-410_5_2_0.jpg
cs-410_5_2_3,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:12,910","00:00:18,040",3,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=12,"In this lecture, we continue talking",pic_cs-410_5_2_0.jpg
cs-410_5_2_4,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:18,040","00:00:21,210",4,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=18,"Particularly, we're going to talk about",pic_cs-410_5_2_0.jpg
cs-410_5_2_5,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:23,930","00:00:29,210",5,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=23,"As we have discussed before,",pic_cs-410_5_2_0.jpg
cs-410_5_2_6,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:29,210","00:00:34,890",6,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=29,of text retrieval system is removed from,pic_cs-410_5_2_0.jpg
cs-410_5_2_7,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:34,890","00:00:37,467",7,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=34,We will have positive examples.,pic_cs-410_5_2_0.jpg
cs-410_5_2_8,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:37,467","00:00:40,669",8,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=37,Those are the documents that,pic_cs-410_5_2_0.jpg
cs-410_5_2_9,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:40,669","00:00:42,610",9,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=40,be charged with being relevant.,pic_cs-410_5_2_0.jpg
cs-410_5_2_10,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:42,610","00:00:45,160",10,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=42,All the documents that,pic_cs-410_5_2_0.jpg
cs-410_5_2_11,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:45,160","00:00:46,910",11,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=45,We also have negative examples.,pic_cs-410_5_2_0.jpg
cs-410_5_2_12,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:46,910","00:00:49,590",12,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=46,Those are documents known,pic_cs-410_5_2_0.jpg
cs-410_5_2_13,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:49,590","00:00:52,960",13,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=49,They can also be the documents,pic_cs-410_5_2_0.jpg
cs-410_5_2_14,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:55,350","00:00:58,570",14,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=55,The general method in,pic_cs-410_5_2_0.jpg
cs-410_5_2_15,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:00:58,570","00:01:02,690",15,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=58,feedback is to modify our query vector.,pic_cs-410_5_2_0.jpg
cs-410_5_2_16,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:04,010","00:01:08,500",16,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=64,We want to place the query vector in,pic_cs-410_5_2_60.jpg
cs-410_5_2_17,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:10,120","00:01:11,520",17,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=70,And what does that mean exactly?,pic_cs-410_5_2_60.jpg
cs-410_5_2_18,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:11,520","00:01:14,930",18,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=71,"Well, if we think about the query vector",pic_cs-410_5_2_60.jpg
cs-410_5_2_19,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:14,930","00:01:17,270",19,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=74,something to the vector elements.,pic_cs-410_5_2_60.jpg
cs-410_5_2_20,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:17,270","00:01:21,240",20,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=77,"And in general,",pic_cs-410_5_2_60.jpg
cs-410_5_2_21,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:21,240","00:01:27,129",21,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=81,Or we might just weight of old terms or,pic_cs-410_5_2_60.jpg
cs-410_5_2_22,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:29,230","00:01:32,780",22,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=89,"As a result, in general,",pic_cs-410_5_2_60.jpg
cs-410_5_2_23,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:32,780","00:01:35,110",23,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=92,We often call this query expansion.,pic_cs-410_5_2_60.jpg
cs-410_5_2_24,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:37,960","00:01:40,920",24,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=97,The most effective method in,pic_cs-410_5_2_60.jpg
cs-410_5_2_25,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:40,920","00:01:44,900",25,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=100,"is called the Rocchio Feedback, which was",pic_cs-410_5_2_60.jpg
cs-410_5_2_26,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:47,490","00:01:49,110",26,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=107,So the idea is quite simple.,pic_cs-410_5_2_60.jpg
cs-410_5_2_27,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:49,110","00:01:53,402",27,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=109,We illustrate this idea by,pic_cs-410_5_2_60.jpg
cs-410_5_2_28,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:53,402","00:01:58,231",28,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=113,of all the documents in the collection and,pic_cs-410_5_2_60.jpg
cs-410_5_2_29,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:01:58,231","00:02:03,935",29,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=118,So now we can see the query,pic_cs-410_5_2_60.jpg
cs-410_5_2_30,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:03,935","00:02:07,428",30,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=123,and these are all the documents.,pic_cs-410_5_2_120.jpg
cs-410_5_2_31,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:07,428","00:02:11,230",31,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=127,So when we use the query back there and,pic_cs-410_5_2_120.jpg
cs-410_5_2_32,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:11,230","00:02:14,780",32,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=131,"the most similar documents,",pic_cs-410_5_2_120.jpg
cs-410_5_2_33,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:14,780","00:02:18,960",33,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=134,that these documents would be,pic_cs-410_5_2_120.jpg
cs-410_5_2_34,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:18,960","00:02:22,512",34,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=138,"And these process are relevant documents,",pic_cs-410_5_2_120.jpg
cs-410_5_2_35,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:22,512","00:02:27,762",35,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=142,"these are relevant documents,",pic_cs-410_5_2_120.jpg
cs-410_5_2_36,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:27,762","00:02:32,360",36,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=147,And then these minuses are negative,pic_cs-410_5_2_120.jpg
cs-410_5_2_37,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:34,310","00:02:40,150",37,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=154,So our goal here is trying to move,pic_cs-410_5_2_120.jpg
cs-410_5_2_38,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:40,150","00:02:42,780",38,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=160,to improve the retrieval accuracy.,pic_cs-410_5_2_120.jpg
cs-410_5_2_39,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:42,780","00:02:48,390",39,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=162,"By looking at this diagram,",pic_cs-410_5_2_120.jpg
cs-410_5_2_40,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:48,390","00:02:50,650",40,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=168,Where should we move the query vector so,pic_cs-410_5_2_120.jpg
cs-410_5_2_41,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:50,650","00:02:53,930",41,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=170,that we can improve,pic_cs-410_5_2_120.jpg
cs-410_5_2_42,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:53,930","00:02:56,990",42,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=173,"Intuitively, where do you",pic_cs-410_5_2_120.jpg
cs-410_5_2_43,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:02:58,050","00:03:01,330",43,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=178,"If you want to think more,",pic_cs-410_5_2_120.jpg
cs-410_5_2_44,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:02,980","00:03:10,090",44,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=182,"If you think about this picture, you can",pic_cs-410_5_2_180.jpg
cs-410_5_2_45,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:10,090","00:03:15,520",45,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=190,case you want the query vector to be as,pic_cs-410_5_2_180.jpg
cs-410_5_2_46,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:15,520","00:03:20,462",46,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=195,"That means ideally, you want to place",pic_cs-410_5_2_180.jpg
cs-410_5_2_47,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:20,462","00:03:24,640",47,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=200,Or we want to move the query,pic_cs-410_5_2_180.jpg
cs-410_5_2_48,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:26,510","00:03:29,100",48,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=206,Now so what exactly is this point?,pic_cs-410_5_2_180.jpg
cs-410_5_2_49,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:29,100","00:03:35,710",49,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=209,"Well, if you want these relevant",pic_cs-410_5_2_180.jpg
cs-410_5_2_50,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:35,710","00:03:41,340",50,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=215,you want this to be in the center of,pic_cs-410_5_2_180.jpg
cs-410_5_2_51,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:41,340","00:03:44,710",51,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=221,Because then if you draw,pic_cs-410_5_2_180.jpg
cs-410_5_2_52,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:44,710","00:03:47,240",52,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=224,you'll get all these relevant documents.,pic_cs-410_5_2_180.jpg
cs-410_5_2_53,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:47,240","00:03:52,250",53,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=227,So that means we can move the query,pic_cs-410_5_2_180.jpg
cs-410_5_2_54,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:52,250","00:03:54,510",54,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=232,all the relevant document vectors.,pic_cs-410_5_2_180.jpg
cs-410_5_2_55,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:55,680","00:03:59,106",55,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=235,And this is basically the idea of Rocchio.,pic_cs-410_5_2_180.jpg
cs-410_5_2_56,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:03:59,106","00:04:03,645",56,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=239,"Of course, you can consider",pic_cs-410_5_2_180.jpg
cs-410_5_2_57,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:03,645","00:04:07,040",57,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=243,we want to move away from,pic_cs-410_5_2_240.jpg
cs-410_5_2_58,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:07,040","00:04:11,971",58,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=247,Now your match that we're talking about,pic_cs-410_5_2_240.jpg
cs-410_5_2_59,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:11,971","00:04:14,202",59,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=251,away from other vectors.,pic_cs-410_5_2_240.jpg
cs-410_5_2_60,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:14,202","00:04:18,340",60,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=254,It just means that we have this formula.,pic_cs-410_5_2_240.jpg
cs-410_5_2_61,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:18,340","00:04:22,891",61,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=258,Here you can see this is,pic_cs-410_5_2_240.jpg
cs-410_5_2_62,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:22,891","00:04:29,680",62,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=262,this average basically is the centroid,pic_cs-410_5_2_240.jpg
cs-410_5_2_63,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:29,680","00:04:32,250",63,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=269,"When we take the average of these vectors,",pic_cs-410_5_2_240.jpg
cs-410_5_2_64,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:32,250","00:04:35,580",64,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=272,then were computing,pic_cs-410_5_2_240.jpg
cs-410_5_2_65,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:35,580","00:04:41,070",65,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=275,"Similarly, this is the average of",pic_cs-410_5_2_240.jpg
cs-410_5_2_66,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:41,070","00:04:46,080",66,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=281,So it's essentially of,pic_cs-410_5_2_240.jpg
cs-410_5_2_67,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:46,080","00:04:51,710",67,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=286,"And we have these three parameters here,",pic_cs-410_5_2_240.jpg
cs-410_5_2_68,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:51,710","00:04:55,200",68,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=291,They are controlling,pic_cs-410_5_2_240.jpg
cs-410_5_2_69,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:55,200","00:04:57,560",69,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=295,"When we add these two vectors together,",pic_cs-410_5_2_240.jpg
cs-410_5_2_70,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:04:57,560","00:05:02,290",70,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=297,we're moving the query vector,pic_cs-410_5_2_240.jpg
cs-410_5_2_71,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:03,620","00:05:05,740",71,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=303,This is when we add them together.,pic_cs-410_5_2_300.jpg
cs-410_5_2_72,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:05,740","00:05:08,350",72,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=305,"When we subtracted this part,",pic_cs-410_5_2_300.jpg
cs-410_5_2_73,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:08,350","00:05:14,660",73,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=308,we kind of move the query,pic_cs-410_5_2_300.jpg
cs-410_5_2_74,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:14,660","00:05:18,420",74,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=314,So this is the main idea,pic_cs-410_5_2_300.jpg
cs-410_5_2_75,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:18,420","00:05:20,720",75,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=318,"And after we have done this,",pic_cs-410_5_2_300.jpg
cs-410_5_2_76,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:20,720","00:05:25,710",76,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=320,we will get a new query vector which,pic_cs-410_5_2_300.jpg
cs-410_5_2_77,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:25,710","00:05:31,905",77,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=325,"This new query vector,",pic_cs-410_5_2_300.jpg
cs-410_5_2_78,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:31,905","00:05:38,878",78,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=331,original query vector toward this,pic_cs-410_5_2_300.jpg
cs-410_5_2_79,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:38,878","00:05:42,900",79,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=338,away from the non-relevant value.,pic_cs-410_5_2_300.jpg
cs-410_5_2_80,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:45,110","00:05:48,200",80,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=345,"Okay, so let's take a look at the example.",pic_cs-410_5_2_300.jpg
cs-410_5_2_81,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:48,200","00:05:51,360",81,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=348,This is the example that,pic_cs-410_5_2_300.jpg
cs-410_5_2_82,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:51,360","00:05:55,600",82,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=351,Only that I deemed that display,pic_cs-410_5_2_300.jpg
cs-410_5_2_83,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:55,600","00:05:59,210",83,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=355,I only showed the vector,pic_cs-410_5_2_300.jpg
cs-410_5_2_84,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:05:59,210","00:06:03,240",84,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=359,We have five documents here and we have,pic_cs-410_5_2_300.jpg
cs-410_5_2_85,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:04,760","00:06:09,667",85,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=364,"to read in the documents here, right.",pic_cs-410_5_2_360.jpg
cs-410_5_2_86,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:09,667","00:06:12,650",86,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=369,And they're displayed in red.,pic_cs-410_5_2_360.jpg
cs-410_5_2_87,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:12,650","00:06:14,760",87,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=372,And these are the term vectors.,pic_cs-410_5_2_360.jpg
cs-410_5_2_88,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:14,760","00:06:18,190",88,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=374,Now I have just assumed some of weights.,pic_cs-410_5_2_360.jpg
cs-410_5_2_89,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:18,190","00:06:20,549",89,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=378,"A lot of terms,",pic_cs-410_5_2_360.jpg
cs-410_5_2_90,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:20,549","00:06:22,745",90,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=380,Now these are negative arguments.,pic_cs-410_5_2_360.jpg
cs-410_5_2_91,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:22,745","00:06:23,952",91,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=382,There are two here.,pic_cs-410_5_2_360.jpg
cs-410_5_2_92,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:23,952","00:06:26,120",92,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=383,There is another one here.,pic_cs-410_5_2_360.jpg
cs-410_5_2_93,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:26,120","00:06:32,520",93,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=386,"Now in this Rocchio method, we first",pic_cs-410_5_2_360.jpg
cs-410_5_2_94,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:32,520","00:06:37,540",94,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=392,"And so let's see,",pic_cs-410_5_2_360.jpg
cs-410_5_2_95,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:37,540","00:06:42,910",95,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=397,"the positive documents, we simply just,",pic_cs-410_5_2_360.jpg
cs-410_5_2_96,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:42,910","00:06:48,490",96,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=402,We just add this with this one,pic_cs-410_5_2_360.jpg
cs-410_5_2_97,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:48,490","00:06:51,560",97,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=408,And then that's down here and,pic_cs-410_5_2_360.jpg
cs-410_5_2_98,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:51,560","00:06:54,801",98,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=411,And then we're going to add,pic_cs-410_5_2_360.jpg
cs-410_5_2_99,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:54,801","00:06:56,580",99,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=414,then just take the average.,pic_cs-410_5_2_360.jpg
cs-410_5_2_100,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:56,580","00:06:58,790",100,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=416,And so we do this for all this.,pic_cs-410_5_2_360.jpg
cs-410_5_2_101,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:06:58,790","00:07:02,520",101,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=418,"In the end, what we have is this one.",pic_cs-410_5_2_360.jpg
cs-410_5_2_102,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:02,520","00:07:08,380",102,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=422,"This is the average vector of these two,",pic_cs-410_5_2_420.jpg
cs-410_5_2_103,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:10,030","00:07:13,770",103,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=430,Let's also look at the centroid,pic_cs-410_5_2_420.jpg
cs-410_5_2_104,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:13,770","00:07:15,052",104,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=433,This is basically the same.,pic_cs-410_5_2_420.jpg
cs-410_5_2_105,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:15,052","00:07:18,150",105,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=435,We're going to take the average,pic_cs-410_5_2_420.jpg
cs-410_5_2_106,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:18,150","00:07:22,420",106,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=438,And these are the corresponding,pic_cs-410_5_2_420.jpg
cs-410_5_2_107,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:22,420","00:07:23,020",107,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=442,on and so forth.,pic_cs-410_5_2_420.jpg
cs-410_5_2_108,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:23,020","00:07:25,120",108,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=443,"So in the end, we have this one.",pic_cs-410_5_2_420.jpg
cs-410_5_2_109,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:26,230","00:07:29,340",109,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=446,Now in the Rocchio feedback,pic_cs-410_5_2_420.jpg
cs-410_5_2_110,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:29,340","00:07:32,920",110,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=449,these with the original,pic_cs-410_5_2_420.jpg
cs-410_5_2_111,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:32,920","00:07:36,083",111,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=452,So now let's see how we,pic_cs-410_5_2_420.jpg
cs-410_5_2_112,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:36,083","00:07:37,420",112,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=456,"Well, that's basically this.",pic_cs-410_5_2_420.jpg
cs-410_5_2_113,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:38,830","00:07:42,385",113,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=458,So we have a parameter alpha,pic_cs-410_5_2_420.jpg
cs-410_5_2_114,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:42,385","00:07:45,210",114,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=462,query times weight that's one.,pic_cs-410_5_2_420.jpg
cs-410_5_2_115,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:45,210","00:07:49,626",115,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=465,And now we have beta to control,pic_cs-410_5_2_420.jpg
cs-410_5_2_116,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:49,626","00:07:52,820",116,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=469,"centroid of the weight, that's 1.5.",pic_cs-410_5_2_420.jpg
cs-410_5_2_117,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:52,820","00:07:54,285",117,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=472,That comes from here.,pic_cs-410_5_2_420.jpg
cs-410_5_2_118,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:07:54,285","00:08:00,400",118,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=474,"All right, so this goes here.",pic_cs-410_5_2_420.jpg
cs-410_5_2_119,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:00,400","00:08:07,555",119,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=480,And we also have this negative,pic_cs-410_5_2_480.jpg
cs-410_5_2_120,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:07,555","00:08:14,520",120,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=487,"And this way, it has come from,",pic_cs-410_5_2_480.jpg
cs-410_5_2_121,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:14,520","00:08:19,051",121,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=494,And we do exactly the same for,pic_cs-410_5_2_480.jpg
cs-410_5_2_122,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:22,244","00:08:23,840",122,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=502,And this is our new vector.,pic_cs-410_5_2_480.jpg
cs-410_5_2_123,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:25,700","00:08:31,530",123,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=505,And we're going to use this new query,pic_cs-410_5_2_480.jpg
cs-410_5_2_124,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:31,530","00:08:33,840",124,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=511,"You can imagine what would happen, right?",pic_cs-410_5_2_480.jpg
cs-410_5_2_125,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:33,840","00:08:38,000",125,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=513,Because of the movement that this one,pic_cs-410_5_2_480.jpg
cs-410_5_2_126,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:38,000","00:08:42,520",126,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=518,better because we moved,pic_cs-410_5_2_480.jpg
cs-410_5_2_127,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:42,520","00:08:47,290",127,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=522,And it's going to penalize these black,pic_cs-410_5_2_480.jpg
cs-410_5_2_128,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:47,290","00:08:49,790",128,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=527,So this is precisely what,pic_cs-410_5_2_480.jpg
cs-410_5_2_129,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:50,820","00:08:57,220",129,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=530,Now of course if we apply this method in,pic_cs-410_5_2_480.jpg
cs-410_5_2_130,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:08:58,240","00:09:04,290",130,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=538,and that is the original query has,pic_cs-410_5_2_480.jpg
cs-410_5_2_131,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:06,410","00:09:08,480",131,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=546,But after we do query explaining and,pic_cs-410_5_2_540.jpg
cs-410_5_2_132,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:08,480","00:09:13,210",132,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=548,"merging, we'll have many times",pic_cs-410_5_2_540.jpg
cs-410_5_2_133,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:13,210","00:09:16,580",133,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=553,So the calculation will,pic_cs-410_5_2_540.jpg
cs-410_5_2_134,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:18,090","00:09:22,160",134,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=558,"In practice,",pic_cs-410_5_2_540.jpg
cs-410_5_2_135,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:22,160","00:09:25,470",135,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=562,only retain the terms,pic_cs-410_5_2_540.jpg
cs-410_5_2_136,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:27,000","00:09:29,440",136,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=567,So let's talk about how we,pic_cs-410_5_2_540.jpg
cs-410_5_2_137,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:30,660","00:09:34,220",137,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=570,I just mentioned that they're,pic_cs-410_5_2_540.jpg
cs-410_5_2_138,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:34,220","00:09:37,400",138,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=574,Consider only a small number of,pic_cs-410_5_2_540.jpg
cs-410_5_2_139,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:37,400","00:09:38,690",139,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=577,the centroid vector.,pic_cs-410_5_2_540.jpg
cs-410_5_2_140,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:38,690","00:09:39,900",140,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=578,This is for efficiency concern.,pic_cs-410_5_2_540.jpg
cs-410_5_2_141,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:41,390","00:09:45,580",141,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=581,"I also said here that negative examples,",pic_cs-410_5_2_540.jpg
cs-410_5_2_142,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:45,580","00:09:49,430",142,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=585,"tend not to be very useful, especially",pic_cs-410_5_2_540.jpg
cs-410_5_2_143,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:50,860","00:09:52,500",143,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=590,Now you can think about why.,pic_cs-410_5_2_540.jpg
cs-410_5_2_144,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:55,320","00:09:59,771",144,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=595,One reason is because negative documents,pic_cs-410_5_2_540.jpg
cs-410_5_2_145,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:09:59,771","00:10:00,645",145,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=599,directions.,pic_cs-410_5_2_540.jpg
cs-410_5_2_146,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:00,645","00:10:02,391",146,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=600,"So, when you take the average,",pic_cs-410_5_2_600.jpg
cs-410_5_2_147,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:02,391","00:10:06,860",147,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=602,it doesn't really tell you where,pic_cs-410_5_2_600.jpg
cs-410_5_2_148,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:06,860","00:10:10,110",148,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=606,Whereas positive documents,pic_cs-410_5_2_600.jpg
cs-410_5_2_149,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:10,110","00:10:14,569",149,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=610,And they will point you to,pic_cs-410_5_2_600.jpg
cs-410_5_2_150,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:14,569","00:10:19,090",150,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=614,So that also means that sometimes we don't,pic_cs-410_5_2_600.jpg
cs-410_5_2_151,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:19,090","00:10:24,580",151,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=619,"But note that in some cases, in difficult",pic_cs-410_5_2_600.jpg
cs-410_5_2_152,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:24,580","00:10:26,390",152,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=624,negative feedback after is very useful.,pic_cs-410_5_2_600.jpg
cs-410_5_2_153,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:27,550","00:10:29,370",153,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=627,Another thing is to avoid over-fitting.,pic_cs-410_5_2_600.jpg
cs-410_5_2_154,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:29,370","00:10:34,425",154,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=629,That means we have to keep relatively,pic_cs-410_5_2_600.jpg
cs-410_5_2_155,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:34,425","00:10:35,724",155,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=634,Why?,pic_cs-410_5_2_600.jpg
cs-410_5_2_156,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:35,724","00:10:42,250",156,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=635,Because the sample that we see in,pic_cs-410_5_2_600.jpg
cs-410_5_2_157,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:42,250","00:10:45,580",157,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=642,We don't want to overly,pic_cs-410_5_2_600.jpg
cs-410_5_2_158,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:45,580","00:10:49,390",158,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=645,And the original query terms,pic_cs-410_5_2_600.jpg
cs-410_5_2_159,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:49,390","00:10:51,753",159,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=649,Those terms are heightened by the user and,pic_cs-410_5_2_600.jpg
cs-410_5_2_160,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:51,753","00:10:55,850",160,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=651,the user has decided that those,pic_cs-410_5_2_600.jpg
cs-410_5_2_161,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:10:55,850","00:11:02,530",161,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=655,So in order to prevent,pic_cs-410_5_2_600.jpg
cs-410_5_2_162,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:02,530","00:11:08,910",162,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=662,"drifting, prevent topic drifting due to",pic_cs-410_5_2_660.jpg
cs-410_5_2_163,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:08,910","00:11:12,740",163,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=668,We generally would have to keep a pretty,pic_cs-410_5_2_660.jpg
cs-410_5_2_164,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:12,740","00:11:13,980",164,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=672,it was safe to do that.,pic_cs-410_5_2_660.jpg
cs-410_5_2_165,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:15,040","00:11:18,910",165,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=675,And this is especially true for,pic_cs-410_5_2_660.jpg
cs-410_5_2_166,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:18,910","00:11:20,910",166,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=678,"Now, this method can be used for",pic_cs-410_5_2_660.jpg
cs-410_5_2_167,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:20,910","00:11:23,790",167,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=680,both relevance feedback and,pic_cs-410_5_2_660.jpg
cs-410_5_2_168,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:23,790","00:11:28,780",168,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=683,"In the case of pseudo-feedback, the prime",pic_cs-410_5_2_660.jpg
cs-410_5_2_169,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:28,780","00:11:32,930",169,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=688,value because the relevant examples,pic_cs-410_5_2_660.jpg
cs-410_5_2_170,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:32,930","00:11:36,780",170,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=692,They're not as reliable as,pic_cs-410_5_2_660.jpg
cs-410_5_2_171,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:36,780","00:11:40,830",171,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=696,"In the case of relevance feedback,",pic_cs-410_5_2_660.jpg
cs-410_5_2_172,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:40,830","00:11:43,580",172,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=700,"So those parameters,",pic_cs-410_5_2_660.jpg
cs-410_5_2_173,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:45,020","00:11:48,550",173,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=705,And the Rocchio Method is,pic_cs-410_5_2_660.jpg
cs-410_5_2_174,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:48,550","00:11:51,961",174,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=708,It's still a very popular method for,pic_cs-410_5_2_660.jpg
cs-410_5_2_175,cs-410,5,2, Feedback in Vector Space Model - Rocchio,"00:11:51,961","00:12:01,961",175,https://www.coursera.org/learn/cs-410/lecture/PyTkW?t=711,[MUSIC],pic_cs-410_5_2_660.jpg
cs-410_5_3_1,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:00,000","00:00:07,194",1,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=0,[SOUND],pic_cs-410_5_3_0.jpg
cs-410_5_3_2,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:07,194","00:00:10,660",2,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=7,lecture is about the feedback in,pic_cs-410_5_3_0.jpg
cs-410_5_3_3,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:12,540","00:00:17,520",3,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=12,"In this lecture, we will continue the",pic_cs-410_5_3_0.jpg
cs-410_5_3_4,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:17,520","00:00:18,089",4,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=17,"In particular,",pic_cs-410_5_3_0.jpg
cs-410_5_3_5,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:18,089","00:00:20,659",5,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=18,we're going to talk about the feedback,pic_cs-410_5_3_0.jpg
cs-410_5_3_6,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:23,450","00:00:29,280",6,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=23,So we derive the query likelihood ranking,pic_cs-410_5_3_0.jpg
cs-410_5_3_7,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:30,410","00:00:35,860",7,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=30,"As a basic retrieval function,",pic_cs-410_5_3_0.jpg
cs-410_5_3_8,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:35,860","00:00:39,920",8,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=35,But if we think about the feedback,pic_cs-410_5_3_0.jpg
cs-410_5_3_9,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:39,920","00:00:44,730",9,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=39,"use query likelihood to perform feedback,",pic_cs-410_5_3_0.jpg
cs-410_5_3_10,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:44,730","00:00:49,620",10,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=44,a lot of times the feedback information is,pic_cs-410_5_3_0.jpg
cs-410_5_3_11,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:49,620","00:00:53,260",11,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=49,But we assume the query has,pic_cs-410_5_3_0.jpg
cs-410_5_3_12,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:53,260","00:00:56,850",12,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=53,from a language model in,pic_cs-410_5_3_0.jpg
cs-410_5_3_13,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:00:56,850","00:01:03,170",13,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=56,It's kind of unnatural to sample,pic_cs-410_5_3_0.jpg
cs-410_5_3_14,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:03,170","00:01:10,330",14,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=63,"As a result, researchers proposed a way",pic_cs-410_5_3_60.jpg
cs-410_5_3_15,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:10,330","00:01:14,070",15,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=70,and it's called Kullback-Leibler,pic_cs-410_5_3_60.jpg
cs-410_5_3_16,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:15,450","00:01:20,422",16,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=75,And this model is actually going,pic_cs-410_5_3_60.jpg
cs-410_5_3_17,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:20,422","00:01:25,780",17,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=80,retrieval function much,pic_cs-410_5_3_60.jpg
cs-410_5_3_18,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:25,780","00:01:32,380",18,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=85,Yet this form of the language model,pic_cs-410_5_3_60.jpg
cs-410_5_3_19,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:32,380","00:01:36,560",19,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=92,"query likelihood, in the sense that it can",pic_cs-410_5_3_60.jpg
cs-410_5_3_20,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:38,180","00:01:39,300",20,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=98,"And in this case,",pic_cs-410_5_3_60.jpg
cs-410_5_3_21,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:39,300","00:01:44,140",21,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=99,then feedback can be achieved through,pic_cs-410_5_3_60.jpg
cs-410_5_3_22,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:44,140","00:01:48,130",22,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=104,"This is very similar to Rocchio,",pic_cs-410_5_3_60.jpg
cs-410_5_3_23,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:50,000","00:01:55,720",23,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=110,So let's see what is this,pic_cs-410_5_3_60.jpg
cs-410_5_3_24,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:01:55,720","00:02:02,306",24,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=115,"So on the top, what you see is a query",pic_cs-410_5_3_60.jpg
cs-410_5_3_25,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:05,072","00:02:11,465",25,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=125,"And then KL-divergence, or",pic_cs-410_5_3_120.jpg
cs-410_5_3_26,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:11,465","00:02:16,292",26,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=131,retrieval model is basically to generalize,pic_cs-410_5_3_120.jpg
cs-410_5_3_27,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:16,292","00:02:21,600",27,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=136,the frequency part here,pic_cs-410_5_3_120.jpg
cs-410_5_3_28,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:21,600","00:02:26,910",28,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=141,So basically it's the difference given,pic_cs-410_5_3_120.jpg
cs-410_5_3_29,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:26,910","00:02:32,260",29,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=146,by the probabilistic model here to,pic_cs-410_5_3_120.jpg
cs-410_5_3_30,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:32,260","00:02:34,640",30,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=152,versus the count of query words there.,pic_cs-410_5_3_120.jpg
cs-410_5_3_31,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:35,820","00:02:42,610",31,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=155,And this difference allows us to plug in,pic_cs-410_5_3_120.jpg
cs-410_5_3_32,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:42,610","00:02:45,690",32,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=162,So this can be estimated,pic_cs-410_5_3_120.jpg
cs-410_5_3_33,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:45,690","00:02:48,260",33,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=165,including using feedback information.,pic_cs-410_5_3_120.jpg
cs-410_5_3_34,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:48,260","00:02:51,370",34,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=168,"But this is called a KL-divergence,",pic_cs-410_5_3_120.jpg
cs-410_5_3_35,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:51,370","00:02:56,232",35,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=171,this can be interpreted as matching,pic_cs-410_5_3_120.jpg
cs-410_5_3_36,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:02:56,232","00:03:02,770",36,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=176,"One is the query model,",pic_cs-410_5_3_120.jpg
cs-410_5_3_37,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:02,770","00:03:06,317",37,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=182,One is the document,pic_cs-410_5_3_180.jpg
cs-410_5_3_38,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:06,317","00:03:11,255",38,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=186,smooth them with a collection,pic_cs-410_5_3_180.jpg
cs-410_5_3_39,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:11,255","00:03:15,377",39,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=191,And we are not going to talk,pic_cs-410_5_3_180.jpg
cs-410_5_3_40,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:15,377","00:03:18,107",40,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=195,you'll find it in some references.,pic_cs-410_5_3_180.jpg
cs-410_5_3_41,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:18,107","00:03:22,023",41,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=198,"It's also called cross entropy because,",pic_cs-410_5_3_180.jpg
cs-410_5_3_42,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:22,023","00:03:26,207",42,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=202,we ignore some terms in,pic_cs-410_5_3_180.jpg
cs-410_5_3_43,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:26,207","00:03:29,690",43,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=206,we will end up having,pic_cs-410_5_3_180.jpg
cs-410_5_3_44,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:29,690","00:03:32,109",44,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=209,And both are terms of information theory.,pic_cs-410_5_3_180.jpg
cs-410_5_3_45,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:34,390","00:03:38,650",45,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=214,"But anyway, for our purposes here,",pic_cs-410_5_3_180.jpg
cs-410_5_3_46,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:38,650","00:03:42,820",46,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=218,you can just see the two,pic_cs-410_5_3_180.jpg
cs-410_5_3_47,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:42,820","00:03:48,330",47,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=222,except that here we have a probability of,pic_cs-410_5_3_180.jpg
cs-410_5_3_48,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:52,140","00:03:57,730",48,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=232,And here the sum is over all the words,pic_cs-410_5_3_180.jpg
cs-410_5_3_49,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:03:57,730","00:04:02,340",49,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=237,also with the nonzero probability for,pic_cs-410_5_3_180.jpg
cs-410_5_3_50,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:02,340","00:04:07,510",50,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=242,"So it's kind of, again, a generalization",pic_cs-410_5_3_240.jpg
cs-410_5_3_51,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:09,930","00:04:15,980",51,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=249,Now you can also easily see we can recover,pic_cs-410_5_3_240.jpg
cs-410_5_3_52,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:15,980","00:04:22,130",52,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=255,by simply setting this query model to the,pic_cs-410_5_3_240.jpg
cs-410_5_3_53,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:23,450","00:04:26,510",53,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=263,This is very easy to,pic_cs-410_5_3_240.jpg
cs-410_5_3_54,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:26,510","00:04:30,005",54,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=266,into here you can eliminate this,pic_cs-410_5_3_240.jpg
cs-410_5_3_55,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:30,005","00:04:33,486",55,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=270,And then you will get exactly like that.,pic_cs-410_5_3_240.jpg
cs-410_5_3_56,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:33,486","00:04:35,879",56,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=273,So you can see the equivalence.,pic_cs-410_5_3_240.jpg
cs-410_5_3_57,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:35,879","00:04:41,581",57,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=275,And that's also why this KL-divergence,pic_cs-410_5_3_240.jpg
cs-410_5_3_58,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:41,581","00:04:47,085",58,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=281,"of query likelihood, because we can cover",pic_cs-410_5_3_240.jpg
cs-410_5_3_59,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:47,085","00:04:49,730",59,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=287,But it would also allow us,pic_cs-410_5_3_240.jpg
cs-410_5_3_60,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:50,770","00:04:56,104",60,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=290,So this is how we can use the,pic_cs-410_5_3_240.jpg
cs-410_5_3_61,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:04:56,104","00:05:00,183",61,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=296,The picture shows that we first,pic_cs-410_5_3_240.jpg
cs-410_5_3_62,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:00,183","00:05:04,836",62,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=300,"then we estimate a query language model,",pic_cs-410_5_3_300.jpg
cs-410_5_3_63,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:04,836","00:05:07,040",63,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=304,This is often denoted by a D here.,pic_cs-410_5_3_300.jpg
cs-410_5_3_64,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:09,560","00:05:14,690",64,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=309,But this basically means this is,pic_cs-410_5_3_300.jpg
cs-410_5_3_65,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:14,690","00:05:19,010",65,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=314,because we compute a vector for the,pic_cs-410_5_3_300.jpg
cs-410_5_3_66,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:19,010","00:05:22,450",66,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=319,"the query, and",pic_cs-410_5_3_300.jpg
cs-410_5_3_67,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:22,450","00:05:26,580",67,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=322,Only that these vectors are of special,pic_cs-410_5_3_300.jpg
cs-410_5_3_68,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:27,910","00:05:31,680",68,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=327,And then we get the results and,pic_cs-410_5_3_300.jpg
cs-410_5_3_69,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:31,680","00:05:37,420",69,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=331,Let's assume they are mostly,pic_cs-410_5_3_300.jpg
cs-410_5_3_70,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:37,420","00:05:40,400",70,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=337,although we could also consider,pic_cs-410_5_3_300.jpg
cs-410_5_3_71,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:40,400","00:05:44,974",71,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=340,"So what we could do is, like in Rocchio,",pic_cs-410_5_3_300.jpg
cs-410_5_3_72,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:44,974","00:05:48,570",72,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=344,model called the feedback,pic_cs-410_5_3_300.jpg
cs-410_5_3_73,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:48,570","00:05:52,568",73,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=348,"Again, this is going to be another vector",pic_cs-410_5_3_300.jpg
cs-410_5_3_74,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:52,568","00:05:53,227",74,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=352,in Rocchio.,pic_cs-410_5_3_300.jpg
cs-410_5_3_75,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:53,227","00:05:58,060",75,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=353,And then this model can be combined,pic_cs-410_5_3_300.jpg
cs-410_5_3_76,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:05:58,060","00:06:02,800",76,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=358,"a linear interpolation, and",pic_cs-410_5_3_300.jpg
cs-410_5_3_77,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:02,800","00:06:06,260",77,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=362,"just like, again, in Rocchio.",pic_cs-410_5_3_360.jpg
cs-410_5_3_78,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:06,260","00:06:10,270",78,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=366,So here we can see the parameter alpha,pic_cs-410_5_3_360.jpg
cs-410_5_3_79,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:10,270","00:06:14,170",79,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=370,"If it's set to zero,",pic_cs-410_5_3_360.jpg
cs-410_5_3_80,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:14,170","00:06:19,050",80,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=374,"If it's set to one, we get full feedback",pic_cs-410_5_3_360.jpg
cs-410_5_3_81,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:19,050","00:06:21,820",81,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=379,"And this is generally not desirable,",pic_cs-410_5_3_360.jpg
cs-410_5_3_82,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:21,820","00:06:26,370",82,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=381,So unless you are absolutely sure you,pic_cs-410_5_3_360.jpg
cs-410_5_3_83,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:26,370","00:06:29,250",83,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=386,then the query terms are not important.,pic_cs-410_5_3_360.jpg
cs-410_5_3_84,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:31,180","00:06:34,870",84,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=391,"So of course, the main question here is,",pic_cs-410_5_3_360.jpg
cs-410_5_3_85,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:34,870","00:06:39,340",85,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=394,"This is the big question here, and",pic_cs-410_5_3_360.jpg
cs-410_5_3_86,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:39,340","00:06:41,760",86,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=399,So here we will talk about,pic_cs-410_5_3_360.jpg
cs-410_5_3_87,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:41,760","00:06:43,260",87,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=401,"there are many approaches, of course.",pic_cs-410_5_3_360.jpg
cs-410_5_3_88,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:43,260","00:06:45,891",88,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=403,This approach is based,pic_cs-410_5_3_360.jpg
cs-410_5_3_89,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:45,891","00:06:47,823",89,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=405,I'm going to show you how it works.,pic_cs-410_5_3_360.jpg
cs-410_5_3_90,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:47,823","00:06:50,560",90,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=407,This will use a generative mixture model.,pic_cs-410_5_3_360.jpg
cs-410_5_3_91,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:50,560","00:06:55,030",91,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=410,So this picture shows that,pic_cs-410_5_3_360.jpg
cs-410_5_3_92,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:55,030","00:06:57,060",92,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=415,the feedback model that,pic_cs-410_5_3_360.jpg
cs-410_5_3_93,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:06:58,080","00:07:00,490",93,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=418,And the basis is the feedback documents.,pic_cs-410_5_3_360.jpg
cs-410_5_3_94,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:00,490","00:07:04,110",94,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=420,Let's say we are observing,pic_cs-410_5_3_420.jpg
cs-410_5_3_95,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:04,110","00:07:09,012",95,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=424,These are the clicked documents by users,pic_cs-410_5_3_420.jpg
cs-410_5_3_96,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:09,012","00:07:12,679",96,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=429,or are simply top ranked documents,pic_cs-410_5_3_420.jpg
cs-410_5_3_97,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:14,710","00:07:17,834",97,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=434,Now imagine how we can,pic_cs-410_5_3_420.jpg
cs-410_5_3_98,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:17,834","00:07:20,630",98,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=437,these documents by using language model.,pic_cs-410_5_3_420.jpg
cs-410_5_3_99,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:20,630","00:07:23,330",99,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=440,One approach is simply to assume,pic_cs-410_5_3_420.jpg
cs-410_5_3_100,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:23,330","00:07:26,820",100,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=443,these documents are generated,pic_cs-410_5_3_420.jpg
cs-410_5_3_101,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:26,820","00:07:31,287",101,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=446,"As we did before, what we could do",pic_cs-410_5_3_420.jpg
cs-410_5_3_102,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:31,287","00:07:34,940",102,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=451,here to here and,pic_cs-410_5_3_420.jpg
cs-410_5_3_103,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:36,210","00:07:41,260",103,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=456,Now the question is whether this,pic_cs-410_5_3_420.jpg
cs-410_5_3_104,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:41,260","00:07:45,430",104,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=461,"Well, you can imagine the top",pic_cs-410_5_3_420.jpg
cs-410_5_3_105,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:45,430","00:07:46,190",105,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=465,What do you think?,pic_cs-410_5_3_420.jpg
cs-410_5_3_106,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:48,280","00:07:51,560",106,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=468,"Well, those words would be common words.",pic_cs-410_5_3_420.jpg
cs-410_5_3_107,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:51,560","00:07:53,770",107,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=471,"As we always see in a language model,",pic_cs-410_5_3_420.jpg
cs-410_5_3_108,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:53,770","00:07:57,850",108,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=473,the top ranked words are actually,pic_cs-410_5_3_420.jpg
cs-410_5_3_109,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:07:57,850","00:08:02,570",109,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=477,"So it's not very good for feedback,",pic_cs-410_5_3_420.jpg
cs-410_5_3_110,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:02,570","00:08:07,330",110,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=482,words to our query when we interpolate,pic_cs-410_5_3_480.jpg
cs-410_5_3_111,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:08,880","00:08:13,100",111,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=488,"So this was not good, so",pic_cs-410_5_3_480.jpg
cs-410_5_3_112,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:13,100","00:08:17,059",112,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=493,"In particular, we are trying to",pic_cs-410_5_3_480.jpg
cs-410_5_3_113,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:17,059","00:08:21,855",113,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=497,And we have seen actually one way,pic_cs-410_5_3_480.jpg
cs-410_5_3_114,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:21,855","00:08:27,020",114,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=501,language model in the case of,pic_cs-410_5_3_480.jpg
cs-410_5_3_115,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:27,020","00:08:30,830",115,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=507,the words that are related,pic_cs-410_5_3_480.jpg
cs-410_5_3_116,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:30,830","00:08:34,590",116,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=510,We could do that and that would be,pic_cs-410_5_3_480.jpg
cs-410_5_3_117,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:34,590","00:08:39,160",117,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=514,are going to talk about another approach,pic_cs-410_5_3_480.jpg
cs-410_5_3_118,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:39,160","00:08:43,990",118,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=519,"In this case, we're going to say well,",pic_cs-410_5_3_480.jpg
cs-410_5_3_119,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:43,990","00:08:48,818",119,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=523,in these documents that should not,pic_cs-410_5_3_480.jpg
cs-410_5_3_120,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:50,310","00:08:53,527",120,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=530,"So now what we can do is to assume that,",pic_cs-410_5_3_480.jpg
cs-410_5_3_121,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:53,527","00:08:58,019",121,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=533,those words are generated from,pic_cs-410_5_3_480.jpg
cs-410_5_3_122,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:08:58,019","00:09:02,020",122,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=538,"they will generate those words like the,",pic_cs-410_5_3_480.jpg
cs-410_5_3_123,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:02,020","00:09:05,302",123,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=542,"And if we use maximum likelihood estimate,",pic_cs-410_5_3_540.jpg
cs-410_5_3_124,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:05,302","00:09:10,182",124,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=545,note that if all the words here,pic_cs-410_5_3_540.jpg
cs-410_5_3_125,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:10,182","00:09:15,681",125,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=550,then this model is forced to assign,pic_cs-410_5_3_540.jpg
cs-410_5_3_126,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:15,681","00:09:19,620",126,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=555,because it occurs so frequently here.,pic_cs-410_5_3_540.jpg
cs-410_5_3_127,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:19,620","00:09:25,100",127,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=559,Note that in order to reduce its,pic_cs-410_5_3_540.jpg
cs-410_5_3_128,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:25,100","00:09:31,280",128,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=565,"another model, which is this one,",pic_cs-410_5_3_540.jpg
cs-410_5_3_129,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:31,280","00:09:32,218",129,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=571,"And in this case,",pic_cs-410_5_3_540.jpg
cs-410_5_3_130,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:32,218","00:09:37,200",130,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=572,it's not appropriate to use the background,pic_cs-410_5_3_540.jpg
cs-410_5_3_131,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:37,200","00:09:42,320",131,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=577,goal because this model would assign high,pic_cs-410_5_3_540.jpg
cs-410_5_3_132,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:43,370","00:09:46,000",132,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=583,"So in this approach, then,",pic_cs-410_5_3_540.jpg
cs-410_5_3_133,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:46,000","00:09:50,810",133,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=586,we assume this machine that was generating,pic_cs-410_5_3_540.jpg
cs-410_5_3_134,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:50,810","00:09:53,630",134,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=590,We have a source control up here.,pic_cs-410_5_3_540.jpg
cs-410_5_3_135,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:53,630","00:09:59,110",135,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=593,Imagine we flip a coin here to,pic_cs-410_5_3_540.jpg
cs-410_5_3_136,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:09:59,110","00:10:03,238",136,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=599,"With probability of lambda,",pic_cs-410_5_3_540.jpg
cs-410_5_3_137,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:03,238","00:10:05,400",137,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=603,we're going to use,pic_cs-410_5_3_600.jpg
cs-410_5_3_138,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:05,400","00:10:08,540",138,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=605,And we're going to do that in,pic_cs-410_5_3_600.jpg
cs-410_5_3_139,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:08,540","00:10:12,570",139,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=608,"With probability of 1 minus lambda,",pic_cs-410_5_3_600.jpg
cs-410_5_3_140,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:12,570","00:10:17,460",140,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=612,"to use a known topic model, here,",pic_cs-410_5_3_600.jpg
cs-410_5_3_141,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:17,460","00:10:20,100",141,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=617,And we're going to then,pic_cs-410_5_3_600.jpg
cs-410_5_3_142,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:20,100","00:10:25,450",142,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=620,If we make this assumption and this whole,pic_cs-410_5_3_600.jpg
cs-410_5_3_143,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:25,450","00:10:30,420",143,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=625,this a mixture model because there are two,pic_cs-410_5_3_600.jpg
cs-410_5_3_144,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:30,420","00:10:33,940",144,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=630,And we actually don't know when,pic_cs-410_5_3_600.jpg
cs-410_5_3_145,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:35,770","00:10:40,320",145,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=635,"So again,",pic_cs-410_5_3_600.jpg
cs-410_5_3_146,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:42,270","00:10:47,920",146,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=642,and we can still ask for words and it will,pic_cs-410_5_3_600.jpg
cs-410_5_3_147,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:47,920","00:10:51,920",147,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=647,"And of course, which word will show up",pic_cs-410_5_3_600.jpg
cs-410_5_3_148,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:51,920","00:10:53,003",148,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=651,that distribution.,pic_cs-410_5_3_600.jpg
cs-410_5_3_149,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:53,003","00:10:55,780",149,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=653,"In addition,",pic_cs-410_5_3_600.jpg
cs-410_5_3_150,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:55,780","00:10:58,751",150,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=655,because if you say lambda is very high and,pic_cs-410_5_3_600.jpg
cs-410_5_3_151,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:10:58,751","00:11:02,769",151,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=658,"always use the background distribution,",pic_cs-410_5_3_600.jpg
cs-410_5_3_152,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:02,769","00:11:07,260",152,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=662,"Then if you say, well, lambda is",pic_cs-410_5_3_660.jpg
cs-410_5_3_153,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:07,260","00:11:12,353",153,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=667,So all of these,pic_cs-410_5_3_660.jpg
cs-410_5_3_154,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:12,353","00:11:15,108",154,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=672,"And then if you're thinking this way,",pic_cs-410_5_3_660.jpg
cs-410_5_3_155,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:15,108","00:11:19,206",155,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=675,basically we can do exactly,pic_cs-410_5_3_660.jpg
cs-410_5_3_156,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:19,206","00:11:23,445",156,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=679,We're going to use maximum likelihood,pic_cs-410_5_3_660.jpg
cs-410_5_3_157,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:23,445","00:11:25,760",157,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=683,to estimate the parameters.,pic_cs-410_5_3_660.jpg
cs-410_5_3_158,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:25,760","00:11:30,201",158,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=685,Basically we're going to,pic_cs-410_5_3_660.jpg
cs-410_5_3_159,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:30,201","00:11:33,512",159,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=690,that we can best explain all the data.,pic_cs-410_5_3_660.jpg
cs-410_5_3_160,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:33,512","00:11:41,200",160,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=693,The difference now is that we are not,pic_cs-410_5_3_660.jpg
cs-410_5_3_161,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:41,200","00:11:46,633",161,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=701,But rather we are going to ask this whole,pic_cs-410_5_3_660.jpg
cs-410_5_3_162,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:46,633","00:11:50,049",162,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=706,Because it has got some help,pic_cs-410_5_3_660.jpg
cs-410_5_3_163,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:50,049","00:11:54,080",163,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=710,it doesn't have to assign high,pic_cs-410_5_3_660.jpg
cs-410_5_3_164,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:54,080","00:11:58,890",164,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=714,"As a result, it will then assign higher",pic_cs-410_5_3_660.jpg
cs-410_5_3_165,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:11:58,890","00:12:04,950",165,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=718,are common here but,pic_cs-410_5_3_660.jpg
cs-410_5_3_166,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:04,950","00:12:06,877",166,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=724,So those would be common here.,pic_cs-410_5_3_720.jpg
cs-410_5_3_167,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:11,321","00:12:14,907",167,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=731,"And if they're common, they would",pic_cs-410_5_3_720.jpg
cs-410_5_3_168,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:14,907","00:12:17,661",168,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=734,according to a maximum,pic_cs-410_5_3_720.jpg
cs-410_5_3_169,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:17,661","00:12:23,692",169,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=737,"And if they are rare here,",pic_cs-410_5_3_720.jpg
cs-410_5_3_170,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:23,692","00:12:29,620",170,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=743,much help from this background model.,pic_cs-410_5_3_720.jpg
cs-410_5_3_171,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:29,620","00:12:33,940",171,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=749,"As a result, this topic model",pic_cs-410_5_3_720.jpg
cs-410_5_3_172,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:33,940","00:12:37,410",172,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=753,"So the high probability words,",pic_cs-410_5_3_720.jpg
cs-410_5_3_173,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:37,410","00:12:41,630",173,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=757,would be those that are common here but,pic_cs-410_5_3_720.jpg
cs-410_5_3_174,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:43,960","00:12:48,897",174,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=763,So this is basically a little bit,pic_cs-410_5_3_720.jpg
cs-410_5_3_175,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:48,897","00:12:53,664",175,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=768,But this would allow us to achieve the,pic_cs-410_5_3_720.jpg
cs-410_5_3_176,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:53,664","00:12:55,770",176,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=773,are meaningless in the feedback.,pic_cs-410_5_3_720.jpg
cs-410_5_3_177,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:12:56,780","00:13:01,200",177,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=776,"So mathematically, what we have is",pic_cs-410_5_3_720.jpg
cs-410_5_3_178,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:01,200","00:13:04,794",178,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=781,"local likelihood,",pic_cs-410_5_3_780.jpg
cs-410_5_3_179,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:06,200","00:13:08,860",179,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=786,And note that we also have another,pic_cs-410_5_3_780.jpg
cs-410_5_3_180,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:08,860","00:13:13,150",180,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=788,we assume that the lambda denotes,pic_cs-410_5_3_780.jpg
cs-410_5_3_181,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:13,150","00:13:16,010",181,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=793,"So we are going to,",pic_cs-410_5_3_780.jpg
cs-410_5_3_182,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:16,010","00:13:21,800",182,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=796,Let's say 50% of the words are noise or,pic_cs-410_5_3_780.jpg
cs-410_5_3_183,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:21,800","00:13:24,295",183,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=801,And this can then be,pic_cs-410_5_3_780.jpg
cs-410_5_3_184,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:24,295","00:13:30,896",184,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=804,"If we assume this is fixed, then we only",pic_cs-410_5_3_780.jpg
cs-410_5_3_185,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:30,896","00:13:35,090",185,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=810,just like in the simple,pic_cs-410_5_3_780.jpg
cs-410_5_3_186,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:35,090","00:13:39,090",186,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=815,"We have n parameters,",pic_cs-410_5_3_780.jpg
cs-410_5_3_187,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:39,090","00:13:41,289",187,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=819,And then the likelihood,pic_cs-410_5_3_780.jpg
cs-410_5_3_188,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:42,760","00:13:47,643",188,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=822,It's very similar to the global,pic_cs-410_5_3_780.jpg
cs-410_5_3_189,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:47,643","00:13:51,537",189,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=827,except that inside the logarithm,pic_cs-410_5_3_780.jpg
cs-410_5_3_190,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:51,537","00:13:57,070",190,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=831,And this sum is because we,pic_cs-410_5_3_780.jpg
cs-410_5_3_191,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:13:57,070","00:14:01,300",191,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=837,And which one is used would depend on,pic_cs-410_5_3_780.jpg
cs-410_5_3_192,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:02,460","00:14:08,790",192,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=842,"But mathematically, this is the function",pic_cs-410_5_3_840.jpg
cs-410_5_3_193,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:08,790","00:14:10,510",193,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=848,So this is just a function.,pic_cs-410_5_3_840.jpg
cs-410_5_3_194,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:10,510","00:14:13,620",194,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=850,All the other values are known except for,pic_cs-410_5_3_840.jpg
cs-410_5_3_195,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:15,010","00:14:19,834",195,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=855,So we can then choose this,pic_cs-410_5_3_840.jpg
cs-410_5_3_196,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:19,834","00:14:21,531",196,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=859,"this log likelihood,",pic_cs-410_5_3_840.jpg
cs-410_5_3_197,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:21,531","00:14:27,357",197,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=861,the same idea as the maximum likelihood,pic_cs-410_5_3_840.jpg
cs-410_5_3_198,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:27,357","00:14:30,060",198,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=867,We just have to solve this,pic_cs-410_5_3_840.jpg
cs-410_5_3_199,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:30,060","00:14:34,460",199,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=870,We essentially would try all,pic_cs-410_5_3_840.jpg
cs-410_5_3_200,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:34,460","00:14:37,670",200,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=874,that gives this whole thing,pic_cs-410_5_3_840.jpg
cs-410_5_3_201,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:37,670","00:14:39,210",201,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=877,So it's a well-defined math problem.,pic_cs-410_5_3_840.jpg
cs-410_5_3_202,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:40,900","00:14:45,720",202,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=880,"Once we have done that, we obtain this",pic_cs-410_5_3_840.jpg
cs-410_5_3_203,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:45,720","00:14:47,812",203,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=885,original query model to the feedback.,pic_cs-410_5_3_840.jpg
cs-410_5_3_204,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:50,980","00:14:55,963",204,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=890,So here are some examples of,pic_cs-410_5_3_840.jpg
cs-410_5_3_205,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:55,963","00:14:57,817",205,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=895,document collection.,pic_cs-410_5_3_840.jpg
cs-410_5_3_206,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:14:57,817","00:15:01,673",206,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=897,And we do pseudo-feedback we just,pic_cs-410_5_3_840.jpg
cs-410_5_3_207,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:01,673","00:15:03,750",207,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=901,we use this mixture model.,pic_cs-410_5_3_900.jpg
cs-410_5_3_208,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:03,750","00:15:06,090",208,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=903,So the query is airport security.,pic_cs-410_5_3_900.jpg
cs-410_5_3_209,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:06,090","00:15:11,480",209,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=906,What we do is we first retrieve ten,pic_cs-410_5_3_900.jpg
cs-410_5_3_210,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:11,480","00:15:14,520",210,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=911,this is of course pseudo-feedback.,pic_cs-410_5_3_900.jpg
cs-410_5_3_211,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:14,520","00:15:20,000",211,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=914,And then we're going to feed that,pic_cs-410_5_3_900.jpg
cs-410_5_3_212,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:21,130","00:15:25,770",212,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=921,And these are the words,pic_cs-410_5_3_900.jpg
cs-410_5_3_213,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:25,770","00:15:30,220",213,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=925,This is the probability of a word given,pic_cs-410_5_3_900.jpg
cs-410_5_3_214,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:31,600","00:15:34,350",214,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=931,So in both cases you can see the highest,pic_cs-410_5_3_900.jpg
cs-410_5_3_215,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:34,350","00:15:38,480",215,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=934,probability words include the very,pic_cs-410_5_3_900.jpg
cs-410_5_3_216,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:38,480","00:15:40,208",216,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=938,"So airport security, for example,",pic_cs-410_5_3_900.jpg
cs-410_5_3_217,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:40,208","00:15:45,450",217,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=940,these query words still show up as high,pic_cs-410_5_3_900.jpg
cs-410_5_3_218,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:45,450","00:15:48,850",218,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=945,because they occur frequently,pic_cs-410_5_3_900.jpg
cs-410_5_3_219,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:48,850","00:15:53,830",219,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=948,"But we also see beverage,",pic_cs-410_5_3_900.jpg
cs-410_5_3_220,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:53,830","00:15:59,436",220,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=953,"So these are relevant to this topic,",pic_cs-410_5_3_900.jpg
cs-410_5_3_221,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:15:59,436","00:16:05,280",221,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=959,"if combined with original query, can help",pic_cs-410_5_3_900.jpg
cs-410_5_3_222,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:05,280","00:16:11,200",222,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=965,And also they can help us bring up,pic_cs-410_5_3_960.jpg
cs-410_5_3_223,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:11,200","00:16:16,980",223,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=971,"these other words, maybe, for example,",pic_cs-410_5_3_960.jpg
cs-410_5_3_224,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:18,070","00:16:20,680",224,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=978,So this is how pseudo-feedback works.,pic_cs-410_5_3_960.jpg
cs-410_5_3_225,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:20,680","00:16:26,790",225,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=980,It shows that this model really works and,pic_cs-410_5_3_960.jpg
cs-410_5_3_226,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:26,790","00:16:31,546",226,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=986,What's also interesting is that if,pic_cs-410_5_3_960.jpg
cs-410_5_3_227,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:31,546","00:16:35,154",227,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=991,"you compare them,",pic_cs-410_5_3_960.jpg
cs-410_5_3_228,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:35,154","00:16:40,415",228,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=995,"when lambda is set to a small value,",pic_cs-410_5_3_960.jpg
cs-410_5_3_229,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:40,415","00:16:45,473",229,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1000,"And that means, well,",pic_cs-410_5_3_960.jpg
cs-410_5_3_230,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:45,473","00:16:48,575",230,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1005,"Remember, lambda confuses the probability",pic_cs-410_5_3_960.jpg
cs-410_5_3_231,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:48,575","00:16:50,925",231,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1008,to generate the text.,pic_cs-410_5_3_960.jpg
cs-410_5_3_232,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:50,925","00:16:53,245",232,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1010,"If we don't rely much on background model,",pic_cs-410_5_3_960.jpg
cs-410_5_3_233,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:53,245","00:16:58,100",233,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1013,we still have to use this topic model,pic_cs-410_5_3_960.jpg
cs-410_5_3_234,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:16:58,100","00:17:01,340",234,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1018,Whereas if we set lambda,pic_cs-410_5_3_960.jpg
cs-410_5_3_235,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:01,340","00:17:05,550",235,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1021,we will use the background model,pic_cs-410_5_3_1020.jpg
cs-410_5_3_236,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:05,550","00:17:08,930",236,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1025,Then there's no burden on,pic_cs-410_5_3_1020.jpg
cs-410_5_3_237,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:08,930","00:17:11,790",237,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1028,in the feedback documents,pic_cs-410_5_3_1020.jpg
cs-410_5_3_238,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:11,790","00:17:17,430",238,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1031,"So as a result, the topic model",pic_cs-410_5_3_1020.jpg
cs-410_5_3_239,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:17,430","00:17:20,060",239,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1037,It contains all the relevant,pic_cs-410_5_3_1020.jpg
cs-410_5_3_240,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:21,260","00:17:26,100",240,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1041,So this can be added to the original,pic_cs-410_5_3_1020.jpg
cs-410_5_3_241,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:28,140","00:17:29,900",241,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1048,"So to summarize,",pic_cs-410_5_3_1020.jpg
cs-410_5_3_242,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:29,900","00:17:34,470",242,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1049,in this lecture we have talked about,pic_cs-410_5_3_1020.jpg
cs-410_5_3_243,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:34,470","00:17:38,290",243,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1054,"In general,",pic_cs-410_5_3_1020.jpg
cs-410_5_3_244,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:38,290","00:17:43,610",244,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1058,"These examples can be assumed examples,",pic_cs-410_5_3_1020.jpg
cs-410_5_3_245,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:43,610","00:17:48,770",245,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1063,like assume the top ten documents,pic_cs-410_5_3_1020.jpg
cs-410_5_3_246,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:48,770","00:17:51,419",246,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1068,"They could be based on user interactions,",pic_cs-410_5_3_1020.jpg
cs-410_5_3_247,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:51,419","00:17:55,260",247,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1071,like feedback based on clickthroughs or,pic_cs-410_5_3_1020.jpg
cs-410_5_3_248,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:55,260","00:17:59,308",248,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1075,We talked about the three major,pic_cs-410_5_3_1020.jpg
cs-410_5_3_249,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:17:59,308","00:18:01,657",249,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1079,"pseudo feedback, and implicit feedback.",pic_cs-410_5_3_1020.jpg
cs-410_5_3_250,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:01,657","00:18:08,108",250,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1081,We talked about how to use Rocchio to,pic_cs-410_5_3_1080.jpg
cs-410_5_3_251,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:08,108","00:18:14,047",251,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1088,how to use query model estimation for,pic_cs-410_5_3_1080.jpg
cs-410_5_3_252,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:14,047","00:18:18,350",252,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1094,And we briefly talked about,pic_cs-410_5_3_1080.jpg
cs-410_5_3_253,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:19,790","00:18:21,650",253,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1099,There are many other methods.,pic_cs-410_5_3_1080.jpg
cs-410_5_3_254,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:21,650","00:18:22,170",254,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1101,"For example,",pic_cs-410_5_3_1080.jpg
cs-410_5_3_255,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:22,170","00:18:26,990",255,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1102,the relevance model is a very effective,pic_cs-410_5_3_1080.jpg
cs-410_5_3_256,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:26,990","00:18:31,130",256,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1106,So you can read more about these,pic_cs-410_5_3_1080.jpg
cs-410_5_3_257,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:32,170","00:18:36,200",257,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1112,are listed at the end of this lecture.,pic_cs-410_5_3_1080.jpg
cs-410_5_3_258,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:36,200","00:18:38,420",258,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1116,So there are two additional readings here.,pic_cs-410_5_3_1080.jpg
cs-410_5_3_259,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:38,420","00:18:42,047",259,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1118,The first one is a book that,pic_cs-410_5_3_1080.jpg
cs-410_5_3_260,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:42,047","00:18:46,170",260,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1122,discussion of language models for,pic_cs-410_5_3_1080.jpg
cs-410_5_3_261,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:46,170","00:18:49,745",261,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1126,And the second one is a important research,pic_cs-410_5_3_1080.jpg
cs-410_5_3_262,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:49,745","00:18:54,549",262,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1129,paper that's about relevance,pic_cs-410_5_3_1080.jpg
cs-410_5_3_263,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:54,549","00:18:59,471",263,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1134,and it's a very effective way,pic_cs-410_5_3_1080.jpg
cs-410_5_3_264,cs-410,5,3, Feedback in Text Retrieval - Feedback in LM,"00:18:59,471","00:19:09,471",264,https://www.coursera.org/learn/cs-410/lecture/M7ylk?t=1139,[MUSIC],pic_cs-410_5_3_1080.jpg
cs-410_5_4_1,cs-410,5,4, Web Search,"00:00:07,440","00:00:09,410",1,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=7,This lecture is about Web Search.,pic_cs-410_5_4_0.jpg
cs-410_5_4_2,cs-410,5,4, Web Search,"00:00:11,950","00:00:14,750",2,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=11,"In this lecture,",pic_cs-410_5_4_0.jpg
cs-410_5_4_3,cs-410,5,4, Web Search,"00:00:14,750","00:00:19,150",3,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=14,of the most important applications of,pic_cs-410_5_4_0.jpg
cs-410_5_4_4,cs-410,5,4, Web Search,"00:00:19,150","00:00:21,520",4,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=19,So let's first look at some,pic_cs-410_5_4_0.jpg
cs-410_5_4_5,cs-410,5,4, Web Search,"00:00:21,520","00:00:23,380",5,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=21,opportunities in web search.,pic_cs-410_5_4_0.jpg
cs-410_5_4_6,cs-410,5,4, Web Search,"00:00:23,380","00:00:26,010",6,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=23,"Now, many informational",pic_cs-410_5_4_0.jpg
cs-410_5_4_7,cs-410,5,4, Web Search,"00:00:26,010","00:00:29,010",7,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=26,had been developed,pic_cs-410_5_4_0.jpg
cs-410_5_4_8,cs-410,5,4, Web Search,"00:00:29,010","00:00:33,890",8,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=29,"So when the web was born,",pic_cs-410_5_4_0.jpg
cs-410_5_4_9,cs-410,5,4, Web Search,"00:00:33,890","00:00:39,890",9,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=33,those algorithms to major application,pic_cs-410_5_4_0.jpg
cs-410_5_4_10,cs-410,5,4, Web Search,"00:00:39,890","00:00:45,780",10,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=39,"So naturally, there have to be some",pic_cs-410_5_4_0.jpg
cs-410_5_4_11,cs-410,5,4, Web Search,"00:00:45,780","00:00:53,460",11,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=45,search algorithms to address new,pic_cs-410_5_4_0.jpg
cs-410_5_4_12,cs-410,5,4, Web Search,"00:00:53,460","00:00:56,210",12,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=53,So here are some general challenges.,pic_cs-410_5_4_0.jpg
cs-410_5_4_13,cs-410,5,4, Web Search,"00:00:56,210","00:00:58,510",13,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=56,"First, this is a scalability challenge.",pic_cs-410_5_4_0.jpg
cs-410_5_4_14,cs-410,5,4, Web Search,"00:00:58,510","00:01:00,200",14,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=58,How to handle the size of the web and,pic_cs-410_5_4_0.jpg
cs-410_5_4_15,cs-410,5,4, Web Search,"00:01:00,200","00:01:02,750",15,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=60,ensure completeness of,pic_cs-410_5_4_60.jpg
cs-410_5_4_16,cs-410,5,4, Web Search,"00:01:03,870","00:01:07,820",16,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=63,How to serve many users quickly and,pic_cs-410_5_4_60.jpg
cs-410_5_4_17,cs-410,5,4, Web Search,"00:01:07,820","00:01:10,801",17,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=67,And so that's one major challenge and,pic_cs-410_5_4_60.jpg
cs-410_5_4_18,cs-410,5,4, Web Search,"00:01:10,801","00:01:16,480",18,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=70,before the web was born the scale,pic_cs-410_5_4_60.jpg
cs-410_5_4_19,cs-410,5,4, Web Search,"00:01:16,480","00:01:20,190",19,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=76,The second problem is that there's,pic_cs-410_5_4_60.jpg
cs-410_5_4_20,cs-410,5,4, Web Search,"00:01:20,190","00:01:21,960",20,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=80,there are often spams.,pic_cs-410_5_4_60.jpg
cs-410_5_4_21,cs-410,5,4, Web Search,"00:01:21,960","00:01:24,334",21,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=81,The third challenge is,pic_cs-410_5_4_60.jpg
cs-410_5_4_22,cs-410,5,4, Web Search,"00:01:24,334","00:01:31,879",22,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=84,The new pages are constantly create and,pic_cs-410_5_4_60.jpg
cs-410_5_4_23,cs-410,5,4, Web Search,"00:01:31,879","00:01:36,281",23,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=91,so it makes it harder to,pic_cs-410_5_4_60.jpg
cs-410_5_4_24,cs-410,5,4, Web Search,"00:01:36,281","00:01:40,391",24,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=96,So these are some of the challenges,pic_cs-410_5_4_60.jpg
cs-410_5_4_25,cs-410,5,4, Web Search,"00:01:40,391","00:01:42,880",25,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=100,deal with high quality web searching.,pic_cs-410_5_4_60.jpg
cs-410_5_4_26,cs-410,5,4, Web Search,"00:01:44,090","00:01:47,492",26,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=104,On the other hand there are also some,pic_cs-410_5_4_60.jpg
cs-410_5_4_27,cs-410,5,4, Web Search,"00:01:47,492","00:01:49,930",27,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=107,leverage to include the search results.,pic_cs-410_5_4_60.jpg
cs-410_5_4_28,cs-410,5,4, Web Search,"00:01:49,930","00:01:53,330",28,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=109,"There are many additional heuristics,",pic_cs-410_5_4_60.jpg
cs-410_5_4_29,cs-410,5,4, Web Search,"00:01:55,070","00:02:00,020",29,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=115,using links that we can,pic_cs-410_5_4_60.jpg
cs-410_5_4_30,cs-410,5,4, Web Search,"00:02:00,020","00:02:03,510",30,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=120,Now everything that we talked about,pic_cs-410_5_4_120.jpg
cs-410_5_4_31,cs-410,5,4, Web Search,"00:02:03,510","00:02:04,459",31,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=123,are general algorithms.,pic_cs-410_5_4_120.jpg
cs-410_5_4_32,cs-410,5,4, Web Search,"00:02:05,630","00:02:11,050",32,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=125,They can be applied to any search,pic_cs-410_5_4_120.jpg
cs-410_5_4_33,cs-410,5,4, Web Search,"00:02:11,050","00:02:15,890",33,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=131,"On the other hand, they also don't take",pic_cs-410_5_4_120.jpg
cs-410_5_4_34,cs-410,5,4, Web Search,"00:02:15,890","00:02:21,375",34,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=135,of pages or documents in the specific,pic_cs-410_5_4_120.jpg
cs-410_5_4_35,cs-410,5,4, Web Search,"00:02:21,375","00:02:23,855",35,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=141,"Web pages are linked with each other,",pic_cs-410_5_4_120.jpg
cs-410_5_4_36,cs-410,5,4, Web Search,"00:02:23,855","00:02:28,645",36,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=143,the linking is something,pic_cs-410_5_4_120.jpg
cs-410_5_4_37,cs-410,5,4, Web Search,"00:02:28,645","00:02:33,610",37,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=148,"So, because of these challenges and",pic_cs-410_5_4_120.jpg
cs-410_5_4_38,cs-410,5,4, Web Search,"00:02:33,610","00:02:39,110",38,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=153,that have been developed for,pic_cs-410_5_4_120.jpg
cs-410_5_4_39,cs-410,5,4, Web Search,"00:02:39,110","00:02:41,390",39,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=159,One is parallel indexing and searching and,pic_cs-410_5_4_120.jpg
cs-410_5_4_40,cs-410,5,4, Web Search,"00:02:41,390","00:02:44,410",40,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=161,this is to address,pic_cs-410_5_4_120.jpg
cs-410_5_4_41,cs-410,5,4, Web Search,"00:02:44,410","00:02:49,930",41,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=164,"In particular, Google's imaging of",pic_cs-410_5_4_120.jpg
cs-410_5_4_42,cs-410,5,4, Web Search,"00:02:49,930","00:02:53,590",42,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=169,has been very helpful in that aspect.,pic_cs-410_5_4_120.jpg
cs-410_5_4_43,cs-410,5,4, Web Search,"00:02:53,590","00:02:56,680",43,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=173,"Second, there are techniques",pic_cs-410_5_4_120.jpg
cs-410_5_4_44,cs-410,5,4, Web Search,"00:02:56,680","00:03:00,460",44,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=176,"addressing the problem of spams,",pic_cs-410_5_4_120.jpg
cs-410_5_4_45,cs-410,5,4, Web Search,"00:03:00,460","00:03:03,570",45,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=180,We'll have to prevent those spam,pic_cs-410_5_4_180.jpg
cs-410_5_4_46,cs-410,5,4, Web Search,"00:03:04,680","00:03:07,338",46,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=184,And there are also techniques,pic_cs-410_5_4_180.jpg
cs-410_5_4_47,cs-410,5,4, Web Search,"00:03:07,338","00:03:10,520",47,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=187,And we're going to use a lot,pic_cs-410_5_4_180.jpg
cs-410_5_4_48,cs-410,5,4, Web Search,"00:03:10,520","00:03:15,410",48,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=190,that it's not easy to spam the search,pic_cs-410_5_4_180.jpg
cs-410_5_4_49,cs-410,5,4, Web Search,"00:03:15,410","00:03:19,810",49,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=195,And the third line of techniques is link,pic_cs-410_5_4_180.jpg
cs-410_5_4_50,cs-410,5,4, Web Search,"00:03:19,810","00:03:24,730",50,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=199,analysis and these are techniques that can,pic_cs-410_5_4_180.jpg
cs-410_5_4_51,cs-410,5,4, Web Search,"00:03:24,730","00:03:30,780",51,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=204,allow us to improve such results,pic_cs-410_5_4_180.jpg
cs-410_5_4_52,cs-410,5,4, Web Search,"00:03:30,780","00:03:35,230",52,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=210,"And in general in web searching,",pic_cs-410_5_4_180.jpg
cs-410_5_4_53,cs-410,5,4, Web Search,"00:03:35,230","00:03:37,690",53,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=215,ranking not just for link analysis.,pic_cs-410_5_4_180.jpg
cs-410_5_4_54,cs-410,5,4, Web Search,"00:03:37,690","00:03:43,300",54,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=217,But also exploring all kinds,pic_cs-410_5_4_180.jpg
cs-410_5_4_55,cs-410,5,4, Web Search,"00:03:43,300","00:03:47,730",55,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=223,anchor text that describes,pic_cs-410_5_4_180.jpg
cs-410_5_4_56,cs-410,5,4, Web Search,"00:03:47,730","00:03:51,310",56,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=227,"So, here's a picture showing",pic_cs-410_5_4_180.jpg
cs-410_5_4_57,cs-410,5,4, Web Search,"00:03:51,310","00:03:55,820",57,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=231,"Basically, this is the web on the left and",pic_cs-410_5_4_180.jpg
cs-410_5_4_58,cs-410,5,4, Web Search,"00:03:55,820","00:04:00,550",58,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=235,we're going to help this user to get,pic_cs-410_5_4_180.jpg
cs-410_5_4_59,cs-410,5,4, Web Search,"00:04:00,550","00:04:05,410",59,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=240,And the first component is a Crawler that,pic_cs-410_5_4_240.jpg
cs-410_5_4_60,cs-410,5,4, Web Search,"00:04:05,410","00:04:09,810",60,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=245,component is Indexer that would take,pic_cs-410_5_4_240.jpg
cs-410_5_4_61,cs-410,5,4, Web Search,"00:04:10,920","00:04:15,720",61,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=250,The third component there is a Retriever,pic_cs-410_5_4_240.jpg
cs-410_5_4_62,cs-410,5,4, Web Search,"00:04:15,720","00:04:19,840",62,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=255,answer user's query by talking,pic_cs-410_5_4_240.jpg
cs-410_5_4_63,cs-410,5,4, Web Search,"00:04:19,840","00:04:24,790",63,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=259,And then the search results will be given,pic_cs-410_5_4_240.jpg
cs-410_5_4_64,cs-410,5,4, Web Search,"00:04:24,790","00:04:29,090",64,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=264,"show those results, it allows",pic_cs-410_5_4_240.jpg
cs-410_5_4_65,cs-410,5,4, Web Search,"00:04:29,090","00:04:32,417",65,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=269,"So, we're going to talk about",pic_cs-410_5_4_240.jpg
cs-410_5_4_66,cs-410,5,4, Web Search,"00:04:32,417","00:04:37,552",66,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=272,"First of all, we're going to talk about",pic_cs-410_5_4_240.jpg
cs-410_5_4_67,cs-410,5,4, Web Search,"00:04:37,552","00:04:42,459",67,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=277,software robot that would do something,pic_cs-410_5_4_240.jpg
cs-410_5_4_68,cs-410,5,4, Web Search,"00:04:42,459","00:04:44,954",68,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=282,"To build a toy crawler is relatively easy,",pic_cs-410_5_4_240.jpg
cs-410_5_4_69,cs-410,5,4, Web Search,"00:04:44,954","00:04:47,875",69,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=284,because you just need to start,pic_cs-410_5_4_240.jpg
cs-410_5_4_70,cs-410,5,4, Web Search,"00:04:47,875","00:04:51,912",70,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=287,And then fetch pages from the web and,pic_cs-410_5_4_240.jpg
cs-410_5_4_71,cs-410,5,4, Web Search,"00:04:51,912","00:04:53,517",71,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=291,figure out new links.,pic_cs-410_5_4_240.jpg
cs-410_5_4_72,cs-410,5,4, Web Search,"00:04:53,517","00:05:00,994",72,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=293,And then add them to the priority que and,pic_cs-410_5_4_240.jpg
cs-410_5_4_73,cs-410,5,4, Web Search,"00:05:00,994","00:05:04,764",73,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=300,But to be able to real crawler,pic_cs-410_5_4_300.jpg
cs-410_5_4_74,cs-410,5,4, Web Search,"00:05:04,764","00:05:09,249",74,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=304,there are some complicated issues,pic_cs-410_5_4_300.jpg
cs-410_5_4_75,cs-410,5,4, Web Search,"00:05:09,249","00:05:13,736",75,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=309,"For example robustness,",pic_cs-410_5_4_300.jpg
cs-410_5_4_76,cs-410,5,4, Web Search,"00:05:13,736","00:05:18,722",76,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=313,what if there's a trap that generates,pic_cs-410_5_4_300.jpg
cs-410_5_4_77,cs-410,5,4, Web Search,"00:05:18,722","00:05:23,456",77,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=318,that might attract your crawler to,pic_cs-410_5_4_300.jpg
cs-410_5_4_78,cs-410,5,4, Web Search,"00:05:23,456","00:05:26,700",78,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=323,to fetch dynamic generated pages?,pic_cs-410_5_4_300.jpg
cs-410_5_4_79,cs-410,5,4, Web Search,"00:05:26,700","00:05:30,093",79,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=326,The results of this issue,pic_cs-410_5_4_300.jpg
cs-410_5_4_80,cs-410,5,4, Web Search,"00:05:30,093","00:05:35,668",80,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=330,you don't want to overload one particular,pic_cs-410_5_4_300.jpg
cs-410_5_4_81,cs-410,5,4, Web Search,"00:05:35,668","00:05:39,158",81,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=335,you have to respect the robot,pic_cs-410_5_4_300.jpg
cs-410_5_4_82,cs-410,5,4, Web Search,"00:05:39,158","00:05:43,340",82,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=339,You also need to handle different,pic_cs-410_5_4_300.jpg
cs-410_5_4_83,cs-410,5,4, Web Search,"00:05:43,340","00:05:46,019",83,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=343,"PDF files,",pic_cs-410_5_4_300.jpg
cs-410_5_4_84,cs-410,5,4, Web Search,"00:05:46,019","00:05:50,189",84,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=346,And you have to also,pic_cs-410_5_4_300.jpg
cs-410_5_4_85,cs-410,5,4, Web Search,"00:05:50,189","00:05:56,237",85,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=350,sometimes those are CGI scripts and,pic_cs-410_5_4_300.jpg
cs-410_5_4_86,cs-410,5,4, Web Search,"00:05:56,237","00:06:01,139",86,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=356,"etc, and sometimes you have",pic_cs-410_5_4_300.jpg
cs-410_5_4_87,cs-410,5,4, Web Search,"00:06:01,139","00:06:03,866",87,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=361,they also create challenges.,pic_cs-410_5_4_360.jpg
cs-410_5_4_88,cs-410,5,4, Web Search,"00:06:03,866","00:06:08,795",88,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=363,And you ideally should also recognize,pic_cs-410_5_4_360.jpg
cs-410_5_4_89,cs-410,5,4, Web Search,"00:06:08,795","00:06:11,475",89,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=368,to duplicate those pages.,pic_cs-410_5_4_360.jpg
cs-410_5_4_90,cs-410,5,4, Web Search,"00:06:11,475","00:06:15,398",90,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=371,"And finally, you may be interested",pic_cs-410_5_4_360.jpg
cs-410_5_4_91,cs-410,5,4, Web Search,"00:06:15,398","00:06:19,935",91,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=375,Those are URLs that may not be linked,pic_cs-410_5_4_360.jpg
cs-410_5_4_92,cs-410,5,4, Web Search,"00:06:19,935","00:06:24,884",92,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=379,"the URL to a shorter path, you might",pic_cs-410_5_4_360.jpg
cs-410_5_4_93,cs-410,5,4, Web Search,"00:06:27,008","00:06:29,298",93,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=387,So what are the Major Crawling Strategies?,pic_cs-410_5_4_360.jpg
cs-410_5_4_94,cs-410,5,4, Web Search,"00:06:29,298","00:06:30,040",94,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=389,"In general,",pic_cs-410_5_4_360.jpg
cs-410_5_4_95,cs-410,5,4, Web Search,"00:06:30,040","00:06:36,560",95,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=390,Breadth-First is most common because,pic_cs-410_5_4_360.jpg
cs-410_5_4_96,cs-410,5,4, Web Search,"00:06:36,560","00:06:41,405",96,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=396,You would not keep probing a particular,pic_cs-410_5_4_360.jpg
cs-410_5_4_97,cs-410,5,4, Web Search,"00:06:42,635","00:06:47,009",97,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=402,Also parallel crawling is very,pic_cs-410_5_4_360.jpg
cs-410_5_4_98,cs-410,5,4, Web Search,"00:06:47,009","00:06:48,554",98,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=407,easy to parallelize.,pic_cs-410_5_4_360.jpg
cs-410_5_4_99,cs-410,5,4, Web Search,"00:06:48,554","00:06:50,887",99,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=408,And there is some variations,pic_cs-410_5_4_360.jpg
cs-410_5_4_100,cs-410,5,4, Web Search,"00:06:50,887","00:06:54,560",100,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=410,and one interesting variation,pic_cs-410_5_4_360.jpg
cs-410_5_4_101,cs-410,5,4, Web Search,"00:06:54,560","00:06:59,850",101,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=414,"In this case, we're going to crawl just",pic_cs-410_5_4_360.jpg
cs-410_5_4_102,cs-410,5,4, Web Search,"00:06:59,850","00:07:04,316",102,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=419,"For example,",pic_cs-410_5_4_360.jpg
cs-410_5_4_103,cs-410,5,4, Web Search,"00:07:04,316","00:07:07,885",103,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=424,And this is typically going to,pic_cs-410_5_4_420.jpg
cs-410_5_4_104,cs-410,5,4, Web Search,"00:07:07,885","00:07:12,953",104,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=427,then you can use the query to get some,pic_cs-410_5_4_420.jpg
cs-410_5_4_105,cs-410,5,4, Web Search,"00:07:12,953","00:07:17,052",105,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=432,And then you can start it with those,pic_cs-410_5_4_420.jpg
cs-410_5_4_106,cs-410,5,4, Web Search,"00:07:17,052","00:07:19,544",106,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=437,"The one channel in crawling,",pic_cs-410_5_4_420.jpg
cs-410_5_4_107,cs-410,5,4, Web Search,"00:07:19,544","00:07:24,230",107,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=439,is you will find the new,pic_cs-410_5_4_420.jpg
cs-410_5_4_108,cs-410,5,4, Web Search,"00:07:24,230","00:07:28,732",108,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=444,people probably are creating,pic_cs-410_5_4_420.jpg
cs-410_5_4_109,cs-410,5,4, Web Search,"00:07:28,732","00:07:33,502",109,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=448,And this is very challenging if,pic_cs-410_5_4_420.jpg
cs-410_5_4_110,cs-410,5,4, Web Search,"00:07:33,502","00:07:35,930",110,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=453,linked to any old pages.,pic_cs-410_5_4_420.jpg
cs-410_5_4_111,cs-410,5,4, Web Search,"00:07:35,930","00:07:41,655",111,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=455,"If they are, then you can probably find",pic_cs-410_5_4_420.jpg
cs-410_5_4_112,cs-410,5,4, Web Search,"00:07:41,655","00:07:46,946",112,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=461,so these are also some interesting,pic_cs-410_5_4_420.jpg
cs-410_5_4_113,cs-410,5,4, Web Search,"00:07:46,946","00:07:51,257",113,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=466,"And finally, we might face the scenario",pic_cs-410_5_4_420.jpg
cs-410_5_4_114,cs-410,5,4, Web Search,"00:07:51,257","00:07:53,157",114,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=471,"repeated crawling, right.",pic_cs-410_5_4_420.jpg
cs-410_5_4_115,cs-410,5,4, Web Search,"00:07:53,157","00:07:56,528",115,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=473,"Let's say,",pic_cs-410_5_4_420.jpg
cs-410_5_4_116,cs-410,5,4, Web Search,"00:07:56,528","00:07:59,448",116,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=476,and you first crawl a lot,pic_cs-410_5_4_420.jpg
cs-410_5_4_117,cs-410,5,4, Web Search,"00:07:59,448","00:08:03,816",117,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=479,"But then,",pic_cs-410_5_4_420.jpg
cs-410_5_4_118,cs-410,5,4, Web Search,"00:08:03,816","00:08:08,968",118,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=483,in the future you just need,pic_cs-410_5_4_480.jpg
cs-410_5_4_119,cs-410,5,4, Web Search,"00:08:08,968","00:08:13,277",119,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=488,"In general, you don't have to",pic_cs-410_5_4_480.jpg
cs-410_5_4_120,cs-410,5,4, Web Search,"00:08:13,277","00:08:14,960",120,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=493,It's not necessary.,pic_cs-410_5_4_480.jpg
cs-410_5_4_121,cs-410,5,4, Web Search,"00:08:16,650","00:08:21,563",121,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=496,"So in this case, your goal is to",pic_cs-410_5_4_480.jpg
cs-410_5_4_122,cs-410,5,4, Web Search,"00:08:21,563","00:08:26,400",122,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=501,by using minimum resources,pic_cs-410_5_4_480.jpg
cs-410_5_4_123,cs-410,5,4, Web Search,"00:08:27,490","00:08:33,986",123,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=507,"So, this is actually a very",pic_cs-410_5_4_480.jpg
cs-410_5_4_124,cs-410,5,4, Web Search,"00:08:33,986","00:08:40,250",124,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=513,"and this is a open research question,",pic_cs-410_5_4_480.jpg
cs-410_5_4_125,cs-410,5,4, Web Search,"00:08:40,250","00:08:46,060",125,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=520,standard algorithms established yet,pic_cs-410_5_4_480.jpg
cs-410_5_4_126,cs-410,5,4, Web Search,"00:08:47,300","00:08:51,300",126,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=527,"But in general, you can imagine,",pic_cs-410_5_4_480.jpg
cs-410_5_4_127,cs-410,5,4, Web Search,"00:08:53,640","00:08:57,040",127,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=533,So the two major factors that,pic_cs-410_5_4_480.jpg
cs-410_5_4_128,cs-410,5,4, Web Search,"00:08:57,040","00:09:00,760",128,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=537,first will this page,pic_cs-410_5_4_480.jpg
cs-410_5_4_129,cs-410,5,4, Web Search,"00:09:00,760","00:09:03,411",129,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=540,And do I have to quote this page again?,pic_cs-410_5_4_540.jpg
cs-410_5_4_130,cs-410,5,4, Web Search,"00:09:03,411","00:09:07,726",130,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=543,If the page is a static page and,pic_cs-410_5_4_540.jpg
cs-410_5_4_131,cs-410,5,4, Web Search,"00:09:07,726","00:09:12,703",131,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=547,you probably don't have to re-crawl it,pic_cs-410_5_4_540.jpg
cs-410_5_4_132,cs-410,5,4, Web Search,"00:09:12,703","00:09:14,401",132,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=552,will changed frequently.,pic_cs-410_5_4_540.jpg
cs-410_5_4_133,cs-410,5,4, Web Search,"00:09:14,401","00:09:20,152",133,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=554,"On the other hand, if it's a sports score",pic_cs-410_5_4_540.jpg
cs-410_5_4_134,cs-410,5,4, Web Search,"00:09:20,152","00:09:25,840",134,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=560,you may need to re-crawl it and,pic_cs-410_5_4_540.jpg
cs-410_5_4_135,cs-410,5,4, Web Search,"00:09:25,840","00:09:30,956",135,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=565,"The other factor to consider is,",pic_cs-410_5_4_540.jpg
cs-410_5_4_136,cs-410,5,4, Web Search,"00:09:30,956","00:09:35,485",136,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=570,"If it is, then it means that",pic_cs-410_5_4_540.jpg
cs-410_5_4_137,cs-410,5,4, Web Search,"00:09:35,485","00:09:40,809",137,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=575,then thus it's more important to,pic_cs-410_5_4_540.jpg
cs-410_5_4_138,cs-410,5,4, Web Search,"00:09:40,809","00:09:45,439",138,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=580,Compared with another page that has,pic_cs-410_5_4_540.jpg
cs-410_5_4_139,cs-410,5,4, Web Search,"00:09:45,439","00:09:49,609",139,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=585,"a year, then even though that",pic_cs-410_5_4_540.jpg
cs-410_5_4_140,cs-410,5,4, Web Search,"00:09:49,609","00:09:55,164",140,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=589,It's probably not that necessary to,pic_cs-410_5_4_540.jpg
cs-410_5_4_141,cs-410,5,4, Web Search,"00:09:55,164","00:10:01,697",141,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=595,not as urgent as to maintain the freshness,pic_cs-410_5_4_540.jpg
cs-410_5_4_142,cs-410,5,4, Web Search,"00:10:01,697","00:10:05,275",142,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=601,"So to summarize, web search is one of",pic_cs-410_5_4_600.jpg
cs-410_5_4_143,cs-410,5,4, Web Search,"00:10:05,275","00:10:08,689",143,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=605,retrieval and there are some new,pic_cs-410_5_4_600.jpg
cs-410_5_4_144,cs-410,5,4, Web Search,"00:10:08,689","00:10:10,463",144,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=608,"efficiency, quality information.",pic_cs-410_5_4_600.jpg
cs-410_5_4_145,cs-410,5,4, Web Search,"00:10:10,463","00:10:15,671",145,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=610,There are also new opportunities,pic_cs-410_5_4_600.jpg
cs-410_5_4_146,cs-410,5,4, Web Search,"00:10:15,671","00:10:16,765",146,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=615,"layout, etc.",pic_cs-410_5_4_600.jpg
cs-410_5_4_147,cs-410,5,4, Web Search,"00:10:17,890","00:10:22,500",147,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=617,A crawler is an essential component,pic_cs-410_5_4_600.jpg
cs-410_5_4_148,cs-410,5,4, Web Search,"00:10:22,500","00:10:24,360",148,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=622,"in general, you can find two scenarios.",pic_cs-410_5_4_600.jpg
cs-410_5_4_149,cs-410,5,4, Web Search,"00:10:24,360","00:10:28,730",149,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=624,One is initial crawling and,pic_cs-410_5_4_600.jpg
cs-410_5_4_150,cs-410,5,4, Web Search,"00:10:30,100","00:10:32,970",150,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=630,of the web if you are doing,pic_cs-410_5_4_600.jpg
cs-410_5_4_151,cs-410,5,4, Web Search,"00:10:32,970","00:10:37,560",151,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=632,focused crawling if you want to just,pic_cs-410_5_4_600.jpg
cs-410_5_4_152,cs-410,5,4, Web Search,"00:10:38,610","00:10:43,262",152,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=638,"And then, there is another scenario that's",pic_cs-410_5_4_600.jpg
cs-410_5_4_153,cs-410,5,4, Web Search,"00:10:43,262","00:10:44,611",153,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=643,incremental crawling.,pic_cs-410_5_4_600.jpg
cs-410_5_4_154,cs-410,5,4, Web Search,"00:10:44,611","00:10:48,692",154,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=644,"In this case,",pic_cs-410_5_4_600.jpg
cs-410_5_4_155,cs-410,5,4, Web Search,"00:10:48,692","00:10:52,588",155,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=648,try to use minimum resource,pic_cs-410_5_4_600.jpg
cs-410_5_4_156,cs-410,5,4, Web Search,"00:10:54,486","00:11:04,486",156,https://www.coursera.org/learn/cs-410/lecture/qkTHD?t=654,[MUSIC],pic_cs-410_5_4_600.jpg
cs-410_5_5_1,cs-410,5,5, Web Indexing,"00:00:00,000","00:00:03,894",1,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=0,[SOUND],pic_cs-410_5_5_0.jpg
cs-410_5_5_2,cs-410,5,5, Web Indexing,"00:00:07,481","00:00:09,920",2,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=7,This lecture is about the Web Indexing.,pic_cs-410_5_5_0.jpg
cs-410_5_5_3,cs-410,5,5, Web Indexing,"00:00:11,980","00:00:16,740",3,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=11,"In this lecture, we will continue",pic_cs-410_5_5_0.jpg
cs-410_5_5_4,cs-410,5,5, Web Indexing,"00:00:16,740","00:00:20,741",4,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=16,we're going to talk about how,pic_cs-410_5_5_0.jpg
cs-410_5_5_5,cs-410,5,5, Web Indexing,"00:00:24,457","00:00:29,720",5,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=24,"So once we crawl the web,",pic_cs-410_5_5_0.jpg
cs-410_5_5_6,cs-410,5,5, Web Indexing,"00:00:29,720","00:00:33,489",6,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=29,The next step is to use the indexer,pic_cs-410_5_5_0.jpg
cs-410_5_5_7,cs-410,5,5, Web Indexing,"00:00:36,540","00:00:41,150",7,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=36,"In general, we can use the same",pic_cs-410_5_5_0.jpg
cs-410_5_5_8,cs-410,5,5, Web Indexing,"00:00:41,150","00:00:45,060",8,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=41,creating an index and that is what we,pic_cs-410_5_5_0.jpg
cs-410_5_5_9,cs-410,5,5, Web Indexing,"00:00:45,060","00:00:48,718",9,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=45,but there are there are new,pic_cs-410_5_5_0.jpg
cs-410_5_5_10,cs-410,5,5, Web Indexing,"00:00:48,718","00:00:55,100",10,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=48,"For web scale indexing, and the two main",pic_cs-410_5_5_0.jpg
cs-410_5_5_11,cs-410,5,5, Web Indexing,"00:00:55,100","00:00:57,450",11,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=55,"The index would be so large,",pic_cs-410_5_5_0.jpg
cs-410_5_5_12,cs-410,5,5, Web Indexing,"00:00:57,450","00:01:03,220",12,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=57,that it cannot actually fit into,pic_cs-410_5_5_0.jpg
cs-410_5_5_13,cs-410,5,5, Web Indexing,"00:01:03,220","00:01:05,879",13,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=63,So we have to store the data,pic_cs-410_5_5_60.jpg
cs-410_5_5_14,cs-410,5,5, Web Indexing,"00:01:06,910","00:01:10,900",14,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=66,"Also, because the data is so",pic_cs-410_5_5_60.jpg
cs-410_5_5_15,cs-410,5,5, Web Indexing,"00:01:10,900","00:01:15,700",15,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=70,"process the data in parallel, so",pic_cs-410_5_5_60.jpg
cs-410_5_5_16,cs-410,5,5, Web Indexing,"00:01:15,700","00:01:20,410",16,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=75,"Now to address these challenges,",pic_cs-410_5_5_60.jpg
cs-410_5_5_17,cs-410,5,5, Web Indexing,"00:01:20,410","00:01:25,430",17,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=80,One is the Google File System that's,pic_cs-410_5_5_60.jpg
cs-410_5_5_18,cs-410,5,5, Web Indexing,"00:01:25,430","00:01:30,900",18,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=85,programmers manage files stored,pic_cs-410_5_5_60.jpg
cs-410_5_5_19,cs-410,5,5, Web Indexing,"00:01:32,000","00:01:33,159",19,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=92,The second is MapReduce.,pic_cs-410_5_5_60.jpg
cs-410_5_5_20,cs-410,5,5, Web Indexing,"00:01:33,159","00:01:37,140",20,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=93,This is a general software framework for,pic_cs-410_5_5_60.jpg
cs-410_5_5_21,cs-410,5,5, Web Indexing,"00:01:38,960","00:01:44,830",21,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=98,Hadoop is the most well known open,pic_cs-410_5_5_60.jpg
cs-410_5_5_22,cs-410,5,5, Web Indexing,"00:01:44,830","00:01:47,790",22,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=104,Now used in many applications.,pic_cs-410_5_5_60.jpg
cs-410_5_5_23,cs-410,5,5, Web Indexing,"00:01:50,000","00:01:52,510",23,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=110,"So, this is the architecture",pic_cs-410_5_5_60.jpg
cs-410_5_5_24,cs-410,5,5, Web Indexing,"00:01:53,790","00:01:56,930",24,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=113,It uses a very simple centralized,pic_cs-410_5_5_60.jpg
cs-410_5_5_25,cs-410,5,5, Web Indexing,"00:01:56,930","00:02:01,210",25,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=116,management mechanism to manage,pic_cs-410_5_5_60.jpg
cs-410_5_5_26,cs-410,5,5, Web Indexing,"00:02:01,210","00:02:05,590",26,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=121,"Files, so",pic_cs-410_5_5_120.jpg
cs-410_5_5_27,cs-410,5,5, Web Indexing,"00:02:05,590","00:02:09,790",27,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=125,look up a table to know where,pic_cs-410_5_5_120.jpg
cs-410_5_5_28,cs-410,5,5, Web Indexing,"00:02:11,040","00:02:16,250",28,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=131,The application client will then,pic_cs-410_5_5_120.jpg
cs-410_5_5_29,cs-410,5,5, Web Indexing,"00:02:16,250","00:02:21,420",29,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=136,that obtains specific locations of,pic_cs-410_5_5_120.jpg
cs-410_5_5_30,cs-410,5,5, Web Indexing,"00:02:22,890","00:02:31,450",30,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=142,And once the GFS file kind obtained,pic_cs-410_5_5_120.jpg
cs-410_5_5_31,cs-410,5,5, Web Indexing,"00:02:31,450","00:02:37,880",31,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=151,then the application client can talk,pic_cs-410_5_5_120.jpg
cs-410_5_5_32,cs-410,5,5, Web Indexing,"00:02:37,880","00:02:43,230",32,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=157,"data actually sits directly, so",pic_cs-410_5_5_120.jpg
cs-410_5_5_33,cs-410,5,5, Web Indexing,"00:02:43,230","00:02:43,970",33,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=163,In the network.,pic_cs-410_5_5_120.jpg
cs-410_5_5_34,cs-410,5,5, Web Indexing,"00:02:46,020","00:02:53,290",34,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=166,So when this file system stores,pic_cs-410_5_5_120.jpg
cs-410_5_5_35,cs-410,5,5, Web Indexing,"00:02:53,290","00:02:59,650",35,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=173,"with great fixed sizes of chunks, so",pic_cs-410_5_5_120.jpg
cs-410_5_5_36,cs-410,5,5, Web Indexing,"00:03:00,720","00:03:01,460",36,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=180,Many chunks.,pic_cs-410_5_5_180.jpg
cs-410_5_5_37,cs-410,5,5, Web Indexing,"00:03:01,460","00:03:05,120",37,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=181,"Each chunk is 64 MB, so it's pretty big.",pic_cs-410_5_5_180.jpg
cs-410_5_5_38,cs-410,5,5, Web Indexing,"00:03:05,120","00:03:09,080",38,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=185,And that's appropriate for,pic_cs-410_5_5_180.jpg
cs-410_5_5_39,cs-410,5,5, Web Indexing,"00:03:09,080","00:03:12,510",39,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=189,These chunks are replicated,pic_cs-410_5_5_180.jpg
cs-410_5_5_40,cs-410,5,5, Web Indexing,"00:03:12,510","00:03:17,210",40,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=192,So this is something that the programmer,pic_cs-410_5_5_180.jpg
cs-410_5_5_41,cs-410,5,5, Web Indexing,"00:03:17,210","00:03:22,210",41,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=197,and it's all taken care,pic_cs-410_5_5_180.jpg
cs-410_5_5_42,cs-410,5,5, Web Indexing,"00:03:22,210","00:03:24,110",42,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=202,"So from the application perspective,",pic_cs-410_5_5_180.jpg
cs-410_5_5_43,cs-410,5,5, Web Indexing,"00:03:24,110","00:03:28,250",43,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=204,the programmer would see this,pic_cs-410_5_5_180.jpg
cs-410_5_5_44,cs-410,5,5, Web Indexing,"00:03:28,250","00:03:32,510",44,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=208,And the programmer doesn't have to,pic_cs-410_5_5_180.jpg
cs-410_5_5_45,cs-410,5,5, Web Indexing,"00:03:32,510","00:03:35,535",45,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=212,can just invoke high level.,pic_cs-410_5_5_180.jpg
cs-410_5_5_46,cs-410,5,5, Web Indexing,"00:03:35,535","00:03:38,275",46,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=215,Operators to process the file.,pic_cs-410_5_5_180.jpg
cs-410_5_5_47,cs-410,5,5, Web Indexing,"00:03:39,975","00:03:44,915",47,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=219,And another feature is that the data,pic_cs-410_5_5_180.jpg
cs-410_5_5_48,cs-410,5,5, Web Indexing,"00:03:44,915","00:03:45,865",48,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=224,and chunk servers.,pic_cs-410_5_5_180.jpg
cs-410_5_5_49,cs-410,5,5, Web Indexing,"00:03:45,865","00:03:48,735",49,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=225,So it's efficient in this sense.,pic_cs-410_5_5_180.jpg
cs-410_5_5_50,cs-410,5,5, Web Indexing,"00:03:51,190","00:03:54,590",50,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=231,"On top of the Google file system, Google",pic_cs-410_5_5_180.jpg
cs-410_5_5_51,cs-410,5,5, Web Indexing,"00:03:54,590","00:03:59,220",51,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=234,also proposed MapReduce as a general,pic_cs-410_5_5_180.jpg
cs-410_5_5_52,cs-410,5,5, Web Indexing,"00:03:59,220","00:04:05,660",52,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=239,"Now, this is very useful to support",pic_cs-410_5_5_180.jpg
cs-410_5_5_53,cs-410,5,5, Web Indexing,"00:04:06,670","00:04:10,618",53,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=246,"And so, this framework is,",pic_cs-410_5_5_240.jpg
cs-410_5_5_54,cs-410,5,5, Web Indexing,"00:04:12,116","00:04:16,170",54,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=252,Hiding a lot of low-level,pic_cs-410_5_5_240.jpg
cs-410_5_5_55,cs-410,5,5, Web Indexing,"00:04:16,170","00:04:21,950",55,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=256,"As a result, the programmer can make",pic_cs-410_5_5_240.jpg
cs-410_5_5_56,cs-410,5,5, Web Indexing,"00:04:21,950","00:04:26,580",56,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=261,that can be run a large,pic_cs-410_5_5_240.jpg
cs-410_5_5_57,cs-410,5,5, Web Indexing,"00:04:28,990","00:04:33,930",57,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=268,So some of the low level details,pic_cs-410_5_5_240.jpg
cs-410_5_5_58,cs-410,5,5, Web Indexing,"00:04:33,930","00:04:39,410",58,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=273,the specific and network communications or,pic_cs-410_5_5_240.jpg
cs-410_5_5_59,cs-410,5,5, Web Indexing,"00:04:39,410","00:04:44,080",59,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=279,where the task are executed.,pic_cs-410_5_5_240.jpg
cs-410_5_5_60,cs-410,5,5, Web Indexing,"00:04:44,080","00:04:46,600",60,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=284,All these details are hidden,pic_cs-410_5_5_240.jpg
cs-410_5_5_61,cs-410,5,5, Web Indexing,"00:04:47,880","00:04:52,560",61,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=287,There is also a nice feature which,pic_cs-410_5_5_240.jpg
cs-410_5_5_62,cs-410,5,5, Web Indexing,"00:04:52,560","00:04:56,490",62,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=292,"If one server is broken,",pic_cs-410_5_5_240.jpg
cs-410_5_5_63,cs-410,5,5, Web Indexing,"00:04:56,490","00:05:01,140",63,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=296,"the server is down, and",pic_cs-410_5_5_240.jpg
cs-410_5_5_64,cs-410,5,5, Web Indexing,"00:05:01,140","00:05:05,300",64,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=301,Then the MapReduce mapper will know,pic_cs-410_5_5_300.jpg
cs-410_5_5_65,cs-410,5,5, Web Indexing,"00:05:05,300","00:05:11,600",65,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=305,So it automatically dispatches a task,pic_cs-410_5_5_300.jpg
cs-410_5_5_66,cs-410,5,5, Web Indexing,"00:05:11,600","00:05:15,400",66,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=311,"And therefore, again the program",pic_cs-410_5_5_300.jpg
cs-410_5_5_67,cs-410,5,5, Web Indexing,"00:05:15,400","00:05:17,570",67,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=315,here's how MapReduce works.,pic_cs-410_5_5_300.jpg
cs-410_5_5_68,cs-410,5,5, Web Indexing,"00:05:17,570","00:05:23,330",68,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=317,The input data would be separated,pic_cs-410_5_5_300.jpg
cs-410_5_5_69,cs-410,5,5, Web Indexing,"00:05:23,330","00:05:26,460",69,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=323,Now what exactly is in the value,pic_cs-410_5_5_300.jpg
cs-410_5_5_70,cs-410,5,5, Web Indexing,"00:05:26,460","00:05:31,520",70,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=326,it's actually a fairly general framework,pic_cs-410_5_5_300.jpg
cs-410_5_5_71,cs-410,5,5, Web Indexing,"00:05:31,520","00:05:35,450",71,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=331,into different parts and each part,pic_cs-410_5_5_300.jpg
cs-410_5_5_72,cs-410,5,5, Web Indexing,"00:05:37,100","00:05:40,984",72,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=337,Each key value pair would be and,pic_cs-410_5_5_300.jpg
cs-410_5_5_73,cs-410,5,5, Web Indexing,"00:05:40,984","00:05:44,750",73,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=340,"The program was right the map function,",pic_cs-410_5_5_300.jpg
cs-410_5_5_74,cs-410,5,5, Web Indexing,"00:05:45,890","00:05:50,043",74,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=345,And then the map function will,pic_cs-410_5_5_300.jpg
cs-410_5_5_75,cs-410,5,5, Web Indexing,"00:05:50,043","00:05:53,870",75,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=350,then generate a number of,pic_cs-410_5_5_300.jpg
cs-410_5_5_76,cs-410,5,5, Web Indexing,"00:05:53,870","00:05:58,370",76,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=353,"Of course, the new key is usually",pic_cs-410_5_5_300.jpg
cs-410_5_5_77,cs-410,5,5, Web Indexing,"00:05:58,370","00:06:02,070",77,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=358,that's given to the map as input.,pic_cs-410_5_5_300.jpg
cs-410_5_5_78,cs-410,5,5, Web Indexing,"00:06:02,070","00:06:06,000",78,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=362,And these key value pairs,pic_cs-410_5_5_360.jpg
cs-410_5_5_79,cs-410,5,5, Web Indexing,"00:06:06,000","00:06:10,420",79,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=366,all the outputs of all the map,pic_cs-410_5_5_360.jpg
cs-410_5_5_80,cs-410,5,5, Web Indexing,"00:06:12,540","00:06:16,670",80,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=372,and then there will be for,pic_cs-410_5_5_360.jpg
cs-410_5_5_81,cs-410,5,5, Web Indexing,"00:06:16,670","00:06:20,600",81,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=376,"And the result is that,",pic_cs-410_5_5_360.jpg
cs-410_5_5_82,cs-410,5,5, Web Indexing,"00:06:20,600","00:06:24,260",82,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=380,with the same key will be,pic_cs-410_5_5_360.jpg
cs-410_5_5_83,cs-410,5,5, Web Indexing,"00:06:24,260","00:06:30,480",83,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=384,So now we've got a pair of of a key and,pic_cs-410_5_5_360.jpg
cs-410_5_5_84,cs-410,5,5, Web Indexing,"00:06:31,630","00:06:34,960",84,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=391,So this would then be sent,pic_cs-410_5_5_360.jpg
cs-410_5_5_85,cs-410,5,5, Web Indexing,"00:06:36,330","00:06:41,330",85,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=396,"Now, of course, each reduce function",pic_cs-410_5_5_360.jpg
cs-410_5_5_86,cs-410,5,5, Web Indexing,"00:06:41,330","00:06:45,990",86,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=401,so we will send these output values to,pic_cs-410_5_5_360.jpg
cs-410_5_5_87,cs-410,5,5, Web Indexing,"00:06:45,990","00:06:50,580",87,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=405,multiple reduce functions,pic_cs-410_5_5_360.jpg
cs-410_5_5_88,cs-410,5,5, Web Indexing,"00:06:52,220","00:06:57,980",88,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=412,A reduce function would then,pic_cs-410_5_5_360.jpg
cs-410_5_5_89,cs-410,5,5, Web Indexing,"00:06:57,980","00:07:04,920",89,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=417,a key in a set of values to produce,pic_cs-410_5_5_360.jpg
cs-410_5_5_90,cs-410,5,5, Web Indexing,"00:07:04,920","00:07:08,670",90,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=424,So these output values would,pic_cs-410_5_5_420.jpg
cs-410_5_5_91,cs-410,5,5, Web Indexing,"00:07:08,670","00:07:11,220",91,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=428,to form the final output.,pic_cs-410_5_5_420.jpg
cs-410_5_5_92,cs-410,5,5, Web Indexing,"00:07:12,420","00:07:17,210",92,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=432,"And so, this is the general",pic_cs-410_5_5_420.jpg
cs-410_5_5_93,cs-410,5,5, Web Indexing,"00:07:17,210","00:07:23,290",93,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=437,Now the programmer only needs to write,pic_cs-410_5_5_420.jpg
cs-410_5_5_94,cs-410,5,5, Web Indexing,"00:07:23,290","00:07:28,090",94,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=443,Everything else is actually taken,pic_cs-410_5_5_420.jpg
cs-410_5_5_95,cs-410,5,5, Web Indexing,"00:07:28,090","00:07:32,920",95,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=448,So you can see the program really,pic_cs-410_5_5_420.jpg
cs-410_5_5_96,cs-410,5,5, Web Indexing,"00:07:32,920","00:07:38,570",96,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=452,"And with such a framework, the input data",pic_cs-410_5_5_420.jpg
cs-410_5_5_97,cs-410,5,5, Web Indexing,"00:07:38,570","00:07:42,780",97,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=458,"which is processing parallel first by map,",pic_cs-410_5_5_420.jpg
cs-410_5_5_98,cs-410,5,5, Web Indexing,"00:07:42,780","00:07:50,130",98,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=462,then being the process after,pic_cs-410_5_5_420.jpg
cs-410_5_5_99,cs-410,5,5, Web Indexing,"00:07:50,130","00:07:54,340",99,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=470,The much more reduced if I'm,pic_cs-410_5_5_420.jpg
cs-410_5_5_100,cs-410,5,5, Web Indexing,"00:07:55,720","00:08:00,390",100,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=475,the different keys and,pic_cs-410_5_5_420.jpg
cs-410_5_5_101,cs-410,5,5, Web Indexing,"00:08:00,390","00:08:02,980",101,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=480,"So it achieves some,",pic_cs-410_5_5_480.jpg
cs-410_5_5_102,cs-410,5,5, Web Indexing,"00:08:05,410","00:08:10,510",102,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=485,it achieves the purpose of parallel,pic_cs-410_5_5_480.jpg
cs-410_5_5_103,cs-410,5,5, Web Indexing,"00:08:10,510","00:08:13,620",103,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=490,So let's take a look at a simple example.,pic_cs-410_5_5_480.jpg
cs-410_5_5_104,cs-410,5,5, Web Indexing,"00:08:13,620","00:08:15,040",104,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=493,And that's Word Counting.,pic_cs-410_5_5_480.jpg
cs-410_5_5_105,cs-410,5,5, Web Indexing,"00:08:16,620","00:08:21,570",105,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=496,"The input is containing words,",pic_cs-410_5_5_480.jpg
cs-410_5_5_106,cs-410,5,5, Web Indexing,"00:08:21,570","00:08:25,990",106,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=501,and the output that we want to generate is,pic_cs-410_5_5_480.jpg
cs-410_5_5_107,cs-410,5,5, Web Indexing,"00:08:25,990","00:08:27,080",107,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=505,So it's the Word Count.,pic_cs-410_5_5_480.jpg
cs-410_5_5_108,cs-410,5,5, Web Indexing,"00:08:28,270","00:08:32,940",108,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=508,We know this kind of counting,pic_cs-410_5_5_480.jpg
cs-410_5_5_109,cs-410,5,5, Web Indexing,"00:08:32,940","00:08:38,290",109,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=512,assess the popularity of a word in,pic_cs-410_5_5_480.jpg
cs-410_5_5_110,cs-410,5,5, Web Indexing,"00:08:38,290","00:08:41,880",110,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=518,achieving a factor of IDF wading for,pic_cs-410_5_5_480.jpg
cs-410_5_5_111,cs-410,5,5, Web Indexing,"00:08:42,880","00:08:44,200",111,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=522,So how can we solve this problem?,pic_cs-410_5_5_480.jpg
cs-410_5_5_112,cs-410,5,5, Web Indexing,"00:08:44,200","00:08:49,200",112,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=524,"Well, one natural thought is that,",pic_cs-410_5_5_480.jpg
cs-410_5_5_113,cs-410,5,5, Web Indexing,"00:08:49,200","00:08:53,860",113,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=529,done in parallel by simply counting,pic_cs-410_5_5_480.jpg
cs-410_5_5_114,cs-410,5,5, Web Indexing,"00:08:53,860","00:08:57,000",114,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=533,and then in the end we just,pic_cs-410_5_5_480.jpg
cs-410_5_5_115,cs-410,5,5, Web Indexing,"00:08:57,000","00:09:01,800",115,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=537,And that's precisely the idea of,pic_cs-410_5_5_480.jpg
cs-410_5_5_116,cs-410,5,5, Web Indexing,"00:09:02,900","00:09:06,440",116,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=542,We can parallelize on,pic_cs-410_5_5_540.jpg
cs-410_5_5_117,cs-410,5,5, Web Indexing,"00:09:07,670","00:09:13,100",117,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=547,"So more specifically, we can assume",pic_cs-410_5_5_540.jpg
cs-410_5_5_118,cs-410,5,5, Web Indexing,"00:09:14,120","00:09:20,450",118,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=554,a key value pair that represents the line,pic_cs-410_5_5_540.jpg
cs-410_5_5_119,cs-410,5,5, Web Indexing,"00:09:20,450","00:09:25,760",119,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=560,"So the first line, for",pic_cs-410_5_5_540.jpg
cs-410_5_5_120,cs-410,5,5, Web Indexing,"00:09:25,760","00:09:32,240",120,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=565,that is another word by word and,pic_cs-410_5_5_540.jpg
cs-410_5_5_121,cs-410,5,5, Web Indexing,"00:09:32,240","00:09:36,250",121,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=572,So this key value pair would,pic_cs-410_5_5_540.jpg
cs-410_5_5_122,cs-410,5,5, Web Indexing,"00:09:36,250","00:09:40,670",122,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=576,The Map Function then would just,pic_cs-410_5_5_540.jpg
cs-410_5_5_123,cs-410,5,5, Web Indexing,"00:09:41,700","00:09:43,880",123,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=581,"And in this case,",pic_cs-410_5_5_540.jpg
cs-410_5_5_124,cs-410,5,5, Web Indexing,"00:09:43,880","00:09:46,360",124,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=583,Each world gets a count of one and,pic_cs-410_5_5_540.jpg
cs-410_5_5_125,cs-410,5,5, Web Indexing,"00:09:46,360","00:09:52,770",125,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=586,these are the output that you see here,pic_cs-410_5_5_540.jpg
cs-410_5_5_126,cs-410,5,5, Web Indexing,"00:09:52,770","00:09:56,270",126,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=592,So the map function is really,pic_cs-410_5_5_540.jpg
cs-410_5_5_127,cs-410,5,5, Web Indexing,"00:09:56,270","00:10:00,450",127,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=596,what the pseudocode looks,pic_cs-410_5_5_540.jpg
cs-410_5_5_128,cs-410,5,5, Web Indexing,"00:10:00,450","00:10:05,370",128,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=600,you see it simply needs to iterate,pic_cs-410_5_5_600.jpg
cs-410_5_5_129,cs-410,5,5, Web Indexing,"00:10:05,370","00:10:08,330",129,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=605,And then just collect the function,pic_cs-410_5_5_600.jpg
cs-410_5_5_130,cs-410,5,5, Web Indexing,"00:10:09,390","00:10:14,080",130,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=609,which means it would then send the word,pic_cs-410_5_5_600.jpg
cs-410_5_5_131,cs-410,5,5, Web Indexing,"00:10:14,080","00:10:18,686",131,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=614,The collector would then try to,pic_cs-410_5_5_600.jpg
cs-410_5_5_132,cs-410,5,5, Web Indexing,"00:10:18,686","00:10:21,205",132,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=618,"different Map Functions, right?",pic_cs-410_5_5_600.jpg
cs-410_5_5_133,cs-410,5,5, Web Indexing,"00:10:21,205","00:10:25,937",133,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=621,So the function is very simple and,pic_cs-410_5_5_600.jpg
cs-410_5_5_134,cs-410,5,5, Web Indexing,"00:10:25,937","00:10:30,300",134,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=625,this function as a way to,pic_cs-410_5_5_600.jpg
cs-410_5_5_135,cs-410,5,5, Web Indexing,"00:10:31,620","00:10:34,780",135,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=631,"Of course, the second line will be",pic_cs-410_5_5_600.jpg
cs-410_5_5_136,cs-410,5,5, Web Indexing,"00:10:34,780","00:10:36,990",136,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=634,which we will produce a single output.,pic_cs-410_5_5_600.jpg
cs-410_5_5_137,cs-410,5,5, Web Indexing,"00:10:36,990","00:10:40,800",137,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=636,"Okay, now the output from the map",pic_cs-410_5_5_600.jpg
cs-410_5_5_138,cs-410,5,5, Web Indexing,"00:10:40,800","00:10:45,550",138,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=640,send it to a collector and the collector,pic_cs-410_5_5_600.jpg
cs-410_5_5_139,cs-410,5,5, Web Indexing,"00:10:45,550","00:10:50,220",139,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=645,"So at this stage, you can see,",pic_cs-410_5_5_600.jpg
cs-410_5_5_140,cs-410,5,5, Web Indexing,"00:10:50,220","00:10:53,850",140,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=650,Each pair is a word and,pic_cs-410_5_5_600.jpg
cs-410_5_5_141,cs-410,5,5, Web Indexing,"00:10:53,850","00:10:58,960",141,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=653,"So, once we see all these pairs.",pic_cs-410_5_5_600.jpg
cs-410_5_5_142,cs-410,5,5, Web Indexing,"00:10:58,960","00:11:03,570",142,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=658,"Then we can sort them based on the key,",pic_cs-410_5_5_600.jpg
cs-410_5_5_143,cs-410,5,5, Web Indexing,"00:11:03,570","00:11:08,570",143,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=663,So we will collect all the counts,pic_cs-410_5_5_660.jpg
cs-410_5_5_144,cs-410,5,5, Web Indexing,"00:11:09,610","00:11:11,790",144,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=669,"And similarly, we do that for other words.",pic_cs-410_5_5_660.jpg
cs-410_5_5_145,cs-410,5,5, Web Indexing,"00:11:11,790","00:11:13,620",145,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=671,"Like Hadoop, Hello, etc.",pic_cs-410_5_5_660.jpg
cs-410_5_5_146,cs-410,5,5, Web Indexing,"00:11:13,620","00:11:19,040",146,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=673,So each word now is attached to,pic_cs-410_5_5_660.jpg
cs-410_5_5_147,cs-410,5,5, Web Indexing,"00:11:20,700","00:11:27,860",147,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=680,And these counts represent the occurrences,pic_cs-410_5_5_660.jpg
cs-410_5_5_148,cs-410,5,5, Web Indexing,"00:11:27,860","00:11:33,330",148,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=687,So now we have got a new pair of a key and,pic_cs-410_5_5_660.jpg
cs-410_5_5_149,cs-410,5,5, Web Indexing,"00:11:33,330","00:11:38,610",149,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=693,this pair will then be fed into reduce,pic_cs-410_5_5_660.jpg
cs-410_5_5_150,cs-410,5,5, Web Indexing,"00:11:38,610","00:11:44,450",150,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=698,would have to finish the job of counting,pic_cs-410_5_5_660.jpg
cs-410_5_5_151,cs-410,5,5, Web Indexing,"00:11:44,450","00:11:47,020",151,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=704,"Now, it has all ready got all",pic_cs-410_5_5_660.jpg
cs-410_5_5_152,cs-410,5,5, Web Indexing,"00:11:47,020","00:11:50,370",152,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=707,all it needs to do is,pic_cs-410_5_5_660.jpg
cs-410_5_5_153,cs-410,5,5, Web Indexing,"00:11:50,370","00:11:53,810",153,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=710,So the reduce function here,pic_cs-410_5_5_660.jpg
cs-410_5_5_154,cs-410,5,5, Web Indexing,"00:11:53,810","00:11:57,130",154,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=713,"You have a counter, and",pic_cs-410_5_5_660.jpg
cs-410_5_5_155,cs-410,5,5, Web Indexing,"00:11:57,130","00:11:59,260",155,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=717,That you'll see in this array.,pic_cs-410_5_5_660.jpg
cs-410_5_5_156,cs-410,5,5, Web Indexing,"00:11:59,260","00:12:02,884",156,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=719,"And that,",pic_cs-410_5_5_660.jpg
cs-410_5_5_157,cs-410,5,5, Web Indexing,"00:12:02,884","00:12:07,203",157,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=722,"And then finally, you output the P and",pic_cs-410_5_5_720.jpg
cs-410_5_5_158,cs-410,5,5, Web Indexing,"00:12:07,203","00:12:11,140",158,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=727,And that's precisely what we want as,pic_cs-410_5_5_720.jpg
cs-410_5_5_159,cs-410,5,5, Web Indexing,"00:12:12,220","00:12:14,830",159,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=732,"So you can see,",pic_cs-410_5_5_720.jpg
cs-410_5_5_160,cs-410,5,5, Web Indexing,"00:12:14,830","00:12:16,842",160,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=734,To building an Invert index.,pic_cs-410_5_5_720.jpg
cs-410_5_5_161,cs-410,5,5, Web Indexing,"00:12:16,842","00:12:21,050",161,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=736,"And if you think about it,",pic_cs-410_5_5_720.jpg
cs-410_5_5_162,cs-410,5,5, Web Indexing,"00:12:21,050","00:12:24,410",162,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=741,"And we have already got a dictionary,",pic_cs-410_5_5_720.jpg
cs-410_5_5_163,cs-410,5,5, Web Indexing,"00:12:24,410","00:12:26,440",163,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=744,We have got the count.,pic_cs-410_5_5_720.jpg
cs-410_5_5_164,cs-410,5,5, Web Indexing,"00:12:26,440","00:12:32,776",164,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=746,But what's missing is,pic_cs-410_5_5_720.jpg
cs-410_5_5_165,cs-410,5,5, Web Indexing,"00:12:32,776","00:12:38,240",165,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=752,frequency counts of words,pic_cs-410_5_5_720.jpg
cs-410_5_5_166,cs-410,5,5, Web Indexing,"00:12:38,240","00:12:43,420",166,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=758,So we can modify this slightly to,pic_cs-410_5_5_720.jpg
cs-410_5_5_167,cs-410,5,5, Web Indexing,"00:12:43,420","00:12:45,800",167,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=763,here's one way to do that.,pic_cs-410_5_5_720.jpg
cs-410_5_5_168,cs-410,5,5, Web Indexing,"00:12:45,800","00:12:51,490",168,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=765,"So in this case, we can assume the input",pic_cs-410_5_5_720.jpg
cs-410_5_5_169,cs-410,5,5, Web Indexing,"00:12:51,490","00:12:56,510",169,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=771,"which denotes the document ID,",pic_cs-410_5_5_720.jpg
cs-410_5_5_170,cs-410,5,5, Web Indexing,"00:12:56,510","00:13:02,420",170,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=776,"denoting the screen for that document,",pic_cs-410_5_5_720.jpg
cs-410_5_5_171,cs-410,5,5, Web Indexing,"00:13:02,420","00:13:05,740",171,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=782,"And so, the map function would do",pic_cs-410_5_5_780.jpg
cs-410_5_5_172,cs-410,5,5, Web Indexing,"00:13:05,740","00:13:07,910",172,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=785,seen in the word campaign example.,pic_cs-410_5_5_780.jpg
cs-410_5_5_173,cs-410,5,5, Web Indexing,"00:13:07,910","00:13:14,640",173,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=787,It simply groups all the counts of,pic_cs-410_5_5_780.jpg
cs-410_5_5_174,cs-410,5,5, Web Indexing,"00:13:14,640","00:13:18,010",174,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=794,And it would then generate,pic_cs-410_5_5_780.jpg
cs-410_5_5_175,cs-410,5,5, Web Indexing,"00:13:18,010","00:13:21,140",175,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=798,"Each key is a word, and",pic_cs-410_5_5_780.jpg
cs-410_5_5_176,cs-410,5,5, Web Indexing,"00:13:21,140","00:13:27,650",176,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=801,the value is the count of this word in,pic_cs-410_5_5_780.jpg
cs-410_5_5_177,cs-410,5,5, Web Indexing,"00:13:27,650","00:13:32,640",177,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=807,"Now, you can easily see why we need to",pic_cs-410_5_5_780.jpg
cs-410_5_5_178,cs-410,5,5, Web Indexing,"00:13:32,640","00:13:36,690",178,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=812,"in inverted index, we would like to",pic_cs-410_5_5_780.jpg
cs-410_5_5_179,cs-410,5,5, Web Indexing,"00:13:36,690","00:13:41,290",179,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=816,"should keep track of it, and this can then",pic_cs-410_5_5_780.jpg
cs-410_5_5_180,cs-410,5,5, Web Indexing,"00:13:41,290","00:13:46,710",180,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=821,Now similarly another document D2,pic_cs-410_5_5_780.jpg
cs-410_5_5_181,cs-410,5,5, Web Indexing,"00:13:46,710","00:13:50,890",181,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=826,"So in the end, again, there is a sorting",pic_cs-410_5_5_780.jpg
cs-410_5_5_182,cs-410,5,5, Web Indexing,"00:13:50,890","00:13:55,690",182,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=830,"And then we will have just a key,",pic_cs-410_5_5_780.jpg
cs-410_5_5_183,cs-410,5,5, Web Indexing,"00:13:55,690","00:14:00,340",183,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=835,associated with all the documents,pic_cs-410_5_5_780.jpg
cs-410_5_5_184,cs-410,5,5, Web Indexing,"00:14:00,340","00:14:02,870",184,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=840,Or all the documents where java occurred.,pic_cs-410_5_5_840.jpg
cs-410_5_5_185,cs-410,5,5, Web Indexing,"00:14:04,500","00:14:09,520",185,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=844,"And the counts, so",pic_cs-410_5_5_840.jpg
cs-410_5_5_186,cs-410,5,5, Web Indexing,"00:14:09,520","00:14:11,880",186,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=849,And this will be collected together.,pic_cs-410_5_5_840.jpg
cs-410_5_5_187,cs-410,5,5, Web Indexing,"00:14:11,880","00:14:15,840",187,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=851,"And this will be, so",pic_cs-410_5_5_840.jpg
cs-410_5_5_188,cs-410,5,5, Web Indexing,"00:14:15,840","00:14:20,010",188,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=855,So now you can see the reduce function,pic_cs-410_5_5_840.jpg
cs-410_5_5_189,cs-410,5,5, Web Indexing,"00:14:20,010","00:14:21,800",189,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=860,an inverted index entry.,pic_cs-410_5_5_840.jpg
cs-410_5_5_190,cs-410,5,5, Web Indexing,"00:14:21,800","00:14:27,280",190,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=861,So it's just the word and all,pic_cs-410_5_5_840.jpg
cs-410_5_5_191,cs-410,5,5, Web Indexing,"00:14:27,280","00:14:30,900",191,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=867,the frequencies of the word,pic_cs-410_5_5_840.jpg
cs-410_5_5_192,cs-410,5,5, Web Indexing,"00:14:30,900","00:14:35,240",192,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=870,So all you need to do is,pic_cs-410_5_5_840.jpg
cs-410_5_5_193,cs-410,5,5, Web Indexing,"00:14:37,670","00:14:40,380",193,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=877,into a continuous chunk of data.,pic_cs-410_5_5_840.jpg
cs-410_5_5_194,cs-410,5,5, Web Indexing,"00:14:40,380","00:14:43,650",194,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=880,And this can be done,pic_cs-410_5_5_840.jpg
cs-410_5_5_195,cs-410,5,5, Web Indexing,"00:14:43,650","00:14:47,520",195,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=883,So basically the reduce function,pic_cs-410_5_5_840.jpg
cs-410_5_5_196,cs-410,5,5, Web Indexing,"00:14:47,520","00:14:48,020",196,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=887,Work.,pic_cs-410_5_5_840.jpg
cs-410_5_5_197,cs-410,5,5, Web Indexing,"00:14:49,450","00:14:53,647",197,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=889,"And so, this is a pseudo-code for",pic_cs-410_5_5_840.jpg
cs-410_5_5_198,cs-410,5,5, Web Indexing,"00:14:53,647","00:14:58,010",198,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=893,[INAUDIBLE] that's construction.,pic_cs-410_5_5_840.jpg
cs-410_5_5_199,cs-410,5,5, Web Indexing,"00:14:58,010","00:15:05,290",199,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=898,"Here we see two functions,",pic_cs-410_5_5_840.jpg
cs-410_5_5_200,cs-410,5,5, Web Indexing,"00:15:05,290","00:15:13,440",200,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=905,And a programmer would specify these two,pic_cs-410_5_5_900.jpg
cs-410_5_5_201,cs-410,5,5, Web Indexing,"00:15:13,440","00:15:18,990",201,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=913,And you can see basically they,pic_cs-410_5_5_900.jpg
cs-410_5_5_202,cs-410,5,5, Web Indexing,"00:15:18,990","00:15:22,870",202,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=918,"In the case of map, it's going to count",pic_cs-410_5_5_900.jpg
cs-410_5_5_203,cs-410,5,5, Web Indexing,"00:15:22,870","00:15:27,040",203,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=922,the occurrences of a word,pic_cs-410_5_5_900.jpg
cs-410_5_5_204,cs-410,5,5, Web Indexing,"00:15:27,040","00:15:34,232",204,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=927,And it would output all the counts,pic_cs-410_5_5_900.jpg
cs-410_5_5_205,cs-410,5,5, Web Indexing,"00:15:34,232","00:15:40,350",205,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=934,"So, this is the reduce function,",pic_cs-410_5_5_900.jpg
cs-410_5_5_206,cs-410,5,5, Web Indexing,"00:15:40,350","00:15:47,380",206,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=940,simply concatenates all the input,pic_cs-410_5_5_900.jpg
cs-410_5_5_207,cs-410,5,5, Web Indexing,"00:15:47,380","00:15:53,580",207,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=947,and then put them together as,pic_cs-410_5_5_900.jpg
cs-410_5_5_208,cs-410,5,5, Web Indexing,"00:15:53,580","00:15:58,250",208,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=953,So this is a very simple,pic_cs-410_5_5_900.jpg
cs-410_5_5_209,cs-410,5,5, Web Indexing,"00:15:58,250","00:16:03,360",209,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=958,it would allow us to construct an inverted,pic_cs-410_5_5_900.jpg
cs-410_5_5_210,cs-410,5,5, Web Indexing,"00:16:03,360","00:16:06,950",210,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=963,the data can be processed,pic_cs-410_5_5_960.jpg
cs-410_5_5_211,cs-410,5,5, Web Indexing,"00:16:06,950","00:16:11,000",211,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=966,And program doesn't have to,pic_cs-410_5_5_960.jpg
cs-410_5_5_212,cs-410,5,5, Web Indexing,"00:16:12,080","00:16:18,930",212,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=972,So this is how we can do parallel,pic_cs-410_5_5_960.jpg
cs-410_5_5_213,cs-410,5,5, Web Indexing,"00:16:20,040","00:16:21,960",213,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=980,"So to summarize,",pic_cs-410_5_5_960.jpg
cs-410_5_5_214,cs-410,5,5, Web Indexing,"00:16:21,960","00:16:26,040",214,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=981,web scale indexing requires some,pic_cs-410_5_5_960.jpg
cs-410_5_5_215,cs-410,5,5, Web Indexing,"00:16:26,040","00:16:29,230",215,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=986,Standard traditional indexing techniques.,pic_cs-410_5_5_960.jpg
cs-410_5_5_216,cs-410,5,5, Web Indexing,"00:16:29,230","00:16:32,800",216,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=989,"Mainly, we have to store",pic_cs-410_5_5_960.jpg
cs-410_5_5_217,cs-410,5,5, Web Indexing,"00:16:32,800","00:16:37,990",217,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=992,And this is usually done by using a filing,pic_cs-410_5_5_960.jpg
cs-410_5_5_218,cs-410,5,5, Web Indexing,"00:16:37,990","00:16:40,240",218,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=997,But this should be through a file system.,pic_cs-410_5_5_960.jpg
cs-410_5_5_219,cs-410,5,5, Web Indexing,"00:16:40,240","00:16:45,320",219,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1000,"And secondly, it requires creating",pic_cs-410_5_5_960.jpg
cs-410_5_5_220,cs-410,5,5, Web Indexing,"00:16:45,320","00:16:50,340",220,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1005,large and takes long time to create,pic_cs-410_5_5_960.jpg
cs-410_5_5_221,cs-410,5,5, Web Indexing,"00:16:50,340","00:16:53,790",221,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1010,"So if we can do it in parallel,",pic_cs-410_5_5_960.jpg
cs-410_5_5_222,cs-410,5,5, Web Indexing,"00:16:53,790","00:16:56,690",222,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1013,this is done by using,pic_cs-410_5_5_960.jpg
cs-410_5_5_223,cs-410,5,5, Web Indexing,"00:16:57,850","00:17:02,182",223,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1017,Note that both the GFS and,pic_cs-410_5_5_960.jpg
cs-410_5_5_224,cs-410,5,5, Web Indexing,"00:17:02,182","00:17:05,251",224,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1022,they can also support,pic_cs-410_5_5_1020.jpg
cs-410_5_5_225,cs-410,5,5, Web Indexing,"00:17:07,795","00:17:17,795",225,https://www.coursera.org/learn/cs-410/lecture/lRm0I?t=1027,[MUSIC],pic_cs-410_5_5_1020.jpg
cs-410_5_6_1,cs-410,5,6, Link Analysis - Part 1,"00:00:00,025","00:00:06,042",1,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=0,[SOUND] This lecture is about,pic_cs-410_5_6_0.jpg
cs-410_5_6_2,cs-410,5,6, Link Analysis - Part 1,"00:00:06,042","00:00:12,849",2,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=6,link analysis for web search.,pic_cs-410_5_6_0.jpg
cs-410_5_6_3,cs-410,5,6, Link Analysis - Part 1,"00:00:12,849","00:00:18,236",3,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=12,"In this lecture, we're going to talk",pic_cs-410_5_6_0.jpg
cs-410_5_6_4,cs-410,5,6, Link Analysis - Part 1,"00:00:18,236","00:00:23,310",4,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=18,focusing on how to do link analysis and,pic_cs-410_5_6_0.jpg
cs-410_5_6_5,cs-410,5,6, Link Analysis - Part 1,"00:00:23,310","00:00:31,220",5,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=23,The main topic of this lecture is to look,pic_cs-410_5_6_0.jpg
cs-410_5_6_6,cs-410,5,6, Link Analysis - Part 1,"00:00:32,420","00:00:35,660",6,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=32,In the previous lecture we talked,pic_cs-410_5_6_0.jpg
cs-410_5_6_7,cs-410,5,6, Link Analysis - Part 1,"00:00:35,660","00:00:42,992",7,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=35,"Now that we have index, we want to see",pic_cs-410_5_6_0.jpg
cs-410_5_6_8,cs-410,5,6, Link Analysis - Part 1,"00:00:42,992","00:00:44,900",8,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=42,The web.,pic_cs-410_5_6_0.jpg
cs-410_5_6_9,cs-410,5,6, Link Analysis - Part 1,"00:00:44,900","00:00:48,320",9,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=44,"Now standard IR models,",pic_cs-410_5_6_0.jpg
cs-410_5_6_10,cs-410,5,6, Link Analysis - Part 1,"00:00:48,320","00:00:51,410",10,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=48,"In fact,",pic_cs-410_5_6_0.jpg
cs-410_5_6_11,cs-410,5,6, Link Analysis - Part 1,"00:00:51,410","00:00:54,390",11,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=51,"improve, for supporting web search.",pic_cs-410_5_6_0.jpg
cs-410_5_6_12,cs-410,5,6, Link Analysis - Part 1,"00:00:54,390","00:00:56,380",12,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=54,But they aren't sufficient.,pic_cs-410_5_6_0.jpg
cs-410_5_6_13,cs-410,5,6, Link Analysis - Part 1,"00:00:56,380","00:00:58,550",13,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=56,And mainly for the following reasons.,pic_cs-410_5_6_0.jpg
cs-410_5_6_14,cs-410,5,6, Link Analysis - Part 1,"00:00:58,550","00:01:02,630",14,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=58,"First, on the web, we tend to have",pic_cs-410_5_6_0.jpg
cs-410_5_6_15,cs-410,5,6, Link Analysis - Part 1,"00:01:02,630","00:01:07,150",15,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=62,"example, people might search for",pic_cs-410_5_6_60.jpg
cs-410_5_6_16,cs-410,5,6, Link Analysis - Part 1,"00:01:07,150","00:01:11,230",16,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=67,And this is different from,pic_cs-410_5_6_60.jpg
cs-410_5_6_17,cs-410,5,6, Link Analysis - Part 1,"00:01:11,230","00:01:15,870",17,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=71,where people are primarily interested,pic_cs-410_5_6_60.jpg
cs-410_5_6_18,cs-410,5,6, Link Analysis - Part 1,"00:01:15,870","00:01:19,070",18,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=75,So this kind of query is often,pic_cs-410_5_6_60.jpg
cs-410_5_6_19,cs-410,5,6, Link Analysis - Part 1,"00:01:19,070","00:01:23,250",19,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=79,The purpose is to navigate into,pic_cs-410_5_6_60.jpg
cs-410_5_6_20,cs-410,5,6, Link Analysis - Part 1,"00:01:23,250","00:01:28,255",20,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=83,So for such queries we might benefit,pic_cs-410_5_6_60.jpg
cs-410_5_6_21,cs-410,5,6, Link Analysis - Part 1,"00:01:28,255","00:01:33,020",21,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=88,"Secondly, documents have additional",pic_cs-410_5_6_60.jpg
cs-410_5_6_22,cs-410,5,6, Link Analysis - Part 1,"00:01:33,020","00:01:37,220",22,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=93,"are web format,",pic_cs-410_5_6_60.jpg
cs-410_5_6_23,cs-410,5,6, Link Analysis - Part 1,"00:01:37,220","00:01:40,538",23,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=97,"such as the layout, the title,",pic_cs-410_5_6_60.jpg
cs-410_5_6_24,cs-410,5,6, Link Analysis - Part 1,"00:01:40,538","00:01:45,340",24,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=100,So this has provided opportunity to use,pic_cs-410_5_6_60.jpg
cs-410_5_6_25,cs-410,5,6, Link Analysis - Part 1,"00:01:45,340","00:01:49,800",25,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=105,extra context information of,pic_cs-410_5_6_60.jpg
cs-410_5_6_26,cs-410,5,6, Link Analysis - Part 1,"00:01:49,800","00:01:52,440",26,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=109,"And finally,",pic_cs-410_5_6_60.jpg
cs-410_5_6_27,cs-410,5,6, Link Analysis - Part 1,"00:01:52,440","00:01:56,570",27,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=112,That means we have to consider,pic_cs-410_5_6_60.jpg
cs-410_5_6_28,cs-410,5,6, Link Analysis - Part 1,"00:01:56,570","00:01:58,400",28,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=116,the range in the algorithm.,pic_cs-410_5_6_60.jpg
cs-410_5_6_29,cs-410,5,6, Link Analysis - Part 1,"00:01:58,400","00:02:03,540",29,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=118,This would give us a more robust way,pic_cs-410_5_6_60.jpg
cs-410_5_6_30,cs-410,5,6, Link Analysis - Part 1,"00:02:03,540","00:02:09,350",30,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=123,any spammer to just manipulate the one,pic_cs-410_5_6_120.jpg
cs-410_5_6_31,cs-410,5,6, Link Analysis - Part 1,"00:02:10,500","00:02:11,370",31,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=130,"So as a result,",pic_cs-410_5_6_120.jpg
cs-410_5_6_32,cs-410,5,6, Link Analysis - Part 1,"00:02:11,370","00:02:15,500",32,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=131,people have made a number of major,pic_cs-410_5_6_120.jpg
cs-410_5_6_33,cs-410,5,6, Link Analysis - Part 1,"00:02:16,830","00:02:21,380",33,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=136,One line is to exploit,pic_cs-410_5_6_120.jpg
cs-410_5_6_34,cs-410,5,6, Link Analysis - Part 1,"00:02:23,020","00:02:24,790",34,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=143,And that's the main topic of this lecture.,pic_cs-410_5_6_120.jpg
cs-410_5_6_35,cs-410,5,6, Link Analysis - Part 1,"00:02:26,380","00:02:31,260",35,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=146,People have also proposed algorithms to,pic_cs-410_5_6_120.jpg
cs-410_5_6_36,cs-410,5,6, Link Analysis - Part 1,"00:02:31,260","00:02:35,370",36,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=151,Feedback information the form of,pic_cs-410_5_6_120.jpg
cs-410_5_6_37,cs-410,5,6, Link Analysis - Part 1,"00:02:35,370","00:02:40,720",37,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=155,in the category of feedback techniques and,pic_cs-410_5_6_120.jpg
cs-410_5_6_38,cs-410,5,6, Link Analysis - Part 1,"00:02:40,720","00:02:45,009",38,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=160,In general in web search the ranking,pic_cs-410_5_6_120.jpg
cs-410_5_6_39,cs-410,5,6, Link Analysis - Part 1,"00:02:45,009","00:02:49,750",39,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=165,algorithms to combine,pic_cs-410_5_6_120.jpg
cs-410_5_6_40,cs-410,5,6, Link Analysis - Part 1,"00:02:49,750","00:02:55,509",40,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=169,Many of them are based on,pic_cs-410_5_6_120.jpg
cs-410_5_6_41,cs-410,5,6, Link Analysis - Part 1,"00:02:55,509","00:03:03,341",41,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=175,as BM25 that we talked about [INAUDIBLE],pic_cs-410_5_6_120.jpg
cs-410_5_6_42,cs-410,5,6, Link Analysis - Part 1,"00:03:03,341","00:03:09,217",42,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=183,to provide additional features,pic_cs-410_5_6_180.jpg
cs-410_5_6_43,cs-410,5,6, Link Analysis - Part 1,"00:03:09,217","00:03:13,364",43,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=189,but link information,pic_cs-410_5_6_180.jpg
cs-410_5_6_44,cs-410,5,6, Link Analysis - Part 1,"00:03:13,364","00:03:17,660",44,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=193,they provide additional scoring signals.,pic_cs-410_5_6_180.jpg
cs-410_5_6_45,cs-410,5,6, Link Analysis - Part 1,"00:03:17,660","00:03:21,080",45,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=197,So let's look at links in,pic_cs-410_5_6_180.jpg
cs-410_5_6_46,cs-410,5,6, Link Analysis - Part 1,"00:03:21,080","00:03:26,450",46,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=201,So this is a snapshot of some,pic_cs-410_5_6_180.jpg
cs-410_5_6_47,cs-410,5,6, Link Analysis - Part 1,"00:03:26,450","00:03:30,790",47,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=206,So we can see there are many links that,pic_cs-410_5_6_180.jpg
cs-410_5_6_48,cs-410,5,6, Link Analysis - Part 1,"00:03:30,790","00:03:35,730",48,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=210,"And in this case, you can also",pic_cs-410_5_6_180.jpg
cs-410_5_6_49,cs-410,5,6, Link Analysis - Part 1,"00:03:35,730","00:03:40,400",49,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=215,a description of a link that's pointing,pic_cs-410_5_6_180.jpg
cs-410_5_6_50,cs-410,5,6, Link Analysis - Part 1,"00:03:40,400","00:03:42,850",50,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=220,"Now, this description text",pic_cs-410_5_6_180.jpg
cs-410_5_6_51,cs-410,5,6, Link Analysis - Part 1,"00:03:44,460","00:03:48,920",51,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=224,"Now if you think about this text,",pic_cs-410_5_6_180.jpg
cs-410_5_6_52,cs-410,5,6, Link Analysis - Part 1,"00:03:48,920","00:03:53,865",52,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=228,because it provides some extra,pic_cs-410_5_6_180.jpg
cs-410_5_6_53,cs-410,5,6, Link Analysis - Part 1,"00:03:53,865","00:03:59,685",53,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=233,"So for example, if someone wants",pic_cs-410_5_6_180.jpg
cs-410_5_6_54,cs-410,5,6, Link Analysis - Part 1,"00:03:59,685","00:04:04,555",54,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=239,the person might say the biggest,pic_cs-410_5_6_180.jpg
cs-410_5_6_55,cs-410,5,6, Link Analysis - Part 1,"00:04:04,555","00:04:07,695",55,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=244,"then the link to Amazon, right?",pic_cs-410_5_6_240.jpg
cs-410_5_6_56,cs-410,5,6, Link Analysis - Part 1,"00:04:07,695","00:04:11,855",56,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=247,"So, the description here after is very",pic_cs-410_5_6_240.jpg
cs-410_5_6_57,cs-410,5,6, Link Analysis - Part 1,"00:04:11,855","00:04:14,350",57,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=251,the query box when they are looking for,pic_cs-410_5_6_240.jpg
cs-410_5_6_58,cs-410,5,6, Link Analysis - Part 1,"00:04:14,350","00:04:19,950",58,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=254,And that's why it's very useful for,pic_cs-410_5_6_240.jpg
cs-410_5_6_59,cs-410,5,6, Link Analysis - Part 1,"00:04:19,950","00:04:25,058",59,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=259,Suppose someone types in,pic_cs-410_5_6_240.jpg
cs-410_5_6_60,cs-410,5,6, Link Analysis - Part 1,"00:04:25,058","00:04:27,517",60,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=265,biggest online bookstore.,pic_cs-410_5_6_240.jpg
cs-410_5_6_61,cs-410,5,6, Link Analysis - Part 1,"00:04:27,517","00:04:35,980",61,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=267,All right the query would match,pic_cs-410_5_6_240.jpg
cs-410_5_6_62,cs-410,5,6, Link Analysis - Part 1,"00:04:35,980","00:04:39,650",62,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=275,And then this actually,pic_cs-410_5_6_240.jpg
cs-410_5_6_63,cs-410,5,6, Link Analysis - Part 1,"00:04:39,650","00:04:44,090",63,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=279,matching the page that's being,pic_cs-410_5_6_240.jpg
cs-410_5_6_64,cs-410,5,6, Link Analysis - Part 1,"00:04:44,090","00:04:45,650",64,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=284,a entry page.,pic_cs-410_5_6_240.jpg
cs-410_5_6_65,cs-410,5,6, Link Analysis - Part 1,"00:04:45,650","00:04:50,120",65,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=285,So if you match anchor text that,pic_cs-410_5_6_240.jpg
cs-410_5_6_66,cs-410,5,6, Link Analysis - Part 1,"00:04:50,120","00:04:58,080",66,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=290,actually that provides good evidence for,pic_cs-410_5_6_240.jpg
cs-410_5_6_67,cs-410,5,6, Link Analysis - Part 1,"00:04:58,080","00:05:00,480",67,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=298,So anchor text is very useful.,pic_cs-410_5_6_240.jpg
cs-410_5_6_68,cs-410,5,6, Link Analysis - Part 1,"00:05:00,480","00:05:03,970",68,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=300,If you look at the bottom part of this,pic_cs-410_5_6_300.jpg
cs-410_5_6_69,cs-410,5,6, Link Analysis - Part 1,"00:05:03,970","00:05:08,380",69,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=303,patterns of some links and these links,pic_cs-410_5_6_300.jpg
cs-410_5_6_70,cs-410,5,6, Link Analysis - Part 1,"00:05:08,380","00:05:09,230",70,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=308,"So for example,",pic_cs-410_5_6_300.jpg
cs-410_5_6_71,cs-410,5,6, Link Analysis - Part 1,"00:05:09,230","00:05:14,200",71,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=309,on the right side you'll see this,pic_cs-410_5_6_300.jpg
cs-410_5_6_72,cs-410,5,6, Link Analysis - Part 1,"00:05:14,200","00:05:17,180",72,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=314,Now that means many other pages,pic_cs-410_5_6_300.jpg
cs-410_5_6_73,cs-410,5,6, Link Analysis - Part 1,"00:05:17,180","00:05:20,270",73,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=317,This shows that this page is quite useful.,pic_cs-410_5_6_300.jpg
cs-410_5_6_74,cs-410,5,6, Link Analysis - Part 1,"00:05:21,370","00:05:24,710",74,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=321,On the left side you can see this,pic_cs-410_5_6_300.jpg
cs-410_5_6_75,cs-410,5,6, Link Analysis - Part 1,"00:05:24,710","00:05:25,920",75,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=324,many other pages.,pic_cs-410_5_6_300.jpg
cs-410_5_6_76,cs-410,5,6, Link Analysis - Part 1,"00:05:25,920","00:05:29,040",76,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=325,So this is a director page,pic_cs-410_5_6_300.jpg
cs-410_5_6_77,cs-410,5,6, Link Analysis - Part 1,"00:05:29,040","00:05:31,260",77,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=329,actually see a lot of other pages.,pic_cs-410_5_6_300.jpg
cs-410_5_6_78,cs-410,5,6, Link Analysis - Part 1,"00:05:32,670","00:05:35,990",78,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=332,So we can call the first,pic_cs-410_5_6_300.jpg
cs-410_5_6_79,cs-410,5,6, Link Analysis - Part 1,"00:05:35,990","00:05:41,250",79,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=335,"the second case half page, but this means",pic_cs-410_5_6_300.jpg
cs-410_5_6_80,cs-410,5,6, Link Analysis - Part 1,"00:05:41,250","00:05:44,080",80,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=341,One is to provide extra text for matching.,pic_cs-410_5_6_300.jpg
cs-410_5_6_81,cs-410,5,6, Link Analysis - Part 1,"00:05:44,080","00:05:49,750",81,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=344,The other is to provide some,pic_cs-410_5_6_300.jpg
cs-410_5_6_82,cs-410,5,6, Link Analysis - Part 1,"00:05:49,750","00:05:53,980",82,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=349,to characterize how likely a page is,pic_cs-410_5_6_300.jpg
cs-410_5_6_83,cs-410,5,6, Link Analysis - Part 1,"00:05:55,820","00:06:02,530",83,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=355,So people then of course and proposed,pic_cs-410_5_6_300.jpg
cs-410_5_6_84,cs-410,5,6, Link Analysis - Part 1,"00:06:02,530","00:06:08,030",84,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=362,Google's PageRank which was the main,pic_cs-410_5_6_360.jpg
cs-410_5_6_85,cs-410,5,6, Link Analysis - Part 1,"00:06:08,030","00:06:13,360",85,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=368,is a good example and,pic_cs-410_5_6_360.jpg
cs-410_5_6_86,cs-410,5,6, Link Analysis - Part 1,"00:06:13,360","00:06:17,070",86,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=373,"popularity, basically to score authority.",pic_cs-410_5_6_360.jpg
cs-410_5_6_87,cs-410,5,6, Link Analysis - Part 1,"00:06:17,070","00:06:21,640",87,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=377,So the intuitions here are links,pic_cs-410_5_6_360.jpg
cs-410_5_6_88,cs-410,5,6, Link Analysis - Part 1,"00:06:21,640","00:06:24,030",88,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=381,Now think about one page,pic_cs-410_5_6_360.jpg
cs-410_5_6_89,cs-410,5,6, Link Analysis - Part 1,"00:06:24,030","00:06:27,440",89,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=384,this is very similar to one,pic_cs-410_5_6_360.jpg
cs-410_5_6_90,cs-410,5,6, Link Analysis - Part 1,"00:06:27,440","00:06:30,360",90,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=387,"So, of course then,",pic_cs-410_5_6_360.jpg
cs-410_5_6_91,cs-410,5,6, Link Analysis - Part 1,"00:06:30,360","00:06:33,920",91,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=390,then we can assume this page,pic_cs-410_5_6_360.jpg
cs-410_5_6_92,cs-410,5,6, Link Analysis - Part 1,"00:06:35,120","00:06:36,570",92,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=395,So that's a very good intuition.,pic_cs-410_5_6_360.jpg
cs-410_5_6_93,cs-410,5,6, Link Analysis - Part 1,"00:06:38,060","00:06:42,950",93,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=398,Now PageRank is essentially to take,pic_cs-410_5_6_360.jpg
cs-410_5_6_94,cs-410,5,6, Link Analysis - Part 1,"00:06:42,950","00:06:46,650",94,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=402,implement with the principal approach.,pic_cs-410_5_6_360.jpg
cs-410_5_6_95,cs-410,5,6, Link Analysis - Part 1,"00:06:46,650","00:06:51,980",95,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=406,"Intuitively, it is essentially doing",pic_cs-410_5_6_360.jpg
cs-410_5_6_96,cs-410,5,6, Link Analysis - Part 1,"00:06:51,980","00:06:56,390",96,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=411,It just improves the simple,pic_cs-410_5_6_360.jpg
cs-410_5_6_97,cs-410,5,6, Link Analysis - Part 1,"00:06:56,390","00:06:59,420",97,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=416,One it will consider indirect citations.,pic_cs-410_5_6_360.jpg
cs-410_5_6_98,cs-410,5,6, Link Analysis - Part 1,"00:06:59,420","00:07:04,010",98,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=419,So that means you don't just look,pic_cs-410_5_6_360.jpg
cs-410_5_6_99,cs-410,5,6, Link Analysis - Part 1,"00:07:04,010","00:07:08,550",99,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=424,You also look at what are those,pic_cs-410_5_6_420.jpg
cs-410_5_6_100,cs-410,5,6, Link Analysis - Part 1,"00:07:08,550","00:07:13,530",100,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=428,If those pages themselves have a lot,pic_cs-410_5_6_420.jpg
cs-410_5_6_101,cs-410,5,6, Link Analysis - Part 1,"00:07:13,530","00:07:16,750",101,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=433,"In some sense,",pic_cs-410_5_6_420.jpg
cs-410_5_6_102,cs-410,5,6, Link Analysis - Part 1,"00:07:16,750","00:07:20,080",102,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=436,But if those pages that,pic_cs-410_5_6_420.jpg
cs-410_5_6_103,cs-410,5,6, Link Analysis - Part 1,"00:07:20,080","00:07:25,095",103,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=440,being pointed to by other pages they,pic_cs-410_5_6_420.jpg
cs-410_5_6_104,cs-410,5,6, Link Analysis - Part 1,"00:07:25,095","00:07:27,360",104,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=445,"then well, you don't get that much.",pic_cs-410_5_6_420.jpg
cs-410_5_6_105,cs-410,5,6, Link Analysis - Part 1,"00:07:27,360","00:07:29,830",105,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=447,So that's the idea of,pic_cs-410_5_6_420.jpg
cs-410_5_6_106,cs-410,5,6, Link Analysis - Part 1,"00:07:29,830","00:07:31,770",106,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=449,"All right, so",pic_cs-410_5_6_420.jpg
cs-410_5_6_107,cs-410,5,6, Link Analysis - Part 1,"00:07:31,770","00:07:37,060",107,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=451,you can also understand this idea by,pic_cs-410_5_6_420.jpg
cs-410_5_6_108,cs-410,5,6, Link Analysis - Part 1,"00:07:37,060","00:07:42,082",108,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=457,"If you're cited by let's say ten papers,",pic_cs-410_5_6_420.jpg
cs-410_5_6_109,cs-410,5,6, Link Analysis - Part 1,"00:07:42,082","00:07:48,450",109,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=462,are just workshop papers or some papers,pic_cs-410_5_6_420.jpg
cs-410_5_6_110,cs-410,5,6, Link Analysis - Part 1,"00:07:49,580","00:07:54,340",110,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=469,"So although you've got ten in-links,",pic_cs-410_5_6_420.jpg
cs-410_5_6_111,cs-410,5,6, Link Analysis - Part 1,"00:07:54,340","00:07:59,910",111,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=474,are cited by ten papers that themselves,pic_cs-410_5_6_420.jpg
cs-410_5_6_112,cs-410,5,6, Link Analysis - Part 1,"00:08:01,770","00:08:06,563",112,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=481,And so in this case where we would,pic_cs-410_5_6_480.jpg
cs-410_5_6_113,cs-410,5,6, Link Analysis - Part 1,"00:08:06,563","00:08:08,530",113,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=486,page does that.,pic_cs-410_5_6_480.jpg
cs-410_5_6_114,cs-410,5,6, Link Analysis - Part 1,"00:08:08,530","00:08:12,174",114,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=488,The other idea is it's,pic_cs-410_5_6_480.jpg
cs-410_5_6_115,cs-410,5,6, Link Analysis - Part 1,"00:08:13,810","00:08:21,120",115,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=493,Assume that basically every page is having,pic_cs-410_5_6_480.jpg
cs-410_5_6_116,cs-410,5,6, Link Analysis - Part 1,"00:08:21,120","00:08:23,950",116,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=501,Essentially you are trying to,pic_cs-410_5_6_480.jpg
cs-410_5_6_117,cs-410,5,6, Link Analysis - Part 1,"00:08:23,950","00:08:27,510",117,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=503,links that will link all,pic_cs-410_5_6_480.jpg
cs-410_5_6_118,cs-410,5,6, Link Analysis - Part 1,"00:08:27,510","00:08:32,740",118,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=507,that you actually get the pseudo,pic_cs-410_5_6_480.jpg
cs-410_5_6_119,cs-410,5,6, Link Analysis - Part 1,"00:08:34,300","00:08:36,760",119,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=514,The reason why they want to do that.,pic_cs-410_5_6_480.jpg
cs-410_5_6_120,cs-410,5,6, Link Analysis - Part 1,"00:08:36,760","00:08:41,980",120,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=516,Is this will allow them,pic_cs-410_5_6_480.jpg
cs-410_5_6_121,cs-410,5,6, Link Analysis - Part 1,"00:08:41,980","00:08:47,348",121,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=521,elegantly with linear algebra technique.,pic_cs-410_5_6_480.jpg
cs-410_5_6_122,cs-410,5,6, Link Analysis - Part 1,"00:08:47,348","00:08:52,354",122,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=527,"So, I think maybe the best",pic_cs-410_5_6_480.jpg
cs-410_5_6_123,cs-410,5,6, Link Analysis - Part 1,"00:08:52,354","00:08:58,172",123,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=532,the PageRank is to think,pic_cs-410_5_6_480.jpg
cs-410_5_6_124,cs-410,5,6, Link Analysis - Part 1,"00:08:58,172","00:09:04,549",124,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=538,probability of random surfer,pic_cs-410_5_6_480.jpg
cs-410_5_6_125,cs-410,5,6, Link Analysis - Part 1,"00:09:04,549","00:09:14,549",125,https://www.coursera.org/learn/cs-410/lecture/nE8nq?t=544,[MUSIC],pic_cs-410_5_6_540.jpg
cs-410_5_7_1,cs-410,5,7, Link Analysis - Part 2,"00:00:00,049","00:00:03,810",1,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=0,[MUSIC],pic_cs-410_5_7_0.jpg
cs-410_5_7_2,cs-410,5,7, Link Analysis - Part 2,"00:00:09,183","00:00:12,025",2,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=9,So let's take a look at this in detail.,pic_cs-410_5_7_0.jpg
cs-410_5_7_3,cs-410,5,7, Link Analysis - Part 2,"00:00:12,025","00:00:17,269",3,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=12,So in this random surfing,pic_cs-410_5_7_0.jpg
cs-410_5_7_4,cs-410,5,7, Link Analysis - Part 2,"00:00:17,269","00:00:22,575",4,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=17,random surfer would choose,pic_cs-410_5_7_0.jpg
cs-410_5_7_5,cs-410,5,7, Link Analysis - Part 2,"00:00:22,575","00:00:25,225",5,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=22,So this is a small graph here.,pic_cs-410_5_7_0.jpg
cs-410_5_7_6,cs-410,5,7, Link Analysis - Part 2,"00:00:25,225","00:00:29,490",6,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=25,"That's of course, over simplification",pic_cs-410_5_7_0.jpg
cs-410_5_7_7,cs-410,5,7, Link Analysis - Part 2,"00:00:29,490","00:00:35,207",7,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=29,But let's say there are four,pic_cs-410_5_7_0.jpg
cs-410_5_7_8,cs-410,5,7, Link Analysis - Part 2,"00:00:35,207","00:00:41,360",8,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=35,And let's assume that a random surfer or,pic_cs-410_5_7_0.jpg
cs-410_5_7_9,cs-410,5,7, Link Analysis - Part 2,"00:00:41,360","00:00:46,373",9,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=41,And then the random,pic_cs-410_5_7_0.jpg
cs-410_5_7_10,cs-410,5,7, Link Analysis - Part 2,"00:00:46,373","00:00:50,439",10,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=46,just randomly jumping to any page or,pic_cs-410_5_7_0.jpg
cs-410_5_7_11,cs-410,5,7, Link Analysis - Part 2,"00:00:50,439","00:00:55,330",11,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=50,follow a link and,pic_cs-410_5_7_0.jpg
cs-410_5_7_12,cs-410,5,7, Link Analysis - Part 2,"00:00:56,440","00:00:59,650",12,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=56,"So if the random surfer is at d1,",pic_cs-410_5_7_0.jpg
cs-410_5_7_13,cs-410,5,7, Link Analysis - Part 2,"00:01:01,100","00:01:06,260",13,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=61,then there is some probability that,pic_cs-410_5_7_60.jpg
cs-410_5_7_14,cs-410,5,7, Link Analysis - Part 2,"00:01:06,260","00:01:09,510",14,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=66,"Now there are two outlinks here,",pic_cs-410_5_7_60.jpg
cs-410_5_7_15,cs-410,5,7, Link Analysis - Part 2,"00:01:09,510","00:01:12,740",15,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=69,the other is pointing to d4.,pic_cs-410_5_7_60.jpg
cs-410_5_7_16,cs-410,5,7, Link Analysis - Part 2,"00:01:12,740","00:01:19,020",16,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=72,So the random surfer could pick any,pic_cs-410_5_7_60.jpg
cs-410_5_7_17,cs-410,5,7, Link Analysis - Part 2,"00:01:19,020","00:01:25,800",17,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=79,But it also assumes that the random so,pic_cs-410_5_7_60.jpg
cs-410_5_7_18,cs-410,5,7, Link Analysis - Part 2,"00:01:25,800","00:01:30,586",18,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=85,So the random surfing which decide,pic_cs-410_5_7_60.jpg
cs-410_5_7_19,cs-410,5,7, Link Analysis - Part 2,"00:01:30,586","00:01:34,050",19,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=90,simply randomly jump,pic_cs-410_5_7_60.jpg
cs-410_5_7_20,cs-410,5,7, Link Analysis - Part 2,"00:01:34,050","00:01:39,760",20,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=94,"So if it does that, it would be able",pic_cs-410_5_7_60.jpg
cs-410_5_7_21,cs-410,5,7, Link Analysis - Part 2,"00:01:39,760","00:01:45,090",21,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=99,"though there's no link you actually,",pic_cs-410_5_7_60.jpg
cs-410_5_7_22,cs-410,5,7, Link Analysis - Part 2,"00:01:46,170","00:01:49,713",22,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=106,So this is to assume that,pic_cs-410_5_7_60.jpg
cs-410_5_7_23,cs-410,5,7, Link Analysis - Part 2,"00:01:49,713","00:01:54,852",23,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=109,Imagine a random surfer is,pic_cs-410_5_7_60.jpg
cs-410_5_7_24,cs-410,5,7, Link Analysis - Part 2,"00:01:54,852","00:01:59,989",24,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=114,then we can ask the question how,pic_cs-410_5_7_60.jpg
cs-410_5_7_25,cs-410,5,7, Link Analysis - Part 2,"00:01:59,989","00:02:05,864",25,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=119,would actually reach a particular,pic_cs-410_5_7_60.jpg
cs-410_5_7_26,cs-410,5,7, Link Analysis - Part 2,"00:02:05,864","00:02:09,824",26,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=125,That's the average probability of,pic_cs-410_5_7_120.jpg
cs-410_5_7_27,cs-410,5,7, Link Analysis - Part 2,"00:02:09,824","00:02:13,830",27,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=129,this probability is precisely,pic_cs-410_5_7_120.jpg
cs-410_5_7_28,cs-410,5,7, Link Analysis - Part 2,"00:02:13,830","00:02:17,558",28,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=133,So the page rank score of,pic_cs-410_5_7_120.jpg
cs-410_5_7_29,cs-410,5,7, Link Analysis - Part 2,"00:02:17,558","00:02:21,644",29,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=137,probability that the surfer,pic_cs-410_5_7_120.jpg
cs-410_5_7_30,cs-410,5,7, Link Analysis - Part 2,"00:02:21,644","00:02:27,220",30,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=141,"Now intuitively, this would basically",pic_cs-410_5_7_120.jpg
cs-410_5_7_31,cs-410,5,7, Link Analysis - Part 2,"00:02:27,220","00:02:30,970",31,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=147,"Because if a page has a lot of inlinks,",pic_cs-410_5_7_120.jpg
cs-410_5_7_32,cs-410,5,7, Link Analysis - Part 2,"00:02:30,970","00:02:34,580",32,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=150,then it would have a higher,pic_cs-410_5_7_120.jpg
cs-410_5_7_33,cs-410,5,7, Link Analysis - Part 2,"00:02:34,580","00:02:37,650",33,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=154,Because there will be more,pic_cs-410_5_7_120.jpg
cs-410_5_7_34,cs-410,5,7, Link Analysis - Part 2,"00:02:37,650","00:02:39,940",34,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=157,follow a link to come to this page.,pic_cs-410_5_7_120.jpg
cs-410_5_7_35,cs-410,5,7, Link Analysis - Part 2,"00:02:41,290","00:02:45,030",35,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=161,And this is why the random surfing model,pic_cs-410_5_7_120.jpg
cs-410_5_7_36,cs-410,5,7, Link Analysis - Part 2,"00:02:45,030","00:02:48,510",36,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=165,actually captures the ID,pic_cs-410_5_7_120.jpg
cs-410_5_7_37,cs-410,5,7, Link Analysis - Part 2,"00:02:48,510","00:02:52,700",37,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=168,Note that it also considers,pic_cs-410_5_7_120.jpg
cs-410_5_7_38,cs-410,5,7, Link Analysis - Part 2,"00:02:52,700","00:02:59,690",38,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=172,Because if the page is that point then,pic_cs-410_5_7_120.jpg
cs-410_5_7_39,cs-410,5,7, Link Analysis - Part 2,"00:02:59,690","00:03:04,550",39,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=179,That would mean the random surfer would,pic_cs-410_5_7_120.jpg
cs-410_5_7_40,cs-410,5,7, Link Analysis - Part 2,"00:03:04,550","00:03:07,680",40,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=184,"therefore, it increase",pic_cs-410_5_7_180.jpg
cs-410_5_7_41,cs-410,5,7, Link Analysis - Part 2,"00:03:07,680","00:03:13,580",41,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=187,So this is just a nice way to capture,pic_cs-410_5_7_180.jpg
cs-410_5_7_42,cs-410,5,7, Link Analysis - Part 2,"00:03:13,580","00:03:18,440",42,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=193,"So mathematically, how can we compute this",pic_cs-410_5_7_180.jpg
cs-410_5_7_43,cs-410,5,7, Link Analysis - Part 2,"00:03:18,440","00:03:22,390",43,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=198,we need to take a look at how this,pic_cs-410_5_7_180.jpg
cs-410_5_7_44,cs-410,5,7, Link Analysis - Part 2,"00:03:22,390","00:03:25,184",44,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=202,So first of all let's take a look,pic_cs-410_5_7_180.jpg
cs-410_5_7_45,cs-410,5,7, Link Analysis - Part 2,"00:03:25,184","00:03:29,437",45,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=205,And this is just metrics with,pic_cs-410_5_7_180.jpg
cs-410_5_7_46,cs-410,5,7, Link Analysis - Part 2,"00:03:29,437","00:03:33,273",46,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=209,the random surfer would go,pic_cs-410_5_7_180.jpg
cs-410_5_7_47,cs-410,5,7, Link Analysis - Part 2,"00:03:33,273","00:03:37,230",47,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=213,So each rule stands for a starting page.,pic_cs-410_5_7_180.jpg
cs-410_5_7_48,cs-410,5,7, Link Analysis - Part 2,"00:03:37,230","00:03:41,754",48,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=217,"For example, rule one would",pic_cs-410_5_7_180.jpg
cs-410_5_7_49,cs-410,5,7, Link Analysis - Part 2,"00:03:41,754","00:03:44,581",49,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=221,to any of the other four pages from d1.,pic_cs-410_5_7_180.jpg
cs-410_5_7_50,cs-410,5,7, Link Analysis - Part 2,"00:03:44,581","00:03:53,097",50,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=224,And here we see there are only,pic_cs-410_5_7_180.jpg
cs-410_5_7_51,cs-410,5,7, Link Analysis - Part 2,"00:03:53,097","00:04:01,492",51,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=233,So this is because if you look at,pic_cs-410_5_7_180.jpg
cs-410_5_7_52,cs-410,5,7, Link Analysis - Part 2,"00:04:01,492","00:04:05,918",52,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=241,There is no link from d1 or d2.,pic_cs-410_5_7_240.jpg
cs-410_5_7_53,cs-410,5,7, Link Analysis - Part 2,"00:04:05,918","00:04:10,579",53,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=245,So we've got 0s for the first 2,pic_cs-410_5_7_240.jpg
cs-410_5_7_54,cs-410,5,7, Link Analysis - Part 2,"00:04:10,579","00:04:15,762",54,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=250,columns and 0.5 for d3 and d4.,pic_cs-410_5_7_240.jpg
cs-410_5_7_55,cs-410,5,7, Link Analysis - Part 2,"00:04:15,762","00:04:19,416",55,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=255,"In general, the M in this matrix,",pic_cs-410_5_7_240.jpg
cs-410_5_7_56,cs-410,5,7, Link Analysis - Part 2,"00:04:19,416","00:04:24,586",56,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=259,M sub ij is the probability,pic_cs-410_5_7_240.jpg
cs-410_5_7_57,cs-410,5,7, Link Analysis - Part 2,"00:04:24,586","00:04:29,668",57,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=264,"And obviously for each rule,",pic_cs-410_5_7_240.jpg
cs-410_5_7_58,cs-410,5,7, Link Analysis - Part 2,"00:04:29,668","00:04:36,115",58,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=269,because the surfer would have to go to,pic_cs-410_5_7_240.jpg
cs-410_5_7_59,cs-410,5,7, Link Analysis - Part 2,"00:04:36,115","00:04:39,196",59,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=276,So this is a transition metric.,pic_cs-410_5_7_240.jpg
cs-410_5_7_60,cs-410,5,7, Link Analysis - Part 2,"00:04:39,196","00:04:43,690",60,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=279,Now how can we compute the probability,pic_cs-410_5_7_240.jpg
cs-410_5_7_61,cs-410,5,7, Link Analysis - Part 2,"00:04:44,900","00:04:49,900",61,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=284,Well if you look at the surf,pic_cs-410_5_7_240.jpg
cs-410_5_7_62,cs-410,5,7, Link Analysis - Part 2,"00:04:50,910","00:04:56,280",62,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=290,we can compute the probability,pic_cs-410_5_7_240.jpg
cs-410_5_7_63,cs-410,5,7, Link Analysis - Part 2,"00:04:56,280","00:05:01,140",63,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=296,"So here on the left hand side,",pic_cs-410_5_7_240.jpg
cs-410_5_7_64,cs-410,5,7, Link Analysis - Part 2,"00:05:02,170","00:05:08,540",64,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=302,"visiting page dj at time plus 1,",pic_cs-410_5_7_300.jpg
cs-410_5_7_65,cs-410,5,7, Link Analysis - Part 2,"00:05:08,540","00:05:14,740",65,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=308,"On the right hand side, you can see",pic_cs-410_5_7_300.jpg
cs-410_5_7_66,cs-410,5,7, Link Analysis - Part 2,"00:05:14,740","00:05:20,000",66,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=314,of at page di at time t.,pic_cs-410_5_7_300.jpg
cs-410_5_7_67,cs-410,5,7, Link Analysis - Part 2,"00:05:21,408","00:05:26,020",67,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=321,So you can see the subscript,pic_cs-410_5_7_300.jpg
cs-410_5_7_68,cs-410,5,7, Link Analysis - Part 2,"00:05:26,020","00:05:34,314",68,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=326,that indicates that's the probability that,pic_cs-410_5_7_300.jpg
cs-410_5_7_69,cs-410,5,7, Link Analysis - Part 2,"00:05:34,314","00:05:38,500",69,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=334,"So the equation basically,",pic_cs-410_5_7_300.jpg
cs-410_5_7_70,cs-410,5,7, Link Analysis - Part 2,"00:05:38,500","00:05:43,790",70,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=338,possibilities of reaching,pic_cs-410_5_7_300.jpg
cs-410_5_7_71,cs-410,5,7, Link Analysis - Part 2,"00:05:43,790","00:05:45,510",71,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=343,What are these two possibilities?,pic_cs-410_5_7_300.jpg
cs-410_5_7_72,cs-410,5,7, Link Analysis - Part 2,"00:05:45,510","00:05:48,200",72,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=345,Well one is through random surfing and,pic_cs-410_5_7_300.jpg
cs-410_5_7_73,cs-410,5,7, Link Analysis - Part 2,"00:05:48,200","00:05:51,930",73,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=348,"one is through following a link,",pic_cs-410_5_7_300.jpg
cs-410_5_7_74,cs-410,5,7, Link Analysis - Part 2,"00:05:53,500","00:05:56,612",74,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=353,So the first part captures the probability,pic_cs-410_5_7_300.jpg
cs-410_5_7_75,cs-410,5,7, Link Analysis - Part 2,"00:05:56,612","00:06:01,373",75,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=356,that the random surfer would reach,pic_cs-410_5_7_300.jpg
cs-410_5_7_76,cs-410,5,7, Link Analysis - Part 2,"00:06:01,373","00:06:06,283",76,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=361,And you can see the random,pic_cs-410_5_7_360.jpg
cs-410_5_7_77,cs-410,5,7, Link Analysis - Part 2,"00:06:06,283","00:06:10,455",77,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=366,with probability 1 minus,pic_cs-410_5_7_360.jpg
cs-410_5_7_78,cs-410,5,7, Link Analysis - Part 2,"00:06:10,455","00:06:14,200",78,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=370,And so,pic_cs-410_5_7_360.jpg
cs-410_5_7_79,cs-410,5,7, Link Analysis - Part 2,"00:06:14,200","00:06:18,250",79,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=374,But the main party is realist,pic_cs-410_5_7_360.jpg
cs-410_5_7_80,cs-410,5,7, Link Analysis - Part 2,"00:06:18,250","00:06:22,060",80,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=378,that the surfer could have been at time t.,pic_cs-410_5_7_360.jpg
cs-410_5_7_81,cs-410,5,7, Link Analysis - Part 2,"00:06:23,760","00:06:27,890",81,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=383,There are n pages so,pic_cs-410_5_7_360.jpg
cs-410_5_7_82,cs-410,5,7, Link Analysis - Part 2,"00:06:27,890","00:06:31,730",82,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=387,Inside the sum is a product,pic_cs-410_5_7_360.jpg
cs-410_5_7_83,cs-410,5,7, Link Analysis - Part 2,"00:06:31,730","00:06:36,763",83,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=391,One is the probability that the surfer,pic_cs-410_5_7_360.jpg
cs-410_5_7_84,cs-410,5,7, Link Analysis - Part 2,"00:06:36,763","00:06:42,115",84,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=396,"was at di at time t, that's p sub t of di.",pic_cs-410_5_7_360.jpg
cs-410_5_7_85,cs-410,5,7, Link Analysis - Part 2,"00:06:42,115","00:06:47,422",85,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=402,The other is the transition,pic_cs-410_5_7_360.jpg
cs-410_5_7_86,cs-410,5,7, Link Analysis - Part 2,"00:06:47,422","00:06:52,217",86,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=407,"And so in order to reach this dj page,",pic_cs-410_5_7_360.jpg
cs-410_5_7_87,cs-410,5,7, Link Analysis - Part 2,"00:06:52,217","00:06:57,880",87,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=412,the surfer must first be at di at time t.,pic_cs-410_5_7_360.jpg
cs-410_5_7_88,cs-410,5,7, Link Analysis - Part 2,"00:06:57,880","00:07:03,090",88,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=417,"And then also, would also have to",pic_cs-410_5_7_360.jpg
cs-410_5_7_89,cs-410,5,7, Link Analysis - Part 2,"00:07:03,090","00:07:09,090",89,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=423,So the probability is the probability,pic_cs-410_5_7_420.jpg
cs-410_5_7_90,cs-410,5,7, Link Analysis - Part 2,"00:07:09,090","00:07:15,950",90,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=429,the probability of going from that,pic_cs-410_5_7_420.jpg
cs-410_5_7_91,cs-410,5,7, Link Analysis - Part 2,"00:07:15,950","00:07:20,792",91,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=435,"The second part is a similar sum, the only",pic_cs-410_5_7_420.jpg
cs-410_5_7_92,cs-410,5,7, Link Analysis - Part 2,"00:07:20,792","00:07:23,980",92,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=440,probability is a uniform,pic_cs-410_5_7_420.jpg
cs-410_5_7_93,cs-410,5,7, Link Analysis - Part 2,"00:07:23,980","00:07:27,708",93,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=443,1 over n and,pic_cs-410_5_7_420.jpg
cs-410_5_7_94,cs-410,5,7, Link Analysis - Part 2,"00:07:27,708","00:07:31,110",94,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=447,of reaching this page,pic_cs-410_5_7_420.jpg
cs-410_5_7_95,cs-410,5,7, Link Analysis - Part 2,"00:07:32,630","00:07:37,520",95,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=452,So the form is exactly the same and,pic_cs-410_5_7_420.jpg
cs-410_5_7_96,cs-410,5,7, Link Analysis - Part 2,"00:07:37,520","00:07:43,310",96,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=457,see on why PageRank is essentially assumed,pic_cs-410_5_7_420.jpg
cs-410_5_7_97,cs-410,5,7, Link Analysis - Part 2,"00:07:43,310","00:07:49,070",97,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=463,If you think about this 1 over n as,pic_cs-410_5_7_420.jpg
cs-410_5_7_98,cs-410,5,7, Link Analysis - Part 2,"00:07:49,070","00:07:55,320",98,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=469,that has all the elements being,pic_cs-410_5_7_420.jpg
cs-410_5_7_99,cs-410,5,7, Link Analysis - Part 2,"00:07:55,320","00:07:59,621",99,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=475,Then you can see very clearly,pic_cs-410_5_7_420.jpg
cs-410_5_7_100,cs-410,5,7, Link Analysis - Part 2,"00:07:59,621","00:08:01,784",100,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=479,because they are of the same form.,pic_cs-410_5_7_420.jpg
cs-410_5_7_101,cs-410,5,7, Link Analysis - Part 2,"00:08:01,784","00:08:07,310",101,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=481,We can imagine there's a different,pic_cs-410_5_7_480.jpg
cs-410_5_7_102,cs-410,5,7, Link Analysis - Part 2,"00:08:07,310","00:08:11,340",102,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=487,that uniform metrics where,pic_cs-410_5_7_480.jpg
cs-410_5_7_103,cs-410,5,7, Link Analysis - Part 2,"00:08:11,340","00:08:16,347",103,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=491,And in this sense PageRank uses,pic_cs-410_5_7_480.jpg
cs-410_5_7_104,cs-410,5,7, Link Analysis - Part 2,"00:08:16,347","00:08:22,312",104,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=496,ensuring that there's no zero entry,pic_cs-410_5_7_480.jpg
cs-410_5_7_105,cs-410,5,7, Link Analysis - Part 2,"00:08:22,312","00:08:28,530",105,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=502,Now of course this is the time dependent,pic_cs-410_5_7_480.jpg
cs-410_5_7_106,cs-410,5,7, Link Analysis - Part 2,"00:08:28,530","00:08:32,480",106,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=508,"Now we can imagine, if we'll compute",pic_cs-410_5_7_480.jpg
cs-410_5_7_107,cs-410,5,7, Link Analysis - Part 2,"00:08:32,480","00:08:36,420",107,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=512,the average of probabilities probably,pic_cs-410_5_7_480.jpg
cs-410_5_7_108,cs-410,5,7, Link Analysis - Part 2,"00:08:36,420","00:08:38,320",108,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=516,without considering the time index.,pic_cs-410_5_7_480.jpg
cs-410_5_7_109,cs-410,5,7, Link Analysis - Part 2,"00:08:38,320","00:08:41,780",109,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=518,So let's drop the time index and,pic_cs-410_5_7_480.jpg
cs-410_5_7_110,cs-410,5,7, Link Analysis - Part 2,"00:08:42,910","00:08:47,100",110,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=522,"Now this would give us any equations,",pic_cs-410_5_7_480.jpg
cs-410_5_7_111,cs-410,5,7, Link Analysis - Part 2,"00:08:47,100","00:08:49,520",111,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=527,each page we have such equation.,pic_cs-410_5_7_480.jpg
cs-410_5_7_112,cs-410,5,7, Link Analysis - Part 2,"00:08:49,520","00:08:52,800",112,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=529,And if you look at the what,pic_cs-410_5_7_480.jpg
cs-410_5_7_113,cs-410,5,7, Link Analysis - Part 2,"00:08:52,800","00:08:55,170",113,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=532,there are also precisely n variables.,pic_cs-410_5_7_480.jpg
cs-410_5_7_114,cs-410,5,7, Link Analysis - Part 2,"00:08:58,280","00:09:03,220",114,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=538,"So this basically means,",pic_cs-410_5_7_480.jpg
cs-410_5_7_115,cs-410,5,7, Link Analysis - Part 2,"00:09:04,600","00:09:10,260",115,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=544,n equations with n variables and,pic_cs-410_5_7_540.jpg
cs-410_5_7_116,cs-410,5,7, Link Analysis - Part 2,"00:09:10,260","00:09:16,420",116,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=550,"So basically, now the problem boils",pic_cs-410_5_7_540.jpg
cs-410_5_7_117,cs-410,5,7, Link Analysis - Part 2,"00:09:16,420","00:09:20,950",117,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=556,"And here, I also show",pic_cs-410_5_7_540.jpg
cs-410_5_7_118,cs-410,5,7, Link Analysis - Part 2,"00:09:20,950","00:09:26,690",118,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=560,It's the vector p here equals a matrix or,pic_cs-410_5_7_540.jpg
cs-410_5_7_119,cs-410,5,7, Link Analysis - Part 2,"00:09:26,690","00:09:31,390",119,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=566,the transpose of the matrix here and,pic_cs-410_5_7_540.jpg
cs-410_5_7_120,cs-410,5,7, Link Analysis - Part 2,"00:09:32,580","00:09:36,890",120,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=572,"Now, if you still remember some knowledge",pic_cs-410_5_7_540.jpg
cs-410_5_7_121,cs-410,5,7, Link Analysis - Part 2,"00:09:36,890","00:09:42,140",121,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=576,"and then you will realize, this is",pic_cs-410_5_7_540.jpg
cs-410_5_7_122,cs-410,5,7, Link Analysis - Part 2,"00:09:42,140","00:09:47,690",122,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=582,"When multiply the metrics by this vector,",pic_cs-410_5_7_540.jpg
cs-410_5_7_123,cs-410,5,7, Link Analysis - Part 2,"00:09:47,690","00:09:52,280",123,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=587,this can be solved by,pic_cs-410_5_7_540.jpg
cs-410_5_7_124,cs-410,5,7, Link Analysis - Part 2,"00:09:54,700","00:09:57,380",124,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=594,So because the equations here,pic_cs-410_5_7_540.jpg
cs-410_5_7_125,cs-410,5,7, Link Analysis - Part 2,"00:09:57,380","00:10:02,002",125,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=597,on the back are basically,pic_cs-410_5_7_540.jpg
cs-410_5_7_126,cs-410,5,7, Link Analysis - Part 2,"00:10:02,002","00:10:09,170",126,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=602,So you'll see the relation between the,pic_cs-410_5_7_600.jpg
cs-410_5_7_127,cs-410,5,7, Link Analysis - Part 2,"00:10:09,170","00:10:13,844",127,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=609,And this iterative approach or,pic_cs-410_5_7_600.jpg
cs-410_5_7_128,cs-410,5,7, Link Analysis - Part 2,"00:10:13,844","00:10:19,093",128,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=613,we simply start with s,pic_cs-410_5_7_600.jpg
cs-410_5_7_129,cs-410,5,7, Link Analysis - Part 2,"00:10:19,093","00:10:24,242",129,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=619,And then we repeatedly,pic_cs-410_5_7_600.jpg
cs-410_5_7_130,cs-410,5,7, Link Analysis - Part 2,"00:10:24,242","00:10:29,970",130,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=624,multiplying the metrics,pic_cs-410_5_7_600.jpg
cs-410_5_7_131,cs-410,5,7, Link Analysis - Part 2,"00:10:31,360","00:10:37,820",131,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=631,I also show a concrete example here.,pic_cs-410_5_7_600.jpg
cs-410_5_7_132,cs-410,5,7, Link Analysis - Part 2,"00:10:37,820","00:10:40,130",132,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=637,So you can see this now.,pic_cs-410_5_7_600.jpg
cs-410_5_7_133,cs-410,5,7, Link Analysis - Part 2,"00:10:40,130","00:10:43,368",133,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=640,"If we assume alpha is 0.2,",pic_cs-410_5_7_600.jpg
cs-410_5_7_134,cs-410,5,7, Link Analysis - Part 2,"00:10:43,368","00:10:49,066",134,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=643,then with the example that,pic_cs-410_5_7_600.jpg
cs-410_5_7_135,cs-410,5,7, Link Analysis - Part 2,"00:10:49,066","00:10:54,393",135,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=649,we have the original,pic_cs-410_5_7_600.jpg
cs-410_5_7_136,cs-410,5,7, Link Analysis - Part 2,"00:10:54,393","00:10:59,943",136,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=654,"That includes the graph, the actual links",pic_cs-410_5_7_600.jpg
cs-410_5_7_137,cs-410,5,7, Link Analysis - Part 2,"00:10:59,943","00:11:04,856",137,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=659,"metrics, uniform transition metrics",pic_cs-410_5_7_600.jpg
cs-410_5_7_138,cs-410,5,7, Link Analysis - Part 2,"00:11:04,856","00:11:09,707",138,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=664,And we can combine them together with,pic_cs-410_5_7_660.jpg
cs-410_5_7_139,cs-410,5,7, Link Analysis - Part 2,"00:11:09,707","00:11:12,260",139,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=669,metric that would be like this.,pic_cs-410_5_7_660.jpg
cs-410_5_7_140,cs-410,5,7, Link Analysis - Part 2,"00:11:12,260","00:11:13,320",140,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=672,"So essentially,",pic_cs-410_5_7_660.jpg
cs-410_5_7_141,cs-410,5,7, Link Analysis - Part 2,"00:11:13,320","00:11:18,300",141,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=673,we can imagine now the web looks like,pic_cs-410_5_7_660.jpg
cs-410_5_7_142,cs-410,5,7, Link Analysis - Part 2,"00:11:18,300","00:11:22,300",142,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=678,They're all virtual links,pic_cs-410_5_7_660.jpg
cs-410_5_7_143,cs-410,5,7, Link Analysis - Part 2,"00:11:22,300","00:11:27,210",143,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=682,The page we're on now would just,pic_cs-410_5_7_660.jpg
cs-410_5_7_144,cs-410,5,7, Link Analysis - Part 2,"00:11:27,210","00:11:30,270",144,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=687,then just computed the updating of this,pic_cs-410_5_7_660.jpg
cs-410_5_7_145,cs-410,5,7, Link Analysis - Part 2,"00:11:30,270","00:11:34,899",145,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=690,p vector by using this,pic_cs-410_5_7_660.jpg
cs-410_5_7_146,cs-410,5,7, Link Analysis - Part 2,"00:11:36,600","00:11:40,640",146,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=696,Now if you rewrite this,pic_cs-410_5_7_660.jpg
cs-410_5_7_147,cs-410,5,7, Link Analysis - Part 2,"00:11:42,020","00:11:45,080",147,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=702,"terms of individual equations,",pic_cs-410_5_7_660.jpg
cs-410_5_7_148,cs-410,5,7, Link Analysis - Part 2,"00:11:46,530","00:11:51,435",148,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=706,"And this is basically,",pic_cs-410_5_7_660.jpg
cs-410_5_7_149,cs-410,5,7, Link Analysis - Part 2,"00:11:51,435","00:11:54,385",149,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=711,this particular pages and page score.,pic_cs-410_5_7_660.jpg
cs-410_5_7_150,cs-410,5,7, Link Analysis - Part 2,"00:11:54,385","00:11:59,364",150,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=714,So you can also see if you want to compute,pic_cs-410_5_7_660.jpg
cs-410_5_7_151,cs-410,5,7, Link Analysis - Part 2,"00:11:59,364","00:12:04,617",151,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=719,You basically multiply,pic_cs-410_5_7_660.jpg
cs-410_5_7_152,cs-410,5,7, Link Analysis - Part 2,"00:12:04,617","00:12:09,379",152,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=724,and we'll take the third,pic_cs-410_5_7_720.jpg
cs-410_5_7_153,cs-410,5,7, Link Analysis - Part 2,"00:12:09,379","00:12:13,950",153,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=729,And that will give us the value for,pic_cs-410_5_7_720.jpg
cs-410_5_7_154,cs-410,5,7, Link Analysis - Part 2,"00:12:16,000","00:12:20,170",154,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=736,So this is how we updated the vector,pic_cs-410_5_7_720.jpg
cs-410_5_7_155,cs-410,5,7, Link Analysis - Part 2,"00:12:20,170","00:12:23,270",155,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=740,these guys for this.,pic_cs-410_5_7_720.jpg
cs-410_5_7_156,cs-410,5,7, Link Analysis - Part 2,"00:12:23,270","00:12:28,080",156,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=743,And then we just revise,pic_cs-410_5_7_720.jpg
cs-410_5_7_157,cs-410,5,7, Link Analysis - Part 2,"00:12:28,080","00:12:31,550",157,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=748,set of scores and,pic_cs-410_5_7_720.jpg
cs-410_5_7_158,cs-410,5,7, Link Analysis - Part 2,"00:12:33,150","00:12:37,590",158,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=753,So we just repeatedly apply this and,pic_cs-410_5_7_720.jpg
cs-410_5_7_159,cs-410,5,7, Link Analysis - Part 2,"00:12:37,590","00:12:41,432",159,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=757,"And when the matrix is like this,",pic_cs-410_5_7_720.jpg
cs-410_5_7_160,cs-410,5,7, Link Analysis - Part 2,"00:12:41,432","00:12:43,510",160,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=761,it can be guaranteed to converge.,pic_cs-410_5_7_720.jpg
cs-410_5_7_161,cs-410,5,7, Link Analysis - Part 2,"00:12:44,790","00:12:49,765",161,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=764,And at that point the we will just have,pic_cs-410_5_7_720.jpg
cs-410_5_7_162,cs-410,5,7, Link Analysis - Part 2,"00:12:49,765","00:12:53,101",162,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=769,We typically go to sets of,pic_cs-410_5_7_720.jpg
cs-410_5_7_163,cs-410,5,7, Link Analysis - Part 2,"00:12:55,300","00:12:58,543",163,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=775,"So interestingly,",pic_cs-410_5_7_720.jpg
cs-410_5_7_164,cs-410,5,7, Link Analysis - Part 2,"00:12:58,543","00:13:03,296",164,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=778,also interpreted as propagating,pic_cs-410_5_7_720.jpg
cs-410_5_7_165,cs-410,5,7, Link Analysis - Part 2,"00:13:03,296","00:13:08,847",165,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=783,Or if you look at this formula and,pic_cs-410_5_7_780.jpg
cs-410_5_7_166,cs-410,5,7, Link Analysis - Part 2,"00:13:08,847","00:13:13,440",166,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=788,"can you imagine,",pic_cs-410_5_7_780.jpg
cs-410_5_7_167,cs-410,5,7, Link Analysis - Part 2,"00:13:13,440","00:13:17,479",167,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=793,essentially propagating,pic_cs-410_5_7_780.jpg
cs-410_5_7_168,cs-410,5,7, Link Analysis - Part 2,"00:13:17,479","00:13:19,801",168,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=797,"I hope you will see that indeed,",pic_cs-410_5_7_780.jpg
cs-410_5_7_169,cs-410,5,7, Link Analysis - Part 2,"00:13:19,801","00:13:24,565",169,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=799,we can imagine we have values,pic_cs-410_5_7_780.jpg
cs-410_5_7_170,cs-410,5,7, Link Analysis - Part 2,"00:13:24,565","00:13:30,220",170,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=804,So we can have values here and,pic_cs-410_5_7_780.jpg
cs-410_5_7_171,cs-410,5,7, Link Analysis - Part 2,"00:13:30,220","00:13:35,170",171,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=810,And then we're going to use these,pic_cs-410_5_7_780.jpg
cs-410_5_7_172,cs-410,5,7, Link Analysis - Part 2,"00:13:35,170","00:13:42,290",172,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=815,And if you look at the equation here,pic_cs-410_5_7_780.jpg
cs-410_5_7_173,cs-410,5,7, Link Analysis - Part 2,"00:13:42,290","00:13:48,890",173,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=822,to combine the scores of the pages that,pic_cs-410_5_7_780.jpg
cs-410_5_7_174,cs-410,5,7, Link Analysis - Part 2,"00:13:48,890","00:13:54,067",174,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=828,So we'll look at all the pages,pic_cs-410_5_7_780.jpg
cs-410_5_7_175,cs-410,5,7, Link Analysis - Part 2,"00:13:54,067","00:14:00,916",175,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=834,then combine this score and propagate the,pic_cs-410_5_7_780.jpg
cs-410_5_7_176,cs-410,5,7, Link Analysis - Part 2,"00:14:00,916","00:14:06,145",176,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=840,To look at the scores that we present,pic_cs-410_5_7_840.jpg
cs-410_5_7_177,cs-410,5,7, Link Analysis - Part 2,"00:14:06,145","00:14:11,410",177,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=846,surfer would be visiting the other,pic_cs-410_5_7_840.jpg
cs-410_5_7_178,cs-410,5,7, Link Analysis - Part 2,"00:14:11,410","00:14:16,275",178,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=851,And then just do,pic_cs-410_5_7_840.jpg
cs-410_5_7_179,cs-410,5,7, Link Analysis - Part 2,"00:14:16,275","00:14:21,409",179,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=856,"the probability of reaching this page, d1.",pic_cs-410_5_7_840.jpg
cs-410_5_7_180,cs-410,5,7, Link Analysis - Part 2,"00:14:21,409","00:14:23,910",180,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=861,So there are two interpretations here.,pic_cs-410_5_7_840.jpg
cs-410_5_7_181,cs-410,5,7, Link Analysis - Part 2,"00:14:23,910","00:14:26,364",181,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=863,One is just the matrix multiplication.,pic_cs-410_5_7_840.jpg
cs-410_5_7_182,cs-410,5,7, Link Analysis - Part 2,"00:14:26,364","00:14:31,498",182,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=866,We repeat the multiplying,pic_cs-410_5_7_840.jpg
cs-410_5_7_183,cs-410,5,7, Link Analysis - Part 2,"00:14:31,498","00:14:35,204",183,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=871,The other is to just think,pic_cs-410_5_7_840.jpg
cs-410_5_7_184,cs-410,5,7, Link Analysis - Part 2,"00:14:35,204","00:14:38,180",184,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=875,these scores repeatedly on the web.,pic_cs-410_5_7_840.jpg
cs-410_5_7_185,cs-410,5,7, Link Analysis - Part 2,"00:14:38,180","00:14:43,150",185,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=878,"So in practice, the combination of",pic_cs-410_5_7_840.jpg
cs-410_5_7_186,cs-410,5,7, Link Analysis - Part 2,"00:14:43,150","00:14:48,820",186,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=883,Because the matrices is fast and there,pic_cs-410_5_7_840.jpg
cs-410_5_7_187,cs-410,5,7, Link Analysis - Part 2,"00:14:48,820","00:14:53,820",187,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=888,So that you avoid actually,pic_cs-410_5_7_840.jpg
cs-410_5_7_188,cs-410,5,7, Link Analysis - Part 2,"00:14:53,820","00:14:55,260",188,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=893,all those elements.,pic_cs-410_5_7_840.jpg
cs-410_5_7_189,cs-410,5,7, Link Analysis - Part 2,"00:14:56,670","00:15:00,670",189,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=896,Sometimes you may also normalize the,pic_cs-410_5_7_840.jpg
cs-410_5_7_190,cs-410,5,7, Link Analysis - Part 2,"00:15:00,670","00:15:05,249",190,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=900,"different form of the equation, but",pic_cs-410_5_7_900.jpg
cs-410_5_7_191,cs-410,5,7, Link Analysis - Part 2,"00:15:06,290","00:15:09,650",191,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=906,The results of this potential,pic_cs-410_5_7_900.jpg
cs-410_5_7_192,cs-410,5,7, Link Analysis - Part 2,"00:15:10,740","00:15:17,540",192,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=910,"In that case, if a page does not have",pic_cs-410_5_7_900.jpg
cs-410_5_7_193,cs-410,5,7, Link Analysis - Part 2,"00:15:17,540","00:15:22,250",193,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=917,these pages would not sum to 1.,pic_cs-410_5_7_900.jpg
cs-410_5_7_194,cs-410,5,7, Link Analysis - Part 2,"00:15:22,250","00:15:26,349",194,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=922,"Basically, the probability of reaching the",pic_cs-410_5_7_900.jpg
cs-410_5_7_195,cs-410,5,7, Link Analysis - Part 2,"00:15:26,349","00:15:29,246",195,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=926,"1, mainly because we have lost",pic_cs-410_5_7_900.jpg
cs-410_5_7_196,cs-410,5,7, Link Analysis - Part 2,"00:15:29,246","00:15:33,588",196,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=929,One would assume there's some probability,pic_cs-410_5_7_900.jpg
cs-410_5_7_197,cs-410,5,7, Link Analysis - Part 2,"00:15:33,588","00:15:37,160",197,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=933,"the links, but",pic_cs-410_5_7_900.jpg
cs-410_5_7_198,cs-410,5,7, Link Analysis - Part 2,"00:15:37,160","00:15:42,980",198,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=937,And one possible solution is simply to use,pic_cs-410_5_7_900.jpg
cs-410_5_7_199,cs-410,5,7, Link Analysis - Part 2,"00:15:42,980","00:15:45,270",199,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=942,and that could easily fix this.,pic_cs-410_5_7_900.jpg
cs-410_5_7_200,cs-410,5,7, Link Analysis - Part 2,"00:15:46,740","00:15:50,750",200,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=946,"Basically, that's to say alpha would",pic_cs-410_5_7_900.jpg
cs-410_5_7_201,cs-410,5,7, Link Analysis - Part 2,"00:15:50,750","00:15:54,130",201,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=950,"In that case,",pic_cs-410_5_7_900.jpg
cs-410_5_7_202,cs-410,5,7, Link Analysis - Part 2,"00:15:54,130","00:15:57,330",202,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=954,randomly jump to another page,pic_cs-410_5_7_900.jpg
cs-410_5_7_203,cs-410,5,7, Link Analysis - Part 2,"00:15:59,120","00:16:05,060",203,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=959,"There are many extensions of PageRank, one",pic_cs-410_5_7_900.jpg
cs-410_5_7_204,cs-410,5,7, Link Analysis - Part 2,"00:16:05,060","00:16:11,639",204,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=965,Note that PageRank doesn't merely,pic_cs-410_5_7_960.jpg
cs-410_5_7_205,cs-410,5,7, Link Analysis - Part 2,"00:16:11,639","00:16:15,370",205,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=971,So we can make PageRank specific however.,pic_cs-410_5_7_960.jpg
cs-410_5_7_206,cs-410,5,7, Link Analysis - Part 2,"00:16:15,370","00:16:19,260",206,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=975,"So for example,",pic_cs-410_5_7_960.jpg
cs-410_5_7_207,cs-410,5,7, Link Analysis - Part 2,"00:16:19,260","00:16:22,567",207,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=979,we can simply assume,pic_cs-410_5_7_960.jpg
cs-410_5_7_208,cs-410,5,7, Link Analysis - Part 2,"00:16:22,567","00:16:25,660",208,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=982,The surfer is not randomly,pic_cs-410_5_7_960.jpg
cs-410_5_7_209,cs-410,5,7, Link Analysis - Part 2,"00:16:25,660","00:16:32,320",209,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=985,"Instead, he's going to jump to only those",pic_cs-410_5_7_960.jpg
cs-410_5_7_210,cs-410,5,7, Link Analysis - Part 2,"00:16:32,320","00:16:36,780",210,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=992,"For example, if the query is not sports",pic_cs-410_5_7_960.jpg
cs-410_5_7_211,cs-410,5,7, Link Analysis - Part 2,"00:16:36,780","00:16:40,670",211,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=996,"doing random jumping, it's going",pic_cs-410_5_7_960.jpg
cs-410_5_7_212,cs-410,5,7, Link Analysis - Part 2,"00:16:40,670","00:16:45,350",212,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1000,"By doing this, then we can buy",pic_cs-410_5_7_960.jpg
cs-410_5_7_213,cs-410,5,7, Link Analysis - Part 2,"00:16:45,350","00:16:49,054",213,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1005,And then if you know the current,pic_cs-410_5_7_960.jpg
cs-410_5_7_214,cs-410,5,7, Link Analysis - Part 2,"00:16:49,054","00:16:53,368",214,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1009,then you can use this specialized,pic_cs-410_5_7_960.jpg
cs-410_5_7_215,cs-410,5,7, Link Analysis - Part 2,"00:16:53,368","00:16:57,754",215,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1013,That would be better than if you,pic_cs-410_5_7_960.jpg
cs-410_5_7_216,cs-410,5,7, Link Analysis - Part 2,"00:16:57,754","00:17:01,877",216,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1017,PageRank is also a channel that can be,pic_cs-410_5_7_960.jpg
cs-410_5_7_217,cs-410,5,7, Link Analysis - Part 2,"00:17:01,877","00:17:06,100",217,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1021,network analysis particularly for,pic_cs-410_5_7_1020.jpg
cs-410_5_7_218,cs-410,5,7, Link Analysis - Part 2,"00:17:06,100","00:17:09,970",218,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1026,You can imagine if you compute,pic_cs-410_5_7_1020.jpg
cs-410_5_7_219,cs-410,5,7, Link Analysis - Part 2,"00:17:09,970","00:17:14,356",219,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1029,"social network, where a link",pic_cs-410_5_7_1020.jpg
cs-410_5_7_220,cs-410,5,7, Link Analysis - Part 2,"00:17:14,356","00:17:18,744",220,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1034,"a relation, you would get some",pic_cs-410_5_7_1020.jpg
cs-410_5_7_221,cs-410,5,7, Link Analysis - Part 2,"00:17:18,744","00:17:28,744",221,https://www.coursera.org/learn/cs-410/lecture/GUQ1Q?t=1038,[MUSIC],pic_cs-410_5_7_1020.jpg
cs-410_5_8_1,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:00,000","00:00:06,073",1,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=0,[SOUND],pic_cs-410_5_8_0.jpg
cs-410_5_8_2,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:06,073","00:00:13,035",2,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=6,we talked about PageRank as,pic_cs-410_5_8_0.jpg
cs-410_5_8_3,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:14,155","00:00:21,245",3,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=14,"Now, we also looked at some other examples",pic_cs-410_5_8_0.jpg
cs-410_5_8_4,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:21,245","00:00:24,425",4,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=21,"So there is another algorithm called HITS,",pic_cs-410_5_8_0.jpg
cs-410_5_8_5,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:24,425","00:00:28,257",5,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=24,that going to compute the scores for,pic_cs-410_5_8_0.jpg
cs-410_5_8_6,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:28,257","00:00:33,167",6,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=28,The intuitions are pages that are widely,pic_cs-410_5_8_0.jpg
cs-410_5_8_7,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:33,167","00:00:38,577",7,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=33,whereas pages that cite many,pic_cs-410_5_8_0.jpg
cs-410_5_8_8,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:38,577","00:00:42,650",8,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=38,I think that the most interesting,pic_cs-410_5_8_0.jpg
cs-410_5_8_9,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:42,650","00:00:46,930",9,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=42,is it's going to use,pic_cs-410_5_8_0.jpg
cs-410_5_8_10,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:46,930","00:00:51,470",10,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=46,to kind of help improve the scoring for,pic_cs-410_5_8_0.jpg
cs-410_5_8_11,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:51,470","00:00:53,318",11,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=51,"And so here's the idea,",pic_cs-410_5_8_0.jpg
cs-410_5_8_12,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:53,318","00:00:57,809",12,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=53,it was assumed that good,pic_cs-410_5_8_0.jpg
cs-410_5_8_13,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:00:58,870","00:01:03,847",13,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=58,That means if you are cited by many,pic_cs-410_5_8_0.jpg
cs-410_5_8_14,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:03,847","00:01:07,266",14,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=63,"that inquiry says, you're an authority.",pic_cs-410_5_8_60.jpg
cs-410_5_8_15,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:07,266","00:01:11,740",15,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=67,"And similarly, good hubs are those",pic_cs-410_5_8_60.jpg
cs-410_5_8_16,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:11,740","00:01:15,560",16,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=71,So if you pointed to a lot,pic_cs-410_5_8_60.jpg
cs-410_5_8_17,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:15,560","00:01:17,880",17,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=75,then your hubs score would be increased.,pic_cs-410_5_8_60.jpg
cs-410_5_8_18,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:17,880","00:01:22,115",18,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=77,So then you will have literally reinforced,pic_cs-410_5_8_60.jpg
cs-410_5_8_19,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:22,115","00:01:22,968",19,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=82,some good hubs.,pic_cs-410_5_8_60.jpg
cs-410_5_8_20,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:22,968","00:01:27,635",20,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=82,And so you have pointed to some good,pic_cs-410_5_8_60.jpg
cs-410_5_8_21,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:27,635","00:01:30,544",21,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=87,whereas those authority,pic_cs-410_5_8_60.jpg
cs-410_5_8_22,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:30,544","00:01:34,736",22,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=90,improved because they,pic_cs-410_5_8_60.jpg
cs-410_5_8_23,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:34,736","00:01:39,380",23,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=94,And this is algorithms is also general it,pic_cs-410_5_8_60.jpg
cs-410_5_8_24,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:39,380","00:01:40,730",24,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=99,network analysis.,pic_cs-410_5_8_60.jpg
cs-410_5_8_25,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:40,730","00:01:43,170",25,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=100,"So just briefly, here's how it works.",pic_cs-410_5_8_60.jpg
cs-410_5_8_26,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:43,170","00:01:47,090",26,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=103,"We first also construct a matrix, but this",pic_cs-410_5_8_60.jpg
cs-410_5_8_27,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:47,090","00:01:49,750",27,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=107,matrix and,pic_cs-410_5_8_60.jpg
cs-410_5_8_28,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:49,750","00:01:54,100",28,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=109,"So if there's a link there's a 1,",pic_cs-410_5_8_60.jpg
cs-410_5_8_29,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:54,100","00:01:56,185",29,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=114,"Again, it's the same graph.",pic_cs-410_5_8_60.jpg
cs-410_5_8_30,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:01:56,185","00:02:01,335",30,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=116,And then we're going to,pic_cs-410_5_8_60.jpg
cs-410_5_8_31,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:01,335","00:02:06,955",31,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=121,as the sum of the authority scores of,pic_cs-410_5_8_120.jpg
cs-410_5_8_32,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:08,270","00:02:09,620",32,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=128,"So whether you are hub,",pic_cs-410_5_8_120.jpg
cs-410_5_8_33,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:09,620","00:02:14,430",33,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=129,really depends on whether you are pointing,pic_cs-410_5_8_120.jpg
cs-410_5_8_34,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:14,430","00:02:17,080",34,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=134,That's what it says in the first equation.,pic_cs-410_5_8_120.jpg
cs-410_5_8_35,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:17,080","00:02:22,130",35,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=137,"In the second equation,",pic_cs-410_5_8_120.jpg
cs-410_5_8_36,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:22,130","00:02:27,350",36,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=142,as a sum of the hub scores of all,pic_cs-410_5_8_120.jpg
cs-410_5_8_37,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:27,350","00:02:30,260",37,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=147,So whether you are good authority,pic_cs-410_5_8_120.jpg
cs-410_5_8_38,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:30,260","00:02:33,420",38,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=150,pages that are pointing,pic_cs-410_5_8_120.jpg
cs-410_5_8_39,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:33,420","00:02:37,380",39,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=153,So you can see this forms,pic_cs-410_5_8_120.jpg
cs-410_5_8_40,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:38,770","00:02:44,586",40,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=158,"Now, these three questions can be",pic_cs-410_5_8_120.jpg
cs-410_5_8_41,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:44,586","00:02:50,707",41,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=164,So what we get here is then the hub,pic_cs-410_5_8_120.jpg
cs-410_5_8_42,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:50,707","00:02:55,770",42,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=170,of the adjacency matrix and,pic_cs-410_5_8_120.jpg
cs-410_5_8_43,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:02:55,770","00:03:00,026",43,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=175,and this is basically the first equation.,pic_cs-410_5_8_120.jpg
cs-410_5_8_44,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:00,026","00:03:05,292",44,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=180,"And similarly, the second equation",pic_cs-410_5_8_180.jpg
cs-410_5_8_45,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:05,292","00:03:11,034",45,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=185,vector is equal to the product of,pic_cs-410_5_8_180.jpg
cs-410_5_8_46,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:11,034","00:03:15,820",46,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=191,"Now, these are just different ways",pic_cs-410_5_8_180.jpg
cs-410_5_8_47,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:15,820","00:03:19,967",47,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=195,But what's interesting is that,pic_cs-410_5_8_180.jpg
cs-410_5_8_48,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:19,967","00:03:26,680",48,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=199,you can also plug in the authority,pic_cs-410_5_8_180.jpg
cs-410_5_8_49,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:26,680","00:03:31,500",49,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=206,"So if you do that, you have actually",pic_cs-410_5_8_180.jpg
cs-410_5_8_50,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:31,500","00:03:33,930",50,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=211,and you get the equations,pic_cs-410_5_8_180.jpg
cs-410_5_8_51,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:34,980","00:03:39,032",51,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=214,The hubs score vector is,pic_cs-410_5_8_180.jpg
cs-410_5_8_52,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:39,032","00:03:43,522",52,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=219,by a transpose multiplied,pic_cs-410_5_8_180.jpg
cs-410_5_8_53,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:43,522","00:03:47,511",53,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=223,"Similarly, we can do a transformation",pic_cs-410_5_8_180.jpg
cs-410_5_8_54,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:47,511","00:03:49,440",54,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=227,just the authorities also.,pic_cs-410_5_8_180.jpg
cs-410_5_8_55,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:49,440","00:03:54,370",55,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=229,So although we frame the problem,pic_cs-410_5_8_180.jpg
cs-410_5_8_56,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:54,370","00:03:58,410",56,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=234,we can actually eliminate one of them to,pic_cs-410_5_8_180.jpg
cs-410_5_8_57,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:03:59,530","00:04:03,810",57,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=239,"Now, the difference between this and page",pic_cs-410_5_8_180.jpg
cs-410_5_8_58,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:03,810","00:04:07,960",58,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=243,a multiplication of the adjacency,pic_cs-410_5_8_240.jpg
cs-410_5_8_59,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:07,960","00:04:09,840",59,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=247,So this is different from page rank.,pic_cs-410_5_8_240.jpg
cs-410_5_8_60,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:11,250","00:04:15,373",60,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=251,"But mathematically, then we will",pic_cs-410_5_8_240.jpg
cs-410_5_8_61,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:15,373","00:04:19,777",61,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=255,"So in HITS,",pic_cs-410_5_8_240.jpg
cs-410_5_8_62,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:19,777","00:04:22,215",62,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=259,"Let's say, 1 for all these values, and",pic_cs-410_5_8_240.jpg
cs-410_5_8_63,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:22,215","00:04:26,580",63,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=262,then we would iteratively apply,pic_cs-410_5_8_240.jpg
cs-410_5_8_64,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:26,580","00:04:33,100",64,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=266,And this is equivalent to multiply,pic_cs-410_5_8_240.jpg
cs-410_5_8_65,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:34,720","00:04:37,740",65,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=274,So the arrows of these is exactly,pic_cs-410_5_8_240.jpg
cs-410_5_8_66,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:37,740","00:04:43,035",66,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=277,But here because the adjacency,pic_cs-410_5_8_240.jpg
cs-410_5_8_67,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:43,035","00:04:47,463",67,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=283,So what we have to do is after each,pic_cs-410_5_8_240.jpg
cs-410_5_8_68,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:47,463","00:04:50,980",68,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=287,this would allow us to,pic_cs-410_5_8_240.jpg
cs-410_5_8_69,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:50,980","00:04:53,671",69,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=290,Otherwise they would grow larger and,pic_cs-410_5_8_240.jpg
cs-410_5_8_70,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:53,671","00:04:57,180",70,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=293,"And if we do that, and",pic_cs-410_5_8_240.jpg
cs-410_5_8_71,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:04:58,360","00:05:03,920",71,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=298,"That was the computer, the hubs scores,",pic_cs-410_5_8_240.jpg
cs-410_5_8_72,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:03,920","00:05:08,647",72,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=303,And these scores can then be used in,pic_cs-410_5_8_300.jpg
cs-410_5_8_73,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:09,860","00:05:14,525",73,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=309,"So to summarize in this lecture, we have",pic_cs-410_5_8_300.jpg
cs-410_5_8_74,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:14,525","00:05:19,302",74,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=314,"In particular,",pic_cs-410_5_8_300.jpg
cs-410_5_8_75,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:19,302","00:05:23,737",75,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=319,increase the text,pic_cs-410_5_8_300.jpg
cs-410_5_8_76,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:23,737","00:05:25,959",76,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=323,And we also talk about the PageRank and,pic_cs-410_5_8_300.jpg
cs-410_5_8_77,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:25,959","00:05:29,380",77,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=325,page anchor as two major,pic_cs-410_5_8_300.jpg
cs-410_5_8_78,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:29,380","00:05:35,930",78,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=329,Both can generate scores for web pages,pic_cs-410_5_8_300.jpg
cs-410_5_8_79,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:35,930","00:05:39,600",79,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=335,Note that PageRank and,pic_cs-410_5_8_300.jpg
cs-410_5_8_80,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:39,600","00:05:46,663",80,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=339,So they have many applications in,pic_cs-410_5_8_300.jpg
cs-410_5_8_81,cs-410,5,8, Link Analysis - Part 3 (OPTIONAL),"00:05:46,663","00:05:56,663",81,https://www.coursera.org/learn/cs-410/lecture/d6INf?t=346,[MUSIC],pic_cs-410_5_8_300.jpg
cs-410_6_1_1,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:00,000","00:00:02,695",1,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=0,[MUSIC],pic_cs-410_6_1_0.jpg
cs-410_6_1_2,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:07,363","00:00:10,900",2,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=7,This lecture is about,pic_cs-410_6_1_0.jpg
cs-410_6_1_3,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:10,900","00:00:15,210",3,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=10,"In this lecture, we are going to",pic_cs-410_6_1_0.jpg
cs-410_6_1_4,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:15,210","00:00:17,860",4,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=15,In particular we're going to talk,pic_cs-410_6_1_0.jpg
cs-410_6_1_5,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:17,860","00:00:21,339",5,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=17,to combine different features,pic_cs-410_6_1_0.jpg
cs-410_6_1_6,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:22,340","00:00:28,500",6,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=22,So the question that we address in,pic_cs-410_6_1_0.jpg
cs-410_6_1_7,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:28,500","00:00:36,230",7,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=28,many features to generate a single ranking,pic_cs-410_6_1_0.jpg
cs-410_6_1_8,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:36,230","00:00:42,140",8,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=36,In the previous lectures we have talked,pic_cs-410_6_1_0.jpg
cs-410_6_1_9,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:42,140","00:00:48,270",9,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=42,We have talked about some retrieval,pic_cs-410_6_1_0.jpg
cs-410_6_1_10,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:48,270","00:00:54,760",10,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=48,They can generate a based this course for,pic_cs-410_6_1_0.jpg
cs-410_6_1_11,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:54,760","00:00:58,130",11,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=54,And we also talked about the link,pic_cs-410_6_1_0.jpg
cs-410_6_1_12,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:00:59,230","00:01:02,759",12,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=59,that can give additional scores,pic_cs-410_6_1_0.jpg
cs-410_6_1_13,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:03,940","00:01:07,313",13,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=63,"Now the question now is,",pic_cs-410_6_1_60.jpg
cs-410_6_1_14,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:07,313","00:01:09,843",14,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=67,potentially many other,pic_cs-410_6_1_60.jpg
cs-410_6_1_15,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:09,843","00:01:14,912",15,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=69,And this will be very useful for,pic_cs-410_6_1_60.jpg
cs-410_6_1_16,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:14,912","00:01:19,833",16,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=74,"accuracy, but also to improve",pic_cs-410_6_1_60.jpg
cs-410_6_1_17,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:19,833","00:01:24,176",17,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=79,So that it's not easy for,pic_cs-410_6_1_60.jpg
cs-410_6_1_18,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:24,176","00:01:26,720",18,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=84,a few features to promote a page.,pic_cs-410_6_1_60.jpg
cs-410_6_1_19,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:27,910","00:01:32,000",19,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=87,So the general idea of learning,pic_cs-410_6_1_60.jpg
cs-410_6_1_20,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:32,000","00:01:36,030",20,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=92,learning to combine this,pic_cs-410_6_1_60.jpg
cs-410_6_1_21,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:36,030","00:01:39,160",21,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=96,on different features to generate,pic_cs-410_6_1_60.jpg
cs-410_6_1_22,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:40,610","00:01:44,720",22,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=100,So we will assume that the given,pic_cs-410_6_1_60.jpg
cs-410_6_1_23,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:44,720","00:01:49,680",23,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=104,we can define a number of features.,pic_cs-410_6_1_60.jpg
cs-410_6_1_24,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:49,680","00:01:55,180",24,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=109,And these features can vary from,pic_cs-410_6_1_60.jpg
cs-410_6_1_25,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:55,180","00:01:59,875",25,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=115,a score of the document with,pic_cs-410_6_1_60.jpg
cs-410_6_1_26,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:01:59,875","00:02:05,060",26,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=119,a retrieval function such as BM25 or,pic_cs-410_6_1_60.jpg
cs-410_6_1_27,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:05,060","00:02:10,440",27,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=125,of punitive commands from a machine or,pic_cs-410_6_1_120.jpg
cs-410_6_1_28,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:10,440","00:02:15,410",28,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=130,It can also be a link based score like or,pic_cs-410_6_1_120.jpg
cs-410_6_1_29,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:15,410","00:02:23,470",29,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=135,It can be also application of retrieval,pic_cs-410_6_1_120.jpg
cs-410_6_1_30,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:23,470","00:02:28,240",30,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=143,Those are the types of descriptions,pic_cs-410_6_1_120.jpg
cs-410_6_1_31,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:29,520","00:02:33,909",31,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=149,"So, these can all the clues whether",pic_cs-410_6_1_120.jpg
cs-410_6_1_32,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:35,070","00:02:41,320",32,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=155,We can even include a feature,pic_cs-410_6_1_120.jpg
cs-410_6_1_33,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:41,320","00:02:46,680",33,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=161,has a tilde because this might be,pic_cs-410_6_1_120.jpg
cs-410_6_1_34,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:48,170","00:02:52,180",34,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=168,So all these features can then be combined,pic_cs-410_6_1_120.jpg
cs-410_6_1_35,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:52,180","00:02:53,610",35,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=172,"The question is, of course.",pic_cs-410_6_1_120.jpg
cs-410_6_1_36,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:53,610","00:02:55,250",36,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=173,How can we combine them?,pic_cs-410_6_1_120.jpg
cs-410_6_1_37,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:02:55,250","00:03:00,580",37,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=175,"In this approach,",pic_cs-410_6_1_120.jpg
cs-410_6_1_38,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:00,580","00:03:07,730",38,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=180,that this document isn't relevant to this,pic_cs-410_6_1_180.jpg
cs-410_6_1_39,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:07,730","00:03:10,329",39,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=187,So we can hypothesize this,pic_cs-410_6_1_180.jpg
cs-410_6_1_40,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:11,450","00:03:16,730",40,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=191,that the probability of relevance,pic_cs-410_6_1_180.jpg
cs-410_6_1_41,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:16,730","00:03:22,070",41,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=196,through a particular form of,pic_cs-410_6_1_180.jpg
cs-410_6_1_42,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:22,070","00:03:25,510",42,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=202,These parameters can control,pic_cs-410_6_1_180.jpg
cs-410_6_1_43,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:25,510","00:03:29,410",43,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=205,the influence of different,pic_cs-410_6_1_180.jpg
cs-410_6_1_44,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:29,410","00:03:33,820",44,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=209,Now this is of course just an assumption.,pic_cs-410_6_1_180.jpg
cs-410_6_1_45,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:33,820","00:03:38,925",45,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=213,Whether this assumption really,pic_cs-410_6_1_180.jpg
cs-410_6_1_46,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:38,925","00:03:43,570",46,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=218,that's they have to empirically,pic_cs-410_6_1_180.jpg
cs-410_6_1_47,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:45,020","00:03:50,450",47,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=225,But by hypothesizing that,pic_cs-410_6_1_180.jpg
cs-410_6_1_48,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:50,450","00:03:55,783",48,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=230,"features in the particular way, we can",pic_cs-410_6_1_180.jpg
cs-410_6_1_49,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:03:55,783","00:04:00,805",49,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=235,the potential more powerful ranking,pic_cs-410_6_1_180.jpg
cs-410_6_1_50,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:00,805","00:04:05,342",50,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=240,Naturally the next question is how,pic_cs-410_6_1_240.jpg
cs-410_6_1_51,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:05,342","00:04:08,732",51,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=245,How do we know which features,pic_cs-410_6_1_240.jpg
cs-410_6_1_52,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:08,732","00:04:11,922",52,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=248,and which features will have lower weight?,pic_cs-410_6_1_240.jpg
cs-410_6_1_53,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:11,922","00:04:15,732",53,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=251,So this is the task of training or,pic_cs-410_6_1_240.jpg
cs-410_6_1_54,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:15,732","00:04:20,000",54,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=255,in this approach what we will,pic_cs-410_6_1_240.jpg
cs-410_6_1_55,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:20,000","00:04:24,910",55,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=260,Those are the data that have,pic_cs-410_6_1_240.jpg
cs-410_6_1_56,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:24,910","00:04:27,370",56,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=264,that we already know,pic_cs-410_6_1_240.jpg
cs-410_6_1_57,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:27,370","00:04:31,443",57,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=267,We already know which documents should,pic_cs-410_6_1_240.jpg
cs-410_6_1_58,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:31,443","00:04:36,074",58,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=271,And this information can be based,pic_cs-410_6_1_240.jpg
cs-410_6_1_59,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:36,074","00:04:41,508",59,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=276,this can also be approximated by just,pic_cs-410_6_1_240.jpg
cs-410_6_1_60,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:41,508","00:04:47,477",60,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=281,where we can assume the clicked documents,pic_cs-410_6_1_240.jpg
cs-410_6_1_61,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:47,477","00:04:53,500",61,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=287,clicked documents are relevant and,pic_cs-410_6_1_240.jpg
cs-410_6_1_62,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:53,500","00:04:58,222",62,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=293,So in general with the fit,pic_cs-410_6_1_240.jpg
cs-410_6_1_63,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:04:58,222","00:05:00,960",63,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=298,function to the training data,pic_cs-410_6_1_240.jpg
cs-410_6_1_64,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:00,960","00:05:06,650",64,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=300,meaning that we will try to optimize it's,pic_cs-410_6_1_300.jpg
cs-410_6_1_65,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:06,650","00:05:08,920",65,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=306,And we can adjust these parameters to see,pic_cs-410_6_1_300.jpg
cs-410_6_1_66,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:09,960","00:05:14,780",66,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=309,how we can optimize the performance of,pic_cs-410_6_1_300.jpg
cs-410_6_1_67,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:16,030","00:05:19,180",67,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=316,in terms of some measures such as MAP or,pic_cs-410_6_1_300.jpg
cs-410_6_1_68,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:20,600","00:05:25,440",68,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=320,So the training date would,pic_cs-410_6_1_300.jpg
cs-410_6_1_69,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:25,440","00:05:32,800",69,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=325,"Each tuple has three elements, the query,",pic_cs-410_6_1_300.jpg
cs-410_6_1_70,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:32,800","00:05:37,469",70,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=332,So it looks very much like our,pic_cs-410_6_1_300.jpg
cs-410_6_1_71,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:37,469","00:05:40,933",71,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=337,about in the evaluation,pic_cs-410_6_1_300.jpg
cs-410_6_1_72,cs-410,6,1, Learning to Rank - Part 1 (OPTIONAL),"00:05:40,933","00:05:50,933",72,https://www.coursera.org/learn/cs-410/lecture/mFYTD?t=340,[MUSIC],pic_cs-410_6_1_300.jpg
cs-410_6_2_1,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:00,076","00:00:03,466",1,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=0,[MUSIC],pic_cs-410_6_2_0.jpg
cs-410_6_2_2,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:06,698","00:00:14,732",2,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=6,So now let's take a look at the specific,pic_cs-410_6_2_0.jpg
cs-410_6_2_3,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:14,732","00:00:17,627",3,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=14,"Now, this is one of the many",pic_cs-410_6_2_0.jpg
cs-410_6_2_4,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:17,627","00:00:19,309",4,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=17,it's one of the simplest methods.,pic_cs-410_6_2_0.jpg
cs-410_6_2_5,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:19,309","00:00:24,550",5,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=19,And I choose this to explain,pic_cs-410_6_2_0.jpg
cs-410_6_2_6,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:26,360","00:00:34,350",6,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=26,"So in this approach, we simply assume",pic_cs-410_6_2_0.jpg
cs-410_6_2_7,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:34,350","00:00:39,760",7,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=34,respect to a query is related to a linear,pic_cs-410_6_2_0.jpg
cs-410_6_2_8,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:39,760","00:00:47,400",8,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=39,Here I used Xi to denote the feature.,pic_cs-410_6_2_0.jpg
cs-410_6_2_9,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:47,400","00:00:51,770",9,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=47,So Xi of Q and D is a feature.,pic_cs-410_6_2_0.jpg
cs-410_6_2_10,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:51,770","00:00:54,720",10,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=51,And we can have as many,pic_cs-410_6_2_0.jpg
cs-410_6_2_11,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:00:55,890","00:01:01,670",11,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=55,And we assume that these features,pic_cs-410_6_2_0.jpg
cs-410_6_2_12,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:03,580","00:01:06,150",12,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=63,And each feature is controlled,pic_cs-410_6_2_60.jpg
cs-410_6_2_13,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:06,150","00:01:08,880",13,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=66,and this beta i is a parameter.,pic_cs-410_6_2_60.jpg
cs-410_6_2_14,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:08,880","00:01:10,790",14,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=68,That's a weighting parameter.,pic_cs-410_6_2_60.jpg
cs-410_6_2_15,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:10,790","00:01:15,940",15,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=70,A larger value would mean the feature,pic_cs-410_6_2_60.jpg
cs-410_6_2_16,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:15,940","00:01:18,525",16,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=75,and it would contribute more,pic_cs-410_6_2_60.jpg
cs-410_6_2_17,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:18,525","00:01:23,069",17,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=78,This specific form of the function,pic_cs-410_6_2_60.jpg
cs-410_6_2_18,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:23,069","00:01:27,154",18,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=83,a transformation of,pic_cs-410_6_2_60.jpg
cs-410_6_2_19,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:27,154","00:01:30,428",19,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=87,So this is the probability of relevance.,pic_cs-410_6_2_60.jpg
cs-410_6_2_20,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:30,428","00:01:35,611",20,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=90,And we know that the probability of,pic_cs-410_6_2_60.jpg
cs-410_6_2_21,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:36,870","00:01:41,863",21,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=96,And we could have just assumed that,pic_cs-410_6_2_60.jpg
cs-410_6_2_22,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:41,863","00:01:43,856",22,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=101,this linear combination.,pic_cs-410_6_2_60.jpg
cs-410_6_2_23,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:43,856","00:01:47,479",23,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=103,So we can do a linear regression.,pic_cs-410_6_2_60.jpg
cs-410_6_2_24,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:47,479","00:01:53,940",24,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=107,"But then, the value of this linear",pic_cs-410_6_2_60.jpg
cs-410_6_2_25,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:53,940","00:01:59,220",25,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=113,So this transformation,pic_cs-410_6_2_60.jpg
cs-410_6_2_26,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:01:59,220","00:02:05,540",26,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=119,to 1 range to the whole,pic_cs-410_6_2_60.jpg
cs-410_6_2_27,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:05,540","00:02:08,330",27,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=125,you can verify it by yourself.,pic_cs-410_6_2_120.jpg
cs-410_6_2_28,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:10,350","00:02:16,700",28,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=130,So this allows us then to connect,pic_cs-410_6_2_120.jpg
cs-410_6_2_29,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:16,700","00:02:23,010",29,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=136,which is between 0 and 1 to a linear,pic_cs-410_6_2_120.jpg
cs-410_6_2_30,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:23,010","00:02:28,133",30,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=143,And if we rewrite this into a probability,pic_cs-410_6_2_120.jpg
cs-410_6_2_31,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:28,133","00:02:34,299",31,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=148,"So on this equation, now we'll",pic_cs-410_6_2_120.jpg
cs-410_6_2_32,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:35,690","00:02:39,168",32,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=155,"And on the right hand side,",pic_cs-410_6_2_120.jpg
cs-410_6_2_33,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:39,168","00:02:42,448",33,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=159,"Now, this form is clearly nonnegative, and",pic_cs-410_6_2_120.jpg
cs-410_6_2_34,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:42,448","00:02:46,344",34,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=162,it still involves a linear,pic_cs-410_6_2_120.jpg
cs-410_6_2_35,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:46,344","00:02:50,890",35,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=166,"And it's also clear that if this value is,",pic_cs-410_6_2_120.jpg
cs-410_6_2_36,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:50,890","00:02:58,991",36,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=170,this is actually negative of the linear,pic_cs-410_6_2_120.jpg
cs-410_6_2_37,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:02:58,991","00:03:04,415",37,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=178,"If this value here is large,",pic_cs-410_6_2_120.jpg
cs-410_6_2_38,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:04,415","00:03:11,879",38,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=184,then it would mean this value is small.,pic_cs-410_6_2_180.jpg
cs-410_6_2_39,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:11,879","00:03:17,278",39,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=191,"And therefore,",pic_cs-410_6_2_180.jpg
cs-410_6_2_40,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:17,278","00:03:22,034",40,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=197,"And that's we expect, that basically,",pic_cs-410_6_2_180.jpg
cs-410_6_2_41,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:22,034","00:03:26,496",41,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=202,"gives us a high value, then",pic_cs-410_6_2_180.jpg
cs-410_6_2_42,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:26,496","00:03:29,015",42,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=206,So this is our hypothesis.,pic_cs-410_6_2_180.jpg
cs-410_6_2_43,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:29,015","00:03:33,955",43,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=209,"Again, this is not necessarily the best",pic_cs-410_6_2_180.jpg
cs-410_6_2_44,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:33,955","00:03:39,109",44,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=213,way to connect these features with,pic_cs-410_6_2_180.jpg
cs-410_6_2_45,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:40,470","00:03:44,578",45,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=220,So now we have this combination function.,pic_cs-410_6_2_180.jpg
cs-410_6_2_46,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:44,578","00:03:48,617",46,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=224,The next task is to,pic_cs-410_6_2_180.jpg
cs-410_6_2_47,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:48,617","00:03:52,346",47,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=228,that the function cache will be applied.,pic_cs-410_6_2_180.jpg
cs-410_6_2_48,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:52,346","00:03:57,430",48,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=232,"But without knowing the beta values,",pic_cs-410_6_2_180.jpg
cs-410_6_2_49,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:03:58,520","00:04:04,068",49,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=238,So let's see how can,pic_cs-410_6_2_180.jpg
cs-410_6_2_50,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:04,068","00:04:07,190",50,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=244,"All right,",pic_cs-410_6_2_240.jpg
cs-410_6_2_51,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:08,780","00:04:11,405",51,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=248,"In this example, we have three features.",pic_cs-410_6_2_240.jpg
cs-410_6_2_52,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:11,405","00:04:15,060",52,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=251,One is the BM25 score of the document and,pic_cs-410_6_2_240.jpg
cs-410_6_2_53,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:15,060","00:04:19,044",53,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=255,"One is the PageRank score of the document,",pic_cs-410_6_2_240.jpg
cs-410_6_2_54,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:19,044","00:04:21,211",54,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=259,might not depend on the query.,pic_cs-410_6_2_240.jpg
cs-410_6_2_55,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:21,211","00:04:25,681",55,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=261,"We might have a topic-sensitive PageRank,",pic_cs-410_6_2_240.jpg
cs-410_6_2_56,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:25,681","00:04:29,946",56,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=265,"Otherwise, the general PageRank",pic_cs-410_6_2_240.jpg
cs-410_6_2_57,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:29,946","00:04:35,221",57,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=269,And then we have BM25 score on,pic_cs-410_6_2_240.jpg
cs-410_6_2_58,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:35,221","00:04:40,630",58,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=275,"Now, these are then the feature values for",pic_cs-410_6_2_240.jpg
cs-410_6_2_59,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:41,910","00:04:47,370",59,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=281,"And in this case, the document is D1 and",pic_cs-410_6_2_240.jpg
cs-410_6_2_60,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:48,790","00:04:54,547",60,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=288,Here's another training instance and,pic_cs-410_6_2_240.jpg
cs-410_6_2_61,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:54,547","00:04:57,832",61,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=294,"but in this case, it's not relevant.",pic_cs-410_6_2_240.jpg
cs-410_6_2_62,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:04:57,832","00:05:02,806",62,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=297,This is an oversimplified case where,pic_cs-410_6_2_240.jpg
cs-410_6_2_63,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:02,806","00:05:06,675",63,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=302,it's sufficient to illustrate the point.,pic_cs-410_6_2_300.jpg
cs-410_6_2_64,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:06,675","00:05:09,885",64,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=306,So what we can do is we use,pic_cs-410_6_2_300.jpg
cs-410_6_2_65,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:09,885","00:05:11,797",65,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=309,actually estimate the parameters.,pic_cs-410_6_2_300.jpg
cs-410_6_2_66,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:13,170","00:05:17,801",66,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=313,"Basically, we're going to",pic_cs-410_6_2_300.jpg
cs-410_6_2_67,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:17,801","00:05:22,040",67,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=317,of the document based,pic_cs-410_6_2_300.jpg
cs-410_6_2_68,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:22,040","00:05:25,534",68,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=322,"That is, given that we observed",pic_cs-410_6_2_300.jpg
cs-410_6_2_69,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:28,264","00:05:32,653",69,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=328,Can we predict the relevance here?,pic_cs-410_6_2_300.jpg
cs-410_6_2_70,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:32,653","00:05:39,070",70,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=332,"Now, of course, the prediction would be",pic_cs-410_6_2_300.jpg
cs-410_6_2_71,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:39,070","00:05:42,680",71,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=339,And we hypothesize that the probability,pic_cs-410_6_2_300.jpg
cs-410_6_2_72,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:42,680","00:05:43,920",72,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=342,features in this way.,pic_cs-410_6_2_300.jpg
cs-410_6_2_73,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:43,920","00:05:51,260",73,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=343,"So we are going to see, for what values of",pic_cs-410_6_2_300.jpg
cs-410_6_2_74,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:51,260","00:05:58,480",74,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=351,What do we mean by predicting,pic_cs-410_6_2_300.jpg
cs-410_6_2_75,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:05:58,480","00:06:02,037",75,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=358,"Well, we just mean, in the first case, for",pic_cs-410_6_2_300.jpg
cs-410_6_2_76,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:02,037","00:06:06,667",76,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=362,D1 this expression right here,pic_cs-410_6_2_360.jpg
cs-410_6_2_77,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:06,667","00:06:10,452",77,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=366,"In fact, we'll hope this",pic_cs-410_6_2_360.jpg
cs-410_6_2_78,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:10,452","00:06:13,470",78,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=370,Why?,pic_cs-410_6_2_360.jpg
cs-410_6_2_79,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:14,750","00:06:17,954",79,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=374,"On the other hand,",pic_cs-410_6_2_360.jpg
cs-410_6_2_80,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:17,954","00:06:22,310",80,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=377,"we hope this value will be small, right.",pic_cs-410_6_2_360.jpg
cs-410_6_2_81,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:22,310","00:06:23,040",81,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=382,Why?,pic_cs-410_6_2_360.jpg
cs-410_6_2_82,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:23,040","00:06:26,310",82,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=383,Because it's a non-relevant document.,pic_cs-410_6_2_360.jpg
cs-410_6_2_83,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:26,310","00:06:30,250",83,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=386,So now let's see how this can,pic_cs-410_6_2_360.jpg
cs-410_6_2_84,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:30,250","00:06:34,954",84,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=390,And this is similar to expressing,pic_cs-410_6_2_360.jpg
cs-410_6_2_85,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:34,954","00:06:39,657",85,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=394,only that we are not talking about,pic_cs-410_6_2_360.jpg
cs-410_6_2_86,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:39,657","00:06:43,771",86,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=399,talking about the probability,pic_cs-410_6_2_360.jpg
cs-410_6_2_87,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:43,771","00:06:48,736",87,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=403,So what's the probability,pic_cs-410_6_2_360.jpg
cs-410_6_2_88,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:48,736","00:06:52,880",88,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=408,relevant if it has these feature values?,pic_cs-410_6_2_360.jpg
cs-410_6_2_89,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:54,250","00:06:57,890",89,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=414,"Well, this is just this expression.",pic_cs-410_6_2_360.jpg
cs-410_6_2_90,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:06:57,890","00:07:00,970",90,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=417,We just need to plug in the Xi's.,pic_cs-410_6_2_360.jpg
cs-410_6_2_91,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:00,970","00:07:03,296",91,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=420,So that's what we will get.,pic_cs-410_6_2_420.jpg
cs-410_6_2_92,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:03,296","00:07:08,116",92,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=423,"It's exactly like what we have seen above,",pic_cs-410_6_2_420.jpg
cs-410_6_2_93,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:08,116","00:07:14,772",93,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=428,only that we replaced these,pic_cs-410_6_2_420.jpg
cs-410_6_2_94,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:14,772","00:07:21,247",94,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=434,"So for example, this 0.7 goes to here and",pic_cs-410_6_2_420.jpg
cs-410_6_2_95,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:21,247","00:07:25,451",95,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=441,this 0.11 goes to here.,pic_cs-410_6_2_420.jpg
cs-410_6_2_96,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:25,451","00:07:27,369",96,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=445,"And these are different feature values,",pic_cs-410_6_2_420.jpg
cs-410_6_2_97,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:27,369","00:07:29,405",97,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=447,and we combine them in,pic_cs-410_6_2_420.jpg
cs-410_6_2_98,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:29,405","00:07:31,770",98,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=449,The beta values are still unknown.,pic_cs-410_6_2_420.jpg
cs-410_6_2_99,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:31,770","00:07:37,202",99,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=451,But this gives us the probability,pic_cs-410_6_2_420.jpg
cs-410_6_2_100,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:37,202","00:07:39,342",100,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=457,if we assume such a model.,pic_cs-410_6_2_420.jpg
cs-410_6_2_101,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:39,342","00:07:39,853",101,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=459,Okay?,pic_cs-410_6_2_420.jpg
cs-410_6_2_102,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:39,853","00:07:44,553",102,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=459,"And we want to maximize this probability,",pic_cs-410_6_2_420.jpg
cs-410_6_2_103,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:44,553","00:07:47,850",103,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=464,What do we do for the second document?,pic_cs-410_6_2_420.jpg
cs-410_6_2_104,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:47,850","00:07:53,309",104,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=467,"Well, we want to compute the probability",pic_cs-410_6_2_420.jpg
cs-410_6_2_105,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:07:53,309","00:08:00,257",105,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=473,So this would mean we have to,pic_cs-410_6_2_420.jpg
cs-410_6_2_106,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:00,257","00:08:07,880",106,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=480,since this expression is actually,pic_cs-410_6_2_480.jpg
cs-410_6_2_107,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:07,880","00:08:12,524",107,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=487,So to compute the non-relevance,pic_cs-410_6_2_480.jpg
cs-410_6_2_108,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:12,524","00:08:17,062",108,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=492,we just do 1 minus,pic_cs-410_6_2_480.jpg
cs-410_6_2_109,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:17,062","00:08:18,480",109,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=497,Okay?,pic_cs-410_6_2_480.jpg
cs-410_6_2_110,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:18,480","00:08:24,450",110,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=498,So this whole expression then,pic_cs-410_6_2_480.jpg
cs-410_6_2_111,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:24,450","00:08:29,220",111,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=504,predicting these two relevance values.,pic_cs-410_6_2_480.jpg
cs-410_6_2_112,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:29,220","00:08:32,759",112,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=509,"One is 1 here, one is 0.",pic_cs-410_6_2_480.jpg
cs-410_6_2_113,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:32,759","00:08:37,782",113,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=512,And this whole equation,pic_cs-410_6_2_480.jpg
cs-410_6_2_114,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:37,782","00:08:42,680",114,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=517,observing a 1 here and observing a 0 here.,pic_cs-410_6_2_480.jpg
cs-410_6_2_115,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:44,090","00:08:48,370",115,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=524,"Of course, this probability",pic_cs-410_6_2_480.jpg
cs-410_6_2_116,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:50,130","00:08:55,318",116,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=530,So then our goal is to adjust,pic_cs-410_6_2_480.jpg
cs-410_6_2_117,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:08:55,318","00:09:00,121",117,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=535,"thing reach its maximum,",pic_cs-410_6_2_480.jpg
cs-410_6_2_118,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:00,121","00:09:02,540",118,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=540,So that means we're going to compute this.,pic_cs-410_6_2_540.jpg
cs-410_6_2_119,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:02,540","00:09:07,280",119,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=542,The beta is just the parameter,pic_cs-410_6_2_540.jpg
cs-410_6_2_120,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:07,280","00:09:11,914",120,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=547,maximize this whole likelihood expression.,pic_cs-410_6_2_540.jpg
cs-410_6_2_121,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:11,914","00:09:16,284",121,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=551,"And what it means is,",pic_cs-410_6_2_540.jpg
cs-410_6_2_122,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:16,284","00:09:21,224",122,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=556,we're going to choose betas to,pic_cs-410_6_2_540.jpg
cs-410_6_2_123,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:21,224","00:09:26,449",123,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=561,"make this also as large as possible,",pic_cs-410_6_2_540.jpg
cs-410_6_2_124,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:26,449","00:09:29,400",124,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=566,make this part as small as possible.,pic_cs-410_6_2_540.jpg
cs-410_6_2_125,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:30,560","00:09:32,360",125,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=570,And this is precisely what we want.,pic_cs-410_6_2_540.jpg
cs-410_6_2_126,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:34,530","00:09:38,834",126,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=574,"So once we do the training,",pic_cs-410_6_2_540.jpg
cs-410_6_2_127,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:38,834","00:09:43,330",127,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=578,So then this function,pic_cs-410_6_2_540.jpg
cs-410_6_2_128,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:43,330","00:09:50,690",128,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=583,"Once beta values are known, both this and",pic_cs-410_6_2_540.jpg
cs-410_6_2_129,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:50,690","00:09:53,380",129,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=590,"So for any new query and new document,",pic_cs-410_6_2_540.jpg
cs-410_6_2_130,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:53,380","00:09:56,924",130,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=593,we can simply compute the features for,pic_cs-410_6_2_540.jpg
cs-410_6_2_131,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:09:56,924","00:10:00,941",131,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=596,And then we just use this formula,pic_cs-410_6_2_540.jpg
cs-410_6_2_132,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:10:00,941","00:10:06,700",132,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=600,And this scoring function can be used to,pic_cs-410_6_2_600.jpg
cs-410_6_2_133,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:10:06,700","00:10:11,787",133,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=606,So that's the basic idea,pic_cs-410_6_2_600.jpg
cs-410_6_2_134,cs-410,6,2, Learning to Rank - Part 2 (OPTIONAL),"00:10:11,787","00:10:21,787",134,https://www.coursera.org/learn/cs-410/lecture/3d9fD?t=611,[MUSIC],pic_cs-410_6_2_600.jpg
cs-410_6_3_1,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:00,008","00:00:07,386",1,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=0,[SOUND],pic_cs-410_6_3_0.jpg
cs-410_6_3_2,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:07,386","00:00:11,551",2,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=7,more of the Munster learning algorithms,pic_cs-410_6_3_0.jpg
cs-410_6_3_3,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:11,551","00:00:15,009",3,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=11,they generally attempt to direct,pic_cs-410_6_3_0.jpg
cs-410_6_3_4,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:16,690","00:00:18,010",4,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=16,Like a MAP or nDCG.,pic_cs-410_6_3_0.jpg
cs-410_6_3_5,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:19,020","00:00:24,640",5,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=19,Note that the optimization object or,pic_cs-410_6_3_0.jpg
cs-410_6_3_6,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:24,640","00:00:29,610",6,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=24,on the previous slide is not directly,pic_cs-410_6_3_0.jpg
cs-410_6_3_7,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:31,390","00:00:33,870",7,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=31,By maximizing the prediction of one or,pic_cs-410_6_3_0.jpg
cs-410_6_3_8,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:33,870","00:00:39,430",8,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=33,"zero, we don't necessarily optimize",pic_cs-410_6_3_0.jpg
cs-410_6_3_9,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:39,430","00:00:44,106",9,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=39,One can imagine that our,pic_cs-410_6_3_0.jpg
cs-410_6_3_10,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:44,106","00:00:46,626",10,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=44,And let's say both are around 0.5.,pic_cs-410_6_3_0.jpg
cs-410_6_3_11,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:46,626","00:00:51,230",11,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=46,So it's kind of in the middle of zero and,pic_cs-410_6_3_0.jpg
cs-410_6_3_12,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:00:51,230","00:00:58,250",12,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=51,"But the ranking can be wrong, so we might",pic_cs-410_6_3_0.jpg
cs-410_6_3_13,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:00,750","00:01:04,235",13,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=60,So that won't be good from,pic_cs-410_6_3_60.jpg
cs-410_6_3_14,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:04,235","00:01:07,420",14,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=64,"even though function, it's not bad.",pic_cs-410_6_3_60.jpg
cs-410_6_3_15,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:07,420","00:01:11,773",15,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=67,"In contrast, we might have another",pic_cs-410_6_3_60.jpg
cs-410_6_3_16,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:11,773","00:01:14,000",16,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=71,"around the 0.9, it said.",pic_cs-410_6_3_60.jpg
cs-410_6_3_17,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:14,000","00:01:17,580",17,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=74,"And by the objective function,",pic_cs-410_6_3_60.jpg
cs-410_6_3_18,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:17,580","00:01:20,500",18,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=77,But if we didn't get the order,pic_cs-410_6_3_60.jpg
cs-410_6_3_19,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:20,500","00:01:22,970",19,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=80,that's actually a better result.,pic_cs-410_6_3_60.jpg
cs-410_6_3_20,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:22,970","00:01:28,070",20,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=82,"So these new, more advanced approaches",pic_cs-410_6_3_60.jpg
cs-410_6_3_21,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:28,070","00:01:32,120",21,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=88,"Of course, then the challenge is",pic_cs-410_6_3_60.jpg
cs-410_6_3_22,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:32,120","00:01:33,700",22,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=92,be harder to solve.,pic_cs-410_6_3_60.jpg
cs-410_6_3_23,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:33,700","00:01:39,143",23,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=93,"And then, researchers have posed",pic_cs-410_6_3_60.jpg
cs-410_6_3_24,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:39,143","00:01:46,153",24,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=99,and you can read more of the references at,pic_cs-410_6_3_60.jpg
cs-410_6_3_25,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:46,153","00:01:50,540",25,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=106,"Now, these learning ranked",pic_cs-410_6_3_60.jpg
cs-410_6_3_26,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:50,540","00:01:53,530",26,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=110,So there accounts would be be applied,pic_cs-410_6_3_60.jpg
cs-410_6_3_27,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:53,530","00:01:55,350",27,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=113,not just the retrieval problem.,pic_cs-410_6_3_60.jpg
cs-410_6_3_28,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:55,350","00:01:58,993",28,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=115,So some people will go,pic_cs-410_6_3_60.jpg
cs-410_6_3_29,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:01:58,993","00:02:02,810",29,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=118,"computational advertising,",pic_cs-410_6_3_60.jpg
cs-410_6_3_30,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:02,810","00:02:08,636",30,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=122,there are many others that you can,pic_cs-410_6_3_120.jpg
cs-410_6_3_31,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:11,157","00:02:15,884",31,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=131,To summarize this lecture we,pic_cs-410_6_3_120.jpg
cs-410_6_3_32,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:15,884","00:02:21,270",32,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=135,learning to combine much more,pic_cs-410_6_3_120.jpg
cs-410_6_3_33,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:22,780","00:02:24,690",33,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=142,Actually the use of machine learning,pic_cs-410_6_3_120.jpg
cs-410_6_3_34,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:25,810","00:02:29,840",34,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=145,in information retrieval has,pic_cs-410_6_3_120.jpg
cs-410_6_3_35,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:29,840","00:02:35,212",35,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=149,"So for example, the Rocchio feedback",pic_cs-410_6_3_120.jpg
cs-410_6_3_36,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:35,212","00:02:40,700",36,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=155,was a machine learning approach,pic_cs-410_6_3_120.jpg
cs-410_6_3_37,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:40,700","00:02:46,750",37,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=160,But the most recent use of machine,pic_cs-410_6_3_120.jpg
cs-410_6_3_38,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:46,750","00:02:51,000",38,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=166,changes in the environment of,pic_cs-410_6_3_120.jpg
cs-410_6_3_39,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:52,550","00:02:58,650",39,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=172,"First, it's mostly freedom of",pic_cs-410_6_3_120.jpg
cs-410_6_3_40,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:02:58,650","00:03:04,250",40,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=178,"in the form of critical, such as",pic_cs-410_6_3_120.jpg
cs-410_6_3_41,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:04,250","00:03:11,106",41,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=184,So the data can provide a lot of,pic_cs-410_6_3_180.jpg
cs-410_6_3_42,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:11,106","00:03:17,487",42,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=191,machine learning methods can be,pic_cs-410_6_3_180.jpg
cs-410_6_3_43,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:17,487","00:03:21,744",43,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=197,"Secondly, it's also freedom by",pic_cs-410_6_3_180.jpg
cs-410_6_3_44,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:21,744","00:03:24,464",44,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=201,and this is not only just,pic_cs-410_6_3_180.jpg
cs-410_6_3_45,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:24,464","00:03:29,840",45,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=204,features available on the web that can,pic_cs-410_6_3_180.jpg
cs-410_6_3_46,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:29,840","00:03:36,208",46,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=209,"It's also because by combining them,",pic_cs-410_6_3_180.jpg
cs-410_6_3_47,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:36,208","00:03:41,168",47,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=216,"of ranking, so this is desired for",pic_cs-410_6_3_180.jpg
cs-410_6_3_48,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:41,168","00:03:45,887",48,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=221,Modern search engines all use some,pic_cs-410_6_3_180.jpg
cs-410_6_3_49,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:45,887","00:03:48,855",49,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=225,combine many features,pic_cs-410_6_3_180.jpg
cs-410_6_3_50,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:48,855","00:03:53,590",50,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=228,this is a major feature of these,pic_cs-410_6_3_180.jpg
cs-410_6_3_51,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:03:56,190","00:04:02,368",51,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=236,The topic of learning to rank is still,pic_cs-410_6_3_180.jpg
cs-410_6_3_52,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:04:02,368","00:04:08,265",52,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=242,and so we can expect to see new results,pic_cs-410_6_3_240.jpg
cs-410_6_3_53,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:04:08,265","00:04:09,119",53,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=248,perhaps.,pic_cs-410_6_3_240.jpg
cs-410_6_3_54,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:04:12,753","00:04:17,686",54,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=252,Here are some additional readings,pic_cs-410_6_3_240.jpg
cs-410_6_3_55,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:04:17,686","00:04:22,544",55,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=257,about how learning to rank at works and,pic_cs-410_6_3_240.jpg
cs-410_6_3_56,cs-410,6,3, Learning to Rank - Part 3 (OPTIONAL),"00:04:25,281","00:04:35,281",56,https://www.coursera.org/learn/cs-410/lecture/h3Jru?t=265,[MUSIC],pic_cs-410_6_3_240.jpg
cs-410_6_4_1,cs-410,6,4, Future of Web Search,"00:00:00,012","00:00:04,047",1,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=0,[SOUND].,pic_cs-410_6_4_0.jpg
cs-410_6_4_2,cs-410,6,4, Future of Web Search,"00:00:07,043","00:00:10,080",2,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=7,This lecture is about,pic_cs-410_6_4_0.jpg
cs-410_6_4_3,cs-410,6,4, Future of Web Search,"00:00:12,660","00:00:17,560",3,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=12,"In this lecture, we're going to talk",pic_cs-410_6_4_0.jpg
cs-410_6_4_4,cs-410,6,4, Future of Web Search,"00:00:17,560","00:00:22,489",4,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=17,of web search and intelligent information,pic_cs-410_6_4_0.jpg
cs-410_6_4_5,cs-410,6,4, Future of Web Search,"00:00:24,370","00:00:28,561",5,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=24,In order to further improve,pic_cs-410_6_4_0.jpg
cs-410_6_4_6,cs-410,6,4, Future of Web Search,"00:00:28,561","00:00:33,752",6,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=28,it's important that to consider,pic_cs-410_6_4_0.jpg
cs-410_6_4_7,cs-410,6,4, Future of Web Search,"00:00:33,752","00:00:39,056",7,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=33,So one particular trend could be to,pic_cs-410_6_4_0.jpg
cs-410_6_4_8,cs-410,6,4, Future of Web Search,"00:00:39,056","00:00:44,630",8,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=39,"customized search engines, and they",pic_cs-410_6_4_0.jpg
cs-410_6_4_9,cs-410,6,4, Future of Web Search,"00:00:46,260","00:00:50,180",9,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=46,These vertical search engines can be,pic_cs-410_6_4_0.jpg
cs-410_6_4_10,cs-410,6,4, Future of Web Search,"00:00:50,180","00:00:55,940",10,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=50,the current general search engines,pic_cs-410_6_4_0.jpg
cs-410_6_4_11,cs-410,6,4, Future of Web Search,"00:00:55,940","00:01:02,070",11,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=55,users are a special group of users that,pic_cs-410_6_4_0.jpg
cs-410_6_4_12,cs-410,6,4, Future of Web Search,"00:01:02,070","00:01:06,420",12,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=62,and then the search engine can be,pic_cs-410_6_4_60.jpg
cs-410_6_4_13,cs-410,6,4, Future of Web Search,"00:01:07,970","00:01:12,150",13,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=67,"And because of the customization,",pic_cs-410_6_4_60.jpg
cs-410_6_4_14,cs-410,6,4, Future of Web Search,"00:01:12,150","00:01:14,360",14,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=72,"So the search can be personalized,",pic_cs-410_6_4_60.jpg
cs-410_6_4_15,cs-410,6,4, Future of Web Search,"00:01:15,430","00:01:18,190",15,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=75,because we have a better,pic_cs-410_6_4_60.jpg
cs-410_6_4_16,cs-410,6,4, Future of Web Search,"00:01:20,330","00:01:25,550",16,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=80,"Because of the restrictions with domain,",pic_cs-410_6_4_60.jpg
cs-410_6_4_17,cs-410,6,4, Future of Web Search,"00:01:25,550","00:01:29,590",17,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=85,"in handling the documents, because we can",pic_cs-410_6_4_60.jpg
cs-410_6_4_18,cs-410,6,4, Future of Web Search,"00:01:29,590","00:01:33,880",18,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=89,"For example, particular words may",pic_cs-410_6_4_60.jpg
cs-410_6_4_19,cs-410,6,4, Future of Web Search,"00:01:33,880","00:01:36,750",19,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=93,So we can bypass the problem of ambiguity.,pic_cs-410_6_4_60.jpg
cs-410_6_4_20,cs-410,6,4, Future of Web Search,"00:01:38,390","00:01:41,460",20,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=98,"Another trend we can expect to see,",pic_cs-410_6_4_60.jpg
cs-410_6_4_21,cs-410,6,4, Future of Web Search,"00:01:41,460","00:01:45,600",21,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=101,is the search engine will,pic_cs-410_6_4_60.jpg
cs-410_6_4_22,cs-410,6,4, Future of Web Search,"00:01:45,600","00:01:52,430",22,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=105,It's like a lifetime learning or,pic_cs-410_6_4_60.jpg
cs-410_6_4_23,cs-410,6,4, Future of Web Search,"00:01:52,430","00:01:57,800",23,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=112,very attractive because that means the,pic_cs-410_6_4_60.jpg
cs-410_6_4_24,cs-410,6,4, Future of Web Search,"00:01:57,800","00:02:01,780",24,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=117,"As more people are using it, the search",pic_cs-410_6_4_60.jpg
cs-410_6_4_25,cs-410,6,4, Future of Web Search,"00:02:01,780","00:02:03,260",25,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=121,"this is already happening,",pic_cs-410_6_4_120.jpg
cs-410_6_4_26,cs-410,6,4, Future of Web Search,"00:02:03,260","00:02:06,980",26,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=123,because the search engines can learn,pic_cs-410_6_4_120.jpg
cs-410_6_4_27,cs-410,6,4, Future of Web Search,"00:02:06,980","00:02:10,800",27,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=126,"More users use it, and the quality",pic_cs-410_6_4_120.jpg
cs-410_6_4_28,cs-410,6,4, Future of Web Search,"00:02:10,800","00:02:15,840",28,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=130,the popular queries that are typed in by,pic_cs-410_6_4_120.jpg
cs-410_6_4_29,cs-410,6,4, Future of Web Search,"00:02:15,840","00:02:19,110",29,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=135,so this is sort of another,pic_cs-410_6_4_120.jpg
cs-410_6_4_30,cs-410,6,4, Future of Web Search,"00:02:21,260","00:02:24,600",30,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=141,The third trend might be,pic_cs-410_6_4_120.jpg
cs-410_6_4_31,cs-410,6,4, Future of Web Search,"00:02:24,600","00:02:27,190",31,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=144,bottles of information access.,pic_cs-410_6_4_120.jpg
cs-410_6_4_32,cs-410,6,4, Future of Web Search,"00:02:27,190","00:02:32,050",32,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=147,"So search, navigation, and",pic_cs-410_6_4_120.jpg
cs-410_6_4_33,cs-410,6,4, Future of Web Search,"00:02:32,050","00:02:37,480",33,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=152,combined to form a full-fledged,pic_cs-410_6_4_120.jpg
cs-410_6_4_34,cs-410,6,4, Future of Web Search,"00:02:37,480","00:02:42,470",34,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=157,"And in the beginning of this course,",pic_cs-410_6_4_120.jpg
cs-410_6_4_35,cs-410,6,4, Future of Web Search,"00:02:42,470","00:02:47,100",35,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=162,These are different modes of information,pic_cs-410_6_4_120.jpg
cs-410_6_4_36,cs-410,6,4, Future of Web Search,"00:02:48,170","00:02:53,660",36,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=168,"And similarly, in the pull mode, querying",pic_cs-410_6_4_120.jpg
cs-410_6_4_37,cs-410,6,4, Future of Web Search,"00:02:53,660","00:02:58,390",37,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=173,"And in fact we're doing that basically,",pic_cs-410_6_4_120.jpg
cs-410_6_4_38,cs-410,6,4, Future of Web Search,"00:02:58,390","00:03:02,000",38,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=178,"We are querying, sometimes browsing,",pic_cs-410_6_4_120.jpg
cs-410_6_4_39,cs-410,6,4, Future of Web Search,"00:03:02,000","00:03:05,120",39,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=182,Sometimes we've got some,pic_cs-410_6_4_180.jpg
cs-410_6_4_40,cs-410,6,4, Future of Web Search,"00:03:05,120","00:03:11,466",40,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=185,Although most of the cases the information,pic_cs-410_6_4_180.jpg
cs-410_6_4_41,cs-410,6,4, Future of Web Search,"00:03:11,466","00:03:16,305",41,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=191,"But in the future, you can imagine",pic_cs-410_6_4_180.jpg
cs-410_6_4_42,cs-410,6,4, Future of Web Search,"00:03:16,305","00:03:21,380",42,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=196,"multi-mode for information access, and",pic_cs-410_6_4_180.jpg
cs-410_6_4_43,cs-410,6,4, Future of Web Search,"00:03:23,160","00:03:27,160",43,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=203,Another trend is that we might see systems,pic_cs-410_6_4_180.jpg
cs-410_6_4_44,cs-410,6,4, Future of Web Search,"00:03:27,160","00:03:30,970",44,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=207,that try to go beyond the searches,pic_cs-410_6_4_180.jpg
cs-410_6_4_45,cs-410,6,4, Future of Web Search,"00:03:30,970","00:03:36,730",45,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=210,"After all, the reason why people want",pic_cs-410_6_4_180.jpg
cs-410_6_4_46,cs-410,6,4, Future of Web Search,"00:03:36,730","00:03:39,690",46,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=216,to make a decision or perform a task.,pic_cs-410_6_4_180.jpg
cs-410_6_4_47,cs-410,6,4, Future of Web Search,"00:03:39,690","00:03:42,380",47,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=219,For example consumers might search for,pic_cs-410_6_4_180.jpg
cs-410_6_4_48,cs-410,6,4, Future of Web Search,"00:03:42,380","00:03:45,385",48,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=222,opinions about products in,pic_cs-410_6_4_180.jpg
cs-410_6_4_49,cs-410,6,4, Future of Web Search,"00:03:45,385","00:03:50,160",49,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=225,"choose a good product by, so",pic_cs-410_6_4_180.jpg
cs-410_6_4_50,cs-410,6,4, Future of Web Search,"00:03:50,160","00:03:55,330",50,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=230,support the whole workflow of purchasing,pic_cs-410_6_4_180.jpg
cs-410_6_4_51,cs-410,6,4, Future of Web Search,"00:03:56,732","00:04:00,300",51,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=236,"In this era, after the common search",pic_cs-410_6_4_180.jpg
cs-410_6_4_52,cs-410,6,4, Future of Web Search,"00:04:00,300","00:04:04,190",52,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=240,"For example, you can sometimes look at the",pic_cs-410_6_4_240.jpg
cs-410_6_4_53,cs-410,6,4, Future of Web Search,"00:04:04,190","00:04:09,040",53,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=244,you can just click on the button to go the,pic_cs-410_6_4_240.jpg
cs-410_6_4_54,cs-410,6,4, Future of Web Search,"00:04:09,040","00:04:12,840",54,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=249,"But it does not provide a,",pic_cs-410_6_4_240.jpg
cs-410_6_4_55,cs-410,6,4, Future of Web Search,"00:04:12,840","00:04:14,720",55,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=252,"For example, for researchers,",pic_cs-410_6_4_240.jpg
cs-410_6_4_56,cs-410,6,4, Future of Web Search,"00:04:14,720","00:04:18,800",56,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=254,you might want to find the realm in,pic_cs-410_6_4_240.jpg
cs-410_6_4_57,cs-410,6,4, Future of Web Search,"00:04:18,800","00:04:26,550",57,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=258,"And then, there's no, not much support for",pic_cs-410_6_4_240.jpg
cs-410_6_4_58,cs-410,6,4, Future of Web Search,"00:04:26,550","00:04:31,130",58,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=266,"So, in general, I think,",pic_cs-410_6_4_240.jpg
cs-410_6_4_59,cs-410,6,4, Future of Web Search,"00:04:31,130","00:04:34,980",59,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=271,"So in the following few slides, I'll",pic_cs-410_6_4_240.jpg
cs-410_6_4_60,cs-410,6,4, Future of Web Search,"00:04:34,980","00:04:39,900",60,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=274,"specific ideas or thoughts that hopefully,",pic_cs-410_6_4_240.jpg
cs-410_6_4_61,cs-410,6,4, Future of Web Search,"00:04:39,900","00:04:43,720",61,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=279,can help you in imagining new,pic_cs-410_6_4_240.jpg
cs-410_6_4_62,cs-410,6,4, Future of Web Search,"00:04:43,720","00:04:51,330",62,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=283,Some of them might be already relevant,pic_cs-410_6_4_240.jpg
cs-410_6_4_63,cs-410,6,4, Future of Web Search,"00:04:51,330","00:04:55,370",63,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=291,"In general, we can think about any",pic_cs-410_6_4_240.jpg
cs-410_6_4_64,cs-410,6,4, Future of Web Search,"00:04:55,370","00:05:02,390",64,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=295,"information system, as we specified",pic_cs-410_6_4_240.jpg
cs-410_6_4_65,cs-410,6,4, Future of Web Search,"00:05:02,390","00:05:05,680",65,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=302,And so,pic_cs-410_6_4_300.jpg
cs-410_6_4_66,cs-410,6,4, Future of Web Search,"00:05:05,680","00:05:09,250",66,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=305,then we'll able to specify,pic_cs-410_6_4_300.jpg
cs-410_6_4_67,cs-410,6,4, Future of Web Search,"00:05:09,250","00:05:12,480",67,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=309,And I call this,pic_cs-410_6_4_300.jpg
cs-410_6_4_68,cs-410,6,4, Future of Web Search,"00:05:12,480","00:05:18,110",68,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=312,So basically the three questions you,pic_cs-410_6_4_300.jpg
cs-410_6_4_69,cs-410,6,4, Future of Web Search,"00:05:18,110","00:05:23,470",69,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=318,what kind of data are you are managing and,pic_cs-410_6_4_300.jpg
cs-410_6_4_70,cs-410,6,4, Future of Web Search,"00:05:24,580","00:05:29,360",70,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=324,"Right there, this would help us",pic_cs-410_6_4_300.jpg
cs-410_6_4_71,cs-410,6,4, Future of Web Search,"00:05:30,650","00:05:33,400",71,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=330,And there are many different ways,pic_cs-410_6_4_300.jpg
cs-410_6_4_72,cs-410,6,4, Future of Web Search,"00:05:33,400","00:05:36,040",72,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=333,"how you connect them,",pic_cs-410_6_4_300.jpg
cs-410_6_4_73,cs-410,6,4, Future of Web Search,"00:05:36,040","00:05:37,650",73,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=336,So let me give you some examples.,pic_cs-410_6_4_300.jpg
cs-410_6_4_74,cs-410,6,4, Future of Web Search,"00:05:37,650","00:05:40,470",74,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=337,"On the top,",pic_cs-410_6_4_300.jpg
cs-410_6_4_75,cs-410,6,4, Future of Web Search,"00:05:40,470","00:05:45,250",75,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=340,"On the left side, you can see different",pic_cs-410_6_4_300.jpg
cs-410_6_4_76,cs-410,6,4, Future of Web Search,"00:05:45,250","00:05:48,740",76,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=345,"on the bottom,",pic_cs-410_6_4_300.jpg
cs-410_6_4_77,cs-410,6,4, Future of Web Search,"00:05:48,740","00:05:51,760",77,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=348,Now imagine you can connect,pic_cs-410_6_4_300.jpg
cs-410_6_4_78,cs-410,6,4, Future of Web Search,"00:05:51,760","00:05:55,990",78,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=351,"So, for example, you can connect",pic_cs-410_6_4_300.jpg
cs-410_6_4_79,cs-410,6,4, Future of Web Search,"00:05:55,990","00:05:59,140",79,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=355,the support search and,pic_cs-410_6_4_300.jpg
cs-410_6_4_80,cs-410,6,4, Future of Web Search,"00:05:59,140","00:06:01,050",80,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=359,"Well, that's web search, right?",pic_cs-410_6_4_300.jpg
cs-410_6_4_81,cs-410,6,4, Future of Web Search,"00:06:02,440","00:06:07,680",81,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=362,What if we connect UIUC employees with,pic_cs-410_6_4_360.jpg
cs-410_6_4_82,cs-410,6,4, Future of Web Search,"00:06:07,680","00:06:12,720",82,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=367,documents to support the search and,pic_cs-410_6_4_360.jpg
cs-410_6_4_83,cs-410,6,4, Future of Web Search,"00:06:12,720","00:06:17,110",83,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=372,If you connect the scientist,pic_cs-410_6_4_360.jpg
cs-410_6_4_84,cs-410,6,4, Future of Web Search,"00:06:17,110","00:06:22,050",84,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=377,"to provide all kinds of service,",pic_cs-410_6_4_360.jpg
cs-410_6_4_85,cs-410,6,4, Future of Web Search,"00:06:22,050","00:06:28,310",85,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=382,alert of new random documents or,pic_cs-410_6_4_360.jpg
cs-410_6_4_86,cs-410,6,4, Future of Web Search,"00:06:28,310","00:06:31,490",86,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=388,or provide the task with support or,pic_cs-410_6_4_360.jpg
cs-410_6_4_87,cs-410,6,4, Future of Web Search,"00:06:31,490","00:06:36,600",87,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=391,"For example, we might be,",pic_cs-410_6_4_360.jpg
cs-410_6_4_88,cs-410,6,4, Future of Web Search,"00:06:36,600","00:06:40,140",88,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=396,automatically generating,pic_cs-410_6_4_360.jpg
cs-410_6_4_89,cs-410,6,4, Future of Web Search,"00:06:40,140","00:06:44,440",89,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=400,"a research paper, and",pic_cs-410_6_4_360.jpg
cs-410_6_4_90,cs-410,6,4, Future of Web Search,"00:06:44,440","00:06:45,270",90,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=404,Right?,pic_cs-410_6_4_360.jpg
cs-410_6_4_91,cs-410,6,4, Future of Web Search,"00:06:45,270","00:06:48,010",91,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=405,we can imagine this would,pic_cs-410_6_4_360.jpg
cs-410_6_4_92,cs-410,6,4, Future of Web Search,"00:06:48,010","00:06:52,800",92,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=408,If we connect the online shoppers,pic_cs-410_6_4_360.jpg
cs-410_6_4_93,cs-410,6,4, Future of Web Search,"00:06:53,890","00:06:59,825",93,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=413,then we can help these people,pic_cs-410_6_4_360.jpg
cs-410_6_4_94,cs-410,6,4, Future of Web Search,"00:06:59,825","00:07:05,465",94,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=419,"So we can provide, for example data mining",pic_cs-410_6_4_360.jpg
cs-410_6_4_95,cs-410,6,4, Future of Web Search,"00:07:05,465","00:07:11,950",95,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=425,"to compare products, compare sentiment of",pic_cs-410_6_4_420.jpg
cs-410_6_4_96,cs-410,6,4, Future of Web Search,"00:07:11,950","00:07:15,950",96,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=431,decision support to have them,pic_cs-410_6_4_420.jpg
cs-410_6_4_97,cs-410,6,4, Future of Web Search,"00:07:15,950","00:07:21,510",97,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=435,Or we can connect customer service,pic_cs-410_6_4_420.jpg
cs-410_6_4_98,cs-410,6,4, Future of Web Search,"00:07:22,630","00:07:27,660",98,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=442,"and, and we can imagine a system",pic_cs-410_6_4_420.jpg
cs-410_6_4_99,cs-410,6,4, Future of Web Search,"00:07:27,660","00:07:31,460",99,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=447,of these emails to find that the major,pic_cs-410_6_4_420.jpg
cs-410_6_4_100,cs-410,6,4, Future of Web Search,"00:07:31,460","00:07:35,150",100,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=451,We can imagine a system we,pic_cs-410_6_4_420.jpg
cs-410_6_4_101,cs-410,6,4, Future of Web Search,"00:07:35,150","00:07:39,630",101,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=455,by automatically generating,pic_cs-410_6_4_420.jpg
cs-410_6_4_102,cs-410,6,4, Future of Web Search,"00:07:39,630","00:07:45,720",102,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=459,Maybe intelligently attach,pic_cs-410_6_4_420.jpg
cs-410_6_4_103,cs-410,6,4, Future of Web Search,"00:07:45,720","00:07:49,830",103,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=465,"if appropriate, if they detect that that's",pic_cs-410_6_4_420.jpg
cs-410_6_4_104,cs-410,6,4, Future of Web Search,"00:07:49,830","00:07:55,290",104,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=469,then you might take this opportunity,pic_cs-410_6_4_420.jpg
cs-410_6_4_105,cs-410,6,4, Future of Web Search,"00:07:55,290","00:07:57,770",105,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=475,"Whereas if it's a complaint,",pic_cs-410_6_4_420.jpg
cs-410_6_4_106,cs-410,6,4, Future of Web Search,"00:07:59,510","00:08:03,810",106,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=479,automatically generate some,pic_cs-410_6_4_420.jpg
cs-410_6_4_107,cs-410,6,4, Future of Web Search,"00:08:03,810","00:08:08,790",107,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=483,tell the customer that he or she can,pic_cs-410_6_4_480.jpg
cs-410_6_4_108,cs-410,6,4, Future of Web Search,"00:08:08,790","00:08:14,210",108,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=488,All of these are trying to help,pic_cs-410_6_4_480.jpg
cs-410_6_4_109,cs-410,6,4, Future of Web Search,"00:08:15,570","00:08:19,850",109,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=495,So this shows that,pic_cs-410_6_4_480.jpg
cs-410_6_4_110,cs-410,6,4, Future of Web Search,"00:08:19,850","00:08:22,090",110,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=499,It's just only restricted,pic_cs-410_6_4_480.jpg
cs-410_6_4_111,cs-410,6,4, Future of Web Search,"00:08:22,090","00:08:27,400",111,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=502,So this picture shows the trend,pic_cs-410_6_4_480.jpg
cs-410_6_4_112,cs-410,6,4, Future of Web Search,"00:08:27,400","00:08:33,770",112,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=507,"it characterizes the, intelligent",pic_cs-410_6_4_480.jpg
cs-410_6_4_113,cs-410,6,4, Future of Web Search,"00:08:33,770","00:08:39,065",113,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=513,"You can see in the center, there's",pic_cs-410_6_4_480.jpg
cs-410_6_4_114,cs-410,6,4, Future of Web Search,"00:08:39,065","00:08:41,225",114,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=519,to search a bag of words representation.,pic_cs-410_6_4_480.jpg
cs-410_6_4_115,cs-410,6,4, Future of Web Search,"00:08:41,225","00:08:46,721",115,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=521,That means the current search engines,pic_cs-410_6_4_480.jpg
cs-410_6_4_116,cs-410,6,4, Future of Web Search,"00:08:46,721","00:08:54,085",116,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=526,to users and mostly model,pic_cs-410_6_4_480.jpg
cs-410_6_4_117,cs-410,6,4, Future of Web Search,"00:08:54,085","00:08:59,105",117,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=534,and sees the data through,pic_cs-410_6_4_480.jpg
cs-410_6_4_118,cs-410,6,4, Future of Web Search,"00:08:59,105","00:09:06,190",118,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=539,So it's a very simple approximation of,pic_cs-410_6_4_480.jpg
cs-410_6_4_119,cs-410,6,4, Future of Web Search,"00:09:06,190","00:09:08,500",119,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=546,But that's what the current system does.,pic_cs-410_6_4_540.jpg
cs-410_6_4_120,cs-410,6,4, Future of Web Search,"00:09:08,500","00:09:12,150",120,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=548,It connects these three nodes,pic_cs-410_6_4_540.jpg
cs-410_6_4_121,cs-410,6,4, Future of Web Search,"00:09:12,150","00:09:17,655",121,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=552,it only provides a basic search function,pic_cs-410_6_4_540.jpg
cs-410_6_4_122,cs-410,6,4, Future of Web Search,"00:09:17,655","00:09:24,405",122,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=557,and it doesn't really understand that,pic_cs-410_6_4_540.jpg
cs-410_6_4_123,cs-410,6,4, Future of Web Search,"00:09:24,405","00:09:31,862",123,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=564,"Now, I showed some trends to push each",pic_cs-410_6_4_540.jpg
cs-410_6_4_124,cs-410,6,4, Future of Web Search,"00:09:31,862","00:09:35,332",124,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=571,"So think about the user node here, right?",pic_cs-410_6_4_540.jpg
cs-410_6_4_125,cs-410,6,4, Future of Web Search,"00:09:35,332","00:09:39,432",125,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=575,"So we can go beyond the keyword queries,",pic_cs-410_6_4_540.jpg
cs-410_6_4_126,cs-410,6,4, Future of Web Search,"00:09:39,432","00:09:43,882",126,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=579,and then further model the user,pic_cs-410_6_4_540.jpg
cs-410_6_4_127,cs-410,6,4, Future of Web Search,"00:09:43,882","00:09:49,622",127,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=583,"the user's task environment,",pic_cs-410_6_4_540.jpg
cs-410_6_4_128,cs-410,6,4, Future of Web Search,"00:09:49,622","00:09:55,120",128,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=589,"Okay, so this is pushing for",pic_cs-410_6_4_540.jpg
cs-410_6_4_129,cs-410,6,4, Future of Web Search,"00:09:55,120","00:09:58,630",129,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=595,And this is a major,pic_cs-410_6_4_540.jpg
cs-410_6_4_130,cs-410,6,4, Future of Web Search,"00:09:58,630","00:10:01,810",130,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=598,in order to build intelligent,pic_cs-410_6_4_540.jpg
cs-410_6_4_131,cs-410,6,4, Future of Web Search,"00:10:01,810","00:10:05,810",131,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=601,"On the document side,",pic_cs-410_6_4_600.jpg
cs-410_6_4_132,cs-410,6,4, Future of Web Search,"00:10:05,810","00:10:10,640",132,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=605,go beyond bag of words implementation,pic_cs-410_6_4_600.jpg
cs-410_6_4_133,cs-410,6,4, Future of Web Search,"00:10:10,640","00:10:16,040",133,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=610,"This means we'll recognize people's names,",pic_cs-410_6_4_600.jpg
cs-410_6_4_134,cs-410,6,4, Future of Web Search,"00:10:16,040","00:10:20,430",134,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=616,And this is already feasible with,pic_cs-410_6_4_600.jpg
cs-410_6_4_135,cs-410,6,4, Future of Web Search,"00:10:20,430","00:10:24,130",135,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=620,And Google is the reason,pic_cs-410_6_4_600.jpg
cs-410_6_4_136,cs-410,6,4, Future of Web Search,"00:10:24,130","00:10:28,310",136,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=624,"If you haven't heard of it,",pic_cs-410_6_4_600.jpg
cs-410_6_4_137,cs-410,6,4, Future of Web Search,"00:10:28,310","00:10:33,820",137,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=628,And once we can get to that level without,pic_cs-410_6_4_600.jpg
cs-410_6_4_138,cs-410,6,4, Future of Web Search,"00:10:33,820","00:10:38,170",138,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=633,it can enable the search engine,pic_cs-410_6_4_600.jpg
cs-410_6_4_139,cs-410,6,4, Future of Web Search,"00:10:38,170","00:10:41,470",139,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=638,In the future we would like to have,pic_cs-410_6_4_600.jpg
cs-410_6_4_140,cs-410,6,4, Future of Web Search,"00:10:41,470","00:10:45,450",140,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=641,knowledge representation where we,pic_cs-410_6_4_600.jpg
cs-410_6_4_141,cs-410,6,4, Future of Web Search,"00:10:45,450","00:10:47,750",141,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=645,then the search engine would,pic_cs-410_6_4_600.jpg
cs-410_6_4_142,cs-410,6,4, Future of Web Search,"00:10:49,390","00:10:53,490",142,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=649,So this calls for,pic_cs-410_6_4_600.jpg
cs-410_6_4_143,cs-410,6,4, Future of Web Search,"00:10:53,490","00:10:57,150",143,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=653,perhaps this is more feasible for,pic_cs-410_6_4_600.jpg
cs-410_6_4_144,cs-410,6,4, Future of Web Search,"00:10:57,150","00:10:59,800",144,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=657,It's easier to make progress,pic_cs-410_6_4_600.jpg
cs-410_6_4_145,cs-410,6,4, Future of Web Search,"00:10:59,800","00:11:01,240",145,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=659,"Now on the service side,",pic_cs-410_6_4_600.jpg
cs-410_6_4_146,cs-410,6,4, Future of Web Search,"00:11:01,240","00:11:05,920",146,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=661,we see we need to go beyond the search of,pic_cs-410_6_4_660.jpg
cs-410_6_4_147,cs-410,6,4, Future of Web Search,"00:11:07,510","00:11:13,702",147,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=667,So search is only one way to get access,pic_cs-410_6_4_660.jpg
cs-410_6_4_148,cs-410,6,4, Future of Web Search,"00:11:13,702","00:11:19,980",148,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=673,systems and push and pull so different,pic_cs-410_6_4_660.jpg
cs-410_6_4_149,cs-410,6,4, Future of Web Search,"00:11:19,980","00:11:21,630",149,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=679,"But going beyond access,",pic_cs-410_6_4_660.jpg
cs-410_6_4_150,cs-410,6,4, Future of Web Search,"00:11:21,630","00:11:25,820",150,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=681,we also need to help people digest the,pic_cs-410_6_4_660.jpg
cs-410_6_4_151,cs-410,6,4, Future of Web Search,"00:11:25,820","00:11:30,560",151,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=685,and this step has to do with analysis,pic_cs-410_6_4_660.jpg
cs-410_6_4_152,cs-410,6,4, Future of Web Search,"00:11:30,560","00:11:35,540",152,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=690,We have to find patterns or,pic_cs-410_6_4_660.jpg
cs-410_6_4_153,cs-410,6,4, Future of Web Search,"00:11:35,540","00:11:38,865",153,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=695,real knowledge that can,pic_cs-410_6_4_660.jpg
cs-410_6_4_154,cs-410,6,4, Future of Web Search,"00:11:38,865","00:11:43,055",154,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=698,actionable knowledge that can be used for,pic_cs-410_6_4_660.jpg
cs-410_6_4_155,cs-410,6,4, Future of Web Search,"00:11:43,055","00:11:47,165",155,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=703,And furthermore the knowledge,pic_cs-410_6_4_660.jpg
cs-410_6_4_156,cs-410,6,4, Future of Web Search,"00:11:47,165","00:11:52,580",156,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=707,"improve productivity in finishing a task,",pic_cs-410_6_4_660.jpg
cs-410_6_4_157,cs-410,6,4, Future of Web Search,"00:11:52,580","00:11:54,000",157,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=712,"Right, so this is a trend.",pic_cs-410_6_4_660.jpg
cs-410_6_4_158,cs-410,6,4, Future of Web Search,"00:11:54,000","00:11:59,370",158,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=714,"And, and, and so basically,",pic_cs-410_6_4_660.jpg
cs-410_6_4_159,cs-410,6,4, Future of Web Search,"00:11:59,370","00:12:04,210",159,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=719,in the future intelligent information,pic_cs-410_6_4_660.jpg
cs-410_6_4_160,cs-410,6,4, Future of Web Search,"00:12:04,210","00:12:06,940",160,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=724,interactive task support.,pic_cs-410_6_4_720.jpg
cs-410_6_4_161,cs-410,6,4, Future of Web Search,"00:12:06,940","00:12:11,200",161,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=726,Now I should also emphasize interactive,pic_cs-410_6_4_720.jpg
cs-410_6_4_162,cs-410,6,4, Future of Web Search,"00:12:11,200","00:12:16,790",162,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=731,the combined intelligence of the users and,pic_cs-410_6_4_720.jpg
cs-410_6_4_163,cs-410,6,4, Future of Web Search,"00:12:16,790","00:12:22,140",163,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=736,"So we, we can get some help",pic_cs-410_6_4_720.jpg
cs-410_6_4_164,cs-410,6,4, Future of Web Search,"00:12:22,140","00:12:26,970",164,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=742,And we don't have to assume the system,pic_cs-410_6_4_720.jpg
cs-410_6_4_165,cs-410,6,4, Future of Web Search,"00:12:26,970","00:12:32,290",165,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=746,"user, and the machine can collaborate in",pic_cs-410_6_4_720.jpg
cs-410_6_4_166,cs-410,6,4, Future of Web Search,"00:12:32,290","00:12:37,270",166,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=752,then the combined intelligence,pic_cs-410_6_4_720.jpg
cs-410_6_4_167,cs-410,6,4, Future of Web Search,"00:12:37,270","00:12:41,010",167,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=757,we can minimize the user's overall,pic_cs-410_6_4_720.jpg
cs-410_6_4_168,cs-410,6,4, Future of Web Search,"00:12:42,700","00:12:47,947",168,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=762,So this is the big picture of future,pic_cs-410_6_4_720.jpg
cs-410_6_4_169,cs-410,6,4, Future of Web Search,"00:12:47,947","00:12:52,582",169,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=767,and this hopefully can provide,pic_cs-410_6_4_720.jpg
cs-410_6_4_170,cs-410,6,4, Future of Web Search,"00:12:52,582","00:12:57,313",170,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=772,how to make further innovations,pic_cs-410_6_4_720.jpg
cs-410_6_4_171,cs-410,6,4, Future of Web Search,"00:12:57,313","00:13:07,313",171,https://www.coursera.org/learn/cs-410/lecture/kM78U?t=777,[MUSIC],pic_cs-410_6_4_720.jpg
cs-410_6_5_1,cs-410,6,5, Recommender Systems,"00:00:00,000","00:00:07,248",1,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=0,[MUSIC],pic_cs-410_6_5_0.jpg
cs-410_6_5_2,cs-410,6,5, Recommender Systems,"00:00:07,248","00:00:09,548",2,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=7,This lecture is about,pic_cs-410_6_5_0.jpg
cs-410_6_5_3,cs-410,6,5, Recommender Systems,"00:00:12,888","00:00:18,400",3,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=12,So far we have talked about a lot,pic_cs-410_6_5_0.jpg
cs-410_6_5_4,cs-410,6,5, Recommender Systems,"00:00:19,680","00:00:24,675",4,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=19,We have talked about the problem,pic_cs-410_6_5_0.jpg
cs-410_6_5_5,cs-410,6,5, Recommender Systems,"00:00:24,675","00:00:30,134",5,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=24,"different methods for ranking,",pic_cs-410_6_5_0.jpg
cs-410_6_5_6,cs-410,6,5, Recommender Systems,"00:00:30,134","00:00:33,198",6,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=30,"how to evaluate a search engine, etc.",pic_cs-410_6_5_0.jpg
cs-410_6_5_7,cs-410,6,5, Recommender Systems,"00:00:36,028","00:00:40,719",7,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=36,This is important because we know,pic_cs-410_6_5_0.jpg
cs-410_6_5_8,cs-410,6,5, Recommender Systems,"00:00:40,719","00:00:44,980",8,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=40,the most important applications,pic_cs-410_6_5_0.jpg
cs-410_6_5_9,cs-410,6,5, Recommender Systems,"00:00:44,980","00:00:49,820",9,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=44,And they are the most useful tools,pic_cs-410_6_5_0.jpg
cs-410_6_5_10,cs-410,6,5, Recommender Systems,"00:00:49,820","00:00:53,889",10,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=49,data into a small set,pic_cs-410_6_5_0.jpg
cs-410_6_5_11,cs-410,6,5, Recommender Systems,"00:00:56,330","00:01:00,959",11,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=56,Another reason why we spend so,pic_cs-410_6_5_0.jpg
cs-410_6_5_12,cs-410,6,5, Recommender Systems,"00:01:00,959","00:01:06,961",12,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=60,is because many techniques used in search,pic_cs-410_6_5_60.jpg
cs-410_6_5_13,cs-410,6,5, Recommender Systems,"00:01:06,961","00:01:11,266",13,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=66,"Recommender Systems,",pic_cs-410_6_5_60.jpg
cs-410_6_5_14,cs-410,6,5, Recommender Systems,"00:01:11,266","00:01:16,840",14,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=71,"And so, overall, the two systems",pic_cs-410_6_5_60.jpg
cs-410_6_5_15,cs-410,6,5, Recommender Systems,"00:01:16,840","00:01:19,110",15,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=76,And there are many techniques,pic_cs-410_6_5_60.jpg
cs-410_6_5_16,cs-410,6,5, Recommender Systems,"00:01:22,690","00:01:24,860",16,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=82,So this is a slide that,pic_cs-410_6_5_60.jpg
cs-410_6_5_17,cs-410,6,5, Recommender Systems,"00:01:24,860","00:01:29,020",17,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=84,when we talked about the two,pic_cs-410_6_5_60.jpg
cs-410_6_5_18,cs-410,6,5, Recommender Systems,"00:01:29,020","00:01:30,230",18,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=89,Pull and the Push.,pic_cs-410_6_5_60.jpg
cs-410_6_5_19,cs-410,6,5, Recommender Systems,"00:01:31,240","00:01:36,362",19,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=91,And we mentioned that recommender,pic_cs-410_6_5_60.jpg
cs-410_6_5_20,cs-410,6,5, Recommender Systems,"00:01:36,362","00:01:42,079",20,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=96,"users in the Push Mode, where the systems",pic_cs-410_6_5_60.jpg
cs-410_6_5_21,cs-410,6,5, Recommender Systems,"00:01:42,079","00:01:47,228",21,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=102,the information to the user or,pic_cs-410_6_5_60.jpg
cs-410_6_5_22,cs-410,6,5, Recommender Systems,"00:01:47,228","00:01:51,429",22,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=107,And this often works,pic_cs-410_6_5_60.jpg
cs-410_6_5_23,cs-410,6,5, Recommender Systems,"00:01:51,429","00:01:56,341",23,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=111,stable information need,pic_cs-410_6_5_60.jpg
cs-410_6_5_24,cs-410,6,5, Recommender Systems,"00:01:56,341","00:02:01,649",24,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=116,So a Recommender System is sometimes,pic_cs-410_6_5_60.jpg
cs-410_6_5_25,cs-410,6,5, Recommender Systems,"00:02:01,649","00:02:07,431",25,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=121,it's because recommending useful,pic_cs-410_6_5_120.jpg
cs-410_6_5_26,cs-410,6,5, Recommender Systems,"00:02:07,431","00:02:10,749",26,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=127,"filtering out the the useless articles,",pic_cs-410_6_5_120.jpg
cs-410_6_5_27,cs-410,6,5, Recommender Systems,"00:02:10,749","00:02:14,370",27,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=130,and so,pic_cs-410_6_5_120.jpg
cs-410_6_5_28,cs-410,6,5, Recommender Systems,"00:02:16,070","00:02:20,412",28,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=136,And in all the cases the system,pic_cs-410_6_5_120.jpg
cs-410_6_5_29,cs-410,6,5, Recommender Systems,"00:02:20,412","00:02:24,840",29,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=140,usually there's a dynamic source,pic_cs-410_6_5_120.jpg
cs-410_6_5_30,cs-410,6,5, Recommender Systems,"00:02:24,840","00:02:29,028",30,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=144,that you have some knowledge,pic_cs-410_6_5_120.jpg
cs-410_6_5_31,cs-410,6,5, Recommender Systems,"00:02:29,028","00:02:31,788",31,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=149,And then the system would make a decision,pic_cs-410_6_5_120.jpg
cs-410_6_5_32,cs-410,6,5, Recommender Systems,"00:02:31,788","00:02:34,950",32,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=151,about whether this item is,pic_cs-410_6_5_120.jpg
cs-410_6_5_33,cs-410,6,5, Recommender Systems,"00:02:34,950","00:02:39,678",33,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=154,then if it's interesting then the system,pic_cs-410_6_5_120.jpg
cs-410_6_5_34,cs-410,6,5, Recommender Systems,"00:02:43,008","00:02:49,520",34,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=163,So the basic filtering question here is,pic_cs-410_6_5_120.jpg
cs-410_6_5_35,cs-410,6,5, Recommender Systems,"00:02:49,520","00:02:52,426",35,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=169,Will U like item X?,pic_cs-410_6_5_120.jpg
cs-410_6_5_36,cs-410,6,5, Recommender Systems,"00:02:52,426","00:02:55,640",36,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=172,And there are two ways to answer this,pic_cs-410_6_5_120.jpg
cs-410_6_5_37,cs-410,6,5, Recommender Systems,"00:02:56,738","00:03:00,040",37,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=176,And one is look at what items U likes and,pic_cs-410_6_5_120.jpg
cs-410_6_5_38,cs-410,6,5, Recommender Systems,"00:03:00,040","00:03:03,655",38,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=180,then we can see if X is,pic_cs-410_6_5_180.jpg
cs-410_6_5_39,cs-410,6,5, Recommender Systems,"00:03:05,610","00:03:10,460",39,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=185,"The other is to look at who likes X,",pic_cs-410_6_5_180.jpg
cs-410_6_5_40,cs-410,6,5, Recommender Systems,"00:03:10,460","00:03:16,000",40,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=190,"user looks like a one of those users,",pic_cs-410_6_5_180.jpg
cs-410_6_5_41,cs-410,6,5, Recommender Systems,"00:03:16,000","00:03:18,640",41,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=196,And these strategies can be combined.,pic_cs-410_6_5_180.jpg
cs-410_6_5_42,cs-410,6,5, Recommender Systems,"00:03:18,640","00:03:20,800",42,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=198,If we follow the first strategy and,pic_cs-410_6_5_180.jpg
cs-410_6_5_43,cs-410,6,5, Recommender Systems,"00:03:20,800","00:03:26,170",43,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=200,look at item similarity in the case,pic_cs-410_6_5_180.jpg
cs-410_6_5_44,cs-410,6,5, Recommender Systems,"00:03:26,170","00:03:31,460",44,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=206,then we're talking about a content-based,pic_cs-410_6_5_180.jpg
cs-410_6_5_45,cs-410,6,5, Recommender Systems,"00:03:31,460","00:03:38,195",45,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=211,"If we look at the second strategy, then,",pic_cs-410_6_5_180.jpg
cs-410_6_5_46,cs-410,6,5, Recommender Systems,"00:03:38,195","00:03:43,110",46,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=218,we're user similarity and the technique,pic_cs-410_6_5_180.jpg
cs-410_6_5_47,cs-410,6,5, Recommender Systems,"00:03:46,010","00:03:49,190",47,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=226,"So, let's first look at",pic_cs-410_6_5_180.jpg
cs-410_6_5_48,cs-410,6,5, Recommender Systems,"00:03:49,190","00:03:51,530",48,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=229,This is what the system would look like.,pic_cs-410_6_5_180.jpg
cs-410_6_5_49,cs-410,6,5, Recommender Systems,"00:03:52,600","00:03:56,420",49,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=232,"Inside the system, there will be",pic_cs-410_6_5_180.jpg
cs-410_6_5_50,cs-410,6,5, Recommender Systems,"00:03:56,420","00:04:00,860",50,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=236,"knowledge about the user's interests, and",pic_cs-410_6_5_180.jpg
cs-410_6_5_51,cs-410,6,5, Recommender Systems,"00:04:02,210","00:04:06,815",51,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=242,It maintains this profile to keep,pic_cs-410_6_5_240.jpg
cs-410_6_5_52,cs-410,6,5, Recommender Systems,"00:04:06,815","00:04:10,865",52,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=246,then there is a utility function,pic_cs-410_6_5_240.jpg
cs-410_6_5_53,cs-410,6,5, Recommender Systems,"00:04:10,865","00:04:13,955",53,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=250,a nice plan utility,pic_cs-410_6_5_240.jpg
cs-410_6_5_54,cs-410,6,5, Recommender Systems,"00:04:13,955","00:04:17,977",54,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=253,It helps the system decide,pic_cs-410_6_5_240.jpg
cs-410_6_5_55,cs-410,6,5, Recommender Systems,"00:04:17,977","00:04:21,307",55,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=257,And then the accepted documents will,pic_cs-410_6_5_240.jpg
cs-410_6_5_56,cs-410,6,5, Recommender Systems,"00:04:21,307","00:04:23,457",56,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=261,according to the classified.,pic_cs-410_6_5_240.jpg
cs-410_6_5_57,cs-410,6,5, Recommender Systems,"00:04:23,457","00:04:28,327",57,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=263,There should be also an initialization,pic_cs-410_6_5_240.jpg
cs-410_6_5_58,cs-410,6,5, Recommender Systems,"00:04:28,327","00:04:34,167",58,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=268,maybe from a user's specified keywords or,pic_cs-410_6_5_240.jpg
cs-410_6_5_59,cs-410,6,5, Recommender Systems,"00:04:34,167","00:04:38,519",59,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=274,"etc., and this would be to feed into",pic_cs-410_6_5_240.jpg
cs-410_6_5_60,cs-410,6,5, Recommender Systems,"00:04:39,900","00:04:43,210",60,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=279,There is also typically a learning,pic_cs-410_6_5_240.jpg
cs-410_6_5_61,cs-410,6,5, Recommender Systems,"00:04:43,210","00:04:45,310",61,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=283,users' feedback over time.,pic_cs-410_6_5_240.jpg
cs-410_6_5_62,cs-410,6,5, Recommender Systems,"00:04:45,310","00:04:49,310",62,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=285,Now note that in this case typical,pic_cs-410_6_5_240.jpg
cs-410_6_5_63,cs-410,6,5, Recommender Systems,"00:04:49,310","00:04:53,420",63,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=289,the system would have a lot more,pic_cs-410_6_5_240.jpg
cs-410_6_5_64,cs-410,6,5, Recommender Systems,"00:04:53,420","00:04:58,590",64,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=293,"If the user has taken a recommended item,",pic_cs-410_6_5_240.jpg
cs-410_6_5_65,cs-410,6,5, Recommender Systems,"00:04:58,590","00:05:04,020",65,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=298,this a signal to indicate that,pic_cs-410_6_5_240.jpg
cs-410_6_5_66,cs-410,6,5, Recommender Systems,"00:05:04,020","00:05:07,010",66,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=304,"If the user discarded it,",pic_cs-410_6_5_300.jpg
cs-410_6_5_67,cs-410,6,5, Recommender Systems,"00:05:07,010","00:05:11,640",67,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=307,And so such feedback can be a long term,pic_cs-410_6_5_300.jpg
cs-410_6_5_68,cs-410,6,5, Recommender Systems,"00:05:11,640","00:05:16,660",68,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=311,And the system can collect a lot of,pic_cs-410_6_5_300.jpg
cs-410_6_5_69,cs-410,6,5, Recommender Systems,"00:05:16,660","00:05:19,500",69,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=316,this then can then be used,pic_cs-410_6_5_300.jpg
cs-410_6_5_70,cs-410,6,5, Recommender Systems,"00:05:19,500","00:05:23,780",70,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=319,Now what's the criteria for,pic_cs-410_6_5_300.jpg
cs-410_6_5_71,cs-410,6,5, Recommender Systems,"00:05:24,860","00:05:31,190",71,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=324,How do we know this filtering,pic_cs-410_6_5_300.jpg
cs-410_6_5_72,cs-410,6,5, Recommender Systems,"00:05:31,190","00:05:36,440",72,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=331,Now in this case we cannot use the ranking,pic_cs-410_6_5_300.jpg
cs-410_6_5_73,cs-410,6,5, Recommender Systems,"00:05:36,440","00:05:39,300",73,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=336,because we can't afford waiting for,pic_cs-410_6_5_300.jpg
cs-410_6_5_74,cs-410,6,5, Recommender Systems,"00:05:39,300","00:05:42,960",74,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=339,then rank the documents to,pic_cs-410_6_5_300.jpg
cs-410_6_5_75,cs-410,6,5, Recommender Systems,"00:05:42,960","00:05:47,930",75,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=342,And so the system must make,pic_cs-410_6_5_300.jpg
cs-410_6_5_76,cs-410,6,5, Recommender Systems,"00:05:47,930","00:05:51,830",76,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=347,to decide whether the item is,pic_cs-410_6_5_300.jpg
cs-410_6_5_77,cs-410,6,5, Recommender Systems,"00:05:51,830","00:05:55,520",77,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=351,"So in other words, we're trying",pic_cs-410_6_5_300.jpg
cs-410_6_5_78,cs-410,6,5, Recommender Systems,"00:05:56,800","00:05:57,899",78,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=356,"So in this case,",pic_cs-410_6_5_300.jpg
cs-410_6_5_79,cs-410,6,5, Recommender Systems,"00:05:57,899","00:06:03,600",79,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=357,one common user strategy is to use,pic_cs-410_6_5_300.jpg
cs-410_6_5_80,cs-410,6,5, Recommender Systems,"00:06:03,600","00:06:06,560",80,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=363,"So here, I show linear utility function.",pic_cs-410_6_5_360.jpg
cs-410_6_5_81,cs-410,6,5, Recommender Systems,"00:06:06,560","00:06:11,550",81,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=366,That's defined as for example three,pic_cs-410_6_5_360.jpg
cs-410_6_5_82,cs-410,6,5, Recommender Systems,"00:06:11,550","00:06:17,490",82,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=371,"you delivered, minus two multiplied by the",pic_cs-410_6_5_360.jpg
cs-410_6_5_83,cs-410,6,5, Recommender Systems,"00:06:17,490","00:06:20,775",83,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=377,"So in other words, we could kind of just",pic_cs-410_6_5_360.jpg
cs-410_6_5_84,cs-410,6,5, Recommender Systems,"00:06:22,245","00:06:26,215",84,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=382,treat this as almost in a gambling game.,pic_cs-410_6_5_360.jpg
cs-410_6_5_85,cs-410,6,5, Recommender Systems,"00:06:26,215","00:06:32,767",85,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=386,"If you delete one good item,",pic_cs-410_6_5_360.jpg
cs-410_6_5_86,cs-410,6,5, Recommender Systems,"00:06:32,767","00:06:37,660",86,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=392,you gain three dollars but if you deliver,pic_cs-410_6_5_360.jpg
cs-410_6_5_87,cs-410,6,5, Recommender Systems,"00:06:37,660","00:06:41,120",87,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=397,And this utility function,pic_cs-410_6_5_360.jpg
cs-410_6_5_88,cs-410,6,5, Recommender Systems,"00:06:41,120","00:06:45,375",88,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=401,how much money you are get by,pic_cs-410_6_5_360.jpg
cs-410_6_5_89,cs-410,6,5, Recommender Systems,"00:06:45,375","00:06:52,420",89,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=405,And so it's clear that if you want,pic_cs-410_6_5_360.jpg
cs-410_6_5_90,cs-410,6,5, Recommender Systems,"00:06:52,420","00:06:57,760",90,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=412,this strategy should be delivered,pic_cs-410_6_5_360.jpg
cs-410_6_5_91,cs-410,6,5, Recommender Systems,"00:06:57,760","00:07:01,160",91,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=417,and minimize the delivery of bad articles.,pic_cs-410_6_5_360.jpg
cs-410_6_5_92,cs-410,6,5, Recommender Systems,"00:07:01,160","00:07:02,370",92,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=421,"That's obvious, right?",pic_cs-410_6_5_420.jpg
cs-410_6_5_93,cs-410,6,5, Recommender Systems,"00:07:03,570","00:07:08,130",93,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=423,Now one interesting question here is,pic_cs-410_6_5_420.jpg
cs-410_6_5_94,cs-410,6,5, Recommender Systems,"00:07:08,130","00:07:14,140",94,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=428,I just showed a three and,pic_cs-410_6_5_420.jpg
cs-410_6_5_95,cs-410,6,5, Recommender Systems,"00:07:14,140","00:07:16,950",95,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=434,"But one can ask the question,",pic_cs-410_6_5_420.jpg
cs-410_6_5_96,cs-410,6,5, Recommender Systems,"00:07:17,990","00:07:19,030",96,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=437,So what do you think?,pic_cs-410_6_5_420.jpg
cs-410_6_5_97,cs-410,6,5, Recommender Systems,"00:07:21,080","00:07:23,450",97,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=441,Do you think that's a reasonable choice?,pic_cs-410_6_5_420.jpg
cs-410_6_5_98,cs-410,6,5, Recommender Systems,"00:07:23,450","00:07:24,510",98,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=443,What about the other choices?,pic_cs-410_6_5_420.jpg
cs-410_6_5_99,cs-410,6,5, Recommender Systems,"00:07:26,220","00:07:33,058",99,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=446,"So for example, we can have 10 and",pic_cs-410_6_5_420.jpg
cs-410_6_5_100,cs-410,6,5, Recommender Systems,"00:07:33,058","00:07:34,750",100,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=453,What's the difference?,pic_cs-410_6_5_420.jpg
cs-410_6_5_101,cs-410,6,5, Recommender Systems,"00:07:34,750","00:07:35,330",101,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=454,What do you think?,pic_cs-410_6_5_420.jpg
cs-410_6_5_102,cs-410,6,5, Recommender Systems,"00:07:36,920","00:07:41,820",102,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=456,How would this utility function affect,pic_cs-410_6_5_420.jpg
cs-410_6_5_103,cs-410,6,5, Recommender Systems,"00:07:43,600","00:07:45,589",103,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=463,"Right, you can think of",pic_cs-410_6_5_420.jpg
cs-410_6_5_104,cs-410,6,5, Recommender Systems,"00:07:45,589","00:07:51,284",104,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=465,"(10, -1) + (1, -10), which one do",pic_cs-410_6_5_420.jpg
cs-410_6_5_105,cs-410,6,5, Recommender Systems,"00:07:51,284","00:07:57,760",105,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=471,system to over do it and which one would,pic_cs-410_6_5_420.jpg
cs-410_6_5_106,cs-410,6,5, Recommender Systems,"00:07:57,760","00:08:03,380",106,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=477,If you think about it you will see that,pic_cs-410_6_5_420.jpg
cs-410_6_5_107,cs-410,6,5, Recommender Systems,"00:08:03,380","00:08:08,410",107,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=483,our good document you incur only a small,pic_cs-410_6_5_480.jpg
cs-410_6_5_108,cs-410,6,5, Recommender Systems,"00:08:08,410","00:08:11,740",108,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=488,"Intuitively, you would be",pic_cs-410_6_5_480.jpg
cs-410_6_5_109,cs-410,6,5, Recommender Systems,"00:08:11,740","00:08:16,370",109,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=491,And you can try to deliver more in,pic_cs-410_6_5_480.jpg
cs-410_6_5_110,cs-410,6,5, Recommender Systems,"00:08:16,370","00:08:17,870",110,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=496,And then we'll get a big reward.,pic_cs-410_6_5_480.jpg
cs-410_6_5_111,cs-410,6,5, Recommender Systems,"00:08:19,600","00:08:23,364",111,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=499,"So on the other hand,",pic_cs-410_6_5_480.jpg
cs-410_6_5_112,cs-410,6,5, Recommender Systems,"00:08:23,364","00:08:28,228",112,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=503,you really don't get such a big prize,pic_cs-410_6_5_480.jpg
cs-410_6_5_113,cs-410,6,5, Recommender Systems,"00:08:28,228","00:08:31,250",113,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=508,"On the other hand, you will have",pic_cs-410_6_5_480.jpg
cs-410_6_5_114,cs-410,6,5, Recommender Systems,"00:08:31,250","00:08:32,710",114,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=511,"You can imagine that,",pic_cs-410_6_5_480.jpg
cs-410_6_5_115,cs-410,6,5, Recommender Systems,"00:08:32,710","00:08:36,590",115,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=512,the system would be very reluctant,pic_cs-410_6_5_480.jpg
cs-410_6_5_116,cs-410,6,5, Recommender Systems,"00:08:36,590","00:08:41,198",116,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=516,It has to be absolutely,pic_cs-410_6_5_480.jpg
cs-410_6_5_117,cs-410,6,5, Recommender Systems,"00:08:41,198","00:08:45,990",117,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=521,So this utility function has to be,pic_cs-410_6_5_480.jpg
cs-410_6_5_118,cs-410,6,5, Recommender Systems,"00:08:45,990","00:08:49,660",118,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=525,The three basic problems in content-based,pic_cs-410_6_5_480.jpg
cs-410_6_5_119,cs-410,6,5, Recommender Systems,"00:08:49,660","00:08:53,620",119,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=529,"first, it has to make",pic_cs-410_6_5_480.jpg
cs-410_6_5_120,cs-410,6,5, Recommender Systems,"00:08:53,620","00:08:58,200",120,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=533,"So it has to be a binary decision maker,",pic_cs-410_6_5_480.jpg
cs-410_6_5_121,cs-410,6,5, Recommender Systems,"00:08:58,200","00:09:03,620",121,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=538,Given a text document and,pic_cs-410_6_5_480.jpg
cs-410_6_5_122,cs-410,6,5, Recommender Systems,"00:09:03,620","00:09:07,040",122,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=543,"it has to say yes or no, whether this",pic_cs-410_6_5_540.jpg
cs-410_6_5_123,cs-410,6,5, Recommender Systems,"00:09:08,050","00:09:12,375",123,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=548,"So that's a decision module, and",pic_cs-410_6_5_540.jpg
cs-410_6_5_124,cs-410,6,5, Recommender Systems,"00:09:12,375","00:09:17,220",124,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=552,module as you have seen earlier and,pic_cs-410_6_5_540.jpg
cs-410_6_5_125,cs-410,6,5, Recommender Systems,"00:09:17,220","00:09:22,050",125,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=557,And we have to initialize the system,pic_cs-410_6_5_540.jpg
cs-410_6_5_126,cs-410,6,5, Recommender Systems,"00:09:22,050","00:09:25,250",126,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=562,text exclusion or,pic_cs-410_6_5_540.jpg
cs-410_6_5_127,cs-410,6,5, Recommender Systems,"00:09:26,710","00:09:30,375",127,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=566,And the third model is,pic_cs-410_6_5_540.jpg
cs-410_6_5_128,cs-410,6,5, Recommender Systems,"00:09:30,375","00:09:35,445",128,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=570,has to be able to learn from limited,pic_cs-410_6_5_540.jpg
cs-410_6_5_129,cs-410,6,5, Recommender Systems,"00:09:35,445","00:09:41,100",129,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=575,counted them from the user about their,pic_cs-410_6_5_540.jpg
cs-410_6_5_130,cs-410,6,5, Recommender Systems,"00:09:41,100","00:09:45,702",130,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=581,If we don't deliver document,pic_cs-410_6_5_540.jpg
cs-410_6_5_131,cs-410,6,5, Recommender Systems,"00:09:45,702","00:09:48,900",131,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=585,be able to know whether,pic_cs-410_6_5_540.jpg
cs-410_6_5_132,cs-410,6,5, Recommender Systems,"00:09:50,460","00:09:55,130",132,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=590,And we had accumulate a lot of documents,pic_cs-410_6_5_540.jpg
cs-410_6_5_133,cs-410,6,5, Recommender Systems,"00:09:56,220","00:10:01,470",133,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=596,All these modules will have to be,pic_cs-410_6_5_540.jpg
cs-410_6_5_134,cs-410,6,5, Recommender Systems,"00:10:01,470","00:10:03,050",134,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=601,So how can we deal with such a system?,pic_cs-410_6_5_600.jpg
cs-410_6_5_135,cs-410,6,5, Recommender Systems,"00:10:03,050","00:10:05,260",135,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=603,And there are many different approaches.,pic_cs-410_6_5_600.jpg
cs-410_6_5_136,cs-410,6,5, Recommender Systems,"00:10:05,260","00:10:09,600",136,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=605,Here we're going to talk about,pic_cs-410_6_5_600.jpg
cs-410_6_5_137,cs-410,6,5, Recommender Systems,"00:10:09,600","00:10:12,120",137,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=609,a search engine for information filtering.,pic_cs-410_6_5_600.jpg
cs-410_6_5_138,cs-410,6,5, Recommender Systems,"00:10:12,120","00:10:15,880",138,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=612,"Again, here's why we've spent a lot of",pic_cs-410_6_5_600.jpg
cs-410_6_5_139,cs-410,6,5, Recommender Systems,"00:10:15,880","00:10:20,830",139,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=615,Because it's actually not very hard,pic_cs-410_6_5_600.jpg
cs-410_6_5_140,cs-410,6,5, Recommender Systems,"00:10:20,830","00:10:22,320",140,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=620,information filtering.,pic_cs-410_6_5_600.jpg
cs-410_6_5_141,cs-410,6,5, Recommender Systems,"00:10:22,320","00:10:26,410",141,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=622,So here's the basic idea for,pic_cs-410_6_5_600.jpg
cs-410_6_5_142,cs-410,6,5, Recommender Systems,"00:10:26,410","00:10:27,960",142,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=626,information filtering.,pic_cs-410_6_5_600.jpg
cs-410_6_5_143,cs-410,6,5, Recommender Systems,"00:10:27,960","00:10:31,180",143,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=627,"First, we can reuse a lot of",pic_cs-410_6_5_600.jpg
cs-410_6_5_144,cs-410,6,5, Recommender Systems,"00:10:31,180","00:10:34,950",144,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=631,"Right, so we know how to score",pic_cs-410_6_5_600.jpg
cs-410_6_5_145,cs-410,6,5, Recommender Systems,"00:10:34,950","00:10:39,620",145,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=634,We're going to match the similarity,pic_cs-410_6_5_600.jpg
cs-410_6_5_146,cs-410,6,5, Recommender Systems,"00:10:39,620","00:10:40,930",146,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=639,a document.,pic_cs-410_6_5_600.jpg
cs-410_6_5_147,cs-410,6,5, Recommender Systems,"00:10:40,930","00:10:44,320",147,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=640,And then we can use a score threshold for,pic_cs-410_6_5_600.jpg
cs-410_6_5_148,cs-410,6,5, Recommender Systems,"00:10:44,320","00:10:49,290",148,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=644,We do retrieval and then we kind of find,pic_cs-410_6_5_600.jpg
cs-410_6_5_149,cs-410,6,5, Recommender Systems,"00:10:49,290","00:10:56,890",149,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=649,apply a threshold to see whether the,pic_cs-410_6_5_600.jpg
cs-410_6_5_150,cs-410,6,5, Recommender Systems,"00:10:56,890","00:10:58,230",150,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=656,"And if it's passing the threshold,",pic_cs-410_6_5_600.jpg
cs-410_6_5_151,cs-410,6,5, Recommender Systems,"00:10:58,230","00:11:02,900",151,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=658,we're going to say it's relevant and,pic_cs-410_6_5_600.jpg
cs-410_6_5_152,cs-410,6,5, Recommender Systems,"00:11:02,900","00:11:08,310",152,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=662,"Another component that we have to add is,",pic_cs-410_6_5_660.jpg
cs-410_6_5_153,cs-410,6,5, Recommender Systems,"00:11:08,310","00:11:13,080",153,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=668,we had used is the traditional feedback,pic_cs-410_6_5_660.jpg
cs-410_6_5_154,cs-410,6,5, Recommender Systems,"00:11:13,080","00:11:18,632",154,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=673,And we know rock hill can be using for,pic_cs-410_6_5_660.jpg
cs-410_6_5_155,cs-410,6,5, Recommender Systems,"00:11:18,632","00:11:25,008",155,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=678,"And, but we have to develop a new",pic_cs-410_6_5_660.jpg
cs-410_6_5_156,cs-410,6,5, Recommender Systems,"00:11:25,008","00:11:27,279",156,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=685,And we need to set it initially and,pic_cs-410_6_5_660.jpg
cs-410_6_5_157,cs-410,6,5, Recommender Systems,"00:11:27,279","00:11:32,170",157,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=687,then we have to learn how to,pic_cs-410_6_5_660.jpg
cs-410_6_5_158,cs-410,6,5, Recommender Systems,"00:11:32,170","00:11:37,276",158,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=692,So here's what the system,pic_cs-410_6_5_660.jpg
cs-410_6_5_159,cs-410,6,5, Recommender Systems,"00:11:37,276","00:11:45,040",159,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=697,generalize the vector-space model for,pic_cs-410_6_5_660.jpg
cs-410_6_5_160,cs-410,6,5, Recommender Systems,"00:11:45,040","00:11:49,348",160,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=705,So you can see the document vector could,pic_cs-410_6_5_660.jpg
cs-410_6_5_161,cs-410,6,5, Recommender Systems,"00:11:49,348","00:11:53,820",161,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=709,already exists in a search engine,pic_cs-410_6_5_660.jpg
cs-410_6_5_162,cs-410,6,5, Recommender Systems,"00:11:53,820","00:11:58,630",162,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=713,And the profile will be treated,pic_cs-410_6_5_660.jpg
cs-410_6_5_163,cs-410,6,5, Recommender Systems,"00:11:58,630","00:12:02,100",163,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=718,the profile vector can be matched with,pic_cs-410_6_5_660.jpg
cs-410_6_5_164,cs-410,6,5, Recommender Systems,"00:12:03,130","00:12:06,960",164,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=723,And then this score would be fed into a,pic_cs-410_6_5_720.jpg
cs-410_6_5_165,cs-410,6,5, Recommender Systems,"00:12:06,960","00:12:13,690",165,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=726,"no, and then the evaluation would be based",pic_cs-410_6_5_720.jpg
cs-410_6_5_166,cs-410,6,5, Recommender Systems,"00:12:13,690","00:12:16,870",166,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=733,If it says yes and then the document,pic_cs-410_6_5_720.jpg
cs-410_6_5_167,cs-410,6,5, Recommender Systems,"00:12:16,870","00:12:19,660",167,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=736,And then user could give some feedback.,pic_cs-410_6_5_720.jpg
cs-410_6_5_168,cs-410,6,5, Recommender Systems,"00:12:19,660","00:12:25,530",168,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=739,The feedback information would be,pic_cs-410_6_5_720.jpg
cs-410_6_5_169,cs-410,6,5, Recommender Systems,"00:12:25,530","00:12:28,500",169,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=745,to adjust the vector representation.,pic_cs-410_6_5_720.jpg
cs-410_6_5_170,cs-410,6,5, Recommender Systems,"00:12:28,500","00:12:33,150",170,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=748,So the vector learning is essentially,pic_cs-410_6_5_720.jpg
cs-410_6_5_171,cs-410,6,5, Recommender Systems,"00:12:33,150","00:12:36,140",171,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=753,feedback in the case of search.,pic_cs-410_6_5_720.jpg
cs-410_6_5_172,cs-410,6,5, Recommender Systems,"00:12:36,140","00:12:39,480",172,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=756,The threshold of learning,pic_cs-410_6_5_720.jpg
cs-410_6_5_173,cs-410,6,5, Recommender Systems,"00:12:39,480","00:12:42,580",173,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=759,that we need to talk,pic_cs-410_6_5_720.jpg
cs-410_6_5_174,cs-410,6,5, Recommender Systems,"00:12:42,580","00:12:52,580",174,https://www.coursera.org/learn/cs-410/lecture/QORNe?t=762,[MUSIC],pic_cs-410_6_5_720.jpg
cs-410_6_6_1,cs-410,6,6,  Recommender Systems,"00:00:00,012","00:00:03,574",1,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=0,[SOUND],pic_cs-410_6_6_0.jpg
cs-410_6_6_2,cs-410,6,6,  Recommender Systems,"00:00:09,704","00:00:11,535",2,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=9,There are some interesting challenges,pic_cs-410_6_6_0.jpg
cs-410_6_6_3,cs-410,6,6,  Recommender Systems,"00:00:11,535","00:00:15,015",3,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=11,in threshold for,pic_cs-410_6_6_0.jpg
cs-410_6_6_4,cs-410,6,6,  Recommender Systems,"00:00:15,015","00:00:19,965",4,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=15,So here I show the historical data that,pic_cs-410_6_6_0.jpg
cs-410_6_6_5,cs-410,6,6,  Recommender Systems,"00:00:19,965","00:00:25,195",5,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=19,so you can see the scores and,pic_cs-410_6_6_0.jpg
cs-410_6_6_6,cs-410,6,6,  Recommender Systems,"00:00:25,195","00:00:30,682",6,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=25,So the first one has a score of 36.5 and,pic_cs-410_6_6_0.jpg
cs-410_6_6_7,cs-410,6,6,  Recommender Systems,"00:00:30,682","00:00:34,852",7,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=30,The second one is not relevant and,pic_cs-410_6_6_0.jpg
cs-410_6_6_8,cs-410,6,6,  Recommender Systems,"00:00:34,852","00:00:37,902",8,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=34,"Of course, we have a lot of documents for",pic_cs-410_6_6_0.jpg
cs-410_6_6_9,cs-410,6,6,  Recommender Systems,"00:00:37,902","00:00:40,910",9,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=37,because we have never,pic_cs-410_6_6_0.jpg
cs-410_6_6_10,cs-410,6,6,  Recommender Systems,"00:00:40,910","00:00:42,060",10,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=40,"So as you can see here,",pic_cs-410_6_6_0.jpg
cs-410_6_6_11,cs-410,6,6,  Recommender Systems,"00:00:42,060","00:00:47,380",11,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=42,we only see the judgements of,pic_cs-410_6_6_0.jpg
cs-410_6_6_12,cs-410,6,6,  Recommender Systems,"00:00:47,380","00:00:52,100",12,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=47,"So this is not a random sample,",pic_cs-410_6_6_0.jpg
cs-410_6_6_13,cs-410,6,6,  Recommender Systems,"00:00:52,100","00:00:56,930",13,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=52,"It's kind of biased, so that creates",pic_cs-410_6_6_0.jpg
cs-410_6_6_14,cs-410,6,6,  Recommender Systems,"00:00:58,366","00:01:04,230",14,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=58,"Secondly, there are in general very little",pic_cs-410_6_6_0.jpg
cs-410_6_6_15,cs-410,6,6,  Recommender Systems,"00:01:04,230","00:01:07,920",15,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=64,so it's also challenging for,pic_cs-410_6_6_60.jpg
cs-410_6_6_16,cs-410,6,6,  Recommender Systems,"00:01:07,920","00:01:12,550",16,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=67,typically they require more training data.,pic_cs-410_6_6_60.jpg
cs-410_6_6_17,cs-410,6,6,  Recommender Systems,"00:01:13,830","00:01:17,560",17,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=73,And in the extreme case at,pic_cs-410_6_6_60.jpg
cs-410_6_6_18,cs-410,6,6,  Recommender Systems,"00:01:17,560","00:01:18,550",18,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=77,labeled data as well.,pic_cs-410_6_6_60.jpg
cs-410_6_6_19,cs-410,6,6,  Recommender Systems,"00:01:18,550","00:01:20,940",19,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=78,"The system there has to make a decision,",pic_cs-410_6_6_60.jpg
cs-410_6_6_20,cs-410,6,6,  Recommender Systems,"00:01:20,940","00:01:24,440",20,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=80,so that's a very difficult,pic_cs-410_6_6_60.jpg
cs-410_6_6_21,cs-410,6,6,  Recommender Systems,"00:01:24,440","00:01:29,348",21,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=84,"Finally, there is also this issue of",pic_cs-410_6_6_60.jpg
cs-410_6_6_22,cs-410,6,6,  Recommender Systems,"00:01:29,348","00:01:34,987",22,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=89,"Now, this means we also want",pic_cs-410_6_6_60.jpg
cs-410_6_6_23,cs-410,6,6,  Recommender Systems,"00:01:34,987","00:01:39,983",23,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=94,space a little bit and,pic_cs-410_6_6_60.jpg
cs-410_6_6_24,cs-410,6,6,  Recommender Systems,"00:01:39,983","00:01:45,899",24,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=99,interested in documents that,pic_cs-410_6_6_60.jpg
cs-410_6_6_25,cs-410,6,6,  Recommender Systems,"00:01:45,899","00:01:51,330",25,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=105,"So in other words, we're going to",pic_cs-410_6_6_60.jpg
cs-410_6_6_26,cs-410,6,6,  Recommender Systems,"00:01:51,330","00:01:54,890",26,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=111,by testing whether the user might be,pic_cs-410_6_6_60.jpg
cs-410_6_6_27,cs-410,6,6,  Recommender Systems,"00:01:56,550","00:02:00,530",27,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=116,currently are not matching,pic_cs-410_6_6_60.jpg
cs-410_6_6_28,cs-410,6,6,  Recommender Systems,"00:02:01,660","00:02:02,650",28,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=121,So how do we do that?,pic_cs-410_6_6_120.jpg
cs-410_6_6_29,cs-410,6,6,  Recommender Systems,"00:02:02,650","00:02:06,580",29,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=122,"Well, we could lower the threshold",pic_cs-410_6_6_120.jpg
cs-410_6_6_30,cs-410,6,6,  Recommender Systems,"00:02:06,580","00:02:11,260",30,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=126,deliver some near misses to the user,pic_cs-410_6_6_120.jpg
cs-410_6_6_31,cs-410,6,6,  Recommender Systems,"00:02:13,160","00:02:18,870",31,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=133,to see how the user would,pic_cs-410_6_6_120.jpg
cs-410_6_6_32,cs-410,6,6,  Recommender Systems,"00:02:20,540","00:02:24,920",32,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=140,"And this is a tradeoff, because on",pic_cs-410_6_6_120.jpg
cs-410_6_6_33,cs-410,6,6,  Recommender Systems,"00:02:24,920","00:02:28,130",33,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=144,"on the other hand,",pic_cs-410_6_6_120.jpg
cs-410_6_6_34,cs-410,6,6,  Recommender Systems,"00:02:28,130","00:02:31,960",34,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=148,because then you will over,pic_cs-410_6_6_120.jpg
cs-410_6_6_35,cs-410,6,6,  Recommender Systems,"00:02:31,960","00:02:36,310",35,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=151,So exploitation means you would,pic_cs-410_6_6_120.jpg
cs-410_6_6_36,cs-410,6,6,  Recommender Systems,"00:02:36,310","00:02:39,790",36,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=156,Let's say you know the user is,pic_cs-410_6_6_120.jpg
cs-410_6_6_37,cs-410,6,6,  Recommender Systems,"00:02:39,790","00:02:42,950",37,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=159,"you don't want to deviate that much, but",pic_cs-410_6_6_120.jpg
cs-410_6_6_38,cs-410,6,6,  Recommender Systems,"00:02:42,950","00:02:47,220",38,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=162,if you don't deviate at all then you don't,pic_cs-410_6_6_120.jpg
cs-410_6_6_39,cs-410,6,6,  Recommender Systems,"00:02:47,220","00:02:50,710",39,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=167,You might miss opportunity to learn,pic_cs-410_6_6_120.jpg
cs-410_6_6_40,cs-410,6,6,  Recommender Systems,"00:02:51,930","00:02:53,700",40,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=171,So this is a dilemma.,pic_cs-410_6_6_120.jpg
cs-410_6_6_41,cs-410,6,6,  Recommender Systems,"00:02:54,790","00:02:57,710",41,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=174,And that's also a difficulty,pic_cs-410_6_6_120.jpg
cs-410_6_6_42,cs-410,6,6,  Recommender Systems,"00:02:58,890","00:03:00,320",42,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=178,"Now, how do we solve these problems?",pic_cs-410_6_6_120.jpg
cs-410_6_6_43,cs-410,6,6,  Recommender Systems,"00:03:00,320","00:03:04,499",43,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=180,"In general, I think one can use the",pic_cs-410_6_6_180.jpg
cs-410_6_6_44,cs-410,6,6,  Recommender Systems,"00:03:04,499","00:03:09,611",44,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=184,And this strategy is basically to optimize,pic_cs-410_6_6_180.jpg
cs-410_6_6_45,cs-410,6,6,  Recommender Systems,"00:03:09,611","00:03:12,480",45,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=189,just as you have seen,pic_cs-410_6_6_180.jpg
cs-410_6_6_46,cs-410,6,6,  Recommender Systems,"00:03:12,480","00:03:16,610",46,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=192,"Right, so you can just compute",pic_cs-410_6_6_180.jpg
cs-410_6_6_47,cs-410,6,6,  Recommender Systems,"00:03:16,610","00:03:18,950",47,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=196,each candidate score threshold.,pic_cs-410_6_6_180.jpg
cs-410_6_6_48,cs-410,6,6,  Recommender Systems,"00:03:18,950","00:03:21,830",48,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=198,"Pretend that, what if I cut at this point.",pic_cs-410_6_6_180.jpg
cs-410_6_6_49,cs-410,6,6,  Recommender Systems,"00:03:21,830","00:03:27,090",49,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=201,What if I cut at the different scoring,pic_cs-410_6_6_180.jpg
cs-410_6_6_50,cs-410,6,6,  Recommender Systems,"00:03:27,090","00:03:28,900",50,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=207,What's utility?,pic_cs-410_6_6_180.jpg
cs-410_6_6_51,cs-410,6,6,  Recommender Systems,"00:03:28,900","00:03:34,030",51,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=208,"Since these are training data,",pic_cs-410_6_6_180.jpg
cs-410_6_6_52,cs-410,6,6,  Recommender Systems,"00:03:34,030","00:03:38,440",52,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=214,"and we know that relevant status,",pic_cs-410_6_6_180.jpg
cs-410_6_6_53,cs-410,6,6,  Recommender Systems,"00:03:38,440","00:03:43,220",53,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=218,relevant status based on,pic_cs-410_6_6_180.jpg
cs-410_6_6_54,cs-410,6,6,  Recommender Systems,"00:03:43,220","00:03:47,190",54,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=223,So then we can just choose the threshold,pic_cs-410_6_6_180.jpg
cs-410_6_6_55,cs-410,6,6,  Recommender Systems,"00:03:47,190","00:03:49,810",55,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=227,on the training data.,pic_cs-410_6_6_180.jpg
cs-410_6_6_56,cs-410,6,6,  Recommender Systems,"00:03:49,810","00:03:55,160",56,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=229,"But this of course, doesn't account for",pic_cs-410_6_6_180.jpg
cs-410_6_6_57,cs-410,6,6,  Recommender Systems,"00:03:56,870","00:04:00,400",57,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=236,And there is also the difficulty of,pic_cs-410_6_6_180.jpg
cs-410_6_6_58,cs-410,6,6,  Recommender Systems,"00:04:01,530","00:04:07,300",58,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=241,"So, in general, we can only get the upper",pic_cs-410_6_6_240.jpg
cs-410_6_6_59,cs-410,6,6,  Recommender Systems,"00:04:07,300","00:04:13,190",59,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=247,because the threshold might,pic_cs-410_6_6_240.jpg
cs-410_6_6_60,cs-410,6,6,  Recommender Systems,"00:04:13,190","00:04:17,115",60,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=253,"So, it's possible that this could",pic_cs-410_6_6_240.jpg
cs-410_6_6_61,cs-410,6,6,  Recommender Systems,"00:04:17,115","00:04:18,610",61,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=257,interesting to the user.,pic_cs-410_6_6_240.jpg
cs-410_6_6_62,cs-410,6,6,  Recommender Systems,"00:04:19,790","00:04:21,400",62,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=259,So how do we solve this problem?,pic_cs-410_6_6_240.jpg
cs-410_6_6_63,cs-410,6,6,  Recommender Systems,"00:04:21,400","00:04:22,896",63,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=261,"Well, we generally, and",pic_cs-410_6_6_240.jpg
cs-410_6_6_64,cs-410,6,6,  Recommender Systems,"00:04:22,896","00:04:27,190",64,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=262,as I said we can low with this,pic_cs-410_6_6_240.jpg
cs-410_6_6_65,cs-410,6,6,  Recommender Systems,"00:04:27,190","00:04:30,760",65,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=267,So here's on particular approach,pic_cs-410_6_6_240.jpg
cs-410_6_6_66,cs-410,6,6,  Recommender Systems,"00:04:30,760","00:04:32,680",66,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=270,So the idea is falling.,pic_cs-410_6_6_240.jpg
cs-410_6_6_67,cs-410,6,6,  Recommender Systems,"00:04:32,680","00:04:37,400",67,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=272,So here I show a ranked list of all the,pic_cs-410_6_6_240.jpg
cs-410_6_6_68,cs-410,6,6,  Recommender Systems,"00:04:37,400","00:04:40,610",68,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=277,"far, and",pic_cs-410_6_6_240.jpg
cs-410_6_6_69,cs-410,6,6,  Recommender Systems,"00:04:40,610","00:04:44,680",69,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=280,"And on the y axis we show the utility,",pic_cs-410_6_6_240.jpg
cs-410_6_6_70,cs-410,6,6,  Recommender Systems,"00:04:44,680","00:04:48,670",70,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=284,how you specify the coefficients,pic_cs-410_6_6_240.jpg
cs-410_6_6_71,cs-410,6,6,  Recommender Systems,"00:04:48,670","00:04:53,160",71,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=288,"we can then imagine, that depending on the",pic_cs-410_6_6_240.jpg
cs-410_6_6_72,cs-410,6,6,  Recommender Systems,"00:04:54,930","00:04:59,828",72,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=294,Suppose I cut at this position and,pic_cs-410_6_6_240.jpg
cs-410_6_6_73,cs-410,6,6,  Recommender Systems,"00:04:59,828","00:05:06,690",73,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=299,"For example,",pic_cs-410_6_6_240.jpg
cs-410_6_6_74,cs-410,6,6,  Recommender Systems,"00:05:06,690","00:05:11,640",74,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=306,"The optimal point,",pic_cs-410_6_6_300.jpg
cs-410_6_6_75,cs-410,6,6,  Recommender Systems,"00:05:11,640","00:05:16,355",75,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=311,when it will achieve the maximum utility,pic_cs-410_6_6_300.jpg
cs-410_6_6_76,cs-410,6,6,  Recommender Systems,"00:05:17,510","00:05:23,097",76,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=317,And there is also zero utility threshold.,pic_cs-410_6_6_300.jpg
cs-410_6_6_77,cs-410,6,6,  Recommender Systems,"00:05:23,097","00:05:27,720",77,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=323,You can see at this cutoff,pic_cs-410_6_6_300.jpg
cs-410_6_6_78,cs-410,6,6,  Recommender Systems,"00:05:27,720","00:05:28,740",78,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=327,What does that mean?,pic_cs-410_6_6_300.jpg
cs-410_6_6_79,cs-410,6,6,  Recommender Systems,"00:05:28,740","00:05:34,250",79,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=328,That means if I lower the threshold,pic_cs-410_6_6_300.jpg
cs-410_6_6_80,cs-410,6,6,  Recommender Systems,"00:05:34,250","00:05:41,305",80,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=334,The utility would be lower but,pic_cs-410_6_6_300.jpg
cs-410_6_6_81,cs-410,6,6,  Recommender Systems,"00:05:41,305","00:05:45,835",81,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=341,So it's not as high as,pic_cs-410_6_6_300.jpg
cs-410_6_6_82,cs-410,6,6,  Recommender Systems,"00:05:45,835","00:05:51,492",82,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=345,But it gives us as a safe point,pic_cs-410_6_6_300.jpg
cs-410_6_6_83,cs-410,6,6,  Recommender Systems,"00:05:51,492","00:05:56,052",83,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=351,"as I have explained, it's desirable",pic_cs-410_6_6_300.jpg
cs-410_6_6_84,cs-410,6,6,  Recommender Systems,"00:05:56,052","00:06:00,622",84,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=356,So it's desirable to lower the threshold,pic_cs-410_6_6_300.jpg
cs-410_6_6_85,cs-410,6,6,  Recommender Systems,"00:06:00,622","00:06:04,850",85,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=360,"So that means, in general, we want to set",pic_cs-410_6_6_360.jpg
cs-410_6_6_86,cs-410,6,6,  Recommender Systems,"00:06:04,850","00:06:06,730",86,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=364,Let's say we can use the alpha to control,pic_cs-410_6_6_360.jpg
cs-410_6_6_87,cs-410,6,6,  Recommender Systems,"00:06:08,310","00:06:13,210",87,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=368,the deviation from,pic_cs-410_6_6_360.jpg
cs-410_6_6_88,cs-410,6,6,  Recommender Systems,"00:06:13,210","00:06:16,570",88,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=373,So you can see the formula of the,pic_cs-410_6_6_360.jpg
cs-410_6_6_89,cs-410,6,6,  Recommender Systems,"00:06:16,570","00:06:21,210",89,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=376,of the zero utility threshold and,pic_cs-410_6_6_360.jpg
cs-410_6_6_90,cs-410,6,6,  Recommender Systems,"00:06:22,490","00:06:25,600",90,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=382,"Now, the question is,",pic_cs-410_6_6_360.jpg
cs-410_6_6_91,cs-410,6,6,  Recommender Systems,"00:06:27,420","00:06:31,880",91,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=387,And when should we deviate more,pic_cs-410_6_6_360.jpg
cs-410_6_6_92,cs-410,6,6,  Recommender Systems,"00:06:33,720","00:06:38,450",92,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=393,"Well, this can depend on multiple factors,",pic_cs-410_6_6_360.jpg
cs-410_6_6_93,cs-410,6,6,  Recommender Systems,"00:06:38,450","00:06:43,880",93,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=398,encourage this threshold,pic_cs-410_6_6_360.jpg
cs-410_6_6_94,cs-410,6,6,  Recommender Systems,"00:06:43,880","00:06:48,630",94,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=403,"up to the zero point, and",pic_cs-410_6_6_360.jpg
cs-410_6_6_95,cs-410,6,6,  Recommender Systems,"00:06:48,630","00:06:52,990",95,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=408,we're not going to necessarily reach,pic_cs-410_6_6_360.jpg
cs-410_6_6_96,cs-410,6,6,  Recommender Systems,"00:06:52,990","00:06:57,947",96,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=412,"Rather, we're going to use other",pic_cs-410_6_6_360.jpg
cs-410_6_6_97,cs-410,6,6,  Recommender Systems,"00:06:57,947","00:07:01,030",97,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=417,this specifically is as follows.,pic_cs-410_6_6_360.jpg
cs-410_6_6_98,cs-410,6,6,  Recommender Systems,"00:07:01,030","00:07:06,680",98,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=421,So there will be a beta parameter to,pic_cs-410_6_6_420.jpg
cs-410_6_6_99,cs-410,6,6,  Recommender Systems,"00:07:06,680","00:07:12,000",99,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=426,threshold and this can be based on can,pic_cs-410_6_6_420.jpg
cs-410_6_6_100,cs-410,6,6,  Recommender Systems,"00:07:12,000","00:07:17,960",100,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=432,"to the training data let's say, and so",pic_cs-410_6_6_420.jpg
cs-410_6_6_101,cs-410,6,6,  Recommender Systems,"00:07:17,960","00:07:20,500",101,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=437,But what's more interesting,pic_cs-410_6_6_420.jpg
cs-410_6_6_102,cs-410,6,6,  Recommender Systems,"00:07:20,500","00:07:25,510",102,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=440,"Here, and you can see in this formula,",pic_cs-410_6_6_420.jpg
cs-410_6_6_103,cs-410,6,6,  Recommender Systems,"00:07:25,510","00:07:31,134",103,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=445,gamma is controlling the inference,pic_cs-410_6_6_420.jpg
cs-410_6_6_104,cs-410,6,6,  Recommender Systems,"00:07:31,134","00:07:36,210",104,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=451,of the number of examples,pic_cs-410_6_6_420.jpg
cs-410_6_6_105,cs-410,6,6,  Recommender Systems,"00:07:36,210","00:07:43,340",105,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=456,So you can see in this formula as N which,pic_cs-410_6_6_420.jpg
cs-410_6_6_106,cs-410,6,6,  Recommender Systems,"00:07:43,340","00:07:50,820",106,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=463,"becomes bigger, then it would",pic_cs-410_6_6_420.jpg
cs-410_6_6_107,cs-410,6,6,  Recommender Systems,"00:07:50,820","00:07:55,140",107,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=470,"In other words, when these very",pic_cs-410_6_6_420.jpg
cs-410_6_6_108,cs-410,6,6,  Recommender Systems,"00:07:55,140","00:07:59,630",108,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=475,And that just means if we have seen few,pic_cs-410_6_6_420.jpg
cs-410_6_6_109,cs-410,6,6,  Recommender Systems,"00:07:59,630","00:08:04,330",109,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=479,examples we're not sure whether we,pic_cs-410_6_6_420.jpg
cs-410_6_6_110,cs-410,6,6,  Recommender Systems,"00:08:04,330","00:08:09,590",110,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=484,So we need to explore but as we have,pic_cs-410_6_6_480.jpg
cs-410_6_6_111,cs-410,6,6,  Recommender Systems,"00:08:09,590","00:08:13,510",111,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=489,many that have we feel that we,pic_cs-410_6_6_480.jpg
cs-410_6_6_112,cs-410,6,6,  Recommender Systems,"00:08:13,510","00:08:17,950",112,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=493,So this gives us a beta gamma for,pic_cs-410_6_6_480.jpg
cs-410_6_6_113,cs-410,6,6,  Recommender Systems,"00:08:17,950","00:08:21,500",113,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=497,The more examples we have seen,pic_cs-410_6_6_480.jpg
cs-410_6_6_114,cs-410,6,6,  Recommender Systems,"00:08:21,500","00:08:25,960",114,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=501,So the threshold would be closer,pic_cs-410_6_6_480.jpg
cs-410_6_6_115,cs-410,6,6,  Recommender Systems,"00:08:25,960","00:08:28,490",115,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=505,that's the basic idea of this approach.,pic_cs-410_6_6_480.jpg
cs-410_6_6_116,cs-410,6,6,  Recommender Systems,"00:08:28,490","00:08:34,120",116,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=508,This approach actually has been working,pic_cs-410_6_6_480.jpg
cs-410_6_6_117,cs-410,6,6,  Recommender Systems,"00:08:34,120","00:08:36,030",117,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=514,particularly effective.,pic_cs-410_6_6_480.jpg
cs-410_6_6_118,cs-410,6,6,  Recommender Systems,"00:08:36,030","00:08:42,300",118,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=516,And also can work on arbitrary utility,pic_cs-410_6_6_480.jpg
cs-410_6_6_119,cs-410,6,6,  Recommender Systems,"00:08:43,710","00:08:48,020",119,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=523,And explicitly addresses,pic_cs-410_6_6_480.jpg
cs-410_6_6_120,cs-410,6,6,  Recommender Systems,"00:08:48,020","00:08:53,234",120,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=528,it kind of uses the zero utility,pic_cs-410_6_6_480.jpg
cs-410_6_6_121,cs-410,6,6,  Recommender Systems,"00:08:53,234","00:08:56,810",121,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=533,exploration-exploitation tradeoff.,pic_cs-410_6_6_480.jpg
cs-410_6_6_122,cs-410,6,6,  Recommender Systems,"00:08:56,810","00:09:02,770",122,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=536,We're not never going to explore,pic_cs-410_6_6_480.jpg
cs-410_6_6_123,cs-410,6,6,  Recommender Systems,"00:09:02,770","00:09:05,530",123,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=542,"So if you take the analogy of gambling,",pic_cs-410_6_6_540.jpg
cs-410_6_6_124,cs-410,6,6,  Recommender Systems,"00:09:05,530","00:09:08,950",124,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=545,you don't want to risk on losing money.,pic_cs-410_6_6_540.jpg
cs-410_6_6_125,cs-410,6,6,  Recommender Systems,"00:09:08,950","00:09:12,140",125,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=548,"So it's a safe spend, really",pic_cs-410_6_6_540.jpg
cs-410_6_6_126,cs-410,6,6,  Recommender Systems,"00:09:13,270","00:09:18,250",126,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=553,"And the problem is of course,",pic_cs-410_6_6_540.jpg
cs-410_6_6_127,cs-410,6,6,  Recommender Systems,"00:09:18,250","00:09:23,643",127,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=558,the zero utility lower boundary is also,pic_cs-410_6_6_540.jpg
cs-410_6_6_128,cs-410,6,6,  Recommender Systems,"00:09:23,643","00:09:28,855",128,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=563,"course, more advance in machine learning",pic_cs-410_6_6_540.jpg
cs-410_6_6_129,cs-410,6,6,  Recommender Systems,"00:09:28,855","00:09:33,815",129,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=568,solving this problems and,pic_cs-410_6_6_540.jpg
cs-410_6_6_130,cs-410,6,6,  Recommender Systems,"00:09:35,225","00:09:41,550",130,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=575,"So to summarize, there are two",pic_cs-410_6_6_540.jpg
cs-410_6_6_131,cs-410,6,6,  Recommender Systems,"00:09:41,550","00:09:47,070",131,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=581,"filtering systems, one is content based,",pic_cs-410_6_6_540.jpg
cs-410_6_6_132,cs-410,6,6,  Recommender Systems,"00:09:47,070","00:09:51,302",132,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=587,and the other is collaborative filtering,pic_cs-410_6_6_540.jpg
cs-410_6_6_133,cs-410,6,6,  Recommender Systems,"00:09:51,302","00:09:56,710",133,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=591,We've covered content-based,pic_cs-410_6_6_540.jpg
cs-410_6_6_134,cs-410,6,6,  Recommender Systems,"00:09:56,710","00:09:59,566",134,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=596,"In the next lecture, we will talk",pic_cs-410_6_6_540.jpg
cs-410_6_6_135,cs-410,6,6,  Recommender Systems,"00:09:59,566","00:10:07,030",135,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=599,"In content-based filtering system,",pic_cs-410_6_6_540.jpg
cs-410_6_6_136,cs-410,6,6,  Recommender Systems,"00:10:07,030","00:10:11,750",136,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=607,several problems relative to,pic_cs-410_6_6_600.jpg
cs-410_6_6_137,cs-410,6,6,  Recommender Systems,"00:10:11,750","00:10:17,130",137,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=611,And such a system can actually be,pic_cs-410_6_6_600.jpg
cs-410_6_6_138,cs-410,6,6,  Recommender Systems,"00:10:17,130","00:10:22,978",138,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=617,by adding a threshold mechanism and,pic_cs-410_6_6_600.jpg
cs-410_6_6_139,cs-410,6,6,  Recommender Systems,"00:10:22,978","00:10:28,011",139,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=622,allow the system to learn from,pic_cs-410_6_6_600.jpg
cs-410_6_6_140,cs-410,6,6,  Recommender Systems,"00:10:30,357","00:10:40,357",140,https://www.coursera.org/learn/cs-410/lecture/7M0GD?t=630,[MUSIC],pic_cs-410_6_6_600.jpg
cs-410_6_7_1,cs-410,6,7, Recommender Systems,"00:00:07,400","00:00:09,600",1,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=7,This lecture is about,pic_cs-410_6_7_0.jpg
cs-410_6_7_2,cs-410,6,7, Recommender Systems,"00:00:11,540","00:00:16,250",2,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=11,In this lecture we're going to continue,pic_cs-410_6_7_0.jpg
cs-410_6_7_3,cs-410,6,7, Recommender Systems,"00:00:16,250","00:00:21,390",3,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=16,"In particular, we're going to look at",pic_cs-410_6_7_0.jpg
cs-410_6_7_4,cs-410,6,7, Recommender Systems,"00:00:21,390","00:00:25,710",4,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=21,You have seen this slide before when,pic_cs-410_6_7_0.jpg
cs-410_6_7_5,cs-410,6,7, Recommender Systems,"00:00:25,710","00:00:30,310",5,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=25,"answer the basic question,",pic_cs-410_6_7_0.jpg
cs-410_6_7_6,cs-410,6,7, Recommender Systems,"00:00:30,310","00:00:31,290",6,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=30,"In the previous lecture,",pic_cs-410_6_7_0.jpg
cs-410_6_7_7,cs-410,6,7, Recommender Systems,"00:00:31,290","00:00:36,180",7,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=31,"we looked at the item similarity,",pic_cs-410_6_7_0.jpg
cs-410_6_7_8,cs-410,6,7, Recommender Systems,"00:00:36,180","00:00:39,580",8,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=36,"In this lecture, we're going to",pic_cs-410_6_7_0.jpg
cs-410_6_7_9,cs-410,6,7, Recommender Systems,"00:00:39,580","00:00:42,490",9,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=39,"This is a different strategy,",pic_cs-410_6_7_0.jpg
cs-410_6_7_10,cs-410,6,7, Recommender Systems,"00:00:44,090","00:00:45,630",10,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=44,"So first, what is collaborative filtering?",pic_cs-410_6_7_0.jpg
cs-410_6_7_11,cs-410,6,7, Recommender Systems,"00:00:47,460","00:00:49,525",11,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=47,It is to make filtering decisions for,pic_cs-410_6_7_0.jpg
cs-410_6_7_12,cs-410,6,7, Recommender Systems,"00:00:49,525","00:00:52,660",12,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=49,individual user based on,pic_cs-410_6_7_0.jpg
cs-410_6_7_13,cs-410,6,7, Recommender Systems,"00:00:54,240","00:00:58,000",13,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=54,And that is to say we will,pic_cs-410_6_7_0.jpg
cs-410_6_7_14,cs-410,6,7, Recommender Systems,"00:00:58,000","00:01:02,080",14,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=58,preferences from that,pic_cs-410_6_7_0.jpg
cs-410_6_7_15,cs-410,6,7, Recommender Systems,"00:01:02,080","00:01:04,530",15,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=62,So the general idea is the following.,pic_cs-410_6_7_60.jpg
cs-410_6_7_16,cs-410,6,7, Recommender Systems,"00:01:04,530","00:01:11,693",16,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=64,"Given a user u, we're going to first",pic_cs-410_6_7_60.jpg
cs-410_6_7_17,cs-410,6,7, Recommender Systems,"00:01:11,693","00:01:15,581",17,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=71,And then we're going to,pic_cs-410_6_7_60.jpg
cs-410_6_7_18,cs-410,6,7, Recommender Systems,"00:01:15,581","00:01:20,540",18,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=75,based on the preferences of,pic_cs-410_6_7_60.jpg
cs-410_6_7_19,cs-410,6,7, Recommender Systems,"00:01:22,390","00:01:26,960",19,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=82,"Now, the user similarity here can",pic_cs-410_6_7_60.jpg
cs-410_6_7_20,cs-410,6,7, Recommender Systems,"00:01:26,960","00:01:29,610",20,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=86,the preferences on a common set of items.,pic_cs-410_6_7_60.jpg
cs-410_6_7_21,cs-410,6,7, Recommender Systems,"00:01:31,070","00:01:36,020",21,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=91,Now here you can see the exact,pic_cs-410_6_7_60.jpg
cs-410_6_7_22,cs-410,6,7, Recommender Systems,"00:01:36,020","00:01:40,430",22,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=96,We're going to look at the only the,pic_cs-410_6_7_60.jpg
cs-410_6_7_23,cs-410,6,7, Recommender Systems,"00:01:41,730","00:01:44,120",23,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=101,So this means this,pic_cs-410_6_7_60.jpg
cs-410_6_7_24,cs-410,6,7, Recommender Systems,"00:01:44,120","00:01:49,450",24,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=104,"It can be applied to any items,",pic_cs-410_6_7_60.jpg
cs-410_6_7_25,cs-410,6,7, Recommender Systems,"00:01:49,450","00:01:53,700",25,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=109,So this approach would work well,pic_cs-410_6_7_60.jpg
cs-410_6_7_26,cs-410,6,7, Recommender Systems,"00:01:53,700","00:01:59,230",26,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=113,"First, users with the same interest",pic_cs-410_6_7_60.jpg
cs-410_6_7_27,cs-410,6,7, Recommender Systems,"00:01:59,230","00:02:03,570",27,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=119,"Second, the users with similar preferences",pic_cs-410_6_7_60.jpg
cs-410_6_7_28,cs-410,6,7, Recommender Systems,"00:02:03,570","00:02:08,650",28,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=123,"So for example, if the interest of",pic_cs-410_6_7_120.jpg
cs-410_6_7_29,cs-410,6,7, Recommender Systems,"00:02:08,650","00:02:12,960",29,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=128,then we can infer the user,pic_cs-410_6_7_120.jpg
cs-410_6_7_30,cs-410,6,7, Recommender Systems,"00:02:14,280","00:02:17,270",30,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=134,So those who are interested in,pic_cs-410_6_7_120.jpg
cs-410_6_7_31,cs-410,6,7, Recommender Systems,"00:02:17,270","00:02:19,840",31,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=137,probably all favor SIGIR papers.,pic_cs-410_6_7_120.jpg
cs-410_6_7_32,cs-410,6,7, Recommender Systems,"00:02:19,840","00:02:21,880",32,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=139,That's an assumption that we make.,pic_cs-410_6_7_120.jpg
cs-410_6_7_33,cs-410,6,7, Recommender Systems,"00:02:21,880","00:02:23,440",33,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=141,"And if this assumption is true,",pic_cs-410_6_7_120.jpg
cs-410_6_7_34,cs-410,6,7, Recommender Systems,"00:02:23,440","00:02:27,490",34,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=143,then it would help collaborative,pic_cs-410_6_7_120.jpg
cs-410_6_7_35,cs-410,6,7, Recommender Systems,"00:02:27,490","00:02:34,055",35,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=147,We can also assume that if we see,pic_cs-410_6_7_120.jpg
cs-410_6_7_36,cs-410,6,7, Recommender Systems,"00:02:34,055","00:02:38,215",36,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=154,then we can infer their interest,pic_cs-410_6_7_120.jpg
cs-410_6_7_37,cs-410,6,7, Recommender Systems,"00:02:38,215","00:02:43,025",37,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=158,"So in these simple examples,",pic_cs-410_6_7_120.jpg
cs-410_6_7_38,cs-410,6,7, Recommender Systems,"00:02:43,025","00:02:48,492",38,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=163,in many cases such assumption,pic_cs-410_6_7_120.jpg
cs-410_6_7_39,cs-410,6,7, Recommender Systems,"00:02:48,492","00:02:52,896",39,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=168,So another assumption we have to make,pic_cs-410_6_7_120.jpg
cs-410_6_7_40,cs-410,6,7, Recommender Systems,"00:02:52,896","00:02:56,012",40,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=172,number of user preferences,pic_cs-410_6_7_120.jpg
cs-410_6_7_41,cs-410,6,7, Recommender Systems,"00:02:56,012","00:03:00,722",41,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=176,"So for example, if you see a lot",pic_cs-410_6_7_120.jpg
cs-410_6_7_42,cs-410,6,7, Recommender Systems,"00:03:00,722","00:03:03,160",42,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=180,those indicate their,pic_cs-410_6_7_180.jpg
cs-410_6_7_43,cs-410,6,7, Recommender Systems,"00:03:03,160","00:03:06,832",43,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=183,"And if you have a lot of such data,",pic_cs-410_6_7_180.jpg
cs-410_6_7_44,cs-410,6,7, Recommender Systems,"00:03:06,832","00:03:08,689",44,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=186,filtering can be very effective.,pic_cs-410_6_7_180.jpg
cs-410_6_7_45,cs-410,6,7, Recommender Systems,"00:03:09,960","00:03:14,680",45,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=189,"If not, there will be a problem, and",pic_cs-410_6_7_180.jpg
cs-410_6_7_46,cs-410,6,7, Recommender Systems,"00:03:14,680","00:03:18,640",46,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=194,That means you don't have many,pic_cs-410_6_7_180.jpg
cs-410_6_7_47,cs-410,6,7, Recommender Systems,"00:03:18,640","00:03:23,722",47,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=198,the system could not fully take advantage,pic_cs-410_6_7_180.jpg
cs-410_6_7_48,cs-410,6,7, Recommender Systems,"00:03:23,722","00:03:28,690",48,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=203,So let's look at the filtering,pic_cs-410_6_7_180.jpg
cs-410_6_7_49,cs-410,6,7, Recommender Systems,"00:03:30,340","00:03:33,791",49,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=210,"So this picture shows that we are,",pic_cs-410_6_7_180.jpg
cs-410_6_7_50,cs-410,6,7, Recommender Systems,"00:03:33,791","00:03:38,075",50,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=213,"in general, considering a lot of users and",pic_cs-410_6_7_180.jpg
cs-410_6_7_51,cs-410,6,7, Recommender Systems,"00:03:38,075","00:03:42,956",51,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=218,"we're showing m users here, so U1 through.",pic_cs-410_6_7_180.jpg
cs-410_6_7_52,cs-410,6,7, Recommender Systems,"00:03:42,956","00:03:46,040",52,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=222,And we're also considering,pic_cs-410_6_7_180.jpg
cs-410_6_7_53,cs-410,6,7, Recommender Systems,"00:03:46,040","00:03:49,870",53,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=226,Let's say n objects in,pic_cs-410_6_7_180.jpg
cs-410_6_7_54,cs-410,6,7, Recommender Systems,"00:03:49,870","00:03:55,330",54,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=229,And then we will assume that,pic_cs-410_6_7_180.jpg
cs-410_6_7_55,cs-410,6,7, Recommender Systems,"00:03:55,330","00:04:01,510",55,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=235,objects and the user could for,pic_cs-410_6_7_180.jpg
cs-410_6_7_56,cs-410,6,7, Recommender Systems,"00:04:01,510","00:04:06,490",56,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=241,"For example, those items could be movies,",pic_cs-410_6_7_240.jpg
cs-410_6_7_57,cs-410,6,7, Recommender Systems,"00:04:06,490","00:04:10,500",57,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=246,then the users would give,pic_cs-410_6_7_240.jpg
cs-410_6_7_58,cs-410,6,7, Recommender Systems,"00:04:10,500","00:04:14,829",58,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=250,So what you see here is that we have,pic_cs-410_6_7_240.jpg
cs-410_6_7_59,cs-410,6,7, Recommender Systems,"00:04:14,829","00:04:16,231",59,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=254,some combinations.,pic_cs-410_6_7_240.jpg
cs-410_6_7_60,cs-410,6,7, Recommender Systems,"00:04:16,231","00:04:21,751",60,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=256,"So some users have watched some movies,",pic_cs-410_6_7_240.jpg
cs-410_6_7_61,cs-410,6,7, Recommender Systems,"00:04:21,751","00:04:26,075",61,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=261,they obviously won't be able,pic_cs-410_6_7_240.jpg
cs-410_6_7_62,cs-410,6,7, Recommender Systems,"00:04:26,075","00:04:30,040",62,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=266,some users may actually,pic_cs-410_6_7_240.jpg
cs-410_6_7_63,cs-410,6,7, Recommender Systems,"00:04:30,040","00:04:34,410",63,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=270,So this is in general a small symmetrics.,pic_cs-410_6_7_240.jpg
cs-410_6_7_64,cs-410,6,7, Recommender Systems,"00:04:34,410","00:04:38,030",64,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=274,So many items and,pic_cs-410_6_7_240.jpg
cs-410_6_7_65,cs-410,6,7, Recommender Systems,"00:04:39,160","00:04:46,070",65,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=279,And what's interesting here is we,pic_cs-410_6_7_240.jpg
cs-410_6_7_66,cs-410,6,7, Recommender Systems,"00:04:46,070","00:04:51,780",66,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=286,of an element in this matrix,pic_cs-410_6_7_240.jpg
cs-410_6_7_67,cs-410,6,7, Recommender Systems,"00:04:51,780","00:04:56,610",67,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=291,And that's after the essential question,pic_cs-410_6_7_240.jpg
cs-410_6_7_68,cs-410,6,7, Recommender Systems,"00:04:56,610","00:04:59,950",68,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=296,we assume there's an unknown,pic_cs-410_6_7_240.jpg
cs-410_6_7_69,cs-410,6,7, Recommender Systems,"00:04:59,950","00:05:04,400",69,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=299,That would map a pair of user and,pic_cs-410_6_7_240.jpg
cs-410_6_7_70,cs-410,6,7, Recommender Systems,"00:05:04,400","00:05:07,610",70,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=304,And we have observed the sum,pic_cs-410_6_7_300.jpg
cs-410_6_7_71,cs-410,6,7, Recommender Systems,"00:05:08,960","00:05:14,296",71,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=308,And we want to infer the value,pic_cs-410_6_7_300.jpg
cs-410_6_7_72,cs-410,6,7, Recommender Systems,"00:05:14,296","00:05:20,168",72,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=314,other pairs that don't have,pic_cs-410_6_7_300.jpg
cs-410_6_7_73,cs-410,6,7, Recommender Systems,"00:05:20,168","00:05:26,198",73,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=320,So this is very similar to other,pic_cs-410_6_7_300.jpg
cs-410_6_7_74,cs-410,6,7, Recommender Systems,"00:05:26,198","00:05:31,784",74,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=326,know the values of the function,pic_cs-410_6_7_300.jpg
cs-410_6_7_75,cs-410,6,7, Recommender Systems,"00:05:31,784","00:05:37,384",75,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=331,And we hope to predict the values of,pic_cs-410_6_7_300.jpg
cs-410_6_7_76,cs-410,6,7, Recommender Systems,"00:05:37,384","00:05:40,344",76,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=337,this is a function approximation.,pic_cs-410_6_7_300.jpg
cs-410_6_7_77,cs-410,6,7, Recommender Systems,"00:05:40,344","00:05:47,440",77,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=340,And how can we pick out the function,pic_cs-410_6_7_300.jpg
cs-410_6_7_78,cs-410,6,7, Recommender Systems,"00:05:47,440","00:05:50,230",78,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=347,So this is the setup.,pic_cs-410_6_7_300.jpg
cs-410_6_7_79,cs-410,6,7, Recommender Systems,"00:05:50,230","00:05:54,680",79,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=350,Now there are many approaches,pic_cs-410_6_7_300.jpg
cs-410_6_7_80,cs-410,6,7, Recommender Systems,"00:05:54,680","00:06:00,415",80,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=354,"In fact,",pic_cs-410_6_7_300.jpg
cs-410_6_7_81,cs-410,6,7, Recommender Systems,"00:06:00,415","00:06:09,095",81,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=360,reason that there are special,pic_cs-410_6_7_360.jpg
cs-410_6_7_82,cs-410,6,7, Recommender Systems,"00:06:10,419","00:06:15,730",82,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=370,major conference devoted to the problem.,pic_cs-410_6_7_360.jpg
cs-410_6_7_83,cs-410,6,7, Recommender Systems,"00:06:15,730","00:06:20,199",83,https://www.coursera.org/learn/cs-410/lecture/cIFsU?t=375,[MUSIC],pic_cs-410_6_7_360.jpg
cs-410_6_8_1,cs-410,6,8, Recommender Systems,"00:00:00,012","00:00:09,135",1,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=0,[SOUND],pic_cs-410_6_8_0.jpg
cs-410_6_8_2,cs-410,6,8, Recommender Systems,"00:00:09,135","00:00:12,960",2,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=9,here we're going to talk,pic_cs-410_6_8_0.jpg
cs-410_6_8_3,cs-410,6,8, Recommender Systems,"00:00:12,960","00:00:18,430",3,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=12,And that would be based on,pic_cs-410_6_8_0.jpg
cs-410_6_8_4,cs-410,6,8, Recommender Systems,"00:00:18,430","00:00:23,220",4,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=18,then predicting the rating of and,pic_cs-410_6_8_0.jpg
cs-410_6_8_5,cs-410,6,8, Recommender Systems,"00:00:23,220","00:00:32,540",5,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=23,object by an active user using the ratings,pic_cs-410_6_8_0.jpg
cs-410_6_8_6,cs-410,6,8, Recommender Systems,"00:00:32,540","00:00:38,600",6,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=32,This is called a memory based approach,pic_cs-410_6_8_0.jpg
cs-410_6_8_7,cs-410,6,8, Recommender Systems,"00:00:40,120","00:00:44,460",7,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=40,storing all the user information and,pic_cs-410_6_8_0.jpg
cs-410_6_8_8,cs-410,6,8, Recommender Systems,"00:00:44,460","00:00:49,713",8,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=44,when we are considering a particular,pic_cs-410_6_8_0.jpg
cs-410_6_8_9,cs-410,6,8, Recommender Systems,"00:00:49,713","00:00:56,210",9,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=49,retrieve the rating users or,pic_cs-410_6_8_0.jpg
cs-410_6_8_10,cs-410,6,8, Recommender Systems,"00:00:56,210","00:01:01,140",10,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=56,And then try to use this,pic_cs-410_6_8_0.jpg
cs-410_6_8_11,cs-410,6,8, Recommender Systems,"00:01:01,140","00:01:05,120",11,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=61,to predict the preference of this user.,pic_cs-410_6_8_60.jpg
cs-410_6_8_12,cs-410,6,8, Recommender Systems,"00:01:05,120","00:01:11,700",12,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=65,So here is the general idea and,pic_cs-410_6_8_60.jpg
cs-410_6_8_13,cs-410,6,8, Recommender Systems,"00:01:11,700","00:01:16,570",13,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=71,x sub i j denotes the rating,pic_cs-410_6_8_60.jpg
cs-410_6_8_14,cs-410,6,8, Recommender Systems,"00:01:17,910","00:01:23,460",14,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=77,and n sub i is average rating,pic_cs-410_6_8_60.jpg
cs-410_6_8_15,cs-410,6,8, Recommender Systems,"00:01:26,100","00:01:31,050",15,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=86,So this n i is needed because,pic_cs-410_6_8_60.jpg
cs-410_6_8_16,cs-410,6,8, Recommender Systems,"00:01:31,050","00:01:35,500",16,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=91,we would like to normalize,pic_cs-410_6_8_60.jpg
cs-410_6_8_17,cs-410,6,8, Recommender Systems,"00:01:35,500","00:01:39,190",17,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=95,So how do you do normalization?,pic_cs-410_6_8_60.jpg
cs-410_6_8_18,cs-410,6,8, Recommender Systems,"00:01:39,190","00:01:46,440",18,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=99,"Well, we're going to just subtract",pic_cs-410_6_8_60.jpg
cs-410_6_8_19,cs-410,6,8, Recommender Systems,"00:01:46,440","00:01:49,890",19,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=106,"Now, this is to normalize these ratings so",pic_cs-410_6_8_60.jpg
cs-410_6_8_20,cs-410,6,8, Recommender Systems,"00:01:49,890","00:01:53,510",20,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=109,that the ratings from different,pic_cs-410_6_8_60.jpg
cs-410_6_8_21,cs-410,6,8, Recommender Systems,"00:01:55,590","00:02:00,220",21,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=115,"Because some users might be more generous,",pic_cs-410_6_8_60.jpg
cs-410_6_8_22,cs-410,6,8, Recommender Systems,"00:02:00,220","00:02:05,160",22,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=120,ratings but some others might be,pic_cs-410_6_8_120.jpg
cs-410_6_8_23,cs-410,6,8, Recommender Systems,"00:02:05,160","00:02:10,850",23,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=125,cannot be directly compared with each,pic_cs-410_6_8_120.jpg
cs-410_6_8_24,cs-410,6,8, Recommender Systems,"00:02:10,850","00:02:13,450",24,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=130,So we need to do this normalization.,pic_cs-410_6_8_120.jpg
cs-410_6_8_25,cs-410,6,8, Recommender Systems,"00:02:13,450","00:02:18,420",25,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=133,Another prediction of,pic_cs-410_6_8_120.jpg
cs-410_6_8_26,cs-410,6,8, Recommender Systems,"00:02:18,420","00:02:22,880",26,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=138,by another user or,pic_cs-410_6_8_120.jpg
cs-410_6_8_27,cs-410,6,8, Recommender Systems,"00:02:24,460","00:02:29,419",27,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=144,can be based on the average,pic_cs-410_6_8_120.jpg
cs-410_6_8_28,cs-410,6,8, Recommender Systems,"00:02:30,630","00:02:36,960",28,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=150,So the user u sub a is the user that we,pic_cs-410_6_8_120.jpg
cs-410_6_8_29,cs-410,6,8, Recommender Systems,"00:02:36,960","00:02:42,400",29,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=156,And we now are interested in,pic_cs-410_6_8_120.jpg
cs-410_6_8_30,cs-410,6,8, Recommender Systems,"00:02:42,400","00:02:47,910",30,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=162,So we're interested in knowing how,pic_cs-410_6_8_120.jpg
cs-410_6_8_31,cs-410,6,8, Recommender Systems,"00:02:47,910","00:02:49,370",31,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=167,How do we know that?,pic_cs-410_6_8_120.jpg
cs-410_6_8_32,cs-410,6,8, Recommender Systems,"00:02:50,370","00:02:55,560",32,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=170,Where the idea here is to look at,pic_cs-410_6_8_120.jpg
cs-410_6_8_33,cs-410,6,8, Recommender Systems,"00:02:55,560","00:02:57,260",33,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=175,have liked this object.,pic_cs-410_6_8_120.jpg
cs-410_6_8_34,cs-410,6,8, Recommender Systems,"00:02:59,530","00:03:04,720",34,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=179,So mathematically this is to say,pic_cs-410_6_8_120.jpg
cs-410_6_8_35,cs-410,6,8, Recommender Systems,"00:03:04,720","00:03:12,130",35,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=184,"this user on this app object,",pic_cs-410_6_8_180.jpg
cs-410_6_8_36,cs-410,6,8, Recommender Systems,"00:03:12,130","00:03:18,640",36,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=192,combination of the normalized,pic_cs-410_6_8_180.jpg
cs-410_6_8_37,cs-410,6,8, Recommender Systems,"00:03:18,640","00:03:23,950",37,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=198,"and in fact here,",pic_cs-410_6_8_180.jpg
cs-410_6_8_38,cs-410,6,8, Recommender Systems,"00:03:23,950","00:03:29,180",38,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=203,But not all users contribute,pic_cs-410_6_8_180.jpg
cs-410_6_8_39,cs-410,6,8, Recommender Systems,"00:03:29,180","00:03:31,748",39,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=209,and this is conjured by the weights.,pic_cs-410_6_8_180.jpg
cs-410_6_8_40,cs-410,6,8, Recommender Systems,"00:03:31,748","00:03:37,191",40,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=211,So this weight controls the inference,pic_cs-410_6_8_180.jpg
cs-410_6_8_41,cs-410,6,8, Recommender Systems,"00:03:37,191","00:03:41,618",41,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=217,of the user on the prediction.,pic_cs-410_6_8_180.jpg
cs-410_6_8_42,cs-410,6,8, Recommender Systems,"00:03:41,618","00:03:46,763",42,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=221,"And of course,",pic_cs-410_6_8_180.jpg
cs-410_6_8_43,cs-410,6,8, Recommender Systems,"00:03:46,763","00:03:51,917",43,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=226,the similarity between ua and,pic_cs-410_6_8_180.jpg
cs-410_6_8_44,cs-410,6,8, Recommender Systems,"00:03:51,917","00:03:57,650",44,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=231,"The more similar they are,",pic_cs-410_6_8_180.jpg
cs-410_6_8_45,cs-410,6,8, Recommender Systems,"00:03:57,650","00:04:02,420",45,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=237,user ui can make in predicting,pic_cs-410_6_8_180.jpg
cs-410_6_8_46,cs-410,6,8, Recommender Systems,"00:04:03,950","00:04:06,060",46,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=243,"So, the formula is extremely simple.",pic_cs-410_6_8_240.jpg
cs-410_6_8_47,cs-410,6,8, Recommender Systems,"00:04:06,060","00:04:10,140",47,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=246,"You can see,",pic_cs-410_6_8_240.jpg
cs-410_6_8_48,cs-410,6,8, Recommender Systems,"00:04:10,140","00:04:14,040",48,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=250,"And inside the sum we have their ratings,",pic_cs-410_6_8_240.jpg
cs-410_6_8_49,cs-410,6,8, Recommender Systems,"00:04:14,040","00:04:17,380",49,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=254,their normalized ratings,pic_cs-410_6_8_240.jpg
cs-410_6_8_50,cs-410,6,8, Recommender Systems,"00:04:17,380","00:04:21,400",50,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=257,The ratings need to be normalized in,pic_cs-410_6_8_240.jpg
cs-410_6_8_51,cs-410,6,8, Recommender Systems,"00:04:22,690","00:04:25,739",51,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=262,And then these ratings,pic_cs-410_6_8_240.jpg
cs-410_6_8_52,cs-410,6,8, Recommender Systems,"00:04:26,750","00:04:33,310",52,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=266,So you can imagine w of a and i is just,pic_cs-410_6_8_240.jpg
cs-410_6_8_53,cs-410,6,8, Recommender Systems,"00:04:34,470","00:04:35,350",53,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=274,Now what's k here?,pic_cs-410_6_8_240.jpg
cs-410_6_8_54,cs-410,6,8, Recommender Systems,"00:04:35,350","00:04:39,120",54,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=275,Well k is simply a normalizer.,pic_cs-410_6_8_240.jpg
cs-410_6_8_55,cs-410,6,8, Recommender Systems,"00:04:39,120","00:04:45,670",55,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=279,It's just one over the sum of all,pic_cs-410_6_8_240.jpg
cs-410_6_8_56,cs-410,6,8, Recommender Systems,"00:04:47,860","00:04:54,680",56,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=287,"So this means, basically, if you consider",pic_cs-410_6_8_240.jpg
cs-410_6_8_57,cs-410,6,8, Recommender Systems,"00:04:54,680","00:04:59,259",57,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=294,we have coefficients of weight that,pic_cs-410_6_8_240.jpg
cs-410_6_8_58,cs-410,6,8, Recommender Systems,"00:05:00,430","00:05:05,690",58,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=300,And it's just a normalization strategy so,pic_cs-410_6_8_300.jpg
cs-410_6_8_59,cs-410,6,8, Recommender Systems,"00:05:05,690","00:05:12,319",59,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=305,in the same range as these ratings,pic_cs-410_6_8_300.jpg
cs-410_6_8_60,cs-410,6,8, Recommender Systems,"00:05:13,650","00:05:14,560",60,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=313,Right?,pic_cs-410_6_8_300.jpg
cs-410_6_8_61,cs-410,6,8, Recommender Systems,"00:05:14,560","00:05:20,320",61,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=314,So this is basically the main idea,pic_cs-410_6_8_300.jpg
cs-410_6_8_62,cs-410,6,8, Recommender Systems,"00:05:20,320","00:05:21,270",62,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=320,collaborative filtering.,pic_cs-410_6_8_300.jpg
cs-410_6_8_63,cs-410,6,8, Recommender Systems,"00:05:22,750","00:05:27,880",63,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=322,"Once we make this prediction,",pic_cs-410_6_8_300.jpg
cs-410_6_8_64,cs-410,6,8, Recommender Systems,"00:05:27,880","00:05:33,270",64,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=327,back through the rating that,pic_cs-410_6_8_300.jpg
cs-410_6_8_65,cs-410,6,8, Recommender Systems,"00:05:33,270","00:05:38,520",65,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=333,"the user would actually make,",pic_cs-410_6_8_300.jpg
cs-410_6_8_66,cs-410,6,8, Recommender Systems,"00:05:38,520","00:05:44,110",66,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=338,and this is to further,pic_cs-410_6_8_300.jpg
cs-410_6_8_67,cs-410,6,8, Recommender Systems,"00:05:44,110","00:05:49,980",67,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=344,average rating of this user u,pic_cs-410_6_8_300.jpg
cs-410_6_8_68,cs-410,6,8, Recommender Systems,"00:05:49,980","00:05:54,290",68,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=349,This would recover a meaningful rating for,pic_cs-410_6_8_300.jpg
cs-410_6_8_69,cs-410,6,8, Recommender Systems,"00:05:54,290","00:05:59,410",69,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=354,"So if this user is generous, then",pic_cs-410_6_8_300.jpg
cs-410_6_8_70,cs-410,6,8, Recommender Systems,"00:05:59,410","00:06:04,580",70,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=359,and when we add that the rating will be,pic_cs-410_6_8_300.jpg
cs-410_6_8_71,cs-410,6,8, Recommender Systems,"00:06:04,580","00:06:10,459",71,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=364,Now when you recommend an item to a user,pic_cs-410_6_8_360.jpg
cs-410_6_8_72,cs-410,6,8, Recommender Systems,"00:06:10,459","00:06:15,093",72,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=370,because you are interested in,pic_cs-410_6_8_360.jpg
cs-410_6_8_73,cs-410,6,8, Recommender Systems,"00:06:15,093","00:06:17,158",73,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=375,that's more meaningful.,pic_cs-410_6_8_360.jpg
cs-410_6_8_74,cs-410,6,8, Recommender Systems,"00:06:17,158","00:06:22,624",74,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=377,But when they evaluate these,pic_cs-410_6_8_360.jpg
cs-410_6_8_75,cs-410,6,8, Recommender Systems,"00:06:22,624","00:06:27,563",75,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=382,they typically assume that,pic_cs-410_6_8_360.jpg
cs-410_6_8_76,cs-410,6,8, Recommender Systems,"00:06:27,563","00:06:32,923",76,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=387,these objects to be unknown and,pic_cs-410_6_8_360.jpg
cs-410_6_8_77,cs-410,6,8, Recommender Systems,"00:06:32,923","00:06:38,938",77,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=392,then you compare the predicted,pic_cs-410_6_8_360.jpg
cs-410_6_8_78,cs-410,6,8, Recommender Systems,"00:06:38,938","00:06:42,020",78,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=398,"So, you do have access",pic_cs-410_6_8_360.jpg
cs-410_6_8_79,cs-410,6,8, Recommender Systems,"00:06:42,020","00:06:44,100",79,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=402,"But, then you pretend that you don't know,",pic_cs-410_6_8_360.jpg
cs-410_6_8_80,cs-410,6,8, Recommender Systems,"00:06:44,100","00:06:48,420",80,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=404,then you compare your systems,pic_cs-410_6_8_360.jpg
cs-410_6_8_81,cs-410,6,8, Recommender Systems,"00:06:48,420","00:06:54,130",81,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=408,"In that case, obviously, the systems",pic_cs-410_6_8_360.jpg
cs-410_6_8_82,cs-410,6,8, Recommender Systems,"00:06:54,130","00:06:59,570",82,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=414,the actual ratings of the user and,pic_cs-410_6_8_360.jpg
cs-410_6_8_83,cs-410,6,8, Recommender Systems,"00:07:01,040","00:07:05,160",83,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=421,Okay so this is the memory based approach.,pic_cs-410_6_8_420.jpg
cs-410_6_8_84,cs-410,6,8, Recommender Systems,"00:07:05,160","00:07:07,000",84,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=425,"Now, of course,",pic_cs-410_6_8_420.jpg
cs-410_6_8_85,cs-410,6,8, Recommender Systems,"00:07:07,000","00:07:09,430",85,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=427,if you want to write,pic_cs-410_6_8_420.jpg
cs-410_6_8_86,cs-410,6,8, Recommender Systems,"00:07:09,430","00:07:15,510",86,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=429,you still face the problem of,pic_cs-410_6_8_420.jpg
cs-410_6_8_87,cs-410,6,8, Recommender Systems,"00:07:15,510","00:07:20,890",87,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=435,"Once you know the w function, then",pic_cs-410_6_8_420.jpg
cs-410_6_8_88,cs-410,6,8, Recommender Systems,"00:07:22,740","00:07:28,910",88,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=442,"So, indeed, there are many different ways",pic_cs-410_6_8_420.jpg
cs-410_6_8_89,cs-410,6,8, Recommender Systems,"00:07:28,910","00:07:33,550",89,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=448,"w, and specific approaches generally",pic_cs-410_6_8_420.jpg
cs-410_6_8_90,cs-410,6,8, Recommender Systems,"00:07:35,500","00:07:38,220",90,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=455,So here are some possibilities and,pic_cs-410_6_8_420.jpg
cs-410_6_8_91,cs-410,6,8, Recommender Systems,"00:07:38,220","00:07:42,010",91,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=458,you can imagine there,pic_cs-410_6_8_420.jpg
cs-410_6_8_92,cs-410,6,8, Recommender Systems,"00:07:42,010","00:07:46,460",92,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=462,One popular approach is we use,pic_cs-410_6_8_420.jpg
cs-410_6_8_93,cs-410,6,8, Recommender Systems,"00:07:48,130","00:07:52,380",93,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=468,This would be a sum over,pic_cs-410_6_8_420.jpg
cs-410_6_8_94,cs-410,6,8, Recommender Systems,"00:07:52,380","00:07:56,280",94,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=472,And the formula is a standard,pic_cs-410_6_8_420.jpg
cs-410_6_8_95,cs-410,6,8, Recommender Systems,"00:07:56,280","00:07:58,690",95,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=476,coefficient formula as shown here.,pic_cs-410_6_8_420.jpg
cs-410_6_8_96,cs-410,6,8, Recommender Systems,"00:08:00,060","00:08:05,300",96,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=480,So this basically measures,pic_cs-410_6_8_480.jpg
cs-410_6_8_97,cs-410,6,8, Recommender Systems,"00:08:05,300","00:08:10,229",97,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=485,to all give higher ratings to similar,pic_cs-410_6_8_480.jpg
cs-410_6_8_98,cs-410,6,8, Recommender Systems,"00:08:11,780","00:08:15,990",98,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=491,"Another measure is the cosine measure,",pic_cs-410_6_8_480.jpg
cs-410_6_8_99,cs-410,6,8, Recommender Systems,"00:08:15,990","00:08:20,820",99,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=495,vectors as vectors in the vector space.,pic_cs-410_6_8_480.jpg
cs-410_6_8_100,cs-410,6,8, Recommender Systems,"00:08:20,820","00:08:24,400",100,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=500,"And then,",pic_cs-410_6_8_480.jpg
cs-410_6_8_101,cs-410,6,8, Recommender Systems,"00:08:24,400","00:08:27,880",101,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=504,compute the cosine of,pic_cs-410_6_8_480.jpg
cs-410_6_8_102,cs-410,6,8, Recommender Systems,"00:08:27,880","00:08:32,590",102,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=507,And this measure has been using the vector,pic_cs-410_6_8_480.jpg
cs-410_6_8_103,cs-410,6,8, Recommender Systems,"00:08:32,590","00:08:36,400",103,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=512,So as you can imagine there are just,pic_cs-410_6_8_480.jpg
cs-410_6_8_104,cs-410,6,8, Recommender Systems,"00:08:36,400","00:08:41,330",104,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=516,"In all these cases, note that the user's",pic_cs-410_6_8_480.jpg
cs-410_6_8_105,cs-410,6,8, Recommender Systems,"00:08:41,330","00:08:47,135",105,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=521,on items and we did not actually use,pic_cs-410_6_8_480.jpg
cs-410_6_8_106,cs-410,6,8, Recommender Systems,"00:08:47,135","00:08:51,802",106,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=527,"It didn't matter these items are,",pic_cs-410_6_8_480.jpg
cs-410_6_8_107,cs-410,6,8, Recommender Systems,"00:08:51,802","00:08:55,276",107,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=531,"they can be books, they can be products,",pic_cs-410_6_8_480.jpg
cs-410_6_8_108,cs-410,6,8, Recommender Systems,"00:08:55,276","00:09:00,541",108,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=535,they can be text documents which,pic_cs-410_6_8_480.jpg
cs-410_6_8_109,cs-410,6,8, Recommender Systems,"00:09:00,541","00:09:07,120",109,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=540,so this allows such approach to be,pic_cs-410_6_8_540.jpg
cs-410_6_8_110,cs-410,6,8, Recommender Systems,"00:09:07,120","00:09:08,920",110,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=547,"Now in some newer approaches of course,",pic_cs-410_6_8_540.jpg
cs-410_6_8_111,cs-410,6,8, Recommender Systems,"00:09:08,920","00:09:11,830",111,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=548,we would like to use more,pic_cs-410_6_8_540.jpg
cs-410_6_8_112,cs-410,6,8, Recommender Systems,"00:09:11,830","00:09:18,750",112,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=551,"Clearly, we know more about the user,",pic_cs-410_6_8_540.jpg
cs-410_6_8_113,cs-410,6,8, Recommender Systems,"00:09:18,750","00:09:23,659",113,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=558,"So in the actual filtering system,",pic_cs-410_6_8_540.jpg
cs-410_6_8_114,cs-410,6,8, Recommender Systems,"00:09:23,659","00:09:27,820",114,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=563,we could also combine that,pic_cs-410_6_8_540.jpg
cs-410_6_8_115,cs-410,6,8, Recommender Systems,"00:09:27,820","00:09:34,040",115,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=567,"We could use more context information,",pic_cs-410_6_8_540.jpg
cs-410_6_8_116,cs-410,6,8, Recommender Systems,"00:09:34,040","00:09:39,140",116,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=574,"that people are just starting, and",pic_cs-410_6_8_540.jpg
cs-410_6_8_117,cs-410,6,8, Recommender Systems,"00:09:39,140","00:09:44,147",117,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=579,"But, this memory based approach has",pic_cs-410_6_8_540.jpg
cs-410_6_8_118,cs-410,6,8, Recommender Systems,"00:09:44,147","00:09:48,750",118,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=584,and it's easy to implement in,pic_cs-410_6_8_540.jpg
cs-410_6_8_119,cs-410,6,8, Recommender Systems,"00:09:48,750","00:09:53,698",119,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=588,a starting point to see if the strategy,pic_cs-410_6_8_540.jpg
cs-410_6_8_120,cs-410,6,8, Recommender Systems,"00:09:56,108","00:10:01,305",120,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=596,"So, there are some obvious ways",pic_cs-410_6_8_540.jpg
cs-410_6_8_121,cs-410,6,8, Recommender Systems,"00:10:01,305","00:10:07,070",121,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=601,mainly we would like to improve,pic_cs-410_6_8_600.jpg
cs-410_6_8_122,cs-410,6,8, Recommender Systems,"00:10:07,070","00:10:09,690",122,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=607,And there are some practical,pic_cs-410_6_8_600.jpg
cs-410_6_8_123,cs-410,6,8, Recommender Systems,"00:10:09,690","00:10:11,960",123,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=609,"So for example,",pic_cs-410_6_8_600.jpg
cs-410_6_8_124,cs-410,6,8, Recommender Systems,"00:10:11,960","00:10:12,990",124,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=611,What do you do with them?,pic_cs-410_6_8_600.jpg
cs-410_6_8_125,cs-410,6,8, Recommender Systems,"00:10:12,990","00:10:18,060",125,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=612,"Well, you can set them to default values",pic_cs-410_6_8_600.jpg
cs-410_6_8_126,cs-410,6,8, Recommender Systems,"00:10:18,060","00:10:20,310",126,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=618,And that would be a simple solution.,pic_cs-410_6_8_600.jpg
cs-410_6_8_127,cs-410,6,8, Recommender Systems,"00:10:20,310","00:10:26,388",127,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=620,But there are advanced approaches that,pic_cs-410_6_8_600.jpg
cs-410_6_8_128,cs-410,6,8, Recommender Systems,"00:10:26,388","00:10:32,878",128,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=626,"missing values, and then use predictive",pic_cs-410_6_8_600.jpg
cs-410_6_8_129,cs-410,6,8, Recommender Systems,"00:10:32,878","00:10:38,880",129,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=632,So in fact that the memory based apology,pic_cs-410_6_8_600.jpg
cs-410_6_8_130,cs-410,6,8, Recommender Systems,"00:10:38,880","00:10:43,128",130,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=638,So you get you have iterative approach,pic_cs-410_6_8_600.jpg
cs-410_6_8_131,cs-410,6,8, Recommender Systems,"00:10:43,128","00:10:43,895",131,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=643,prediction and,pic_cs-410_6_8_600.jpg
cs-410_6_8_132,cs-410,6,8, Recommender Systems,"00:10:43,895","00:10:48,095",132,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=643,then you can use the predictive values to,pic_cs-410_6_8_600.jpg
cs-410_6_8_133,cs-410,6,8, Recommender Systems,"00:10:49,525","00:10:54,840",133,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=649,So this is a heuristic,pic_cs-410_6_8_600.jpg
cs-410_6_8_134,cs-410,6,8, Recommender Systems,"00:10:54,840","00:10:59,639",134,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=654,And the strategy obviously would affect,pic_cs-410_6_8_600.jpg
cs-410_6_8_135,cs-410,6,8, Recommender Systems,"00:10:59,639","00:11:04,040",135,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=659,just like any other heuristics would,pic_cs-410_6_8_600.jpg
cs-410_6_8_136,cs-410,6,8, Recommender Systems,"00:11:06,290","00:11:10,460",136,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=666,Another idea which is actually very,pic_cs-410_6_8_660.jpg
cs-410_6_8_137,cs-410,6,8, Recommender Systems,"00:11:10,460","00:11:15,150",137,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=670,have seen in text search is called,pic_cs-410_6_8_660.jpg
cs-410_6_8_138,cs-410,6,8, Recommender Systems,"00:11:15,150","00:11:23,980",138,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=675,Now here the idea is to look at where,pic_cs-410_6_8_660.jpg
cs-410_6_8_139,cs-410,6,8, Recommender Systems,"00:11:23,980","00:11:29,092",139,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=683,If the item is a popular item that,pic_cs-410_6_8_660.jpg
cs-410_6_8_140,cs-410,6,8, Recommender Systems,"00:11:29,092","00:11:35,110",140,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=689,seen [INAUDIBLE] to people interested,pic_cs-410_6_8_660.jpg
cs-410_6_8_141,cs-410,6,8, Recommender Systems,"00:11:35,110","00:11:40,620",141,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=695,"interesting but if it's a rare item,",pic_cs-410_6_8_660.jpg
cs-410_6_8_142,cs-410,6,8, Recommender Systems,"00:11:40,620","00:11:44,770",142,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=700,But these two users deal with this,pic_cs-410_6_8_660.jpg
cs-410_6_8_143,cs-410,6,8, Recommender Systems,"00:11:44,770","00:11:47,370",143,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=704,"And, that says more",pic_cs-410_6_8_660.jpg
cs-410_6_8_144,cs-410,6,8, Recommender Systems,"00:11:47,370","00:11:52,177",144,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=707,It's kind of to emphasize,pic_cs-410_6_8_660.jpg
cs-410_6_8_145,cs-410,6,8, Recommender Systems,"00:11:52,177","00:11:56,738",145,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=712,on items that are not,pic_cs-410_6_8_660.jpg
cs-410_6_8_146,cs-410,6,8, Recommender Systems,"00:11:56,738","00:12:06,738",146,https://www.coursera.org/learn/cs-410/lecture/awVwS?t=716,[MUSIC],pic_cs-410_6_8_660.jpg
cs-410_6_9_1,cs-410,6,9, Recommender Systems,"00:00:00,012","00:00:07,878",1,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=0,[SOUND],pic_cs-410_6_9_0.jpg
cs-410_6_9_2,cs-410,6,9, Recommender Systems,"00:00:07,878","00:00:12,848",2,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=7,to summarize our discussion of,pic_cs-410_6_9_0.jpg
cs-410_6_9_3,cs-410,6,9, Recommender Systems,"00:00:12,848","00:00:16,640",3,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=12,the filtering task for,pic_cs-410_6_9_0.jpg
cs-410_6_9_4,cs-410,6,9, Recommender Systems,"00:00:16,640","00:00:21,020",4,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=16,"in some other sense,",pic_cs-410_6_9_0.jpg
cs-410_6_9_5,cs-410,6,9, Recommender Systems,"00:00:21,020","00:00:24,230",5,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=21,So it's easy because,pic_cs-410_6_9_0.jpg
cs-410_6_9_6,cs-410,6,9, Recommender Systems,"00:00:24,230","00:00:30,300",6,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=24,In this case the system takes initiative,pic_cs-410_6_9_0.jpg
cs-410_6_9_7,cs-410,6,9, Recommender Systems,"00:00:30,300","00:00:33,230",7,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=30,"The user doesn't really make any effort,",pic_cs-410_6_9_0.jpg
cs-410_6_9_8,cs-410,6,9, Recommender Systems,"00:00:33,230","00:00:36,100",8,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=33,any recommendation is better than nothing.,pic_cs-410_6_9_0.jpg
cs-410_6_9_9,cs-410,6,9, Recommender Systems,"00:00:36,100","00:00:41,710",9,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=36,All right.,pic_cs-410_6_9_0.jpg
cs-410_6_9_10,cs-410,6,9, Recommender Systems,"00:00:41,710","00:00:44,180",10,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=41,items or useless documents.,pic_cs-410_6_9_0.jpg
cs-410_6_9_11,cs-410,6,9, Recommender Systems,"00:00:44,180","00:00:47,220",11,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=44,If you can recommend,pic_cs-410_6_9_0.jpg
cs-410_6_9_12,cs-410,6,9, Recommender Systems,"00:00:47,220","00:00:52,390",12,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=47,"users generally will appreciate it,",pic_cs-410_6_9_0.jpg
cs-410_6_9_13,cs-410,6,9, Recommender Systems,"00:00:52,390","00:00:56,810",13,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=52,"However, filtering is actually much harder",pic_cs-410_6_9_0.jpg
cs-410_6_9_14,cs-410,6,9, Recommender Systems,"00:00:56,810","00:01:01,860",14,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=56,make a binary decision and you can't,pic_cs-410_6_9_0.jpg
cs-410_6_9_15,cs-410,6,9, Recommender Systems,"00:01:01,860","00:01:06,520",15,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=61,then you're going to see whether,pic_cs-410_6_9_60.jpg
cs-410_6_9_16,cs-410,6,9, Recommender Systems,"00:01:06,520","00:01:10,040",16,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=66,You have to make a decision,pic_cs-410_6_9_60.jpg
cs-410_6_9_17,cs-410,6,9, Recommender Systems,"00:01:10,040","00:01:11,260",17,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=70,Think about news filtering.,pic_cs-410_6_9_60.jpg
cs-410_6_9_18,cs-410,6,9, Recommender Systems,"00:01:11,260","00:01:15,060",18,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=71,As soon as you see the news enough,pic_cs-410_6_9_60.jpg
cs-410_6_9_19,cs-410,6,9, Recommender Systems,"00:01:15,060","00:01:16,780",19,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=75,interesting to the user.,pic_cs-410_6_9_60.jpg
cs-410_6_9_20,cs-410,6,9, Recommender Systems,"00:01:16,780","00:01:21,190",20,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=76,"If you wait for a few days, well, even if",pic_cs-410_6_9_60.jpg
cs-410_6_9_21,cs-410,6,9, Recommender Systems,"00:01:21,190","00:01:26,690",21,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=81,"the most relevant news, the utility is",pic_cs-410_6_9_60.jpg
cs-410_6_9_22,cs-410,6,9, Recommender Systems,"00:01:28,160","00:01:32,140",22,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=88,Another reason why it's hard,pic_cs-410_6_9_60.jpg
cs-410_6_9_23,cs-410,6,9, Recommender Systems,"00:01:32,140","00:01:34,620",23,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=92,if you think of this,pic_cs-410_6_9_60.jpg
cs-410_6_9_24,cs-410,6,9, Recommender Systems,"00:01:34,620","00:01:36,010",24,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=94,"Collaborative filtering, for",pic_cs-410_6_9_60.jpg
cs-410_6_9_25,cs-410,6,9, Recommender Systems,"00:01:36,010","00:01:41,030",25,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=96,"example, is purely based on",pic_cs-410_6_9_60.jpg
cs-410_6_9_26,cs-410,6,9, Recommender Systems,"00:01:41,030","00:01:48,120",26,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=101,So if you don't have many ratings there's,pic_cs-410_6_9_60.jpg
cs-410_6_9_27,cs-410,6,9, Recommender Systems,"00:01:48,120","00:01:51,470",27,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=108,And yeah I just mentioned,pic_cs-410_6_9_60.jpg
cs-410_6_9_28,cs-410,6,9, Recommender Systems,"00:01:51,470","00:01:54,450",28,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=111,"This is actually a very serious,",pic_cs-410_6_9_60.jpg
cs-410_6_9_29,cs-410,6,9, Recommender Systems,"00:01:54,450","00:01:59,180",29,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=114,But of course there are strategies that,pic_cs-410_6_9_60.jpg
cs-410_6_9_30,cs-410,6,9, Recommender Systems,"00:02:00,680","00:02:04,930",30,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=120,and there are different strategies that,pic_cs-410_6_9_120.jpg
cs-410_6_9_31,cs-410,6,9, Recommender Systems,"00:02:04,930","00:02:09,620",31,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=124,"You can use, for example, more user",pic_cs-410_6_9_120.jpg
cs-410_6_9_32,cs-410,6,9, Recommender Systems,"00:02:09,620","00:02:14,470",32,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=129,instead of using the preferences,pic_cs-410_6_9_120.jpg
cs-410_6_9_33,cs-410,6,9, Recommender Systems,"00:02:14,470","00:02:19,000",33,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=134,items give me additional information,pic_cs-410_6_9_120.jpg
cs-410_6_9_34,cs-410,6,9, Recommender Systems,"00:02:21,110","00:02:26,840",34,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=141,And we also talk about two strategies for,pic_cs-410_6_9_120.jpg
cs-410_6_9_35,cs-410,6,9, Recommender Systems,"00:02:26,840","00:02:30,140",35,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=146,One is content-based where,pic_cs-410_6_9_120.jpg
cs-410_6_9_36,cs-410,6,9, Recommender Systems,"00:02:30,140","00:02:34,670",36,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=150,is collaborative filtering where,pic_cs-410_6_9_120.jpg
cs-410_6_9_37,cs-410,6,9, Recommender Systems,"00:02:34,670","00:02:37,990",37,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=154,And they obviously can be,pic_cs-410_6_9_120.jpg
cs-410_6_9_38,cs-410,6,9, Recommender Systems,"00:02:37,990","00:02:41,480",38,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=157,You can imagine they generally,pic_cs-410_6_9_120.jpg
cs-410_6_9_39,cs-410,6,9, Recommender Systems,"00:02:41,480","00:02:46,166",39,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=161,So that would give us a hybrid,pic_cs-410_6_9_120.jpg
cs-410_6_9_40,cs-410,6,9, Recommender Systems,"00:02:46,166","00:02:52,620",40,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=166,And we also could recall that we talked,pic_cs-410_6_9_120.jpg
cs-410_6_9_41,cs-410,6,9, Recommender Systems,"00:02:52,620","00:02:58,470",41,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=172,about push versus pull as two strategies,pic_cs-410_6_9_120.jpg
cs-410_6_9_42,cs-410,6,9, Recommender Systems,"00:02:58,470","00:03:03,110",42,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=178,And recommender system easy to,pic_cs-410_6_9_120.jpg
cs-410_6_9_43,cs-410,6,9, Recommender Systems,"00:03:03,110","00:03:06,650",43,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=183,search engines are serving,pic_cs-410_6_9_180.jpg
cs-410_6_9_44,cs-410,6,9, Recommender Systems,"00:03:06,650","00:03:09,740",44,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=186,"Obviously the two should be combined,",pic_cs-410_6_9_180.jpg
cs-410_6_9_45,cs-410,6,9, Recommender Systems,"00:03:09,740","00:03:13,400",45,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=189,The two have a system,pic_cs-410_6_9_180.jpg
cs-410_6_9_46,cs-410,6,9, Recommender Systems,"00:03:13,400","00:03:16,600",46,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=193,with multiple mode information access.,pic_cs-410_6_9_180.jpg
cs-410_6_9_47,cs-410,6,9, Recommender Systems,"00:03:16,600","00:03:22,870",47,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=196,So in the future we could anticipate such,pic_cs-410_6_9_180.jpg
cs-410_6_9_48,cs-410,6,9, Recommender Systems,"00:03:22,870","00:03:27,570",48,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=202,"And either,",pic_cs-410_6_9_180.jpg
cs-410_6_9_49,cs-410,6,9, Recommender Systems,"00:03:27,570","00:03:33,740",49,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=207,there are a lot of new algorithms,pic_cs-410_6_9_180.jpg
cs-410_6_9_50,cs-410,6,9, Recommender Systems,"00:03:33,740","00:03:39,070",50,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=213,In particular those new algorithms tend,pic_cs-410_6_9_180.jpg
cs-410_6_9_51,cs-410,6,9, Recommender Systems,"00:03:39,070","00:03:42,850",51,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=219,Now the context here could be,pic_cs-410_6_9_180.jpg
cs-410_6_9_52,cs-410,6,9, Recommender Systems,"00:03:42,850","00:03:44,920",52,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=222,could also be the context of the user.,pic_cs-410_6_9_180.jpg
cs-410_6_9_53,cs-410,6,9, Recommender Systems,"00:03:44,920","00:03:45,750",53,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=224,Items.,pic_cs-410_6_9_180.jpg
cs-410_6_9_54,cs-410,6,9, Recommender Systems,"00:03:45,750","00:03:47,570",54,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=225,The items are not the isolated.,pic_cs-410_6_9_180.jpg
cs-410_6_9_55,cs-410,6,9, Recommender Systems,"00:03:47,570","00:03:50,290",55,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=227,They're connected in many ways.,pic_cs-410_6_9_180.jpg
cs-410_6_9_56,cs-410,6,9, Recommender Systems,"00:03:50,290","00:03:54,590",56,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=230,The users might form,pic_cs-410_6_9_180.jpg
cs-410_6_9_57,cs-410,6,9, Recommender Systems,"00:03:54,590","00:03:58,980",57,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=234,so there's a rich context there,pic_cs-410_6_9_180.jpg
cs-410_6_9_58,cs-410,6,9, Recommender Systems,"00:03:59,980","00:04:04,100",58,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=239,really solve the problem well and,pic_cs-410_6_9_180.jpg
cs-410_6_9_59,cs-410,6,9, Recommender Systems,"00:04:04,100","00:04:09,650",59,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=244,research area where also machine,pic_cs-410_6_9_240.jpg
cs-410_6_9_60,cs-410,6,9, Recommender Systems,"00:04:09,650","00:04:13,624",60,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=249,Here are some additional readings in,pic_cs-410_6_9_240.jpg
cs-410_6_9_61,cs-410,6,9, Recommender Systems,"00:04:13,624","00:04:18,494",61,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=253,the handbook called,pic_cs-410_6_9_240.jpg
cs-410_6_9_62,cs-410,6,9, Recommender Systems,"00:04:18,494","00:04:23,364",62,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=258,has a collection of a lot,pic_cs-410_6_9_240.jpg
cs-410_6_9_63,cs-410,6,9, Recommender Systems,"00:04:23,364","00:04:28,362",63,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=263,can give you an overview,pic_cs-410_6_9_240.jpg
cs-410_6_9_64,cs-410,6,9, Recommender Systems,"00:04:28,362","00:04:33,122",64,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=268,approaches through recommender systems.,pic_cs-410_6_9_240.jpg
cs-410_6_9_65,cs-410,6,9, Recommender Systems,"00:04:33,122","00:04:43,122",65,https://www.coursera.org/learn/cs-410/lecture/tfXZ4?t=273,[MUSIC],pic_cs-410_6_9_240.jpg
cs-410_6_10_1,cs-410,6,10, Summary for Exam 1,"00:00:00,012","00:00:03,145",1,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=0,[NOISE],pic_cs-410_6_10_0.jpg
cs-410_6_10_2,cs-410,6,10, Summary for Exam 1,"00:00:06,848","00:00:10,210",2,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=6,This lecture is a summary of this course.,pic_cs-410_6_10_0.jpg
cs-410_6_10_3,cs-410,6,10, Summary for Exam 1,"00:00:12,890","00:00:17,110",3,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=12,This map shows the major topics,pic_cs-410_6_10_0.jpg
cs-410_6_10_4,cs-410,6,10, Summary for Exam 1,"00:00:19,170","00:00:24,230",4,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=19,And here are some key,pic_cs-410_6_10_0.jpg
cs-410_6_10_5,cs-410,6,10, Summary for Exam 1,"00:00:24,230","00:00:28,020",5,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=24,"First, we talked about natural",pic_cs-410_6_10_0.jpg
cs-410_6_10_6,cs-410,6,10, Summary for Exam 1,"00:00:29,170","00:00:33,120",6,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=29,Here the main take-away messages,pic_cs-410_6_10_0.jpg
cs-410_6_10_7,cs-410,6,10, Summary for Exam 1,"00:00:33,120","00:00:39,210",7,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=33,"a foundation for text retrieval, but",pic_cs-410_6_10_0.jpg
cs-410_6_10_8,cs-410,6,10, Summary for Exam 1,"00:00:39,210","00:00:48,540",8,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=39,the battle of wars is generally the main,pic_cs-410_6_10_0.jpg
cs-410_6_10_9,cs-410,6,10, Summary for Exam 1,"00:00:48,540","00:00:52,730",9,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=48,And it's often sufficient before,pic_cs-410_6_10_0.jpg
cs-410_6_10_10,cs-410,6,10, Summary for Exam 1,"00:00:52,730","00:00:58,290",10,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=52,obviously for,pic_cs-410_6_10_0.jpg
cs-410_6_10_11,cs-410,6,10, Summary for Exam 1,"00:00:58,290","00:01:02,640",11,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=58,a deeper natural language,pic_cs-410_6_10_0.jpg
cs-410_6_10_12,cs-410,6,10, Summary for Exam 1,"00:01:02,640","00:01:05,070",12,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=62,We then talked about the high,pic_cs-410_6_10_60.jpg
cs-410_6_10_13,cs-410,6,10, Summary for Exam 1,"00:01:05,070","00:01:09,170",13,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=65,text access and,pic_cs-410_6_10_60.jpg
cs-410_6_10_14,cs-410,6,10, Summary for Exam 1,"00:01:09,170","00:01:12,200",14,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=69,In pull we talked about,pic_cs-410_6_10_60.jpg
cs-410_6_10_15,cs-410,6,10, Summary for Exam 1,"00:01:13,250","00:01:17,800",15,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=73,"Now in general in future search engines,",pic_cs-410_6_10_60.jpg
cs-410_6_10_16,cs-410,6,10, Summary for Exam 1,"00:01:17,800","00:01:20,466",16,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=77,to provide a math involved,pic_cs-410_6_10_60.jpg
cs-410_6_10_17,cs-410,6,10, Summary for Exam 1,"00:01:23,022","00:01:27,680",17,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=83,And now we'll talk about a number of,pic_cs-410_6_10_60.jpg
cs-410_6_10_18,cs-410,6,10, Summary for Exam 1,"00:01:27,680","00:01:30,350",18,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=87,We talked about the search problem.,pic_cs-410_6_10_60.jpg
cs-410_6_10_19,cs-410,6,10, Summary for Exam 1,"00:01:30,350","00:01:32,280",19,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=90,And we framed that as a ranking problem.,pic_cs-410_6_10_60.jpg
cs-410_6_10_20,cs-410,6,10, Summary for Exam 1,"00:01:34,830","00:01:38,680",20,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=94,And we talked about a number,pic_cs-410_6_10_60.jpg
cs-410_6_10_21,cs-410,6,10, Summary for Exam 1,"00:01:38,680","00:01:42,170",21,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=98,We start with the overview,pic_cs-410_6_10_60.jpg
cs-410_6_10_22,cs-410,6,10, Summary for Exam 1,"00:01:42,170","00:01:46,710",22,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=102,the probabilistic model and then we talked,pic_cs-410_6_10_60.jpg
cs-410_6_10_23,cs-410,6,10, Summary for Exam 1,"00:01:48,400","00:01:53,730",23,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=108,We also later talked about,pic_cs-410_6_10_60.jpg
cs-410_6_10_24,cs-410,6,10, Summary for Exam 1,"00:01:53,730","00:01:56,280",24,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=113,that's probabilistic model.,pic_cs-410_6_10_60.jpg
cs-410_6_10_25,cs-410,6,10, Summary for Exam 1,"00:01:56,280","00:02:01,910",25,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=116,"And here, many take-away message is that",pic_cs-410_6_10_60.jpg
cs-410_6_10_26,cs-410,6,10, Summary for Exam 1,"00:02:01,910","00:02:07,510",26,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=121,"look similar, and",pic_cs-410_6_10_120.jpg
cs-410_6_10_27,cs-410,6,10, Summary for Exam 1,"00:02:07,510","00:02:13,730",27,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=127,"Most important ones are TF-IDF weighting,",pic_cs-410_6_10_120.jpg
cs-410_6_10_28,cs-410,6,10, Summary for Exam 1,"00:02:13,730","00:02:20,460",28,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=133,And the TF is often transformed through,pic_cs-410_6_10_120.jpg
cs-410_6_10_29,cs-410,6,10, Summary for Exam 1,"00:02:22,070","00:02:27,730",29,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=142,And then we talked about how to,pic_cs-410_6_10_120.jpg
cs-410_6_10_30,cs-410,6,10, Summary for Exam 1,"00:02:27,730","00:02:33,940",30,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=147,"the main techniques that we talked about,",pic_cs-410_6_10_120.jpg
cs-410_6_10_31,cs-410,6,10, Summary for Exam 1,"00:02:33,940","00:02:39,590",31,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=153,that we can prepare the system,pic_cs-410_6_10_120.jpg
cs-410_6_10_32,cs-410,6,10, Summary for Exam 1,"00:02:39,590","00:02:45,100",32,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=159,And we talked about how to do a faster,pic_cs-410_6_10_120.jpg
cs-410_6_10_33,cs-410,6,10, Summary for Exam 1,"00:02:46,180","00:02:50,800",33,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=166,And we then talked about how to,pic_cs-410_6_10_120.jpg
cs-410_6_10_34,cs-410,6,10, Summary for Exam 1,"00:02:50,800","00:02:54,860",34,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=170,mainly introduced to,pic_cs-410_6_10_120.jpg
cs-410_6_10_35,cs-410,6,10, Summary for Exam 1,"00:02:54,860","00:02:58,770",35,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=174,This was a very important,pic_cs-410_6_10_120.jpg
cs-410_6_10_36,cs-410,6,10, Summary for Exam 1,"00:02:58,770","00:03:00,490",36,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=178,applied to many tasks.,pic_cs-410_6_10_120.jpg
cs-410_6_10_37,cs-410,6,10, Summary for Exam 1,"00:03:01,980","00:03:05,450",37,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=181,We talked about the major,pic_cs-410_6_10_180.jpg
cs-410_6_10_38,cs-410,6,10, Summary for Exam 1,"00:03:05,450","00:03:10,800",38,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=185,"So, the most important measures for",pic_cs-410_6_10_180.jpg
cs-410_6_10_39,cs-410,6,10, Summary for Exam 1,"00:03:10,800","00:03:16,400",39,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=190,"are MAP, mean average precision,",pic_cs-410_6_10_180.jpg
cs-410_6_10_40,cs-410,6,10, Summary for Exam 1,"00:03:16,400","00:03:20,880",40,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=196,accumulative gain and also precision and,pic_cs-410_6_10_180.jpg
cs-410_6_10_41,cs-410,6,10, Summary for Exam 1,"00:03:22,580","00:03:25,540",41,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=202,And we then talked about,pic_cs-410_6_10_180.jpg
cs-410_6_10_42,cs-410,6,10, Summary for Exam 1,"00:03:25,540","00:03:29,180",42,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=205,And we talked about the Rocchio,pic_cs-410_6_10_180.jpg
cs-410_6_10_43,cs-410,6,10, Summary for Exam 1,"00:03:29,180","00:03:32,200",43,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=209,the mixture model and,pic_cs-410_6_10_180.jpg
cs-410_6_10_44,cs-410,6,10, Summary for Exam 1,"00:03:32,200","00:03:36,630",44,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=212,Feedback is a very important,pic_cs-410_6_10_180.jpg
cs-410_6_10_45,cs-410,6,10, Summary for Exam 1,"00:03:36,630","00:03:41,430",45,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=216,the opportunity of learning from,pic_cs-410_6_10_180.jpg
cs-410_6_10_46,cs-410,6,10, Summary for Exam 1,"00:03:42,960","00:03:45,800",46,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=222,We then talked about Web search.,pic_cs-410_6_10_180.jpg
cs-410_6_10_47,cs-410,6,10, Summary for Exam 1,"00:03:45,800","00:03:50,150",47,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=225,And here we talked about how,pic_cs-410_6_10_180.jpg
cs-410_6_10_48,cs-410,6,10, Summary for Exam 1,"00:03:50,150","00:03:55,330",48,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=230,to solve the scalability issue in that,pic_cs-410_6_10_180.jpg
cs-410_6_10_49,cs-410,6,10, Summary for Exam 1,"00:03:55,330","00:03:59,130",49,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=235,Then we talked about how to use linking,pic_cs-410_6_10_180.jpg
cs-410_6_10_50,cs-410,6,10, Summary for Exam 1,"00:03:59,130","00:04:01,490",50,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=239,We talked about page rank and,pic_cs-410_6_10_180.jpg
cs-410_6_10_51,cs-410,6,10, Summary for Exam 1,"00:04:01,490","00:04:06,010",51,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=241,hits as the major hours is to,pic_cs-410_6_10_240.jpg
cs-410_6_10_52,cs-410,6,10, Summary for Exam 1,"00:04:07,320","00:04:09,562",52,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=247,We then talked about,pic_cs-410_6_10_240.jpg
cs-410_6_10_53,cs-410,6,10, Summary for Exam 1,"00:04:09,562","00:04:14,810",53,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=249,This is the use of machine learning,pic_cs-410_6_10_240.jpg
cs-410_6_10_54,cs-410,6,10, Summary for Exam 1,"00:04:14,810","00:04:16,640",54,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=254,improvement scoring.,pic_cs-410_6_10_240.jpg
cs-410_6_10_55,cs-410,6,10, Summary for Exam 1,"00:04:16,640","00:04:21,460",55,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=256,Not only that the effectiveness can be,pic_cs-410_6_10_240.jpg
cs-410_6_10_56,cs-410,6,10, Summary for Exam 1,"00:04:21,460","00:04:23,620",56,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=261,we can also improve the robustness of the.,pic_cs-410_6_10_240.jpg
cs-410_6_10_57,cs-410,6,10, Summary for Exam 1,"00:04:23,620","00:04:28,560",57,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=263,The ranking function so that it's,pic_cs-410_6_10_240.jpg
cs-410_6_10_58,cs-410,6,10, Summary for Exam 1,"00:04:28,560","00:04:34,540",58,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=268,It just some features to promote the page.,pic_cs-410_6_10_240.jpg
cs-410_6_10_59,cs-410,6,10, Summary for Exam 1,"00:04:36,270","00:04:39,279",59,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=276,And finally we talked about,pic_cs-410_6_10_240.jpg
cs-410_6_10_60,cs-410,6,10, Summary for Exam 1,"00:04:40,460","00:04:45,730",60,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=280,About the some major reactions,pic_cs-410_6_10_240.jpg
cs-410_6_10_61,cs-410,6,10, Summary for Exam 1,"00:04:45,730","00:04:49,390",61,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=285,in the future in improving the count,pic_cs-410_6_10_240.jpg
cs-410_6_10_62,cs-410,6,10, Summary for Exam 1,"00:04:50,610","00:04:54,030",62,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=290,And then finally we talked about,pic_cs-410_6_10_240.jpg
cs-410_6_10_63,cs-410,6,10, Summary for Exam 1,"00:04:54,030","00:04:57,890",63,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=294,these are systems to,pic_cs-410_6_10_240.jpg
cs-410_6_10_64,cs-410,6,10, Summary for Exam 1,"00:04:57,890","00:05:02,120",64,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=297,"And we'll talk about the two approaches,",pic_cs-410_6_10_240.jpg
cs-410_6_10_65,cs-410,6,10, Summary for Exam 1,"00:05:02,120","00:05:06,240",65,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=302,one is collaborative filtering and,pic_cs-410_6_10_300.jpg
cs-410_6_10_66,cs-410,6,10, Summary for Exam 1,"00:05:07,330","00:05:11,930",66,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=307,"Now, an obvious missing piece",pic_cs-410_6_10_300.jpg
cs-410_6_10_67,cs-410,6,10, Summary for Exam 1,"00:05:11,930","00:05:16,884",67,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=311,"in this picture is the user,",pic_cs-410_6_10_300.jpg
cs-410_6_10_68,cs-410,6,10, Summary for Exam 1,"00:05:16,884","00:05:21,620",68,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=316,so user interface is also an important,pic_cs-410_6_10_300.jpg
cs-410_6_10_69,cs-410,6,10, Summary for Exam 1,"00:05:21,620","00:05:25,850",69,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=321,Even though the current search interface,pic_cs-410_6_10_300.jpg
cs-410_6_10_70,cs-410,6,10, Summary for Exam 1,"00:05:25,850","00:05:32,020",70,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=325,done a lot of studies of user interfaces,pic_cs-410_6_10_300.jpg
cs-410_6_10_71,cs-410,6,10, Summary for Exam 1,"00:05:32,020","00:05:34,680",71,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=332,"And this is the topic to that,",pic_cs-410_6_10_300.jpg
cs-410_6_10_72,cs-410,6,10, Summary for Exam 1,"00:05:34,680","00:05:40,350",72,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=334,you can learn more by reading this book.,pic_cs-410_6_10_300.jpg
cs-410_6_10_73,cs-410,6,10, Summary for Exam 1,"00:05:40,350","00:05:47,430",73,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=340,It's an excellent book about all kinds,pic_cs-410_6_10_300.jpg
cs-410_6_10_74,cs-410,6,10, Summary for Exam 1,"00:05:48,800","00:05:53,360",74,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=348,If you want to know more about,pic_cs-410_6_10_300.jpg
cs-410_6_10_75,cs-410,6,10, Summary for Exam 1,"00:05:53,360","00:05:57,590",75,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=353,you can also read some additional,pic_cs-410_6_10_300.jpg
cs-410_6_10_76,cs-410,6,10, Summary for Exam 1,"00:05:57,590","00:06:01,110",76,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=357,In this short course we only,pic_cs-410_6_10_300.jpg
cs-410_6_10_77,cs-410,6,10, Summary for Exam 1,"00:06:01,110","00:06:03,610",77,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=361,topics in text retrievals and,pic_cs-410_6_10_360.jpg
cs-410_6_10_78,cs-410,6,10, Summary for Exam 1,"00:06:04,770","00:06:09,930",78,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=364,And these resources provide additional,pic_cs-410_6_10_360.jpg
cs-410_6_10_79,cs-410,6,10, Summary for Exam 1,"00:06:09,930","00:06:16,220",79,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=369,they give a more thorough treatment of,pic_cs-410_6_10_360.jpg
cs-410_6_10_80,cs-410,6,10, Summary for Exam 1,"00:06:16,220","00:06:19,410",80,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=376,And a main source is,pic_cs-410_6_10_360.jpg
cs-410_6_10_81,cs-410,6,10, Summary for Exam 1,"00:06:21,570","00:06:26,916",81,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=381,that you can see a lot of short,pic_cs-410_6_10_360.jpg
cs-410_6_10_82,cs-410,6,10, Summary for Exam 1,"00:06:26,916","00:06:30,290",82,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=386,or long tutorials.,pic_cs-410_6_10_360.jpg
cs-410_6_10_83,cs-410,6,10, Summary for Exam 1,"00:06:30,290","00:06:35,260",83,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=390,They tend to provide a lot of,pic_cs-410_6_10_360.jpg
cs-410_6_10_84,cs-410,6,10, Summary for Exam 1,"00:06:35,260","00:06:40,830",84,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=395,And there a lot of series that,pic_cs-410_6_10_360.jpg
cs-410_6_10_85,cs-410,6,10, Summary for Exam 1,"00:06:40,830","00:06:44,660",85,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=400,"One is information concepts,",pic_cs-410_6_10_360.jpg
cs-410_6_10_86,cs-410,6,10, Summary for Exam 1,"00:06:44,660","00:06:46,310",86,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=404,One is human langauge technology.,pic_cs-410_6_10_360.jpg
cs-410_6_10_87,cs-410,6,10, Summary for Exam 1,"00:06:46,310","00:06:49,452",87,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=406,And yet another is artificial,pic_cs-410_6_10_360.jpg
cs-410_6_10_88,cs-410,6,10, Summary for Exam 1,"00:06:49,452","00:06:55,535",88,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=409,There are also some major journals and,pic_cs-410_6_10_360.jpg
cs-410_6_10_89,cs-410,6,10, Summary for Exam 1,"00:06:55,535","00:07:00,485",89,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=415,tend to have a lot of research papers,pic_cs-410_6_10_360.jpg
cs-410_6_10_90,cs-410,6,10, Summary for Exam 1,"00:07:00,485","00:07:05,370",90,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=420,"And finally, for more information",pic_cs-410_6_10_420.jpg
cs-410_6_10_91,cs-410,6,10, Summary for Exam 1,"00:07:05,370","00:07:08,930",91,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=425,"tool kits, etc you can check out his URL.",pic_cs-410_6_10_420.jpg
cs-410_6_10_92,cs-410,6,10, Summary for Exam 1,"00:07:10,010","00:07:16,320",92,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=430,"So, if you have not taken the text",pic_cs-410_6_10_420.jpg
cs-410_6_10_93,cs-410,6,10, Summary for Exam 1,"00:07:16,320","00:07:22,630",93,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=436,specialization series then naturally,pic_cs-410_6_10_420.jpg
cs-410_6_10_94,cs-410,6,10, Summary for Exam 1,"00:07:22,630","00:07:27,900",94,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=442,"As this picture shows,",pic_cs-410_6_10_420.jpg
cs-410_6_10_95,cs-410,6,10, Summary for Exam 1,"00:07:27,900","00:07:31,800",95,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=447,we generally need two kinds of techniques.,pic_cs-410_6_10_420.jpg
cs-410_6_10_96,cs-410,6,10, Summary for Exam 1,"00:07:31,800","00:07:34,710",96,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=451,"One is text retrieval,",pic_cs-410_6_10_420.jpg
cs-410_6_10_97,cs-410,6,10, Summary for Exam 1,"00:07:34,710","00:07:39,490",97,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=454,And these techniques will help us,pic_cs-410_6_10_420.jpg
cs-410_6_10_98,cs-410,6,10, Summary for Exam 1,"00:07:39,490","00:07:45,550",98,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=459,"relevant text data, which are actually",pic_cs-410_6_10_420.jpg
cs-410_6_10_99,cs-410,6,10, Summary for Exam 1,"00:07:45,550","00:07:51,190",99,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=465,Now human plays important role in mining,pic_cs-410_6_10_420.jpg
cs-410_6_10_100,cs-410,6,10, Summary for Exam 1,"00:07:51,190","00:07:54,630",100,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=471,written for humans to consume.,pic_cs-410_6_10_420.jpg
cs-410_6_10_101,cs-410,6,10, Summary for Exam 1,"00:07:54,630","00:08:00,580",101,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=474,So involving humans in the process,pic_cs-410_6_10_420.jpg
cs-410_6_10_102,cs-410,6,10, Summary for Exam 1,"00:08:00,580","00:08:05,050",102,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=480,in this course we have covered,pic_cs-410_6_10_480.jpg
cs-410_6_10_103,cs-410,6,10, Summary for Exam 1,"00:08:05,050","00:08:08,300",103,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=485,access to the most relevant data.,pic_cs-410_6_10_480.jpg
cs-410_6_10_104,cs-410,6,10, Summary for Exam 1,"00:08:08,300","00:08:13,210",104,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=488,These techniques are always so,pic_cs-410_6_10_480.jpg
cs-410_6_10_105,cs-410,6,10, Summary for Exam 1,"00:08:13,210","00:08:17,770",105,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=493,to help provide prominence and,pic_cs-410_6_10_480.jpg
cs-410_6_10_106,cs-410,6,10, Summary for Exam 1,"00:08:17,770","00:08:23,990",106,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=497,patterns that the user will,pic_cs-410_6_10_480.jpg
cs-410_6_10_107,cs-410,6,10, Summary for Exam 1,"00:08:23,990","00:08:27,870",107,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=503,"So, in general, the user would have",pic_cs-410_6_10_480.jpg
cs-410_6_10_108,cs-410,6,10, Summary for Exam 1,"00:08:27,870","00:08:29,359",108,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=507,better understand the patterns.,pic_cs-410_6_10_480.jpg
cs-410_6_10_109,cs-410,6,10, Summary for Exam 1,"00:08:30,360","00:08:36,200",109,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=510,"So the text mining cause, or rather,",pic_cs-410_6_10_480.jpg
cs-410_6_10_110,cs-410,6,10, Summary for Exam 1,"00:08:36,200","00:08:41,660",110,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=516,will be dealing with what to do once,pic_cs-410_6_10_480.jpg
cs-410_6_10_111,cs-410,6,10, Summary for Exam 1,"00:08:41,660","00:08:46,010",111,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=521,So this is a second step in this,pic_cs-410_6_10_480.jpg
cs-410_6_10_112,cs-410,6,10, Summary for Exam 1,"00:08:46,010","00:08:48,790",112,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=526,the text data into actionable knowledge.,pic_cs-410_6_10_480.jpg
cs-410_6_10_113,cs-410,6,10, Summary for Exam 1,"00:08:49,830","00:08:55,900",113,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=529,And this has to do with helping users to,pic_cs-410_6_10_480.jpg
cs-410_6_10_114,cs-410,6,10, Summary for Exam 1,"00:08:55,900","00:08:59,750",114,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=535,to find the patterns and,pic_cs-410_6_10_480.jpg
cs-410_6_10_115,cs-410,6,10, Summary for Exam 1,"00:08:59,750","00:09:04,640",115,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=539,In text and such knowledge can,pic_cs-410_6_10_480.jpg
cs-410_6_10_116,cs-410,6,10, Summary for Exam 1,"00:09:04,640","00:09:10,500",116,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=544,systems to help decision making or,pic_cs-410_6_10_540.jpg
cs-410_6_10_117,cs-410,6,10, Summary for Exam 1,"00:09:10,500","00:09:16,624",117,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=550,"So, if you have not taken that course,",pic_cs-410_6_10_540.jpg
cs-410_6_10_118,cs-410,6,10, Summary for Exam 1,"00:09:16,624","00:09:22,030",118,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=556,that natural next step would,pic_cs-410_6_10_540.jpg
cs-410_6_10_119,cs-410,6,10, Summary for Exam 1,"00:09:24,000","00:09:25,770",119,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=564,Thank you for taking this course.,pic_cs-410_6_10_540.jpg
cs-410_6_10_120,cs-410,6,10, Summary for Exam 1,"00:09:25,770","00:09:29,570",120,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=565,I hope you had fun and,pic_cs-410_6_10_540.jpg
cs-410_6_10_121,cs-410,6,10, Summary for Exam 1,"00:09:29,570","00:09:34,236",121,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=569,And I look forward to interacting,pic_cs-410_6_10_540.jpg
cs-410_6_10_122,cs-410,6,10, Summary for Exam 1,"00:09:34,236","00:09:44,236",122,https://www.coursera.org/learn/cs-410/lecture/9CAed?t=574,[MUSIC],pic_cs-410_6_10_540.jpg
cs-410_7_1_1,cs-410,7,1,Overview,"00:00:00,012","00:00:06,665",1,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=0,[SOUND],pic_cs-410_7_1_0.jpg
cs-410_7_1_2,cs-410,7,1,Overview,"00:00:06,665","00:00:11,700",2,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=6,this lecture we give an overview,pic_cs-410_7_1_0.jpg
cs-410_7_1_3,cs-410,7,1,Overview,"00:00:13,743","00:00:19,830",3,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=13,"First, let's define the term text mining,",pic_cs-410_7_1_0.jpg
cs-410_7_1_4,cs-410,7,1,Overview,"00:00:19,830","00:00:24,200",4,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=19,The title of this course is,pic_cs-410_7_1_0.jpg
cs-410_7_1_5,cs-410,7,1,Overview,"00:00:25,590","00:00:31,250",5,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=25,"But the two terms text mining, and text",pic_cs-410_7_1_0.jpg
cs-410_7_1_6,cs-410,7,1,Overview,"00:00:32,670","00:00:36,370",6,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=32,So we are not really going to,pic_cs-410_7_1_0.jpg
cs-410_7_1_7,cs-410,7,1,Overview,"00:00:36,370","00:00:38,230",7,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=36,we're going to use them interchangeably.,pic_cs-410_7_1_0.jpg
cs-410_7_1_8,cs-410,7,1,Overview,"00:00:38,230","00:00:42,880",8,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=38,But the reason that we have chosen to use,pic_cs-410_7_1_0.jpg
cs-410_7_1_9,cs-410,7,1,Overview,"00:00:42,880","00:00:47,720",9,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=42,both terms in the title is because,pic_cs-410_7_1_0.jpg
cs-410_7_1_10,cs-410,7,1,Overview,"00:00:47,720","00:00:51,070",10,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=47,if you look at the two phrases literally.,pic_cs-410_7_1_0.jpg
cs-410_7_1_11,cs-410,7,1,Overview,"00:00:52,110","00:00:55,640",11,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=52,Mining emphasizes more on the process.,pic_cs-410_7_1_0.jpg
cs-410_7_1_12,cs-410,7,1,Overview,"00:00:55,640","00:01:01,683",12,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=55,So it gives us a error rate,pic_cs-410_7_1_0.jpg
cs-410_7_1_13,cs-410,7,1,Overview,"00:01:01,683","00:01:06,359",13,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=61,"Analytics, on the other hand",pic_cs-410_7_1_60.jpg
cs-410_7_1_14,cs-410,7,1,Overview,"00:01:07,600","00:01:09,900",14,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=67,or having a problem in mind.,pic_cs-410_7_1_60.jpg
cs-410_7_1_15,cs-410,7,1,Overview,"00:01:09,900","00:01:14,720",15,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=69,We are going to look at text,pic_cs-410_7_1_60.jpg
cs-410_7_1_16,cs-410,7,1,Overview,"00:01:16,010","00:01:19,940",16,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=76,"But again as I said, we can treat",pic_cs-410_7_1_60.jpg
cs-410_7_1_17,cs-410,7,1,Overview,"00:01:21,150","00:01:24,820",17,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=81,And I think in the literature,pic_cs-410_7_1_60.jpg
cs-410_7_1_18,cs-410,7,1,Overview,"00:01:24,820","00:01:27,820",18,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=84,So we're not going to really,pic_cs-410_7_1_60.jpg
cs-410_7_1_19,cs-410,7,1,Overview,"00:01:29,850","00:01:35,450",19,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=89,Both text mining and,pic_cs-410_7_1_60.jpg
cs-410_7_1_20,cs-410,7,1,Overview,"00:01:35,450","00:01:40,250",20,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=95,want to turn text data into high quality,pic_cs-410_7_1_60.jpg
cs-410_7_1_21,cs-410,7,1,Overview,"00:01:42,570","00:01:44,020",21,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=102,"So in both cases, we",pic_cs-410_7_1_60.jpg
cs-410_7_1_22,cs-410,7,1,Overview,"00:01:45,830","00:01:50,670",22,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=105,have the problem of dealing with,pic_cs-410_7_1_60.jpg
cs-410_7_1_23,cs-410,7,1,Overview,"00:01:50,670","00:01:56,090",23,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=110,Turn these text data into something more,pic_cs-410_7_1_60.jpg
cs-410_7_1_24,cs-410,7,1,Overview,"00:01:57,730","00:02:00,380",24,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=117,And here we distinguish,pic_cs-410_7_1_60.jpg
cs-410_7_1_25,cs-410,7,1,Overview,"00:02:00,380","00:02:04,530",25,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=120,"One is high-quality information,",pic_cs-410_7_1_120.jpg
cs-410_7_1_26,cs-410,7,1,Overview,"00:02:05,740","00:02:08,680",26,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=125,Sometimes the boundary between,pic_cs-410_7_1_120.jpg
cs-410_7_1_27,cs-410,7,1,Overview,"00:02:09,850","00:02:11,690",27,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=129,But I also want to say a little bit about,pic_cs-410_7_1_120.jpg
cs-410_7_1_28,cs-410,7,1,Overview,"00:02:12,780","00:02:17,590",28,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=132,these two different angles of,pic_cs-410_7_1_120.jpg
cs-410_7_1_29,cs-410,7,1,Overview,"00:02:19,250","00:02:22,982",29,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=139,"In the case of high quality information,",pic_cs-410_7_1_120.jpg
cs-410_7_1_30,cs-410,7,1,Overview,"00:02:22,982","00:02:27,715",30,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=142,concise information about the topic.,pic_cs-410_7_1_120.jpg
cs-410_7_1_31,cs-410,7,1,Overview,"00:02:28,895","00:02:34,205",31,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=148,Which might be much easier for,pic_cs-410_7_1_120.jpg
cs-410_7_1_32,cs-410,7,1,Overview,"00:02:34,205","00:02:37,045",32,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=154,"For example, you might face",pic_cs-410_7_1_120.jpg
cs-410_7_1_33,cs-410,7,1,Overview,"00:02:38,260","00:02:42,700",33,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=158,A more concise form of information,pic_cs-410_7_1_120.jpg
cs-410_7_1_34,cs-410,7,1,Overview,"00:02:42,700","00:02:46,570",34,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=162,of the major opinions about,pic_cs-410_7_1_120.jpg
cs-410_7_1_35,cs-410,7,1,Overview,"00:02:46,570","00:02:50,874",35,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=166,"Positive about,",pic_cs-410_7_1_120.jpg
cs-410_7_1_36,cs-410,7,1,Overview,"00:02:53,436","00:02:58,260",36,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=173,Now this kind of results are very useful,pic_cs-410_7_1_120.jpg
cs-410_7_1_37,cs-410,7,1,Overview,"00:02:59,930","00:03:05,030",37,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=179,And so this is to minimize a human effort,pic_cs-410_7_1_120.jpg
cs-410_7_1_38,cs-410,7,1,Overview,"00:03:06,250","00:03:09,880",38,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=186,The other kind of output,pic_cs-410_7_1_180.jpg
cs-410_7_1_39,cs-410,7,1,Overview,"00:03:09,880","00:03:15,300",39,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=189,Here we emphasize the utility,pic_cs-410_7_1_180.jpg
cs-410_7_1_40,cs-410,7,1,Overview,"00:03:15,300","00:03:17,190",40,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=195,knowledge we discover from text data.,pic_cs-410_7_1_180.jpg
cs-410_7_1_41,cs-410,7,1,Overview,"00:03:18,270","00:03:23,830",41,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=198,It's actionable knowledge for some,pic_cs-410_7_1_180.jpg
cs-410_7_1_42,cs-410,7,1,Overview,"00:03:24,990","00:03:31,510",42,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=204,"For example, we might be able to determine",pic_cs-410_7_1_180.jpg
cs-410_7_1_43,cs-410,7,1,Overview,"00:03:31,510","00:03:36,270",43,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=211,or a better choice for,pic_cs-410_7_1_180.jpg
cs-410_7_1_44,cs-410,7,1,Overview,"00:03:38,115","00:03:43,190",44,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=218,"Now, such an outcome could be",pic_cs-410_7_1_180.jpg
cs-410_7_1_45,cs-410,7,1,Overview,"00:03:43,190","00:03:49,550",45,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=223,because a consumer can take the knowledge,pic_cs-410_7_1_180.jpg
cs-410_7_1_46,cs-410,7,1,Overview,"00:03:49,550","00:03:55,131",46,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=229,"So, in this case text mining supplies",pic_cs-410_7_1_180.jpg
cs-410_7_1_47,cs-410,7,1,Overview,"00:03:55,131","00:03:59,424",47,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=235,"But again, the two are not so",pic_cs-410_7_1_180.jpg
cs-410_7_1_48,cs-410,7,1,Overview,"00:03:59,424","00:04:03,281",48,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=239,we don't necessarily have,pic_cs-410_7_1_180.jpg
cs-410_7_1_49,cs-410,7,1,Overview,"00:04:06,253","00:04:09,821",49,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=246,Text mining is also,pic_cs-410_7_1_240.jpg
cs-410_7_1_50,cs-410,7,1,Overview,"00:04:09,821","00:04:14,380",50,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=249,which is a essential component,pic_cs-410_7_1_240.jpg
cs-410_7_1_51,cs-410,7,1,Overview,"00:04:15,910","00:04:20,434",51,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=255,"Now, text retrieval refers to",pic_cs-410_7_1_240.jpg
cs-410_7_1_52,cs-410,7,1,Overview,"00:04:20,434","00:04:22,380",52,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=260,a large amount of text data.,pic_cs-410_7_1_240.jpg
cs-410_7_1_53,cs-410,7,1,Overview,"00:04:24,140","00:04:30,210",53,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=264,So I've taught another separate MOOC,pic_cs-410_7_1_240.jpg
cs-410_7_1_54,cs-410,7,1,Overview,"00:04:31,710","00:04:34,680",54,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=271,Where we discussed various techniques for,pic_cs-410_7_1_240.jpg
cs-410_7_1_55,cs-410,7,1,Overview,"00:04:36,360","00:04:41,080",55,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=276,"If you have taken that MOOC,",pic_cs-410_7_1_240.jpg
cs-410_7_1_56,cs-410,7,1,Overview,"00:04:42,120","00:04:46,700",56,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=282,And it will be useful To know,pic_cs-410_7_1_240.jpg
cs-410_7_1_57,cs-410,7,1,Overview,"00:04:46,700","00:04:50,010",57,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=286,of understanding some of,pic_cs-410_7_1_240.jpg
cs-410_7_1_58,cs-410,7,1,Overview,"00:04:51,750","00:04:54,350",58,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=291,"But, if you have not taken that MOOC,",pic_cs-410_7_1_240.jpg
cs-410_7_1_59,cs-410,7,1,Overview,"00:04:54,350","00:04:59,440",59,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=294,it's also fine because in this MOOC,pic_cs-410_7_1_240.jpg
cs-410_7_1_60,cs-410,7,1,Overview,"00:04:59,440","00:05:03,260",60,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=299,going to repeat some of the key concepts,pic_cs-410_7_1_240.jpg
cs-410_7_1_61,cs-410,7,1,Overview,"00:05:03,260","00:05:06,540",61,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=303,But they're at the high level and,pic_cs-410_7_1_300.jpg
cs-410_7_1_62,cs-410,7,1,Overview,"00:05:06,540","00:05:10,710",62,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=306,they also explain the relation between,pic_cs-410_7_1_300.jpg
cs-410_7_1_63,cs-410,7,1,Overview,"00:05:12,320","00:05:18,278",63,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=312,Text retrieval is very useful for,pic_cs-410_7_1_300.jpg
cs-410_7_1_64,cs-410,7,1,Overview,"00:05:18,278","00:05:23,200",64,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=318,"First, text retrieval can be",pic_cs-410_7_1_300.jpg
cs-410_7_1_65,cs-410,7,1,Overview,"00:05:23,200","00:05:27,600",65,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=323,Meaning that it can help,pic_cs-410_7_1_300.jpg
cs-410_7_1_66,cs-410,7,1,Overview,"00:05:27,600","00:05:32,030",66,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=327,a relatively small amount,pic_cs-410_7_1_300.jpg
cs-410_7_1_67,cs-410,7,1,Overview,"00:05:32,030","00:05:35,180",67,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=332,Which is often what's needed for,pic_cs-410_7_1_300.jpg
cs-410_7_1_68,cs-410,7,1,Overview,"00:05:36,580","00:05:41,186",68,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=336,"And in this sense, text retrieval",pic_cs-410_7_1_300.jpg
cs-410_7_1_69,cs-410,7,1,Overview,"00:05:43,323","00:05:46,365",69,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=343,Text retrieval is also needed for,pic_cs-410_7_1_300.jpg
cs-410_7_1_70,cs-410,7,1,Overview,"00:05:46,365","00:05:50,976",70,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=346,And this roughly corresponds,pic_cs-410_7_1_300.jpg
cs-410_7_1_71,cs-410,7,1,Overview,"00:05:50,976","00:05:56,350",71,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=350,mining as turning text data,pic_cs-410_7_1_300.jpg
cs-410_7_1_72,cs-410,7,1,Overview,"00:05:56,350","00:05:58,970",72,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=356,"Once we find the patterns in text data, or",pic_cs-410_7_1_300.jpg
cs-410_7_1_73,cs-410,7,1,Overview,"00:05:58,970","00:06:04,040",73,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=358,"actionable knowledge, we generally",pic_cs-410_7_1_300.jpg
cs-410_7_1_74,cs-410,7,1,Overview,"00:06:04,040","00:06:06,580",74,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=364,By looking at the original text data.,pic_cs-410_7_1_360.jpg
cs-410_7_1_75,cs-410,7,1,Overview,"00:06:06,580","00:06:11,110",75,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=366,So the users would have to have some text,pic_cs-410_7_1_360.jpg
cs-410_7_1_76,cs-410,7,1,Overview,"00:06:11,110","00:06:16,010",76,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=371,text data to interpret the pattern or,pic_cs-410_7_1_360.jpg
cs-410_7_1_77,cs-410,7,1,Overview,"00:06:16,010","00:06:19,910",77,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=376,to verify whether a pattern,pic_cs-410_7_1_360.jpg
cs-410_7_1_78,cs-410,7,1,Overview,"00:06:19,910","00:06:23,830",78,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=379,So this is a high level introduction,pic_cs-410_7_1_360.jpg
cs-410_7_1_79,cs-410,7,1,Overview,"00:06:23,830","00:06:29,530",79,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=383,and the relationship between,pic_cs-410_7_1_360.jpg
cs-410_7_1_80,cs-410,7,1,Overview,"00:06:32,110","00:06:36,554",80,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=392,"Next, let's talk about text",pic_cs-410_7_1_360.jpg
cs-410_7_1_81,cs-410,7,1,Overview,"00:06:39,689","00:06:45,607",81,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=399,Now it's interesting to,pic_cs-410_7_1_360.jpg
cs-410_7_1_82,cs-410,7,1,Overview,"00:06:45,607","00:06:51,380",82,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=405,generated by humans as subjective sensors.,pic_cs-410_7_1_360.jpg
cs-410_7_1_83,cs-410,7,1,Overview,"00:06:53,200","00:07:03,420",83,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=413,"So, this slide shows an analogy",pic_cs-410_7_1_360.jpg
cs-410_7_1_84,cs-410,7,1,Overview,"00:07:03,420","00:07:07,832",84,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=423,And between humans as,pic_cs-410_7_1_420.jpg
cs-410_7_1_85,cs-410,7,1,Overview,"00:07:07,832","00:07:13,993",85,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=427,"physical sensors,",pic_cs-410_7_1_420.jpg
cs-410_7_1_86,cs-410,7,1,Overview,"00:07:16,292","00:07:21,377",86,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=436,So in general a sensor would,pic_cs-410_7_1_420.jpg
cs-410_7_1_87,cs-410,7,1,Overview,"00:07:21,377","00:07:26,483",87,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=441,It would sense some signal,pic_cs-410_7_1_420.jpg
cs-410_7_1_88,cs-410,7,1,Overview,"00:07:26,483","00:07:32,205",88,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=446,"then would report the signal as data,",pic_cs-410_7_1_420.jpg
cs-410_7_1_89,cs-410,7,1,Overview,"00:07:32,205","00:07:38,346",89,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=452,"For example, a thermometer would watch",pic_cs-410_7_1_420.jpg
cs-410_7_1_90,cs-410,7,1,Overview,"00:07:38,346","00:07:43,310",90,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=458,then we report the temperature,pic_cs-410_7_1_420.jpg
cs-410_7_1_91,cs-410,7,1,Overview,"00:07:44,962","00:07:49,098",91,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=464,"Similarly, a geo sensor would sense",pic_cs-410_7_1_420.jpg
cs-410_7_1_92,cs-410,7,1,Overview,"00:07:49,098","00:07:53,740",92,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=469,"The location specification, for",pic_cs-410_7_1_420.jpg
cs-410_7_1_93,cs-410,7,1,Overview,"00:07:53,740","00:07:57,140",93,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=473,"example, in the form of longitude",pic_cs-410_7_1_420.jpg
cs-410_7_1_94,cs-410,7,1,Overview,"00:07:57,140","00:08:02,580",94,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=477,A network sends over,pic_cs-410_7_1_420.jpg
cs-410_7_1_95,cs-410,7,1,Overview,"00:08:02,580","00:08:04,873",95,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=482,or activities in the network and,pic_cs-410_7_1_480.jpg
cs-410_7_1_96,cs-410,7,1,Overview,"00:08:04,873","00:08:09,477",96,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=484,Some digital format of data.,pic_cs-410_7_1_480.jpg
cs-410_7_1_97,cs-410,7,1,Overview,"00:08:09,477","00:08:16,460",97,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=489,Similarly we can think of,pic_cs-410_7_1_480.jpg
cs-410_7_1_98,cs-410,7,1,Overview,"00:08:16,460","00:08:22,050",98,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=496,That will observe the real world and,pic_cs-410_7_1_480.jpg
cs-410_7_1_99,cs-410,7,1,Overview,"00:08:22,050","00:08:28,440",99,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=502,And then humans will express what they,pic_cs-410_7_1_480.jpg
cs-410_7_1_100,cs-410,7,1,Overview,"00:08:28,440","00:08:33,330",100,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=508,"So, in this sense, human is actually",pic_cs-410_7_1_480.jpg
cs-410_7_1_101,cs-410,7,1,Overview,"00:08:33,330","00:08:36,200",101,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=513,sense what's happening in the world and,pic_cs-410_7_1_480.jpg
cs-410_7_1_102,cs-410,7,1,Overview,"00:08:36,200","00:08:43,060",102,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=516,then express what's observed in the form,pic_cs-410_7_1_480.jpg
cs-410_7_1_103,cs-410,7,1,Overview,"00:08:43,060","00:08:47,350",103,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=523,"Now, looking at the text data in",pic_cs-410_7_1_480.jpg
cs-410_7_1_104,cs-410,7,1,Overview,"00:08:47,350","00:08:50,240",104,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=527,able to integrate all,pic_cs-410_7_1_480.jpg
cs-410_7_1_105,cs-410,7,1,Overview,"00:08:50,240","00:08:54,381",105,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=530,And that's indeed needed in,pic_cs-410_7_1_480.jpg
cs-410_7_1_106,cs-410,7,1,Overview,"00:08:56,123","00:09:01,672",106,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=536,So here we are looking at,pic_cs-410_7_1_480.jpg
cs-410_7_1_107,cs-410,7,1,Overview,"00:09:02,725","00:09:07,518",107,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=542,And in general we would Be,pic_cs-410_7_1_540.jpg
cs-410_7_1_108,cs-410,7,1,Overview,"00:09:07,518","00:09:11,982",108,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=547,about our world that,pic_cs-410_7_1_540.jpg
cs-410_7_1_109,cs-410,7,1,Overview,"00:09:11,982","00:09:17,180",109,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=551,And in general it will be dealing with,pic_cs-410_7_1_540.jpg
cs-410_7_1_110,cs-410,7,1,Overview,"00:09:17,180","00:09:21,348",110,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=557,And of course the non-text data,pic_cs-410_7_1_540.jpg
cs-410_7_1_111,cs-410,7,1,Overview,"00:09:21,348","00:09:26,330",111,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=561,And those non-text data can,pic_cs-410_7_1_540.jpg
cs-410_7_1_112,cs-410,7,1,Overview,"00:09:27,840","00:09:30,830",112,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=567,"Numerical data, categorical,",pic_cs-410_7_1_540.jpg
cs-410_7_1_113,cs-410,7,1,Overview,"00:09:30,830","00:09:33,800",113,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=570,or multi-media data like video or speech.,pic_cs-410_7_1_540.jpg
cs-410_7_1_114,cs-410,7,1,Overview,"00:09:36,360","00:09:41,900",114,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=576,"So, these non text data are often",pic_cs-410_7_1_540.jpg
cs-410_7_1_115,cs-410,7,1,Overview,"00:09:41,900","00:09:45,590",115,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=581,"But text data is also very important,",pic_cs-410_7_1_540.jpg
cs-410_7_1_116,cs-410,7,1,Overview,"00:09:45,590","00:09:50,960",116,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=585,mostly because they contain,pic_cs-410_7_1_540.jpg
cs-410_7_1_117,cs-410,7,1,Overview,"00:09:50,960","00:09:55,930",117,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=590,And they often contain,pic_cs-410_7_1_540.jpg
cs-410_7_1_118,cs-410,7,1,Overview,"00:09:55,930","00:09:58,860",118,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=595,especially preferences and,pic_cs-410_7_1_540.jpg
cs-410_7_1_119,cs-410,7,1,Overview,"00:10:01,360","00:10:07,990",119,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=601,"So, but by treating text data as",pic_cs-410_7_1_600.jpg
cs-410_7_1_120,cs-410,7,1,Overview,"00:10:07,990","00:10:14,510",120,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=607,we can treat all this data,pic_cs-410_7_1_600.jpg
cs-410_7_1_121,cs-410,7,1,Overview,"00:10:14,510","00:10:18,110",121,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=614,So the data mining problem is,pic_cs-410_7_1_600.jpg
cs-410_7_1_122,cs-410,7,1,Overview,"00:10:18,110","00:10:22,960",122,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=618,turn all the data in your actionable,pic_cs-410_7_1_600.jpg
cs-410_7_1_123,cs-410,7,1,Overview,"00:10:22,960","00:10:26,260",123,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=622,of it to change the real,pic_cs-410_7_1_600.jpg
cs-410_7_1_124,cs-410,7,1,Overview,"00:10:26,260","00:10:31,490",124,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=626,So this means the data mining problem is,pic_cs-410_7_1_600.jpg
cs-410_7_1_125,cs-410,7,1,Overview,"00:10:31,490","00:10:37,450",125,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=631,basically taking a lot of data as input,pic_cs-410_7_1_600.jpg
cs-410_7_1_126,cs-410,7,1,Overview,"00:10:37,450","00:10:42,260",126,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=637,"Inside of the data mining module,",pic_cs-410_7_1_600.jpg
cs-410_7_1_127,cs-410,7,1,Overview,"00:10:42,260","00:10:46,510",127,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=642,we have a number of different,pic_cs-410_7_1_600.jpg
cs-410_7_1_128,cs-410,7,1,Overview,"00:10:46,510","00:10:49,940",128,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=646,"And this is because, for",pic_cs-410_7_1_600.jpg
cs-410_7_1_129,cs-410,7,1,Overview,"00:10:49,940","00:10:55,180",129,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=649,we generally need different algorithms for,pic_cs-410_7_1_600.jpg
cs-410_7_1_130,cs-410,7,1,Overview,"00:10:56,390","00:10:57,100",130,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=656,"For example,",pic_cs-410_7_1_600.jpg
cs-410_7_1_131,cs-410,7,1,Overview,"00:10:57,100","00:11:01,870",131,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=657,video data might require computer,pic_cs-410_7_1_600.jpg
cs-410_7_1_132,cs-410,7,1,Overview,"00:11:01,870","00:11:06,050",132,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=661,And that would facilitate,pic_cs-410_7_1_660.jpg
cs-410_7_1_133,cs-410,7,1,Overview,"00:11:06,050","00:11:11,110",133,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=666,And we also have a lot of general,pic_cs-410_7_1_660.jpg
cs-410_7_1_134,cs-410,7,1,Overview,"00:11:11,110","00:11:16,948",134,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=671,"to all kinds of data and those algorithms,",pic_cs-410_7_1_660.jpg
cs-410_7_1_135,cs-410,7,1,Overview,"00:11:16,948","00:11:19,692",135,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=676,"Although, for a particular kind of data,",pic_cs-410_7_1_660.jpg
cs-410_7_1_136,cs-410,7,1,Overview,"00:11:19,692","00:11:23,287",136,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=679,we generally want to also,pic_cs-410_7_1_660.jpg
cs-410_7_1_137,cs-410,7,1,Overview,"00:11:23,287","00:11:27,939",137,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=683,So this course will cover,pic_cs-410_7_1_660.jpg
cs-410_7_1_138,cs-410,7,1,Overview,"00:11:27,939","00:11:31,994",138,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=687,are particularly useful for,pic_cs-410_7_1_660.jpg
cs-410_7_1_139,cs-410,7,1,Overview,"00:11:31,994","00:11:41,994",139,https://www.coursera.org/learn/cs-410/lecture/7zA4L?t=691,[MUSIC],pic_cs-410_7_1_660.jpg
cs-410_7_2_1,cs-410,7,2,Overview,"00:00:00,266","00:00:09,956",1,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=0,[SOUND],pic_cs-410_7_2_0.jpg
cs-410_7_2_2,cs-410,7,2,Overview,"00:00:09,956","00:00:14,850",2,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=9,looking at the text mining problem more,pic_cs-410_7_2_0.jpg
cs-410_7_2_3,cs-410,7,2,Overview,"00:00:14,850","00:00:20,300",3,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=14,"similar to general data mining, except",pic_cs-410_7_2_0.jpg
cs-410_7_2_4,cs-410,7,2,Overview,"00:00:21,710","00:00:26,400",4,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=21,And we're going to have text mining,pic_cs-410_7_2_0.jpg
cs-410_7_2_5,cs-410,7,2,Overview,"00:00:26,400","00:00:32,240",5,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=26,into actionable knowledge that,pic_cs-410_7_2_0.jpg
cs-410_7_2_6,cs-410,7,2,Overview,"00:00:32,240","00:00:34,130",6,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=32,"especially for decision making, or",pic_cs-410_7_2_0.jpg
cs-410_7_2_7,cs-410,7,2,Overview,"00:00:34,130","00:00:39,350",7,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=34,for completing whatever tasks that,pic_cs-410_7_2_0.jpg
cs-410_7_2_8,cs-410,7,2,Overview,"00:00:39,350","00:00:45,000",8,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=39,"Because, in general,",pic_cs-410_7_2_0.jpg
cs-410_7_2_9,cs-410,7,2,Overview,"00:00:45,000","00:00:49,720",9,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=45,we also tend to have other kinds,pic_cs-410_7_2_0.jpg
cs-410_7_2_10,cs-410,7,2,Overview,"00:00:49,720","00:00:54,950",10,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=49,So a more general picture would be,pic_cs-410_7_2_0.jpg
cs-410_7_2_11,cs-410,7,2,Overview,"00:00:56,000","00:01:00,875",11,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=56,And for this reason we might be,pic_cs-410_7_2_0.jpg
cs-410_7_2_12,cs-410,7,2,Overview,"00:01:00,875","00:01:02,060",12,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=60,non-text data.,pic_cs-410_7_2_60.jpg
cs-410_7_2_13,cs-410,7,2,Overview,"00:01:02,060","00:01:05,860",13,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=62,And so in this course we're,pic_cs-410_7_2_60.jpg
cs-410_7_2_14,cs-410,7,2,Overview,"00:01:05,860","00:01:10,628",14,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=65,but we're also going to also touch how do,pic_cs-410_7_2_60.jpg
cs-410_7_2_15,cs-410,7,2,Overview,"00:01:10,628","00:01:12,450",15,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=70,non-text data.,pic_cs-410_7_2_60.jpg
cs-410_7_2_16,cs-410,7,2,Overview,"00:01:12,450","00:01:16,630",16,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=72,With this problem definition we,pic_cs-410_7_2_60.jpg
cs-410_7_2_17,cs-410,7,2,Overview,"00:01:16,630","00:01:19,419",17,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=76,the topics in text mining and analytics.,pic_cs-410_7_2_60.jpg
cs-410_7_2_18,cs-410,7,2,Overview,"00:01:21,010","00:01:25,770",18,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=81,Now this slide shows the process of,pic_cs-410_7_2_60.jpg
cs-410_7_2_19,cs-410,7,2,Overview,"00:01:27,018","00:01:29,800",19,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=87,"More specifically, a human sensor or",pic_cs-410_7_2_60.jpg
cs-410_7_2_20,cs-410,7,2,Overview,"00:01:29,800","00:01:33,420",20,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=89,human observer would look at,pic_cs-410_7_2_60.jpg
cs-410_7_2_21,cs-410,7,2,Overview,"00:01:34,660","00:01:38,820",21,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=94,Different people would be looking at,pic_cs-410_7_2_60.jpg
cs-410_7_2_22,cs-410,7,2,Overview,"00:01:38,820","00:01:41,210",22,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=98,they'll pay attention to different things.,pic_cs-410_7_2_60.jpg
cs-410_7_2_23,cs-410,7,2,Overview,"00:01:41,210","00:01:46,090",23,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=101,The same person at different times might,pic_cs-410_7_2_60.jpg
cs-410_7_2_24,cs-410,7,2,Overview,"00:01:46,090","00:01:50,990",24,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=106,of the observed world.,pic_cs-410_7_2_60.jpg
cs-410_7_2_25,cs-410,7,2,Overview,"00:01:50,990","00:01:55,450",25,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=110,And so the humans are able to perceive,pic_cs-410_7_2_60.jpg
cs-410_7_2_26,cs-410,7,2,Overview,"00:01:55,450","00:02:01,480",26,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=115,"And that human, the sensor,",pic_cs-410_7_2_60.jpg
cs-410_7_2_27,cs-410,7,2,Overview,"00:02:01,480","00:02:05,150",27,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=121,And that can be called the Observed World.,pic_cs-410_7_2_120.jpg
cs-410_7_2_28,cs-410,7,2,Overview,"00:02:05,150","00:02:10,040",28,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=125,"Of course, this would be different from",pic_cs-410_7_2_120.jpg
cs-410_7_2_29,cs-410,7,2,Overview,"00:02:10,040","00:02:14,830",29,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=130,that the person has taken,pic_cs-410_7_2_120.jpg
cs-410_7_2_30,cs-410,7,2,Overview,"00:02:16,840","00:02:22,642",30,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=136,Now the Observed World can be,pic_cs-410_7_2_120.jpg
cs-410_7_2_31,cs-410,7,2,Overview,"00:02:22,642","00:02:27,535",31,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=142,entity-relation graphs or,pic_cs-410_7_2_120.jpg
cs-410_7_2_32,cs-410,7,2,Overview,"00:02:27,535","00:02:31,890",32,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=147,using knowledge representation language.,pic_cs-410_7_2_120.jpg
cs-410_7_2_33,cs-410,7,2,Overview,"00:02:31,890","00:02:39,190",33,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=151,"But in general, this is basically what",pic_cs-410_7_2_120.jpg
cs-410_7_2_34,cs-410,7,2,Overview,"00:02:39,190","00:02:43,800",34,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=159,And we don't really know what,pic_cs-410_7_2_120.jpg
cs-410_7_2_35,cs-410,7,2,Overview,"00:02:43,800","00:02:48,250",35,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=163,But then the human would,pic_cs-410_7_2_120.jpg
cs-410_7_2_36,cs-410,7,2,Overview,"00:02:48,250","00:02:52,920",36,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=168,"observed using a natural language,",pic_cs-410_7_2_120.jpg
cs-410_7_2_37,cs-410,7,2,Overview,"00:02:52,920","00:02:54,760",37,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=172,And the result is text data.,pic_cs-410_7_2_120.jpg
cs-410_7_2_38,cs-410,7,2,Overview,"00:02:55,870","00:03:00,610",38,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=175,Of course a person could have used,pic_cs-410_7_2_120.jpg
cs-410_7_2_39,cs-410,7,2,Overview,"00:03:00,610","00:03:02,660",39,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=180,she has observed.,pic_cs-410_7_2_180.jpg
cs-410_7_2_40,cs-410,7,2,Overview,"00:03:02,660","00:03:08,220",40,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=182,In that case we might have text data of,pic_cs-410_7_2_180.jpg
cs-410_7_2_41,cs-410,7,2,Overview,"00:03:10,590","00:03:15,790",41,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=190,The main goal of text mining,pic_cs-410_7_2_180.jpg
cs-410_7_2_42,cs-410,7,2,Overview,"00:03:15,790","00:03:19,280",42,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=195,process of generating text data.,pic_cs-410_7_2_180.jpg
cs-410_7_2_43,cs-410,7,2,Overview,"00:03:19,280","00:03:24,480",43,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=199,We hope to be able to uncover,pic_cs-410_7_2_180.jpg
cs-410_7_2_44,cs-410,7,2,Overview,"00:03:28,340","00:03:34,480",44,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=208,"Specifically, we can think about mining,",pic_cs-410_7_2_180.jpg
cs-410_7_2_45,cs-410,7,2,Overview,"00:03:35,560","00:03:40,130",45,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=215,And that means by looking at text data,pic_cs-410_7_2_180.jpg
cs-410_7_2_46,cs-410,7,2,Overview,"00:03:40,130","00:03:46,310",46,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=220,"something about English, some usage",pic_cs-410_7_2_180.jpg
cs-410_7_2_47,cs-410,7,2,Overview,"00:03:47,780","00:03:52,340",47,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=227,"So this is one type of mining problems,",pic_cs-410_7_2_180.jpg
cs-410_7_2_48,cs-410,7,2,Overview,"00:03:52,340","00:03:57,100",48,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=232,some knowledge about language which,pic_cs-410_7_2_180.jpg
cs-410_7_2_49,cs-410,7,2,Overview,"00:03:58,920","00:04:00,620",49,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=238,"If you look at the picture,",pic_cs-410_7_2_180.jpg
cs-410_7_2_50,cs-410,7,2,Overview,"00:04:00,620","00:04:06,380",50,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=240,we can also then mine knowledge,pic_cs-410_7_2_240.jpg
cs-410_7_2_51,cs-410,7,2,Overview,"00:04:06,380","00:04:10,030",51,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=246,And so this has much to do with,pic_cs-410_7_2_240.jpg
cs-410_7_2_52,cs-410,7,2,Overview,"00:04:11,490","00:04:15,640",52,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=251,We're going to look at what the text,pic_cs-410_7_2_240.jpg
cs-410_7_2_53,cs-410,7,2,Overview,"00:04:15,640","00:04:20,820",53,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=255,get the essence of it or,pic_cs-410_7_2_240.jpg
cs-410_7_2_54,cs-410,7,2,Overview,"00:04:20,820","00:04:25,600",54,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=260,about a particular aspect of,pic_cs-410_7_2_240.jpg
cs-410_7_2_55,cs-410,7,2,Overview,"00:04:26,900","00:04:30,890",55,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=266,"For example, everything that has been",pic_cs-410_7_2_240.jpg
cs-410_7_2_56,cs-410,7,2,Overview,"00:04:30,890","00:04:31,630",56,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=270,a particular entity.,pic_cs-410_7_2_240.jpg
cs-410_7_2_57,cs-410,7,2,Overview,"00:04:31,630","00:04:36,550",57,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=271,And this can be regarded as mining content,pic_cs-410_7_2_240.jpg
cs-410_7_2_58,cs-410,7,2,Overview,"00:04:36,550","00:04:43,330",58,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=276,to describe the observed world in,pic_cs-410_7_2_240.jpg
cs-410_7_2_59,cs-410,7,2,Overview,"00:04:45,020","00:04:50,060",59,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=285,"If you look further,",pic_cs-410_7_2_240.jpg
cs-410_7_2_60,cs-410,7,2,Overview,"00:04:50,060","00:04:54,710",60,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=290,"we can mine knowledge about this observer,",pic_cs-410_7_2_240.jpg
cs-410_7_2_61,cs-410,7,2,Overview,"00:04:54,710","00:05:00,630",61,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=294,So this has also to do with,pic_cs-410_7_2_240.jpg
cs-410_7_2_62,cs-410,7,2,Overview,"00:05:00,630","00:05:02,270",62,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=300,some properties of this person.,pic_cs-410_7_2_300.jpg
cs-410_7_2_63,cs-410,7,2,Overview,"00:05:03,380","00:05:07,410",63,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=303,And these properties could,pic_cs-410_7_2_300.jpg
cs-410_7_2_64,cs-410,7,2,Overview,"00:05:07,410","00:05:09,010",64,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=307,sentiment of the person.,pic_cs-410_7_2_300.jpg
cs-410_7_2_65,cs-410,7,2,Overview,"00:05:10,200","00:05:15,250",65,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=310,And note that we distinguish,pic_cs-410_7_2_300.jpg
cs-410_7_2_66,cs-410,7,2,Overview,"00:05:15,250","00:05:21,040",66,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=315,because text data can't describe what the,pic_cs-410_7_2_300.jpg
cs-410_7_2_67,cs-410,7,2,Overview,"00:05:21,040","00:05:25,960",67,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=321,But the description can be also,pic_cs-410_7_2_300.jpg
cs-410_7_2_68,cs-410,7,2,Overview,"00:05:25,960","00:05:30,280",68,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=325,"in general, you can imagine the text",pic_cs-410_7_2_300.jpg
cs-410_7_2_69,cs-410,7,2,Overview,"00:05:30,280","00:05:34,770",69,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=330,descriptions of the world plus,pic_cs-410_7_2_300.jpg
cs-410_7_2_70,cs-410,7,2,Overview,"00:05:34,770","00:05:37,330",70,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=334,So that's why it's also possible to,pic_cs-410_7_2_300.jpg
cs-410_7_2_71,cs-410,7,2,Overview,"00:05:37,330","00:05:41,970",71,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=337,do text mining to mine,pic_cs-410_7_2_300.jpg
cs-410_7_2_72,cs-410,7,2,Overview,"00:05:41,970","00:05:45,980",72,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=341,"Finally, if you look at the picture",pic_cs-410_7_2_300.jpg
cs-410_7_2_73,cs-410,7,2,Overview,"00:05:45,980","00:05:50,150",73,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=345,then you can see we can certainly also,pic_cs-410_7_2_300.jpg
cs-410_7_2_74,cs-410,7,2,Overview,"00:05:50,150","00:05:50,880",74,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=350,Right?,pic_cs-410_7_2_300.jpg
cs-410_7_2_75,cs-410,7,2,Overview,"00:05:50,880","00:05:56,860",75,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=350,So indeed we can do text mining to,pic_cs-410_7_2_300.jpg
cs-410_7_2_76,cs-410,7,2,Overview,"00:05:56,860","00:05:59,220",76,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=356,And this is often called,pic_cs-410_7_2_300.jpg
cs-410_7_2_77,cs-410,7,2,Overview,"00:06:00,600","00:06:04,000",77,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=360,And we want to predict the value,pic_cs-410_7_2_360.jpg
cs-410_7_2_78,cs-410,7,2,Overview,"00:06:04,000","00:06:08,935",78,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=364,"So, this picture basically covered",pic_cs-410_7_2_360.jpg
cs-410_7_2_79,cs-410,7,2,Overview,"00:06:08,935","00:06:13,440",79,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=368,multiple types of knowledge that,pic_cs-410_7_2_360.jpg
cs-410_7_2_80,cs-410,7,2,Overview,"00:06:14,550","00:06:19,260",80,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=374,When we infer other,pic_cs-410_7_2_360.jpg
cs-410_7_2_81,cs-410,7,2,Overview,"00:06:19,260","00:06:24,990",81,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=379,could also use some of the results from,pic_cs-410_7_2_360.jpg
cs-410_7_2_82,cs-410,7,2,Overview,"00:06:24,990","00:06:30,910",82,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=384,mining text data as intermediate,pic_cs-410_7_2_360.jpg
cs-410_7_2_83,cs-410,7,2,Overview,"00:06:30,910","00:06:31,940",83,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=390,"For example,",pic_cs-410_7_2_360.jpg
cs-410_7_2_84,cs-410,7,2,Overview,"00:06:31,940","00:06:38,050",84,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=391,after we mine the content of text data we,pic_cs-410_7_2_360.jpg
cs-410_7_2_85,cs-410,7,2,Overview,"00:06:38,050","00:06:41,190",85,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=398,And that summary could be then used,pic_cs-410_7_2_360.jpg
cs-410_7_2_86,cs-410,7,2,Overview,"00:06:41,190","00:06:45,003",86,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=401,to help us predict the variables,pic_cs-410_7_2_360.jpg
cs-410_7_2_87,cs-410,7,2,Overview,"00:06:45,003","00:06:51,940",87,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=405,Now of course this is still generated,pic_cs-410_7_2_360.jpg
cs-410_7_2_88,cs-410,7,2,Overview,"00:06:51,940","00:06:58,410",88,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=411,but I want to emphasize here that,pic_cs-410_7_2_360.jpg
cs-410_7_2_89,cs-410,7,2,Overview,"00:06:58,410","00:07:03,780",89,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=418,to generate some features that can help,pic_cs-410_7_2_360.jpg
cs-410_7_2_90,cs-410,7,2,Overview,"00:07:04,960","00:07:10,100",90,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=424,And that's why here we show the results of,pic_cs-410_7_2_420.jpg
cs-410_7_2_91,cs-410,7,2,Overview,"00:07:10,100","00:07:15,010",91,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=430,"some other mining tasks, including",pic_cs-410_7_2_420.jpg
cs-410_7_2_92,cs-410,7,2,Overview,"00:07:15,010","00:07:19,520",92,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=435,"mining knowledge about the observer,",pic_cs-410_7_2_420.jpg
cs-410_7_2_93,cs-410,7,2,Overview,"00:07:21,380","00:07:26,690",93,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=441,"In fact, when we have non-text data,",pic_cs-410_7_2_420.jpg
cs-410_7_2_94,cs-410,7,2,Overview,"00:07:26,690","00:07:31,496",94,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=446,"data to help prediction, and",pic_cs-410_7_2_420.jpg
cs-410_7_2_95,cs-410,7,2,Overview,"00:07:31,496","00:07:39,260",95,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=451,"In general, non-text data can be very",pic_cs-410_7_2_420.jpg
cs-410_7_2_96,cs-410,7,2,Overview,"00:07:39,260","00:07:44,530",96,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=459,"For example,",pic_cs-410_7_2_420.jpg
cs-410_7_2_97,cs-410,7,2,Overview,"00:07:44,530","00:07:49,870",97,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=464,changes of stock prices based on,pic_cs-410_7_2_420.jpg
cs-410_7_2_98,cs-410,7,2,Overview,"00:07:49,870","00:07:53,730",98,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=469,"in social media, then this is an example",pic_cs-410_7_2_420.jpg
cs-410_7_2_99,cs-410,7,2,Overview,"00:07:53,730","00:07:58,520",99,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=473,of using text data to predict,pic_cs-410_7_2_420.jpg
cs-410_7_2_100,cs-410,7,2,Overview,"00:07:58,520","00:07:59,950",100,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=478,"But in this case, obviously,",pic_cs-410_7_2_420.jpg
cs-410_7_2_101,cs-410,7,2,Overview,"00:07:59,950","00:08:04,480",101,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=479,the historical stock price data would,pic_cs-410_7_2_420.jpg
cs-410_7_2_102,cs-410,7,2,Overview,"00:08:04,480","00:08:09,633",102,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=484,And so that's an example of,pic_cs-410_7_2_480.jpg
cs-410_7_2_103,cs-410,7,2,Overview,"00:08:09,633","00:08:13,750",103,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=489,useful for the prediction.,pic_cs-410_7_2_480.jpg
cs-410_7_2_104,cs-410,7,2,Overview,"00:08:13,750","00:08:17,161",104,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=493,And we're going to combine both kinds,pic_cs-410_7_2_480.jpg
cs-410_7_2_105,cs-410,7,2,Overview,"00:08:17,161","00:08:24,510",105,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=497,Now non-text data can be also used for,pic_cs-410_7_2_480.jpg
cs-410_7_2_106,cs-410,7,2,Overview,"00:08:25,580","00:08:27,050",106,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=505,"When we look at the text data alone,",pic_cs-410_7_2_480.jpg
cs-410_7_2_107,cs-410,7,2,Overview,"00:08:27,050","00:08:31,770",107,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=507,we'll be mostly looking at the content,pic_cs-410_7_2_480.jpg
cs-410_7_2_108,cs-410,7,2,Overview,"00:08:32,790","00:08:36,149",108,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=512,But text data generally also,pic_cs-410_7_2_480.jpg
cs-410_7_2_109,cs-410,7,2,Overview,"00:08:37,470","00:08:44,150",109,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=517,"For example, the time and the location",pic_cs-410_7_2_480.jpg
cs-410_7_2_110,cs-410,7,2,Overview,"00:08:44,150","00:08:47,500",110,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=524,And these are useful context information.,pic_cs-410_7_2_480.jpg
cs-410_7_2_111,cs-410,7,2,Overview,"00:08:48,740","00:08:54,020",111,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=528,And the context can provide interesting,pic_cs-410_7_2_480.jpg
cs-410_7_2_112,cs-410,7,2,Overview,"00:08:54,020","00:08:57,980",112,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=534,"For example, we might partition text",pic_cs-410_7_2_480.jpg
cs-410_7_2_113,cs-410,7,2,Overview,"00:08:57,980","00:09:00,680",113,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=537,because of the availability of the time.,pic_cs-410_7_2_480.jpg
cs-410_7_2_114,cs-410,7,2,Overview,"00:09:00,680","00:09:06,480",114,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=540,Now we can analyze text data in each,pic_cs-410_7_2_540.jpg
cs-410_7_2_115,cs-410,7,2,Overview,"00:09:06,480","00:09:09,970",115,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=546,Similarly we can partition text,pic_cs-410_7_2_540.jpg
cs-410_7_2_116,cs-410,7,2,Overview,"00:09:09,970","00:09:15,920",116,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=549,any meta data that's associated to,pic_cs-410_7_2_540.jpg
cs-410_7_2_117,cs-410,7,2,Overview,"00:09:15,920","00:09:20,980",117,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=555,"So, in this sense,",pic_cs-410_7_2_540.jpg
cs-410_7_2_118,cs-410,7,2,Overview,"00:09:20,980","00:09:24,580",118,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=560,interesting angles or,pic_cs-410_7_2_540.jpg
cs-410_7_2_119,cs-410,7,2,Overview,"00:09:24,580","00:09:29,340",119,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=564,And it can help us make context-sensitive,pic_cs-410_7_2_540.jpg
cs-410_7_2_120,cs-410,7,2,Overview,"00:09:29,340","00:09:33,680",120,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=569,analysis of content or,pic_cs-410_7_2_540.jpg
cs-410_7_2_121,cs-410,7,2,Overview,"00:09:36,390","00:09:42,920",121,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=576,the opinions about the observer or,pic_cs-410_7_2_540.jpg
cs-410_7_2_122,cs-410,7,2,Overview,"00:09:42,920","00:09:46,920",122,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=582,We could analyze the sentiment,pic_cs-410_7_2_540.jpg
cs-410_7_2_123,cs-410,7,2,Overview,"00:09:46,920","00:09:54,500",123,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=586,So this is a fairly general landscape of,pic_cs-410_7_2_540.jpg
cs-410_7_2_124,cs-410,7,2,Overview,"00:09:54,500","00:09:59,850",124,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=594,In this course we're going to,pic_cs-410_7_2_540.jpg
cs-410_7_2_125,cs-410,7,2,Overview,"00:09:59,850","00:10:03,796",125,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=599,We actually hope to cover,pic_cs-410_7_2_540.jpg
cs-410_7_2_126,cs-410,7,2,Overview,"00:10:06,675","00:10:11,321",126,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=606,First we're going to cover,pic_cs-410_7_2_600.jpg
cs-410_7_2_127,cs-410,7,2,Overview,"00:10:11,321","00:10:16,053",127,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=611,briefly because this has to do,pic_cs-410_7_2_600.jpg
cs-410_7_2_128,cs-410,7,2,Overview,"00:10:16,053","00:10:21,580",128,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=616,this determines how we can represent,pic_cs-410_7_2_600.jpg
cs-410_7_2_129,cs-410,7,2,Overview,"00:10:21,580","00:10:27,870",129,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=621,"Second, we're going to talk about how to",pic_cs-410_7_2_600.jpg
cs-410_7_2_130,cs-410,7,2,Overview,"00:10:27,870","00:10:34,340",130,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=627,And word associations is a form of use for,pic_cs-410_7_2_600.jpg
cs-410_7_2_131,cs-410,7,2,Overview,"00:10:34,340","00:10:38,340",131,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=634,"Third, we're going to talk about",pic_cs-410_7_2_600.jpg
cs-410_7_2_132,cs-410,7,2,Overview,"00:10:38,340","00:10:43,190",132,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=638,And this is only one way to,pic_cs-410_7_2_600.jpg
cs-410_7_2_133,cs-410,7,2,Overview,"00:10:43,190","00:10:46,100",133,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=643,it's a very useful ways,pic_cs-410_7_2_600.jpg
cs-410_7_2_134,cs-410,7,2,Overview,"00:10:46,100","00:10:51,400",134,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=646,It's also one of the most useful,pic_cs-410_7_2_600.jpg
cs-410_7_2_135,cs-410,7,2,Overview,"00:10:53,750","00:10:59,510",135,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=653,Then we're going to talk about,pic_cs-410_7_2_600.jpg
cs-410_7_2_136,cs-410,7,2,Overview,"00:10:59,510","00:11:05,250",136,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=659,So this can be regarded as one example,pic_cs-410_7_2_600.jpg
cs-410_7_2_137,cs-410,7,2,Overview,"00:11:07,140","00:11:11,510",137,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=667,And finally we're going to,pic_cs-410_7_2_660.jpg
cs-410_7_2_138,cs-410,7,2,Overview,"00:11:11,510","00:11:16,020",138,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=671,problems where we try to predict some,pic_cs-410_7_2_660.jpg
cs-410_7_2_139,cs-410,7,2,Overview,"00:11:17,400","00:11:24,880",139,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=677,So this slide also serves as,pic_cs-410_7_2_660.jpg
cs-410_7_2_140,cs-410,7,2,Overview,"00:11:24,880","00:11:27,554",140,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=684,And we're going to use,pic_cs-410_7_2_660.jpg
cs-410_7_2_141,cs-410,7,2,Overview,"00:11:27,554","00:11:30,962",141,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=687,the topics that we'll cover,pic_cs-410_7_2_660.jpg
cs-410_7_2_142,cs-410,7,2,Overview,"00:11:30,962","00:11:40,962",142,https://www.coursera.org/learn/cs-410/lecture/hgSh4?t=690,[MUSIC],pic_cs-410_7_2_660.jpg
cs-410_7_3_1,cs-410,7,3,Natural,"00:00:00,300","00:00:03,380",1,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=0,[SOUND],pic_cs-410_7_3_0.jpg
cs-410_7_3_2,cs-410,7,3,Natural,"00:00:09,170","00:00:13,893",2,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=9,This lecture is about natural language,pic_cs-410_7_3_0.jpg
cs-410_7_3_3,cs-410,7,3,Natural,"00:00:13,893","00:00:16,330",3,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=13,content analysis.,pic_cs-410_7_3_0.jpg
cs-410_7_3_4,cs-410,7,3,Natural,"00:00:16,330","00:00:21,510",4,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=16,Natural language content analysis,pic_cs-410_7_3_0.jpg
cs-410_7_3_5,cs-410,7,3,Natural,"00:00:21,510","00:00:23,780",5,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=21,So we're going to first talk about this.,pic_cs-410_7_3_0.jpg
cs-410_7_3_6,cs-410,7,3,Natural,"00:00:24,980","00:00:26,330",6,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=24,"And in particular,",pic_cs-410_7_3_0.jpg
cs-410_7_3_7,cs-410,7,3,Natural,"00:00:26,330","00:00:31,540",7,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=26,natural language processing with,pic_cs-410_7_3_0.jpg
cs-410_7_3_8,cs-410,7,3,Natural,"00:00:33,210","00:00:38,230",8,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=33,And this determines what algorithms can,pic_cs-410_7_3_0.jpg
cs-410_7_3_9,cs-410,7,3,Natural,"00:00:40,820","00:00:44,991",9,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=40,We're going to take a look at the basic,pic_cs-410_7_3_0.jpg
cs-410_7_3_10,cs-410,7,3,Natural,"00:00:46,330","00:00:48,970",10,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=46,And I'm going to explain these concepts,pic_cs-410_7_3_0.jpg
cs-410_7_3_11,cs-410,7,3,Natural,"00:00:48,970","00:00:52,600",11,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=48,using a similar example,pic_cs-410_7_3_0.jpg
cs-410_7_3_12,cs-410,7,3,Natural,"00:00:52,600","00:00:55,650",12,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=52,A dog is chasing a boy on the playground.,pic_cs-410_7_3_0.jpg
cs-410_7_3_13,cs-410,7,3,Natural,"00:00:55,650","00:00:58,310",13,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=55,Now this is a very simple sentence.,pic_cs-410_7_3_0.jpg
cs-410_7_3_14,cs-410,7,3,Natural,"00:00:58,310","00:01:01,160",14,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=58,When we read such a sentence,pic_cs-410_7_3_0.jpg
cs-410_7_3_15,cs-410,7,3,Natural,"00:01:01,160","00:01:05,200",15,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=61,about it to get the meaning of it.,pic_cs-410_7_3_60.jpg
cs-410_7_3_16,cs-410,7,3,Natural,"00:01:05,200","00:01:09,460",16,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=65,But when a computer has to,pic_cs-410_7_3_60.jpg
cs-410_7_3_17,cs-410,7,3,Natural,"00:01:09,460","00:01:12,340",17,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=69,the computer has to go,pic_cs-410_7_3_60.jpg
cs-410_7_3_18,cs-410,7,3,Natural,"00:01:13,430","00:01:16,532",18,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=73,"First, the computer needs",pic_cs-410_7_3_60.jpg
cs-410_7_3_19,cs-410,7,3,Natural,"00:01:16,532","00:01:18,630",19,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=76,how to segment the words in English.,pic_cs-410_7_3_60.jpg
cs-410_7_3_20,cs-410,7,3,Natural,"00:01:18,630","00:01:22,010",20,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=78,"And this is very easy,",pic_cs-410_7_3_60.jpg
cs-410_7_3_21,cs-410,7,3,Natural,"00:01:22,010","00:01:26,136",21,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=82,And then the computer will need,pic_cs-410_7_3_60.jpg
cs-410_7_3_22,cs-410,7,3,Natural,"00:01:26,136","00:01:27,870",22,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=86,syntactical categories.,pic_cs-410_7_3_60.jpg
cs-410_7_3_23,cs-410,7,3,Natural,"00:01:27,870","00:01:34,510",23,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=87,"So for example, dog is a noun,",pic_cs-410_7_3_60.jpg
cs-410_7_3_24,cs-410,7,3,Natural,"00:01:34,510","00:01:37,350",24,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=94,And this is called a Lexical analysis.,pic_cs-410_7_3_60.jpg
cs-410_7_3_25,cs-410,7,3,Natural,"00:01:37,350","00:01:41,590",25,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=97,"In particular, tagging these words",pic_cs-410_7_3_60.jpg
cs-410_7_3_26,cs-410,7,3,Natural,"00:01:41,590","00:01:43,270",26,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=101,is called a part-of-speech tagging.,pic_cs-410_7_3_60.jpg
cs-410_7_3_27,cs-410,7,3,Natural,"00:01:45,030","00:01:48,383",27,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=105,After that the computer also needs to,pic_cs-410_7_3_60.jpg
cs-410_7_3_28,cs-410,7,3,Natural,"00:01:48,383","00:01:49,040",28,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=108,these words.,pic_cs-410_7_3_60.jpg
cs-410_7_3_29,cs-410,7,3,Natural,"00:01:49,040","00:01:53,300",29,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=109,So a and dog would form a noun phrase.,pic_cs-410_7_3_60.jpg
cs-410_7_3_30,cs-410,7,3,Natural,"00:01:53,300","00:01:57,590",30,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=113,On the playground would be,pic_cs-410_7_3_60.jpg
cs-410_7_3_31,cs-410,7,3,Natural,"00:01:57,590","00:02:01,378",31,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=117,And there is certain way for,pic_cs-410_7_3_60.jpg
cs-410_7_3_32,cs-410,7,3,Natural,"00:02:01,378","00:02:03,620",32,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=121,them to create meaning.,pic_cs-410_7_3_120.jpg
cs-410_7_3_33,cs-410,7,3,Natural,"00:02:03,620","00:02:06,469",33,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=123,Some other combinations,pic_cs-410_7_3_120.jpg
cs-410_7_3_34,cs-410,7,3,Natural,"00:02:07,720","00:02:12,620",34,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=127,"And this is called syntactical parsing, or",pic_cs-410_7_3_120.jpg
cs-410_7_3_35,cs-410,7,3,Natural,"00:02:12,620","00:02:17,090",35,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=132,"syntactical analysis,",pic_cs-410_7_3_120.jpg
cs-410_7_3_36,cs-410,7,3,Natural,"00:02:17,090","00:02:21,180",36,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=137,The outcome is a parse tree,pic_cs-410_7_3_120.jpg
cs-410_7_3_37,cs-410,7,3,Natural,"00:02:21,180","00:02:24,050",37,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=141,That tells us the structure,pic_cs-410_7_3_120.jpg
cs-410_7_3_38,cs-410,7,3,Natural,"00:02:24,050","00:02:27,430",38,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=144,that we know how we can,pic_cs-410_7_3_120.jpg
cs-410_7_3_39,cs-410,7,3,Natural,"00:02:27,430","00:02:29,740",39,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=147,But this is not semantics yet.,pic_cs-410_7_3_120.jpg
cs-410_7_3_40,cs-410,7,3,Natural,"00:02:29,740","00:02:34,530",40,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=149,So in order to get the meaning we,pic_cs-410_7_3_120.jpg
cs-410_7_3_41,cs-410,7,3,Natural,"00:02:34,530","00:02:39,860",41,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=154,these structures into some real world,pic_cs-410_7_3_120.jpg
cs-410_7_3_42,cs-410,7,3,Natural,"00:02:39,860","00:02:45,500",42,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=159,"So dog is a concept that we know,",pic_cs-410_7_3_120.jpg
cs-410_7_3_43,cs-410,7,3,Natural,"00:02:45,500","00:02:50,870",43,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=165,So connecting these phrases,pic_cs-410_7_3_120.jpg
cs-410_7_3_44,cs-410,7,3,Natural,"00:02:52,160","00:02:58,788",44,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=172,"Now for a computer, would have to formally",pic_cs-410_7_3_120.jpg
cs-410_7_3_45,cs-410,7,3,Natural,"00:02:58,788","00:03:03,630",45,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=178,"So dog, d1 means d1 is a dog.",pic_cs-410_7_3_120.jpg
cs-410_7_3_46,cs-410,7,3,Natural,"00:03:04,690","00:03:09,420",46,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=184,"Boy, b1 means b1 refers to a boy etc.",pic_cs-410_7_3_180.jpg
cs-410_7_3_47,cs-410,7,3,Natural,"00:03:09,420","00:03:13,430",47,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=189,And also represents the chasing,pic_cs-410_7_3_180.jpg
cs-410_7_3_48,cs-410,7,3,Natural,"00:03:13,430","00:03:18,334",48,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=193,"So, chasing is a predicate here with",pic_cs-410_7_3_180.jpg
cs-410_7_3_49,cs-410,7,3,Natural,"00:03:18,334","00:03:23,720",49,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=198,"three arguments, d1, b1, and p1.",pic_cs-410_7_3_180.jpg
cs-410_7_3_50,cs-410,7,3,Natural,"00:03:23,720","00:03:25,920",50,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=203,Which is playground.,pic_cs-410_7_3_180.jpg
cs-410_7_3_51,cs-410,7,3,Natural,"00:03:25,920","00:03:31,320",51,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=205,So this formal rendition of,pic_cs-410_7_3_180.jpg
cs-410_7_3_52,cs-410,7,3,Natural,"00:03:31,320","00:03:35,950",52,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=211,"Once we reach that level of understanding,",pic_cs-410_7_3_180.jpg
cs-410_7_3_53,cs-410,7,3,Natural,"00:03:35,950","00:03:42,050",53,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=215,"For example, if we assume there's a rule",pic_cs-410_7_3_180.jpg
cs-410_7_3_54,cs-410,7,3,Natural,"00:03:42,050","00:03:48,420",54,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=222,"the person can get scared, then we",pic_cs-410_7_3_180.jpg
cs-410_7_3_55,cs-410,7,3,Natural,"00:03:48,420","00:03:52,800",55,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=228,"This is the inferred meaning,",pic_cs-410_7_3_180.jpg
cs-410_7_3_56,cs-410,7,3,Natural,"00:03:52,800","00:03:58,485",56,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=232,"And finally, we might even further infer",pic_cs-410_7_3_180.jpg
cs-410_7_3_57,cs-410,7,3,Natural,"00:03:58,485","00:04:06,170",57,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=238,"what this sentence is requesting,",pic_cs-410_7_3_180.jpg
cs-410_7_3_58,cs-410,7,3,Natural,"00:04:06,170","00:04:12,920",58,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=246,or why the person who say it in,pic_cs-410_7_3_240.jpg
cs-410_7_3_59,cs-410,7,3,Natural,"00:04:12,920","00:04:18,310",59,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=252,"And so, this has to do with",pic_cs-410_7_3_240.jpg
cs-410_7_3_60,cs-410,7,3,Natural,"00:04:18,310","00:04:24,550",60,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=258,This is called speech act analysis or,pic_cs-410_7_3_240.jpg
cs-410_7_3_61,cs-410,7,3,Natural,"00:04:24,550","00:04:27,920",61,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=264,Which first to the use of language.,pic_cs-410_7_3_240.jpg
cs-410_7_3_62,cs-410,7,3,Natural,"00:04:27,920","00:04:32,704",62,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=267,"So, in this case a person saying this",pic_cs-410_7_3_240.jpg
cs-410_7_3_63,cs-410,7,3,Natural,"00:04:32,704","00:04:34,070",63,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=272,bring back the dog.,pic_cs-410_7_3_240.jpg
cs-410_7_3_64,cs-410,7,3,Natural,"00:04:35,240","00:04:42,320",64,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=275,"So this means when saying a sentence,",pic_cs-410_7_3_240.jpg
cs-410_7_3_65,cs-410,7,3,Natural,"00:04:42,320","00:04:44,769",65,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=282,So the action here is to make a request.,pic_cs-410_7_3_240.jpg
cs-410_7_3_66,cs-410,7,3,Natural,"00:04:46,770","00:04:51,408",66,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=286,"Now, this slide clearly shows that",pic_cs-410_7_3_240.jpg
cs-410_7_3_67,cs-410,7,3,Natural,"00:04:51,408","00:04:55,720",67,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=291,a sentence there are a lot of,pic_cs-410_7_3_240.jpg
cs-410_7_3_68,cs-410,7,3,Natural,"00:04:55,720","00:05:00,337",68,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=295,"Now, in general it's very hard for",pic_cs-410_7_3_240.jpg
cs-410_7_3_69,cs-410,7,3,Natural,"00:05:00,337","00:05:04,910",69,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=300,especially if you would want,pic_cs-410_7_3_300.jpg
cs-410_7_3_70,cs-410,7,3,Natural,"00:05:04,910","00:05:06,450",70,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=304,This is very difficult.,pic_cs-410_7_3_300.jpg
cs-410_7_3_71,cs-410,7,3,Natural,"00:05:08,190","00:05:11,094",71,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=308,"Now, the main reason why natural",pic_cs-410_7_3_300.jpg
cs-410_7_3_72,cs-410,7,3,Natural,"00:05:11,094","00:05:14,820",72,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=311,it's because it's designed it will,pic_cs-410_7_3_300.jpg
cs-410_7_3_73,cs-410,7,3,Natural,"00:05:15,990","00:05:20,040",73,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=315,"As a result, for example,",pic_cs-410_7_3_300.jpg
cs-410_7_3_74,cs-410,7,3,Natural,"00:05:21,250","00:05:25,150",74,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=321,Because we assume all of,pic_cs-410_7_3_300.jpg
cs-410_7_3_75,cs-410,7,3,Natural,"00:05:25,150","00:05:28,170",75,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=325,there's no need to encode this knowledge.,pic_cs-410_7_3_300.jpg
cs-410_7_3_76,cs-410,7,3,Natural,"00:05:29,780","00:05:31,360",76,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=329,That makes communication efficient.,pic_cs-410_7_3_300.jpg
cs-410_7_3_77,cs-410,7,3,Natural,"00:05:32,480","00:05:37,460",77,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=332,"We also keep a lot of ambiguities,",pic_cs-410_7_3_300.jpg
cs-410_7_3_78,cs-410,7,3,Natural,"00:05:39,090","00:05:45,130",78,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=339,"And this is again, because we assume we",pic_cs-410_7_3_300.jpg
cs-410_7_3_79,cs-410,7,3,Natural,"00:05:45,130","00:05:48,800",79,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=345,"So, there's no problem with",pic_cs-410_7_3_300.jpg
cs-410_7_3_80,cs-410,7,3,Natural,"00:05:48,800","00:05:50,869",80,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=348,possibly different things,pic_cs-410_7_3_300.jpg
cs-410_7_3_81,cs-410,7,3,Natural,"00:05:52,610","00:05:55,880",81,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=352,Yet for,pic_cs-410_7_3_300.jpg
cs-410_7_3_82,cs-410,7,3,Natural,"00:05:55,880","00:06:00,250",82,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=355,because a computer does not have,pic_cs-410_7_3_300.jpg
cs-410_7_3_83,cs-410,7,3,Natural,"00:06:00,250","00:06:03,620",83,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=360,So the computer will be confused indeed.,pic_cs-410_7_3_360.jpg
cs-410_7_3_84,cs-410,7,3,Natural,"00:06:03,620","00:06:06,980",84,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=363,And this makes it hard for,pic_cs-410_7_3_360.jpg
cs-410_7_3_85,cs-410,7,3,Natural,"00:06:06,980","00:06:09,440",85,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=366,"Indeed, it makes it very hard for",pic_cs-410_7_3_360.jpg
cs-410_7_3_86,cs-410,7,3,Natural,"00:06:09,440","00:06:15,140",86,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=369,every step in the slide,pic_cs-410_7_3_360.jpg
cs-410_7_3_87,cs-410,7,3,Natural,"00:06:16,550","00:06:19,380",87,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=376,Ambiguity is a main killer.,pic_cs-410_7_3_360.jpg
cs-410_7_3_88,cs-410,7,3,Natural,"00:06:19,380","00:06:22,820",88,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=379,Meaning that in every step,pic_cs-410_7_3_360.jpg
cs-410_7_3_89,cs-410,7,3,Natural,"00:06:22,820","00:06:26,790",89,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=382,and the computer would have to,pic_cs-410_7_3_360.jpg
cs-410_7_3_90,cs-410,7,3,Natural,"00:06:26,790","00:06:30,550",90,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=386,that decision can be very difficult,pic_cs-410_7_3_360.jpg
cs-410_7_3_91,cs-410,7,3,Natural,"00:06:31,690","00:06:32,300",91,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=391,"And in general,",pic_cs-410_7_3_360.jpg
cs-410_7_3_92,cs-410,7,3,Natural,"00:06:32,300","00:06:37,530",92,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=392,we need common sense reasoning in order,pic_cs-410_7_3_360.jpg
cs-410_7_3_93,cs-410,7,3,Natural,"00:06:37,530","00:06:40,595",93,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=397,And computers today don't yet have that.,pic_cs-410_7_3_360.jpg
cs-410_7_3_94,cs-410,7,3,Natural,"00:06:40,595","00:06:42,820",94,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=400,That's why it's very hard for,pic_cs-410_7_3_360.jpg
cs-410_7_3_95,cs-410,7,3,Natural,"00:06:42,820","00:06:47,310",95,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=402,computers to precisely understand,pic_cs-410_7_3_360.jpg
cs-410_7_3_96,cs-410,7,3,Natural,"00:06:48,310","00:06:51,280",96,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=408,So here are some specific,pic_cs-410_7_3_360.jpg
cs-410_7_3_97,cs-410,7,3,Natural,"00:06:51,280","00:06:53,390",97,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=411,Think about the world-level ambiguity.,pic_cs-410_7_3_360.jpg
cs-410_7_3_98,cs-410,7,3,Natural,"00:06:53,390","00:06:56,940",98,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=413,A word like design can be a noun or,pic_cs-410_7_3_360.jpg
cs-410_7_3_99,cs-410,7,3,Natural,"00:06:56,940","00:06:59,200",99,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=416,we've got ambiguous part of speech tag.,pic_cs-410_7_3_360.jpg
cs-410_7_3_100,cs-410,7,3,Natural,"00:07:00,980","00:07:06,190",100,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=420,"Root also has multiple meanings,",pic_cs-410_7_3_420.jpg
cs-410_7_3_101,cs-410,7,3,Natural,"00:07:06,190","00:07:10,670",101,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=426,"like in the square of, or",pic_cs-410_7_3_420.jpg
cs-410_7_3_102,cs-410,7,3,Natural,"00:07:12,310","00:07:17,310",102,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=432,Syntactic ambiguity refers,pic_cs-410_7_3_420.jpg
cs-410_7_3_103,cs-410,7,3,Natural,"00:07:19,440","00:07:21,670",103,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=439,of a sentence in terms structures.,pic_cs-410_7_3_420.jpg
cs-410_7_3_104,cs-410,7,3,Natural,"00:07:21,670","00:07:23,010",104,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=441,"So for example,",pic_cs-410_7_3_420.jpg
cs-410_7_3_105,cs-410,7,3,Natural,"00:07:23,010","00:07:26,219",105,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=443,natural language processing can,pic_cs-410_7_3_420.jpg
cs-410_7_3_106,cs-410,7,3,Natural,"00:07:28,240","00:07:33,410",106,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=448,So one is the ordinary meaning that we,pic_cs-410_7_3_420.jpg
cs-410_7_3_107,cs-410,7,3,Natural,"00:07:33,410","00:07:38,690",107,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=453,will be getting as we're,pic_cs-410_7_3_420.jpg
cs-410_7_3_108,cs-410,7,3,Natural,"00:07:38,690","00:07:41,670",108,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=458,"So, it's processing of natural language.",pic_cs-410_7_3_420.jpg
cs-410_7_3_109,cs-410,7,3,Natural,"00:07:41,670","00:07:44,600",109,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=461,But there's is also another,pic_cs-410_7_3_420.jpg
cs-410_7_3_110,cs-410,7,3,Natural,"00:07:44,600","00:07:47,190",110,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=464,which is to say language,pic_cs-410_7_3_420.jpg
cs-410_7_3_111,cs-410,7,3,Natural,"00:07:48,950","00:07:53,500",111,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=468,"Now we don't generally have this problem,",pic_cs-410_7_3_420.jpg
cs-410_7_3_112,cs-410,7,3,Natural,"00:07:53,500","00:07:56,960",112,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=473,"the structure, the computer would have",pic_cs-410_7_3_420.jpg
cs-410_7_3_113,cs-410,7,3,Natural,"00:07:59,040","00:08:03,530",113,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=479,Another classic example is a man,pic_cs-410_7_3_420.jpg
cs-410_7_3_114,cs-410,7,3,Natural,"00:08:03,530","00:08:10,230",114,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=483,And this ambiguity lies in,pic_cs-410_7_3_480.jpg
cs-410_7_3_115,cs-410,7,3,Natural,"00:08:10,230","00:08:13,630",115,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=490,This is called a prepositional,pic_cs-410_7_3_480.jpg
cs-410_7_3_116,cs-410,7,3,Natural,"00:08:14,960","00:08:20,440",116,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=494,Meaning where to attach this,pic_cs-410_7_3_480.jpg
cs-410_7_3_117,cs-410,7,3,Natural,"00:08:20,440","00:08:22,670",117,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=500,Should it modify the boy?,pic_cs-410_7_3_480.jpg
cs-410_7_3_118,cs-410,7,3,Natural,"00:08:22,670","00:08:28,330",118,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=502,"Or should it be modifying, saw, the verb.",pic_cs-410_7_3_480.jpg
cs-410_7_3_119,cs-410,7,3,Natural,"00:08:28,330","00:08:31,330",119,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=508,Another problem is anaphora resolution.,pic_cs-410_7_3_480.jpg
cs-410_7_3_120,cs-410,7,3,Natural,"00:08:31,330","00:08:35,740",120,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=511,In John persuaded Bill to buy a TV for,pic_cs-410_7_3_480.jpg
cs-410_7_3_121,cs-410,7,3,Natural,"00:08:35,740","00:08:37,960",121,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=515,Does himself refer to John or Bill?,pic_cs-410_7_3_480.jpg
cs-410_7_3_122,cs-410,7,3,Natural,"00:08:39,380","00:08:41,790",122,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=519,Presupposition is another difficulty.,pic_cs-410_7_3_480.jpg
cs-410_7_3_123,cs-410,7,3,Natural,"00:08:41,790","00:08:45,459",123,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=521,He has quit smoking implies,pic_cs-410_7_3_480.jpg
cs-410_7_3_124,cs-410,7,3,Natural,"00:08:45,459","00:08:50,180",124,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=525,we need to have such a knowledge in,pic_cs-410_7_3_480.jpg
cs-410_7_3_125,cs-410,7,3,Natural,"00:08:52,630","00:08:57,614",125,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=532,"Because of these problems, the state",pic_cs-410_7_3_480.jpg
cs-410_7_3_126,cs-410,7,3,Natural,"00:08:57,614","00:09:01,410",126,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=537,techniques can not do anything perfectly.,pic_cs-410_7_3_480.jpg
cs-410_7_3_127,cs-410,7,3,Natural,"00:09:01,410","00:09:04,560",127,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=541,Even for,pic_cs-410_7_3_540.jpg
cs-410_7_3_128,cs-410,7,3,Natural,"00:09:04,560","00:09:07,700",128,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=544,we still can not solve the whole problem.,pic_cs-410_7_3_540.jpg
cs-410_7_3_129,cs-410,7,3,Natural,"00:09:07,700","00:09:12,930",129,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=547,"The accuracy that are listed here,",pic_cs-410_7_3_540.jpg
cs-410_7_3_130,cs-410,7,3,Natural,"00:09:12,930","00:09:16,100",130,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=552,was just taken from some studies earlier.,pic_cs-410_7_3_540.jpg
cs-410_7_3_131,cs-410,7,3,Natural,"00:09:17,330","00:09:22,840",131,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=557,And these studies obviously have to,pic_cs-410_7_3_540.jpg
cs-410_7_3_132,cs-410,7,3,Natural,"00:09:22,840","00:09:27,640",132,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=562,the numbers here are not,pic_cs-410_7_3_540.jpg
cs-410_7_3_133,cs-410,7,3,Natural,"00:09:27,640","00:09:33,210",133,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=567,take it out of the context of the data,pic_cs-410_7_3_540.jpg
cs-410_7_3_134,cs-410,7,3,Natural,"00:09:33,210","00:09:39,350",134,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=573,But I show these numbers mainly to give,pic_cs-410_7_3_540.jpg
cs-410_7_3_135,cs-410,7,3,Natural,"00:09:39,350","00:09:42,080",135,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=579,or how well we can do things like this.,pic_cs-410_7_3_540.jpg
cs-410_7_3_136,cs-410,7,3,Natural,"00:09:42,080","00:09:47,670",136,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=582,It doesn't mean any data set,pic_cs-410_7_3_540.jpg
cs-410_7_3_137,cs-410,7,3,Natural,"00:09:47,670","00:09:52,780",137,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=587,"But, in general, we can do parsing speech",pic_cs-410_7_3_540.jpg
cs-410_7_3_138,cs-410,7,3,Natural,"00:09:53,980","00:09:59,030",138,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=593,"Parsing would be more difficult, but for",pic_cs-410_7_3_540.jpg
cs-410_7_3_139,cs-410,7,3,Natural,"00:09:59,030","00:10:04,870",139,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=599,"phrases correct, we can probably",pic_cs-410_7_3_540.jpg
cs-410_7_3_140,cs-410,7,3,Natural,"00:10:06,920","00:10:12,330",140,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=606,But to get the complete parse tree,pic_cs-410_7_3_600.jpg
cs-410_7_3_141,cs-410,7,3,Natural,"00:10:13,610","00:10:18,210",141,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=613,"For semantic analysis, we can also do",pic_cs-410_7_3_600.jpg
cs-410_7_3_142,cs-410,7,3,Natural,"00:10:18,210","00:10:22,570",142,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=618,"particularly, extraction of entities and",pic_cs-410_7_3_600.jpg
cs-410_7_3_143,cs-410,7,3,Natural,"00:10:22,570","00:10:27,910",143,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=622,"For example, recognizing this is",pic_cs-410_7_3_600.jpg
cs-410_7_3_144,cs-410,7,3,Natural,"00:10:27,910","00:10:33,380",144,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=627,this person and,pic_cs-410_7_3_600.jpg
cs-410_7_3_145,cs-410,7,3,Natural,"00:10:33,380","00:10:36,470",145,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=633,We can also do word sense to some extent.,pic_cs-410_7_3_600.jpg
cs-410_7_3_146,cs-410,7,3,Natural,"00:10:38,000","00:10:45,360",146,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=638,The occurrence of root in this sentence,pic_cs-410_7_3_600.jpg
cs-410_7_3_147,cs-410,7,3,Natural,"00:10:45,360","00:10:49,330",147,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=645,Sentiment analysis is another aspect,pic_cs-410_7_3_600.jpg
cs-410_7_3_148,cs-410,7,3,Natural,"00:10:50,480","00:10:55,840",148,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=650,That means we can tag the senses,pic_cs-410_7_3_600.jpg
cs-410_7_3_149,cs-410,7,3,Natural,"00:10:55,840","00:11:00,670",149,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=655,it's talking about the product or,pic_cs-410_7_3_600.jpg
cs-410_7_3_150,cs-410,7,3,Natural,"00:11:02,790","00:11:08,600",150,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=662,"Inference, however, is very hard,",pic_cs-410_7_3_660.jpg
cs-410_7_3_151,cs-410,7,3,Natural,"00:11:08,600","00:11:14,040",151,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=668,any big domain and if it's only,pic_cs-410_7_3_660.jpg
cs-410_7_3_152,cs-410,7,3,Natural,"00:11:14,040","00:11:18,800",152,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=674,And that's a generally difficult,pic_cs-410_7_3_660.jpg
cs-410_7_3_153,cs-410,7,3,Natural,"00:11:18,800","00:11:21,961",153,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=678,Speech act analysis is,pic_cs-410_7_3_660.jpg
cs-410_7_3_154,cs-410,7,3,Natural,"00:11:21,961","00:11:26,480",154,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=681,we can only do this probably for,pic_cs-410_7_3_660.jpg
cs-410_7_3_155,cs-410,7,3,Natural,"00:11:26,480","00:11:32,090",155,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=686,And with a lot of help from humans,pic_cs-410_7_3_660.jpg
cs-410_7_3_156,cs-410,7,3,Natural,"00:11:32,090","00:11:34,180",156,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=692,the computers to learn from.,pic_cs-410_7_3_660.jpg
cs-410_7_3_157,cs-410,7,3,Natural,"00:11:36,380","00:11:38,890",157,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=696,So the slide also shows that,pic_cs-410_7_3_660.jpg
cs-410_7_3_158,cs-410,7,3,Natural,"00:11:38,890","00:11:44,300",158,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=698,computers are far from being able to,pic_cs-410_7_3_660.jpg
cs-410_7_3_159,cs-410,7,3,Natural,"00:11:44,300","00:11:50,320",159,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=704,And that also explains why the text,pic_cs-410_7_3_660.jpg
cs-410_7_3_160,cs-410,7,3,Natural,"00:11:50,320","00:11:54,390",160,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=710,Because we cannot rely on,pic_cs-410_7_3_660.jpg
cs-410_7_3_161,cs-410,7,3,Natural,"00:11:54,390","00:11:58,940",161,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=714,computational methods to,pic_cs-410_7_3_660.jpg
cs-410_7_3_162,cs-410,7,3,Natural,"00:11:58,940","00:12:04,770",162,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=718,"Therefore, we have to use",pic_cs-410_7_3_660.jpg
cs-410_7_3_163,cs-410,7,3,Natural,"00:12:04,770","00:12:10,090",163,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=724,A particular statistical machine learning,pic_cs-410_7_3_720.jpg
cs-410_7_3_164,cs-410,7,3,Natural,"00:12:10,090","00:12:16,092",164,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=730,to try to get as much meaning,pic_cs-410_7_3_720.jpg
cs-410_7_3_165,cs-410,7,3,Natural,"00:12:16,092","00:12:19,320",165,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=736,"And, later you will see",pic_cs-410_7_3_720.jpg
cs-410_7_3_166,cs-410,7,3,Natural,"00:12:20,360","00:12:25,450",166,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=740,many such algorithms,pic_cs-410_7_3_720.jpg
cs-410_7_3_167,cs-410,7,3,Natural,"00:12:25,450","00:12:30,790",167,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=745,interesting model from text even though,pic_cs-410_7_3_720.jpg
cs-410_7_3_168,cs-410,7,3,Natural,"00:12:30,790","00:12:36,010",168,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=750,Meaning of all the natural,pic_cs-410_7_3_720.jpg
cs-410_7_3_169,cs-410,7,3,Natural,"00:12:36,010","00:12:46,010",169,https://www.coursera.org/learn/cs-410/lecture/qNSPo?t=756,[MUSIC],pic_cs-410_7_3_720.jpg
cs-410_7_4_1,cs-410,7,4,Natural,"00:00:00,266","00:00:05,440",1,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=0,[SOUND],pic_cs-410_7_4_0.jpg
cs-410_7_4_2,cs-410,7,4,Natural,"00:00:10,218","00:00:13,988",2,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=10,So here are some specific examples of what,pic_cs-410_7_4_0.jpg
cs-410_7_4_3,cs-410,7,4,Natural,"00:00:13,988","00:00:15,926",3,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=13,we can't do today and,pic_cs-410_7_4_0.jpg
cs-410_7_4_4,cs-410,7,4,Natural,"00:00:15,926","00:00:21,970",4,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=15,part of speech tagging is still,pic_cs-410_7_4_0.jpg
cs-410_7_4_5,cs-410,7,4,Natural,"00:00:21,970","00:00:27,938",5,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=21,"So in the example, he turned off the",pic_cs-410_7_4_0.jpg
cs-410_7_4_6,cs-410,7,4,Natural,"00:00:27,938","00:00:33,342",6,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=27,the two offs actually have somewhat,pic_cs-410_7_4_0.jpg
cs-410_7_4_7,cs-410,7,4,Natural,"00:00:33,342","00:00:39,633",7,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=33,categories and also its very difficult,pic_cs-410_7_4_0.jpg
cs-410_7_4_8,cs-410,7,4,Natural,"00:00:39,633","00:00:44,450",8,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=39,"Again, the example, a man saw a boy",pic_cs-410_7_4_0.jpg
cs-410_7_4_9,cs-410,7,4,Natural,"00:00:44,450","00:00:48,400",9,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=44,be very difficult to parse,pic_cs-410_7_4_0.jpg
cs-410_7_4_10,cs-410,7,4,Natural,"00:00:48,400","00:00:52,278",10,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=48,Precise deep semantic,pic_cs-410_7_4_0.jpg
cs-410_7_4_11,cs-410,7,4,Natural,"00:00:52,278","00:00:55,648",11,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=52,"For example, to define the meaning of own,",pic_cs-410_7_4_0.jpg
cs-410_7_4_12,cs-410,7,4,Natural,"00:00:55,648","00:01:01,493",12,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=55,precisely is very difficult in,pic_cs-410_7_4_0.jpg
cs-410_7_4_13,cs-410,7,4,Natural,"00:01:01,493","00:01:04,737",13,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=61,So the state of the off can,pic_cs-410_7_4_60.jpg
cs-410_7_4_14,cs-410,7,4,Natural,"00:01:04,737","00:01:05,506",14,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=64,Robust and,pic_cs-410_7_4_60.jpg
cs-410_7_4_15,cs-410,7,4,Natural,"00:01:05,506","00:01:11,070",15,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=65,general NLP tends to be shallow while,pic_cs-410_7_4_60.jpg
cs-410_7_4_16,cs-410,7,4,Natural,"00:01:12,430","00:01:18,565",16,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=72,"For this reason in this course,",pic_cs-410_7_4_60.jpg
cs-410_7_4_17,cs-410,7,4,Natural,"00:01:18,565","00:01:23,610",17,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=78,"general, shallow techniques for",pic_cs-410_7_4_60.jpg
cs-410_7_4_18,cs-410,7,4,Natural,"00:01:23,610","00:01:29,630",18,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=83,mining text data and they are generally,pic_cs-410_7_4_60.jpg
cs-410_7_4_19,cs-410,7,4,Natural,"00:01:29,630","00:01:34,510",19,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=89,So there are robust and,pic_cs-410_7_4_60.jpg
cs-410_7_4_20,cs-410,7,4,Natural,"00:01:36,540","00:01:39,550",20,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=96,the in category of shallow analysis.,pic_cs-410_7_4_60.jpg
cs-410_7_4_21,cs-410,7,4,Natural,"00:01:39,550","00:01:44,320",21,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=99,So such techniques have,pic_cs-410_7_4_60.jpg
cs-410_7_4_22,cs-410,7,4,Natural,"00:01:44,320","00:01:49,099",22,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=104,applied to any text data in,pic_cs-410_7_4_60.jpg
cs-410_7_4_23,cs-410,7,4,Natural,"00:01:49,099","00:01:55,425",23,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=109,"But the downside is that, they don't",pic_cs-410_7_4_60.jpg
cs-410_7_4_24,cs-410,7,4,Natural,"00:01:55,425","00:01:59,159",24,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=115,"For that, we have to rely on",pic_cs-410_7_4_60.jpg
cs-410_7_4_25,cs-410,7,4,Natural,"00:02:00,960","00:02:05,930",25,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=120,That typically would require,pic_cs-410_7_4_120.jpg
cs-410_7_4_26,cs-410,7,4,Natural,"00:02:05,930","00:02:10,940",26,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=125,a lot of examples of analysis that would,pic_cs-410_7_4_120.jpg
cs-410_7_4_27,cs-410,7,4,Natural,"00:02:10,940","00:02:16,120",27,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=130,machine learning techniques and learn from,pic_cs-410_7_4_120.jpg
cs-410_7_4_28,cs-410,7,4,Natural,"00:02:16,120","00:02:21,880",28,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=136,"So in practical applications, we generally",pic_cs-410_7_4_120.jpg
cs-410_7_4_29,cs-410,7,4,Natural,"00:02:21,880","00:02:29,150",29,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=141,with the general statistical and,pic_cs-410_7_4_120.jpg
cs-410_7_4_30,cs-410,7,4,Natural,"00:02:29,150","00:02:32,010",30,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=149,These can be applied to any text data.,pic_cs-410_7_4_120.jpg
cs-410_7_4_31,cs-410,7,4,Natural,"00:02:32,010","00:02:37,060",31,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=152,"And on top of that, we're going to use",pic_cs-410_7_4_120.jpg
cs-410_7_4_32,cs-410,7,4,Natural,"00:02:37,060","00:02:42,770",32,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=157,to use supervised machine learning,pic_cs-410_7_4_120.jpg
cs-410_7_4_33,cs-410,7,4,Natural,"00:02:42,770","00:02:48,640",33,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=162,especially for those important,pic_cs-410_7_4_120.jpg
cs-410_7_4_34,cs-410,7,4,Natural,"00:02:48,640","00:02:55,170",34,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=168,to analyze text data more precisely.,pic_cs-410_7_4_120.jpg
cs-410_7_4_35,cs-410,7,4,Natural,"00:02:55,170","00:03:00,036",35,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=175,But this course will cover,pic_cs-410_7_4_120.jpg
cs-410_7_4_36,cs-410,7,4,Natural,"00:03:00,036","00:03:04,177",36,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=180,"that generally,",pic_cs-410_7_4_180.jpg
cs-410_7_4_37,cs-410,7,4,Natural,"00:03:04,177","00:03:09,386",37,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=184,"So they're practically,",pic_cs-410_7_4_180.jpg
cs-410_7_4_38,cs-410,7,4,Natural,"00:03:09,386","00:03:16,409",38,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=189,analysis techniques that require a lot of,pic_cs-410_7_4_180.jpg
cs-410_7_4_39,cs-410,7,4,Natural,"00:03:16,409","00:03:21,302",39,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=196,"So to summarize,",pic_cs-410_7_4_180.jpg
cs-410_7_4_40,cs-410,7,4,Natural,"00:03:21,302","00:03:24,580",40,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=201,is the foundation for text mining.,pic_cs-410_7_4_180.jpg
cs-410_7_4_41,cs-410,7,4,Natural,"00:03:24,580","00:03:27,465",41,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=204,"So obviously, the better we",pic_cs-410_7_4_180.jpg
cs-410_7_4_42,cs-410,7,4,Natural,"00:03:27,465","00:03:29,090",42,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=207,the better we can do text mining.,pic_cs-410_7_4_180.jpg
cs-410_7_4_43,cs-410,7,4,Natural,"00:03:30,420","00:03:34,930",43,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=210,Computers today are far from being able,pic_cs-410_7_4_180.jpg
cs-410_7_4_44,cs-410,7,4,Natural,"00:03:34,930","00:03:38,030",44,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=214,Deep NLP requires common sense,pic_cs-410_7_4_180.jpg
cs-410_7_4_45,cs-410,7,4,Natural,"00:03:38,030","00:03:42,803",45,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=218,"Thus, only working for",pic_cs-410_7_4_180.jpg
cs-410_7_4_46,cs-410,7,4,Natural,"00:03:42,803","00:03:44,833",46,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=222,large scale text mining.,pic_cs-410_7_4_180.jpg
cs-410_7_4_47,cs-410,7,4,Natural,"00:03:44,833","00:03:50,003",47,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=224,Shallow NLP based on statistical,pic_cs-410_7_4_180.jpg
cs-410_7_4_48,cs-410,7,4,Natural,"00:03:50,003","00:03:52,543",48,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=230,is the main topic of this course and,pic_cs-410_7_4_180.jpg
cs-410_7_4_49,cs-410,7,4,Natural,"00:03:52,543","00:03:56,763",49,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=232,they are generally applicable,pic_cs-410_7_4_180.jpg
cs-410_7_4_50,cs-410,7,4,Natural,"00:03:56,763","00:04:02,081",50,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=236,"They are in some sense also,",pic_cs-410_7_4_180.jpg
cs-410_7_4_51,cs-410,7,4,Natural,"00:04:02,081","00:04:06,834",51,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=242,"In practice,",pic_cs-410_7_4_240.jpg
cs-410_7_4_52,cs-410,7,4,Natural,"00:04:06,834","00:04:11,810",52,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=246,we'll have humans for,pic_cs-410_7_4_240.jpg
cs-410_7_4_53,cs-410,7,4,Natural,"00:04:11,810","00:04:21,810",53,https://www.coursera.org/learn/cs-410/lecture/07UZq?t=251,[MUSIC],pic_cs-410_7_4_240.jpg
cs-410_7_5_1,cs-410,7,5,Text,"00:00:06,440","00:00:11,320",1,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=6,This lecture is about the,pic_cs-410_7_5_0.jpg
cs-410_7_5_2,cs-410,7,5,Text,"00:00:12,020","00:00:14,100",2,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=12,"In this lecture, we are going",pic_cs-410_7_5_0.jpg
cs-410_7_5_3,cs-410,7,5,Text,"00:00:14,100","00:00:16,680",3,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=14,to discuss textual,pic_cs-410_7_5_0.jpg
cs-410_7_5_4,cs-410,7,5,Text,"00:00:16,680","00:00:20,475",4,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=16,and discuss how natural,pic_cs-410_7_5_0.jpg
cs-410_7_5_5,cs-410,7,5,Text,"00:00:20,475","00:00:24,450",5,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=20,allow us to represent text,pic_cs-410_7_5_0.jpg
cs-410_7_5_6,cs-410,7,5,Text,"00:00:24,450","00:00:28,590",6,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=24,Let's take a look at this,pic_cs-410_7_5_0.jpg
cs-410_7_5_7,cs-410,7,5,Text,"00:00:28,590","00:00:33,900",7,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=28,We can represent this sentence,pic_cs-410_7_5_0.jpg
cs-410_7_5_8,cs-410,7,5,Text,"00:00:33,900","00:00:37,680",8,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=33,"First, we can always",pic_cs-410_7_5_0.jpg
cs-410_7_5_9,cs-410,7,5,Text,"00:00:37,680","00:00:42,090",9,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=37,represent such a sentence,pic_cs-410_7_5_0.jpg
cs-410_7_5_10,cs-410,7,5,Text,"00:00:42,090","00:00:45,135",10,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=42,This is true for,pic_cs-410_7_5_0.jpg
cs-410_7_5_11,cs-410,7,5,Text,"00:00:45,135","00:00:49,210",11,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=45,when we store them,pic_cs-410_7_5_0.jpg
cs-410_7_5_12,cs-410,7,5,Text,"00:00:49,310","00:00:53,480",12,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=49,When we store a natural,pic_cs-410_7_5_0.jpg
cs-410_7_5_13,cs-410,7,5,Text,"00:00:53,480","00:00:55,805",13,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=53,"as a string of characters,",pic_cs-410_7_5_0.jpg
cs-410_7_5_14,cs-410,7,5,Text,"00:00:55,805","00:01:00,350",14,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=55,we have perhaps the most general,pic_cs-410_7_5_0.jpg
cs-410_7_5_15,cs-410,7,5,Text,"00:01:00,350","00:01:02,270",15,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=60,since we always use,pic_cs-410_7_5_60.jpg
cs-410_7_5_16,cs-410,7,5,Text,"00:01:02,270","00:01:05,360",16,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=62,this approach to,pic_cs-410_7_5_60.jpg
cs-410_7_5_17,cs-410,7,5,Text,"00:01:05,360","00:01:10,070",17,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=65,"But unfortunately, using",pic_cs-410_7_5_60.jpg
cs-410_7_5_18,cs-410,7,5,Text,"00:01:10,070","00:01:12,950",18,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=70,"help us to do semantic analysis,",pic_cs-410_7_5_60.jpg
cs-410_7_5_19,cs-410,7,5,Text,"00:01:12,950","00:01:14,135",19,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=72,which is often needed,pic_cs-410_7_5_60.jpg
cs-410_7_5_20,cs-410,7,5,Text,"00:01:14,135","00:01:17,600",20,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=74,for many applications,pic_cs-410_7_5_60.jpg
cs-410_7_5_21,cs-410,7,5,Text,"00:01:17,600","00:01:21,650",21,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=77,The reason is because we're,pic_cs-410_7_5_60.jpg
cs-410_7_5_22,cs-410,7,5,Text,"00:01:21,650","00:01:22,880",22,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=81,"So as a string,",pic_cs-410_7_5_60.jpg
cs-410_7_5_23,cs-410,7,5,Text,"00:01:22,880","00:01:25,100",23,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=82,we're going to keep,pic_cs-410_7_5_60.jpg
cs-410_7_5_24,cs-410,7,5,Text,"00:01:25,100","00:01:29,005",24,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=85,and these ASCII symbols.,pic_cs-410_7_5_60.jpg
cs-410_7_5_25,cs-410,7,5,Text,"00:01:29,005","00:01:32,090",25,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=89,We can perhaps count what's,pic_cs-410_7_5_60.jpg
cs-410_7_5_26,cs-410,7,5,Text,"00:01:32,090","00:01:35,225",26,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=92,the most frequent character,pic_cs-410_7_5_60.jpg
cs-410_7_5_27,cs-410,7,5,Text,"00:01:35,225","00:01:38,960",27,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=95,or the correlation,pic_cs-410_7_5_60.jpg
cs-410_7_5_28,cs-410,7,5,Text,"00:01:38,960","00:01:43,075",28,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=98,but we can't really,pic_cs-410_7_5_60.jpg
cs-410_7_5_29,cs-410,7,5,Text,"00:01:43,075","00:01:47,300",29,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=103,"Yet, this is the most",pic_cs-410_7_5_60.jpg
cs-410_7_5_30,cs-410,7,5,Text,"00:01:47,300","00:01:49,250",30,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=107,text because we can use,pic_cs-410_7_5_60.jpg
cs-410_7_5_31,cs-410,7,5,Text,"00:01:49,250","00:01:53,045",31,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=109,this to represent any,pic_cs-410_7_5_60.jpg
cs-410_7_5_32,cs-410,7,5,Text,"00:01:53,045","00:01:55,220",32,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=113,If we try to do,pic_cs-410_7_5_60.jpg
cs-410_7_5_33,cs-410,7,5,Text,"00:01:55,220","00:01:57,635",33,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=115,a little bit more natural,pic_cs-410_7_5_60.jpg
cs-410_7_5_34,cs-410,7,5,Text,"00:01:57,635","00:02:00,540",34,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=117,"by doing word segmentation,",pic_cs-410_7_5_60.jpg
cs-410_7_5_35,cs-410,7,5,Text,"00:02:00,540","00:02:05,210",35,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=120,then we can obtain a,pic_cs-410_7_5_120.jpg
cs-410_7_5_36,cs-410,7,5,Text,"00:02:05,210","00:02:08,315",36,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=125,but in the form of a,pic_cs-410_7_5_120.jpg
cs-410_7_5_37,cs-410,7,5,Text,"00:02:08,315","00:02:11,750",37,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=128,So here we see that,pic_cs-410_7_5_120.jpg
cs-410_7_5_38,cs-410,7,5,Text,"00:02:11,750","00:02:17,100",38,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=131,words like a dog is chasing etc.,pic_cs-410_7_5_120.jpg
cs-410_7_5_39,cs-410,7,5,Text,"00:02:17,230","00:02:20,600",39,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=137,Now with this level,pic_cs-410_7_5_120.jpg
cs-410_7_5_40,cs-410,7,5,Text,"00:02:20,600","00:02:23,965",40,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=140,we certainly can do,pic_cs-410_7_5_120.jpg
cs-410_7_5_41,cs-410,7,5,Text,"00:02:23,965","00:02:27,065",41,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=143,and this is mainly because,pic_cs-410_7_5_120.jpg
cs-410_7_5_42,cs-410,7,5,Text,"00:02:27,065","00:02:30,275",42,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=147,of human communication,pic_cs-410_7_5_120.jpg
cs-410_7_5_43,cs-410,7,5,Text,"00:02:30,275","00:02:33,035",43,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=150,so they are very powerful.,pic_cs-410_7_5_120.jpg
cs-410_7_5_44,cs-410,7,5,Text,"00:02:33,035","00:02:36,080",44,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=153,"By identifying words, we can for",pic_cs-410_7_5_120.jpg
cs-410_7_5_45,cs-410,7,5,Text,"00:02:36,080","00:02:38,780",45,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=156,example easily count what are,pic_cs-410_7_5_120.jpg
cs-410_7_5_46,cs-410,7,5,Text,"00:02:38,780","00:02:40,820",46,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=158,the most frequent words in,pic_cs-410_7_5_120.jpg
cs-410_7_5_47,cs-410,7,5,Text,"00:02:40,820","00:02:45,185",47,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=160,this document or in,pic_cs-410_7_5_120.jpg
cs-410_7_5_48,cs-410,7,5,Text,"00:02:45,185","00:02:48,200",48,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=165,These words can be used to form,pic_cs-410_7_5_120.jpg
cs-410_7_5_49,cs-410,7,5,Text,"00:02:48,200","00:02:51,940",49,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=168,topics when we combine,pic_cs-410_7_5_120.jpg
cs-410_7_5_50,cs-410,7,5,Text,"00:02:51,940","00:02:54,060",50,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=171,"and some words are positive,",pic_cs-410_7_5_120.jpg
cs-410_7_5_51,cs-410,7,5,Text,"00:02:54,060","00:02:55,700",51,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=174,"some words negative, so we can",pic_cs-410_7_5_120.jpg
cs-410_7_5_52,cs-410,7,5,Text,"00:02:55,700","00:02:58,410",52,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=175,also do sentiment analysis.,pic_cs-410_7_5_120.jpg
cs-410_7_5_53,cs-410,7,5,Text,"00:02:58,660","00:03:02,690",53,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=178,So representing text data,pic_cs-410_7_5_120.jpg
cs-410_7_5_54,cs-410,7,5,Text,"00:03:02,690","00:03:06,775",54,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=182,opens up a lot of interesting,pic_cs-410_7_5_180.jpg
cs-410_7_5_55,cs-410,7,5,Text,"00:03:06,775","00:03:09,060",55,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=186,"However, this level of",pic_cs-410_7_5_180.jpg
cs-410_7_5_56,cs-410,7,5,Text,"00:03:09,060","00:03:12,080",56,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=189,representation is slightly,pic_cs-410_7_5_180.jpg
cs-410_7_5_57,cs-410,7,5,Text,"00:03:12,080","00:03:17,300",57,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=192,of characters because in,pic_cs-410_7_5_180.jpg
cs-410_7_5_58,cs-410,7,5,Text,"00:03:17,300","00:03:21,350",58,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=197,it's actually not,pic_cs-410_7_5_180.jpg
cs-410_7_5_59,cs-410,7,5,Text,"00:03:21,350","00:03:25,010",59,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=201,all the word boundaries,pic_cs-410_7_5_180.jpg
cs-410_7_5_60,cs-410,7,5,Text,"00:03:25,010","00:03:27,685",60,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=205,you see text as a sequence of,pic_cs-410_7_5_180.jpg
cs-410_7_5_61,cs-410,7,5,Text,"00:03:27,685","00:03:31,345",61,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=207,characters with,pic_cs-410_7_5_180.jpg
cs-410_7_5_62,cs-410,7,5,Text,"00:03:31,345","00:03:33,160",62,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=211,So you'll have to rely on,pic_cs-410_7_5_180.jpg
cs-410_7_5_63,cs-410,7,5,Text,"00:03:33,160","00:03:36,925",63,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=213,some special techniques,pic_cs-410_7_5_180.jpg
cs-410_7_5_64,cs-410,7,5,Text,"00:03:36,925","00:03:39,940",64,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=216,"In such a language,",pic_cs-410_7_5_180.jpg
cs-410_7_5_65,cs-410,7,5,Text,"00:03:39,940","00:03:43,600",65,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=219,we might make mistakes,pic_cs-410_7_5_180.jpg
cs-410_7_5_66,cs-410,7,5,Text,"00:03:43,600","00:03:46,480",66,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=223,So the sequence of,pic_cs-410_7_5_180.jpg
cs-410_7_5_67,cs-410,7,5,Text,"00:03:46,480","00:03:50,230",67,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=226,not as robust as,pic_cs-410_7_5_180.jpg
cs-410_7_5_68,cs-410,7,5,Text,"00:03:50,230","00:03:53,230",68,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=230,"But in English, it's very",pic_cs-410_7_5_180.jpg
cs-410_7_5_69,cs-410,7,5,Text,"00:03:53,230","00:03:56,005",69,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=233,easy to obtain this level,pic_cs-410_7_5_180.jpg
cs-410_7_5_70,cs-410,7,5,Text,"00:03:56,005","00:03:59,270",70,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=236,so we can do that all the time.,pic_cs-410_7_5_180.jpg
cs-410_7_5_71,cs-410,7,5,Text,"00:04:00,860","00:04:03,295",71,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=240,"Now, if we go further",pic_cs-410_7_5_240.jpg
cs-410_7_5_72,cs-410,7,5,Text,"00:04:03,295","00:04:04,645",72,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=243,to do naturally,pic_cs-410_7_5_240.jpg
cs-410_7_5_73,cs-410,7,5,Text,"00:04:04,645","00:04:08,125",73,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=244,we can add a part of speech tags.,pic_cs-410_7_5_240.jpg
cs-410_7_5_74,cs-410,7,5,Text,"00:04:08,125","00:04:09,955",74,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=248,"Now once we do that,",pic_cs-410_7_5_240.jpg
cs-410_7_5_75,cs-410,7,5,Text,"00:04:09,955","00:04:11,850",75,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=249,"we can count, for example,",pic_cs-410_7_5_240.jpg
cs-410_7_5_76,cs-410,7,5,Text,"00:04:11,850","00:04:14,680",76,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=251,the most frequent,pic_cs-410_7_5_240.jpg
cs-410_7_5_77,cs-410,7,5,Text,"00:04:14,680","00:04:18,220",77,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=254,nouns are associated with,pic_cs-410_7_5_240.jpg
cs-410_7_5_78,cs-410,7,5,Text,"00:04:18,220","00:04:19,480",78,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=258,So this opens up,pic_cs-410_7_5_240.jpg
cs-410_7_5_79,cs-410,7,5,Text,"00:04:19,480","00:04:23,020",79,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=259,a little bit more,pic_cs-410_7_5_240.jpg
cs-410_7_5_80,cs-410,7,5,Text,"00:04:23,020","00:04:24,625",80,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=263,for further analysis.,pic_cs-410_7_5_240.jpg
cs-410_7_5_81,cs-410,7,5,Text,"00:04:24,625","00:04:28,270",81,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=264,Note that I use a plus sign,pic_cs-410_7_5_240.jpg
cs-410_7_5_82,cs-410,7,5,Text,"00:04:28,270","00:04:32,305",82,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=268,representing text as a sequence,pic_cs-410_7_5_240.jpg
cs-410_7_5_83,cs-410,7,5,Text,"00:04:32,305","00:04:35,395",83,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=272,we don't necessarily replace,pic_cs-410_7_5_240.jpg
cs-410_7_5_84,cs-410,7,5,Text,"00:04:35,395","00:04:37,840",84,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=275,the original word,pic_cs-410_7_5_240.jpg
cs-410_7_5_85,cs-410,7,5,Text,"00:04:37,840","00:04:40,975",85,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=277,"Instead, we add this as",pic_cs-410_7_5_240.jpg
cs-410_7_5_86,cs-410,7,5,Text,"00:04:40,975","00:04:44,050",86,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=280,an additional way of,pic_cs-410_7_5_240.jpg
cs-410_7_5_87,cs-410,7,5,Text,"00:04:44,050","00:04:47,230",87,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=284,so that now the data is,pic_cs-410_7_5_240.jpg
cs-410_7_5_88,cs-410,7,5,Text,"00:04:47,230","00:04:50,965",88,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=287,of words and a sequence,pic_cs-410_7_5_240.jpg
cs-410_7_5_89,cs-410,7,5,Text,"00:04:50,965","00:04:54,055",89,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=290,This enriches the,pic_cs-410_7_5_240.jpg
cs-410_7_5_90,cs-410,7,5,Text,"00:04:54,055","00:04:59,160",90,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=294,and thus also enables,pic_cs-410_7_5_240.jpg
cs-410_7_5_91,cs-410,7,5,Text,"00:05:00,340","00:05:04,040",91,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=300,"If we go further, then we'll",pic_cs-410_7_5_300.jpg
cs-410_7_5_92,cs-410,7,5,Text,"00:05:04,040","00:05:08,020",92,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=304,often to obtain,pic_cs-410_7_5_300.jpg
cs-410_7_5_93,cs-410,7,5,Text,"00:05:08,020","00:05:09,700",93,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=308,"Now this of course,",pic_cs-410_7_5_300.jpg
cs-410_7_5_94,cs-410,7,5,Text,"00:05:09,700","00:05:12,230",94,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=309,further open up,pic_cs-410_7_5_300.jpg
cs-410_7_5_95,cs-410,7,5,Text,"00:05:12,230","00:05:14,840",95,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=312,"of, for example,",pic_cs-410_7_5_300.jpg
cs-410_7_5_96,cs-410,7,5,Text,"00:05:14,840","00:05:22,520",96,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=314,the writing styles or,pic_cs-410_7_5_300.jpg
cs-410_7_5_97,cs-410,7,5,Text,"00:05:22,520","00:05:26,440",97,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=322,If we go further for,pic_cs-410_7_5_300.jpg
cs-410_7_5_98,cs-410,7,5,Text,"00:05:26,440","00:05:31,684",98,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=326,then we might be able to,pic_cs-410_7_5_300.jpg
cs-410_7_5_99,cs-410,7,5,Text,"00:05:31,684","00:05:35,515",99,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=331,and we also can recognize,pic_cs-410_7_5_300.jpg
cs-410_7_5_100,cs-410,7,5,Text,"00:05:35,515","00:05:38,055",100,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=335,and playground as a location.,pic_cs-410_7_5_300.jpg
cs-410_7_5_101,cs-410,7,5,Text,"00:05:38,055","00:05:41,335",101,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=338,We can further analyze,pic_cs-410_7_5_300.jpg
cs-410_7_5_102,cs-410,7,5,Text,"00:05:41,335","00:05:45,830",102,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=341,dog is chasing the boy and,pic_cs-410_7_5_300.jpg
cs-410_7_5_103,cs-410,7,5,Text,"00:05:45,830","00:05:48,875",103,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=345,Now this will add,pic_cs-410_7_5_300.jpg
cs-410_7_5_104,cs-410,7,5,Text,"00:05:48,875","00:05:52,945",104,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=348,relations through,pic_cs-410_7_5_300.jpg
cs-410_7_5_105,cs-410,7,5,Text,"00:05:52,945","00:05:54,790",105,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=352,"At this level,",pic_cs-410_7_5_300.jpg
cs-410_7_5_106,cs-410,7,5,Text,"00:05:54,790","00:05:57,605",106,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=354,then we can do even more,pic_cs-410_7_5_300.jpg
cs-410_7_5_107,cs-410,7,5,Text,"00:05:57,605","00:05:59,795",107,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=357,"For example, now we",pic_cs-410_7_5_300.jpg
cs-410_7_5_108,cs-410,7,5,Text,"00:05:59,795","00:06:02,360",108,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=359,the most frequent person that's,pic_cs-410_7_5_300.jpg
cs-410_7_5_109,cs-410,7,5,Text,"00:06:02,360","00:06:06,284",109,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=362,mentioning this whole collection,pic_cs-410_7_5_360.jpg
cs-410_7_5_110,cs-410,7,5,Text,"00:06:06,284","00:06:09,205",110,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=366,or whenever you,pic_cs-410_7_5_360.jpg
cs-410_7_5_111,cs-410,7,5,Text,"00:06:09,205","00:06:13,655",111,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=369,you also tend to see mentioning,pic_cs-410_7_5_360.jpg
cs-410_7_5_112,cs-410,7,5,Text,"00:06:13,655","00:06:19,480",112,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=373,So this is a very,pic_cs-410_7_5_360.jpg
cs-410_7_5_113,cs-410,7,5,Text,"00:06:19,480","00:06:21,830",113,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=379,and it's also related to,pic_cs-410_7_5_360.jpg
cs-410_7_5_114,cs-410,7,5,Text,"00:06:21,830","00:06:24,500",114,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=381,the knowledge graph that,pic_cs-410_7_5_360.jpg
cs-410_7_5_115,cs-410,7,5,Text,"00:06:24,500","00:06:27,620",115,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=384,of that Google is doing as,pic_cs-410_7_5_360.jpg
cs-410_7_5_116,cs-410,7,5,Text,"00:06:27,620","00:06:31,690",116,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=387,a more semantic way of,pic_cs-410_7_5_360.jpg
cs-410_7_5_117,cs-410,7,5,Text,"00:06:31,690","00:06:39,080",117,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=391,"However, it's also less robust",pic_cs-410_7_5_360.jpg
cs-410_7_5_118,cs-410,7,5,Text,"00:06:39,080","00:06:42,410",118,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=399,even syntactical analysis,pic_cs-410_7_5_360.jpg
cs-410_7_5_119,cs-410,7,5,Text,"00:06:42,410","00:06:43,985",119,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=402,always easy to identify,pic_cs-410_7_5_360.jpg
cs-410_7_5_120,cs-410,7,5,Text,"00:06:43,985","00:06:46,160",120,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=403,all the entities with,pic_cs-410_7_5_360.jpg
cs-410_7_5_121,cs-410,7,5,Text,"00:06:46,160","00:06:47,735",121,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=406,"and we might make mistakes,",pic_cs-410_7_5_360.jpg
cs-410_7_5_122,cs-410,7,5,Text,"00:06:47,735","00:06:50,270",122,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=407,and relations are,pic_cs-410_7_5_360.jpg
cs-410_7_5_123,cs-410,7,5,Text,"00:06:50,270","00:06:52,685",123,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=410,and we might make mistakes.,pic_cs-410_7_5_360.jpg
cs-410_7_5_124,cs-410,7,5,Text,"00:06:52,685","00:06:56,120",124,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=412,So this makes this level of,pic_cs-410_7_5_360.jpg
cs-410_7_5_125,cs-410,7,5,Text,"00:06:56,120","00:06:58,190",125,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=416,yet it's very useful.,pic_cs-410_7_5_360.jpg
cs-410_7_5_126,cs-410,7,5,Text,"00:06:58,190","00:07:01,700",126,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=418,Now if we move further,pic_cs-410_7_5_360.jpg
cs-410_7_5_127,cs-410,7,5,Text,"00:07:01,700","00:07:05,465",127,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=421,then we can have predicates,pic_cs-410_7_5_420.jpg
cs-410_7_5_128,cs-410,7,5,Text,"00:07:05,465","00:07:08,630",128,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=425,"With inference rules, we can",pic_cs-410_7_5_420.jpg
cs-410_7_5_129,cs-410,7,5,Text,"00:07:08,630","00:07:13,700",129,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=428,infer interesting derived,pic_cs-410_7_5_420.jpg
cs-410_7_5_130,cs-410,7,5,Text,"00:07:13,700","00:07:15,020",130,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=433,so that's very useful.,pic_cs-410_7_5_420.jpg
cs-410_7_5_131,cs-410,7,5,Text,"00:07:15,020","00:07:17,420",131,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=435,"But unfortunately,",pic_cs-410_7_5_420.jpg
cs-410_7_5_132,cs-410,7,5,Text,"00:07:17,420","00:07:19,940",132,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=437,representation is even less,pic_cs-410_7_5_420.jpg
cs-410_7_5_133,cs-410,7,5,Text,"00:07:19,940","00:07:22,010",133,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=439,robust and we can make,pic_cs-410_7_5_420.jpg
cs-410_7_5_134,cs-410,7,5,Text,"00:07:22,010","00:07:25,120",134,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=442,mistakes and we can't do,pic_cs-410_7_5_420.jpg
cs-410_7_5_135,cs-410,7,5,Text,"00:07:25,120","00:07:28,885",135,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=445,that all the time for,pic_cs-410_7_5_420.jpg
cs-410_7_5_136,cs-410,7,5,Text,"00:07:28,885","00:07:33,820",136,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=448,"Finally, speech acts would",pic_cs-410_7_5_420.jpg
cs-410_7_5_137,cs-410,7,5,Text,"00:07:33,820","00:07:38,605",137,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=453,of repetition of the intent,pic_cs-410_7_5_420.jpg
cs-410_7_5_138,cs-410,7,5,Text,"00:07:38,605","00:07:40,000",138,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=458,"So in this case,",pic_cs-410_7_5_420.jpg
cs-410_7_5_139,cs-410,7,5,Text,"00:07:40,000","00:07:41,485",139,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=460,it might be a request.,pic_cs-410_7_5_420.jpg
cs-410_7_5_140,cs-410,7,5,Text,"00:07:41,485","00:07:44,650",140,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=461,So knowing that would,pic_cs-410_7_5_420.jpg
cs-410_7_5_141,cs-410,7,5,Text,"00:07:44,650","00:07:47,140",141,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=464,even more interesting,pic_cs-410_7_5_420.jpg
cs-410_7_5_142,cs-410,7,5,Text,"00:07:47,140","00:07:51,765",142,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=467,this observer or the author,pic_cs-410_7_5_420.jpg
cs-410_7_5_143,cs-410,7,5,Text,"00:07:51,765","00:07:53,895",143,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=471,What's the intention,pic_cs-410_7_5_420.jpg
cs-410_7_5_144,cs-410,7,5,Text,"00:07:53,895","00:07:57,210",144,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=473,What's scenarios? What kind,pic_cs-410_7_5_420.jpg
cs-410_7_5_145,cs-410,7,5,Text,"00:07:57,210","00:08:02,740",145,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=477,So this is another level,pic_cs-410_7_5_420.jpg
cs-410_7_5_146,cs-410,7,5,Text,"00:08:02,740","00:08:05,755",146,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=482,of analysis that would,pic_cs-410_7_5_480.jpg
cs-410_7_5_147,cs-410,7,5,Text,"00:08:05,755","00:08:10,250",147,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=485,So this picture shows,pic_cs-410_7_5_480.jpg
cs-410_7_5_148,cs-410,7,5,Text,"00:08:10,250","00:08:12,530",148,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=490,we generally see,pic_cs-410_7_5_480.jpg
cs-410_7_5_149,cs-410,7,5,Text,"00:08:12,530","00:08:15,535",149,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=492,natural language processing,pic_cs-410_7_5_480.jpg
cs-410_7_5_150,cs-410,7,5,Text,"00:08:15,535","00:08:18,080",150,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=495,"Unfortunately,",pic_cs-410_7_5_480.jpg
cs-410_7_5_151,cs-410,7,5,Text,"00:08:18,080","00:08:20,330",151,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=498,"require more human effort,",pic_cs-410_7_5_480.jpg
cs-410_7_5_152,cs-410,7,5,Text,"00:08:20,330","00:08:23,060",152,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=500,and they are less accurate.,pic_cs-410_7_5_480.jpg
cs-410_7_5_153,cs-410,7,5,Text,"00:08:23,060","00:08:26,570",153,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=503,That means there are mistakes.,pic_cs-410_7_5_480.jpg
cs-410_7_5_154,cs-410,7,5,Text,"00:08:26,570","00:08:29,945",154,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=506,So if we add an texts that are at,pic_cs-410_7_5_480.jpg
cs-410_7_5_155,cs-410,7,5,Text,"00:08:29,945","00:08:32,240",155,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=509,the levels that are,pic_cs-410_7_5_480.jpg
cs-410_7_5_156,cs-410,7,5,Text,"00:08:32,240","00:08:34,970",156,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=512,representing deeper,pic_cs-410_7_5_480.jpg
cs-410_7_5_157,cs-410,7,5,Text,"00:08:34,970","00:08:37,790",157,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=514,then we have to,pic_cs-410_7_5_480.jpg
cs-410_7_5_158,cs-410,7,5,Text,"00:08:37,790","00:08:42,380",158,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=517,So that also means it's,pic_cs-410_7_5_480.jpg
cs-410_7_5_159,cs-410,7,5,Text,"00:08:42,380","00:08:46,835",159,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=522,such deep analysis with,pic_cs-410_7_5_480.jpg
cs-410_7_5_160,cs-410,7,5,Text,"00:08:46,835","00:08:48,695",160,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=526,"for example, sequence of words.",pic_cs-410_7_5_480.jpg
cs-410_7_5_161,cs-410,7,5,Text,"00:08:48,695","00:08:50,675",161,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=528,"On the right side,",pic_cs-410_7_5_480.jpg
cs-410_7_5_162,cs-410,7,5,Text,"00:08:50,675","00:08:55,240",162,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=530,you'll see the arrow points,pic_cs-410_7_5_480.jpg
cs-410_7_5_163,cs-410,7,5,Text,"00:08:55,240","00:08:56,960",163,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=535,"As we go down,",pic_cs-410_7_5_480.jpg
cs-410_7_5_164,cs-410,7,5,Text,"00:08:56,960","00:08:59,780",164,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=536,we are representation,pic_cs-410_7_5_480.jpg
cs-410_7_5_165,cs-410,7,5,Text,"00:08:59,780","00:09:02,600",165,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=539,to knowledge representation,pic_cs-410_7_5_480.jpg
cs-410_7_5_166,cs-410,7,5,Text,"00:09:02,600","00:09:08,210",166,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=542,and need for solving,pic_cs-410_7_5_540.jpg
cs-410_7_5_167,cs-410,7,5,Text,"00:09:08,210","00:09:11,750",167,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=548,Now this is desirable because as,pic_cs-410_7_5_540.jpg
cs-410_7_5_168,cs-410,7,5,Text,"00:09:11,750","00:09:15,110",168,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=551,we can represent text at,pic_cs-410_7_5_540.jpg
cs-410_7_5_169,cs-410,7,5,Text,"00:09:15,110","00:09:17,315",169,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=555,we can easily extract,pic_cs-410_7_5_540.jpg
cs-410_7_5_170,cs-410,7,5,Text,"00:09:17,315","00:09:19,280",170,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=557,That's the purpose,pic_cs-410_7_5_540.jpg
cs-410_7_5_171,cs-410,7,5,Text,"00:09:19,280","00:09:21,965",171,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=559,So there is a trade-off,pic_cs-410_7_5_540.jpg
cs-410_7_5_172,cs-410,7,5,Text,"00:09:21,965","00:09:24,920",172,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=561,here between doing,pic_cs-410_7_5_540.jpg
cs-410_7_5_173,cs-410,7,5,Text,"00:09:24,920","00:09:27,260",173,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=564,might have errors but would give,pic_cs-410_7_5_540.jpg
cs-410_7_5_174,cs-410,7,5,Text,"00:09:27,260","00:09:31,225",174,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=567,us direct knowledge that,pic_cs-410_7_5_540.jpg
cs-410_7_5_175,cs-410,7,5,Text,"00:09:31,225","00:09:33,910",175,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=571,"Doing shallow analysis, which",pic_cs-410_7_5_540.jpg
cs-410_7_5_176,cs-410,7,5,Text,"00:09:33,910","00:09:37,010",176,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=573,is more robust but,pic_cs-410_7_5_540.jpg
cs-410_7_5_177,cs-410,7,5,Text,"00:09:37,010","00:09:42,665",177,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=577,give us the necessary deeper,pic_cs-410_7_5_540.jpg
cs-410_7_5_178,cs-410,7,5,Text,"00:09:42,665","00:09:45,740",178,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=582,I should also say that,pic_cs-410_7_5_540.jpg
cs-410_7_5_179,cs-410,7,5,Text,"00:09:45,740","00:09:49,085",179,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=585,humans and are meant to,pic_cs-410_7_5_540.jpg
cs-410_7_5_180,cs-410,7,5,Text,"00:09:49,085","00:09:52,340",180,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=589,"So as a result, in",pic_cs-410_7_5_540.jpg
cs-410_7_5_181,cs-410,7,5,Text,"00:09:52,340","00:09:56,090",181,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=592,text-mining humans play,pic_cs-410_7_5_540.jpg
cs-410_7_5_182,cs-410,7,5,Text,"00:09:56,090","00:09:58,010",182,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=596,they are always in the loop.,pic_cs-410_7_5_540.jpg
cs-410_7_5_183,cs-410,7,5,Text,"00:09:58,010","00:10:00,650",183,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=598,Meaning that we should optimize,pic_cs-410_7_5_540.jpg
cs-410_7_5_184,cs-410,7,5,Text,"00:10:00,650","00:10:03,695",184,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=600,the collaboration of,pic_cs-410_7_5_600.jpg
cs-410_7_5_185,cs-410,7,5,Text,"00:10:03,695","00:10:05,540",185,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=603,"So in that sense,",pic_cs-410_7_5_600.jpg
cs-410_7_5_186,cs-410,7,5,Text,"00:10:05,540","00:10:08,480",186,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=605,it's okay that computers,pic_cs-410_7_5_600.jpg
cs-410_7_5_187,cs-410,7,5,Text,"00:10:08,480","00:10:12,920",187,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=608,to have compute accurately,pic_cs-410_7_5_600.jpg
cs-410_7_5_188,cs-410,7,5,Text,"00:10:12,920","00:10:15,290",188,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=612,and the patterns,pic_cs-410_7_5_600.jpg
cs-410_7_5_189,cs-410,7,5,Text,"00:10:15,290","00:10:18,035",189,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=615,from text data can be,pic_cs-410_7_5_600.jpg
cs-410_7_5_190,cs-410,7,5,Text,"00:10:18,035","00:10:20,840",190,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=618,and humans can,pic_cs-410_7_5_600.jpg
cs-410_7_5_191,cs-410,7,5,Text,"00:10:20,840","00:10:24,650",191,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=620,to do more accurate analysis,pic_cs-410_7_5_600.jpg
cs-410_7_5_192,cs-410,7,5,Text,"00:10:24,650","00:10:28,640",192,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=624,by providing features,pic_cs-410_7_5_600.jpg
cs-410_7_5_193,cs-410,7,5,Text,"00:10:28,640","00:10:33,870",193,https://www.coursera.org/learn/cs-410/lecture/6T38K?t=628,learning programs to make,pic_cs-410_7_5_600.jpg
cs-410_7_6_1,cs-410,7,6,Text,"00:00:00,532","00:00:08,683",1,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=0,[SOUND].,pic_cs-410_7_6_0.jpg
cs-410_7_6_2,cs-410,7,6,Text,"00:00:08,683","00:00:11,442",2,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=8,"So, as we explained the different text",pic_cs-410_7_6_0.jpg
cs-410_7_6_3,cs-410,7,6,Text,"00:00:11,442","00:00:15,299",3,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=11,representation tends to,pic_cs-410_7_6_0.jpg
cs-410_7_6_4,cs-410,7,6,Text,"00:00:16,560","00:00:19,780",4,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=16,"In particular,",pic_cs-410_7_6_0.jpg
cs-410_7_6_5,cs-410,7,6,Text,"00:00:19,780","00:00:24,720",5,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=19,more deeper analysis results,pic_cs-410_7_6_0.jpg
cs-410_7_6_6,cs-410,7,6,Text,"00:00:24,720","00:00:27,810",6,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=24,And that would open up a more,pic_cs-410_7_6_0.jpg
cs-410_7_6_7,cs-410,7,6,Text,"00:00:29,520","00:00:33,780",7,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=29,opportunities and,pic_cs-410_7_6_0.jpg
cs-410_7_6_8,cs-410,7,6,Text,"00:00:33,780","00:00:37,470",8,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=33,"So, this table summarizes",pic_cs-410_7_6_0.jpg
cs-410_7_6_9,cs-410,7,6,Text,"00:00:37,470","00:00:39,800",9,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=37,So the first column shows,pic_cs-410_7_6_0.jpg
cs-410_7_6_10,cs-410,7,6,Text,"00:00:39,800","00:00:44,820",10,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=39,The second visualizes the generality,pic_cs-410_7_6_0.jpg
cs-410_7_6_11,cs-410,7,6,Text,"00:00:44,820","00:00:48,430",11,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=44,Meaning whether we can do this,pic_cs-410_7_6_0.jpg
cs-410_7_6_12,cs-410,7,6,Text,"00:00:48,430","00:00:51,880",12,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=48,all the text data or only some of them.,pic_cs-410_7_6_0.jpg
cs-410_7_6_13,cs-410,7,6,Text,"00:00:51,880","00:00:54,970",13,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=51,And the third column shows,pic_cs-410_7_6_0.jpg
cs-410_7_6_14,cs-410,7,6,Text,"00:00:56,040","00:01:00,130",14,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=56,And the final column shows some,pic_cs-410_7_6_0.jpg
cs-410_7_6_15,cs-410,7,6,Text,"00:01:00,130","00:01:04,670",15,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=60,can be achieved through this,pic_cs-410_7_6_60.jpg
cs-410_7_6_16,cs-410,7,6,Text,"00:01:04,670","00:01:06,310",16,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=64,So let's take a look at them.,pic_cs-410_7_6_60.jpg
cs-410_7_6_17,cs-410,7,6,Text,"00:01:06,310","00:01:12,180",17,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=66,So as a stream text can only be processed,pic_cs-410_7_6_60.jpg
cs-410_7_6_18,cs-410,7,6,Text,"00:01:12,180","00:01:14,050",18,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=72,"It's very robust, it's general.",pic_cs-410_7_6_60.jpg
cs-410_7_6_19,cs-410,7,6,Text,"00:01:15,100","00:01:17,690",19,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=75,And there was still some interesting,pic_cs-410_7_6_60.jpg
cs-410_7_6_20,cs-410,7,6,Text,"00:01:17,690","00:01:18,290",20,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=77,at this level.,pic_cs-410_7_6_60.jpg
cs-410_7_6_21,cs-410,7,6,Text,"00:01:18,290","00:01:20,380",21,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=78,"For example, compression of text.",pic_cs-410_7_6_60.jpg
cs-410_7_6_22,cs-410,7,6,Text,"00:01:20,380","00:01:24,080",22,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=80,Doesn't necessarily need to,pic_cs-410_7_6_60.jpg
cs-410_7_6_23,cs-410,7,6,Text,"00:01:24,080","00:01:27,270",23,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=84,Although knowing word boundaries,pic_cs-410_7_6_60.jpg
cs-410_7_6_24,cs-410,7,6,Text,"00:01:28,540","00:01:32,470",24,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=88,Word base repetition is a very,pic_cs-410_7_6_60.jpg
cs-410_7_6_25,cs-410,7,6,Text,"00:01:32,470","00:01:34,630",25,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=92,It's quite general and,pic_cs-410_7_6_60.jpg
cs-410_7_6_26,cs-410,7,6,Text,"00:01:34,630","00:01:39,140",26,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=94,"relatively robust, indicating they",pic_cs-410_7_6_60.jpg
cs-410_7_6_27,cs-410,7,6,Text,"00:01:39,140","00:01:44,480",27,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=99,"Such as word relation analysis,",pic_cs-410_7_6_60.jpg
cs-410_7_6_28,cs-410,7,6,Text,"00:01:44,480","00:01:48,930",28,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=104,And there are many applications that can,pic_cs-410_7_6_60.jpg
cs-410_7_6_29,cs-410,7,6,Text,"00:01:48,930","00:01:54,930",29,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=108,"For example, thesaurus discovery has",pic_cs-410_7_6_60.jpg
cs-410_7_6_30,cs-410,7,6,Text,"00:01:54,930","00:02:00,550",30,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=114,And topic and,pic_cs-410_7_6_60.jpg
cs-410_7_6_31,cs-410,7,6,Text,"00:02:00,550","00:02:03,360",31,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=120,"And there are, for example, people",pic_cs-410_7_6_120.jpg
cs-410_7_6_32,cs-410,7,6,Text,"00:02:03,360","00:02:08,190",32,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=123,might be interesting in knowing the major,pic_cs-410_7_6_120.jpg
cs-410_7_6_33,cs-410,7,6,Text,"00:02:08,190","00:02:12,730",33,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=128,And this can be the case,pic_cs-410_7_6_120.jpg
cs-410_7_6_34,cs-410,7,6,Text,"00:02:12,730","00:02:18,500",34,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=132,And scientists want to know what are the,pic_cs-410_7_6_120.jpg
cs-410_7_6_35,cs-410,7,6,Text,"00:02:18,500","00:02:22,950",35,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=138,Or customer service people might want to,pic_cs-410_7_6_120.jpg
cs-410_7_6_36,cs-410,7,6,Text,"00:02:22,950","00:02:28,480",36,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=142,customers by mining their e-mail messages.,pic_cs-410_7_6_120.jpg
cs-410_7_6_37,cs-410,7,6,Text,"00:02:28,480","00:02:33,850",37,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=148,And business intelligence,pic_cs-410_7_6_120.jpg
cs-410_7_6_38,cs-410,7,6,Text,"00:02:33,850","00:02:38,090",38,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=153,understanding consumers' opinions about,pic_cs-410_7_6_120.jpg
cs-410_7_6_39,cs-410,7,6,Text,"00:02:38,090","00:02:42,060",39,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=158,products to figure out what are the,pic_cs-410_7_6_120.jpg
cs-410_7_6_40,cs-410,7,6,Text,"00:02:43,170","00:02:47,140",40,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=163,"And, in general, there are many",pic_cs-410_7_6_120.jpg
cs-410_7_6_41,cs-410,7,6,Text,"00:02:47,140","00:02:51,300",41,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=167,applications that can be enabled by,pic_cs-410_7_6_120.jpg
cs-410_7_6_42,cs-410,7,6,Text,"00:02:53,720","00:02:58,550",42,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=173,"Now, moving down, we'll see we can",pic_cs-410_7_6_120.jpg
cs-410_7_6_43,cs-410,7,6,Text,"00:02:58,550","00:03:01,640",43,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=178,"By adding syntactical structures,",pic_cs-410_7_6_120.jpg
cs-410_7_6_44,cs-410,7,6,Text,"00:03:01,640","00:03:03,890",44,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=181,syntactical graph analysis.,pic_cs-410_7_6_180.jpg
cs-410_7_6_45,cs-410,7,6,Text,"00:03:03,890","00:03:09,490",45,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=183,We can use graph mining algorithms,pic_cs-410_7_6_180.jpg
cs-410_7_6_46,cs-410,7,6,Text,"00:03:09,490","00:03:13,550",46,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=189,And some applications are related,pic_cs-410_7_6_180.jpg
cs-410_7_6_47,cs-410,7,6,Text,"00:03:13,550","00:03:14,090",47,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=193,"For example,",pic_cs-410_7_6_180.jpg
cs-410_7_6_48,cs-410,7,6,Text,"00:03:14,090","00:03:18,440",48,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=194,stylistic analysis generally requires,pic_cs-410_7_6_180.jpg
cs-410_7_6_49,cs-410,7,6,Text,"00:03:22,000","00:03:26,240",49,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=202,We can also generate,pic_cs-410_7_6_180.jpg
cs-410_7_6_50,cs-410,7,6,Text,"00:03:26,240","00:03:32,090",50,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=206,And those are features that might help us,pic_cs-410_7_6_180.jpg
cs-410_7_6_51,cs-410,7,6,Text,"00:03:32,090","00:03:37,320",51,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=212,categories by looking at the structures,pic_cs-410_7_6_180.jpg
cs-410_7_6_52,cs-410,7,6,Text,"00:03:37,320","00:03:39,350",52,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=217,It can be more accurate.,pic_cs-410_7_6_180.jpg
cs-410_7_6_53,cs-410,7,6,Text,"00:03:39,350","00:03:43,360",53,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=219,"For example,",pic_cs-410_7_6_180.jpg
cs-410_7_6_54,cs-410,7,6,Text,"00:03:45,120","00:03:49,298",54,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=225,different categories corresponding,pic_cs-410_7_6_180.jpg
cs-410_7_6_55,cs-410,7,6,Text,"00:03:49,298","00:03:56,320",55,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=229,You want to figure out which of,pic_cs-410_7_6_180.jpg
cs-410_7_6_56,cs-410,7,6,Text,"00:03:56,320","00:04:01,440",56,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=236,"this article, then you generally need",pic_cs-410_7_6_180.jpg
cs-410_7_6_57,cs-410,7,6,Text,"00:04:03,340","00:04:05,400",57,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=243,"When we add entities and relations,",pic_cs-410_7_6_240.jpg
cs-410_7_6_58,cs-410,7,6,Text,"00:04:05,400","00:04:09,690",58,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=245,then we can enable other techniques,pic_cs-410_7_6_240.jpg
cs-410_7_6_59,cs-410,7,6,Text,"00:04:09,690","00:04:13,920",59,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=249,"answers, or information network and",pic_cs-410_7_6_240.jpg
cs-410_7_6_60,cs-410,7,6,Text,"00:04:13,920","00:04:20,956",60,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=253,And this analysis enable,pic_cs-410_7_6_240.jpg
cs-410_7_6_61,cs-410,7,6,Text,"00:04:22,285","00:04:22,875",61,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=262,"For example,",pic_cs-410_7_6_240.jpg
cs-410_7_6_62,cs-410,7,6,Text,"00:04:22,875","00:04:27,525",62,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=262,discovery of all the knowledge and,pic_cs-410_7_6_240.jpg
cs-410_7_6_63,cs-410,7,6,Text,"00:04:28,865","00:04:31,825",63,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=268,You can also use this level representation,pic_cs-410_7_6_240.jpg
cs-410_7_6_64,cs-410,7,6,Text,"00:04:31,825","00:04:35,820",64,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=271,to integrate everything about,pic_cs-410_7_6_240.jpg
cs-410_7_6_65,cs-410,7,6,Text,"00:04:37,520","00:04:40,280",65,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=277,"Finally, when we add logical predicates,",pic_cs-410_7_6_240.jpg
cs-410_7_6_66,cs-410,7,6,Text,"00:04:40,280","00:04:44,330",66,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=280,"that would enable large inference,",pic_cs-410_7_6_240.jpg
cs-410_7_6_67,cs-410,7,6,Text,"00:04:44,330","00:04:46,190",67,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=284,And this can be very useful for,pic_cs-410_7_6_240.jpg
cs-410_7_6_68,cs-410,7,6,Text,"00:04:46,190","00:04:48,780",68,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=286,integrating analysis of,pic_cs-410_7_6_240.jpg
cs-410_7_6_69,cs-410,7,6,Text,"00:04:50,190","00:04:53,560",69,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=290,"For example,",pic_cs-410_7_6_240.jpg
cs-410_7_6_70,cs-410,7,6,Text,"00:04:54,920","00:04:58,370",70,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=294,"extracted the information from text,",pic_cs-410_7_6_240.jpg
cs-410_7_6_71,cs-410,7,6,Text,"00:04:59,830","00:05:04,470",71,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=299,A good of example of application in this,pic_cs-410_7_6_240.jpg
cs-410_7_6_72,cs-410,7,6,Text,"00:05:04,470","00:05:07,375",72,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=304,is a knowledge assistant for biologists.,pic_cs-410_7_6_300.jpg
cs-410_7_6_73,cs-410,7,6,Text,"00:05:07,375","00:05:14,535",73,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=307,And this program that can help a biologist,pic_cs-410_7_6_300.jpg
cs-410_7_6_74,cs-410,7,6,Text,"00:05:14,535","00:05:21,040",74,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=314,literature about a research problem such,pic_cs-410_7_6_300.jpg
cs-410_7_6_75,cs-410,7,6,Text,"00:05:22,070","00:05:27,143",75,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=322,And the computer can make inferences,pic_cs-410_7_6_300.jpg
cs-410_7_6_76,cs-410,7,6,Text,"00:05:27,143","00:05:32,490",76,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=327,about some of the hypothesis that,pic_cs-410_7_6_300.jpg
cs-410_7_6_77,cs-410,7,6,Text,"00:05:32,490","00:05:36,110",77,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=332,"For example,",pic_cs-410_7_6_300.jpg
cs-410_7_6_78,cs-410,7,6,Text,"00:05:36,110","00:05:42,135",78,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=336,then the intelligent program can read the,pic_cs-410_7_6_300.jpg
cs-410_7_6_79,cs-410,7,6,Text,"00:05:42,135","00:05:45,250",79,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=342,doing compiling and,pic_cs-410_7_6_300.jpg
cs-410_7_6_80,cs-410,7,6,Text,"00:05:45,250","00:05:50,891",80,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=345,And then using a logic system to,pic_cs-410_7_6_300.jpg
cs-410_7_6_81,cs-410,7,6,Text,"00:05:50,891","00:05:56,060",81,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=350,to researchers questioning about what,pic_cs-410_7_6_300.jpg
cs-410_7_6_82,cs-410,7,6,Text,"00:05:57,990","00:06:01,240",82,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=357,So in order to support,pic_cs-410_7_6_300.jpg
cs-410_7_6_83,cs-410,7,6,Text,"00:06:01,240","00:06:04,910",83,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=361,we need to go as far as,pic_cs-410_7_6_360.jpg
cs-410_7_6_84,cs-410,7,6,Text,"00:06:04,910","00:06:10,585",84,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=364,"Now, this course is covering techniques",pic_cs-410_7_6_360.jpg
cs-410_7_6_85,cs-410,7,6,Text,"00:06:12,090","00:06:14,610",85,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=372,And these techniques are general and,pic_cs-410_7_6_360.jpg
cs-410_7_6_86,cs-410,7,6,Text,"00:06:14,610","00:06:19,460",86,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=374,robust and that's more widely,pic_cs-410_7_6_360.jpg
cs-410_7_6_87,cs-410,7,6,Text,"00:06:21,120","00:06:26,565",87,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=381,"In fact, in virtually all the text mining",pic_cs-410_7_6_360.jpg
cs-410_7_6_88,cs-410,7,6,Text,"00:06:26,565","00:06:32,368",88,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=386,representation and then techniques that,pic_cs-410_7_6_360.jpg
cs-410_7_6_89,cs-410,7,6,Text,"00:06:35,909","00:06:39,652",89,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=395,But obviously all these other,pic_cs-410_7_6_360.jpg
cs-410_7_6_90,cs-410,7,6,Text,"00:06:39,652","00:06:45,440",90,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=399,should be combined in order to support,pic_cs-410_7_6_360.jpg
cs-410_7_6_91,cs-410,7,6,Text,"00:06:45,440","00:06:48,790",91,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=405,"So to summarize,",pic_cs-410_7_6_360.jpg
cs-410_7_6_92,cs-410,7,6,Text,"00:06:48,790","00:06:53,615",92,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=408,Text representation determines what,pic_cs-410_7_6_360.jpg
cs-410_7_6_93,cs-410,7,6,Text,"00:06:53,615","00:06:57,908",93,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=413,And there are multiple ways to,pic_cs-410_7_6_360.jpg
cs-410_7_6_94,cs-410,7,6,Text,"00:06:57,908","00:07:03,099",94,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=417,"syntactic structures, entity-relation",pic_cs-410_7_6_360.jpg
cs-410_7_6_95,cs-410,7,6,Text,"00:07:03,099","00:07:08,326",95,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=423,And these different,pic_cs-410_7_6_420.jpg
cs-410_7_6_96,cs-410,7,6,Text,"00:07:08,326","00:07:13,540",96,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=428,be combined in real applications,pic_cs-410_7_6_420.jpg
cs-410_7_6_97,cs-410,7,6,Text,"00:07:13,540","00:07:20,263",97,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=433,"For example, even if we cannot",pic_cs-410_7_6_420.jpg
cs-410_7_6_98,cs-410,7,6,Text,"00:07:20,263","00:07:25,380",98,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=440,"of syntactic structures, we can state",pic_cs-410_7_6_420.jpg
cs-410_7_6_99,cs-410,7,6,Text,"00:07:25,380","00:07:29,610",99,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=445,"And if we can recognize some entities,",pic_cs-410_7_6_420.jpg
cs-410_7_6_100,cs-410,7,6,Text,"00:07:29,610","00:07:32,660",100,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=449,So in general we want to,pic_cs-410_7_6_420.jpg
cs-410_7_6_101,cs-410,7,6,Text,"00:07:34,570","00:07:37,210",101,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=454,And when different levels,pic_cs-410_7_6_420.jpg
cs-410_7_6_102,cs-410,7,6,Text,"00:07:37,210","00:07:41,520",102,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=457,"we can enable a richer analysis,",pic_cs-410_7_6_420.jpg
cs-410_7_6_103,cs-410,7,6,Text,"00:07:42,830","00:07:46,610",103,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=462,This course however focuses,pic_cs-410_7_6_420.jpg
cs-410_7_6_104,cs-410,7,6,Text,"00:07:46,610","00:07:52,170",104,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=466,Such techniques have also several,pic_cs-410_7_6_420.jpg
cs-410_7_6_105,cs-410,7,6,Text,"00:07:52,170","00:07:55,460",105,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=472,"robust, so they are applicable",pic_cs-410_7_6_420.jpg
cs-410_7_6_106,cs-410,7,6,Text,"00:07:55,460","00:07:59,780",106,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=475,That's a big advantage over,pic_cs-410_7_6_420.jpg
cs-410_7_6_107,cs-410,7,6,Text,"00:07:59,780","00:08:03,510",107,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=479,more fragile natural language,pic_cs-410_7_6_420.jpg
cs-410_7_6_108,cs-410,7,6,Text,"00:08:03,510","00:08:07,680",108,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=483,"Secondly, it does not require",pic_cs-410_7_6_480.jpg
cs-410_7_6_109,cs-410,7,6,Text,"00:08:07,680","00:08:11,520",109,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=487,"sometimes, it does not",pic_cs-410_7_6_480.jpg
cs-410_7_6_110,cs-410,7,6,Text,"00:08:11,520","00:08:14,037",110,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=491,"So that's, again, an important benefit,",pic_cs-410_7_6_480.jpg
cs-410_7_6_111,cs-410,7,6,Text,"00:08:14,037","00:08:17,962",111,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=494,because that means that you can apply,pic_cs-410_7_6_480.jpg
cs-410_7_6_112,cs-410,7,6,Text,"00:08:20,910","00:08:25,373",112,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=500,"Third, these techniques are actually",pic_cs-410_7_6_480.jpg
cs-410_7_6_113,cs-410,7,6,Text,"00:08:25,373","00:08:27,690",113,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=505,effective form in implications.,pic_cs-410_7_6_480.jpg
cs-410_7_6_114,cs-410,7,6,Text,"00:08:29,210","00:08:32,180",114,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=509,Although not all of course,pic_cs-410_7_6_480.jpg
cs-410_7_6_115,cs-410,7,6,Text,"00:08:34,340","00:08:38,460",115,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=514,Now they are very effective,pic_cs-410_7_6_480.jpg
cs-410_7_6_116,cs-410,7,6,Text,"00:08:38,460","00:08:44,610",116,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=518,are invented by humans as basically,pic_cs-410_7_6_480.jpg
cs-410_7_6_117,cs-410,7,6,Text,"00:08:45,610","00:08:51,120",117,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=525,So they are actually quite sufficient for,pic_cs-410_7_6_480.jpg
cs-410_7_6_118,cs-410,7,6,Text,"00:08:53,680","00:09:00,310",118,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=533,So that makes this kind of word-based,pic_cs-410_7_6_480.jpg
cs-410_7_6_119,cs-410,7,6,Text,"00:09:00,310","00:09:05,010",119,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=540,"And finally, such a word-based",pic_cs-410_7_6_540.jpg
cs-410_7_6_120,cs-410,7,6,Text,"00:09:05,010","00:09:11,690",120,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=545,by such a representation can be combined,pic_cs-410_7_6_540.jpg
cs-410_7_6_121,cs-410,7,6,Text,"00:09:14,020","00:09:15,191",121,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=554,So they're not competing with each other.,pic_cs-410_7_6_540.jpg
cs-410_7_6_122,cs-410,7,6,Text,"00:09:15,191","00:09:25,191",122,https://www.coursera.org/learn/cs-410/lecture/PK3Gd?t=555,[MUSIC],pic_cs-410_7_6_540.jpg
cs-410_7_7_1,cs-410,7,7,Word,"00:00:00,025","00:00:04,546",1,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=0,[SOUND] This lecture is,pic_cs-410_7_7_0.jpg
cs-410_7_7_2,cs-410,7,7,Word,"00:00:04,546","00:00:10,323",2,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=4,about the word association,pic_cs-410_7_7_0.jpg
cs-410_7_7_3,cs-410,7,7,Word,"00:00:10,323","00:00:15,100",3,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=10,mining and analysis.,pic_cs-410_7_7_0.jpg
cs-410_7_7_4,cs-410,7,7,Word,"00:00:15,100","00:00:19,884",4,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=15,"In this lecture,",pic_cs-410_7_7_0.jpg
cs-410_7_7_5,cs-410,7,7,Word,"00:00:19,884","00:00:22,902",5,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=19,associations of words from text.,pic_cs-410_7_7_0.jpg
cs-410_7_7_6,cs-410,7,7,Word,"00:00:22,902","00:00:27,900",6,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=22,Now this is an example of knowledge,pic_cs-410_7_7_0.jpg
cs-410_7_7_7,cs-410,7,7,Word,"00:00:27,900","00:00:29,960",7,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=27,we can mine from text data.,pic_cs-410_7_7_0.jpg
cs-410_7_7_8,cs-410,7,7,Word,"00:00:33,942","00:00:35,090",8,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=33,Here's the outline.,pic_cs-410_7_7_0.jpg
cs-410_7_7_9,cs-410,7,7,Word,"00:00:35,090","00:00:39,828",9,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=35,We're going to first talk about,pic_cs-410_7_7_0.jpg
cs-410_7_7_10,cs-410,7,7,Word,"00:00:39,828","00:00:45,100",10,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=39,then explain why discovering such,pic_cs-410_7_7_0.jpg
cs-410_7_7_11,cs-410,7,7,Word,"00:00:45,100","00:00:50,070",11,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=45,we're going to talk about some general,pic_cs-410_7_7_0.jpg
cs-410_7_7_12,cs-410,7,7,Word,"00:00:50,070","00:00:55,209",12,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=50,In general there are two word,pic_cs-410_7_7_0.jpg
cs-410_7_7_13,cs-410,7,7,Word,"00:00:56,680","00:00:58,680",13,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=56,One is called a paradigmatic relation.,pic_cs-410_7_7_0.jpg
cs-410_7_7_14,cs-410,7,7,Word,"00:00:58,680","00:01:03,000",14,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=58,The other is syntagmatic relation.,pic_cs-410_7_7_0.jpg
cs-410_7_7_15,cs-410,7,7,Word,"00:01:03,000","00:01:07,780",15,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=63,A and B have paradigmatic relation,pic_cs-410_7_7_60.jpg
cs-410_7_7_16,cs-410,7,7,Word,"00:01:07,780","00:01:11,700",16,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=67,if they can be substituted for each other.,pic_cs-410_7_7_60.jpg
cs-410_7_7_17,cs-410,7,7,Word,"00:01:11,700","00:01:17,910",17,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=71,That means the two words that,pic_cs-410_7_7_60.jpg
cs-410_7_7_18,cs-410,7,7,Word,"00:01:17,910","00:01:23,130",18,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=77,"would be in the same semantic class,",pic_cs-410_7_7_60.jpg
cs-410_7_7_19,cs-410,7,7,Word,"00:01:23,130","00:01:26,910",19,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=83,And we can in general,pic_cs-410_7_7_60.jpg
cs-410_7_7_20,cs-410,7,7,Word,"00:01:26,910","00:01:30,310",20,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=86,without affecting,pic_cs-410_7_7_60.jpg
cs-410_7_7_21,cs-410,7,7,Word,"00:01:30,310","00:01:33,810",21,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=90,That means we would still,pic_cs-410_7_7_60.jpg
cs-410_7_7_22,cs-410,7,7,Word,"00:01:33,810","00:01:41,530",22,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=93,"For example, cat and dog, these two",pic_cs-410_7_7_60.jpg
cs-410_7_7_23,cs-410,7,7,Word,"00:01:41,530","00:01:47,710",23,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=101,because they are in,pic_cs-410_7_7_60.jpg
cs-410_7_7_24,cs-410,7,7,Word,"00:01:47,710","00:01:51,827",24,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=107,"And in general,",pic_cs-410_7_7_60.jpg
cs-410_7_7_25,cs-410,7,7,Word,"00:01:51,827","00:01:56,880",25,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=111,the sentence would still be a valid,pic_cs-410_7_7_60.jpg
cs-410_7_7_26,cs-410,7,7,Word,"00:01:58,320","00:02:01,990",26,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=118,Similarly Monday and,pic_cs-410_7_7_60.jpg
cs-410_7_7_27,cs-410,7,7,Word,"00:02:04,930","00:02:09,390",27,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=124,The second kind of relation is,pic_cs-410_7_7_120.jpg
cs-410_7_7_28,cs-410,7,7,Word,"00:02:10,610","00:02:17,200",28,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=130,"In this case, the two words that have this",pic_cs-410_7_7_120.jpg
cs-410_7_7_29,cs-410,7,7,Word,"00:02:17,200","00:02:22,190",29,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=137,So A and B have syntagmatic relation if,pic_cs-410_7_7_120.jpg
cs-410_7_7_30,cs-410,7,7,Word,"00:02:22,190","00:02:29,500",30,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=142,"a sentence, that means these two",pic_cs-410_7_7_120.jpg
cs-410_7_7_31,cs-410,7,7,Word,"00:02:30,720","00:02:36,830",31,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=150,"So for example, cat and sit are related",pic_cs-410_7_7_120.jpg
cs-410_7_7_32,cs-410,7,7,Word,"00:02:38,060","00:02:43,870",32,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=158,"Similarly, car and",pic_cs-410_7_7_120.jpg
cs-410_7_7_33,cs-410,7,7,Word,"00:02:43,870","00:02:47,550",33,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=163,they can be combined with,pic_cs-410_7_7_120.jpg
cs-410_7_7_34,cs-410,7,7,Word,"00:02:47,550","00:02:54,150",34,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=167,"However, in general, we can not",pic_cs-410_7_7_120.jpg
cs-410_7_7_35,cs-410,7,7,Word,"00:02:54,150","00:02:59,590",35,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=174,car with drive in the sentence,pic_cs-410_7_7_120.jpg
cs-410_7_7_36,cs-410,7,7,Word,"00:02:59,590","00:03:03,950",36,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=179,"meaning that if we do that, the sentence",pic_cs-410_7_7_120.jpg
cs-410_7_7_37,cs-410,7,7,Word,"00:03:03,950","00:03:10,135",37,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=183,So this is different from,pic_cs-410_7_7_180.jpg
cs-410_7_7_38,cs-410,7,7,Word,"00:03:10,135","00:03:15,875",38,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=190,And these two relations are in fact so,pic_cs-410_7_7_180.jpg
cs-410_7_7_39,cs-410,7,7,Word,"00:03:17,365","00:03:24,180",39,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=197,generalized to capture basic relations,pic_cs-410_7_7_180.jpg
cs-410_7_7_40,cs-410,7,7,Word,"00:03:24,180","00:03:27,880",40,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=204,And definitely they can be,pic_cs-410_7_7_180.jpg
cs-410_7_7_41,cs-410,7,7,Word,"00:03:27,880","00:03:31,630",41,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=207,relations of any items in a language.,pic_cs-410_7_7_180.jpg
cs-410_7_7_42,cs-410,7,7,Word,"00:03:31,630","00:03:36,620",42,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=211,"So, A and B don't have to be words and",pic_cs-410_7_7_180.jpg
cs-410_7_7_43,cs-410,7,7,Word,"00:03:37,960","00:03:44,710",43,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=217,And they can even be more complex,pic_cs-410_7_7_180.jpg
cs-410_7_7_44,cs-410,7,7,Word,"00:03:44,710","00:03:48,820",44,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=224,If you think about the general,pic_cs-410_7_7_180.jpg
cs-410_7_7_45,cs-410,7,7,Word,"00:03:48,820","00:03:53,066",45,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=228,then we can think about the units,pic_cs-410_7_7_180.jpg
cs-410_7_7_46,cs-410,7,7,Word,"00:03:53,066","00:03:58,980",46,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=233,Then we think of paradigmatic,pic_cs-410_7_7_180.jpg
cs-410_7_7_47,cs-410,7,7,Word,"00:03:58,980","00:04:05,890",47,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=238,are applied to units that tend to occur,pic_cs-410_7_7_180.jpg
cs-410_7_7_48,cs-410,7,7,Word,"00:04:05,890","00:04:11,660",48,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=245,or in a sequence of data,pic_cs-410_7_7_240.jpg
cs-410_7_7_49,cs-410,7,7,Word,"00:04:11,660","00:04:20,980",49,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=251,So they occur in similar locations,pic_cs-410_7_7_240.jpg
cs-410_7_7_50,cs-410,7,7,Word,"00:04:20,980","00:04:25,415",50,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=260,Syntagmatical relation on,pic_cs-410_7_7_240.jpg
cs-410_7_7_51,cs-410,7,7,Word,"00:04:25,415","00:04:30,210",51,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=265,co-occurrent elements that tend,pic_cs-410_7_7_240.jpg
cs-410_7_7_52,cs-410,7,7,Word,"00:04:33,150","00:04:38,470",52,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=273,So these two are complimentary and,pic_cs-410_7_7_240.jpg
cs-410_7_7_53,cs-410,7,7,Word,"00:04:38,470","00:04:42,810",53,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=278,And we're interested in discovering,pic_cs-410_7_7_240.jpg
cs-410_7_7_54,cs-410,7,7,Word,"00:04:42,810","00:04:46,420",54,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=282,Discovering such worded,pic_cs-410_7_7_240.jpg
cs-410_7_7_55,cs-410,7,7,Word,"00:04:47,480","00:04:52,920",55,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=287,"First, such relations can be directly",pic_cs-410_7_7_240.jpg
cs-410_7_7_56,cs-410,7,7,Word,"00:04:52,920","00:04:58,880",56,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=292,"tasks, and this is because this is part",pic_cs-410_7_7_240.jpg
cs-410_7_7_57,cs-410,7,7,Word,"00:04:58,880","00:05:02,440",57,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=298,So if you know these two words,pic_cs-410_7_7_240.jpg
cs-410_7_7_58,cs-410,7,7,Word,"00:05:02,440","00:05:04,970",58,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=302,and then you can help a lot of tasks.,pic_cs-410_7_7_300.jpg
cs-410_7_7_59,cs-410,7,7,Word,"00:05:05,980","00:05:10,970",59,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=305,And grammar learning can be also,pic_cs-410_7_7_300.jpg
cs-410_7_7_60,cs-410,7,7,Word,"00:05:10,970","00:05:15,130",60,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=310,Because if we can learn,pic_cs-410_7_7_300.jpg
cs-410_7_7_61,cs-410,7,7,Word,"00:05:15,130","00:05:20,000",61,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=315,"then we form classes of words,",pic_cs-410_7_7_300.jpg
cs-410_7_7_62,cs-410,7,7,Word,"00:05:20,000","00:05:25,630",62,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=320,"And if we learn syntagmatic relations,",pic_cs-410_7_7_300.jpg
cs-410_7_7_63,cs-410,7,7,Word,"00:05:25,630","00:05:32,400",63,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=325,the rules for putting together a larger,pic_cs-410_7_7_300.jpg
cs-410_7_7_64,cs-410,7,7,Word,"00:05:32,400","00:05:37,390",64,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=332,So we learn the structure and,pic_cs-410_7_7_300.jpg
cs-410_7_7_65,cs-410,7,7,Word,"00:05:39,855","00:05:43,070",65,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=339,Word relations can be also very useful for,pic_cs-410_7_7_300.jpg
cs-410_7_7_66,cs-410,7,7,Word,"00:05:43,070","00:05:46,580",66,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=343,many applications in text retrieval and,pic_cs-410_7_7_300.jpg
cs-410_7_7_67,cs-410,7,7,Word,"00:05:46,580","00:05:50,520",67,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=346,"For example, in search and",pic_cs-410_7_7_300.jpg
cs-410_7_7_68,cs-410,7,7,Word,"00:05:50,520","00:05:55,930",68,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=350,"associations to modify a query,",pic_cs-410_7_7_300.jpg
cs-410_7_7_69,cs-410,7,7,Word,"00:05:55,930","00:06:00,480",69,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=355,introduce additional related words into,pic_cs-410_7_7_300.jpg
cs-410_7_7_70,cs-410,7,7,Word,"00:06:01,590","00:06:03,390",70,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=361,It's often called a query expansion.,pic_cs-410_7_7_360.jpg
cs-410_7_7_71,cs-410,7,7,Word,"00:06:05,290","00:06:10,030",71,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=365,Or you can use related words to,pic_cs-410_7_7_360.jpg
cs-410_7_7_72,cs-410,7,7,Word,"00:06:10,030","00:06:11,660",72,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=370,to explore the information space.,pic_cs-410_7_7_360.jpg
cs-410_7_7_73,cs-410,7,7,Word,"00:06:12,740","00:06:15,610",73,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=372,Another application is to,pic_cs-410_7_7_360.jpg
cs-410_7_7_74,cs-410,7,7,Word,"00:06:15,610","00:06:19,790",74,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=375,automatically construct the top,pic_cs-410_7_7_360.jpg
cs-410_7_7_75,cs-410,7,7,Word,"00:06:19,790","00:06:24,540",75,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=379,We can have words as nodes and,pic_cs-410_7_7_360.jpg
cs-410_7_7_76,cs-410,7,7,Word,"00:06:24,540","00:06:27,930",76,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=384,A user could navigate from,pic_cs-410_7_7_360.jpg
cs-410_7_7_77,cs-410,7,7,Word,"00:06:28,990","00:06:31,180",77,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=388,find information in the information space.,pic_cs-410_7_7_360.jpg
cs-410_7_7_78,cs-410,7,7,Word,"00:06:33,620","00:06:40,620",78,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=393,"Finally, such word associations can also",pic_cs-410_7_7_360.jpg
cs-410_7_7_79,cs-410,7,7,Word,"00:06:40,620","00:06:45,680",79,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=400,"For example, we might be interested",pic_cs-410_7_7_360.jpg
cs-410_7_7_80,cs-410,7,7,Word,"00:06:45,680","00:06:48,620",80,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=405,negative opinions about the iPhone 6.,pic_cs-410_7_7_360.jpg
cs-410_7_7_81,cs-410,7,7,Word,"00:06:48,620","00:06:55,180",81,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=408,"In order to do that, we can look at what",pic_cs-410_7_7_360.jpg
cs-410_7_7_82,cs-410,7,7,Word,"00:06:55,180","00:07:01,630",82,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=415,a feature word like battery in,pic_cs-410_7_7_360.jpg
cs-410_7_7_83,cs-410,7,7,Word,"00:07:01,630","00:07:05,147",83,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=421,Such a syntagmatical,pic_cs-410_7_7_420.jpg
cs-410_7_7_84,cs-410,7,7,Word,"00:07:05,147","00:07:08,854",84,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=425,show the detailed opinions,pic_cs-410_7_7_420.jpg
cs-410_7_7_85,cs-410,7,7,Word,"00:07:16,696","00:07:20,837",85,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=436,"So, how can we discover such",pic_cs-410_7_7_420.jpg
cs-410_7_7_86,cs-410,7,7,Word,"00:07:20,837","00:07:24,450",86,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=440,"Now, here are some intuitions",pic_cs-410_7_7_420.jpg
cs-410_7_7_87,cs-410,7,7,Word,"00:07:24,450","00:07:27,479",87,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=444,Now let's first look at,pic_cs-410_7_7_420.jpg
cs-410_7_7_88,cs-410,7,7,Word,"00:07:29,080","00:07:32,940",88,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=449,Here we essentially can take,pic_cs-410_7_7_420.jpg
cs-410_7_7_89,cs-410,7,7,Word,"00:07:34,150","00:07:38,440",89,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=454,So here you see some simple,pic_cs-410_7_7_420.jpg
cs-410_7_7_90,cs-410,7,7,Word,"00:07:38,440","00:07:43,416",90,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=458,You can see they generally,pic_cs-410_7_7_420.jpg
cs-410_7_7_91,cs-410,7,7,Word,"00:07:43,416","00:07:48,390",91,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=463,and that after all is the definition,pic_cs-410_7_7_420.jpg
cs-410_7_7_92,cs-410,7,7,Word,"00:07:49,540","00:07:54,510",92,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=469,On the right side you can kind,pic_cs-410_7_7_420.jpg
cs-410_7_7_93,cs-410,7,7,Word,"00:07:54,510","00:07:59,090",93,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=474,the context of cat and,pic_cs-410_7_7_420.jpg
cs-410_7_7_94,cs-410,7,7,Word,"00:08:00,640","00:08:05,230",94,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=480,I've taken away cat and,pic_cs-410_7_7_480.jpg
cs-410_7_7_95,cs-410,7,7,Word,"00:08:05,230","00:08:07,280",95,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=485,that you can see just the context.,pic_cs-410_7_7_480.jpg
cs-410_7_7_96,cs-410,7,7,Word,"00:08:08,810","00:08:12,660",96,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=488,"Now, of course we can have different",pic_cs-410_7_7_480.jpg
cs-410_7_7_97,cs-410,7,7,Word,"00:08:13,810","00:08:19,528",97,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=493,"For example, we can look at",pic_cs-410_7_7_480.jpg
cs-410_7_7_98,cs-410,7,7,Word,"00:08:19,528","00:08:24,222",98,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=499,part of this context.,pic_cs-410_7_7_480.jpg
cs-410_7_7_99,cs-410,7,7,Word,"00:08:24,222","00:08:28,000",99,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=504,So we can call this left context.,pic_cs-410_7_7_480.jpg
cs-410_7_7_100,cs-410,7,7,Word,"00:08:28,000","00:08:34,800",100,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=508,What words occur before we see cat or dog?,pic_cs-410_7_7_480.jpg
cs-410_7_7_101,cs-410,7,7,Word,"00:08:34,800","00:08:39,910",101,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=514,"So, you can see in this case, clearly",pic_cs-410_7_7_480.jpg
cs-410_7_7_102,cs-410,7,7,Word,"00:08:41,810","00:08:47,860",102,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=521,You generally say his cat or my cat and,pic_cs-410_7_7_480.jpg
cs-410_7_7_103,cs-410,7,7,Word,"00:08:47,860","00:08:52,290",103,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=527,So that makes them similar,pic_cs-410_7_7_480.jpg
cs-410_7_7_104,cs-410,7,7,Word,"00:08:53,660","00:08:58,880",104,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=533,"Similarly, if you look at the words",pic_cs-410_7_7_480.jpg
cs-410_7_7_105,cs-410,7,7,Word,"00:08:58,880","00:09:03,970",105,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=538,"which we can call right context,",pic_cs-410_7_7_480.jpg
cs-410_7_7_106,cs-410,7,7,Word,"00:09:03,970","00:09:07,490",106,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=543,"Of course, it's an extreme case,",pic_cs-410_7_7_540.jpg
cs-410_7_7_107,cs-410,7,7,Word,"00:09:08,670","00:09:12,883",107,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=548,"And in general,",pic_cs-410_7_7_540.jpg
cs-410_7_7_108,cs-410,7,7,Word,"00:09:12,883","00:09:15,170",108,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=552,that can't follow cat and dog.,pic_cs-410_7_7_540.jpg
cs-410_7_7_109,cs-410,7,7,Word,"00:09:17,830","00:09:21,700",109,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=557,You can also even look,pic_cs-410_7_7_540.jpg
cs-410_7_7_110,cs-410,7,7,Word,"00:09:21,700","00:09:24,690",110,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=561,And that might include all,pic_cs-410_7_7_540.jpg
cs-410_7_7_111,cs-410,7,7,Word,"00:09:24,690","00:09:26,640",111,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=564,in sentences around this word.,pic_cs-410_7_7_540.jpg
cs-410_7_7_112,cs-410,7,7,Word,"00:09:27,658","00:09:34,300",112,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=567,"And even in the general context, you also",pic_cs-410_7_7_540.jpg
cs-410_7_7_113,cs-410,7,7,Word,"00:09:35,400","00:09:41,480",113,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=575,So this was just a suggestion,pic_cs-410_7_7_540.jpg
cs-410_7_7_114,cs-410,7,7,Word,"00:09:41,480","00:09:47,000",114,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=581,relation by looking at,pic_cs-410_7_7_540.jpg
cs-410_7_7_115,cs-410,7,7,Word,"00:09:47,000","00:09:50,900",115,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=587,"So, for example,",pic_cs-410_7_7_540.jpg
cs-410_7_7_116,cs-410,7,7,Word,"00:09:50,900","00:09:54,760",116,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=590,How similar are context of cat and,pic_cs-410_7_7_540.jpg
cs-410_7_7_117,cs-410,7,7,Word,"00:09:56,240","00:10:01,630",117,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=596,In contrast how similar are context,pic_cs-410_7_7_540.jpg
cs-410_7_7_118,cs-410,7,7,Word,"00:10:02,660","00:10:07,610",118,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=602,"Now, intuitively,",pic_cs-410_7_7_600.jpg
cs-410_7_7_119,cs-410,7,7,Word,"00:10:07,610","00:10:11,030",119,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=607,the context of dog would,pic_cs-410_7_7_600.jpg
cs-410_7_7_120,cs-410,7,7,Word,"00:10:11,030","00:10:16,550",120,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=611,the context of cat and,pic_cs-410_7_7_600.jpg
cs-410_7_7_121,cs-410,7,7,Word,"00:10:16,550","00:10:20,680",121,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=616,"That means, in the first case",pic_cs-410_7_7_600.jpg
cs-410_7_7_122,cs-410,7,7,Word,"00:10:21,910","00:10:25,940",122,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=621,between the context of cat and,pic_cs-410_7_7_600.jpg
cs-410_7_7_123,cs-410,7,7,Word,"00:10:25,940","00:10:30,248",123,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=625,the similarity between context of cat and,pic_cs-410_7_7_600.jpg
cs-410_7_7_124,cs-410,7,7,Word,"00:10:30,248","00:10:35,750",124,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=630,because they all not having a paradigmatic,pic_cs-410_7_7_600.jpg
cs-410_7_7_125,cs-410,7,7,Word,"00:10:35,750","00:10:40,550",125,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=635,relationship and imagine what words,pic_cs-410_7_7_600.jpg
cs-410_7_7_126,cs-410,7,7,Word,"00:10:40,550","00:10:44,900",126,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=640,It would be very different from,pic_cs-410_7_7_600.jpg
cs-410_7_7_127,cs-410,7,7,Word,"00:10:46,620","00:10:50,340",127,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=646,So this is the basic idea of what,pic_cs-410_7_7_600.jpg
cs-410_7_7_128,cs-410,7,7,Word,"00:10:52,040","00:10:54,180",128,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=652,What about the syntagmatic relation?,pic_cs-410_7_7_600.jpg
cs-410_7_7_129,cs-410,7,7,Word,"00:10:54,180","00:10:58,550",129,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=654,"Well, here we're going to explore",pic_cs-410_7_7_600.jpg
cs-410_7_7_130,cs-410,7,7,Word,"00:10:58,550","00:11:02,430",130,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=658,again based on the definition,pic_cs-410_7_7_600.jpg
cs-410_7_7_131,cs-410,7,7,Word,"00:11:03,990","00:11:05,600",131,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=663,Here you see the same sample of text.,pic_cs-410_7_7_660.jpg
cs-410_7_7_132,cs-410,7,7,Word,"00:11:06,640","00:11:10,710",132,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=666,But here we're interested in knowing,pic_cs-410_7_7_660.jpg
cs-410_7_7_133,cs-410,7,7,Word,"00:11:10,710","00:11:14,780",133,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=670,with the verb eats and,pic_cs-410_7_7_660.jpg
cs-410_7_7_134,cs-410,7,7,Word,"00:11:16,380","00:11:20,880",134,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=676,And if you look at the right,pic_cs-410_7_7_660.jpg
cs-410_7_7_135,cs-410,7,7,Word,"00:11:20,880","00:11:25,245",135,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=680,"you see,",pic_cs-410_7_7_660.jpg
cs-410_7_7_136,cs-410,7,7,Word,"00:11:27,110","00:11:30,140",136,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=687,I've taken away the word to its left and,pic_cs-410_7_7_660.jpg
cs-410_7_7_137,cs-410,7,7,Word,"00:11:30,140","00:11:33,970",137,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=690,also the word to its,pic_cs-410_7_7_660.jpg
cs-410_7_7_138,cs-410,7,7,Word,"00:11:35,340","00:11:41,900",138,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=695,"And then we ask the question, what words",pic_cs-410_7_7_660.jpg
cs-410_7_7_139,cs-410,7,7,Word,"00:11:43,650","00:11:47,960",139,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=703,And what words tend to,pic_cs-410_7_7_660.jpg
cs-410_7_7_140,cs-410,7,7,Word,"00:11:49,560","00:11:54,997",140,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=709,Now thinking about this question,pic_cs-410_7_7_660.jpg
cs-410_7_7_141,cs-410,7,7,Word,"00:11:54,997","00:12:00,830",141,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=714,relations because syntagmatic relations,pic_cs-410_7_7_660.jpg
cs-410_7_7_142,cs-410,7,7,Word,"00:12:03,070","00:12:07,290",142,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=723,So the important question to ask for,pic_cs-410_7_7_720.jpg
cs-410_7_7_143,cs-410,7,7,Word,"00:12:07,290","00:12:14,570",143,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=727,"whenever eats occurs,",pic_cs-410_7_7_720.jpg
cs-410_7_7_144,cs-410,7,7,Word,"00:12:16,180","00:12:19,120",144,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=736,So the question here has,pic_cs-410_7_7_720.jpg
cs-410_7_7_145,cs-410,7,7,Word,"00:12:19,120","00:12:23,940",145,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=739,are some other words that tend,pic_cs-410_7_7_720.jpg
cs-410_7_7_146,cs-410,7,7,Word,"00:12:23,940","00:12:28,240",146,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=743,Meaning that whenever you see eats,pic_cs-410_7_7_720.jpg
cs-410_7_7_147,cs-410,7,7,Word,"00:12:29,620","00:12:34,660",147,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=749,"And if you don't see eats, probably,",pic_cs-410_7_7_720.jpg
cs-410_7_7_148,cs-410,7,7,Word,"00:12:36,560","00:12:40,210",148,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=756,So this intuition can help,pic_cs-410_7_7_720.jpg
cs-410_7_7_149,cs-410,7,7,Word,"00:12:41,530","00:12:43,200",149,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=761,"Now again, consider example.",pic_cs-410_7_7_720.jpg
cs-410_7_7_150,cs-410,7,7,Word,"00:12:44,210","00:12:48,170",150,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=764,How helpful is occurrence of eats for,pic_cs-410_7_7_720.jpg
cs-410_7_7_151,cs-410,7,7,Word,"00:12:49,870","00:12:53,056",151,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=769,Right.,pic_cs-410_7_7_720.jpg
cs-410_7_7_152,cs-410,7,7,Word,"00:12:53,056","00:12:58,930",152,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=773,in a sentence would generally help us,pic_cs-410_7_7_720.jpg
cs-410_7_7_153,cs-410,7,7,Word,"00:12:58,930","00:13:01,801",153,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=778,"And if we see eats occur in the sentence,",pic_cs-410_7_7_720.jpg
cs-410_7_7_154,cs-410,7,7,Word,"00:13:01,801","00:13:05,770",154,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=781,that should increase the chance,pic_cs-410_7_7_780.jpg
cs-410_7_7_155,cs-410,7,7,Word,"00:13:08,490","00:13:12,150",155,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=788,"In contrast,",pic_cs-410_7_7_780.jpg
cs-410_7_7_156,cs-410,7,7,Word,"00:13:12,150","00:13:15,710",156,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=792,how helpful is the occurrence of eats for,pic_cs-410_7_7_780.jpg
cs-410_7_7_157,cs-410,7,7,Word,"00:13:17,330","00:13:20,270",157,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=797,Because eats and,pic_cs-410_7_7_780.jpg
cs-410_7_7_158,cs-410,7,7,Word,"00:13:20,270","00:13:24,840",158,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=800,knowing whether eats occurred,pic_cs-410_7_7_780.jpg
cs-410_7_7_159,cs-410,7,7,Word,"00:13:24,840","00:13:30,140",159,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=804,"really help us predict the weather,",pic_cs-410_7_7_780.jpg
cs-410_7_7_160,cs-410,7,7,Word,"00:13:30,140","00:13:34,100",160,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=810,So this is in contrast to,pic_cs-410_7_7_780.jpg
cs-410_7_7_161,cs-410,7,7,Word,"00:13:35,550","00:13:38,790",161,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=815,This also helps explain that intuition,pic_cs-410_7_7_780.jpg
cs-410_7_7_162,cs-410,7,7,Word,"00:13:38,790","00:13:43,100",162,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=818,behind the methods of what,pic_cs-410_7_7_780.jpg
cs-410_7_7_163,cs-410,7,7,Word,"00:13:43,100","00:13:49,090",163,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=823,Mainly we need to capture the correlation,pic_cs-410_7_7_780.jpg
cs-410_7_7_164,cs-410,7,7,Word,"00:13:50,440","00:13:52,860",164,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=830,So to summarize the general ideas for,pic_cs-410_7_7_780.jpg
cs-410_7_7_165,cs-410,7,7,Word,"00:13:52,860","00:13:55,810",165,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=832,discovering word associations,pic_cs-410_7_7_780.jpg
cs-410_7_7_166,cs-410,7,7,Word,"00:13:56,880","00:14:02,240",166,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=836,"For paradigmatic relation,",pic_cs-410_7_7_780.jpg
cs-410_7_7_167,cs-410,7,7,Word,"00:14:02,240","00:14:04,830",167,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=842,And then compute its context similarity.,pic_cs-410_7_7_840.jpg
cs-410_7_7_168,cs-410,7,7,Word,"00:14:04,830","00:14:09,030",168,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=844,We're going to assume the words,pic_cs-410_7_7_840.jpg
cs-410_7_7_169,cs-410,7,7,Word,"00:14:09,030","00:14:12,260",169,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=849,to have paradigmatic relation.,pic_cs-410_7_7_840.jpg
cs-410_7_7_170,cs-410,7,7,Word,"00:14:14,640","00:14:19,970",170,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=854,"For syntagmatic relation, we will count",pic_cs-410_7_7_840.jpg
cs-410_7_7_171,cs-410,7,7,Word,"00:14:19,970","00:14:25,180",171,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=859,"in a context, which can be a sentence,",pic_cs-410_7_7_840.jpg
cs-410_7_7_172,cs-410,7,7,Word,"00:14:25,180","00:14:28,180",172,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=865,And we're going to compare,pic_cs-410_7_7_840.jpg
cs-410_7_7_173,cs-410,7,7,Word,"00:14:28,180","00:14:31,660",173,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=868,their co-occurrences with,pic_cs-410_7_7_840.jpg
cs-410_7_7_174,cs-410,7,7,Word,"00:14:33,280","00:14:36,660",174,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=873,We're going to assume words,pic_cs-410_7_7_840.jpg
cs-410_7_7_175,cs-410,7,7,Word,"00:14:36,660","00:14:42,335",175,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=876,relatively low individual occurrences,pic_cs-410_7_7_840.jpg
cs-410_7_7_176,cs-410,7,7,Word,"00:14:42,335","00:14:46,581",176,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=882,because they attempt to occur together and,pic_cs-410_7_7_840.jpg
cs-410_7_7_177,cs-410,7,7,Word,"00:14:46,581","00:14:51,635",177,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=886,Note that the paradigmatic relation and,pic_cs-410_7_7_840.jpg
cs-410_7_7_178,cs-410,7,7,Word,"00:14:51,635","00:14:57,065",178,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=891,are actually closely related,pic_cs-410_7_7_840.jpg
cs-410_7_7_179,cs-410,7,7,Word,"00:14:57,065","00:15:02,810",179,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=897,related words tend to have syntagmatic,pic_cs-410_7_7_840.jpg
cs-410_7_7_180,cs-410,7,7,Word,"00:15:02,810","00:15:05,420",180,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=902,They tend to be associated,pic_cs-410_7_7_900.jpg
cs-410_7_7_181,cs-410,7,7,Word,"00:15:05,420","00:15:10,870",181,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=905,that suggests that we can also do join,pic_cs-410_7_7_900.jpg
cs-410_7_7_182,cs-410,7,7,Word,"00:15:10,870","00:15:15,190",182,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=910,So these general ideas can be,pic_cs-410_7_7_900.jpg
cs-410_7_7_183,cs-410,7,7,Word,"00:15:15,190","00:15:19,129",183,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=915,"And the course won't cover all of them,",pic_cs-410_7_7_900.jpg
cs-410_7_7_184,cs-410,7,7,Word,"00:15:19,129","00:15:24,774",184,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=919,we will cover at least some of,pic_cs-410_7_7_900.jpg
cs-410_7_7_185,cs-410,7,7,Word,"00:15:24,774","00:15:27,669",185,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=924,discovering these relations.,pic_cs-410_7_7_900.jpg
cs-410_7_7_186,cs-410,7,7,Word,"00:15:27,669","00:15:37,669",186,https://www.coursera.org/learn/cs-410/lecture/Uufkz?t=927,[MUSIC],pic_cs-410_7_7_900.jpg
cs-410_7_8_1,cs-410,7,8,Paradigmatic,"00:00:00,025","00:00:07,935",1,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=0,[SOUND],pic_cs-410_7_8_0.jpg
cs-410_7_8_2,cs-410,7,8,Paradigmatic,"00:00:07,935","00:00:14,253",2,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=7,lecture is about,pic_cs-410_7_8_0.jpg
cs-410_7_8_3,cs-410,7,8,Paradigmatic,"00:00:14,253","00:00:19,131",3,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=14,In this lecture we are going to talk about,pic_cs-410_7_8_0.jpg
cs-410_7_8_4,cs-410,7,8,Paradigmatic,"00:00:19,131","00:00:22,160",4,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=19,association called,pic_cs-410_7_8_0.jpg
cs-410_7_8_5,cs-410,7,8,Paradigmatic,"00:00:25,400","00:00:30,307",5,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=25,"By definition,",pic_cs-410_7_8_0.jpg
cs-410_7_8_6,cs-410,7,8,Paradigmatic,"00:00:30,307","00:00:34,503",6,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=30,related if they share a similar context.,pic_cs-410_7_8_0.jpg
cs-410_7_8_7,cs-410,7,8,Paradigmatic,"00:00:34,503","00:00:39,086",7,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=34,"Namely, they occur in",pic_cs-410_7_8_0.jpg
cs-410_7_8_8,cs-410,7,8,Paradigmatic,"00:00:39,086","00:00:44,280",8,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=39,So naturally our idea of discovering such,pic_cs-410_7_8_0.jpg
cs-410_7_8_9,cs-410,7,8,Paradigmatic,"00:00:44,280","00:00:49,080",9,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=44,of each word and then try to compute,pic_cs-410_7_8_0.jpg
cs-410_7_8_10,cs-410,7,8,Paradigmatic,"00:00:50,160","00:00:54,360",10,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=50,So here is an example of,pic_cs-410_7_8_0.jpg
cs-410_7_8_11,cs-410,7,8,Paradigmatic,"00:00:55,800","00:01:01,690",11,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=55,Here I have taken the word,pic_cs-410_7_8_0.jpg
cs-410_7_8_12,cs-410,7,8,Paradigmatic,"00:01:01,690","00:01:08,080",12,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=61,you can see we are seeing some remaining,pic_cs-410_7_8_60.jpg
cs-410_7_8_13,cs-410,7,8,Paradigmatic,"00:01:09,610","00:01:12,479",13,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=69,"Now, we can do the same thing for",pic_cs-410_7_8_60.jpg
cs-410_7_8_14,cs-410,7,8,Paradigmatic,"00:01:13,660","00:01:18,370",14,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=73,So in general we would like to capture,pic_cs-410_7_8_60.jpg
cs-410_7_8_15,cs-410,7,8,Paradigmatic,"00:01:18,370","00:01:23,340",15,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=78,the similarity of the context of cat and,pic_cs-410_7_8_60.jpg
cs-410_7_8_16,cs-410,7,8,Paradigmatic,"00:01:24,790","00:01:29,970",16,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=84,So now the question is how can we,pic_cs-410_7_8_60.jpg
cs-410_7_8_17,cs-410,7,8,Paradigmatic,"00:01:29,970","00:01:31,458",17,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=89,then define the similarity function.,pic_cs-410_7_8_60.jpg
cs-410_7_8_18,cs-410,7,8,Paradigmatic,"00:01:33,340","00:01:38,560",18,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=93,"So first, we note that the context",pic_cs-410_7_8_60.jpg
cs-410_7_8_19,cs-410,7,8,Paradigmatic,"00:01:38,560","00:01:43,637",19,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=98,"So, they can be regarded as",pic_cs-410_7_8_60.jpg
cs-410_7_8_20,cs-410,7,8,Paradigmatic,"00:01:43,637","00:01:49,370",20,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=103,"document, but there are also different",pic_cs-410_7_8_60.jpg
cs-410_7_8_21,cs-410,7,8,Paradigmatic,"00:01:49,370","00:01:57,470",21,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=109,"For example, we can look at the word",pic_cs-410_7_8_60.jpg
cs-410_7_8_22,cs-410,7,8,Paradigmatic,"00:01:57,470","00:02:00,440",22,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=117,We can call this context Left1 context.,pic_cs-410_7_8_60.jpg
cs-410_7_8_23,cs-410,7,8,Paradigmatic,"00:02:00,440","00:02:04,980",23,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=120,"All right, so in this case you",pic_cs-410_7_8_120.jpg
cs-410_7_8_24,cs-410,7,8,Paradigmatic,"00:02:04,980","00:02:07,430",24,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=124,"big, a, the, et cetera.",pic_cs-410_7_8_120.jpg
cs-410_7_8_25,cs-410,7,8,Paradigmatic,"00:02:07,430","00:02:12,690",25,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=127,These are the words that can,pic_cs-410_7_8_120.jpg
cs-410_7_8_26,cs-410,7,8,Paradigmatic,"00:02:12,690","00:02:19,280",26,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=132,"So we say my cat, his cat,",pic_cs-410_7_8_120.jpg
cs-410_7_8_27,cs-410,7,8,Paradigmatic,"00:02:19,280","00:02:24,180",27,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=139,"Similarly, we can also collect the words",pic_cs-410_7_8_120.jpg
cs-410_7_8_28,cs-410,7,8,Paradigmatic,"00:02:24,180","00:02:28,156",28,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=144,"We can call this context Right1, and",pic_cs-410_7_8_120.jpg
cs-410_7_8_29,cs-410,7,8,Paradigmatic,"00:02:28,156","00:02:34,128",29,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=148,"here we see words like eats,",pic_cs-410_7_8_120.jpg
cs-410_7_8_30,cs-410,7,8,Paradigmatic,"00:02:34,128","00:02:35,907",30,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=154,"Or, more generally,",pic_cs-410_7_8_120.jpg
cs-410_7_8_31,cs-410,7,8,Paradigmatic,"00:02:35,907","00:02:41,253",31,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=155,we can look at all the words in,pic_cs-410_7_8_120.jpg
cs-410_7_8_32,cs-410,7,8,Paradigmatic,"00:02:41,253","00:02:46,960",32,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=161,"Here, let's say we can take a window",pic_cs-410_7_8_120.jpg
cs-410_7_8_33,cs-410,7,8,Paradigmatic,"00:02:46,960","00:02:48,720",33,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=166,We call this context Window8.,pic_cs-410_7_8_120.jpg
cs-410_7_8_34,cs-410,7,8,Paradigmatic,"00:02:49,850","00:02:54,680",34,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=169,"Now, of course, you can see all",pic_cs-410_7_8_120.jpg
cs-410_7_8_35,cs-410,7,8,Paradigmatic,"00:02:54,680","00:02:58,829",35,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=174,so we'll have a bag of words in,pic_cs-410_7_8_120.jpg
cs-410_7_8_36,cs-410,7,8,Paradigmatic,"00:03:01,270","00:03:06,410",36,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=181,"Now, such a word based representation",pic_cs-410_7_8_180.jpg
cs-410_7_8_37,cs-410,7,8,Paradigmatic,"00:03:06,410","00:03:12,230",37,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=186,an interesting way to define the,pic_cs-410_7_8_180.jpg
cs-410_7_8_38,cs-410,7,8,Paradigmatic,"00:03:12,230","00:03:15,911",38,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=192,Because if you look at just,pic_cs-410_7_8_180.jpg
cs-410_7_8_39,cs-410,7,8,Paradigmatic,"00:03:15,911","00:03:21,750",39,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=195,then we'll see words that share,pic_cs-410_7_8_180.jpg
cs-410_7_8_40,cs-410,7,8,Paradigmatic,"00:03:21,750","00:03:27,650",40,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=201,and we kind of ignored the other words,pic_cs-410_7_8_180.jpg
cs-410_7_8_41,cs-410,7,8,Paradigmatic,"00:03:27,650","00:03:32,380",41,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=207,So that gives us one perspective to,pic_cs-410_7_8_180.jpg
cs-410_7_8_42,cs-410,7,8,Paradigmatic,"00:03:32,380","00:03:34,244",42,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=212,"if we only use the Right1 context,",pic_cs-410_7_8_180.jpg
cs-410_7_8_43,cs-410,7,8,Paradigmatic,"00:03:34,244","00:03:38,420",43,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=214,we will capture this narrative,pic_cs-410_7_8_180.jpg
cs-410_7_8_44,cs-410,7,8,Paradigmatic,"00:03:38,420","00:03:43,040",44,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=218,Using both the Left1 and,pic_cs-410_7_8_180.jpg
cs-410_7_8_45,cs-410,7,8,Paradigmatic,"00:03:43,040","00:03:47,720",45,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=223,the similarity with even,pic_cs-410_7_8_180.jpg
cs-410_7_8_46,cs-410,7,8,Paradigmatic,"00:03:49,910","00:03:54,744",46,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=229,"So in general, context may contain",pic_cs-410_7_8_180.jpg
cs-410_7_8_47,cs-410,7,8,Paradigmatic,"00:03:54,744","00:03:59,575",47,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=234,"my, that you see here, or",pic_cs-410_7_8_180.jpg
cs-410_7_8_48,cs-410,7,8,Paradigmatic,"00:03:59,575","00:04:02,961",48,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=239,"Tuesday, or",pic_cs-410_7_8_180.jpg
cs-410_7_8_49,cs-410,7,8,Paradigmatic,"00:04:05,461","00:04:10,174",49,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=245,And this flexibility also allows us,pic_cs-410_7_8_240.jpg
cs-410_7_8_50,cs-410,7,8,Paradigmatic,"00:04:10,174","00:04:11,660",50,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=250,different ways.,pic_cs-410_7_8_240.jpg
cs-410_7_8_51,cs-410,7,8,Paradigmatic,"00:04:11,660","00:04:13,500",51,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=251,"Sometimes this is useful,",pic_cs-410_7_8_240.jpg
cs-410_7_8_52,cs-410,7,8,Paradigmatic,"00:04:13,500","00:04:19,130",52,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=253,as we might want to capture,pic_cs-410_7_8_240.jpg
cs-410_7_8_53,cs-410,7,8,Paradigmatic,"00:04:19,130","00:04:25,270",53,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=259,That would give us loosely,pic_cs-410_7_8_240.jpg
cs-410_7_8_54,cs-410,7,8,Paradigmatic,"00:04:25,270","00:04:29,340",54,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=265,Whereas if you use only the words,pic_cs-410_7_8_240.jpg
cs-410_7_8_55,cs-410,7,8,Paradigmatic,"00:04:29,340","00:04:35,520",55,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=269,"to the right of the word, then you",pic_cs-410_7_8_240.jpg
cs-410_7_8_56,cs-410,7,8,Paradigmatic,"00:04:35,520","00:04:39,950",56,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=275,much related by their syntactical,pic_cs-410_7_8_240.jpg
cs-410_7_8_57,cs-410,7,8,Paradigmatic,"00:04:41,170","00:04:46,304",57,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=281,So the general idea of discovering,pic_cs-410_7_8_240.jpg
cs-410_7_8_58,cs-410,7,8,Paradigmatic,"00:04:46,304","00:04:50,754",58,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=286,is to compute the similarity,pic_cs-410_7_8_240.jpg
cs-410_7_8_59,cs-410,7,8,Paradigmatic,"00:04:50,754","00:04:55,264",59,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=290,"So here, for example,",pic_cs-410_7_8_240.jpg
cs-410_7_8_60,cs-410,7,8,Paradigmatic,"00:04:55,264","00:04:59,110",60,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=295,dog based on the similarity,pic_cs-410_7_8_240.jpg
cs-410_7_8_61,cs-410,7,8,Paradigmatic,"00:04:59,110","00:05:02,890",61,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=299,"In general, we can combine all",pic_cs-410_7_8_240.jpg
cs-410_7_8_62,cs-410,7,8,Paradigmatic,"00:05:02,890","00:05:06,395",62,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=302,"And so the similarity function is,",pic_cs-410_7_8_300.jpg
cs-410_7_8_63,cs-410,7,8,Paradigmatic,"00:05:06,395","00:05:10,336",63,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=306,a combination of similarities,pic_cs-410_7_8_300.jpg
cs-410_7_8_64,cs-410,7,8,Paradigmatic,"00:05:10,336","00:05:14,849",64,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=310,"And of course, we can also assign",pic_cs-410_7_8_300.jpg
cs-410_7_8_65,cs-410,7,8,Paradigmatic,"00:05:14,849","00:05:20,170",65,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=314,similarities to allow us to focus,pic_cs-410_7_8_300.jpg
cs-410_7_8_66,cs-410,7,8,Paradigmatic,"00:05:20,170","00:05:24,395",66,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=320,And this would be naturally,pic_cs-410_7_8_300.jpg
cs-410_7_8_67,cs-410,7,8,Paradigmatic,"00:05:24,395","00:05:28,935",67,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=324,here the main idea for discovering,pic_cs-410_7_8_300.jpg
cs-410_7_8_68,cs-410,7,8,Paradigmatic,"00:05:28,935","00:05:32,470",68,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=328,to computer the similarity,pic_cs-410_7_8_300.jpg
cs-410_7_8_69,cs-410,7,8,Paradigmatic,"00:05:32,470","00:05:37,670",69,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=332,So next let's see how we exactly,pic_cs-410_7_8_300.jpg
cs-410_7_8_70,cs-410,7,8,Paradigmatic,"00:05:37,670","00:05:42,235",70,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=337,"Now to answer this question,",pic_cs-410_7_8_300.jpg
cs-410_7_8_71,cs-410,7,8,Paradigmatic,"00:05:42,235","00:05:46,520",71,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=342,representation as vectors,pic_cs-410_7_8_300.jpg
cs-410_7_8_72,cs-410,7,8,Paradigmatic,"00:05:48,340","00:05:53,016",72,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=348,Now those of you who have been,pic_cs-410_7_8_300.jpg
cs-410_7_8_73,cs-410,7,8,Paradigmatic,"00:05:53,016","00:05:57,936",73,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=353,textual retrieval techniques would,pic_cs-410_7_8_300.jpg
cs-410_7_8_74,cs-410,7,8,Paradigmatic,"00:05:57,936","00:06:02,711",74,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=357,been used frequently for,pic_cs-410_7_8_300.jpg
cs-410_7_8_75,cs-410,7,8,Paradigmatic,"00:06:02,711","00:06:08,115",75,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=362,But here we also find it convenient,pic_cs-410_7_8_360.jpg
cs-410_7_8_76,cs-410,7,8,Paradigmatic,"00:06:08,115","00:06:11,130",76,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=368,paradigmatic relation discovery.,pic_cs-410_7_8_360.jpg
cs-410_7_8_77,cs-410,7,8,Paradigmatic,"00:06:11,130","00:06:15,440",77,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=371,So the idea of this,pic_cs-410_7_8_360.jpg
cs-410_7_8_78,cs-410,7,8,Paradigmatic,"00:06:15,440","00:06:20,140",78,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=375,word in our vocabulary as defining one,pic_cs-410_7_8_360.jpg
cs-410_7_8_79,cs-410,7,8,Paradigmatic,"00:06:20,140","00:06:23,615",79,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=380,So we have N words in,pic_cs-410_7_8_360.jpg
cs-410_7_8_80,cs-410,7,8,Paradigmatic,"00:06:23,615","00:06:27,462",80,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=383,"then we have N dimensions,",pic_cs-410_7_8_360.jpg
cs-410_7_8_81,cs-410,7,8,Paradigmatic,"00:06:27,462","00:06:34,311",81,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=387,"And on the bottom, you can see a frequency",pic_cs-410_7_8_360.jpg
cs-410_7_8_82,cs-410,7,8,Paradigmatic,"00:06:34,311","00:06:39,855",82,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=394,and here we see where eats,pic_cs-410_7_8_360.jpg
cs-410_7_8_83,cs-410,7,8,Paradigmatic,"00:06:39,855","00:06:43,140",83,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=399,"ate occurred 3 times, et cetera.",pic_cs-410_7_8_360.jpg
cs-410_7_8_84,cs-410,7,8,Paradigmatic,"00:06:43,140","00:06:48,003",84,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=403,So this vector can then be placed,pic_cs-410_7_8_360.jpg
cs-410_7_8_85,cs-410,7,8,Paradigmatic,"00:06:48,003","00:06:53,347",85,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=408,"So in general,",pic_cs-410_7_8_360.jpg
cs-410_7_8_86,cs-410,7,8,Paradigmatic,"00:06:53,347","00:06:58,933",86,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=413,"context of cat as one vector,",pic_cs-410_7_8_360.jpg
cs-410_7_8_87,cs-410,7,8,Paradigmatic,"00:06:58,933","00:07:04,045",87,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=418,"dog, might give us a different context,",pic_cs-410_7_8_360.jpg
cs-410_7_8_88,cs-410,7,8,Paradigmatic,"00:07:04,045","00:07:07,880",88,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=424,And then we can measure,pic_cs-410_7_8_420.jpg
cs-410_7_8_89,cs-410,7,8,Paradigmatic,"00:07:07,880","00:07:10,980",89,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=427,So by viewing context in,pic_cs-410_7_8_420.jpg
cs-410_7_8_90,cs-410,7,8,Paradigmatic,"00:07:10,980","00:07:15,100",90,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=430,we convert the problem of,pic_cs-410_7_8_420.jpg
cs-410_7_8_91,cs-410,7,8,Paradigmatic,"00:07:15,100","00:07:18,820",91,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=435,into the problem of computing,pic_cs-410_7_8_420.jpg
cs-410_7_8_92,cs-410,7,8,Paradigmatic,"00:07:20,300","00:07:24,170",92,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=440,So the two questions that we,pic_cs-410_7_8_420.jpg
cs-410_7_8_93,cs-410,7,8,Paradigmatic,"00:07:24,170","00:07:28,750",93,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=444,"how to compute each vector, and",pic_cs-410_7_8_420.jpg
cs-410_7_8_94,cs-410,7,8,Paradigmatic,"00:07:31,050","00:07:33,579",94,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=451,And the other question is how,pic_cs-410_7_8_420.jpg
cs-410_7_8_95,cs-410,7,8,Paradigmatic,"00:07:35,580","00:07:40,515",95,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=455,"Now in general, there are many approaches",pic_cs-410_7_8_420.jpg
cs-410_7_8_96,cs-410,7,8,Paradigmatic,"00:07:40,515","00:07:43,795",96,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=460,most of them are developed for,pic_cs-410_7_8_420.jpg
cs-410_7_8_97,cs-410,7,8,Paradigmatic,"00:07:43,795","00:07:47,821",97,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=463,And they have been shown to work well for,pic_cs-410_7_8_420.jpg
cs-410_7_8_98,cs-410,7,8,Paradigmatic,"00:07:47,821","00:07:52,712",98,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=467,matching a query vector and,pic_cs-410_7_8_420.jpg
cs-410_7_8_99,cs-410,7,8,Paradigmatic,"00:07:52,712","00:07:57,555",99,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=472,But we can adapt many of,pic_cs-410_7_8_420.jpg
cs-410_7_8_100,cs-410,7,8,Paradigmatic,"00:07:57,555","00:08:01,378",100,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=477,of context documents for our purpose here.,pic_cs-410_7_8_420.jpg
cs-410_7_8_101,cs-410,7,8,Paradigmatic,"00:08:01,378","00:08:05,829",101,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=481,So let's first look at,pic_cs-410_7_8_480.jpg
cs-410_7_8_102,cs-410,7,8,Paradigmatic,"00:08:05,829","00:08:10,481",102,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=485,where we try to match,pic_cs-410_7_8_480.jpg
cs-410_7_8_103,cs-410,7,8,Paradigmatic,"00:08:10,481","00:08:15,150",103,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=490,"the expected overlap of words,",pic_cs-410_7_8_480.jpg
cs-410_7_8_104,cs-410,7,8,Paradigmatic,"00:08:17,020","00:08:22,495",104,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=497,So the idea here is to represent,pic_cs-410_7_8_480.jpg
cs-410_7_8_105,cs-410,7,8,Paradigmatic,"00:08:22,495","00:08:28,438",105,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=502,where each word has a weight,pic_cs-410_7_8_480.jpg
cs-410_7_8_106,cs-410,7,8,Paradigmatic,"00:08:28,438","00:08:35,336",106,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=508,that a randomly picked word from,pic_cs-410_7_8_480.jpg
cs-410_7_8_107,cs-410,7,8,Paradigmatic,"00:08:35,336","00:08:39,956",107,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=515,"So in other words,",pic_cs-410_7_8_480.jpg
cs-410_7_8_108,cs-410,7,8,Paradigmatic,"00:08:39,956","00:08:43,476",108,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=519,"account of word wi in the context, and",pic_cs-410_7_8_480.jpg
cs-410_7_8_109,cs-410,7,8,Paradigmatic,"00:08:43,476","00:08:48,756",109,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=523,this can be interpreted as,pic_cs-410_7_8_480.jpg
cs-410_7_8_110,cs-410,7,8,Paradigmatic,"00:08:48,756","00:08:54,600",110,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=528,actually pick this word from d1,pic_cs-410_7_8_480.jpg
cs-410_7_8_111,cs-410,7,8,Paradigmatic,"00:08:56,760","00:09:01,620",111,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=536,"Now, of course these xi's would sum to one",pic_cs-410_7_8_480.jpg
cs-410_7_8_112,cs-410,7,8,Paradigmatic,"00:09:02,930","00:09:05,750",112,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=542,and this means the vector is,pic_cs-410_7_8_540.jpg
cs-410_7_8_113,cs-410,7,8,Paradigmatic,"00:09:05,750","00:09:08,193",113,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=545,actually probability of,pic_cs-410_7_8_540.jpg
cs-410_7_8_114,cs-410,7,8,Paradigmatic,"00:09:10,500","00:09:15,883",114,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=550,"So, the vector d2 can be also",pic_cs-410_7_8_540.jpg
cs-410_7_8_115,cs-410,7,8,Paradigmatic,"00:09:15,883","00:09:23,540",115,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=555,this would give us then two probability,pic_cs-410_7_8_540.jpg
cs-410_7_8_116,cs-410,7,8,Paradigmatic,"00:09:24,840","00:09:28,220",116,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=564,"So, that addresses the problem",pic_cs-410_7_8_540.jpg
cs-410_7_8_117,cs-410,7,8,Paradigmatic,"00:09:28,220","00:09:31,760",117,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=568,next let's see how we can define,pic_cs-410_7_8_540.jpg
cs-410_7_8_118,cs-410,7,8,Paradigmatic,"00:09:31,760","00:09:35,668",118,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=571,"Well, here, we simply define",pic_cs-410_7_8_540.jpg
cs-410_7_8_119,cs-410,7,8,Paradigmatic,"00:09:35,668","00:09:39,890",119,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=575,"vectors, and",pic_cs-410_7_8_540.jpg
cs-410_7_8_120,cs-410,7,8,Paradigmatic,"00:09:41,410","00:09:43,960",120,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=581,of the corresponding,pic_cs-410_7_8_540.jpg
cs-410_7_8_121,cs-410,7,8,Paradigmatic,"00:09:46,630","00:09:51,847",121,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=586,"Now, it's interesting to see",pic_cs-410_7_8_540.jpg
cs-410_7_8_122,cs-410,7,8,Paradigmatic,"00:09:51,847","00:09:57,360",122,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=591,"actually has a nice interpretation,",pic_cs-410_7_8_540.jpg
cs-410_7_8_123,cs-410,7,8,Paradigmatic,"00:09:57,360","00:10:02,548",123,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=597,"Dot product, in fact that gives",pic_cs-410_7_8_540.jpg
cs-410_7_8_124,cs-410,7,8,Paradigmatic,"00:10:02,548","00:10:08,570",124,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=602,randomly picked words from,pic_cs-410_7_8_600.jpg
cs-410_7_8_125,cs-410,7,8,Paradigmatic,"00:10:08,570","00:10:12,630",125,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=608,That means if we try to pick a word,pic_cs-410_7_8_600.jpg
cs-410_7_8_126,cs-410,7,8,Paradigmatic,"00:10:12,630","00:10:17,860",126,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=612,"word from another context, we can then",pic_cs-410_7_8_600.jpg
cs-410_7_8_127,cs-410,7,8,Paradigmatic,"00:10:17,860","00:10:22,650",127,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=617,"If the two contexts are very similar,",pic_cs-410_7_8_600.jpg
cs-410_7_8_128,cs-410,7,8,Paradigmatic,"00:10:22,650","00:10:27,390",128,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=622,see the two words picked from,pic_cs-410_7_8_600.jpg
cs-410_7_8_129,cs-410,7,8,Paradigmatic,"00:10:27,390","00:10:30,900",129,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=627,"If they are very different,",pic_cs-410_7_8_600.jpg
cs-410_7_8_130,cs-410,7,8,Paradigmatic,"00:10:30,900","00:10:34,890",130,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=630,identical words being picked from,pic_cs-410_7_8_600.jpg
cs-410_7_8_131,cs-410,7,8,Paradigmatic,"00:10:34,890","00:10:39,865",131,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=634,"So this intuitively makes sense, right,",pic_cs-410_7_8_600.jpg
cs-410_7_8_132,cs-410,7,8,Paradigmatic,"00:10:41,490","00:10:46,819",132,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=641,Now you might want to also take,pic_cs-410_7_8_600.jpg
cs-410_7_8_133,cs-410,7,8,Paradigmatic,"00:10:46,819","00:10:51,627",133,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=646,see why this can be interpreted,pic_cs-410_7_8_600.jpg
cs-410_7_8_134,cs-410,7,8,Paradigmatic,"00:10:51,627","00:10:55,410",134,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=651,two randomly picked words are identical.,pic_cs-410_7_8_600.jpg
cs-410_7_8_135,cs-410,7,8,Paradigmatic,"00:10:57,440","00:11:04,550",135,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=657,So if you just stare at the formula,pic_cs-410_7_8_600.jpg
cs-410_7_8_136,cs-410,7,8,Paradigmatic,"00:11:04,550","00:11:12,034",136,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=664,then you will see basically in each,pic_cs-410_7_8_660.jpg
cs-410_7_8_137,cs-410,7,8,Paradigmatic,"00:11:12,034","00:11:17,170",137,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=672,we will see an overlap on,pic_cs-410_7_8_660.jpg
cs-410_7_8_138,cs-410,7,8,Paradigmatic,"00:11:17,170","00:11:23,661",138,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=677,And where xi gives us a probability that,pic_cs-410_7_8_660.jpg
cs-410_7_8_139,cs-410,7,8,Paradigmatic,"00:11:23,661","00:11:28,503",139,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=683,and yi gives us the probability,pic_cs-410_7_8_660.jpg
cs-410_7_8_140,cs-410,7,8,Paradigmatic,"00:11:28,503","00:11:32,024",140,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=688,And when we pick the same,pic_cs-410_7_8_660.jpg
cs-410_7_8_141,cs-410,7,8,Paradigmatic,"00:11:32,024","00:11:34,920",141,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=692,"then we have an identical pick, right so.",pic_cs-410_7_8_660.jpg
cs-410_7_8_142,cs-410,7,8,Paradigmatic,"00:11:34,920","00:11:42,380",142,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=694,"That's one possible approach, EOWC,",pic_cs-410_7_8_660.jpg
cs-410_7_8_143,cs-410,7,8,Paradigmatic,"00:11:42,380","00:11:49,440",143,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=702,"Now as always, we would like to assess",pic_cs-410_7_8_660.jpg
cs-410_7_8_144,cs-410,7,8,Paradigmatic,"00:11:49,440","00:11:52,880",144,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=709,"Now of course, ultimately we have to",pic_cs-410_7_8_660.jpg
cs-410_7_8_145,cs-410,7,8,Paradigmatic,"00:11:52,880","00:11:56,259",145,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=712,see if it gives us really,pic_cs-410_7_8_660.jpg
cs-410_7_8_146,cs-410,7,8,Paradigmatic,"00:11:57,730","00:12:01,010",146,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=717,"Really give us paradigmatical relations,",pic_cs-410_7_8_660.jpg
cs-410_7_8_147,cs-410,7,8,Paradigmatic,"00:12:01,010","00:12:05,380",147,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=721,analytically we can also analyze,pic_cs-410_7_8_720.jpg
cs-410_7_8_148,cs-410,7,8,Paradigmatic,"00:12:05,380","00:12:11,020",148,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=725,"So first, as I said,",pic_cs-410_7_8_720.jpg
cs-410_7_8_149,cs-410,7,8,Paradigmatic,"00:12:11,020","00:12:15,802",149,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=731,formula will give a higher score if there,pic_cs-410_7_8_720.jpg
cs-410_7_8_150,cs-410,7,8,Paradigmatic,"00:12:15,802","00:12:17,988",150,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=735,So that's exactly what we want.,pic_cs-410_7_8_720.jpg
cs-410_7_8_151,cs-410,7,8,Paradigmatic,"00:12:17,988","00:12:21,170",151,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=737,But if you analyze,pic_cs-410_7_8_720.jpg
cs-410_7_8_152,cs-410,7,8,Paradigmatic,"00:12:21,170","00:12:24,286",152,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=741,then you also see there might,pic_cs-410_7_8_720.jpg
cs-410_7_8_153,cs-410,7,8,Paradigmatic,"00:12:24,286","00:12:27,735",153,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=744,and specifically there,pic_cs-410_7_8_720.jpg
cs-410_7_8_154,cs-410,7,8,Paradigmatic,"00:12:27,735","00:12:33,935",154,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=747,"First, it might favor matching",pic_cs-410_7_8_720.jpg
cs-410_7_8_155,cs-410,7,8,Paradigmatic,"00:12:33,935","00:12:35,795",155,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=753,over matching more distinct terms.,pic_cs-410_7_8_720.jpg
cs-410_7_8_156,cs-410,7,8,Paradigmatic,"00:12:36,825","00:12:44,300",156,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=756,"And that is because in the dot product,",pic_cs-410_7_8_720.jpg
cs-410_7_8_157,cs-410,7,8,Paradigmatic,"00:12:44,300","00:12:50,190",157,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=764,element is shared by both contexts and,pic_cs-410_7_8_720.jpg
cs-410_7_8_158,cs-410,7,8,Paradigmatic,"00:12:51,250","00:12:55,710",158,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=771,it might indeed make the score,pic_cs-410_7_8_720.jpg
cs-410_7_8_159,cs-410,7,8,Paradigmatic,"00:12:55,710","00:13:01,150",159,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=775,where the two vectors actually have,pic_cs-410_7_8_720.jpg
cs-410_7_8_160,cs-410,7,8,Paradigmatic,"00:13:01,150","00:13:06,878",160,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=781,But each term has a relatively low,pic_cs-410_7_8_780.jpg
cs-410_7_8_161,cs-410,7,8,Paradigmatic,"00:13:06,878","00:13:09,586",161,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=786,"Of course, this might be",pic_cs-410_7_8_780.jpg
cs-410_7_8_162,cs-410,7,8,Paradigmatic,"00:13:09,586","00:13:14,527",162,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=789,"But in our case, we should intuitively",pic_cs-410_7_8_780.jpg
cs-410_7_8_163,cs-410,7,8,Paradigmatic,"00:13:14,527","00:13:19,645",163,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=794,"more different terms in the context,",pic_cs-410_7_8_780.jpg
cs-410_7_8_164,cs-410,7,8,Paradigmatic,"00:13:19,645","00:13:24,253",164,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=799,in saying that the two words,pic_cs-410_7_8_780.jpg
cs-410_7_8_165,cs-410,7,8,Paradigmatic,"00:13:24,253","00:13:27,020",165,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=804,If you only rely on one term and,pic_cs-410_7_8_780.jpg
cs-410_7_8_166,cs-410,7,8,Paradigmatic,"00:13:27,020","00:13:32,465",166,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=807,"that's a little bit questionable,",pic_cs-410_7_8_780.jpg
cs-410_7_8_167,cs-410,7,8,Paradigmatic,"00:13:34,675","00:13:38,795",167,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=814,Now the second problem is that it,pic_cs-410_7_8_780.jpg
cs-410_7_8_168,cs-410,7,8,Paradigmatic,"00:13:38,795","00:13:42,131",168,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=818,So if you match a word like the and,pic_cs-410_7_8_780.jpg
cs-410_7_8_169,cs-410,7,8,Paradigmatic,"00:13:42,131","00:13:47,443",169,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=822,it will be the same as,pic_cs-410_7_8_780.jpg
cs-410_7_8_170,cs-410,7,8,Paradigmatic,"00:13:47,443","00:13:52,388",170,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=827,intuitively we know,pic_cs-410_7_8_780.jpg
cs-410_7_8_171,cs-410,7,8,Paradigmatic,"00:13:52,388","00:13:57,816",171,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=832,surprising because the occurs everywhere.,pic_cs-410_7_8_780.jpg
cs-410_7_8_172,cs-410,7,8,Paradigmatic,"00:13:57,816","00:14:02,787",172,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=837,So matching the is not as such,pic_cs-410_7_8_780.jpg
cs-410_7_8_173,cs-410,7,8,Paradigmatic,"00:14:02,787","00:14:07,956",173,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=842,"a word like eats,",pic_cs-410_7_8_840.jpg
cs-410_7_8_174,cs-410,7,8,Paradigmatic,"00:14:07,956","00:14:11,216",174,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=847,So this is another,pic_cs-410_7_8_840.jpg
cs-410_7_8_175,cs-410,7,8,Paradigmatic,"00:14:13,426","00:14:19,003",175,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=853,In the next chapter we are going to talk,pic_cs-410_7_8_840.jpg
cs-410_7_8_176,cs-410,7,8,Paradigmatic,"00:14:19,003","00:14:29,003",176,https://www.coursera.org/learn/cs-410/lecture/wBtIp?t=859,[MUSIC],pic_cs-410_7_8_840.jpg
cs-410_7_9_1,cs-410,7,9,Paradigmatic,"00:00:05,960","00:00:08,625",1,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=5,"In this lecture, we continue",pic_cs-410_7_9_0.jpg
cs-410_7_9_2,cs-410,7,9,Paradigmatic,"00:00:08,625","00:00:11,565",2,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=8,discussing Paradigmatical,pic_cs-410_7_9_0.jpg
cs-410_7_9_3,cs-410,7,9,Paradigmatic,"00:00:11,565","00:00:14,175",3,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=11,Earlier we introduced,pic_cs-410_7_9_0.jpg
cs-410_7_9_4,cs-410,7,9,Paradigmatic,"00:00:14,175","00:00:16,935",4,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=14,Expected Overlap of,pic_cs-410_7_9_0.jpg
cs-410_7_9_5,cs-410,7,9,Paradigmatic,"00:00:16,935","00:00:21,090",5,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=16,"In this method, we",pic_cs-410_7_9_0.jpg
cs-410_7_9_6,cs-410,7,9,Paradigmatic,"00:00:21,090","00:00:23,040",6,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=21,a word vector that represents,pic_cs-410_7_9_0.jpg
cs-410_7_9_7,cs-410,7,9,Paradigmatic,"00:00:23,040","00:00:26,490",7,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=23,the probability of a,pic_cs-410_7_9_0.jpg
cs-410_7_9_8,cs-410,7,9,Paradigmatic,"00:00:26,490","00:00:30,345",8,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=26,We measure the similarity,pic_cs-410_7_9_0.jpg
cs-410_7_9_9,cs-410,7,9,Paradigmatic,"00:00:30,345","00:00:34,320",9,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=30,which can be interpreted as,pic_cs-410_7_9_0.jpg
cs-410_7_9_10,cs-410,7,9,Paradigmatic,"00:00:34,320","00:00:36,240",10,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=34,randomly picked words from,pic_cs-410_7_9_0.jpg
cs-410_7_9_11,cs-410,7,9,Paradigmatic,"00:00:36,240","00:00:38,585",11,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=36,the two contexts are identical.,pic_cs-410_7_9_0.jpg
cs-410_7_9_12,cs-410,7,9,Paradigmatic,"00:00:38,585","00:00:42,515",12,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=38,We also discussed,pic_cs-410_7_9_0.jpg
cs-410_7_9_13,cs-410,7,9,Paradigmatic,"00:00:42,515","00:00:45,920",13,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=42,The first is that,pic_cs-410_7_9_0.jpg
cs-410_7_9_14,cs-410,7,9,Paradigmatic,"00:00:45,920","00:00:47,900",14,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=45,one frequent term very well over,pic_cs-410_7_9_0.jpg
cs-410_7_9_15,cs-410,7,9,Paradigmatic,"00:00:47,900","00:00:50,390",15,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=47,matching more distinct terms.,pic_cs-410_7_9_0.jpg
cs-410_7_9_16,cs-410,7,9,Paradigmatic,"00:00:50,390","00:00:55,235",16,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=50,It put too much emphasis on,pic_cs-410_7_9_0.jpg
cs-410_7_9_17,cs-410,7,9,Paradigmatic,"00:00:55,235","00:01:00,350",17,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=55,The second is that it,pic_cs-410_7_9_0.jpg
cs-410_7_9_18,cs-410,7,9,Paradigmatic,"00:01:00,350","00:01:03,995",18,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=60,Even a common word like,pic_cs-410_7_9_60.jpg
cs-410_7_9_19,cs-410,7,9,Paradigmatic,"00:01:03,995","00:01:08,885",19,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=63,equally as content,pic_cs-410_7_9_60.jpg
cs-410_7_9_20,cs-410,7,9,Paradigmatic,"00:01:08,885","00:01:11,270",20,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=68,So now we are,pic_cs-410_7_9_60.jpg
cs-410_7_9_21,cs-410,7,9,Paradigmatic,"00:01:11,270","00:01:13,715",21,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=71,going to talk about how,pic_cs-410_7_9_60.jpg
cs-410_7_9_22,cs-410,7,9,Paradigmatic,"00:01:13,715","00:01:15,965",22,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=73,"More specifically, we're",pic_cs-410_7_9_60.jpg
cs-410_7_9_23,cs-410,7,9,Paradigmatic,"00:01:15,965","00:01:19,790",23,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=75,some retrieval heuristics,pic_cs-410_7_9_60.jpg
cs-410_7_9_24,cs-410,7,9,Paradigmatic,"00:01:19,790","00:01:23,900",24,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=79,These heuristics can effectively,pic_cs-410_7_9_60.jpg
cs-410_7_9_25,cs-410,7,9,Paradigmatic,"00:01:23,900","00:01:26,975",25,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=83,as these problems also,pic_cs-410_7_9_60.jpg
cs-410_7_9_26,cs-410,7,9,Paradigmatic,"00:01:26,975","00:01:30,680",26,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=86,when we match a query that,pic_cs-410_7_9_60.jpg
cs-410_7_9_27,cs-410,7,9,Paradigmatic,"00:01:30,680","00:01:32,920",27,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=90,"So to address the first problem,",pic_cs-410_7_9_60.jpg
cs-410_7_9_28,cs-410,7,9,Paradigmatic,"00:01:32,920","00:01:36,385",28,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=92,we can use a sublinear,pic_cs-410_7_9_60.jpg
cs-410_7_9_29,cs-410,7,9,Paradigmatic,"00:01:36,385","00:01:37,970",29,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=96,"That is, we don't have to use",pic_cs-410_7_9_60.jpg
cs-410_7_9_30,cs-410,7,9,Paradigmatic,"00:01:37,970","00:01:39,650",30,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=97,the raw frequency count of,pic_cs-410_7_9_60.jpg
cs-410_7_9_31,cs-410,7,9,Paradigmatic,"00:01:39,650","00:01:42,140",31,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=99,a term to represent the context.,pic_cs-410_7_9_60.jpg
cs-410_7_9_32,cs-410,7,9,Paradigmatic,"00:01:42,140","00:01:44,780",32,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=102,We can transform,pic_cs-410_7_9_60.jpg
cs-410_7_9_33,cs-410,7,9,Paradigmatic,"00:01:44,780","00:01:48,025",33,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=104,that wouldn't emphasize so,pic_cs-410_7_9_60.jpg
cs-410_7_9_34,cs-410,7,9,Paradigmatic,"00:01:48,025","00:01:50,130",34,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=108,To address the,pic_cs-410_7_9_60.jpg
cs-410_7_9_35,cs-410,7,9,Paradigmatic,"00:01:50,130","00:01:53,195",35,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=110,we can put more weight,pic_cs-410_7_9_60.jpg
cs-410_7_9_36,cs-410,7,9,Paradigmatic,"00:01:53,195","00:01:56,330",36,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=113,That is we can reward,pic_cs-410_7_9_60.jpg
cs-410_7_9_37,cs-410,7,9,Paradigmatic,"00:01:56,330","00:01:58,760",37,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=116,This heuristic is called the IDF,pic_cs-410_7_9_60.jpg
cs-410_7_9_38,cs-410,7,9,Paradigmatic,"00:01:58,760","00:02:01,135",38,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=118,term weighting in text retrieval.,pic_cs-410_7_9_60.jpg
cs-410_7_9_39,cs-410,7,9,Paradigmatic,"00:02:01,135","00:02:05,085",39,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=121,IDF stands for,pic_cs-410_7_9_120.jpg
cs-410_7_9_40,cs-410,7,9,Paradigmatic,"00:02:05,085","00:02:07,010",40,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=125,"So now, we're going to talk about",pic_cs-410_7_9_120.jpg
cs-410_7_9_41,cs-410,7,9,Paradigmatic,"00:02:07,010","00:02:10,130",41,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=127,the two heuristics,pic_cs-410_7_9_120.jpg
cs-410_7_9_42,cs-410,7,9,Paradigmatic,"00:02:10,130","00:02:13,930",42,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=130,First let's talk about,pic_cs-410_7_9_120.jpg
cs-410_7_9_43,cs-410,7,9,Paradigmatic,"00:02:13,930","00:02:16,400",43,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=133,That is to convert,pic_cs-410_7_9_120.jpg
cs-410_7_9_44,cs-410,7,9,Paradigmatic,"00:02:16,400","00:02:19,565",44,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=136,a word in the document,pic_cs-410_7_9_120.jpg
cs-410_7_9_45,cs-410,7,9,Paradigmatic,"00:02:19,565","00:02:23,195",45,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=139,that reflects our belief,pic_cs-410_7_9_120.jpg
cs-410_7_9_46,cs-410,7,9,Paradigmatic,"00:02:23,195","00:02:27,200",46,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=143,about how important,pic_cs-410_7_9_120.jpg
cs-410_7_9_47,cs-410,7,9,Paradigmatic,"00:02:27,200","00:02:32,370",47,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=147,So that will be,pic_cs-410_7_9_120.jpg
cs-410_7_9_48,cs-410,7,9,Paradigmatic,"00:02:32,370","00:02:36,415",48,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=152,That's shown in the y-axis.,pic_cs-410_7_9_120.jpg
cs-410_7_9_49,cs-410,7,9,Paradigmatic,"00:02:36,415","00:02:39,920",49,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=156,"Now, in general, there are",pic_cs-410_7_9_120.jpg
cs-410_7_9_50,cs-410,7,9,Paradigmatic,"00:02:39,920","00:02:44,250",50,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=159,Let's first look at,pic_cs-410_7_9_120.jpg
cs-410_7_9_51,cs-410,7,9,Paradigmatic,"00:02:44,250","00:02:47,920",51,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=164,"In this case, we're",pic_cs-410_7_9_120.jpg
cs-410_7_9_52,cs-410,7,9,Paradigmatic,"00:02:47,920","00:02:51,510",52,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=167,any non-zero counts,pic_cs-410_7_9_120.jpg
cs-410_7_9_53,cs-410,7,9,Paradigmatic,"00:02:51,510","00:02:55,450",53,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=171,one and the zero count,pic_cs-410_7_9_120.jpg
cs-410_7_9_54,cs-410,7,9,Paradigmatic,"00:02:55,450","00:02:56,990",54,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=175,So with this mapping,pic_cs-410_7_9_120.jpg
cs-410_7_9_55,cs-410,7,9,Paradigmatic,"00:02:56,990","00:02:59,240",55,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=176,all the frequencies will be,pic_cs-410_7_9_120.jpg
cs-410_7_9_56,cs-410,7,9,Paradigmatic,"00:02:59,240","00:03:02,605",56,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=179,mapped to only two,pic_cs-410_7_9_120.jpg
cs-410_7_9_57,cs-410,7,9,Paradigmatic,"00:03:02,605","00:03:11,015",57,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=182,The mapping function is shown,pic_cs-410_7_9_180.jpg
cs-410_7_9_58,cs-410,7,9,Paradigmatic,"00:03:11,015","00:03:14,030",58,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=191,"Now, this is naive",pic_cs-410_7_9_180.jpg
cs-410_7_9_59,cs-410,7,9,Paradigmatic,"00:03:14,030","00:03:16,660",59,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=194,because it's not,pic_cs-410_7_9_180.jpg
cs-410_7_9_60,cs-410,7,9,Paradigmatic,"00:03:16,660","00:03:20,195",60,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=196,"However, this actually",pic_cs-410_7_9_180.jpg
cs-410_7_9_61,cs-410,7,9,Paradigmatic,"00:03:20,195","00:03:25,700",61,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=200,emphasizing matching all,pic_cs-410_7_9_180.jpg
cs-410_7_9_62,cs-410,7,9,Paradigmatic,"00:03:25,700","00:03:27,725",62,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=205,So it does not allow,pic_cs-410_7_9_180.jpg
cs-410_7_9_63,cs-410,7,9,Paradigmatic,"00:03:27,725","00:03:30,505",63,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=207,a frequency of word to,pic_cs-410_7_9_180.jpg
cs-410_7_9_64,cs-410,7,9,Paradigmatic,"00:03:30,505","00:03:32,930",64,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=210,"Now, the approach",pic_cs-410_7_9_180.jpg
cs-410_7_9_65,cs-410,7,9,Paradigmatic,"00:03:32,930","00:03:36,650",65,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=212,earlier in the expected,pic_cs-410_7_9_180.jpg
cs-410_7_9_66,cs-410,7,9,Paradigmatic,"00:03:36,650","00:03:38,225",66,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=216,is a linear transformation.,pic_cs-410_7_9_180.jpg
cs-410_7_9_67,cs-410,7,9,Paradigmatic,"00:03:38,225","00:03:41,870",67,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=218,"We basically, take",pic_cs-410_7_9_180.jpg
cs-410_7_9_68,cs-410,7,9,Paradigmatic,"00:03:41,870","00:03:45,445",68,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=221,So we use the raw count,pic_cs-410_7_9_180.jpg
cs-410_7_9_69,cs-410,7,9,Paradigmatic,"00:03:45,445","00:03:48,140",69,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=225,That created the problem,pic_cs-410_7_9_180.jpg
cs-410_7_9_70,cs-410,7,9,Paradigmatic,"00:03:48,140","00:03:50,360",70,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=228,that we just talked about namely;,pic_cs-410_7_9_180.jpg
cs-410_7_9_71,cs-410,7,9,Paradigmatic,"00:03:50,360","00:03:54,935",71,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=230,it emphasize too much on just,pic_cs-410_7_9_180.jpg
cs-410_7_9_72,cs-410,7,9,Paradigmatic,"00:03:54,935","00:03:58,520",72,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=234,Matching one frequent term,pic_cs-410_7_9_180.jpg
cs-410_7_9_73,cs-410,7,9,Paradigmatic,"00:03:58,520","00:04:02,750",73,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=238,So we can have a lot,pic_cs-410_7_9_180.jpg
cs-410_7_9_74,cs-410,7,9,Paradigmatic,"00:04:02,750","00:04:04,475",74,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=242,of other interesting,pic_cs-410_7_9_240.jpg
cs-410_7_9_75,cs-410,7,9,Paradigmatic,"00:04:04,475","00:04:06,875",75,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=244,"in between the two extremes,",pic_cs-410_7_9_240.jpg
cs-410_7_9_76,cs-410,7,9,Paradigmatic,"00:04:06,875","00:04:10,640",76,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=246,and they generally form,pic_cs-410_7_9_240.jpg
cs-410_7_9_77,cs-410,7,9,Paradigmatic,"00:04:10,640","00:04:13,340",77,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=250,"So for example,",pic_cs-410_7_9_240.jpg
cs-410_7_9_78,cs-410,7,9,Paradigmatic,"00:04:13,340","00:04:16,080",78,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=253,"logarithm of the raw count,",pic_cs-410_7_9_240.jpg
cs-410_7_9_79,cs-410,7,9,Paradigmatic,"00:04:16,080","00:04:19,400",79,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=256,and this will give us curve,pic_cs-410_7_9_240.jpg
cs-410_7_9_80,cs-410,7,9,Paradigmatic,"00:04:19,400","00:04:21,260",80,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=259,that you are seeing here.,pic_cs-410_7_9_240.jpg
cs-410_7_9_81,cs-410,7,9,Paradigmatic,"00:04:21,260","00:04:25,295",81,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=261,"In this case, you can see",pic_cs-410_7_9_240.jpg
cs-410_7_9_82,cs-410,7,9,Paradigmatic,"00:04:25,295","00:04:29,330",82,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=265,The high counts are,pic_cs-410_7_9_240.jpg
cs-410_7_9_83,cs-410,7,9,Paradigmatic,"00:04:29,330","00:04:33,470",83,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=269,so the curve is a sublinear,pic_cs-410_7_9_240.jpg
cs-410_7_9_84,cs-410,7,9,Paradigmatic,"00:04:33,470","00:04:39,240",84,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=273,the weight of,pic_cs-410_7_9_240.jpg
cs-410_7_9_85,cs-410,7,9,Paradigmatic,"00:04:39,240","00:04:42,875",85,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=279,"This is what we want,",pic_cs-410_7_9_240.jpg
cs-410_7_9_86,cs-410,7,9,Paradigmatic,"00:04:42,875","00:04:47,340",86,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=282,terms from dominating,pic_cs-410_7_9_240.jpg
cs-410_7_9_87,cs-410,7,9,Paradigmatic,"00:04:48,620","00:04:50,870",87,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=288,"Now, there is also",pic_cs-410_7_9_240.jpg
cs-410_7_9_88,cs-410,7,9,Paradigmatic,"00:04:50,870","00:04:52,760",88,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=290,another interesting,pic_cs-410_7_9_240.jpg
cs-410_7_9_89,cs-410,7,9,Paradigmatic,"00:04:52,760","00:04:55,430",89,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=292,a BM25 transformation which,pic_cs-410_7_9_240.jpg
cs-410_7_9_90,cs-410,7,9,Paradigmatic,"00:04:55,430","00:04:59,945",90,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=295,has been shown to be very,pic_cs-410_7_9_240.jpg
cs-410_7_9_91,cs-410,7,9,Paradigmatic,"00:04:59,945","00:05:02,735",91,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=299,"In this transformation, we have",pic_cs-410_7_9_240.jpg
cs-410_7_9_92,cs-410,7,9,Paradigmatic,"00:05:02,735","00:05:07,225",92,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=302,a form that looks like this.,pic_cs-410_7_9_300.jpg
cs-410_7_9_93,cs-410,7,9,Paradigmatic,"00:05:07,225","00:05:11,640",93,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=307,So it's k plus one multiplied,pic_cs-410_7_9_300.jpg
cs-410_7_9_94,cs-410,7,9,Paradigmatic,"00:05:11,640","00:05:13,800",94,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=311,"where k is a parameter,",pic_cs-410_7_9_300.jpg
cs-410_7_9_95,cs-410,7,9,Paradigmatic,"00:05:13,800","00:05:16,485",95,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=313,"x is the count,",pic_cs-410_7_9_300.jpg
cs-410_7_9_96,cs-410,7,9,Paradigmatic,"00:05:16,485","00:05:18,690",96,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=316,the raw count of a word.,pic_cs-410_7_9_300.jpg
cs-410_7_9_97,cs-410,7,9,Paradigmatic,"00:05:18,690","00:05:22,190",97,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=318,"Now, the transformation",pic_cs-410_7_9_300.jpg
cs-410_7_9_98,cs-410,7,9,Paradigmatic,"00:05:22,190","00:05:25,430",98,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=322,that it can actually go from,pic_cs-410_7_9_300.jpg
cs-410_7_9_99,cs-410,7,9,Paradigmatic,"00:05:25,430","00:05:28,910",99,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=325,one extreme to the other,pic_cs-410_7_9_300.jpg
cs-410_7_9_100,cs-410,7,9,Paradigmatic,"00:05:28,910","00:05:34,725",100,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=328,k. It also interesting,pic_cs-410_7_9_300.jpg
cs-410_7_9_101,cs-410,7,9,Paradigmatic,"00:05:34,725","00:05:37,135",101,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=334,k plus one in this case.,pic_cs-410_7_9_300.jpg
cs-410_7_9_102,cs-410,7,9,Paradigmatic,"00:05:37,135","00:05:41,435",102,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=337,So this puts,pic_cs-410_7_9_300.jpg
cs-410_7_9_103,cs-410,7,9,Paradigmatic,"00:05:41,435","00:05:43,040",103,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=341,"on high frequency terms,",pic_cs-410_7_9_300.jpg
cs-410_7_9_104,cs-410,7,9,Paradigmatic,"00:05:43,040","00:05:46,460",104,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=343,because their weight would,pic_cs-410_7_9_300.jpg
cs-410_7_9_105,cs-410,7,9,Paradigmatic,"00:05:46,460","00:05:50,900",105,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=346,"As we vary k, if we can",pic_cs-410_7_9_300.jpg
cs-410_7_9_106,cs-410,7,9,Paradigmatic,"00:05:50,900","00:05:52,590",106,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=350,"So when k is set to zero,",pic_cs-410_7_9_300.jpg
cs-410_7_9_107,cs-410,7,9,Paradigmatic,"00:05:52,590","00:05:55,680",107,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=352,"we roughly have the 0,1 vector.",pic_cs-410_7_9_300.jpg
cs-410_7_9_108,cs-410,7,9,Paradigmatic,"00:05:55,680","00:05:59,090",108,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=355,Whereas when we set k,pic_cs-410_7_9_300.jpg
cs-410_7_9_109,cs-410,7,9,Paradigmatic,"00:05:59,090","00:06:02,075",109,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=359,it will behave more like,pic_cs-410_7_9_300.jpg
cs-410_7_9_110,cs-410,7,9,Paradigmatic,"00:06:02,075","00:06:05,270",110,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=362,So this transformation,pic_cs-410_7_9_360.jpg
cs-410_7_9_111,cs-410,7,9,Paradigmatic,"00:06:05,270","00:06:07,880",111,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=365,far the most effective,pic_cs-410_7_9_360.jpg
cs-410_7_9_112,cs-410,7,9,Paradigmatic,"00:06:07,880","00:06:10,880",112,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=367,text retrieval and it also makes,pic_cs-410_7_9_360.jpg
cs-410_7_9_113,cs-410,7,9,Paradigmatic,"00:06:10,880","00:06:14,285",113,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=370,sense for our problem setup.,pic_cs-410_7_9_360.jpg
cs-410_7_9_114,cs-410,7,9,Paradigmatic,"00:06:14,285","00:06:17,195",114,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=374,So we just talked about how,pic_cs-410_7_9_360.jpg
cs-410_7_9_115,cs-410,7,9,Paradigmatic,"00:06:17,195","00:06:20,660",115,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=377,overemphasizing a frequency term,pic_cs-410_7_9_360.jpg
cs-410_7_9_116,cs-410,7,9,Paradigmatic,"00:06:20,660","00:06:22,850",116,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=380,Now let's look at,pic_cs-410_7_9_360.jpg
cs-410_7_9_117,cs-410,7,9,Paradigmatic,"00:06:22,850","00:06:26,585",117,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=382,and that is how we can,pic_cs-410_7_9_360.jpg
cs-410_7_9_118,cs-410,7,9,Paradigmatic,"00:06:26,585","00:06:28,935",118,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=386,"Matching ""the"" is not surprising,",pic_cs-410_7_9_360.jpg
cs-410_7_9_119,cs-410,7,9,Paradigmatic,"00:06:28,935","00:06:30,645",119,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=388,"because ""the"" occurs everywhere.",pic_cs-410_7_9_360.jpg
cs-410_7_9_120,cs-410,7,9,Paradigmatic,"00:06:30,645","00:06:33,020",120,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=390,"But matching ""eats""",pic_cs-410_7_9_360.jpg
cs-410_7_9_121,cs-410,7,9,Paradigmatic,"00:06:33,020","00:06:35,105",121,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=393,So how can we address,pic_cs-410_7_9_360.jpg
cs-410_7_9_122,cs-410,7,9,Paradigmatic,"00:06:35,105","00:06:38,965",122,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=395,"Now in this case, we can",pic_cs-410_7_9_360.jpg
cs-410_7_9_123,cs-410,7,9,Paradigmatic,"00:06:38,965","00:06:42,095",123,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=398,That's commonly,pic_cs-410_7_9_360.jpg
cs-410_7_9_124,cs-410,7,9,Paradigmatic,"00:06:42,095","00:06:45,065",124,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=402,IDF stands for,pic_cs-410_7_9_360.jpg
cs-410_7_9_125,cs-410,7,9,Paradigmatic,"00:06:45,065","00:06:47,675",125,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=405,Document frequency,pic_cs-410_7_9_360.jpg
cs-410_7_9_126,cs-410,7,9,Paradigmatic,"00:06:47,675","00:06:49,370",126,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=407,of the total number of,pic_cs-410_7_9_360.jpg
cs-410_7_9_127,cs-410,7,9,Paradigmatic,"00:06:49,370","00:06:52,235",127,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=409,documents that contain,pic_cs-410_7_9_360.jpg
cs-410_7_9_128,cs-410,7,9,Paradigmatic,"00:06:52,235","00:06:57,200",128,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=412,So here we show that the IDF,pic_cs-410_7_9_360.jpg
cs-410_7_9_129,cs-410,7,9,Paradigmatic,"00:06:57,200","00:07:00,230",129,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=417,a logarithm function,pic_cs-410_7_9_360.jpg
cs-410_7_9_130,cs-410,7,9,Paradigmatic,"00:07:00,230","00:07:05,065",130,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=420,of documents that match a,pic_cs-410_7_9_420.jpg
cs-410_7_9_131,cs-410,7,9,Paradigmatic,"00:07:05,065","00:07:08,870",131,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=425,So K is the number of,pic_cs-410_7_9_420.jpg
cs-410_7_9_132,cs-410,7,9,Paradigmatic,"00:07:08,870","00:07:11,630",132,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=428,document frequency and M,pic_cs-410_7_9_420.jpg
cs-410_7_9_133,cs-410,7,9,Paradigmatic,"00:07:11,630","00:07:14,615",133,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=431,here is the total number of,pic_cs-410_7_9_420.jpg
cs-410_7_9_134,cs-410,7,9,Paradigmatic,"00:07:14,615","00:07:21,200",134,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=434,The IDF function is giving,pic_cs-410_7_9_420.jpg
cs-410_7_9_135,cs-410,7,9,Paradigmatic,"00:07:21,200","00:07:24,815",135,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=441,meaning that it,pic_cs-410_7_9_420.jpg
cs-410_7_9_136,cs-410,7,9,Paradigmatic,"00:07:24,815","00:07:28,805",136,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=444,The maximum value is,pic_cs-410_7_9_420.jpg
cs-410_7_9_137,cs-410,7,9,Paradigmatic,"00:07:28,805","00:07:33,650",137,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=448,That's when the word occurred,pic_cs-410_7_9_420.jpg
cs-410_7_9_138,cs-410,7,9,Paradigmatic,"00:07:33,650","00:07:37,235",138,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=453,"So that's a very rare term,",pic_cs-410_7_9_420.jpg
cs-410_7_9_139,cs-410,7,9,Paradigmatic,"00:07:37,235","00:07:40,745",139,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=457,the rare is term in,pic_cs-410_7_9_420.jpg
cs-410_7_9_140,cs-410,7,9,Paradigmatic,"00:07:40,745","00:07:46,700",140,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=460,The lowest value you can,pic_cs-410_7_9_420.jpg
cs-410_7_9_141,cs-410,7,9,Paradigmatic,"00:07:46,700","00:07:49,115",141,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=466,its maximum which would be M.,pic_cs-410_7_9_420.jpg
cs-410_7_9_142,cs-410,7,9,Paradigmatic,"00:07:49,115","00:07:53,880",142,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=469,So that would be,pic_cs-410_7_9_420.jpg
cs-410_7_9_143,cs-410,7,9,Paradigmatic,"00:07:53,990","00:07:57,340",143,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=473,close to zero in fact.,pic_cs-410_7_9_420.jpg
cs-410_7_9_144,cs-410,7,9,Paradigmatic,"00:07:57,470","00:08:02,360",144,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=477,So this of course measure,pic_cs-410_7_9_420.jpg
cs-410_7_9_145,cs-410,7,9,Paradigmatic,"00:08:02,360","00:08:06,740",145,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=482,is used in search where we,pic_cs-410_7_9_480.jpg
cs-410_7_9_146,cs-410,7,9,Paradigmatic,"00:08:06,740","00:08:09,960",146,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=486,"In our case, what would",pic_cs-410_7_9_480.jpg
cs-410_7_9_147,cs-410,7,9,Paradigmatic,"00:08:09,960","00:08:13,040",147,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=489,"Well, we can also",pic_cs-410_7_9_480.jpg
cs-410_7_9_148,cs-410,7,9,Paradigmatic,"00:08:13,040","00:08:16,610",148,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=493,we can collect all the words,pic_cs-410_7_9_480.jpg
cs-410_7_9_149,cs-410,7,9,Paradigmatic,"00:08:16,610","00:08:18,590",149,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=496,"That is to say,",pic_cs-410_7_9_480.jpg
cs-410_7_9_150,cs-410,7,9,Paradigmatic,"00:08:18,590","00:08:22,225",150,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=498,a word that's popular in,pic_cs-410_7_9_480.jpg
cs-410_7_9_151,cs-410,7,9,Paradigmatic,"00:08:22,225","00:08:25,650",151,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=502,would also have a low IDF.,pic_cs-410_7_9_480.jpg
cs-410_7_9_152,cs-410,7,9,Paradigmatic,"00:08:25,650","00:08:29,445",152,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=505,"Because depending on the dataset,",pic_cs-410_7_9_480.jpg
cs-410_7_9_153,cs-410,7,9,Paradigmatic,"00:08:29,445","00:08:35,105",153,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=509,we can construct the context,pic_cs-410_7_9_480.jpg
cs-410_7_9_154,cs-410,7,9,Paradigmatic,"00:08:35,105","00:08:38,010",154,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=515,But in the end if a term is,pic_cs-410_7_9_480.jpg
cs-410_7_9_155,cs-410,7,9,Paradigmatic,"00:08:38,010","00:08:41,024",155,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=518,very frequent in,pic_cs-410_7_9_480.jpg
cs-410_7_9_156,cs-410,7,9,Paradigmatic,"00:08:41,024","00:08:43,210",156,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=521,then it will still be frequent,pic_cs-410_7_9_480.jpg
cs-410_7_9_157,cs-410,7,9,Paradigmatic,"00:08:43,210","00:08:47,220",157,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=523,in the collective,pic_cs-410_7_9_480.jpg
cs-410_7_9_158,cs-410,7,9,Paradigmatic,"00:08:47,620","00:08:52,355",158,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=527,So how can we add,pic_cs-410_7_9_480.jpg
cs-410_7_9_159,cs-410,7,9,Paradigmatic,"00:08:52,355","00:08:56,910",159,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=532,improve our similarity function?,pic_cs-410_7_9_480.jpg
cs-410_7_9_160,cs-410,7,9,Paradigmatic,"00:08:56,910","00:08:58,565",160,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=536,"Well, here's one way",pic_cs-410_7_9_480.jpg
cs-410_7_9_161,cs-410,7,9,Paradigmatic,"00:08:58,565","00:09:00,920",161,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=538,many other ways,pic_cs-410_7_9_480.jpg
cs-410_7_9_162,cs-410,7,9,Paradigmatic,"00:09:00,920","00:09:02,520",162,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=540,"But this is a reasonable way,",pic_cs-410_7_9_540.jpg
cs-410_7_9_163,cs-410,7,9,Paradigmatic,"00:09:02,520","00:09:05,825",163,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=542,where we can adapt,pic_cs-410_7_9_540.jpg
cs-410_7_9_164,cs-410,7,9,Paradigmatic,"00:09:05,825","00:09:09,520",164,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=545,for paradigmatical,pic_cs-410_7_9_540.jpg
cs-410_7_9_165,cs-410,7,9,Paradigmatic,"00:09:14,120","00:09:20,555",165,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=554,"In this case, we define the",pic_cs-410_7_9_540.jpg
cs-410_7_9_166,cs-410,7,9,Paradigmatic,"00:09:20,555","00:09:26,825",166,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=560,elements representing,pic_cs-410_7_9_540.jpg
cs-410_7_9_167,cs-410,7,9,Paradigmatic,"00:09:26,825","00:09:29,810",167,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=566,So in this,pic_cs-410_7_9_540.jpg
cs-410_7_9_168,cs-410,7,9,Paradigmatic,"00:09:29,810","00:09:36,985",168,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=569,we take sum over all,pic_cs-410_7_9_540.jpg
cs-410_7_9_169,cs-410,7,9,Paradigmatic,"00:09:36,985","00:09:42,155",169,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=576,normalize the weight of,pic_cs-410_7_9_540.jpg
cs-410_7_9_170,cs-410,7,9,Paradigmatic,"00:09:42,155","00:09:48,210",170,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=582,of the weights of all the words.,pic_cs-410_7_9_540.jpg
cs-410_7_9_171,cs-410,7,9,Paradigmatic,"00:09:48,210","00:09:51,030",171,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=588,This is to again ensure all the,pic_cs-410_7_9_540.jpg
cs-410_7_9_172,cs-410,7,9,Paradigmatic,"00:09:51,030","00:09:53,975",172,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=591,xi's will sum to,pic_cs-410_7_9_540.jpg
cs-410_7_9_173,cs-410,7,9,Paradigmatic,"00:09:53,975","00:09:57,800",173,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=593,So this would be very similar,pic_cs-410_7_9_540.jpg
cs-410_7_9_174,cs-410,7,9,Paradigmatic,"00:09:57,800","00:09:59,420",174,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=597,in that this vector is,pic_cs-410_7_9_540.jpg
cs-410_7_9_175,cs-410,7,9,Paradigmatic,"00:09:59,420","00:10:04,015",175,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=599,actually something similar,pic_cs-410_7_9_540.jpg
cs-410_7_9_176,cs-410,7,9,Paradigmatic,"00:10:04,015","00:10:06,685",176,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=604,all the xi's will sum to one.,pic_cs-410_7_9_600.jpg
cs-410_7_9_177,cs-410,7,9,Paradigmatic,"00:10:06,685","00:10:13,560",177,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=606,"Now, the weight of BM25 for",pic_cs-410_7_9_600.jpg
cs-410_7_9_178,cs-410,7,9,Paradigmatic,"00:10:14,460","00:10:18,940",178,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=614,If you compare this with,pic_cs-410_7_9_600.jpg
cs-410_7_9_179,cs-410,7,9,Paradigmatic,"00:10:18,940","00:10:22,930",179,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=618,have a normalized count,pic_cs-410_7_9_600.jpg
cs-410_7_9_180,cs-410,7,9,Paradigmatic,"00:10:22,930","00:10:26,320",180,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=622,So we only have this one,pic_cs-410_7_9_600.jpg
cs-410_7_9_181,cs-410,7,9,Paradigmatic,"00:10:26,320","00:10:31,090",181,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=626,the total counts of words in,pic_cs-410_7_9_600.jpg
cs-410_7_9_182,cs-410,7,9,Paradigmatic,"00:10:31,090","00:10:33,430",182,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=631,and that's what we had before.,pic_cs-410_7_9_600.jpg
cs-410_7_9_183,cs-410,7,9,Paradigmatic,"00:10:33,430","00:10:36,039",183,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=633,But now with the BM25,pic_cs-410_7_9_600.jpg
cs-410_7_9_184,cs-410,7,9,Paradigmatic,"00:10:36,039","00:10:38,335",184,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=636,we introduced something else.,pic_cs-410_7_9_600.jpg
cs-410_7_9_185,cs-410,7,9,Paradigmatic,"00:10:38,335","00:10:42,040",185,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=638,"First, of course,",pic_cs-410_7_9_600.jpg
cs-410_7_9_186,cs-410,7,9,Paradigmatic,"00:10:42,040","00:10:43,420",186,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=642,this count is just to,pic_cs-410_7_9_600.jpg
cs-410_7_9_187,cs-410,7,9,Paradigmatic,"00:10:43,420","00:10:46,075",187,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=643,achieve the sub-linear,pic_cs-410_7_9_600.jpg
cs-410_7_9_188,cs-410,7,9,Paradigmatic,"00:10:46,075","00:10:50,155",188,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=646,But we also see we introduced,pic_cs-410_7_9_600.jpg
cs-410_7_9_189,cs-410,7,9,Paradigmatic,"00:10:50,155","00:10:56,110",189,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=650,and this parameter is,pic_cs-410_7_9_600.jpg
cs-410_7_9_190,cs-410,7,9,Paradigmatic,"00:10:56,110","00:10:58,810",190,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=656,although zero is also possible.,pic_cs-410_7_9_600.jpg
cs-410_7_9_191,cs-410,7,9,Paradigmatic,"00:10:58,810","00:11:02,950",191,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=658,But this controls,pic_cs-410_7_9_600.jpg
cs-410_7_9_192,cs-410,7,9,Paradigmatic,"00:11:02,950","00:11:06,535",192,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=662,and also controls to what extent,pic_cs-410_7_9_660.jpg
cs-410_7_9_193,cs-410,7,9,Paradigmatic,"00:11:06,535","00:11:11,240",193,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=666,it simulates the,pic_cs-410_7_9_660.jpg
cs-410_7_9_194,cs-410,7,9,Paradigmatic,"00:11:11,250","00:11:14,830",194,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=671,"So this is one parameter,",pic_cs-410_7_9_660.jpg
cs-410_7_9_195,cs-410,7,9,Paradigmatic,"00:11:14,830","00:11:17,140",195,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=674,but we also see there is,pic_cs-410_7_9_660.jpg
cs-410_7_9_196,cs-410,7,9,Paradigmatic,"00:11:17,140","00:11:21,115",196,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=677,"b, and this would be",pic_cs-410_7_9_660.jpg
cs-410_7_9_197,cs-410,7,9,Paradigmatic,"00:11:21,115","00:11:25,405",197,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=681,This is a parameter to,pic_cs-410_7_9_660.jpg
cs-410_7_9_198,cs-410,7,9,Paradigmatic,"00:11:25,405","00:11:27,294",198,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=685,"In this case,",pic_cs-410_7_9_660.jpg
cs-410_7_9_199,cs-410,7,9,Paradigmatic,"00:11:27,294","00:11:29,200",199,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=687,the normalization formula has,pic_cs-410_7_9_660.jpg
cs-410_7_9_200,cs-410,7,9,Paradigmatic,"00:11:29,200","00:11:31,885",200,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=689,a average document lens here.,pic_cs-410_7_9_660.jpg
cs-410_7_9_201,cs-410,7,9,Paradigmatic,"00:11:31,885","00:11:35,770",201,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=691,This is computed up,pic_cs-410_7_9_660.jpg
cs-410_7_9_202,cs-410,7,9,Paradigmatic,"00:11:35,770","00:11:39,880",202,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=695,of the lenses of all the,pic_cs-410_7_9_660.jpg
cs-410_7_9_203,cs-410,7,9,Paradigmatic,"00:11:39,880","00:11:41,605",203,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=699,"In this case, all the lenses of",pic_cs-410_7_9_660.jpg
cs-410_7_9_204,cs-410,7,9,Paradigmatic,"00:11:41,605","00:11:45,340",204,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=701,all the context of documents,pic_cs-410_7_9_660.jpg
cs-410_7_9_205,cs-410,7,9,Paradigmatic,"00:11:45,340","00:11:48,175",205,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=705,So this average documents,pic_cs-410_7_9_660.jpg
cs-410_7_9_206,cs-410,7,9,Paradigmatic,"00:11:48,175","00:11:50,425",206,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=708,will be a constant for,pic_cs-410_7_9_660.jpg
cs-410_7_9_207,cs-410,7,9,Paradigmatic,"00:11:50,425","00:11:52,795",207,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=710,So it actually is only,pic_cs-410_7_9_660.jpg
cs-410_7_9_208,cs-410,7,9,Paradigmatic,"00:11:52,795","00:11:56,530",208,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=712,affecting the effect,pic_cs-410_7_9_660.jpg
cs-410_7_9_209,cs-410,7,9,Paradigmatic,"00:11:56,530","00:12:01,180",209,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=716,"b, here because",pic_cs-410_7_9_660.jpg
cs-410_7_9_210,cs-410,7,9,Paradigmatic,"00:12:01,180","00:12:07,780",210,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=721,But I kept it here because,pic_cs-410_7_9_720.jpg
cs-410_7_9_211,cs-410,7,9,Paradigmatic,"00:12:07,780","00:12:10,840",211,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=727,for in retrieval where it would,pic_cs-410_7_9_720.jpg
cs-410_7_9_212,cs-410,7,9,Paradigmatic,"00:12:10,840","00:12:14,770",212,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=730,give us a stabilized,pic_cs-410_7_9_720.jpg
cs-410_7_9_213,cs-410,7,9,Paradigmatic,"00:12:14,770","00:12:16,570",213,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=734,"But for our purpose,",pic_cs-410_7_9_720.jpg
cs-410_7_9_214,cs-410,7,9,Paradigmatic,"00:12:16,570","00:12:21,430",214,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=736,this will be a constant so,pic_cs-410_7_9_720.jpg
cs-410_7_9_215,cs-410,7,9,Paradigmatic,"00:12:21,430","00:12:28,550",215,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=741,the lens normalization,pic_cs-410_7_9_720.jpg
cs-410_7_9_216,cs-410,7,9,Paradigmatic,"00:12:29,400","00:12:33,295",216,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=749,"Now, with this definition then,",pic_cs-410_7_9_720.jpg
cs-410_7_9_217,cs-410,7,9,Paradigmatic,"00:12:33,295","00:12:37,810",217,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=753,we have a new way to define,pic_cs-410_7_9_720.jpg
cs-410_7_9_218,cs-410,7,9,Paradigmatic,"00:12:37,810","00:12:41,785",218,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=757,and we can compute,pic_cs-410_7_9_720.jpg
cs-410_7_9_219,cs-410,7,9,Paradigmatic,"00:12:41,785","00:12:43,255",219,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=761,The difference is that,pic_cs-410_7_9_720.jpg
cs-410_7_9_220,cs-410,7,9,Paradigmatic,"00:12:43,255","00:12:44,950",220,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=763,the high-frequency terms will now,pic_cs-410_7_9_720.jpg
cs-410_7_9_221,cs-410,7,9,Paradigmatic,"00:12:44,950","00:12:46,930",221,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=764,have a somewhat lower weights.,pic_cs-410_7_9_720.jpg
cs-410_7_9_222,cs-410,7,9,Paradigmatic,"00:12:46,930","00:12:49,690",222,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=766,This would help us control,pic_cs-410_7_9_720.jpg
cs-410_7_9_223,cs-410,7,9,Paradigmatic,"00:12:49,690","00:12:53,575",223,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=769,the inference of,pic_cs-410_7_9_720.jpg
cs-410_7_9_224,cs-410,7,9,Paradigmatic,"00:12:53,575","00:12:58,000",224,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=773,"Now, the idea can be added",pic_cs-410_7_9_720.jpg
cs-410_7_9_225,cs-410,7,9,Paradigmatic,"00:12:58,000","00:12:59,905",225,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=778,That means we'll,pic_cs-410_7_9_720.jpg
cs-410_7_9_226,cs-410,7,9,Paradigmatic,"00:12:59,905","00:13:01,990",226,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=779,for matching each term.,pic_cs-410_7_9_720.jpg
cs-410_7_9_227,cs-410,7,9,Paradigmatic,"00:13:01,990","00:13:05,650",227,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=781,So you may recall,pic_cs-410_7_9_780.jpg
cs-410_7_9_228,cs-410,7,9,Paradigmatic,"00:13:05,650","00:13:08,305",228,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=785,all the possible words,pic_cs-410_7_9_780.jpg
cs-410_7_9_229,cs-410,7,9,Paradigmatic,"00:13:08,305","00:13:11,365",229,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=788,overlap between the two contexts.,pic_cs-410_7_9_780.jpg
cs-410_7_9_230,cs-410,7,9,Paradigmatic,"00:13:11,365","00:13:15,790",230,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=791,The x_i and the y_i,pic_cs-410_7_9_780.jpg
cs-410_7_9_231,cs-410,7,9,Paradigmatic,"00:13:15,790","00:13:20,245",231,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=795,of picking the word,pic_cs-410_7_9_780.jpg
cs-410_7_9_232,cs-410,7,9,Paradigmatic,"00:13:20,245","00:13:22,330",232,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=800,"Therefore, it",pic_cs-410_7_9_780.jpg
cs-410_7_9_233,cs-410,7,9,Paradigmatic,"00:13:22,330","00:13:24,805",233,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=802,we'll see a match on this word.,pic_cs-410_7_9_780.jpg
cs-410_7_9_234,cs-410,7,9,Paradigmatic,"00:13:24,805","00:13:26,695",234,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=804,"Now, IDF would give us",pic_cs-410_7_9_780.jpg
cs-410_7_9_235,cs-410,7,9,Paradigmatic,"00:13:26,695","00:13:29,200",235,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=806,the importance of,pic_cs-410_7_9_780.jpg
cs-410_7_9_236,cs-410,7,9,Paradigmatic,"00:13:29,200","00:13:33,700",236,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=809,A common word will be worth,pic_cs-410_7_9_780.jpg
cs-410_7_9_237,cs-410,7,9,Paradigmatic,"00:13:33,700","00:13:36,715",237,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=813,So we emphasize more on,pic_cs-410_7_9_780.jpg
cs-410_7_9_238,cs-410,7,9,Paradigmatic,"00:13:36,715","00:13:38,785",238,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=816,"So with this modification,",pic_cs-410_7_9_780.jpg
cs-410_7_9_239,cs-410,7,9,Paradigmatic,"00:13:38,785","00:13:40,660",239,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=818,then the new function will,pic_cs-410_7_9_780.jpg
cs-410_7_9_240,cs-410,7,9,Paradigmatic,"00:13:40,660","00:13:43,270",240,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=820,likely address,pic_cs-410_7_9_780.jpg
cs-410_7_9_241,cs-410,7,9,Paradigmatic,"00:13:43,270","00:13:45,310",241,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=823,"Now, interestingly",pic_cs-410_7_9_780.jpg
cs-410_7_9_242,cs-410,7,9,Paradigmatic,"00:13:45,310","00:13:49,825",242,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=825,this approach to discover,pic_cs-410_7_9_780.jpg
cs-410_7_9_243,cs-410,7,9,Paradigmatic,"00:13:49,825","00:13:57,430",243,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=829,"In general, when we re-brand",pic_cs-410_7_9_780.jpg
cs-410_7_9_244,cs-410,7,9,Paradigmatic,"00:13:57,430","00:13:59,365",244,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=837,"a context with a term vector,",pic_cs-410_7_9_780.jpg
cs-410_7_9_245,cs-410,7,9,Paradigmatic,"00:13:59,365","00:14:01,900",245,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=839,we would likely see,pic_cs-410_7_9_780.jpg
cs-410_7_9_246,cs-410,7,9,Paradigmatic,"00:14:01,900","00:14:04,135",246,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=841,some terms have high weights,pic_cs-410_7_9_840.jpg
cs-410_7_9_247,cs-410,7,9,Paradigmatic,"00:14:04,135","00:14:06,040",247,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=844,and other terms have low weights.,pic_cs-410_7_9_840.jpg
cs-410_7_9_248,cs-410,7,9,Paradigmatic,"00:14:06,040","00:14:09,490",248,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=846,Depending on how we assign,pic_cs-410_7_9_840.jpg
cs-410_7_9_249,cs-410,7,9,Paradigmatic,"00:14:09,490","00:14:11,650",249,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=849,we might be able to,pic_cs-410_7_9_840.jpg
cs-410_7_9_250,cs-410,7,9,Paradigmatic,"00:14:11,650","00:14:13,720",250,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=851,discover the words that,pic_cs-410_7_9_840.jpg
cs-410_7_9_251,cs-410,7,9,Paradigmatic,"00:14:13,720","00:14:15,700",251,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=853,are strongly associated with,pic_cs-410_7_9_840.jpg
cs-410_7_9_252,cs-410,7,9,Paradigmatic,"00:14:15,700","00:14:18,400",252,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=855,the candidate word,pic_cs-410_7_9_840.jpg
cs-410_7_9_253,cs-410,7,9,Paradigmatic,"00:14:18,400","00:14:20,560",253,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=858,So let's take a look at,pic_cs-410_7_9_840.jpg
cs-410_7_9_254,cs-410,7,9,Paradigmatic,"00:14:20,560","00:14:23,815",254,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=860,the term vector in,pic_cs-410_7_9_840.jpg
cs-410_7_9_255,cs-410,7,9,Paradigmatic,"00:14:23,815","00:14:29,885",255,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=863,We have each x_i,pic_cs-410_7_9_840.jpg
cs-410_7_9_256,cs-410,7,9,Paradigmatic,"00:14:29,885","00:14:33,610",256,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=869,defined as the normalized,pic_cs-410_7_9_840.jpg
cs-410_7_9_257,cs-410,7,9,Paradigmatic,"00:14:33,610","00:14:37,420",257,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=873,"Now, this weight alone only",pic_cs-410_7_9_840.jpg
cs-410_7_9_258,cs-410,7,9,Paradigmatic,"00:14:37,420","00:14:41,110",258,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=877,reflects how frequent the word,pic_cs-410_7_9_840.jpg
cs-410_7_9_259,cs-410,7,9,Paradigmatic,"00:14:41,110","00:14:43,345",259,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=881,But we can't just say,pic_cs-410_7_9_840.jpg
cs-410_7_9_260,cs-410,7,9,Paradigmatic,"00:14:43,345","00:14:44,500",260,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=883,any frequent term in,pic_cs-410_7_9_840.jpg
cs-410_7_9_261,cs-410,7,9,Paradigmatic,"00:14:44,500","00:14:46,560",261,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=884,the context that would,pic_cs-410_7_9_840.jpg
cs-410_7_9_262,cs-410,7,9,Paradigmatic,"00:14:46,560","00:14:50,235",262,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=886,the candidate word because,pic_cs-410_7_9_840.jpg
cs-410_7_9_263,cs-410,7,9,Paradigmatic,"00:14:50,235","00:14:51,990",263,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=890,many common words like 'the' will,pic_cs-410_7_9_840.jpg
cs-410_7_9_264,cs-410,7,9,Paradigmatic,"00:14:51,990","00:14:54,540",264,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=891,occur frequently in,pic_cs-410_7_9_840.jpg
cs-410_7_9_265,cs-410,7,9,Paradigmatic,"00:14:54,540","00:14:59,645",265,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=894,But if we apply IDF,pic_cs-410_7_9_840.jpg
cs-410_7_9_266,cs-410,7,9,Paradigmatic,"00:14:59,645","00:15:07,090",266,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=899,we can then re-weight,pic_cs-410_7_9_840.jpg
cs-410_7_9_267,cs-410,7,9,Paradigmatic,"00:15:07,090","00:15:09,220",267,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=907,That means the words that are,pic_cs-410_7_9_900.jpg
cs-410_7_9_268,cs-410,7,9,Paradigmatic,"00:15:09,220","00:15:11,920",268,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=909,common like 'the',pic_cs-410_7_9_900.jpg
cs-410_7_9_269,cs-410,7,9,Paradigmatic,"00:15:11,920","00:15:14,920",269,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=911,So now the highest,pic_cs-410_7_9_900.jpg
cs-410_7_9_270,cs-410,7,9,Paradigmatic,"00:15:14,920","00:15:18,220",270,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=914,those common terms because,pic_cs-410_7_9_900.jpg
cs-410_7_9_271,cs-410,7,9,Paradigmatic,"00:15:18,220","00:15:20,980",271,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=918,"Instead, those terms would",pic_cs-410_7_9_900.jpg
cs-410_7_9_272,cs-410,7,9,Paradigmatic,"00:15:20,980","00:15:23,920",272,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=920,be the terms that are,pic_cs-410_7_9_900.jpg
cs-410_7_9_273,cs-410,7,9,Paradigmatic,"00:15:23,920","00:15:26,080",273,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=923,but not frequent,pic_cs-410_7_9_900.jpg
cs-410_7_9_274,cs-410,7,9,Paradigmatic,"00:15:26,080","00:15:29,590",274,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=926,So those are clearly the words,pic_cs-410_7_9_900.jpg
cs-410_7_9_275,cs-410,7,9,Paradigmatic,"00:15:29,590","00:15:33,820",275,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=929,the context of the candidate,pic_cs-410_7_9_900.jpg
cs-410_7_9_276,cs-410,7,9,Paradigmatic,"00:15:33,820","00:15:35,365",276,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=933,"So for this reason,",pic_cs-410_7_9_900.jpg
cs-410_7_9_277,cs-410,7,9,Paradigmatic,"00:15:35,365","00:15:39,865",277,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=935,the highly weighted terms in,pic_cs-410_7_9_900.jpg
cs-410_7_9_278,cs-410,7,9,Paradigmatic,"00:15:39,865","00:15:42,310",278,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=939,can also be assumed to,pic_cs-410_7_9_900.jpg
cs-410_7_9_279,cs-410,7,9,Paradigmatic,"00:15:42,310","00:15:45,940",279,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=942,be candidates for,pic_cs-410_7_9_900.jpg
cs-410_7_9_280,cs-410,7,9,Paradigmatic,"00:15:45,940","00:15:48,895",280,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=945,"Now, of course, this is",pic_cs-410_7_9_900.jpg
cs-410_7_9_281,cs-410,7,9,Paradigmatic,"00:15:48,895","00:15:53,560",281,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=948,our approach for discovering,pic_cs-410_7_9_900.jpg
cs-410_7_9_282,cs-410,7,9,Paradigmatic,"00:15:53,560","00:15:57,025",282,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=953,"In the next lecture, we're",pic_cs-410_7_9_900.jpg
cs-410_7_9_283,cs-410,7,9,Paradigmatic,"00:15:57,025","00:16:01,850",283,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=957,how to discover,pic_cs-410_7_9_900.jpg
cs-410_7_9_284,cs-410,7,9,Paradigmatic,"00:16:02,280","00:16:05,305",284,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=962,But it clearly shows the relation,pic_cs-410_7_9_960.jpg
cs-410_7_9_285,cs-410,7,9,Paradigmatic,"00:16:05,305","00:16:08,995",285,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=965,between discovering,pic_cs-410_7_9_960.jpg
cs-410_7_9_286,cs-410,7,9,Paradigmatic,"00:16:08,995","00:16:12,670",286,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=968,Indeed they can be discovered in,pic_cs-410_7_9_960.jpg
cs-410_7_9_287,cs-410,7,9,Paradigmatic,"00:16:12,670","00:16:18,340",287,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=972,a joint manner by leveraging,pic_cs-410_7_9_960.jpg
cs-410_7_9_288,cs-410,7,9,Paradigmatic,"00:16:18,340","00:16:22,600",288,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=978,"So to summarize,",pic_cs-410_7_9_960.jpg
cs-410_7_9_289,cs-410,7,9,Paradigmatic,"00:16:22,600","00:16:26,050",289,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=982,paradigmatic relations is to,pic_cs-410_7_9_960.jpg
cs-410_7_9_290,cs-410,7,9,Paradigmatic,"00:16:26,050","00:16:27,610",290,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=986,collect the context of,pic_cs-410_7_9_960.jpg
cs-410_7_9_291,cs-410,7,9,Paradigmatic,"00:16:27,610","00:16:30,460",291,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=987,a candidate word to,pic_cs-410_7_9_960.jpg
cs-410_7_9_292,cs-410,7,9,Paradigmatic,"00:16:30,460","00:16:33,685",292,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=990,This is typically represented,pic_cs-410_7_9_960.jpg
cs-410_7_9_293,cs-410,7,9,Paradigmatic,"00:16:33,685","00:16:35,890",293,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=993,Then compute the similarity of,pic_cs-410_7_9_960.jpg
cs-410_7_9_294,cs-410,7,9,Paradigmatic,"00:16:35,890","00:16:38,005",294,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=995,the corresponding,pic_cs-410_7_9_960.jpg
cs-410_7_9_295,cs-410,7,9,Paradigmatic,"00:16:38,005","00:16:40,540",295,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=998,of two candidate words.,pic_cs-410_7_9_960.jpg
cs-410_7_9_296,cs-410,7,9,Paradigmatic,"00:16:40,540","00:16:45,910",296,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1000,Then we can take,pic_cs-410_7_9_960.jpg
cs-410_7_9_297,cs-410,7,9,Paradigmatic,"00:16:45,910","00:16:50,305",297,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1005,and treat them as having,pic_cs-410_7_9_960.jpg
cs-410_7_9_298,cs-410,7,9,Paradigmatic,"00:16:50,305","00:16:53,395",298,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1010,These are the words that,pic_cs-410_7_9_960.jpg
cs-410_7_9_299,cs-410,7,9,Paradigmatic,"00:16:53,395","00:16:55,540",299,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1013,There are many different ways to,pic_cs-410_7_9_960.jpg
cs-410_7_9_300,cs-410,7,9,Paradigmatic,"00:16:55,540","00:16:58,090",300,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1015,implement this general idea.,pic_cs-410_7_9_960.jpg
cs-410_7_9_301,cs-410,7,9,Paradigmatic,"00:16:58,090","00:17:01,435",301,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1018,We just talked about,pic_cs-410_7_9_960.jpg
cs-410_7_9_302,cs-410,7,9,Paradigmatic,"00:17:01,435","00:17:04,510",302,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1021,"More specifically, we",pic_cs-410_7_9_1020.jpg
cs-410_7_9_303,cs-410,7,9,Paradigmatic,"00:17:04,510","00:17:07,765",303,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1024,text retrieval models to help us,pic_cs-410_7_9_1020.jpg
cs-410_7_9_304,cs-410,7,9,Paradigmatic,"00:17:07,765","00:17:10,690",304,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1027,design effective,pic_cs-410_7_9_1020.jpg
cs-410_7_9_305,cs-410,7,9,Paradigmatic,"00:17:10,690","00:17:15,170",305,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1030,compute the,pic_cs-410_7_9_1020.jpg
cs-410_7_9_306,cs-410,7,9,Paradigmatic,"00:17:15,960","00:17:19,330",306,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1035,"More specifically, we have used",pic_cs-410_7_9_1020.jpg
cs-410_7_9_307,cs-410,7,9,Paradigmatic,"00:17:19,330","00:17:23,020",307,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1039,the BM25 and IDF weighting,pic_cs-410_7_9_1020.jpg
cs-410_7_9_308,cs-410,7,9,Paradigmatic,"00:17:23,020","00:17:27,250",308,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1043,to discover,pic_cs-410_7_9_1020.jpg
cs-410_7_9_309,cs-410,7,9,Paradigmatic,"00:17:27,250","00:17:30,100",309,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1047,These approaches also represent,pic_cs-410_7_9_1020.jpg
cs-410_7_9_310,cs-410,7,9,Paradigmatic,"00:17:30,100","00:17:33,310",310,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1050,the state of the art in,pic_cs-410_7_9_1020.jpg
cs-410_7_9_311,cs-410,7,9,Paradigmatic,"00:17:33,310","00:17:37,165",311,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1053,"Finally, syntagmatic relations",pic_cs-410_7_9_1020.jpg
cs-410_7_9_312,cs-410,7,9,Paradigmatic,"00:17:37,165","00:17:42,140",312,https://www.coursera.org/learn/cs-410/lecture/CV8fN?t=1057,as a by-product when we discover,pic_cs-410_7_9_1020.jpg
cs-410_8_1_1,cs-410,8,1,Syntagmatic,"00:00:00,250","00:00:06,380",1,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=0,[SOUND].,pic_cs-410_8_1_0.jpg
cs-410_8_1_2,cs-410,8,1,Syntagmatic,"00:00:06,380","00:00:13,220",2,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=6,This lecture is about the syntagmatic,pic_cs-410_8_1_0.jpg
cs-410_8_1_3,cs-410,8,1,Syntagmatic,"00:00:13,220","00:00:17,760",3,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=13,"In this lecture, we're going to continue",pic_cs-410_8_1_0.jpg
cs-410_8_1_4,cs-410,8,1,Syntagmatic,"00:00:17,760","00:00:22,420",4,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=17,"In particular, we're going to talk about",pic_cs-410_8_1_0.jpg
cs-410_8_1_5,cs-410,8,1,Syntagmatic,"00:00:22,420","00:00:25,770",5,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=22,And we're going to start with,pic_cs-410_8_1_0.jpg
cs-410_8_1_6,cs-410,8,1,Syntagmatic,"00:00:25,770","00:00:29,860",6,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=25,which is the basis for designing some,pic_cs-410_8_1_0.jpg
cs-410_8_1_7,cs-410,8,1,Syntagmatic,"00:00:32,480","00:00:33,110",7,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=32,"By definition,",pic_cs-410_8_1_0.jpg
cs-410_8_1_8,cs-410,8,1,Syntagmatic,"00:00:33,110","00:00:39,890",8,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=33,syntagmatic relations hold between words,pic_cs-410_8_1_0.jpg
cs-410_8_1_9,cs-410,8,1,Syntagmatic,"00:00:39,890","00:00:44,190",9,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=39,"That means,",pic_cs-410_8_1_0.jpg
cs-410_8_1_10,cs-410,8,1,Syntagmatic,"00:00:44,190","00:00:47,350",10,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=44,we tend to see the occurrence,pic_cs-410_8_1_0.jpg
cs-410_8_1_11,cs-410,8,1,Syntagmatic,"00:00:48,370","00:00:53,560",11,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=48,"So, take a more specific example, here.",pic_cs-410_8_1_0.jpg
cs-410_8_1_12,cs-410,8,1,Syntagmatic,"00:00:53,560","00:00:55,470",12,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=53,"We can ask the question,",pic_cs-410_8_1_0.jpg
cs-410_8_1_13,cs-410,8,1,Syntagmatic,"00:00:55,470","00:00:59,750",13,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=55,"whenever eats occurs,",pic_cs-410_8_1_0.jpg
cs-410_8_1_14,cs-410,8,1,Syntagmatic,"00:01:01,140","00:01:06,000",14,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=61,"Looking at the sentences on the left,",pic_cs-410_8_1_60.jpg
cs-410_8_1_15,cs-410,8,1,Syntagmatic,"00:01:06,000","00:01:11,030",15,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=66,"together with eats, like cat,",pic_cs-410_8_1_60.jpg
cs-410_8_1_16,cs-410,8,1,Syntagmatic,"00:01:11,030","00:01:15,870",16,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=71,But if I take them out and,pic_cs-410_8_1_60.jpg
cs-410_8_1_17,cs-410,8,1,Syntagmatic,"00:01:15,870","00:01:21,550",17,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=75,"only show eats and some other words,",pic_cs-410_8_1_60.jpg
cs-410_8_1_18,cs-410,8,1,Syntagmatic,"00:01:21,550","00:01:27,050",18,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=81,Can you predict what other words,pic_cs-410_8_1_60.jpg
cs-410_8_1_19,cs-410,8,1,Syntagmatic,"00:01:28,315","00:01:31,040",19,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=88,Right so,pic_cs-410_8_1_60.jpg
cs-410_8_1_20,cs-410,8,1,Syntagmatic,"00:01:31,040","00:01:33,630",20,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=91,other words are associated with eats.,pic_cs-410_8_1_60.jpg
cs-410_8_1_21,cs-410,8,1,Syntagmatic,"00:01:33,630","00:01:37,610",21,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=93,"If they are associated with eats,",pic_cs-410_8_1_60.jpg
cs-410_8_1_22,cs-410,8,1,Syntagmatic,"00:01:38,625","00:01:43,060",22,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=98,More specifically our,pic_cs-410_8_1_60.jpg
cs-410_8_1_23,cs-410,8,1,Syntagmatic,"00:01:43,060","00:01:47,072",23,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=103,"any text segment which can be a sentence,",pic_cs-410_8_1_60.jpg
cs-410_8_1_24,cs-410,8,1,Syntagmatic,"00:01:47,072","00:01:51,340",24,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=107,"And then ask I the question,",pic_cs-410_8_1_60.jpg
cs-410_8_1_25,cs-410,8,1,Syntagmatic,"00:01:51,340","00:01:52,640",25,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=111,absent in this segment?,pic_cs-410_8_1_60.jpg
cs-410_8_1_26,cs-410,8,1,Syntagmatic,"00:01:54,550","00:01:57,400",26,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=114,Right here we ask about the word W.,pic_cs-410_8_1_60.jpg
cs-410_8_1_27,cs-410,8,1,Syntagmatic,"00:01:57,400","00:02:00,160",27,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=117,Is W present or absent in this segment?,pic_cs-410_8_1_60.jpg
cs-410_8_1_28,cs-410,8,1,Syntagmatic,"00:02:02,400","00:02:05,100",28,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=122,Now what's interesting is that,pic_cs-410_8_1_120.jpg
cs-410_8_1_29,cs-410,8,1,Syntagmatic,"00:02:05,100","00:02:08,230",29,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=125,some words are actually easier,pic_cs-410_8_1_120.jpg
cs-410_8_1_30,cs-410,8,1,Syntagmatic,"00:02:10,150","00:02:14,570",30,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=130,If you take a look at the three,pic_cs-410_8_1_120.jpg
cs-410_8_1_31,cs-410,8,1,Syntagmatic,"00:02:14,570","00:02:17,970",31,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=134,"unicorn, which one do you",pic_cs-410_8_1_120.jpg
cs-410_8_1_32,cs-410,8,1,Syntagmatic,"00:02:20,630","00:02:23,530",32,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=140,Now if you think about it for,pic_cs-410_8_1_120.jpg
cs-410_8_1_33,cs-410,8,1,Syntagmatic,"00:02:24,530","00:02:27,910",33,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=144,the is easier to predict because,pic_cs-410_8_1_120.jpg
cs-410_8_1_34,cs-410,8,1,Syntagmatic,"00:02:27,910","00:02:30,770",34,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=147,"So I can just say,",pic_cs-410_8_1_120.jpg
cs-410_8_1_35,cs-410,8,1,Syntagmatic,"00:02:31,940","00:02:37,946",35,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=151,Unicorn is also relatively easy,pic_cs-410_8_1_120.jpg
cs-410_8_1_36,cs-410,8,1,Syntagmatic,"00:02:37,946","00:02:41,470",36,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=157,And I can bet that it doesn't,pic_cs-410_8_1_120.jpg
cs-410_8_1_37,cs-410,8,1,Syntagmatic,"00:02:42,780","00:02:46,080",37,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=162,But meat is somewhere in,pic_cs-410_8_1_120.jpg
cs-410_8_1_38,cs-410,8,1,Syntagmatic,"00:02:46,080","00:02:50,580",38,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=166,And it makes it harder to predict because,pic_cs-410_8_1_120.jpg
cs-410_8_1_39,cs-410,8,1,Syntagmatic,"00:02:50,580","00:02:52,520",39,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=170,"or the segment, more accurately.",pic_cs-410_8_1_120.jpg
cs-410_8_1_40,cs-410,8,1,Syntagmatic,"00:02:53,842","00:02:58,820",40,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=173,"But it may also not occur in the sentence,",pic_cs-410_8_1_120.jpg
cs-410_8_1_41,cs-410,8,1,Syntagmatic,"00:02:58,820","00:03:01,500",41,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=178,now let's study this,pic_cs-410_8_1_120.jpg
cs-410_8_1_42,cs-410,8,1,Syntagmatic,"00:03:02,680","00:03:06,090",42,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=182,So the problem can be formally defined,pic_cs-410_8_1_180.jpg
cs-410_8_1_43,cs-410,8,1,Syntagmatic,"00:03:06,090","00:03:10,030",43,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=186,as predicting the value of,pic_cs-410_8_1_180.jpg
cs-410_8_1_44,cs-410,8,1,Syntagmatic,"00:03:10,030","00:03:14,080",44,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=190,"Here we denote it by X sub w,",pic_cs-410_8_1_180.jpg
cs-410_8_1_45,cs-410,8,1,Syntagmatic,"00:03:14,080","00:03:17,340",45,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=194,this random variable is associated,pic_cs-410_8_1_180.jpg
cs-410_8_1_46,cs-410,8,1,Syntagmatic,"00:03:18,380","00:03:23,020",46,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=198,"When the value of the variable is 1,",pic_cs-410_8_1_180.jpg
cs-410_8_1_47,cs-410,8,1,Syntagmatic,"00:03:23,020","00:03:26,110",47,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=203,"When it's 0, it means the word is absent.",pic_cs-410_8_1_180.jpg
cs-410_8_1_48,cs-410,8,1,Syntagmatic,"00:03:26,110","00:03:31,010",48,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=206,"And naturally, the probabilities for",pic_cs-410_8_1_180.jpg
cs-410_8_1_49,cs-410,8,1,Syntagmatic,"00:03:31,010","00:03:34,187",49,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=211,because a word is either present or,pic_cs-410_8_1_180.jpg
cs-410_8_1_50,cs-410,8,1,Syntagmatic,"00:03:35,240","00:03:36,070",50,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=215,There's no other choice.,pic_cs-410_8_1_180.jpg
cs-410_8_1_51,cs-410,8,1,Syntagmatic,"00:03:38,290","00:03:43,610",51,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=218,So the intuition with this concept earlier,pic_cs-410_8_1_180.jpg
cs-410_8_1_52,cs-410,8,1,Syntagmatic,"00:03:43,610","00:03:48,280",52,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=223,"The more random this random variable is,",pic_cs-410_8_1_180.jpg
cs-410_8_1_53,cs-410,8,1,Syntagmatic,"00:03:49,710","00:03:53,600",53,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=229,Now the question is how does one,pic_cs-410_8_1_180.jpg
cs-410_8_1_54,cs-410,8,1,Syntagmatic,"00:03:53,600","00:03:55,590",54,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=233,a random variable like X sub w?,pic_cs-410_8_1_180.jpg
cs-410_8_1_55,cs-410,8,1,Syntagmatic,"00:03:56,940","00:04:01,850",55,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=236,"How in general, can we quantify",pic_cs-410_8_1_180.jpg
cs-410_8_1_56,cs-410,8,1,Syntagmatic,"00:04:01,850","00:04:04,690",56,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=241,that's why we need a measure,pic_cs-410_8_1_240.jpg
cs-410_8_1_57,cs-410,8,1,Syntagmatic,"00:04:04,690","00:04:10,560",57,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=244,this measure introduced in information,pic_cs-410_8_1_240.jpg
cs-410_8_1_58,cs-410,8,1,Syntagmatic,"00:04:10,560","00:04:13,790",58,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=250,There is also some connection,pic_cs-410_8_1_240.jpg
cs-410_8_1_59,cs-410,8,1,Syntagmatic,"00:04:13,790","00:04:15,620",59,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=253,that is beyond the scope of this course.,pic_cs-410_8_1_240.jpg
cs-410_8_1_60,cs-410,8,1,Syntagmatic,"00:04:17,460","00:04:20,750",60,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=257,So for,pic_cs-410_8_1_240.jpg
cs-410_8_1_61,cs-410,8,1,Syntagmatic,"00:04:20,750","00:04:22,910",61,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=260,as a function defined,pic_cs-410_8_1_240.jpg
cs-410_8_1_62,cs-410,8,1,Syntagmatic,"00:04:22,910","00:04:27,000",62,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=262,"In this case, it is a binary random",pic_cs-410_8_1_240.jpg
cs-410_8_1_63,cs-410,8,1,Syntagmatic,"00:04:27,000","00:04:30,930",63,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=267,be easily generalized for,pic_cs-410_8_1_240.jpg
cs-410_8_1_64,cs-410,8,1,Syntagmatic,"00:04:32,070","00:04:34,950",64,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=272,"Now the function form looks like this,",pic_cs-410_8_1_240.jpg
cs-410_8_1_65,cs-410,8,1,Syntagmatic,"00:04:34,950","00:04:39,410",65,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=274,there's the sum of all the possible,pic_cs-410_8_1_240.jpg
cs-410_8_1_66,cs-410,8,1,Syntagmatic,"00:04:39,410","00:04:44,030",66,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=279,Inside the sum for each value we,pic_cs-410_8_1_240.jpg
cs-410_8_1_67,cs-410,8,1,Syntagmatic,"00:04:45,210","00:04:52,060",67,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=285,that the random variable equals this,pic_cs-410_8_1_240.jpg
cs-410_8_1_68,cs-410,8,1,Syntagmatic,"00:04:53,380","00:04:55,250",68,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=293,And note that there is also,pic_cs-410_8_1_240.jpg
cs-410_8_1_69,cs-410,8,1,Syntagmatic,"00:04:56,270","00:04:59,900",69,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=296,Now entropy in general is non-negative.,pic_cs-410_8_1_240.jpg
cs-410_8_1_70,cs-410,8,1,Syntagmatic,"00:04:59,900","00:05:01,480",70,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=299,And that can be mathematically proved.,pic_cs-410_8_1_240.jpg
cs-410_8_1_71,cs-410,8,1,Syntagmatic,"00:05:02,620","00:05:10,320",71,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=302,"So if we expand this sum, we'll see that",pic_cs-410_8_1_300.jpg
cs-410_8_1_72,cs-410,8,1,Syntagmatic,"00:05:10,320","00:05:14,130",72,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=310,Where I explicitly plugged,pic_cs-410_8_1_300.jpg
cs-410_8_1_73,cs-410,8,1,Syntagmatic,"00:05:14,130","00:05:18,370",73,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=314,"And sometimes when we have 0 log of 0,",pic_cs-410_8_1_300.jpg
cs-410_8_1_74,cs-410,8,1,Syntagmatic,"00:05:18,370","00:05:25,960",74,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=318,"we would generally define that as 0,",pic_cs-410_8_1_300.jpg
cs-410_8_1_75,cs-410,8,1,Syntagmatic,"00:05:28,480","00:05:30,330",75,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=328,So this is the entropy function.,pic_cs-410_8_1_300.jpg
cs-410_8_1_76,cs-410,8,1,Syntagmatic,"00:05:30,330","00:05:33,020",76,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=330,And this function will,pic_cs-410_8_1_300.jpg
cs-410_8_1_77,cs-410,8,1,Syntagmatic,"00:05:33,020","00:05:35,520",77,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=333,different distributions,pic_cs-410_8_1_300.jpg
cs-410_8_1_78,cs-410,8,1,Syntagmatic,"00:05:37,260","00:05:40,650",78,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=337,And it clearly depends on the probability,pic_cs-410_8_1_300.jpg
cs-410_8_1_79,cs-410,8,1,Syntagmatic,"00:05:40,650","00:05:43,850",79,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=340,that the random variable,pic_cs-410_8_1_300.jpg
cs-410_8_1_80,cs-410,8,1,Syntagmatic,"00:05:43,850","00:05:49,780",80,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=343,If we plot this function against,pic_cs-410_8_1_300.jpg
cs-410_8_1_81,cs-410,8,1,Syntagmatic,"00:05:49,780","00:05:55,114",81,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=349,the probability that the random,pic_cs-410_8_1_300.jpg
cs-410_8_1_82,cs-410,8,1,Syntagmatic,"00:05:56,990","00:05:59,080",82,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=356,And then the function looks like this.,pic_cs-410_8_1_300.jpg
cs-410_8_1_83,cs-410,8,1,Syntagmatic,"00:06:01,310","00:06:06,820",83,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=361,"At the two ends,",pic_cs-410_8_1_360.jpg
cs-410_8_1_84,cs-410,8,1,Syntagmatic,"00:06:07,950","00:06:13,698",84,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=367,"equals 1 is very small or very large,",pic_cs-410_8_1_360.jpg
cs-410_8_1_85,cs-410,8,1,Syntagmatic,"00:06:13,698","00:06:18,280",85,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=373,When it's 0.5 in the middle,pic_cs-410_8_1_360.jpg
cs-410_8_1_86,cs-410,8,1,Syntagmatic,"00:06:20,180","00:06:24,150",86,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=380,Now if we plot the function,pic_cs-410_8_1_360.jpg
cs-410_8_1_87,cs-410,8,1,Syntagmatic,"00:06:25,950","00:06:31,090",87,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=385,is taking a value of 0 and the function,pic_cs-410_8_1_360.jpg
cs-410_8_1_88,cs-410,8,1,Syntagmatic,"00:06:31,090","00:06:37,810",88,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=391,"would show exactly the same curve here,",pic_cs-410_8_1_360.jpg
cs-410_8_1_89,cs-410,8,1,Syntagmatic,"00:06:37,810","00:06:40,620",89,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=397,And so that's because,pic_cs-410_8_1_360.jpg
cs-410_8_1_90,cs-410,8,1,Syntagmatic,"00:06:42,340","00:06:46,730",90,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=402,"the two probabilities are symmetric,",pic_cs-410_8_1_360.jpg
cs-410_8_1_91,cs-410,8,1,Syntagmatic,"00:06:48,740","00:06:52,850",91,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=408,So an interesting question you,pic_cs-410_8_1_360.jpg
cs-410_8_1_92,cs-410,8,1,Syntagmatic,"00:06:52,850","00:06:59,390",92,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=412,what kind of X does entropy,pic_cs-410_8_1_360.jpg
cs-410_8_1_93,cs-410,8,1,Syntagmatic,"00:06:59,390","00:07:02,960",93,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=419,And we can in particular think,pic_cs-410_8_1_360.jpg
cs-410_8_1_94,cs-410,8,1,Syntagmatic,"00:07:02,960","00:07:07,700",94,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=422,"For example, in one case,",pic_cs-410_8_1_420.jpg
cs-410_8_1_95,cs-410,8,1,Syntagmatic,"00:07:08,840","00:07:10,600",95,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=428,always takes a value of 1.,pic_cs-410_8_1_420.jpg
cs-410_8_1_96,cs-410,8,1,Syntagmatic,"00:07:10,600","00:07:14,304",96,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=430,The probability is 1.,pic_cs-410_8_1_420.jpg
cs-410_8_1_97,cs-410,8,1,Syntagmatic,"00:07:16,390","00:07:18,650",97,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=436,Or there's a random variable that,pic_cs-410_8_1_420.jpg
cs-410_8_1_98,cs-410,8,1,Syntagmatic,"00:07:19,890","00:07:24,320",98,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=439,is equally likely taking a value of one or,pic_cs-410_8_1_420.jpg
cs-410_8_1_99,cs-410,8,1,Syntagmatic,"00:07:24,320","00:07:28,750",99,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=444,So in this case the probability,pic_cs-410_8_1_420.jpg
cs-410_8_1_100,cs-410,8,1,Syntagmatic,"00:07:30,700","00:07:32,250",100,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=450,Now which one has a higher entropy?,pic_cs-410_8_1_420.jpg
cs-410_8_1_101,cs-410,8,1,Syntagmatic,"00:07:34,650","00:07:38,530",101,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=454,It's easier to look at the problem,pic_cs-410_8_1_420.jpg
cs-410_8_1_102,cs-410,8,1,Syntagmatic,"00:07:40,800","00:07:42,380",102,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=460,using coin tossing.,pic_cs-410_8_1_420.jpg
cs-410_8_1_103,cs-410,8,1,Syntagmatic,"00:07:43,420","00:07:47,660",103,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=463,So when we think about random,pic_cs-410_8_1_420.jpg
cs-410_8_1_104,cs-410,8,1,Syntagmatic,"00:07:48,770","00:07:55,740",104,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=468,"it gives us a random variable,",pic_cs-410_8_1_420.jpg
cs-410_8_1_105,cs-410,8,1,Syntagmatic,"00:07:55,740","00:07:57,860",105,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=475,It can be head or tail.,pic_cs-410_8_1_420.jpg
cs-410_8_1_106,cs-410,8,1,Syntagmatic,"00:07:57,860","00:08:03,040",106,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=477,So we can define a random variable,pic_cs-410_8_1_420.jpg
cs-410_8_1_107,cs-410,8,1,Syntagmatic,"00:08:03,040","00:08:08,470",107,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=483,"when the coin shows up as head,",pic_cs-410_8_1_480.jpg
cs-410_8_1_108,cs-410,8,1,Syntagmatic,"00:08:09,800","00:08:15,390",108,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=489,So now we can compute the entropy,pic_cs-410_8_1_480.jpg
cs-410_8_1_109,cs-410,8,1,Syntagmatic,"00:08:15,390","00:08:20,050",109,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=495,And this entropy indicates how,pic_cs-410_8_1_480.jpg
cs-410_8_1_110,cs-410,8,1,Syntagmatic,"00:08:22,050","00:08:22,890",110,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=502,of a coin toss.,pic_cs-410_8_1_480.jpg
cs-410_8_1_111,cs-410,8,1,Syntagmatic,"00:08:25,440","00:08:27,530",111,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=505,So we can think about the two cases.,pic_cs-410_8_1_480.jpg
cs-410_8_1_112,cs-410,8,1,Syntagmatic,"00:08:27,530","00:08:29,590",112,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=507,"One is a fair coin, it's completely fair.",pic_cs-410_8_1_480.jpg
cs-410_8_1_113,cs-410,8,1,Syntagmatic,"00:08:29,590","00:08:34,160",113,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=509,The coin shows up as head or,pic_cs-410_8_1_480.jpg
cs-410_8_1_114,cs-410,8,1,Syntagmatic,"00:08:34,160","00:08:39,160",114,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=514,So the two probabilities would be a half.,pic_cs-410_8_1_480.jpg
cs-410_8_1_115,cs-410,8,1,Syntagmatic,"00:08:39,160","00:08:42,890",115,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=519,Right?,pic_cs-410_8_1_480.jpg
cs-410_8_1_116,cs-410,8,1,Syntagmatic,"00:08:44,680","00:08:47,620",116,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=524,Another extreme case is,pic_cs-410_8_1_480.jpg
cs-410_8_1_117,cs-410,8,1,Syntagmatic,"00:08:47,620","00:08:50,420",117,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=527,where the coin always shows up as heads.,pic_cs-410_8_1_480.jpg
cs-410_8_1_118,cs-410,8,1,Syntagmatic,"00:08:50,420","00:08:52,760",118,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=530,So it's a completely biased coin.,pic_cs-410_8_1_480.jpg
cs-410_8_1_119,cs-410,8,1,Syntagmatic,"00:08:54,670","00:08:57,910",119,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=534,Now let's think about,pic_cs-410_8_1_480.jpg
cs-410_8_1_120,cs-410,8,1,Syntagmatic,"00:08:57,910","00:09:04,850",120,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=537,And if you plug in these values you can,pic_cs-410_8_1_480.jpg
cs-410_8_1_121,cs-410,8,1,Syntagmatic,"00:09:04,850","00:09:09,524",121,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=544,For a fair coin we see the entropy,pic_cs-410_8_1_540.jpg
cs-410_8_1_122,cs-410,8,1,Syntagmatic,"00:09:11,270","00:09:14,460",122,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=551,"For the completely biased coin,",pic_cs-410_8_1_540.jpg
cs-410_8_1_123,cs-410,8,1,Syntagmatic,"00:09:14,460","00:09:17,360",123,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=554,And that intuitively makes a lot of sense.,pic_cs-410_8_1_540.jpg
cs-410_8_1_124,cs-410,8,1,Syntagmatic,"00:09:17,360","00:09:20,490",124,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=557,Because a fair coin is,pic_cs-410_8_1_540.jpg
cs-410_8_1_125,cs-410,8,1,Syntagmatic,"00:09:22,080","00:09:24,950",125,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=562,Whereas a completely biased,pic_cs-410_8_1_540.jpg
cs-410_8_1_126,cs-410,8,1,Syntagmatic,"00:09:24,950","00:09:26,860",126,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=564,"We can always say, well, it's a head.",pic_cs-410_8_1_540.jpg
cs-410_8_1_127,cs-410,8,1,Syntagmatic,"00:09:26,860","00:09:29,190",127,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=566,Because it is a head all the time.,pic_cs-410_8_1_540.jpg
cs-410_8_1_128,cs-410,8,1,Syntagmatic,"00:09:29,190","00:09:34,400",128,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=569,So they can be shown on,pic_cs-410_8_1_540.jpg
cs-410_8_1_129,cs-410,8,1,Syntagmatic,"00:09:34,400","00:09:40,300",129,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=574,So the fair coin corresponds to the middle,pic_cs-410_8_1_540.jpg
cs-410_8_1_130,cs-410,8,1,Syntagmatic,"00:09:40,300","00:09:45,410",130,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=580,The completely biased coin,pic_cs-410_8_1_540.jpg
cs-410_8_1_131,cs-410,8,1,Syntagmatic,"00:09:45,410","00:09:48,058",131,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=585,point where we have a probability,pic_cs-410_8_1_540.jpg
cs-410_8_1_132,cs-410,8,1,Syntagmatic,"00:09:48,058","00:09:54,870",132,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=588,"So, now let's see how we can use",pic_cs-410_8_1_540.jpg
cs-410_8_1_133,cs-410,8,1,Syntagmatic,"00:09:54,870","00:09:59,670",133,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=594,Let's think about our problem is,pic_cs-410_8_1_540.jpg
cs-410_8_1_134,cs-410,8,1,Syntagmatic,"00:09:59,670","00:10:01,650",134,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=599,absent in this segment.,pic_cs-410_8_1_540.jpg
cs-410_8_1_135,cs-410,8,1,Syntagmatic,"00:10:01,650","00:10:05,300",135,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=601,"Again, think about the three words,",pic_cs-410_8_1_600.jpg
cs-410_8_1_136,cs-410,8,1,Syntagmatic,"00:10:06,540","00:10:10,130",136,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=606,Now we can assume high entropy,pic_cs-410_8_1_600.jpg
cs-410_8_1_137,cs-410,8,1,Syntagmatic,"00:10:11,910","00:10:18,790",137,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=611,And so we now have a quantitative way to,pic_cs-410_8_1_600.jpg
cs-410_8_1_138,cs-410,8,1,Syntagmatic,"00:10:20,890","00:10:25,810",138,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=620,"Now if you look at the three words meat,",pic_cs-410_8_1_600.jpg
cs-410_8_1_139,cs-410,8,1,Syntagmatic,"00:10:25,810","00:10:33,310",139,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=625,we clearly would expect meat to have,pic_cs-410_8_1_600.jpg
cs-410_8_1_140,cs-410,8,1,Syntagmatic,"00:10:33,310","00:10:39,180",140,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=633,"In fact if you look at the entropy of the,",pic_cs-410_8_1_600.jpg
cs-410_8_1_141,cs-410,8,1,Syntagmatic,"00:10:39,180","00:10:41,570",141,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=639,Because it occurs everywhere.,pic_cs-410_8_1_600.jpg
cs-410_8_1_142,cs-410,8,1,Syntagmatic,"00:10:41,570","00:10:43,390",142,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=641,So it's like a completely biased coin.,pic_cs-410_8_1_600.jpg
cs-410_8_1_143,cs-410,8,1,Syntagmatic,"00:10:44,610","00:10:46,380",143,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=644,Therefore the entropy is zero.,pic_cs-410_8_1_600.jpg
cs-410_8_1_144,cs-410,8,1,Syntagmatic,"00:10:48,710","00:10:58,710",144,https://www.coursera.org/learn/cs-410/lecture/qGZrA?t=648,[MUSIC],pic_cs-410_8_1_600.jpg
cs-410_8_2_1,cs-410,8,2,Syntagmatic,"00:00:00,025","00:00:05,819",1,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=0,[SOUND] This lecture is,pic_cs-410_8_2_0.jpg
cs-410_8_2_2,cs-410,8,2,Syntagmatic,"00:00:05,819","00:00:12,090",2,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=5,relation discovery and,pic_cs-410_8_2_0.jpg
cs-410_8_2_3,cs-410,8,2,Syntagmatic,"00:00:12,090","00:00:12,963",3,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=12,"In this lecture,",pic_cs-410_8_2_0.jpg
cs-410_8_2_4,cs-410,8,2,Syntagmatic,"00:00:12,963","00:00:16,939",4,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=12,we're going to continue the discussion,pic_cs-410_8_2_0.jpg
cs-410_8_2_5,cs-410,8,2,Syntagmatic,"00:00:18,060","00:00:22,930",5,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=18,We're going to talk about the conditional,pic_cs-410_8_2_0.jpg
cs-410_8_2_6,cs-410,8,2,Syntagmatic,"00:00:22,930","00:00:25,700",6,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=22,discovering syntagmatic relations.,pic_cs-410_8_2_0.jpg
cs-410_8_2_7,cs-410,8,2,Syntagmatic,"00:00:25,700","00:00:29,400",7,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=25,"Earlier, we talked about",pic_cs-410_8_2_0.jpg
cs-410_8_2_8,cs-410,8,2,Syntagmatic,"00:00:29,400","00:00:33,030",8,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=29,how easy it is to predict the presence or,pic_cs-410_8_2_0.jpg
cs-410_8_2_9,cs-410,8,2,Syntagmatic,"00:00:34,180","00:00:37,700",9,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=34,"Now, we'll address",pic_cs-410_8_2_0.jpg
cs-410_8_2_10,cs-410,8,2,Syntagmatic,"00:00:37,700","00:00:41,320",10,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=37,we assume that we know something,pic_cs-410_8_2_0.jpg
cs-410_8_2_11,cs-410,8,2,Syntagmatic,"00:00:41,320","00:00:48,830",11,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=41,"So now the question is, suppose we know",pic_cs-410_8_2_0.jpg
cs-410_8_2_12,cs-410,8,2,Syntagmatic,"00:00:48,830","00:00:51,150",12,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=48,How would that help us,pic_cs-410_8_2_0.jpg
cs-410_8_2_13,cs-410,8,2,Syntagmatic,"00:00:51,150","00:00:53,990",13,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=51,"absence of water, like in meat?",pic_cs-410_8_2_0.jpg
cs-410_8_2_14,cs-410,8,2,Syntagmatic,"00:00:53,990","00:00:58,060",14,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=53,"And in particular, we want to",pic_cs-410_8_2_0.jpg
cs-410_8_2_15,cs-410,8,2,Syntagmatic,"00:00:58,060","00:01:00,959",15,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=58,has helped us predict,pic_cs-410_8_2_0.jpg
cs-410_8_2_16,cs-410,8,2,Syntagmatic,"00:01:02,020","00:01:05,040",16,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=62,"And if we frame this using entrophy,",pic_cs-410_8_2_60.jpg
cs-410_8_2_17,cs-410,8,2,Syntagmatic,"00:01:05,040","00:01:10,700",17,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=65,that would mean we are interested,pic_cs-410_8_2_60.jpg
cs-410_8_2_18,cs-410,8,2,Syntagmatic,"00:01:10,700","00:01:15,100",18,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=70,the presence of eats could reduce,pic_cs-410_8_2_60.jpg
cs-410_8_2_19,cs-410,8,2,Syntagmatic,"00:01:15,100","00:01:18,800",19,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=75,"Or, reduce the entrophy",pic_cs-410_8_2_60.jpg
cs-410_8_2_20,cs-410,8,2,Syntagmatic,"00:01:18,800","00:01:23,430",20,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=78,corresponding to the presence or,pic_cs-410_8_2_60.jpg
cs-410_8_2_21,cs-410,8,2,Syntagmatic,"00:01:23,430","00:01:27,950",21,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=83,"We can also ask as a question,",pic_cs-410_8_2_60.jpg
cs-410_8_2_22,cs-410,8,2,Syntagmatic,"00:01:28,950","00:01:33,010",22,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=88,Would that also help us predict,pic_cs-410_8_2_60.jpg
cs-410_8_2_23,cs-410,8,2,Syntagmatic,"00:01:34,720","00:01:39,415",23,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=94,These questions can be,pic_cs-410_8_2_60.jpg
cs-410_8_2_24,cs-410,8,2,Syntagmatic,"00:01:39,415","00:01:43,120",24,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=99,concept called a conditioning entropy.,pic_cs-410_8_2_60.jpg
cs-410_8_2_25,cs-410,8,2,Syntagmatic,"00:01:43,120","00:01:48,460",25,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=103,"So to explain this concept, let's first",pic_cs-410_8_2_60.jpg
cs-410_8_2_26,cs-410,8,2,Syntagmatic,"00:01:48,460","00:01:51,218",26,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=108,when we know nothing about the segment.,pic_cs-410_8_2_60.jpg
cs-410_8_2_27,cs-410,8,2,Syntagmatic,"00:01:51,218","00:01:56,522",27,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=111,So we have these probabilities indicating,pic_cs-410_8_2_60.jpg
cs-410_8_2_28,cs-410,8,2,Syntagmatic,"00:01:56,522","00:01:58,830",28,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=116,or it doesn't occur in the segment.,pic_cs-410_8_2_60.jpg
cs-410_8_2_29,cs-410,8,2,Syntagmatic,"00:01:58,830","00:02:02,650",29,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=118,And we have an entropy function that,pic_cs-410_8_2_60.jpg
cs-410_8_2_30,cs-410,8,2,Syntagmatic,"00:02:03,810","00:02:07,410",30,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=123,"Now suppose we know eats is present, so",pic_cs-410_8_2_120.jpg
cs-410_8_2_31,cs-410,8,2,Syntagmatic,"00:02:07,410","00:02:11,330",31,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=127,now we know the value of another,pic_cs-410_8_2_120.jpg
cs-410_8_2_32,cs-410,8,2,Syntagmatic,"00:02:12,730","00:02:15,270",32,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=132,"Now, that would change all",pic_cs-410_8_2_120.jpg
cs-410_8_2_33,cs-410,8,2,Syntagmatic,"00:02:15,270","00:02:17,550",33,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=135,conditional probabilities.,pic_cs-410_8_2_120.jpg
cs-410_8_2_34,cs-410,8,2,Syntagmatic,"00:02:17,550","00:02:20,580",34,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=137,Where we look at the presence or,pic_cs-410_8_2_120.jpg
cs-410_8_2_35,cs-410,8,2,Syntagmatic,"00:02:21,800","00:02:25,570",35,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=141,given that we know eats,pic_cs-410_8_2_120.jpg
cs-410_8_2_36,cs-410,8,2,Syntagmatic,"00:02:25,570","00:02:27,480",36,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=145,"So as a result,",pic_cs-410_8_2_120.jpg
cs-410_8_2_37,cs-410,8,2,Syntagmatic,"00:02:27,480","00:02:31,820",37,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=147,if we replace these probabilities,pic_cs-410_8_2_120.jpg
cs-410_8_2_38,cs-410,8,2,Syntagmatic,"00:02:31,820","00:02:36,320",38,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=151,"probabilities in the entropy function,",pic_cs-410_8_2_120.jpg
cs-410_8_2_39,cs-410,8,2,Syntagmatic,"00:02:37,650","00:02:42,522",39,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=157,So this equation now here would be,pic_cs-410_8_2_120.jpg
cs-410_8_2_40,cs-410,8,2,Syntagmatic,"00:02:42,522","00:02:46,900",40,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=162,the conditional entropy.,pic_cs-410_8_2_120.jpg
cs-410_8_2_41,cs-410,8,2,Syntagmatic,"00:02:46,900","00:02:49,150",41,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=166,Conditional on the presence of eats.,pic_cs-410_8_2_120.jpg
cs-410_8_2_42,cs-410,8,2,Syntagmatic,"00:02:52,180","00:02:57,070",42,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=172,"So, you can see this is essentially",pic_cs-410_8_2_120.jpg
cs-410_8_2_43,cs-410,8,2,Syntagmatic,"00:02:57,070","00:03:01,900",43,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=177,"seen before, except that all",pic_cs-410_8_2_120.jpg
cs-410_8_2_44,cs-410,8,2,Syntagmatic,"00:03:04,420","00:03:09,550",44,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=184,And this then tells us,pic_cs-410_8_2_180.jpg
cs-410_8_2_45,cs-410,8,2,Syntagmatic,"00:03:09,550","00:03:13,020",45,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=189,after we have known eats,pic_cs-410_8_2_180.jpg
cs-410_8_2_46,cs-410,8,2,Syntagmatic,"00:03:14,380","00:03:17,770",46,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=194,"And of course, we can also define",pic_cs-410_8_2_180.jpg
cs-410_8_2_47,cs-410,8,2,Syntagmatic,"00:03:17,770","00:03:20,540",47,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=197,the scenario where we don't see eats.,pic_cs-410_8_2_180.jpg
cs-410_8_2_48,cs-410,8,2,Syntagmatic,"00:03:20,540","00:03:25,150",48,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=200,So if we know it did not occur in,pic_cs-410_8_2_180.jpg
cs-410_8_2_49,cs-410,8,2,Syntagmatic,"00:03:25,150","00:03:30,710",49,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=205,entropy would capture the instances,pic_cs-410_8_2_180.jpg
cs-410_8_2_50,cs-410,8,2,Syntagmatic,"00:03:30,710","00:03:34,110",50,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=210,"So now,",pic_cs-410_8_2_180.jpg
cs-410_8_2_51,cs-410,8,2,Syntagmatic,"00:03:34,110","00:03:37,609",51,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=214,we have the completed definition,pic_cs-410_8_2_180.jpg
cs-410_8_2_52,cs-410,8,2,Syntagmatic,"00:03:39,250","00:03:48,520",52,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=219,"Basically, we're going to consider both",pic_cs-410_8_2_180.jpg
cs-410_8_2_53,cs-410,8,2,Syntagmatic,"00:03:48,520","00:03:54,280",53,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=228,and this gives us a probability,pic_cs-410_8_2_180.jpg
cs-410_8_2_54,cs-410,8,2,Syntagmatic,"00:03:54,280","00:03:58,040",54,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=234,"Basically, whether eats is present or",pic_cs-410_8_2_180.jpg
cs-410_8_2_55,cs-410,8,2,Syntagmatic,"00:03:58,040","00:03:59,150",55,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=238,"And this of course,",pic_cs-410_8_2_180.jpg
cs-410_8_2_56,cs-410,8,2,Syntagmatic,"00:03:59,150","00:04:04,310",56,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=239,is the conditional entropy of,pic_cs-410_8_2_180.jpg
cs-410_8_2_57,cs-410,8,2,Syntagmatic,"00:04:05,510","00:04:10,110",57,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=245,"So if you expanded this entropy,",pic_cs-410_8_2_240.jpg
cs-410_8_2_58,cs-410,8,2,Syntagmatic,"00:04:10,110","00:04:14,330",58,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=250,then you have the following equation.,pic_cs-410_8_2_240.jpg
cs-410_8_2_59,cs-410,8,2,Syntagmatic,"00:04:15,760","00:04:19,429",59,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=255,Where you see the involvement of,pic_cs-410_8_2_240.jpg
cs-410_8_2_60,cs-410,8,2,Syntagmatic,"00:04:21,530","00:04:26,330",60,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=261,"Now in general, for any discrete",pic_cs-410_8_2_240.jpg
cs-410_8_2_61,cs-410,8,2,Syntagmatic,"00:04:27,940","00:04:35,240",61,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=267,the conditional entropy is no larger,pic_cs-410_8_2_240.jpg
cs-410_8_2_62,cs-410,8,2,Syntagmatic,"00:04:35,240","00:04:41,950",62,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=275,"So basically, this is upper bound for",pic_cs-410_8_2_240.jpg
cs-410_8_2_63,cs-410,8,2,Syntagmatic,"00:04:41,950","00:04:46,380",63,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=281,That means by knowing more,pic_cs-410_8_2_240.jpg
cs-410_8_2_64,cs-410,8,2,Syntagmatic,"00:04:46,380","00:04:49,630",64,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=286,we want to be able to,pic_cs-410_8_2_240.jpg
cs-410_8_2_65,cs-410,8,2,Syntagmatic,"00:04:49,630","00:04:51,570",65,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=289,We can only reduce uncertainty.,pic_cs-410_8_2_240.jpg
cs-410_8_2_66,cs-410,8,2,Syntagmatic,"00:04:51,570","00:04:56,180",66,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=291,And that intuitively makes sense,pic_cs-410_8_2_240.jpg
cs-410_8_2_67,cs-410,8,2,Syntagmatic,"00:04:56,180","00:05:00,180",67,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=296,it should always help,pic_cs-410_8_2_240.jpg
cs-410_8_2_68,cs-410,8,2,Syntagmatic,"00:05:00,180","00:05:04,000",68,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=300,And cannot hurt,pic_cs-410_8_2_300.jpg
cs-410_8_2_69,cs-410,8,2,Syntagmatic,"00:05:05,420","00:05:08,880",69,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=305,"Now, what's interesting here is also to",pic_cs-410_8_2_300.jpg
cs-410_8_2_70,cs-410,8,2,Syntagmatic,"00:05:08,880","00:05:11,770",70,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=308,value of this conditional entropy?,pic_cs-410_8_2_300.jpg
cs-410_8_2_71,cs-410,8,2,Syntagmatic,"00:05:11,770","00:05:16,270",71,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=311,"Now, we know that the maximum",pic_cs-410_8_2_300.jpg
cs-410_8_2_72,cs-410,8,2,Syntagmatic,"00:05:17,940","00:05:20,313",72,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=317,"But what about the minimum,",pic_cs-410_8_2_300.jpg
cs-410_8_2_73,cs-410,8,2,Syntagmatic,"00:05:22,883","00:05:28,552",73,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=322,I hope you can reach the conclusion that,pic_cs-410_8_2_300.jpg
cs-410_8_2_74,cs-410,8,2,Syntagmatic,"00:05:28,552","00:05:33,090",74,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=328,And it will be interesting to think about,pic_cs-410_8_2_300.jpg
cs-410_8_2_75,cs-410,8,2,Syntagmatic,"00:05:34,120","00:05:37,860",75,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=334,"So, let's see how we can use conditional",pic_cs-410_8_2_300.jpg
cs-410_8_2_76,cs-410,8,2,Syntagmatic,"00:05:39,420","00:05:44,250",76,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=339,"Now of course,",pic_cs-410_8_2_300.jpg
cs-410_8_2_77,cs-410,8,2,Syntagmatic,"00:05:44,250","00:05:48,300",77,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=344,one way to measure,pic_cs-410_8_2_300.jpg
cs-410_8_2_78,cs-410,8,2,Syntagmatic,"00:05:48,300","00:05:53,750",78,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=348,"Because it tells us to what extent,",pic_cs-410_8_2_300.jpg
cs-410_8_2_79,cs-410,8,2,Syntagmatic,"00:05:53,750","00:05:58,995",79,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=353,word given that we know the presence or,pic_cs-410_8_2_300.jpg
cs-410_8_2_80,cs-410,8,2,Syntagmatic,"00:05:58,995","00:06:03,900",80,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=358,Now before we look at the intuition,pic_cs-410_8_2_300.jpg
cs-410_8_2_81,cs-410,8,2,Syntagmatic,"00:06:03,900","00:06:09,090",81,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=363,"syntagmatic relations, it's useful to",pic_cs-410_8_2_360.jpg
cs-410_8_2_82,cs-410,8,2,Syntagmatic,"00:06:09,090","00:06:17,910",82,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=369,"That is, the conditional entropy",pic_cs-410_8_2_360.jpg
cs-410_8_2_83,cs-410,8,2,Syntagmatic,"00:06:19,000","00:06:22,980",83,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=379,"So here,",pic_cs-410_8_2_360.jpg
cs-410_8_2_84,cs-410,8,2,Syntagmatic,"00:06:22,980","00:06:28,420",84,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=382,we listed this conditional,pic_cs-410_8_2_360.jpg
cs-410_8_2_85,cs-410,8,2,Syntagmatic,"00:06:28,420","00:06:31,280",85,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=388,"So, it's here.",pic_cs-410_8_2_360.jpg
cs-410_8_2_86,cs-410,8,2,Syntagmatic,"00:06:33,550","00:06:35,100",86,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=393,"So, what is the value of this?",pic_cs-410_8_2_360.jpg
cs-410_8_2_87,cs-410,8,2,Syntagmatic,"00:06:36,380","00:06:43,370",87,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=396,"Now, this means we know where",pic_cs-410_8_2_360.jpg
cs-410_8_2_88,cs-410,8,2,Syntagmatic,"00:06:43,370","00:06:47,717",88,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=403,And we hope to predict whether,pic_cs-410_8_2_360.jpg
cs-410_8_2_89,cs-410,8,2,Syntagmatic,"00:06:47,717","00:06:52,518",89,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=407,"And of course, this is 0 because",pic_cs-410_8_2_360.jpg
cs-410_8_2_90,cs-410,8,2,Syntagmatic,"00:06:52,518","00:06:55,862",90,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=412,Once we know whether the word,pic_cs-410_8_2_360.jpg
cs-410_8_2_91,cs-410,8,2,Syntagmatic,"00:06:55,862","00:06:59,132",91,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=415,we'll already know the answer,pic_cs-410_8_2_360.jpg
cs-410_8_2_92,cs-410,8,2,Syntagmatic,"00:06:59,132","00:07:00,410",92,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=419,So this is zero.,pic_cs-410_8_2_360.jpg
cs-410_8_2_93,cs-410,8,2,Syntagmatic,"00:07:00,410","00:07:03,390",93,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=420,And that's also when this conditional,pic_cs-410_8_2_420.jpg
cs-410_8_2_94,cs-410,8,2,Syntagmatic,"00:07:06,280","00:07:08,280",94,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=426,"So now, let's look at some other cases.",pic_cs-410_8_2_420.jpg
cs-410_8_2_95,cs-410,8,2,Syntagmatic,"00:07:09,530","00:07:15,840",95,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=429,So this is a case of knowing the and,pic_cs-410_8_2_420.jpg
cs-410_8_2_96,cs-410,8,2,Syntagmatic,"00:07:15,840","00:07:20,840",96,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=435,And this is a case of knowing eats and,pic_cs-410_8_2_420.jpg
cs-410_8_2_97,cs-410,8,2,Syntagmatic,"00:07:20,840","00:07:22,870",97,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=440,Which one do you think is smaller?,pic_cs-410_8_2_420.jpg
cs-410_8_2_98,cs-410,8,2,Syntagmatic,"00:07:22,870","00:07:27,763",98,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=442,No doubt smaller entropy means easier for,pic_cs-410_8_2_420.jpg
cs-410_8_2_99,cs-410,8,2,Syntagmatic,"00:07:31,511","00:07:33,260",99,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=451,Which one do you think is higher?,pic_cs-410_8_2_420.jpg
cs-410_8_2_100,cs-410,8,2,Syntagmatic,"00:07:33,260","00:07:34,820",100,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=453,Which one is not smaller?,pic_cs-410_8_2_420.jpg
cs-410_8_2_101,cs-410,8,2,Syntagmatic,"00:07:36,800","00:07:41,732",101,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=456,"Well, if you at the uncertainty,",pic_cs-410_8_2_420.jpg
cs-410_8_2_102,cs-410,8,2,Syntagmatic,"00:07:41,732","00:07:45,730",102,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=461,the doesn't really tell,pic_cs-410_8_2_420.jpg
cs-410_8_2_103,cs-410,8,2,Syntagmatic,"00:07:45,730","00:07:51,520",103,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=465,So knowing the occurrence of the doesn't,pic_cs-410_8_2_420.jpg
cs-410_8_2_104,cs-410,8,2,Syntagmatic,"00:07:51,520","00:07:56,465",104,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=471,So it stays fairly close to,pic_cs-410_8_2_420.jpg
cs-410_8_2_105,cs-410,8,2,Syntagmatic,"00:07:56,465","00:08:01,120",105,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=476,"Whereas in the case of eats,",pic_cs-410_8_2_420.jpg
cs-410_8_2_106,cs-410,8,2,Syntagmatic,"00:08:01,120","00:08:04,420",106,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=481,So knowing presence of eats or,pic_cs-410_8_2_480.jpg
cs-410_8_2_107,cs-410,8,2,Syntagmatic,"00:08:04,420","00:08:07,780",107,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=484,would help us predict whether meat occurs.,pic_cs-410_8_2_480.jpg
cs-410_8_2_108,cs-410,8,2,Syntagmatic,"00:08:07,780","00:08:14,290",108,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=487,So it can help us reduce entropy of meat.,pic_cs-410_8_2_480.jpg
cs-410_8_2_109,cs-410,8,2,Syntagmatic,"00:08:14,290","00:08:20,470",109,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=494,"So we should expect the sigma term, namely",pic_cs-410_8_2_480.jpg
cs-410_8_2_110,cs-410,8,2,Syntagmatic,"00:08:21,630","00:08:25,870",110,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=501,And that means there is a stronger,pic_cs-410_8_2_480.jpg
cs-410_8_2_111,cs-410,8,2,Syntagmatic,"00:08:29,070","00:08:36,360",111,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=509,So we now also know when,pic_cs-410_8_2_480.jpg
cs-410_8_2_112,cs-410,8,2,Syntagmatic,"00:08:36,360","00:08:41,400",112,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=516,"meat, then the conditional entropy",pic_cs-410_8_2_480.jpg
cs-410_8_2_113,cs-410,8,2,Syntagmatic,"00:08:41,400","00:08:45,300",113,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=521,And for what kind of words,pic_cs-410_8_2_480.jpg
cs-410_8_2_114,cs-410,8,2,Syntagmatic,"00:08:45,300","00:08:49,885",114,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=525,"Well, that's when this stuff",pic_cs-410_8_2_480.jpg
cs-410_8_2_115,cs-410,8,2,Syntagmatic,"00:08:49,885","00:08:55,339",115,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=529,"And like the for example,",pic_cs-410_8_2_480.jpg
cs-410_8_2_116,cs-410,8,2,Syntagmatic,"00:08:55,339","00:08:58,480",116,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=535,which is the entropy of meat itself.,pic_cs-410_8_2_480.jpg
cs-410_8_2_117,cs-410,8,2,Syntagmatic,"00:08:59,970","00:09:03,050",117,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=539,So this suggests that when you,pic_cs-410_8_2_480.jpg
cs-410_8_2_118,cs-410,8,2,Syntagmatic,"00:09:03,050","00:09:07,710",118,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=543,"mining syntagmatic relations,",pic_cs-410_8_2_540.jpg
cs-410_8_2_119,cs-410,8,2,Syntagmatic,"00:09:10,140","00:09:14,780",119,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=550,"For each word W1, we're going to",pic_cs-410_8_2_540.jpg
cs-410_8_2_120,cs-410,8,2,Syntagmatic,"00:09:14,780","00:09:21,020",120,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=554,"And then, we can compute",pic_cs-410_8_2_540.jpg
cs-410_8_2_121,cs-410,8,2,Syntagmatic,"00:09:22,170","00:09:26,630",121,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=562,We thought all the candidate was in,pic_cs-410_8_2_540.jpg
cs-410_8_2_122,cs-410,8,2,Syntagmatic,"00:09:26,630","00:09:30,090",122,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=566,"because we're out of favor,",pic_cs-410_8_2_540.jpg
cs-410_8_2_123,cs-410,8,2,Syntagmatic,"00:09:30,090","00:09:34,637",123,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=570,Meaning that it helps us predict,pic_cs-410_8_2_540.jpg
cs-410_8_2_124,cs-410,8,2,Syntagmatic,"00:09:34,637","00:09:38,378",124,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=574,"And then, we're going to take the top ring",pic_cs-410_8_2_540.jpg
cs-410_8_2_125,cs-410,8,2,Syntagmatic,"00:09:38,378","00:09:40,480",125,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=578,potential syntagmatic relations with W1.,pic_cs-410_8_2_540.jpg
cs-410_8_2_126,cs-410,8,2,Syntagmatic,"00:09:41,910","00:09:47,700",126,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=581,Note that we need to use,pic_cs-410_8_2_540.jpg
cs-410_8_2_127,cs-410,8,2,Syntagmatic,"00:09:47,700","00:09:51,474",127,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=587,The stresser can be the number,pic_cs-410_8_2_540.jpg
cs-410_8_2_128,cs-410,8,2,Syntagmatic,"00:09:51,474","00:09:54,550",128,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=591,absolute value for,pic_cs-410_8_2_540.jpg
cs-410_8_2_129,cs-410,8,2,Syntagmatic,"00:09:55,900","00:10:00,110",129,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=595,"Now, this would allow us to mine the most",pic_cs-410_8_2_540.jpg
cs-410_8_2_130,cs-410,8,2,Syntagmatic,"00:10:00,110","00:10:03,700",130,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=600,strongly correlated words with,pic_cs-410_8_2_600.jpg
cs-410_8_2_131,cs-410,8,2,Syntagmatic,"00:10:06,380","00:10:10,560",131,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=606,"But, this algorithm does not",pic_cs-410_8_2_600.jpg
cs-410_8_2_132,cs-410,8,2,Syntagmatic,"00:10:10,560","00:10:14,800",132,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=610,that K syntagmatical relations,pic_cs-410_8_2_600.jpg
cs-410_8_2_133,cs-410,8,2,Syntagmatic,"00:10:14,800","00:10:19,370",133,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=614,"Because in order to do that, we have to",pic_cs-410_8_2_600.jpg
cs-410_8_2_134,cs-410,8,2,Syntagmatic,"00:10:19,370","00:10:24,010",134,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=619,are comparable across different words.,pic_cs-410_8_2_600.jpg
cs-410_8_2_135,cs-410,8,2,Syntagmatic,"00:10:24,010","00:10:28,470",135,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=624,In this case of discovering,pic_cs-410_8_2_600.jpg
cs-410_8_2_136,cs-410,8,2,Syntagmatic,"00:10:28,470","00:10:33,520",136,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=628,"a targeted word like W1, we only need",pic_cs-410_8_2_600.jpg
cs-410_8_2_137,cs-410,8,2,Syntagmatic,"00:10:34,980","00:10:38,600",137,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=634,"for W1, given different words.",pic_cs-410_8_2_600.jpg
cs-410_8_2_138,cs-410,8,2,Syntagmatic,"00:10:38,600","00:10:40,780",138,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=638,"And in this case, they are comparable.",pic_cs-410_8_2_600.jpg
cs-410_8_2_139,cs-410,8,2,Syntagmatic,"00:10:41,860","00:10:43,690",139,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=641,All right.,pic_cs-410_8_2_600.jpg
cs-410_8_2_140,cs-410,8,2,Syntagmatic,"00:10:43,690","00:10:48,040",140,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=643,"So, the conditional entropy of W1, given",pic_cs-410_8_2_600.jpg
cs-410_8_2_141,cs-410,8,2,Syntagmatic,"00:10:48,040","00:10:49,770",141,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=648,given W3 are comparable.,pic_cs-410_8_2_600.jpg
cs-410_8_2_142,cs-410,8,2,Syntagmatic,"00:10:51,100","00:10:55,490",142,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=651,They all measure how hard,pic_cs-410_8_2_600.jpg
cs-410_8_2_143,cs-410,8,2,Syntagmatic,"00:10:55,490","00:11:00,070",143,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=655,"But, if we think about the two pairs,",pic_cs-410_8_2_600.jpg
cs-410_8_2_144,cs-410,8,2,Syntagmatic,"00:11:00,070","00:11:06,370",144,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=660,"where we share W2 in the same condition,",pic_cs-410_8_2_660.jpg
cs-410_8_2_145,cs-410,8,2,Syntagmatic,"00:11:06,370","00:11:11,296",145,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=666,"Then, the conditional entropies",pic_cs-410_8_2_660.jpg
cs-410_8_2_146,cs-410,8,2,Syntagmatic,"00:11:11,296","00:11:15,925",146,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=671,You can think of about this question.,pic_cs-410_8_2_660.jpg
cs-410_8_2_147,cs-410,8,2,Syntagmatic,"00:11:15,925","00:11:17,022",147,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=675,Why?,pic_cs-410_8_2_660.jpg
cs-410_8_2_148,cs-410,8,2,Syntagmatic,"00:11:17,022","00:11:19,870",148,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=677,So why are they not comfortable?,pic_cs-410_8_2_660.jpg
cs-410_8_2_149,cs-410,8,2,Syntagmatic,"00:11:19,870","00:11:23,210",149,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=679,"Well, that was because they",pic_cs-410_8_2_660.jpg
cs-410_8_2_150,cs-410,8,2,Syntagmatic,"00:11:23,210","00:11:25,690",150,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=683,Right?,pic_cs-410_8_2_660.jpg
cs-410_8_2_151,cs-410,8,2,Syntagmatic,"00:11:25,690","00:11:29,230",151,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=685,the entropy of W1 and the entropy of W3.,pic_cs-410_8_2_660.jpg
cs-410_8_2_152,cs-410,8,2,Syntagmatic,"00:11:29,230","00:11:31,150",152,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=689,And they have different upper bounds.,pic_cs-410_8_2_660.jpg
cs-410_8_2_153,cs-410,8,2,Syntagmatic,"00:11:31,150","00:11:35,000",153,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=691,So we cannot really,pic_cs-410_8_2_660.jpg
cs-410_8_2_154,cs-410,8,2,Syntagmatic,"00:11:35,000","00:11:36,420",154,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=695,So how do we address this problem?,pic_cs-410_8_2_660.jpg
cs-410_8_2_155,cs-410,8,2,Syntagmatic,"00:11:38,000","00:11:45,219",155,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=698,"Well later, we'll discuss, we can use",pic_cs-410_8_2_660.jpg
cs-410_8_2_156,cs-410,8,2,Syntagmatic,"00:11:45,219","00:11:55,219",156,https://www.coursera.org/learn/cs-410/lecture/ZAjmz?t=705,[MUSIC],pic_cs-410_8_2_660.jpg
cs-410_8_3_1,cs-410,8,3,Syntagmatic,"00:00:00,025","00:00:07,457",1,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=0,[SOUND].,pic_cs-410_8_3_0.jpg
cs-410_8_3_2,cs-410,8,3,Syntagmatic,"00:00:07,457","00:00:11,800",2,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=7,This lecture is about the syntagmatic,pic_cs-410_8_3_0.jpg
cs-410_8_3_3,cs-410,8,3,Syntagmatic,"00:00:13,400","00:00:18,196",3,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=13,In this lecture we are going to continue,pic_cs-410_8_3_0.jpg
cs-410_8_3_4,cs-410,8,3,Syntagmatic,"00:00:18,196","00:00:20,850",4,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=18,"In particular,",pic_cs-410_8_3_0.jpg
cs-410_8_3_5,cs-410,8,3,Syntagmatic,"00:00:20,850","00:00:24,880",5,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=20,"the concept in the information series,",pic_cs-410_8_3_0.jpg
cs-410_8_3_6,cs-410,8,3,Syntagmatic,"00:00:24,880","00:00:28,760",6,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=24,how it can be used to discover,pic_cs-410_8_3_0.jpg
cs-410_8_3_7,cs-410,8,3,Syntagmatic,"00:00:28,760","00:00:32,880",7,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=28,Before we talked about the problem,pic_cs-410_8_3_0.jpg
cs-410_8_3_8,cs-410,8,3,Syntagmatic,"00:00:32,880","00:00:38,014",8,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=32,that is the conditional entropy,pic_cs-410_8_3_0.jpg
cs-410_8_3_9,cs-410,8,3,Syntagmatic,"00:00:38,014","00:00:42,600",9,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=38,"It is not really comparable, so",pic_cs-410_8_3_0.jpg
cs-410_8_3_10,cs-410,8,3,Syntagmatic,"00:00:42,600","00:00:48,360",10,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=42,strong synagmatic relations,pic_cs-410_8_3_0.jpg
cs-410_8_3_11,cs-410,8,3,Syntagmatic,"00:00:48,360","00:00:53,050",11,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=48,So now we are going to introduce mutual,pic_cs-410_8_3_0.jpg
cs-410_8_3_12,cs-410,8,3,Syntagmatic,"00:00:53,050","00:00:57,370",12,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=53,in the information series,pic_cs-410_8_3_0.jpg
cs-410_8_3_13,cs-410,8,3,Syntagmatic,"00:00:57,370","00:01:03,460",13,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=57,normalize the conditional entropy to make,pic_cs-410_8_3_0.jpg
cs-410_8_3_14,cs-410,8,3,Syntagmatic,"00:01:04,930","00:01:10,090",14,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=64,"In particular, mutual information",pic_cs-410_8_3_60.jpg
cs-410_8_3_15,cs-410,8,3,Syntagmatic,"00:01:10,090","00:01:17,380",15,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=70,matches the entropy reduction,pic_cs-410_8_3_60.jpg
cs-410_8_3_16,cs-410,8,3,Syntagmatic,"00:01:17,380","00:01:22,270",16,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=77,More specifically the question we,pic_cs-410_8_3_60.jpg
cs-410_8_3_17,cs-410,8,3,Syntagmatic,"00:01:22,270","00:01:25,463",17,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=82,of an entropy of X can,pic_cs-410_8_3_60.jpg
cs-410_8_3_18,cs-410,8,3,Syntagmatic,"00:01:27,220","00:01:31,940",18,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=87,So mathematically it can be,pic_cs-410_8_3_60.jpg
cs-410_8_3_19,cs-410,8,3,Syntagmatic,"00:01:31,940","00:01:36,670",19,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=91,"the original entropy of X, and",pic_cs-410_8_3_60.jpg
cs-410_8_3_20,cs-410,8,3,Syntagmatic,"00:01:37,970","00:01:42,730",20,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=97,"And you might see,",pic_cs-410_8_3_60.jpg
cs-410_8_3_21,cs-410,8,3,Syntagmatic,"00:01:42,730","00:01:47,790",21,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=102,as reduction of entropy of,pic_cs-410_8_3_60.jpg
cs-410_8_3_22,cs-410,8,3,Syntagmatic,"00:01:48,930","00:01:54,070",22,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=108,Now normally the two conditional,pic_cs-410_8_3_60.jpg
cs-410_8_3_23,cs-410,8,3,Syntagmatic,"00:01:54,070","00:01:58,240",23,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=114,"the entropy of Y given X are not equal,",pic_cs-410_8_3_60.jpg
cs-410_8_3_24,cs-410,8,3,Syntagmatic,"00:01:58,240","00:02:05,476",24,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=118,the reduction of entropy by knowing,pic_cs-410_8_3_60.jpg
cs-410_8_3_25,cs-410,8,3,Syntagmatic,"00:02:05,476","00:02:12,805",25,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=125,"So, this quantity is called a Mutual",pic_cs-410_8_3_120.jpg
cs-410_8_3_26,cs-410,8,3,Syntagmatic,"00:02:12,805","00:02:17,085",26,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=132,And this function has some interesting,pic_cs-410_8_3_120.jpg
cs-410_8_3_27,cs-410,8,3,Syntagmatic,"00:02:17,085","00:02:21,415",27,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=137,This is easy to understand because,pic_cs-410_8_3_120.jpg
cs-410_8_3_28,cs-410,8,3,Syntagmatic,"00:02:22,782","00:02:29,132",28,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=142,not going to be lower than the possibility,pic_cs-410_8_3_120.jpg
cs-410_8_3_29,cs-410,8,3,Syntagmatic,"00:02:29,132","00:02:33,512",29,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=149,"In other words, the conditional entropy",pic_cs-410_8_3_120.jpg
cs-410_8_3_30,cs-410,8,3,Syntagmatic,"00:02:33,512","00:02:37,784",30,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=153,Knowing some information can,pic_cs-410_8_3_120.jpg
cs-410_8_3_31,cs-410,8,3,Syntagmatic,"00:02:37,784","00:02:40,282",31,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=157,will not hurt us in predicting x.,pic_cs-410_8_3_120.jpg
cs-410_8_3_32,cs-410,8,3,Syntagmatic,"00:02:41,510","00:02:46,375",32,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=161,The signal property is that it,pic_cs-410_8_3_120.jpg
cs-410_8_3_33,cs-410,8,3,Syntagmatic,"00:02:46,375","00:02:51,142",33,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=166,"entropy is not symmetrical,",pic_cs-410_8_3_120.jpg
cs-410_8_3_34,cs-410,8,3,Syntagmatic,"00:02:51,142","00:02:56,394",34,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=171,the third property is that It,pic_cs-410_8_3_120.jpg
cs-410_8_3_35,cs-410,8,3,Syntagmatic,"00:02:56,394","00:03:01,580",35,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=176,only if the two random variables,pic_cs-410_8_3_120.jpg
cs-410_8_3_36,cs-410,8,3,Syntagmatic,"00:03:01,580","00:03:07,949",36,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=181,That means knowing one of them does not,pic_cs-410_8_3_180.jpg
cs-410_8_3_37,cs-410,8,3,Syntagmatic,"00:03:07,949","00:03:14,626",37,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=187,this last property can be verified by,pic_cs-410_8_3_180.jpg
cs-410_8_3_38,cs-410,8,3,Syntagmatic,"00:03:14,626","00:03:19,144",38,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=194,it reaches 0 if and,pic_cs-410_8_3_180.jpg
cs-410_8_3_39,cs-410,8,3,Syntagmatic,"00:03:19,144","00:03:24,102",39,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=199,[INAUDIBLE] Y is exactly the same,pic_cs-410_8_3_180.jpg
cs-410_8_3_40,cs-410,8,3,Syntagmatic,"00:03:24,102","00:03:28,344",40,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=204,So that means knowing why it did not,pic_cs-410_8_3_180.jpg
cs-410_8_3_41,cs-410,8,3,Syntagmatic,"00:03:28,344","00:03:30,520",41,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=208,a Y are completely independent.,pic_cs-410_8_3_180.jpg
cs-410_8_3_42,cs-410,8,3,Syntagmatic,"00:03:32,120","00:03:37,880",42,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=212,Now when we fix X to rank different,pic_cs-410_8_3_180.jpg
cs-410_8_3_43,cs-410,8,3,Syntagmatic,"00:03:37,880","00:03:44,180",43,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=217,would give the same order as,pic_cs-410_8_3_180.jpg
cs-410_8_3_44,cs-410,8,3,Syntagmatic,"00:03:44,180","00:03:49,940",44,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=224,"because in the function here,",pic_cs-410_8_3_180.jpg
cs-410_8_3_45,cs-410,8,3,Syntagmatic,"00:03:49,940","00:03:53,820",45,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=229,So ranking based on mutual entropy is,pic_cs-410_8_3_180.jpg
cs-410_8_3_46,cs-410,8,3,Syntagmatic,"00:03:53,820","00:03:57,600",46,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=233,"the conditional entropy of X given Y, but",pic_cs-410_8_3_180.jpg
cs-410_8_3_47,cs-410,8,3,Syntagmatic,"00:03:57,600","00:04:03,058",47,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=237,the mutual information allows us to,pic_cs-410_8_3_180.jpg
cs-410_8_3_48,cs-410,8,3,Syntagmatic,"00:04:03,058","00:04:07,990",48,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=243,"So, that is why mutual information is",pic_cs-410_8_3_240.jpg
cs-410_8_3_49,cs-410,8,3,Syntagmatic,"00:04:10,688","00:04:14,420",49,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=250,"So, let us examine the intuition",pic_cs-410_8_3_240.jpg
cs-410_8_3_50,cs-410,8,3,Syntagmatic,"00:04:14,420","00:04:15,880",50,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=254,Syntagmatical Relation Mining.,pic_cs-410_8_3_240.jpg
cs-410_8_3_51,cs-410,8,3,Syntagmatic,"00:04:17,150","00:04:20,430",51,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=257,"Now, the question we ask forcing",pic_cs-410_8_3_240.jpg
cs-410_8_3_52,cs-410,8,3,Syntagmatic,"00:04:20,430","00:04:24,300",52,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=260,"whenever ""eats"" occurs,",pic_cs-410_8_3_240.jpg
cs-410_8_3_53,cs-410,8,3,Syntagmatic,"00:04:25,610","00:04:30,710",53,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=265,So this question can be framed as,pic_cs-410_8_3_240.jpg
cs-410_8_3_54,cs-410,8,3,Syntagmatic,"00:04:30,710","00:04:33,055",54,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=270,which words have high mutual,pic_cs-410_8_3_240.jpg
cs-410_8_3_55,cs-410,8,3,Syntagmatic,"00:04:33,055","00:04:37,700",55,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=273,so computer the missing information,pic_cs-410_8_3_240.jpg
cs-410_8_3_56,cs-410,8,3,Syntagmatic,"00:04:39,050","00:04:44,520",56,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=279,"And if we do that, and it is basically",pic_cs-410_8_3_240.jpg
cs-410_8_3_57,cs-410,8,3,Syntagmatic,"00:04:44,520","00:04:48,990",57,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=284,we will see that words that,pic_cs-410_8_3_240.jpg
cs-410_8_3_58,cs-410,8,3,Syntagmatic,"00:04:48,990","00:04:50,960",58,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=288,will have a high point.,pic_cs-410_8_3_240.jpg
cs-410_8_3_59,cs-410,8,3,Syntagmatic,"00:04:50,960","00:04:55,200",59,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=290,Whereas words that are not related,pic_cs-410_8_3_240.jpg
cs-410_8_3_60,cs-410,8,3,Syntagmatic,"00:04:55,200","00:04:58,530",60,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=295,"For this, I will give some example here.",pic_cs-410_8_3_240.jpg
cs-410_8_3_61,cs-410,8,3,Syntagmatic,"00:04:58,530","00:05:01,220",61,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=298,"The mutual information between ""eats"" and",pic_cs-410_8_3_240.jpg
cs-410_8_3_62,cs-410,8,3,Syntagmatic,"00:05:01,220","00:05:05,650",62,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=301,"which is the same as between ""meats"" and",pic_cs-410_8_3_300.jpg
cs-410_8_3_63,cs-410,8,3,Syntagmatic,"00:05:05,650","00:05:10,960",63,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=305,symmetrical is expected to be higher than,pic_cs-410_8_3_300.jpg
cs-410_8_3_64,cs-410,8,3,Syntagmatic,"00:05:10,960","00:05:14,638",64,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=310,"the, because knowing the does not",pic_cs-410_8_3_300.jpg
cs-410_8_3_65,cs-410,8,3,Syntagmatic,"00:05:14,638","00:05:17,998",65,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=314,"It is similar, and",pic_cs-410_8_3_300.jpg
cs-410_8_3_66,cs-410,8,3,Syntagmatic,"00:05:17,998","00:05:22,280",66,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=317,the as well.,pic_cs-410_8_3_300.jpg
cs-410_8_3_67,cs-410,8,3,Syntagmatic,"00:05:22,280","00:05:26,970",67,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=322,And you also can easily,pic_cs-410_8_3_300.jpg
cs-410_8_3_68,cs-410,8,3,Syntagmatic,"00:05:26,970","00:05:32,030",68,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=326,information between a word and,pic_cs-410_8_3_300.jpg
cs-410_8_3_69,cs-410,8,3,Syntagmatic,"00:05:32,030","00:05:37,890",69,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=332,which is equal to,pic_cs-410_8_3_300.jpg
cs-410_8_3_70,cs-410,8,3,Syntagmatic,"00:05:37,890","00:05:42,740",70,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=337,"so, because in this case the reduction is",pic_cs-410_8_3_300.jpg
cs-410_8_3_71,cs-410,8,3,Syntagmatic,"00:05:42,740","00:05:48,530",71,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=342,maximum because knowing one allows,pic_cs-410_8_3_300.jpg
cs-410_8_3_72,cs-410,8,3,Syntagmatic,"00:05:48,530","00:05:50,570",72,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=348,"So the conditional entropy is zero,",pic_cs-410_8_3_300.jpg
cs-410_8_3_73,cs-410,8,3,Syntagmatic,"00:05:50,570","00:05:54,472",73,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=350,therefore the mutual information,pic_cs-410_8_3_300.jpg
cs-410_8_3_74,cs-410,8,3,Syntagmatic,"00:05:54,472","00:06:02,520",74,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=354,"It is going to be larger, then are equal",pic_cs-410_8_3_300.jpg
cs-410_8_3_75,cs-410,8,3,Syntagmatic,"00:06:02,520","00:06:05,420",75,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=362,In other words picking any other word and,pic_cs-410_8_3_360.jpg
cs-410_8_3_76,cs-410,8,3,Syntagmatic,"00:06:05,420","00:06:08,588",76,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=365,the computer picking between eats and,pic_cs-410_8_3_360.jpg
cs-410_8_3_77,cs-410,8,3,Syntagmatic,"00:06:08,588","00:06:13,511",77,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=368,You will not get any information larger,pic_cs-410_8_3_360.jpg
cs-410_8_3_78,cs-410,8,3,Syntagmatic,"00:06:16,386","00:06:21,390",78,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=376,So now let us look at how to,pic_cs-410_8_3_360.jpg
cs-410_8_3_79,cs-410,8,3,Syntagmatic,"00:06:21,390","00:06:23,490",79,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=381,"Now in order to do that, we often",pic_cs-410_8_3_360.jpg
cs-410_8_3_80,cs-410,8,3,Syntagmatic,"00:06:25,110","00:06:29,100",80,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=385,use a different form of mutual,pic_cs-410_8_3_360.jpg
cs-410_8_3_81,cs-410,8,3,Syntagmatic,"00:06:29,100","00:06:34,190",81,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=389,rewrite the mutual information,pic_cs-410_8_3_360.jpg
cs-410_8_3_82,cs-410,8,3,Syntagmatic,"00:06:34,190","00:06:38,655",82,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=394,Where we essentially see,pic_cs-410_8_3_360.jpg
cs-410_8_3_83,cs-410,8,3,Syntagmatic,"00:06:38,655","00:06:43,075",83,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=398,called a KL-divergence or divergence.,pic_cs-410_8_3_360.jpg
cs-410_8_3_84,cs-410,8,3,Syntagmatic,"00:06:43,075","00:06:45,615",84,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=403,This is another term,pic_cs-410_8_3_360.jpg
cs-410_8_3_85,cs-410,8,3,Syntagmatic,"00:06:45,615","00:06:48,865",85,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=405,It measures the divergence,pic_cs-410_8_3_360.jpg
cs-410_8_3_86,cs-410,8,3,Syntagmatic,"00:06:50,615","00:06:54,645",86,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=410,"Now, if you look at the formula,",pic_cs-410_8_3_360.jpg
cs-410_8_3_87,cs-410,8,3,Syntagmatic,"00:06:54,645","00:06:58,190",87,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=414,different values of the two random,pic_cs-410_8_3_360.jpg
cs-410_8_3_88,cs-410,8,3,Syntagmatic,"00:06:58,190","00:07:04,110",88,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=418,mainly we are doing a comparison,pic_cs-410_8_3_360.jpg
cs-410_8_3_89,cs-410,8,3,Syntagmatic,"00:07:04,110","00:07:06,690",89,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=424,"The numerator has the joint,",pic_cs-410_8_3_420.jpg
cs-410_8_3_90,cs-410,8,3,Syntagmatic,"00:07:06,690","00:07:11,110",90,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=426,actual observed the joint distribution,pic_cs-410_8_3_420.jpg
cs-410_8_3_91,cs-410,8,3,Syntagmatic,"00:07:12,690","00:07:15,720",91,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=432,The bottom part or the denominator can be,pic_cs-410_8_3_420.jpg
cs-410_8_3_92,cs-410,8,3,Syntagmatic,"00:07:15,720","00:07:20,695",92,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=435,interpreted as the expected joint,pic_cs-410_8_3_420.jpg
cs-410_8_3_93,cs-410,8,3,Syntagmatic,"00:07:20,695","00:07:26,782",93,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=440,if they were independent because when,pic_cs-410_8_3_420.jpg
cs-410_8_3_94,cs-410,8,3,Syntagmatic,"00:07:26,782","00:07:32,810",94,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=446,they are joined distribution is equal to,pic_cs-410_8_3_420.jpg
cs-410_8_3_95,cs-410,8,3,Syntagmatic,"00:07:35,300","00:07:39,800",95,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=455,So this comparison will tell us whether,pic_cs-410_8_3_420.jpg
cs-410_8_3_96,cs-410,8,3,Syntagmatic,"00:07:39,800","00:07:43,170",96,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=459,If they are indeed independent then we,pic_cs-410_8_3_420.jpg
cs-410_8_3_97,cs-410,8,3,Syntagmatic,"00:07:44,390","00:07:49,470",97,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=464,but if the numerator is different,pic_cs-410_8_3_420.jpg
cs-410_8_3_98,cs-410,8,3,Syntagmatic,"00:07:49,470","00:07:54,530",98,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=469,the two variables are not independent and,pic_cs-410_8_3_420.jpg
cs-410_8_3_99,cs-410,8,3,Syntagmatic,"00:07:56,120","00:08:00,110",99,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=476,The sum is simply to take into,pic_cs-410_8_3_420.jpg
cs-410_8_3_100,cs-410,8,3,Syntagmatic,"00:08:00,110","00:08:04,180",100,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=480,of the values of these,pic_cs-410_8_3_480.jpg
cs-410_8_3_101,cs-410,8,3,Syntagmatic,"00:08:04,180","00:08:08,750",101,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=484,"In our case, each random variable",pic_cs-410_8_3_480.jpg
cs-410_8_3_102,cs-410,8,3,Syntagmatic,"00:08:08,750","00:08:13,950",102,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=488,"zero or one, so",pic_cs-410_8_3_480.jpg
cs-410_8_3_103,cs-410,8,3,Syntagmatic,"00:08:13,950","00:08:17,330",103,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=493,If we look at this form of mutual,pic_cs-410_8_3_480.jpg
cs-410_8_3_104,cs-410,8,3,Syntagmatic,"00:08:17,330","00:08:21,230",104,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=497,information matches the divergence,pic_cs-410_8_3_480.jpg
cs-410_8_3_105,cs-410,8,3,Syntagmatic,"00:08:21,230","00:08:25,800",105,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=501,from the expected distribution,pic_cs-410_8_3_480.jpg
cs-410_8_3_106,cs-410,8,3,Syntagmatic,"00:08:25,800","00:08:30,144",106,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=505,"The larger this divergence is, the higher",pic_cs-410_8_3_480.jpg
cs-410_8_3_107,cs-410,8,3,Syntagmatic,"00:08:33,507","00:08:37,091",107,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=513,So now let us further look at what,pic_cs-410_8_3_480.jpg
cs-410_8_3_108,cs-410,8,3,Syntagmatic,"00:08:37,091","00:08:39,840",108,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=517,involved in this formula,pic_cs-410_8_3_480.jpg
cs-410_8_3_109,cs-410,8,3,Syntagmatic,"00:08:41,300","00:08:45,080",109,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=521,"And here, this is all the probabilities",pic_cs-410_8_3_480.jpg
cs-410_8_3_110,cs-410,8,3,Syntagmatic,"00:08:45,080","00:08:46,500",110,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=525,you to verify that.,pic_cs-410_8_3_480.jpg
cs-410_8_3_111,cs-410,8,3,Syntagmatic,"00:08:46,500","00:08:51,610",111,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=526,"Basically, we have first to",pic_cs-410_8_3_480.jpg
cs-410_8_3_112,cs-410,8,3,Syntagmatic,"00:08:51,610","00:08:56,380",112,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=531,corresponding to the presence or,pic_cs-410_8_3_480.jpg
cs-410_8_3_113,cs-410,8,3,Syntagmatic,"00:08:56,380","00:08:59,610",113,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=536,"So, for w1,",pic_cs-410_8_3_480.jpg
cs-410_8_3_114,cs-410,8,3,Syntagmatic,"00:09:02,600","00:09:07,995",114,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=542,"They should sum to one, because a word",pic_cs-410_8_3_540.jpg
cs-410_8_3_115,cs-410,8,3,Syntagmatic,"00:09:07,995","00:09:13,260",115,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=547,"In the segment, and similarly for",pic_cs-410_8_3_540.jpg
cs-410_8_3_116,cs-410,8,3,Syntagmatic,"00:09:13,260","00:09:18,230",116,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=553,"the second word, we also have two",pic_cs-410_8_3_540.jpg
cs-410_8_3_117,cs-410,8,3,Syntagmatic,"00:09:18,230","00:09:20,920",117,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=558,"absences of this word, and",pic_cs-410_8_3_540.jpg
cs-410_8_3_118,cs-410,8,3,Syntagmatic,"00:09:21,920","00:09:26,162",118,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=561,"And finally, we have a lot of",pic_cs-410_8_3_540.jpg
cs-410_8_3_119,cs-410,8,3,Syntagmatic,"00:09:26,162","00:09:31,100",119,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=566,the scenarios of co-occurrences of,pic_cs-410_8_3_540.jpg
cs-410_8_3_120,cs-410,8,3,Syntagmatic,"00:09:34,513","00:09:39,107",120,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=574,And they sum to one because the two,pic_cs-410_8_3_540.jpg
cs-410_8_3_121,cs-410,8,3,Syntagmatic,"00:09:39,107","00:09:41,420",121,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=579,possible scenarios.,pic_cs-410_8_3_540.jpg
cs-410_8_3_122,cs-410,8,3,Syntagmatic,"00:09:41,420","00:09:43,730",122,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=581,"Either they both occur, so",pic_cs-410_8_3_540.jpg
cs-410_8_3_123,cs-410,8,3,Syntagmatic,"00:09:43,730","00:09:49,500",123,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=583,in that case both variables will have,pic_cs-410_8_3_540.jpg
cs-410_8_3_124,cs-410,8,3,Syntagmatic,"00:09:49,500","00:09:50,579",124,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=589,There are two scenarios.,pic_cs-410_8_3_540.jpg
cs-410_8_3_125,cs-410,8,3,Syntagmatic,"00:09:51,660","00:09:55,910",125,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=591,In these two cases one of the random,pic_cs-410_8_3_540.jpg
cs-410_8_3_126,cs-410,8,3,Syntagmatic,"00:09:55,910","00:10:03,560",126,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=595,the other will be zero and finally we have,pic_cs-410_8_3_540.jpg
cs-410_8_3_127,cs-410,8,3,Syntagmatic,"00:10:03,560","00:10:06,420",127,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=603,This is when the two variables,pic_cs-410_8_3_600.jpg
cs-410_8_3_128,cs-410,8,3,Syntagmatic,"00:10:07,620","00:10:12,855",128,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=607,So these are the probabilities involved,pic_cs-410_8_3_600.jpg
cs-410_8_3_129,cs-410,8,3,Syntagmatic,"00:10:12,855","00:10:13,600",129,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=612,over here.,pic_cs-410_8_3_600.jpg
cs-410_8_3_130,cs-410,8,3,Syntagmatic,"00:10:16,007","00:10:18,416",130,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=616,Once we know how to calculate,pic_cs-410_8_3_600.jpg
cs-410_8_3_131,cs-410,8,3,Syntagmatic,"00:10:18,416","00:10:20,670",131,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=618,we can easily calculate,pic_cs-410_8_3_600.jpg
cs-410_8_3_132,cs-410,8,3,Syntagmatic,"00:10:24,063","00:10:28,231",132,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=624,It is also interesting to know that,pic_cs-410_8_3_600.jpg
cs-410_8_3_133,cs-410,8,3,Syntagmatic,"00:10:28,231","00:10:32,960",133,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=628,"constraint among these probabilities,",pic_cs-410_8_3_600.jpg
cs-410_8_3_134,cs-410,8,3,Syntagmatic,"00:10:32,960","00:10:36,400",134,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=632,"So in the previous slide,",pic_cs-410_8_3_600.jpg
cs-410_8_3_135,cs-410,8,3,Syntagmatic,"00:10:36,400","00:10:41,830",135,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=636,that you have seen that,pic_cs-410_8_3_600.jpg
cs-410_8_3_136,cs-410,8,3,Syntagmatic,"00:10:41,830","00:10:46,114",136,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=641,words sum to one and,pic_cs-410_8_3_600.jpg
cs-410_8_3_137,cs-410,8,3,Syntagmatic,"00:10:46,114","00:10:53,190",137,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=646,that says the two words have these,pic_cs-410_8_3_600.jpg
cs-410_8_3_138,cs-410,8,3,Syntagmatic,"00:10:53,190","00:10:57,370",138,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=653,but we also have some additional,pic_cs-410_8_3_600.jpg
cs-410_8_3_139,cs-410,8,3,Syntagmatic,"00:10:58,600","00:11:03,670",139,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=658,"For example, this one means if we add up",pic_cs-410_8_3_600.jpg
cs-410_8_3_140,cs-410,8,3,Syntagmatic,"00:11:03,670","00:11:07,890",140,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=663,the probabilities that we observe,pic_cs-410_8_3_660.jpg
cs-410_8_3_141,cs-410,8,3,Syntagmatic,"00:11:07,890","00:11:12,500",141,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=667,the probabilities when the first word,pic_cs-410_8_3_660.jpg
cs-410_8_3_142,cs-410,8,3,Syntagmatic,"00:11:12,500","00:11:16,860",142,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=672,We get exactly the probability,pic_cs-410_8_3_660.jpg
cs-410_8_3_143,cs-410,8,3,Syntagmatic,"00:11:16,860","00:11:20,040",143,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=676,"In other words, when the word is observed.",pic_cs-410_8_3_660.jpg
cs-410_8_3_144,cs-410,8,3,Syntagmatic,"00:11:20,040","00:11:22,210",144,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=680,"When the first word is observed, and",pic_cs-410_8_3_660.jpg
cs-410_8_3_145,cs-410,8,3,Syntagmatic,"00:11:22,210","00:11:27,640",145,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=682,"there are only two scenarios, depending on",pic_cs-410_8_3_660.jpg
cs-410_8_3_146,cs-410,8,3,Syntagmatic,"00:11:27,640","00:11:31,750",146,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=687,"So, this probability captures the first",pic_cs-410_8_3_660.jpg
cs-410_8_3_147,cs-410,8,3,Syntagmatic,"00:11:31,750","00:11:33,860",147,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=691,"actually is also observed, and",pic_cs-410_8_3_660.jpg
cs-410_8_3_148,cs-410,8,3,Syntagmatic,"00:11:33,860","00:11:38,130",148,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=693,this captures the second scenario,pic_cs-410_8_3_660.jpg
cs-410_8_3_149,cs-410,8,3,Syntagmatic,"00:11:38,130","00:11:40,145",149,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=698,"So, we only see the first word, and",pic_cs-410_8_3_660.jpg
cs-410_8_3_150,cs-410,8,3,Syntagmatic,"00:11:40,145","00:11:45,410",150,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=700,it is easy to see the other equations,pic_cs-410_8_3_660.jpg
cs-410_8_3_151,cs-410,8,3,Syntagmatic,"00:11:46,980","00:11:50,980",151,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=706,Now these equations allow us to,pic_cs-410_8_3_660.jpg
cs-410_8_3_152,cs-410,8,3,Syntagmatic,"00:11:50,980","00:11:54,610",152,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=710,"other probabilities, and",pic_cs-410_8_3_660.jpg
cs-410_8_3_153,cs-410,8,3,Syntagmatic,"00:11:55,750","00:12:01,010",153,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=715,"So more specifically,",pic_cs-410_8_3_660.jpg
cs-410_8_3_154,cs-410,8,3,Syntagmatic,"00:12:01,010","00:12:06,490",154,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=721,"a word is present, like in this case,",pic_cs-410_8_3_720.jpg
cs-410_8_3_155,cs-410,8,3,Syntagmatic,"00:12:06,490","00:12:12,630",155,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=726,if we know the probability of,pic_cs-410_8_3_720.jpg
cs-410_8_3_156,cs-410,8,3,Syntagmatic,"00:12:12,630","00:12:17,002",156,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=732,then we can easily compute,pic_cs-410_8_3_720.jpg
cs-410_8_3_157,cs-410,8,3,Syntagmatic,"00:12:17,002","00:12:22,770",157,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=737,It is very easy to use this,pic_cs-410_8_3_720.jpg
cs-410_8_3_158,cs-410,8,3,Syntagmatic,"00:12:22,770","00:12:27,820",158,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=742,we take care of the computation of,pic_cs-410_8_3_720.jpg
cs-410_8_3_159,cs-410,8,3,Syntagmatic,"00:12:27,820","00:12:29,950",159,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=747,absence of each word.,pic_cs-410_8_3_720.jpg
cs-410_8_3_160,cs-410,8,3,Syntagmatic,"00:12:29,950","00:12:33,146",160,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=749,Now let's look at,pic_cs-410_8_3_720.jpg
cs-410_8_3_161,cs-410,8,3,Syntagmatic,"00:12:33,146","00:12:36,460",161,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=753,Let us assume that we also have available,pic_cs-410_8_3_720.jpg
cs-410_8_3_162,cs-410,8,3,Syntagmatic,"00:12:36,460","00:12:39,548",162,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=756,the probability that,pic_cs-410_8_3_720.jpg
cs-410_8_3_163,cs-410,8,3,Syntagmatic,"00:12:39,548","00:12:44,220",163,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=759,Now it is easy to see that we can,pic_cs-410_8_3_720.jpg
cs-410_8_3_164,cs-410,8,3,Syntagmatic,"00:12:44,220","00:12:45,829",164,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=764,probabilities based on these.,pic_cs-410_8_3_720.jpg
cs-410_8_3_165,cs-410,8,3,Syntagmatic,"00:12:46,870","00:12:51,170",165,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=766,Specifically for,pic_cs-410_8_3_720.jpg
cs-410_8_3_166,cs-410,8,3,Syntagmatic,"00:12:51,170","00:12:56,260",166,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=771,the probability that the first word,pic_cs-410_8_3_720.jpg
cs-410_8_3_167,cs-410,8,3,Syntagmatic,"00:12:56,260","00:13:02,020",167,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=776,because we know these probabilities in,pic_cs-410_8_3_720.jpg
cs-410_8_3_168,cs-410,8,3,Syntagmatic,"00:13:02,020","00:13:05,364",168,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=782,equation we can compute the probability,pic_cs-410_8_3_780.jpg
cs-410_8_3_169,cs-410,8,3,Syntagmatic,"00:13:05,364","00:13:06,000",169,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=785,Word.,pic_cs-410_8_3_780.jpg
cs-410_8_3_170,cs-410,8,3,Syntagmatic,"00:13:06,000","00:13:10,421",170,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=786,"And then finally,",pic_cs-410_8_3_780.jpg
cs-410_8_3_171,cs-410,8,3,Syntagmatic,"00:13:10,421","00:13:14,745",171,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=790,by using this equation because,pic_cs-410_8_3_780.jpg
cs-410_8_3_172,cs-410,8,3,Syntagmatic,"00:13:14,745","00:13:19,282",172,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=794,"this is also known, and",pic_cs-410_8_3_780.jpg
cs-410_8_3_173,cs-410,8,3,Syntagmatic,"00:13:19,282","00:13:23,120",173,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=799,So this can be easier to calculate.,pic_cs-410_8_3_780.jpg
cs-410_8_3_174,cs-410,8,3,Syntagmatic,"00:13:23,120","00:13:24,430",174,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=803,So now this can be calculated.,pic_cs-410_8_3_780.jpg
cs-410_8_3_175,cs-410,8,3,Syntagmatic,"00:13:26,080","00:13:30,989",175,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=806,So this slide shows that we only,pic_cs-410_8_3_780.jpg
cs-410_8_3_176,cs-410,8,3,Syntagmatic,"00:13:30,989","00:13:35,800",176,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=810,these three probabilities,pic_cs-410_8_3_780.jpg
cs-410_8_3_177,cs-410,8,3,Syntagmatic,"00:13:35,800","00:13:43,092",177,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=815,naming the presence of each word and the,pic_cs-410_8_3_780.jpg
cs-410_8_3_178,cs-410,8,3,Syntagmatic,"00:13:43,092","00:13:53,092",178,https://www.coursera.org/learn/cs-410/lecture/b1ZFI?t=823,[MUSIC],pic_cs-410_8_3_780.jpg
cs-410_8_4_1,cs-410,8,4,Syntagmatic,"00:00:00,000","00:00:04,714",1,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=0,[SOUND],pic_cs-410_8_4_0.jpg
cs-410_8_4_2,cs-410,8,4,Syntagmatic,"00:00:06,455","00:00:09,677",2,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=6,"In general, we can use the empirical count",pic_cs-410_8_4_0.jpg
cs-410_8_4_3,cs-410,8,4,Syntagmatic,"00:00:09,677","00:00:15,340",3,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=9,of events in the observed data,pic_cs-410_8_4_0.jpg
cs-410_8_4_4,cs-410,8,4,Syntagmatic,"00:00:15,340","00:00:19,080",4,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=15,And a commonly used technique is,pic_cs-410_8_4_0.jpg
cs-410_8_4_5,cs-410,8,4,Syntagmatic,"00:00:19,080","00:00:22,600",5,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=19,where we simply normalize,pic_cs-410_8_4_0.jpg
cs-410_8_4_6,cs-410,8,4,Syntagmatic,"00:00:22,600","00:00:30,330",6,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=22,"So if we do that, we can see, we can",pic_cs-410_8_4_0.jpg
cs-410_8_4_7,cs-410,8,4,Syntagmatic,"00:00:30,330","00:00:36,811",7,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=30,For estimating the probability that,pic_cs-410_8_4_0.jpg
cs-410_8_4_8,cs-410,8,4,Syntagmatic,"00:00:36,811","00:00:42,773",8,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=36,we simply normalize the count of,pic_cs-410_8_4_0.jpg
cs-410_8_4_9,cs-410,8,4,Syntagmatic,"00:00:42,773","00:00:47,278",9,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=42,So let's first take,pic_cs-410_8_4_0.jpg
cs-410_8_4_10,cs-410,8,4,Syntagmatic,"00:00:47,278","00:00:52,970",10,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=47,"On the right side, you see a list of some,",pic_cs-410_8_4_0.jpg
cs-410_8_4_11,cs-410,8,4,Syntagmatic,"00:00:52,970","00:00:55,010",11,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=52,These are segments.,pic_cs-410_8_4_0.jpg
cs-410_8_4_12,cs-410,8,4,Syntagmatic,"00:00:55,010","00:00:59,860",12,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=55,And in some segments you see both words,pic_cs-410_8_4_0.jpg
cs-410_8_4_13,cs-410,8,4,Syntagmatic,"00:00:59,860","00:01:01,630",13,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=59,both columns.,pic_cs-410_8_4_0.jpg
cs-410_8_4_14,cs-410,8,4,Syntagmatic,"00:01:01,630","00:01:05,830",14,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=61,"In some other cases only one will occur,",pic_cs-410_8_4_60.jpg
cs-410_8_4_15,cs-410,8,4,Syntagmatic,"00:01:05,830","00:01:07,590",15,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=65,the other column has zero.,pic_cs-410_8_4_60.jpg
cs-410_8_4_16,cs-410,8,4,Syntagmatic,"00:01:07,590","00:01:11,130",16,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=67,"And in all, of course, in some other",pic_cs-410_8_4_60.jpg
cs-410_8_4_17,cs-410,8,4,Syntagmatic,"00:01:11,130","00:01:13,930",17,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=71,so they are both zeros.,pic_cs-410_8_4_60.jpg
cs-410_8_4_18,cs-410,8,4,Syntagmatic,"00:01:13,930","00:01:19,310",18,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=73,"And for estimating these probabilities, we",pic_cs-410_8_4_60.jpg
cs-410_8_4_19,cs-410,8,4,Syntagmatic,"00:01:20,340","00:01:23,560",19,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=80,"So the three counts are first,",pic_cs-410_8_4_60.jpg
cs-410_8_4_20,cs-410,8,4,Syntagmatic,"00:01:23,560","00:01:27,337",20,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=83,And that's the total number of,pic_cs-410_8_4_60.jpg
cs-410_8_4_21,cs-410,8,4,Syntagmatic,"00:01:27,337","00:01:30,950",21,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=87,It's just as the ones in the column of W1.,pic_cs-410_8_4_60.jpg
cs-410_8_4_22,cs-410,8,4,Syntagmatic,"00:01:30,950","00:01:34,470",22,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=90,We can count how many,pic_cs-410_8_4_60.jpg
cs-410_8_4_23,cs-410,8,4,Syntagmatic,"00:01:34,470","00:01:40,460",23,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=94,"The segment count is for word 2, and we",pic_cs-410_8_4_60.jpg
cs-410_8_4_24,cs-410,8,4,Syntagmatic,"00:01:40,460","00:01:45,425",24,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=100,And these will give us the total,pic_cs-410_8_4_60.jpg
cs-410_8_4_25,cs-410,8,4,Syntagmatic,"00:01:45,425","00:01:49,650",25,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=105,The third count is when both words occur.,pic_cs-410_8_4_60.jpg
cs-410_8_4_26,cs-410,8,4,Syntagmatic,"00:01:49,650","00:01:55,370",26,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=109,"So this time, we're going to count",pic_cs-410_8_4_60.jpg
cs-410_8_4_27,cs-410,8,4,Syntagmatic,"00:01:56,580","00:02:00,060",27,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=116,"And then, so this would give us",pic_cs-410_8_4_60.jpg
cs-410_8_4_28,cs-410,8,4,Syntagmatic,"00:02:00,060","00:02:03,510",28,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=120,where we have seen both W1 and W2.,pic_cs-410_8_4_120.jpg
cs-410_8_4_29,cs-410,8,4,Syntagmatic,"00:02:03,510","00:02:08,112",29,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=123,"Once we have these counts,",pic_cs-410_8_4_120.jpg
cs-410_8_4_30,cs-410,8,4,Syntagmatic,"00:02:08,112","00:02:11,019",30,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=128,"which is the total number of segments, and",pic_cs-410_8_4_120.jpg
cs-410_8_4_31,cs-410,8,4,Syntagmatic,"00:02:11,019","00:02:16,706",31,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=131,this will give us the probabilities that,pic_cs-410_8_4_120.jpg
cs-410_8_4_32,cs-410,8,4,Syntagmatic,"00:02:16,706","00:02:22,301",32,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=136,"Now, there is a small problem,",pic_cs-410_8_4_120.jpg
cs-410_8_4_33,cs-410,8,4,Syntagmatic,"00:02:22,301","00:02:27,458",33,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=142,"And in this case, we don't want a zero",pic_cs-410_8_4_120.jpg
cs-410_8_4_34,cs-410,8,4,Syntagmatic,"00:02:27,458","00:02:33,365",34,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=147,"a small sample and in general, we would",pic_cs-410_8_4_120.jpg
cs-410_8_4_35,cs-410,8,4,Syntagmatic,"00:02:33,365","00:02:35,806",35,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=153,a [INAUDIBLE] to avoid any context.,pic_cs-410_8_4_120.jpg
cs-410_8_4_36,cs-410,8,4,Syntagmatic,"00:02:35,806","00:02:39,630",36,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=155,"So, to address this problem,",pic_cs-410_8_4_120.jpg
cs-410_8_4_37,cs-410,8,4,Syntagmatic,"00:02:39,630","00:02:43,780",37,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=159,And that's basically to add some,pic_cs-410_8_4_120.jpg
cs-410_8_4_38,cs-410,8,4,Syntagmatic,"00:02:43,780","00:02:48,410",38,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=163,and so that we don't get,pic_cs-410_8_4_120.jpg
cs-410_8_4_39,cs-410,8,4,Syntagmatic,"00:02:48,410","00:02:54,250",39,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=168,"Now, the best way to understand smoothing",pic_cs-410_8_4_120.jpg
cs-410_8_4_40,cs-410,8,4,Syntagmatic,"00:02:54,250","00:03:00,310",40,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=174,"data than we actually have, because we'll",pic_cs-410_8_4_120.jpg
cs-410_8_4_41,cs-410,8,4,Syntagmatic,"00:03:00,310","00:03:04,650",41,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=180,"I illustrated on the top,",pic_cs-410_8_4_180.jpg
cs-410_8_4_42,cs-410,8,4,Syntagmatic,"00:03:04,650","00:03:10,095",42,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=184,And these pseudo-segments would,pic_cs-410_8_4_180.jpg
cs-410_8_4_43,cs-410,8,4,Syntagmatic,"00:03:10,095","00:03:15,047",43,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=190,of these words so,pic_cs-410_8_4_180.jpg
cs-410_8_4_44,cs-410,8,4,Syntagmatic,"00:03:15,047","00:03:18,169",44,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=195,"Now, in particular we introduce",pic_cs-410_8_4_180.jpg
cs-410_8_4_45,cs-410,8,4,Syntagmatic,"00:03:18,169","00:03:20,990",45,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=198,Each is weighted at one quarter.,pic_cs-410_8_4_180.jpg
cs-410_8_4_46,cs-410,8,4,Syntagmatic,"00:03:20,990","00:03:25,930",46,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=200,And these represent the four different,pic_cs-410_8_4_180.jpg
cs-410_8_4_47,cs-410,8,4,Syntagmatic,"00:03:25,930","00:03:30,490",47,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=205,"So now each event,",pic_cs-410_8_4_180.jpg
cs-410_8_4_48,cs-410,8,4,Syntagmatic,"00:03:30,490","00:03:35,390",48,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=210,at least one count or at least a non-zero,pic_cs-410_8_4_180.jpg
cs-410_8_4_49,cs-410,8,4,Syntagmatic,"00:03:35,390","00:03:39,380",49,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=215,"So, in the actual segments",pic_cs-410_8_4_180.jpg
cs-410_8_4_50,cs-410,8,4,Syntagmatic,"00:03:39,380","00:03:44,231",50,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=219,it's okay if we haven't observed,pic_cs-410_8_4_180.jpg
cs-410_8_4_51,cs-410,8,4,Syntagmatic,"00:03:44,231","00:03:49,671",51,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=224,"So more specifically, you can see",pic_cs-410_8_4_180.jpg
cs-410_8_4_52,cs-410,8,4,Syntagmatic,"00:03:49,671","00:03:55,560",52,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=229,"ones in the two pseudo-segments,",pic_cs-410_8_4_180.jpg
cs-410_8_4_53,cs-410,8,4,Syntagmatic,"00:03:55,560","00:03:59,315",53,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=235,"We add them up, we get 0.5.",pic_cs-410_8_4_180.jpg
cs-410_8_4_54,cs-410,8,4,Syntagmatic,"00:03:59,315","00:04:03,319",54,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=239,"And similar to this,",pic_cs-410_8_4_180.jpg
cs-410_8_4_55,cs-410,8,4,Syntagmatic,"00:04:03,319","00:04:08,240",55,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=243,pseudo-segment that indicates,pic_cs-410_8_4_240.jpg
cs-410_8_4_56,cs-410,8,4,Syntagmatic,"00:04:09,450","00:04:14,000",56,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=249,And of course in the denominator we add,pic_cs-410_8_4_240.jpg
cs-410_8_4_57,cs-410,8,4,Syntagmatic,"00:04:14,000","00:04:17,520",57,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=254,"we add, in this case,",pic_cs-410_8_4_240.jpg
cs-410_8_4_58,cs-410,8,4,Syntagmatic,"00:04:17,520","00:04:21,780",58,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=257,Each is weighed at one quarter so,pic_cs-410_8_4_240.jpg
cs-410_8_4_59,cs-410,8,4,Syntagmatic,"00:04:21,780","00:04:24,110",59,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=261,"So, that's why in the denominator",pic_cs-410_8_4_240.jpg
cs-410_8_4_60,cs-410,8,4,Syntagmatic,"00:04:25,990","00:04:31,460",60,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=265,"So, this basically concludes",pic_cs-410_8_4_240.jpg
cs-410_8_4_61,cs-410,8,4,Syntagmatic,"00:04:31,460","00:04:33,920",61,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=271,four syntagmatic relation discoveries.,pic_cs-410_8_4_240.jpg
cs-410_8_4_62,cs-410,8,4,Syntagmatic,"00:04:36,090","00:04:42,050",62,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=276,"Now, so to summarize,",pic_cs-410_8_4_240.jpg
cs-410_8_4_63,cs-410,8,4,Syntagmatic,"00:04:42,050","00:04:46,240",63,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=282,be discovered by measuring correlations,pic_cs-410_8_4_240.jpg
cs-410_8_4_64,cs-410,8,4,Syntagmatic,"00:04:46,240","00:04:49,580",64,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=286,We've introduced the three,pic_cs-410_8_4_240.jpg
cs-410_8_4_65,cs-410,8,4,Syntagmatic,"00:04:49,580","00:04:53,230",65,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=289,"Entropy, which measures the uncertainty",pic_cs-410_8_4_240.jpg
cs-410_8_4_66,cs-410,8,4,Syntagmatic,"00:04:53,230","00:04:59,060",66,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=293,"Conditional entropy, which measures",pic_cs-410_8_4_240.jpg
cs-410_8_4_67,cs-410,8,4,Syntagmatic,"00:04:59,060","00:05:04,530",67,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=299,"And mutual information of X and Y,",pic_cs-410_8_4_240.jpg
cs-410_8_4_68,cs-410,8,4,Syntagmatic,"00:05:04,530","00:05:11,240",68,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=304,"due to knowing Y, or",pic_cs-410_8_4_300.jpg
cs-410_8_4_69,cs-410,8,4,Syntagmatic,"00:05:11,240","00:05:12,660",69,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=311,They are the same.,pic_cs-410_8_4_300.jpg
cs-410_8_4_70,cs-410,8,4,Syntagmatic,"00:05:12,660","00:05:17,111",70,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=312,So these three concepts are actually very,pic_cs-410_8_4_300.jpg
cs-410_8_4_71,cs-410,8,4,Syntagmatic,"00:05:17,111","00:05:20,340",71,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=317,That's why we spent some time,pic_cs-410_8_4_300.jpg
cs-410_8_4_72,cs-410,8,4,Syntagmatic,"00:05:20,340","00:05:23,150",72,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=320,"But in particular,",pic_cs-410_8_4_300.jpg
cs-410_8_4_73,cs-410,8,4,Syntagmatic,"00:05:23,150","00:05:25,960",73,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=323,discovering syntagmatic relations.,pic_cs-410_8_4_300.jpg
cs-410_8_4_74,cs-410,8,4,Syntagmatic,"00:05:25,960","00:05:30,142",74,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=325,"In particular,",pic_cs-410_8_4_300.jpg
cs-410_8_4_75,cs-410,8,4,Syntagmatic,"00:05:30,142","00:05:32,370",75,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=330,discovering such a relation.,pic_cs-410_8_4_300.jpg
cs-410_8_4_76,cs-410,8,4,Syntagmatic,"00:05:32,370","00:05:37,241",76,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=332,It allows us to have values,pic_cs-410_8_4_300.jpg
cs-410_8_4_77,cs-410,8,4,Syntagmatic,"00:05:37,241","00:05:42,211",77,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=337,words that are comparable and,pic_cs-410_8_4_300.jpg
cs-410_8_4_78,cs-410,8,4,Syntagmatic,"00:05:42,211","00:05:48,208",78,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=342,discover the strongest syntagmatic,pic_cs-410_8_4_300.jpg
cs-410_8_4_79,cs-410,8,4,Syntagmatic,"00:05:48,208","00:05:53,700",79,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=348,"Now, note that there is some relation",pic_cs-410_8_4_300.jpg
cs-410_8_4_80,cs-410,8,4,Syntagmatic,"00:05:53,700","00:05:55,910",80,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=353,[INAUDIBLE] relation discovery.,pic_cs-410_8_4_300.jpg
cs-410_8_4_81,cs-410,8,4,Syntagmatic,"00:05:55,910","00:06:01,835",81,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=355,So we already discussed the possibility,pic_cs-410_8_4_300.jpg
cs-410_8_4_82,cs-410,8,4,Syntagmatic,"00:06:01,835","00:06:06,683",82,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=361,terms in the context to potentially,pic_cs-410_8_4_360.jpg
cs-410_8_4_83,cs-410,8,4,Syntagmatic,"00:06:06,683","00:06:11,187",83,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=366,that have syntagmatic relations,pic_cs-410_8_4_360.jpg
cs-410_8_4_84,cs-410,8,4,Syntagmatic,"00:06:11,187","00:06:17,958",84,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=371,"But here, once we use mutual information",pic_cs-410_8_4_360.jpg
cs-410_8_4_85,cs-410,8,4,Syntagmatic,"00:06:17,958","00:06:24,436",85,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=377,we can also represent the context with,pic_cs-410_8_4_360.jpg
cs-410_8_4_86,cs-410,8,4,Syntagmatic,"00:06:24,436","00:06:29,567",86,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=384,So this would give us,pic_cs-410_8_4_360.jpg
cs-410_8_4_87,cs-410,8,4,Syntagmatic,"00:06:29,567","00:06:33,490",87,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=389,"the context of a word, like a cat.",pic_cs-410_8_4_360.jpg
cs-410_8_4_88,cs-410,8,4,Syntagmatic,"00:06:33,490","00:06:37,394",88,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=393,"And if we do the same for all the words,",pic_cs-410_8_4_360.jpg
cs-410_8_4_89,cs-410,8,4,Syntagmatic,"00:06:37,394","00:06:42,320",89,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=397,compare the similarity between these,pic_cs-410_8_4_360.jpg
cs-410_8_4_90,cs-410,8,4,Syntagmatic,"00:06:42,320","00:06:45,850",90,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=402,So this provides yet,pic_cs-410_8_4_360.jpg
cs-410_8_4_91,cs-410,8,4,Syntagmatic,"00:06:45,850","00:06:48,800",91,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=405,paradigmatic relation discovery.,pic_cs-410_8_4_360.jpg
cs-410_8_4_92,cs-410,8,4,Syntagmatic,"00:06:48,800","00:06:55,770",92,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=408,And so to summarize this whole part,pic_cs-410_8_4_360.jpg
cs-410_8_4_93,cs-410,8,4,Syntagmatic,"00:06:55,770","00:06:59,190",93,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=415,"We introduce two basic associations,",pic_cs-410_8_4_360.jpg
cs-410_8_4_94,cs-410,8,4,Syntagmatic,"00:06:59,190","00:07:01,000",94,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=419,a syntagmatic relations.,pic_cs-410_8_4_360.jpg
cs-410_8_4_95,cs-410,8,4,Syntagmatic,"00:07:01,000","00:07:05,710",95,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=421,"These are fairly general, they apply",pic_cs-410_8_4_420.jpg
cs-410_8_4_96,cs-410,8,4,Syntagmatic,"00:07:05,710","00:07:10,009",96,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=425,"the units don't have to be words,",pic_cs-410_8_4_420.jpg
cs-410_8_4_97,cs-410,8,4,Syntagmatic,"00:07:11,120","00:07:16,235",97,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=431,We introduced multiple statistical,pic_cs-410_8_4_420.jpg
cs-410_8_4_98,cs-410,8,4,Syntagmatic,"00:07:16,235","00:07:20,762",98,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=436,mainly showing that pure,pic_cs-410_8_4_420.jpg
cs-410_8_4_99,cs-410,8,4,Syntagmatic,"00:07:20,762","00:07:24,840",99,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=440,are variable for,pic_cs-410_8_4_420.jpg
cs-410_8_4_100,cs-410,8,4,Syntagmatic,"00:07:24,840","00:07:28,800",100,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=444,And they can be combined to,pic_cs-410_8_4_420.jpg
cs-410_8_4_101,cs-410,8,4,Syntagmatic,"00:07:28,800","00:07:35,040",101,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=448,These approaches can be applied,pic_cs-410_8_4_420.jpg
cs-410_8_4_102,cs-410,8,4,Syntagmatic,"00:07:35,040","00:07:39,940",102,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=455,mostly because they are based,pic_cs-410_8_4_420.jpg
cs-410_8_4_103,cs-410,8,4,Syntagmatic,"00:07:39,940","00:07:42,690",103,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=459,they can actually discover,pic_cs-410_8_4_420.jpg
cs-410_8_4_104,cs-410,8,4,Syntagmatic,"00:07:44,360","00:07:47,880",104,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=464,We can also use different ways with,pic_cs-410_8_4_420.jpg
cs-410_8_4_105,cs-410,8,4,Syntagmatic,"00:07:47,880","00:07:51,360",105,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=467,this would lead us to some interesting,pic_cs-410_8_4_420.jpg
cs-410_8_4_106,cs-410,8,4,Syntagmatic,"00:07:51,360","00:07:56,190",106,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=471,"For example, the context can be very",pic_cs-410_8_4_420.jpg
cs-410_8_4_107,cs-410,8,4,Syntagmatic,"00:07:56,190","00:08:00,760",107,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=476,"a sentence, or maybe paragraphs,",pic_cs-410_8_4_420.jpg
cs-410_8_4_108,cs-410,8,4,Syntagmatic,"00:08:00,760","00:08:05,330",108,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=480,allows to discover different flavors,pic_cs-410_8_4_480.jpg
cs-410_8_4_109,cs-410,8,4,Syntagmatic,"00:08:05,330","00:08:09,362",109,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=485,"And similarly,",pic_cs-410_8_4_480.jpg
cs-410_8_4_110,cs-410,8,4,Syntagmatic,"00:08:09,362","00:08:13,380",110,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=489,visual information to discover,pic_cs-410_8_4_480.jpg
cs-410_8_4_111,cs-410,8,4,Syntagmatic,"00:08:13,380","00:08:19,110",111,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=493,"We also have to define the segment, and",pic_cs-410_8_4_480.jpg
cs-410_8_4_112,cs-410,8,4,Syntagmatic,"00:08:19,110","00:08:22,560",112,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=499,text window or a longer text article.,pic_cs-410_8_4_480.jpg
cs-410_8_4_113,cs-410,8,4,Syntagmatic,"00:08:22,560","00:08:26,508",113,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=502,And this would give us different,pic_cs-410_8_4_480.jpg
cs-410_8_4_114,cs-410,8,4,Syntagmatic,"00:08:26,508","00:08:32,677",114,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=506,These discovery associations can,pic_cs-410_8_4_480.jpg
cs-410_8_4_115,cs-410,8,4,Syntagmatic,"00:08:32,677","00:08:37,701",115,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=512,in both information retrieval and,pic_cs-410_8_4_480.jpg
cs-410_8_4_116,cs-410,8,4,Syntagmatic,"00:08:37,701","00:08:44,100",116,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=517,"So here are some recommended readings,",pic_cs-410_8_4_480.jpg
cs-410_8_4_117,cs-410,8,4,Syntagmatic,"00:08:44,100","00:08:46,880",117,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=524,The first is a book with,pic_cs-410_8_4_480.jpg
cs-410_8_4_118,cs-410,8,4,Syntagmatic,"00:08:46,880","00:08:50,810",118,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=526,which is quite relevant to,pic_cs-410_8_4_480.jpg
cs-410_8_4_119,cs-410,8,4,Syntagmatic,"00:08:50,810","00:08:55,120",119,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=530,The second is an article,pic_cs-410_8_4_480.jpg
cs-410_8_4_120,cs-410,8,4,Syntagmatic,"00:08:55,120","00:08:58,160",120,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=535,statistical measures to,pic_cs-410_8_4_480.jpg
cs-410_8_4_121,cs-410,8,4,Syntagmatic,"00:08:58,160","00:09:03,764",121,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=538,Those are phrases that,pic_cs-410_8_4_480.jpg
cs-410_8_4_122,cs-410,8,4,Syntagmatic,"00:09:03,764","00:09:07,560",122,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=543,"For example,",pic_cs-410_8_4_540.jpg
cs-410_8_4_123,cs-410,8,4,Syntagmatic,"00:09:08,610","00:09:11,550",123,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=548,blue chip is not a chip that's blue.,pic_cs-410_8_4_540.jpg
cs-410_8_4_124,cs-410,8,4,Syntagmatic,"00:09:11,550","00:09:16,180",124,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=551,And the paper has a discussion about some,pic_cs-410_8_4_540.jpg
cs-410_8_4_125,cs-410,8,4,Syntagmatic,"00:09:17,400","00:09:23,227",125,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=557,The third one is a new paper on a unified,pic_cs-410_8_4_540.jpg
cs-410_8_4_126,cs-410,8,4,Syntagmatic,"00:09:23,227","00:09:29,441",126,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=563,"relations and a syntagmatical relations,",pic_cs-410_8_4_540.jpg
cs-410_8_4_127,cs-410,8,4,Syntagmatic,"00:09:29,441","00:09:39,441",127,https://www.coursera.org/learn/cs-410/lecture/8d6Wn?t=569,[SOUND],pic_cs-410_8_4_540.jpg
cs-410_8_5_1,cs-410,8,5,Topic,"00:00:00,025","00:00:06,885",1,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=0,[SOUND],pic_cs-410_8_5_0.jpg
cs-410_8_5_2,cs-410,8,5,Topic,"00:00:06,885","00:00:11,190",2,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=6,lecture is about topic mining and,pic_cs-410_8_5_0.jpg
cs-410_8_5_3,cs-410,8,5,Topic,"00:00:11,190","00:00:14,270",3,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=11,We're going to talk about its,pic_cs-410_8_5_0.jpg
cs-410_8_5_4,cs-410,8,5,Topic,"00:00:17,780","00:00:22,630",4,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=17,In this lecture we're going to talk,pic_cs-410_8_5_0.jpg
cs-410_8_5_5,cs-410,8,5,Topic,"00:00:23,770","00:00:28,310",5,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=23,"As you see on this road map,",pic_cs-410_8_5_0.jpg
cs-410_8_5_6,cs-410,8,5,Topic,"00:00:28,310","00:00:33,190",6,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=28,"mining knowledge about language,",pic_cs-410_8_5_0.jpg
cs-410_8_5_7,cs-410,8,5,Topic,"00:00:33,190","00:00:37,987",7,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=33,word associations such as paradigmatic and,pic_cs-410_8_5_0.jpg
cs-410_8_5_8,cs-410,8,5,Topic,"00:00:39,190","00:00:43,100",8,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=39,"Now, starting from this lecture, we're",pic_cs-410_8_5_0.jpg
cs-410_8_5_9,cs-410,8,5,Topic,"00:00:43,100","00:00:47,570",9,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=43,"knowledge, which is content mining, and",pic_cs-410_8_5_0.jpg
cs-410_8_5_10,cs-410,8,5,Topic,"00:00:47,570","00:00:55,031",10,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=47,trying to discover knowledge about,pic_cs-410_8_5_0.jpg
cs-410_8_5_11,cs-410,8,5,Topic,"00:00:56,140","00:00:58,810",11,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=56,And we call that topic mining and,pic_cs-410_8_5_0.jpg
cs-410_8_5_12,cs-410,8,5,Topic,"00:00:59,920","00:01:04,350",12,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=59,"In this lecture, we're going to talk about",pic_cs-410_8_5_0.jpg
cs-410_8_5_13,cs-410,8,5,Topic,"00:01:04,350","00:01:08,260",13,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=64,"So first of all,",pic_cs-410_8_5_60.jpg
cs-410_8_5_14,cs-410,8,5,Topic,"00:01:08,260","00:01:12,600",14,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=68,So topic is something that we,pic_cs-410_8_5_60.jpg
cs-410_8_5_15,cs-410,8,5,Topic,"00:01:12,600","00:01:15,840",15,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=72,it's actually not that,pic_cs-410_8_5_60.jpg
cs-410_8_5_16,cs-410,8,5,Topic,"00:01:15,840","00:01:20,420",16,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=75,"Roughly speaking, topic is the main",pic_cs-410_8_5_60.jpg
cs-410_8_5_17,cs-410,8,5,Topic,"00:01:20,420","00:01:25,860",17,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=80,And you can think of this as a theme or,pic_cs-410_8_5_60.jpg
cs-410_8_5_18,cs-410,8,5,Topic,"00:01:25,860","00:01:28,420",18,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=85,It can also have different granularities.,pic_cs-410_8_5_60.jpg
cs-410_8_5_19,cs-410,8,5,Topic,"00:01:28,420","00:01:31,240",19,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=88,"For example,",pic_cs-410_8_5_60.jpg
cs-410_8_5_20,cs-410,8,5,Topic,"00:01:31,240","00:01:34,800",20,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=91,"A topic of article,",pic_cs-410_8_5_60.jpg
cs-410_8_5_21,cs-410,8,5,Topic,"00:01:34,800","00:01:40,540",21,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=94,the topic of all the research articles,pic_cs-410_8_5_60.jpg
cs-410_8_5_22,cs-410,8,5,Topic,"00:01:40,540","00:01:45,629",22,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=100,so different grand narratives of topics,pic_cs-410_8_5_60.jpg
cs-410_8_5_23,cs-410,8,5,Topic,"00:01:46,760","00:01:51,628",23,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=106,"Indeed, there are many applications that",pic_cs-410_8_5_60.jpg
cs-410_8_5_24,cs-410,8,5,Topic,"00:01:51,628","00:01:52,980",24,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=111,they're analyzed then.,pic_cs-410_8_5_60.jpg
cs-410_8_5_25,cs-410,8,5,Topic,"00:01:52,980","00:01:54,300",25,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=112,Here are some examples.,pic_cs-410_8_5_60.jpg
cs-410_8_5_26,cs-410,8,5,Topic,"00:01:54,300","00:01:58,280",26,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=114,"For example, we might be interested",pic_cs-410_8_5_60.jpg
cs-410_8_5_27,cs-410,8,5,Topic,"00:01:58,280","00:02:00,470",27,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=118,users are talking about today?,pic_cs-410_8_5_60.jpg
cs-410_8_5_28,cs-410,8,5,Topic,"00:02:00,470","00:02:03,600",28,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=120,"Are they talking about NBA sports, or",pic_cs-410_8_5_120.jpg
cs-410_8_5_29,cs-410,8,5,Topic,"00:02:03,600","00:02:08,540",29,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=123,are they talking about some,pic_cs-410_8_5_120.jpg
cs-410_8_5_30,cs-410,8,5,Topic,"00:02:08,540","00:02:12,970",30,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=128,Or we are interested in,pic_cs-410_8_5_120.jpg
cs-410_8_5_31,cs-410,8,5,Topic,"00:02:12,970","00:02:17,090",31,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=132,"For example, one might be interested in",pic_cs-410_8_5_120.jpg
cs-410_8_5_32,cs-410,8,5,Topic,"00:02:17,090","00:02:21,840",32,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=137,"topics in data mining, and how are they",pic_cs-410_8_5_120.jpg
cs-410_8_5_33,cs-410,8,5,Topic,"00:02:21,840","00:02:26,820",33,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=141,Now this involves discovery of topics,pic_cs-410_8_5_120.jpg
cs-410_8_5_34,cs-410,8,5,Topic,"00:02:26,820","00:02:32,910",34,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=146,also we want to discover topics in,pic_cs-410_8_5_120.jpg
cs-410_8_5_35,cs-410,8,5,Topic,"00:02:32,910","00:02:34,690",35,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=152,And then we can make a comparison.,pic_cs-410_8_5_120.jpg
cs-410_8_5_36,cs-410,8,5,Topic,"00:02:34,690","00:02:38,400",36,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=154,We might also be also interested in,pic_cs-410_8_5_120.jpg
cs-410_8_5_37,cs-410,8,5,Topic,"00:02:38,400","00:02:43,710",37,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=158,"some products like the iPhone 6,",pic_cs-410_8_5_120.jpg
cs-410_8_5_38,cs-410,8,5,Topic,"00:02:43,710","00:02:48,360",38,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=163,And this involves discovering,pic_cs-410_8_5_120.jpg
cs-410_8_5_39,cs-410,8,5,Topic,"00:02:48,360","00:02:52,470",39,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=168,iPhone 6 and,pic_cs-410_8_5_120.jpg
cs-410_8_5_40,cs-410,8,5,Topic,"00:02:52,470","00:02:56,810",40,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=172,Or perhaps we're interested in knowing,pic_cs-410_8_5_120.jpg
cs-410_8_5_41,cs-410,8,5,Topic,"00:02:56,810","00:02:58,110",41,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=176,presidential election?,pic_cs-410_8_5_120.jpg
cs-410_8_5_42,cs-410,8,5,Topic,"00:02:59,780","00:03:04,800",42,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=179,And all these have to do with discovering,pic_cs-410_8_5_120.jpg
cs-410_8_5_43,cs-410,8,5,Topic,"00:03:04,800","00:03:08,680",43,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=184,and we're going to talk about a lot,pic_cs-410_8_5_180.jpg
cs-410_8_5_44,cs-410,8,5,Topic,"00:03:08,680","00:03:12,920",44,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=188,In general we can view a topic as,pic_cs-410_8_5_180.jpg
cs-410_8_5_45,cs-410,8,5,Topic,"00:03:12,920","00:03:17,830",45,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=192,So from text data we expect to,pic_cs-410_8_5_180.jpg
cs-410_8_5_46,cs-410,8,5,Topic,"00:03:17,830","00:03:22,650",46,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=197,then these topics generally provide,pic_cs-410_8_5_180.jpg
cs-410_8_5_47,cs-410,8,5,Topic,"00:03:22,650","00:03:25,690",47,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=202,And it tells us something about the world.,pic_cs-410_8_5_180.jpg
cs-410_8_5_48,cs-410,8,5,Topic,"00:03:25,690","00:03:28,230",48,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=205,"About a product, about a person etc.",pic_cs-410_8_5_180.jpg
cs-410_8_5_49,cs-410,8,5,Topic,"00:03:29,350","00:03:32,390",49,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=209,"Now when we have some non-text data,",pic_cs-410_8_5_180.jpg
cs-410_8_5_50,cs-410,8,5,Topic,"00:03:32,390","00:03:36,420",50,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=212,then we can have more context for,pic_cs-410_8_5_180.jpg
cs-410_8_5_51,cs-410,8,5,Topic,"00:03:36,420","00:03:41,620",51,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=216,"For example, we might know the time",pic_cs-410_8_5_180.jpg
cs-410_8_5_52,cs-410,8,5,Topic,"00:03:41,620","00:03:47,110",52,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=221,locations where the text,pic_cs-410_8_5_180.jpg
cs-410_8_5_53,cs-410,8,5,Topic,"00:03:47,110","00:03:52,945",53,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=227,"or the authors of the text, or",pic_cs-410_8_5_180.jpg
cs-410_8_5_54,cs-410,8,5,Topic,"00:03:52,945","00:03:54,400",54,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=232,"All such meta data, or",pic_cs-410_8_5_180.jpg
cs-410_8_5_55,cs-410,8,5,Topic,"00:03:54,400","00:03:59,610",55,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=234,context variables can be associated,pic_cs-410_8_5_180.jpg
cs-410_8_5_56,cs-410,8,5,Topic,"00:03:59,610","00:04:05,340",56,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=239,then we can use these context variables,pic_cs-410_8_5_180.jpg
cs-410_8_5_57,cs-410,8,5,Topic,"00:04:05,340","00:04:09,320",57,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=245,"For example, looking at topics over time,",pic_cs-410_8_5_240.jpg
cs-410_8_5_58,cs-410,8,5,Topic,"00:04:09,320","00:04:14,290",58,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=249,"whether there's a trending topic, or",pic_cs-410_8_5_240.jpg
cs-410_8_5_59,cs-410,8,5,Topic,"00:04:15,620","00:04:18,950",59,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=255,Soon you are looking at topics,pic_cs-410_8_5_240.jpg
cs-410_8_5_60,cs-410,8,5,Topic,"00:04:18,950","00:04:24,185",60,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=258,We might know some insights about,pic_cs-410_8_5_240.jpg
cs-410_8_5_61,cs-410,8,5,Topic,"00:04:26,150","00:04:29,900",61,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=266,So that's why mining,pic_cs-410_8_5_240.jpg
cs-410_8_5_62,cs-410,8,5,Topic,"00:04:29,900","00:04:34,540",62,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=269,"Now, let's look at the tasks",pic_cs-410_8_5_240.jpg
cs-410_8_5_63,cs-410,8,5,Topic,"00:04:34,540","00:04:39,380",63,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=274,"In general, it would involve first",pic_cs-410_8_5_240.jpg
cs-410_8_5_64,cs-410,8,5,Topic,"00:04:39,380","00:04:40,810",64,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=279,k topics.,pic_cs-410_8_5_240.jpg
cs-410_8_5_65,cs-410,8,5,Topic,"00:04:40,810","00:04:45,430",65,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=280,"And then we also would like to know, which",pic_cs-410_8_5_240.jpg
cs-410_8_5_66,cs-410,8,5,Topic,"00:04:45,430","00:04:46,600",66,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=285,to what extent.,pic_cs-410_8_5_240.jpg
cs-410_8_5_67,cs-410,8,5,Topic,"00:04:46,600","00:04:52,970",67,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=286,"So for example, in document one, we",pic_cs-410_8_5_240.jpg
cs-410_8_5_68,cs-410,8,5,Topic,"00:04:52,970","00:04:57,390",68,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=292,Topic 2 and,pic_cs-410_8_5_240.jpg
cs-410_8_5_69,cs-410,8,5,Topic,"00:04:58,890","00:05:00,712",69,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=298,"And other topics,",pic_cs-410_8_5_240.jpg
cs-410_8_5_70,cs-410,8,5,Topic,"00:05:00,712","00:05:06,778",70,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=300,"Document two, on the other hand,",pic_cs-410_8_5_300.jpg
cs-410_8_5_71,cs-410,8,5,Topic,"00:05:06,778","00:05:10,553",71,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=306,"but it did not cover Topic 1 at all, and",pic_cs-410_8_5_300.jpg
cs-410_8_5_72,cs-410,8,5,Topic,"00:05:10,553","00:05:15,873",72,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=310,"it also covers Topic k to some extent,",pic_cs-410_8_5_300.jpg
cs-410_8_5_73,cs-410,8,5,Topic,"00:05:15,873","00:05:19,995",73,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=315,So now you can see there,pic_cs-410_8_5_300.jpg
cs-410_8_5_74,cs-410,8,5,Topic,"00:05:19,995","00:05:25,760",74,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=319,"sub-tasks, the first is to discover k",pic_cs-410_8_5_300.jpg
cs-410_8_5_75,cs-410,8,5,Topic,"00:05:25,760","00:05:27,140",75,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=325,What are these k topics?,pic_cs-410_8_5_300.jpg
cs-410_8_5_76,cs-410,8,5,Topic,"00:05:27,140","00:05:28,920",76,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=327,"Okay, major topics in the text they are.",pic_cs-410_8_5_300.jpg
cs-410_8_5_77,cs-410,8,5,Topic,"00:05:28,920","00:05:33,180",77,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=328,The second task is to figure out,pic_cs-410_8_5_300.jpg
cs-410_8_5_78,cs-410,8,5,Topic,"00:05:33,180","00:05:34,430",78,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=333,to what extent.,pic_cs-410_8_5_300.jpg
cs-410_8_5_79,cs-410,8,5,Topic,"00:05:34,430","00:05:37,810",79,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=334,"So more formally,",pic_cs-410_8_5_300.jpg
cs-410_8_5_80,cs-410,8,5,Topic,"00:05:37,810","00:05:42,365",80,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=337,"First, we have, as input,",pic_cs-410_8_5_300.jpg
cs-410_8_5_81,cs-410,8,5,Topic,"00:05:42,365","00:05:47,050",81,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=342,Here we can denote the text,pic_cs-410_8_5_300.jpg
cs-410_8_5_82,cs-410,8,5,Topic,"00:05:47,050","00:05:51,740",82,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=347,denote text article as d i.,pic_cs-410_8_5_300.jpg
cs-410_8_5_83,cs-410,8,5,Topic,"00:05:51,740","00:05:56,700",83,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=351,"And, we generally also need to have",pic_cs-410_8_5_300.jpg
cs-410_8_5_84,cs-410,8,5,Topic,"00:05:56,700","00:06:01,730",84,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=356,But there may be techniques that can,pic_cs-410_8_5_300.jpg
cs-410_8_5_85,cs-410,8,5,Topic,"00:06:01,730","00:06:06,735",85,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=361,But in the techniques that we will,pic_cs-410_8_5_360.jpg
cs-410_8_5_86,cs-410,8,5,Topic,"00:06:06,735","00:06:12,340",86,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=366,"techniques, we often need to",pic_cs-410_8_5_360.jpg
cs-410_8_5_87,cs-410,8,5,Topic,"00:06:14,580","00:06:19,860",87,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=374,Now the output would then be the k,pic_cs-410_8_5_360.jpg
cs-410_8_5_88,cs-410,8,5,Topic,"00:06:19,860","00:06:23,210",88,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=379,in order as theta sub,pic_cs-410_8_5_360.jpg
cs-410_8_5_89,cs-410,8,5,Topic,"00:06:24,540","00:06:29,820",89,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=384,Also we want to generate the coverage of,pic_cs-410_8_5_360.jpg
cs-410_8_5_90,cs-410,8,5,Topic,"00:06:29,820","00:06:32,518",90,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=389,this is denoted by pi sub i j.,pic_cs-410_8_5_360.jpg
cs-410_8_5_91,cs-410,8,5,Topic,"00:06:33,562","00:06:38,073",91,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=393,And pi sub ij is the probability,pic_cs-410_8_5_360.jpg
cs-410_8_5_92,cs-410,8,5,Topic,"00:06:38,073","00:06:41,290",92,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=398,covering topic theta sub j.,pic_cs-410_8_5_360.jpg
cs-410_8_5_93,cs-410,8,5,Topic,"00:06:41,290","00:06:45,450",93,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=401,"So obviously for each document, we have",pic_cs-410_8_5_360.jpg
cs-410_8_5_94,cs-410,8,5,Topic,"00:06:45,450","00:06:47,830",94,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=405,"what extent the document covers,",pic_cs-410_8_5_360.jpg
cs-410_8_5_95,cs-410,8,5,Topic,"00:06:48,930","00:06:53,610",95,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=408,And we can assume that these,pic_cs-410_8_5_360.jpg
cs-410_8_5_96,cs-410,8,5,Topic,"00:06:53,610","00:06:57,000",96,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=413,Because a document won't be able to cover,pic_cs-410_8_5_360.jpg
cs-410_8_5_97,cs-410,8,5,Topic,"00:06:57,000","00:07:02,520",97,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=417,other topics outside of the topics,pic_cs-410_8_5_360.jpg
cs-410_8_5_98,cs-410,8,5,Topic,"00:07:02,520","00:07:08,170",98,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=422,"So now, the question is, how do we define",pic_cs-410_8_5_420.jpg
cs-410_8_5_99,cs-410,8,5,Topic,"00:07:08,170","00:07:11,500",99,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=428,Now this problem has not,pic_cs-410_8_5_420.jpg
cs-410_8_5_100,cs-410,8,5,Topic,"00:07:11,500","00:07:15,180",100,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=431,until we define what is exactly theta.,pic_cs-410_8_5_420.jpg
cs-410_8_5_101,cs-410,8,5,Topic,"00:07:16,970","00:07:19,381",101,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=436,"So in the next few lectures,",pic_cs-410_8_5_420.jpg
cs-410_8_5_102,cs-410,8,5,Topic,"00:07:19,381","00:07:24,211",102,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=439,we're going to talk about,pic_cs-410_8_5_420.jpg
cs-410_8_5_103,cs-410,8,5,Topic,"00:07:24,211","00:07:34,211",103,https://www.coursera.org/learn/cs-410/lecture/dmpQ0?t=444,[MUSIC],pic_cs-410_8_5_420.jpg
cs-410_8_6_1,cs-410,8,6,Topic,"00:00:00,000","00:00:02,974",1,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=0,[MUSIC],pic_cs-410_8_6_0.jpg
cs-410_8_6_2,cs-410,8,6,Topic,"00:00:07,749","00:00:11,320",2,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=7,This lecture is about topic mining and,pic_cs-410_8_6_0.jpg
cs-410_8_6_3,cs-410,8,6,Topic,"00:00:12,760","00:00:17,130",3,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=12,We're going to talk about,pic_cs-410_8_6_0.jpg
cs-410_8_6_4,cs-410,8,6,Topic,"00:00:17,130","00:00:20,700",4,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=17,This is a slide that you have,pic_cs-410_8_6_0.jpg
cs-410_8_6_5,cs-410,8,6,Topic,"00:00:20,700","00:00:25,120",5,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=20,where we define the task of,pic_cs-410_8_6_0.jpg
cs-410_8_6_6,cs-410,8,6,Topic,"00:00:25,120","00:00:30,700",6,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=25,"We also raised the question, how do",pic_cs-410_8_6_0.jpg
cs-410_8_6_7,cs-410,8,6,Topic,"00:00:31,780","00:00:36,020",7,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=31,"So in this lecture, we're going to",pic_cs-410_8_6_0.jpg
cs-410_8_6_8,cs-410,8,6,Topic,"00:00:36,020","00:00:37,780",8,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=36,that's our initial idea.,pic_cs-410_8_6_0.jpg
cs-410_8_6_9,cs-410,8,6,Topic,"00:00:37,780","00:00:40,980",9,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=37,Our idea here is defining,pic_cs-410_8_6_0.jpg
cs-410_8_6_10,cs-410,8,6,Topic,"00:00:42,020","00:00:44,200",10,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=42,A term can be a word or a phrase.,pic_cs-410_8_6_0.jpg
cs-410_8_6_11,cs-410,8,6,Topic,"00:00:45,240","00:00:49,500",11,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=45,"And in general,",pic_cs-410_8_6_0.jpg
cs-410_8_6_12,cs-410,8,6,Topic,"00:00:49,500","00:00:54,200",12,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=49,So our first thought is just,pic_cs-410_8_6_0.jpg
cs-410_8_6_13,cs-410,8,6,Topic,"00:00:54,200","00:00:58,820",13,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=54,"For example, we might have terms",pic_cs-410_8_6_0.jpg
cs-410_8_6_14,cs-410,8,6,Topic,"00:00:58,820","00:00:59,440",14,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=58,as you see here.,pic_cs-410_8_6_0.jpg
cs-410_8_6_15,cs-410,8,6,Topic,"00:00:59,440","00:01:02,603",15,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=59,"Now if we define a topic in this way,",pic_cs-410_8_6_0.jpg
cs-410_8_6_16,cs-410,8,6,Topic,"00:01:02,603","00:01:09,200",16,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=62,we can then analyze the coverage,pic_cs-410_8_6_60.jpg
cs-410_8_6_17,cs-410,8,6,Topic,"00:01:09,200","00:01:10,510",17,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=69,"Here for example,",pic_cs-410_8_6_60.jpg
cs-410_8_6_18,cs-410,8,6,Topic,"00:01:10,510","00:01:15,560",18,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=70,we might want to discover to what,pic_cs-410_8_6_60.jpg
cs-410_8_6_19,cs-410,8,6,Topic,"00:01:15,560","00:01:21,260",19,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=75,And we found that 30% of the content,pic_cs-410_8_6_60.jpg
cs-410_8_6_20,cs-410,8,6,Topic,"00:01:21,260","00:01:23,730",20,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=81,"And 12% is about the travel, etc.",pic_cs-410_8_6_60.jpg
cs-410_8_6_21,cs-410,8,6,Topic,"00:01:23,730","00:01:28,880",21,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=83,We might also discover document,pic_cs-410_8_6_60.jpg
cs-410_8_6_22,cs-410,8,6,Topic,"00:01:28,880","00:01:31,240",22,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=88,"So the coverage is zero, etc.",pic_cs-410_8_6_60.jpg
cs-410_8_6_23,cs-410,8,6,Topic,"00:01:32,630","00:01:39,040",23,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=92,"So now, of course,",pic_cs-410_8_6_60.jpg
cs-410_8_6_24,cs-410,8,6,Topic,"00:01:39,040","00:01:42,900",24,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=99,"topic mining and analysis,",pic_cs-410_8_6_60.jpg
cs-410_8_6_25,cs-410,8,6,Topic,"00:01:42,900","00:01:44,960",25,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=102,One is to discover the topics.,pic_cs-410_8_6_60.jpg
cs-410_8_6_26,cs-410,8,6,Topic,"00:01:44,960","00:01:48,110",26,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=104,And the second is to analyze coverage.,pic_cs-410_8_6_60.jpg
cs-410_8_6_27,cs-410,8,6,Topic,"00:01:48,110","00:01:51,550",27,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=108,So let's first think,pic_cs-410_8_6_60.jpg
cs-410_8_6_28,cs-410,8,6,Topic,"00:01:51,550","00:01:55,080",28,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=111,topics if we represent,pic_cs-410_8_6_60.jpg
cs-410_8_6_29,cs-410,8,6,Topic,"00:01:55,080","00:01:59,390",29,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=115,So that means we need to mine k,pic_cs-410_8_6_60.jpg
cs-410_8_6_30,cs-410,8,6,Topic,"00:02:01,050","00:02:04,080",30,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=121,"Now there are, of course,",pic_cs-410_8_6_120.jpg
cs-410_8_6_31,cs-410,8,6,Topic,"00:02:05,670","00:02:08,617",31,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=125,And we're going to talk about,pic_cs-410_8_6_120.jpg
cs-410_8_6_32,cs-410,8,6,Topic,"00:02:08,617","00:02:10,750",32,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=128,which is also likely effective.,pic_cs-410_8_6_120.jpg
cs-410_8_6_33,cs-410,8,6,Topic,"00:02:10,750","00:02:11,641",33,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=130,"So first of all,",pic_cs-410_8_6_120.jpg
cs-410_8_6_34,cs-410,8,6,Topic,"00:02:11,641","00:02:16,655",34,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=131,we're going to parse the text data in,pic_cs-410_8_6_120.jpg
cs-410_8_6_35,cs-410,8,6,Topic,"00:02:16,655","00:02:20,665",35,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=136,Here candidate terms can be words or,pic_cs-410_8_6_120.jpg
cs-410_8_6_36,cs-410,8,6,Topic,"00:02:20,665","00:02:25,475",36,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=140,Let's say the simplest solution is,pic_cs-410_8_6_120.jpg
cs-410_8_6_37,cs-410,8,6,Topic,"00:02:25,475","00:02:29,145",37,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=145,These words then become candidate topics.,pic_cs-410_8_6_120.jpg
cs-410_8_6_38,cs-410,8,6,Topic,"00:02:29,145","00:02:32,790",38,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=149,Then we're going to design a scoring,pic_cs-410_8_6_120.jpg
cs-410_8_6_39,cs-410,8,6,Topic,"00:02:32,790","00:02:33,650",39,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=152,is as a topic.,pic_cs-410_8_6_120.jpg
cs-410_8_6_40,cs-410,8,6,Topic,"00:02:35,460","00:02:37,150",40,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=155,So how can we design such a function?,pic_cs-410_8_6_120.jpg
cs-410_8_6_41,cs-410,8,6,Topic,"00:02:37,150","00:02:40,140",41,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=157,Well there are many things,pic_cs-410_8_6_120.jpg
cs-410_8_6_42,cs-410,8,6,Topic,"00:02:40,140","00:02:44,180",42,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=160,"For example, we can use pure statistics",pic_cs-410_8_6_120.jpg
cs-410_8_6_43,cs-410,8,6,Topic,"00:02:45,550","00:02:48,820",43,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=165,"Intuitively, we would like to",pic_cs-410_8_6_120.jpg
cs-410_8_6_44,cs-410,8,6,Topic,"00:02:48,820","00:02:53,950",44,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=168,meaning terms that can represent,pic_cs-410_8_6_120.jpg
cs-410_8_6_45,cs-410,8,6,Topic,"00:02:53,950","00:02:58,610",45,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=173,So that would mean we want,pic_cs-410_8_6_120.jpg
cs-410_8_6_46,cs-410,8,6,Topic,"00:02:58,610","00:03:03,990",46,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=178,"However, if we simply use the frequency",pic_cs-410_8_6_120.jpg
cs-410_8_6_47,cs-410,8,6,Topic,"00:03:03,990","00:03:07,982",47,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=183,then the highest scored terms,pic_cs-410_8_6_180.jpg
cs-410_8_6_48,cs-410,8,6,Topic,"00:03:07,982","00:03:10,876",48,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=187,"functional terms like the, etc.",pic_cs-410_8_6_180.jpg
cs-410_8_6_49,cs-410,8,6,Topic,"00:03:10,876","00:03:13,510",49,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=190,Those terms occur very frequently English.,pic_cs-410_8_6_180.jpg
cs-410_8_6_50,cs-410,8,6,Topic,"00:03:14,650","00:03:19,340",50,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=194,So we also want to avoid having,pic_cs-410_8_6_180.jpg
cs-410_8_6_51,cs-410,8,6,Topic,"00:03:19,340","00:03:22,150",51,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=199,we want to penalize such words.,pic_cs-410_8_6_180.jpg
cs-410_8_6_52,cs-410,8,6,Topic,"00:03:22,150","00:03:26,480",52,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=202,"But in general, we would like to favor",pic_cs-410_8_6_180.jpg
cs-410_8_6_53,cs-410,8,6,Topic,"00:03:26,480","00:03:28,020",53,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=206,not so frequent.,pic_cs-410_8_6_180.jpg
cs-410_8_6_54,cs-410,8,6,Topic,"00:03:28,020","00:03:34,030",54,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=208,So a particular approach could be based,pic_cs-410_8_6_180.jpg
cs-410_8_6_55,cs-410,8,6,Topic,"00:03:35,140","00:03:37,230",55,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=215,And TF stands for term frequency.,pic_cs-410_8_6_180.jpg
cs-410_8_6_56,cs-410,8,6,Topic,"00:03:37,230","00:03:40,420",56,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=217,IDF stands for inverse document frequency.,pic_cs-410_8_6_180.jpg
cs-410_8_6_57,cs-410,8,6,Topic,"00:03:40,420","00:03:43,310",57,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=220,We talked about some of these,pic_cs-410_8_6_180.jpg
cs-410_8_6_58,cs-410,8,6,Topic,"00:03:43,310","00:03:48,090",58,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=223,ideas in the lectures about,pic_cs-410_8_6_180.jpg
cs-410_8_6_59,cs-410,8,6,Topic,"00:03:48,090","00:03:50,766",59,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=228,"So these are statistical methods,",pic_cs-410_8_6_180.jpg
cs-410_8_6_60,cs-410,8,6,Topic,"00:03:50,766","00:03:56,280",60,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=230,meaning that the function is,pic_cs-410_8_6_180.jpg
cs-410_8_6_61,cs-410,8,6,Topic,"00:03:56,280","00:03:59,080",61,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=236,So the scoring function,pic_cs-410_8_6_180.jpg
cs-410_8_6_62,cs-410,8,6,Topic,"00:03:59,080","00:04:02,890",62,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=239,"It can be applied to any language,",pic_cs-410_8_6_180.jpg
cs-410_8_6_63,cs-410,8,6,Topic,"00:04:02,890","00:04:06,650",63,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=242,But when we apply such a approach,pic_cs-410_8_6_240.jpg
cs-410_8_6_64,cs-410,8,6,Topic,"00:04:06,650","00:04:12,020",64,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=246,we might also be able to leverage,pic_cs-410_8_6_240.jpg
cs-410_8_6_65,cs-410,8,6,Topic,"00:04:12,020","00:04:16,815",65,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=252,"For example, in news we might favor",pic_cs-410_8_6_240.jpg
cs-410_8_6_66,cs-410,8,6,Topic,"00:04:16,815","00:04:21,340",66,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=256,We might want to favor title,pic_cs-410_8_6_240.jpg
cs-410_8_6_67,cs-410,8,6,Topic,"00:04:21,340","00:04:26,100",67,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=261,use the title to describe,pic_cs-410_8_6_240.jpg
cs-410_8_6_68,cs-410,8,6,Topic,"00:04:27,750","00:04:32,480",68,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=267,"If we're dealing with tweets,",pic_cs-410_8_6_240.jpg
cs-410_8_6_69,cs-410,8,6,Topic,"00:04:32,480","00:04:37,430",69,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=272,which are invented to denote topics.,pic_cs-410_8_6_240.jpg
cs-410_8_6_70,cs-410,8,6,Topic,"00:04:37,430","00:04:43,630",70,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=277,"So naturally, hashtags can be good",pic_cs-410_8_6_240.jpg
cs-410_8_6_71,cs-410,8,6,Topic,"00:04:44,780","00:04:50,430",71,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=284,"Anyway, after we have this design",pic_cs-410_8_6_240.jpg
cs-410_8_6_72,cs-410,8,6,Topic,"00:04:50,430","00:04:55,960",72,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=290,the k topical terms by simply picking,pic_cs-410_8_6_240.jpg
cs-410_8_6_73,cs-410,8,6,Topic,"00:04:55,960","00:04:57,240",73,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=295,"Now, of course,",pic_cs-410_8_6_240.jpg
cs-410_8_6_74,cs-410,8,6,Topic,"00:04:57,240","00:05:02,040",74,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=297,we might encounter situation where the,pic_cs-410_8_6_240.jpg
cs-410_8_6_75,cs-410,8,6,Topic,"00:05:02,040","00:05:06,910",75,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=302,"They're semantically similar, or",pic_cs-410_8_6_300.jpg
cs-410_8_6_76,cs-410,8,6,Topic,"00:05:06,910","00:05:08,860",76,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=306,So that's not desirable.,pic_cs-410_8_6_300.jpg
cs-410_8_6_77,cs-410,8,6,Topic,"00:05:08,860","00:05:12,280",77,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=308,So we also want to have coverage over,pic_cs-410_8_6_300.jpg
cs-410_8_6_78,cs-410,8,6,Topic,"00:05:12,280","00:05:15,080",78,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=312,So we would like to remove redundancy.,pic_cs-410_8_6_300.jpg
cs-410_8_6_79,cs-410,8,6,Topic,"00:05:15,080","00:05:19,600",79,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=315,And one way to do that is,pic_cs-410_8_6_300.jpg
cs-410_8_6_80,cs-410,8,6,Topic,"00:05:19,600","00:05:24,450",80,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=319,which is sometimes called a maximal,pic_cs-410_8_6_300.jpg
cs-410_8_6_81,cs-410,8,6,Topic,"00:05:24,450","00:05:29,330",81,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=324,"Basically, the idea is to go down",pic_cs-410_8_6_300.jpg
cs-410_8_6_82,cs-410,8,6,Topic,"00:05:29,330","00:05:34,380",82,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=329,function and gradually take terms,pic_cs-410_8_6_300.jpg
cs-410_8_6_83,cs-410,8,6,Topic,"00:05:34,380","00:05:36,840",83,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=334,"The first term, of course, will be picked.",pic_cs-410_8_6_300.jpg
cs-410_8_6_84,cs-410,8,6,Topic,"00:05:36,840","00:05:40,500",84,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=336,"When we pick the next term, we're",pic_cs-410_8_6_300.jpg
cs-410_8_6_85,cs-410,8,6,Topic,"00:05:40,500","00:05:45,120",85,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=340,been picked and try to avoid,pic_cs-410_8_6_300.jpg
cs-410_8_6_86,cs-410,8,6,Topic,"00:05:45,120","00:05:50,610",86,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=345,So while we are considering,pic_cs-410_8_6_300.jpg
cs-410_8_6_87,cs-410,8,6,Topic,"00:05:50,610","00:05:54,260",87,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=350,we are also considering,pic_cs-410_8_6_300.jpg
cs-410_8_6_88,cs-410,8,6,Topic,"00:05:54,260","00:05:56,970",88,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=354,with respect to the terms,pic_cs-410_8_6_300.jpg
cs-410_8_6_89,cs-410,8,6,Topic,"00:05:58,090","00:06:02,933",89,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=358,"And with some thresholding,",pic_cs-410_8_6_300.jpg
cs-410_8_6_90,cs-410,8,6,Topic,"00:06:02,933","00:06:08,330",90,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=362,the redundancy removal and,pic_cs-410_8_6_360.jpg
cs-410_8_6_91,cs-410,8,6,Topic,"00:06:08,330","00:06:11,990",91,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=368,"Okay, so",pic_cs-410_8_6_360.jpg
cs-410_8_6_92,cs-410,8,6,Topic,"00:06:11,990","00:06:17,550",92,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=371,And those can be regarded as the topics,pic_cs-410_8_6_360.jpg
cs-410_8_6_93,cs-410,8,6,Topic,"00:06:17,550","00:06:21,980",93,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=377,"Next, let's think about how we're going",pic_cs-410_8_6_360.jpg
cs-410_8_6_94,cs-410,8,6,Topic,"00:06:23,430","00:06:26,971",94,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=383,"So looking at this picture,",pic_cs-410_8_6_360.jpg
cs-410_8_6_95,cs-410,8,6,Topic,"00:06:26,971","00:06:28,130",95,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=386,these topics.,pic_cs-410_8_6_360.jpg
cs-410_8_6_96,cs-410,8,6,Topic,"00:06:28,130","00:06:31,190",96,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=388,And now suppose you are give a document.,pic_cs-410_8_6_360.jpg
cs-410_8_6_97,cs-410,8,6,Topic,"00:06:31,190","00:06:35,040",97,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=391,How should we pick out coverage,pic_cs-410_8_6_360.jpg
cs-410_8_6_98,cs-410,8,6,Topic,"00:06:36,660","00:06:42,690",98,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=396,"Well, one approach can be to simply",pic_cs-410_8_6_360.jpg
cs-410_8_6_99,cs-410,8,6,Topic,"00:06:42,690","00:06:46,770",99,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=402,"So for example, sports might have occurred",pic_cs-410_8_6_360.jpg
cs-410_8_6_100,cs-410,8,6,Topic,"00:06:46,770","00:06:49,570",100,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=406,"travel occurred twice, etc.",pic_cs-410_8_6_360.jpg
cs-410_8_6_101,cs-410,8,6,Topic,"00:06:49,570","00:06:54,420",101,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=409,And then we can just normalize these,pic_cs-410_8_6_360.jpg
cs-410_8_6_102,cs-410,8,6,Topic,"00:06:54,420","00:06:56,570",102,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=414,probability for each topic.,pic_cs-410_8_6_360.jpg
cs-410_8_6_103,cs-410,8,6,Topic,"00:06:56,570","00:07:01,780",103,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=416,"So in general, the formula would",pic_cs-410_8_6_360.jpg
cs-410_8_6_104,cs-410,8,6,Topic,"00:07:01,780","00:07:05,240",104,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=421,all the terms that represent the topics.,pic_cs-410_8_6_420.jpg
cs-410_8_6_105,cs-410,8,6,Topic,"00:07:05,240","00:07:10,220",105,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=425,And then simply normalize them so,pic_cs-410_8_6_420.jpg
cs-410_8_6_106,cs-410,8,6,Topic,"00:07:10,220","00:07:13,480",106,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=430,topic in the document would add to one.,pic_cs-410_8_6_420.jpg
cs-410_8_6_107,cs-410,8,6,Topic,"00:07:15,120","00:07:21,200",107,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=435,This forms a distribution of the topics,pic_cs-410_8_6_420.jpg
cs-410_8_6_108,cs-410,8,6,Topic,"00:07:21,200","00:07:26,560",108,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=441,of different topics in the document.,pic_cs-410_8_6_420.jpg
cs-410_8_6_109,cs-410,8,6,Topic,"00:07:26,560","00:07:30,100",109,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=446,"Now, as always,",pic_cs-410_8_6_420.jpg
cs-410_8_6_110,cs-410,8,6,Topic,"00:07:30,100","00:07:34,940",110,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=450,"solving problem, we have to ask",pic_cs-410_8_6_420.jpg
cs-410_8_6_111,cs-410,8,6,Topic,"00:07:34,940","00:07:37,180",111,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=454,Or is this the best way,pic_cs-410_8_6_420.jpg
cs-410_8_6_112,cs-410,8,6,Topic,"00:07:38,690","00:07:41,110",112,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=458,So now let's examine this approach.,pic_cs-410_8_6_420.jpg
cs-410_8_6_113,cs-410,8,6,Topic,"00:07:41,110","00:07:44,940",113,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=461,"In general,",pic_cs-410_8_6_420.jpg
cs-410_8_6_114,cs-410,8,6,Topic,"00:07:46,010","00:07:50,280",114,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=466,by using actual data sets and,pic_cs-410_8_6_420.jpg
cs-410_8_6_115,cs-410,8,6,Topic,"00:07:52,360","00:07:57,340",115,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=472,"Well, in this case let's take",pic_cs-410_8_6_420.jpg
cs-410_8_6_116,cs-410,8,6,Topic,"00:07:57,340","00:08:03,260",116,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=477,And we have a text document that's,pic_cs-410_8_6_420.jpg
cs-410_8_6_117,cs-410,8,6,Topic,"00:08:04,800","00:08:07,700",117,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=484,"So in terms of the content,",pic_cs-410_8_6_480.jpg
cs-410_8_6_118,cs-410,8,6,Topic,"00:08:08,950","00:08:14,600",118,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=488,But if we simply count these,pic_cs-410_8_6_480.jpg
cs-410_8_6_119,cs-410,8,6,Topic,"00:08:14,600","00:08:19,070",119,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=494,we will find that the word sports,pic_cs-410_8_6_480.jpg
cs-410_8_6_120,cs-410,8,6,Topic,"00:08:19,070","00:08:21,420",120,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=499,even though the content,pic_cs-410_8_6_480.jpg
cs-410_8_6_121,cs-410,8,6,Topic,"00:08:22,520","00:08:25,750",121,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=502,So the count of sports is zero.,pic_cs-410_8_6_480.jpg
cs-410_8_6_122,cs-410,8,6,Topic,"00:08:25,750","00:08:31,939",122,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=505,That means the coverage of sports,pic_cs-410_8_6_480.jpg
cs-410_8_6_123,cs-410,8,6,Topic,"00:08:31,939","00:08:36,723",123,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=511,"Now of course,",pic_cs-410_8_6_480.jpg
cs-410_8_6_124,cs-410,8,6,Topic,"00:08:36,723","00:08:40,980",124,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=516,the document and,pic_cs-410_8_6_480.jpg
cs-410_8_6_125,cs-410,8,6,Topic,"00:08:40,980","00:08:42,230",125,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=520,And that's okay.,pic_cs-410_8_6_480.jpg
cs-410_8_6_126,cs-410,8,6,Topic,"00:08:42,230","00:08:47,257",126,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=522,But sports certainly is not okay because,pic_cs-410_8_6_480.jpg
cs-410_8_6_127,cs-410,8,6,Topic,"00:08:47,257","00:08:49,150",127,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=527,So this estimate has problem.,pic_cs-410_8_6_480.jpg
cs-410_8_6_128,cs-410,8,6,Topic,"00:08:50,880","00:08:56,050",128,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=530,"What's worse, the term travel",pic_cs-410_8_6_480.jpg
cs-410_8_6_129,cs-410,8,6,Topic,"00:08:56,050","00:08:59,940",129,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=536,So when we estimate the coverage,pic_cs-410_8_6_480.jpg
cs-410_8_6_130,cs-410,8,6,Topic,"00:08:59,940","00:09:02,140",130,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=539,we have got a non-zero count.,pic_cs-410_8_6_480.jpg
cs-410_8_6_131,cs-410,8,6,Topic,"00:09:02,140","00:09:05,000",131,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=542,So its estimated coverage,pic_cs-410_8_6_540.jpg
cs-410_8_6_132,cs-410,8,6,Topic,"00:09:05,000","00:09:07,770",132,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=545,So this obviously is also not desirable.,pic_cs-410_8_6_540.jpg
cs-410_8_6_133,cs-410,8,6,Topic,"00:09:08,800","00:09:13,910",133,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=548,So this simple example illustrates,pic_cs-410_8_6_540.jpg
cs-410_8_6_134,cs-410,8,6,Topic,"00:09:13,910","00:09:17,704",134,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=553,"First, when we count what",pic_cs-410_8_6_540.jpg
cs-410_8_6_135,cs-410,8,6,Topic,"00:09:17,704","00:09:20,460",135,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=557,we also need to consider related words.,pic_cs-410_8_6_540.jpg
cs-410_8_6_136,cs-410,8,6,Topic,"00:09:20,460","00:09:24,285",136,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=560,We can't simply just count,pic_cs-410_8_6_540.jpg
cs-410_8_6_137,cs-410,8,6,Topic,"00:09:24,285","00:09:26,440",137,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=564,"In this case, it did not occur at all.",pic_cs-410_8_6_540.jpg
cs-410_8_6_138,cs-410,8,6,Topic,"00:09:26,440","00:09:31,340",138,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=566,But there are many related words,pic_cs-410_8_6_540.jpg
cs-410_8_6_139,cs-410,8,6,Topic,"00:09:31,340","00:09:33,860",139,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=571,So we need to count,pic_cs-410_8_6_540.jpg
cs-410_8_6_140,cs-410,8,6,Topic,"00:09:33,860","00:09:38,910",140,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=573,The second problem is that a word,pic_cs-410_8_6_540.jpg
cs-410_8_6_141,cs-410,8,6,Topic,"00:09:38,910","00:09:42,900",141,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=578,So here it probably means,pic_cs-410_8_6_540.jpg
cs-410_8_6_142,cs-410,8,6,Topic,"00:09:42,900","00:09:47,600",142,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=582,we can imagine it might also,pic_cs-410_8_6_540.jpg
cs-410_8_6_143,cs-410,8,6,Topic,"00:09:47,600","00:09:53,120",143,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=587,"So in that case, the star might actually",pic_cs-410_8_6_540.jpg
cs-410_8_6_144,cs-410,8,6,Topic,"00:09:54,210","00:09:56,360",144,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=594,So we need to deal with that as well.,pic_cs-410_8_6_540.jpg
cs-410_8_6_145,cs-410,8,6,Topic,"00:09:56,360","00:10:02,325",145,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=596,"Finally, a main restriction of this",pic_cs-410_8_6_540.jpg
cs-410_8_6_146,cs-410,8,6,Topic,"00:10:02,325","00:10:08,520",146,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=602,"term to describe the topic, so it cannot",pic_cs-410_8_6_600.jpg
cs-410_8_6_147,cs-410,8,6,Topic,"00:10:08,520","00:10:12,040",147,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=608,"For example, a very specialized",pic_cs-410_8_6_600.jpg
cs-410_8_6_148,cs-410,8,6,Topic,"00:10:12,040","00:10:15,210",148,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=612,describe by using just a word or,pic_cs-410_8_6_600.jpg
cs-410_8_6_149,cs-410,8,6,Topic,"00:10:15,210","00:10:17,150",149,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=615,We need to use more words.,pic_cs-410_8_6_600.jpg
cs-410_8_6_150,cs-410,8,6,Topic,"00:10:17,150","00:10:20,760",150,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=617,So this example illustrates,pic_cs-410_8_6_600.jpg
cs-410_8_6_151,cs-410,8,6,Topic,"00:10:20,760","00:10:23,310",151,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=620,this approach of treating a term as topic.,pic_cs-410_8_6_600.jpg
cs-410_8_6_152,cs-410,8,6,Topic,"00:10:23,310","00:10:26,725",152,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=623,"First, it lacks expressive power.",pic_cs-410_8_6_600.jpg
cs-410_8_6_153,cs-410,8,6,Topic,"00:10:26,725","00:10:30,729",153,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=626,Meaning that it can only represent,pic_cs-410_8_6_600.jpg
cs-410_8_6_154,cs-410,8,6,Topic,"00:10:30,729","00:10:36,035",154,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=630,it cannot represent the complicated topics,pic_cs-410_8_6_600.jpg
cs-410_8_6_155,cs-410,8,6,Topic,"00:10:37,055","00:10:40,660",155,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=637,"Second, it's incomplete",pic_cs-410_8_6_600.jpg
cs-410_8_6_156,cs-410,8,6,Topic,"00:10:40,660","00:10:44,930",156,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=640,meaning that the topic itself,pic_cs-410_8_6_600.jpg
cs-410_8_6_157,cs-410,8,6,Topic,"00:10:44,930","00:10:48,820",157,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=644,It does not suggest what other,pic_cs-410_8_6_600.jpg
cs-410_8_6_158,cs-410,8,6,Topic,"00:10:48,820","00:10:52,370",158,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=648,"Even if we're talking about sports,",pic_cs-410_8_6_600.jpg
cs-410_8_6_159,cs-410,8,6,Topic,"00:10:52,370","00:10:57,060",159,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=652,So it does not allow us to easily,pic_cs-410_8_6_600.jpg
cs-410_8_6_160,cs-410,8,6,Topic,"00:10:57,060","00:10:59,200",160,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=657,conversion to coverage of this topic.,pic_cs-410_8_6_600.jpg
cs-410_8_6_161,cs-410,8,6,Topic,"00:10:59,200","00:11:02,410",161,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=659,"Finally, there is this problem",pic_cs-410_8_6_600.jpg
cs-410_8_6_162,cs-410,8,6,Topic,"00:11:02,410","00:11:05,862",162,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=662,A topical term or,pic_cs-410_8_6_660.jpg
cs-410_8_6_163,cs-410,8,6,Topic,"00:11:05,862","00:11:08,540",163,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=665,"For example,",pic_cs-410_8_6_660.jpg
cs-410_8_6_164,cs-410,8,6,Topic,"00:11:10,570","00:11:14,125",164,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=670,"So in the next lecture,",pic_cs-410_8_6_660.jpg
cs-410_8_6_165,cs-410,8,6,Topic,"00:11:14,125","00:11:18,806",165,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=674,about how to solve,pic_cs-410_8_6_660.jpg
cs-410_8_6_166,cs-410,8,6,Topic,"00:11:18,806","00:11:28,806",166,https://www.coursera.org/learn/cs-410/lecture/A1bUb?t=678,[MUSIC],pic_cs-410_8_6_660.jpg
cs-410_8_7_1,cs-410,8,7,Topic,"00:00:06,750","00:00:12,040",1,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=6,This lecture is about Probabilistic Topic,pic_cs-410_8_7_0.jpg
cs-410_8_7_2,cs-410,8,7,Topic,"00:00:13,350","00:00:14,110",2,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=13,"In this lecture,",pic_cs-410_8_7_0.jpg
cs-410_8_7_3,cs-410,8,7,Topic,"00:00:14,110","00:00:16,909",3,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=14,we're going to continue talking,pic_cs-410_8_7_0.jpg
cs-410_8_7_4,cs-410,8,7,Topic,"00:00:18,190","00:00:20,490",4,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=18,We're going to introduce,pic_cs-410_8_7_0.jpg
cs-410_8_7_5,cs-410,8,7,Topic,"00:00:22,410","00:00:26,140",5,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=22,So this is a slide that,pic_cs-410_8_7_0.jpg
cs-410_8_7_6,cs-410,8,7,Topic,"00:00:26,140","00:00:30,640",6,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=26,where we discussed the problems,pic_cs-410_8_7_0.jpg
cs-410_8_7_7,cs-410,8,7,Topic,"00:00:30,640","00:00:35,370",7,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=30,"So, to solve these problems",pic_cs-410_8_7_0.jpg
cs-410_8_7_8,cs-410,8,7,Topic,"00:00:35,370","00:00:37,950",8,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=35,more words to describe the topic.,pic_cs-410_8_7_0.jpg
cs-410_8_7_9,cs-410,8,7,Topic,"00:00:37,950","00:00:43,110",9,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=37,And this will address the problem,pic_cs-410_8_7_0.jpg
cs-410_8_7_10,cs-410,8,7,Topic,"00:00:43,110","00:00:45,040",10,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=43,When we have more words that we,pic_cs-410_8_7_0.jpg
cs-410_8_7_11,cs-410,8,7,Topic,"00:00:45,040","00:00:49,880",11,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=45,that we can describe complicated topics.,pic_cs-410_8_7_0.jpg
cs-410_8_7_12,cs-410,8,7,Topic,"00:00:49,880","00:00:54,030",12,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=49,To address the second problem we,pic_cs-410_8_7_0.jpg
cs-410_8_7_13,cs-410,8,7,Topic,"00:00:54,030","00:00:59,140",13,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=54,This is what allows you to distinguish,pic_cs-410_8_7_0.jpg
cs-410_8_7_14,cs-410,8,7,Topic,"00:00:59,140","00:01:04,600",14,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=59,to introduce semantically,pic_cs-410_8_7_0.jpg
cs-410_8_7_15,cs-410,8,7,Topic,"00:01:04,600","00:01:09,240",15,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=64,"Finally, to solve the problem of",pic_cs-410_8_7_60.jpg
cs-410_8_7_16,cs-410,8,7,Topic,"00:01:09,240","00:01:14,700",16,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=69,"ambiguous word, so",pic_cs-410_8_7_60.jpg
cs-410_8_7_17,cs-410,8,7,Topic,"00:01:15,720","00:01:21,060",17,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=75,It turns out that all these can be done,pic_cs-410_8_7_60.jpg
cs-410_8_7_18,cs-410,8,7,Topic,"00:01:21,060","00:01:25,520",18,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=81,And that's why we're going to spend a lot,pic_cs-410_8_7_60.jpg
cs-410_8_7_19,cs-410,8,7,Topic,"00:01:25,520","00:01:28,130",19,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=85,"So the basic idea here is that,",pic_cs-410_8_7_60.jpg
cs-410_8_7_20,cs-410,8,7,Topic,"00:01:28,130","00:01:32,600",20,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=88,improve the replantation of,pic_cs-410_8_7_60.jpg
cs-410_8_7_21,cs-410,8,7,Topic,"00:01:32,600","00:01:35,650",21,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=92,So what you see now is,pic_cs-410_8_7_60.jpg
cs-410_8_7_22,cs-410,8,7,Topic,"00:01:35,650","00:01:40,730",22,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=95,"Where we replanted each topic, it was just",pic_cs-410_8_7_60.jpg
cs-410_8_7_23,cs-410,8,7,Topic,"00:01:40,730","00:01:45,240",23,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=100,But now we're going to use a word,pic_cs-410_8_7_60.jpg
cs-410_8_7_24,cs-410,8,7,Topic,"00:01:45,240","00:01:47,110",24,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=105,So here you see that for sports.,pic_cs-410_8_7_60.jpg
cs-410_8_7_25,cs-410,8,7,Topic,"00:01:47,110","00:01:50,220",25,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=107,We're going to use,pic_cs-410_8_7_60.jpg
cs-410_8_7_26,cs-410,8,7,Topic,"00:01:50,220","00:01:53,160",26,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=110,theoretical speaking all,pic_cs-410_8_7_60.jpg
cs-410_8_7_27,cs-410,8,7,Topic,"00:01:54,650","00:01:59,150",27,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=114,"So for example, the high",pic_cs-410_8_7_60.jpg
cs-410_8_7_28,cs-410,8,7,Topic,"00:01:59,150","00:02:03,880",28,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=119,"game, basketball,",pic_cs-410_8_7_60.jpg
cs-410_8_7_29,cs-410,8,7,Topic,"00:02:03,880","00:02:06,100",29,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=123,These are sports related terms.,pic_cs-410_8_7_120.jpg
cs-410_8_7_30,cs-410,8,7,Topic,"00:02:06,100","00:02:10,150",30,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=126,And of course it would also give,pic_cs-410_8_7_120.jpg
cs-410_8_7_31,cs-410,8,7,Topic,"00:02:10,150","00:02:15,430",31,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=130,like Trouble which might be,pic_cs-410_8_7_120.jpg
cs-410_8_7_32,cs-410,8,7,Topic,"00:02:15,430","00:02:17,420",32,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=135,not so much related to topic.,pic_cs-410_8_7_120.jpg
cs-410_8_7_33,cs-410,8,7,Topic,"00:02:18,900","00:02:23,030",33,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=138,In general we can imagine a non,pic_cs-410_8_7_120.jpg
cs-410_8_7_34,cs-410,8,7,Topic,"00:02:23,030","00:02:27,890",34,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=143,And some words that are not read and,pic_cs-410_8_7_120.jpg
cs-410_8_7_35,cs-410,8,7,Topic,"00:02:27,890","00:02:29,830",35,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=147,And these probabilities will sum to one.,pic_cs-410_8_7_120.jpg
cs-410_8_7_36,cs-410,8,7,Topic,"00:02:31,780","00:02:34,500",36,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=151,So that it forms a distribution,pic_cs-410_8_7_120.jpg
cs-410_8_7_37,cs-410,8,7,Topic,"00:02:36,650","00:02:41,440",37,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=156,"Now intuitively, this distribution",pic_cs-410_8_7_120.jpg
cs-410_8_7_38,cs-410,8,7,Topic,"00:02:41,440","00:02:46,780",38,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=161,"words from the distribution, we tended",pic_cs-410_8_7_120.jpg
cs-410_8_7_39,cs-410,8,7,Topic,"00:02:48,470","00:02:53,236",39,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=168,"You can also see, as a very special case,",pic_cs-410_8_7_120.jpg
cs-410_8_7_40,cs-410,8,7,Topic,"00:02:53,236","00:02:57,387",40,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=173,is concentrated in entirely on,pic_cs-410_8_7_120.jpg
cs-410_8_7_41,cs-410,8,7,Topic,"00:02:57,387","00:03:01,670",41,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=177,And this basically degenerates,pic_cs-410_8_7_120.jpg
cs-410_8_7_42,cs-410,8,7,Topic,"00:03:01,670","00:03:03,270",42,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=181,of a topic was just one word.,pic_cs-410_8_7_180.jpg
cs-410_8_7_43,cs-410,8,7,Topic,"00:03:04,640","00:03:10,420",43,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=184,"But as a distribution,",pic_cs-410_8_7_180.jpg
cs-410_8_7_44,cs-410,8,7,Topic,"00:03:10,420","00:03:13,980",44,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=190,"in general,",pic_cs-410_8_7_180.jpg
cs-410_8_7_45,cs-410,8,7,Topic,"00:03:13,980","00:03:17,970",45,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=193,can model several differences,pic_cs-410_8_7_180.jpg
cs-410_8_7_46,cs-410,8,7,Topic,"00:03:17,970","00:03:24,500",46,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=197,Similarly we can model Travel and Science,pic_cs-410_8_7_180.jpg
cs-410_8_7_47,cs-410,8,7,Topic,"00:03:24,500","00:03:30,120",47,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=204,In the distribution for Travel we see top,pic_cs-410_8_7_180.jpg
cs-410_8_7_48,cs-410,8,7,Topic,"00:03:31,670","00:03:36,110",48,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=211,"Whereas in Science we see scientist,",pic_cs-410_8_7_180.jpg
cs-410_8_7_49,cs-410,8,7,Topic,"00:03:36,110","00:03:39,820",49,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=216,"genomics, and, you know,",pic_cs-410_8_7_180.jpg
cs-410_8_7_50,cs-410,8,7,Topic,"00:03:39,820","00:03:43,260",50,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=219,Now that doesn't mean sports related terms,pic_cs-410_8_7_180.jpg
cs-410_8_7_51,cs-410,8,7,Topic,"00:03:43,260","00:03:46,330",51,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=223,will necessarily have zero,pic_cs-410_8_7_180.jpg
cs-410_8_7_52,cs-410,8,7,Topic,"00:03:46,330","00:03:51,860",52,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=226,In general we can imagine all of these,pic_cs-410_8_7_180.jpg
cs-410_8_7_53,cs-410,8,7,Topic,"00:03:51,860","00:03:55,250",53,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=231,It's just that for a particular,pic_cs-410_8_7_180.jpg
cs-410_8_7_54,cs-410,8,7,Topic,"00:03:55,250","00:03:56,620",54,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=235,very small probabilities.,pic_cs-410_8_7_180.jpg
cs-410_8_7_55,cs-410,8,7,Topic,"00:03:58,200","00:04:02,770",55,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=238,Now you can also see there are some,pic_cs-410_8_7_180.jpg
cs-410_8_7_56,cs-410,8,7,Topic,"00:04:02,770","00:04:07,600",56,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=242,When I say shared it just means even,pic_cs-410_8_7_240.jpg
cs-410_8_7_57,cs-410,8,7,Topic,"00:04:07,600","00:04:10,990",57,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=247,you can still see one word,pic_cs-410_8_7_240.jpg
cs-410_8_7_58,cs-410,8,7,Topic,"00:04:10,990","00:04:13,140",58,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=250,In this case I mark them in black.,pic_cs-410_8_7_240.jpg
cs-410_8_7_59,cs-410,8,7,Topic,"00:04:13,140","00:04:17,110",59,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=253,"So you can see travel, for example,",pic_cs-410_8_7_240.jpg
cs-410_8_7_60,cs-410,8,7,Topic,"00:04:17,110","00:04:19,420",60,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=257,with different probabilities.,pic_cs-410_8_7_240.jpg
cs-410_8_7_61,cs-410,8,7,Topic,"00:04:19,420","00:04:23,237",61,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=259,It has the highest probability for,pic_cs-410_8_7_240.jpg
cs-410_8_7_62,cs-410,8,7,Topic,"00:04:23,237","00:04:29,050",62,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=263,But with much smaller probabilities for,pic_cs-410_8_7_240.jpg
cs-410_8_7_63,cs-410,8,7,Topic,"00:04:29,050","00:04:32,450",63,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=269,"And similarly, you can see a Star",pic_cs-410_8_7_240.jpg
cs-410_8_7_64,cs-410,8,7,Topic,"00:04:32,450","00:04:35,420",64,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=272,Science with reasonably,pic_cs-410_8_7_240.jpg
cs-410_8_7_65,cs-410,8,7,Topic,"00:04:35,420","00:04:39,690",65,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=275,Because they might be actually,pic_cs-410_8_7_240.jpg
cs-410_8_7_66,cs-410,8,7,Topic,"00:04:39,690","00:04:43,420",66,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=279,So with this replantation it addresses the,pic_cs-410_8_7_240.jpg
cs-410_8_7_67,cs-410,8,7,Topic,"00:04:43,420","00:04:46,750",67,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=283,"First, it now uses multiple",pic_cs-410_8_7_240.jpg
cs-410_8_7_68,cs-410,8,7,Topic,"00:04:46,750","00:04:50,700",68,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=286,So it allows us to describe,pic_cs-410_8_7_240.jpg
cs-410_8_7_69,cs-410,8,7,Topic,"00:04:50,700","00:04:53,400",69,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=290,"Second, it assigns weights to terms.",pic_cs-410_8_7_240.jpg
cs-410_8_7_70,cs-410,8,7,Topic,"00:04:53,400","00:04:57,060",70,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=293,So now we can model several,pic_cs-410_8_7_240.jpg
cs-410_8_7_71,cs-410,8,7,Topic,"00:04:57,060","00:05:02,390",71,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=297,And you can bring in related,pic_cs-410_8_7_240.jpg
cs-410_8_7_72,cs-410,8,7,Topic,"00:05:02,390","00:05:07,930",72,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=302,"Third, because we have probabilities for",pic_cs-410_8_7_300.jpg
cs-410_8_7_73,cs-410,8,7,Topic,"00:05:07,930","00:05:12,210",73,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=307,we can disintegrate the sense of word.,pic_cs-410_8_7_300.jpg
cs-410_8_7_74,cs-410,8,7,Topic,"00:05:12,210","00:05:16,930",74,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=312,In the text to decode,pic_cs-410_8_7_300.jpg
cs-410_8_7_75,cs-410,8,7,Topic,"00:05:16,930","00:05:22,480",75,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=316,to address all these three problems with,pic_cs-410_8_7_300.jpg
cs-410_8_7_76,cs-410,8,7,Topic,"00:05:22,480","00:05:27,650",76,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=322,So now of course our problem definition,pic_cs-410_8_7_300.jpg
cs-410_8_7_77,cs-410,8,7,Topic,"00:05:27,650","00:05:32,090",77,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=327,The slight is very similar to what,pic_cs-410_8_7_300.jpg
cs-410_8_7_78,cs-410,8,7,Topic,"00:05:32,090","00:05:34,920",78,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=332,added refinement for what our topic is.,pic_cs-410_8_7_300.jpg
cs-410_8_7_79,cs-410,8,7,Topic,"00:05:34,920","00:05:41,180",79,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=334,"Now each topic is word distribution,",pic_cs-410_8_7_300.jpg
cs-410_8_7_80,cs-410,8,7,Topic,"00:05:41,180","00:05:45,460",80,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=341,that all the probabilities should sum to,pic_cs-410_8_7_300.jpg
cs-410_8_7_81,cs-410,8,7,Topic,"00:05:45,460","00:05:47,640",81,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=345,So you see a constraint here.,pic_cs-410_8_7_300.jpg
cs-410_8_7_82,cs-410,8,7,Topic,"00:05:47,640","00:05:53,060",82,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=347,And we still have another constraint,pic_cs-410_8_7_300.jpg
cs-410_8_7_83,cs-410,8,7,Topic,"00:05:53,060","00:05:58,180",83,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=353,So all the Pi sub ij's must sum to one for,pic_cs-410_8_7_300.jpg
cs-410_8_7_84,cs-410,8,7,Topic,"00:05:59,620","00:06:01,250",84,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=359,So how do we solve this problem?,pic_cs-410_8_7_300.jpg
cs-410_8_7_85,cs-410,8,7,Topic,"00:06:01,250","00:06:05,470",85,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=361,"Well, let's look at this problem",pic_cs-410_8_7_360.jpg
cs-410_8_7_86,cs-410,8,7,Topic,"00:06:05,470","00:06:07,560",86,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=365,So we clearly specify it's input and,pic_cs-410_8_7_360.jpg
cs-410_8_7_87,cs-410,8,7,Topic,"00:06:07,560","00:06:11,190",87,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=367,output and,pic_cs-410_8_7_360.jpg
cs-410_8_7_88,cs-410,8,7,Topic,"00:06:11,190","00:06:12,920",88,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=371,Input of course is our text data.,pic_cs-410_8_7_360.jpg
cs-410_8_7_89,cs-410,8,7,Topic,"00:06:12,920","00:06:18,620",89,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=372,C is our collection but we also generally,pic_cs-410_8_7_360.jpg
cs-410_8_7_90,cs-410,8,7,Topic,"00:06:18,620","00:06:22,940",90,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=378,Or we hypothesize a number and,pic_cs-410_8_7_360.jpg
cs-410_8_7_91,cs-410,8,7,Topic,"00:06:22,940","00:06:27,820",91,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=382,even though we don't know the exact,pic_cs-410_8_7_360.jpg
cs-410_8_7_92,cs-410,8,7,Topic,"00:06:27,820","00:06:32,960",92,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=387,And V is the vocabulary that has,pic_cs-410_8_7_360.jpg
cs-410_8_7_93,cs-410,8,7,Topic,"00:06:32,960","00:06:38,880",93,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=392,units would be treated as,pic_cs-410_8_7_360.jpg
cs-410_8_7_94,cs-410,8,7,Topic,"00:06:38,880","00:06:44,780",94,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=398,In most cases we'll use words,pic_cs-410_8_7_360.jpg
cs-410_8_7_95,cs-410,8,7,Topic,"00:06:44,780","00:06:46,429",95,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=404,And that means each word is a unique.,pic_cs-410_8_7_360.jpg
cs-410_8_7_96,cs-410,8,7,Topic,"00:06:47,610","00:06:53,560",96,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=407,Now the output would consist of as first,pic_cs-410_8_7_360.jpg
cs-410_8_7_97,cs-410,8,7,Topic,"00:06:53,560","00:06:55,280",97,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=413,Each theta I is a word distribution.,pic_cs-410_8_7_360.jpg
cs-410_8_7_98,cs-410,8,7,Topic,"00:06:56,430","00:07:02,860",98,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=416,And we also want to know the coverage,pic_cs-410_8_7_360.jpg
cs-410_8_7_99,cs-410,8,7,Topic,"00:07:02,860","00:07:03,520",99,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=422,So that's.,pic_cs-410_8_7_420.jpg
cs-410_8_7_100,cs-410,8,7,Topic,"00:07:03,520","00:07:06,250",100,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=423,That the same pi ijs,pic_cs-410_8_7_420.jpg
cs-410_8_7_101,cs-410,8,7,Topic,"00:07:07,470","00:07:13,460",101,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=427,So given a set of text data we would,pic_cs-410_8_7_420.jpg
cs-410_8_7_102,cs-410,8,7,Topic,"00:07:13,460","00:07:16,980",102,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=433,all these coverages as you,pic_cs-410_8_7_420.jpg
cs-410_8_7_103,cs-410,8,7,Topic,"00:07:18,130","00:07:21,520",103,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=438,Now of course there may be many,pic_cs-410_8_7_420.jpg
cs-410_8_7_104,cs-410,8,7,Topic,"00:07:21,520","00:07:24,670",104,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=441,"In theory, you can write the [INAUDIBLE]",pic_cs-410_8_7_420.jpg
cs-410_8_7_105,cs-410,8,7,Topic,"00:07:24,670","00:07:27,050",105,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=444,but here we're going to introduce,pic_cs-410_8_7_420.jpg
cs-410_8_7_106,cs-410,8,7,Topic,"00:07:27,050","00:07:32,200",106,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=447,a general way of solving this,pic_cs-410_8_7_420.jpg
cs-410_8_7_107,cs-410,8,7,Topic,"00:07:32,200","00:07:35,770",107,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=452,"And this is, in fact,",pic_cs-410_8_7_420.jpg
cs-410_8_7_108,cs-410,8,7,Topic,"00:07:35,770","00:07:41,390",108,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=455,it's a principle way of using statistical,pic_cs-410_8_7_420.jpg
cs-410_8_7_109,cs-410,8,7,Topic,"00:07:41,390","00:07:46,190",109,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=461,And here I dimmed the picture,pic_cs-410_8_7_420.jpg
cs-410_8_7_110,cs-410,8,7,Topic,"00:07:46,190","00:07:49,470",110,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=466,in order to show the generation process.,pic_cs-410_8_7_420.jpg
cs-410_8_7_111,cs-410,8,7,Topic,"00:07:49,470","00:07:55,960",111,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=469,So the idea of this approach is actually,pic_cs-410_8_7_420.jpg
cs-410_8_7_112,cs-410,8,7,Topic,"00:07:55,960","00:08:02,070",112,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=475,So we design a probabilistic model,pic_cs-410_8_7_420.jpg
cs-410_8_7_113,cs-410,8,7,Topic,"00:08:02,070","00:08:04,180",113,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=482,"Of course,",pic_cs-410_8_7_480.jpg
cs-410_8_7_114,cs-410,8,7,Topic,"00:08:04,180","00:08:08,060",114,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=484,The actual data aren't,pic_cs-410_8_7_480.jpg
cs-410_8_7_115,cs-410,8,7,Topic,"00:08:08,060","00:08:11,930",115,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=488,So that gave us a probability,pic_cs-410_8_7_480.jpg
cs-410_8_7_116,cs-410,8,7,Topic,"00:08:11,930","00:08:13,980",116,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=491,that you are seeing on this slide.,pic_cs-410_8_7_480.jpg
cs-410_8_7_117,cs-410,8,7,Topic,"00:08:13,980","00:08:18,840",117,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=493,Given a particular model and,pic_cs-410_8_7_480.jpg
cs-410_8_7_118,cs-410,8,7,Topic,"00:08:18,840","00:08:22,040",118,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=498,So this template of actually consists of,pic_cs-410_8_7_480.jpg
cs-410_8_7_119,cs-410,8,7,Topic,"00:08:22,040","00:08:24,380",119,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=502,all the parameters that,pic_cs-410_8_7_480.jpg
cs-410_8_7_120,cs-410,8,7,Topic,"00:08:24,380","00:08:27,780",120,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=504,And these parameters in general,pic_cs-410_8_7_480.jpg
cs-410_8_7_121,cs-410,8,7,Topic,"00:08:27,780","00:08:29,370",121,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=507,the probability risk model.,pic_cs-410_8_7_480.jpg
cs-410_8_7_122,cs-410,8,7,Topic,"00:08:29,370","00:08:32,530",122,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=509,Meaning that if you set these,pic_cs-410_8_7_480.jpg
cs-410_8_7_123,cs-410,8,7,Topic,"00:08:32,530","00:08:36,820",123,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=512,it will give some data points,pic_cs-410_8_7_480.jpg
cs-410_8_7_124,cs-410,8,7,Topic,"00:08:36,820","00:08:39,910",124,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=516,"Now in this case of course,",pic_cs-410_8_7_480.jpg
cs-410_8_7_125,cs-410,8,7,Topic,"00:08:39,910","00:08:44,100",125,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=519,more precisely topic mining problem,pic_cs-410_8_7_480.jpg
cs-410_8_7_126,cs-410,8,7,Topic,"00:08:44,100","00:08:49,450",126,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=524,First of all we have theta i's which,pic_cs-410_8_7_480.jpg
cs-410_8_7_127,cs-410,8,7,Topic,"00:08:49,450","00:08:52,070",127,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=529,a set of pis for each document.,pic_cs-410_8_7_480.jpg
cs-410_8_7_128,cs-410,8,7,Topic,"00:08:52,070","00:08:58,980",128,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=532,"And since we have n documents, so we have",pic_cs-410_8_7_480.jpg
cs-410_8_7_129,cs-410,8,7,Topic,"00:08:58,980","00:09:01,430",129,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=538,The pi values will sum to one.,pic_cs-410_8_7_480.jpg
cs-410_8_7_130,cs-410,8,7,Topic,"00:09:01,430","00:09:06,370",130,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=541,So this is to say that we,pic_cs-410_8_7_540.jpg
cs-410_8_7_131,cs-410,8,7,Topic,"00:09:06,370","00:09:10,640",131,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=546,have these word distributions and,pic_cs-410_8_7_540.jpg
cs-410_8_7_132,cs-410,8,7,Topic,"00:09:10,640","00:09:18,010",132,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=550,And then we can see how we can generate,pic_cs-410_8_7_540.jpg
cs-410_8_7_133,cs-410,8,7,Topic,"00:09:18,010","00:09:21,950",133,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=558,So how do we model the data in this way?,pic_cs-410_8_7_540.jpg
cs-410_8_7_134,cs-410,8,7,Topic,"00:09:21,950","00:09:25,280",134,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=561,And we assume that the data,pic_cs-410_8_7_540.jpg
cs-410_8_7_135,cs-410,8,7,Topic,"00:09:25,280","00:09:29,530",135,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=565,drawn from such a model that,pic_cs-410_8_7_540.jpg
cs-410_8_7_136,cs-410,8,7,Topic,"00:09:29,530","00:09:31,290",136,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=569,Now one interesting question here is to,pic_cs-410_8_7_540.jpg
cs-410_8_7_137,cs-410,8,7,Topic,"00:09:32,320","00:09:35,080",137,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=572,think about how many,pic_cs-410_8_7_540.jpg
cs-410_8_7_138,cs-410,8,7,Topic,"00:09:35,080","00:09:41,360",138,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=575,Now obviously we can already see,pic_cs-410_8_7_540.jpg
cs-410_8_7_139,cs-410,8,7,Topic,"00:09:41,360","00:09:42,140",139,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=581,For pi's.,pic_cs-410_8_7_540.jpg
cs-410_8_7_140,cs-410,8,7,Topic,"00:09:42,140","00:09:44,530",140,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=582,We also see k theta i's.,pic_cs-410_8_7_540.jpg
cs-410_8_7_141,cs-410,8,7,Topic,"00:09:44,530","00:09:49,110",141,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=584,But each theta i is actually a set,pic_cs-410_8_7_540.jpg
cs-410_8_7_142,cs-410,8,7,Topic,"00:09:49,110","00:09:51,580",142,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=589,It's a distribution of words.,pic_cs-410_8_7_540.jpg
cs-410_8_7_143,cs-410,8,7,Topic,"00:09:51,580","00:09:54,000",143,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=591,So I leave this as an exercise for,pic_cs-410_8_7_540.jpg
cs-410_8_7_144,cs-410,8,7,Topic,"00:09:54,000","00:09:59,980",144,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=594,you to figure out exactly how,pic_cs-410_8_7_540.jpg
cs-410_8_7_145,cs-410,8,7,Topic,"00:09:59,980","00:10:04,690",145,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=599,Now once we set up the model then,pic_cs-410_8_7_540.jpg
cs-410_8_7_146,cs-410,8,7,Topic,"00:10:04,690","00:10:07,900",146,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=604,Meaning that we can,pic_cs-410_8_7_600.jpg
cs-410_8_7_147,cs-410,8,7,Topic,"00:10:07,900","00:10:11,010",147,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=607,infer the parameters based on the data.,pic_cs-410_8_7_600.jpg
cs-410_8_7_148,cs-410,8,7,Topic,"00:10:11,010","00:10:14,930",148,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=611,In other words we would like to,pic_cs-410_8_7_600.jpg
cs-410_8_7_149,cs-410,8,7,Topic,"00:10:14,930","00:10:20,330",149,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=614,Until we give our data set,pic_cs-410_8_7_600.jpg
cs-410_8_7_150,cs-410,8,7,Topic,"00:10:20,330","00:10:22,880",150,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=620,"I just said,",pic_cs-410_8_7_600.jpg
cs-410_8_7_151,cs-410,8,7,Topic,"00:10:22,880","00:10:27,090",151,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=622,some data points will have higher,pic_cs-410_8_7_600.jpg
cs-410_8_7_152,cs-410,8,7,Topic,"00:10:27,090","00:10:28,620",152,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=627,"What we're interested in, here,",pic_cs-410_8_7_600.jpg
cs-410_8_7_153,cs-410,8,7,Topic,"00:10:28,620","00:10:33,420",153,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=628,is what parameter values will give,pic_cs-410_8_7_600.jpg
cs-410_8_7_154,cs-410,8,7,Topic,"00:10:33,420","00:10:37,620",154,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=633,So I also illustrate the problem,pic_cs-410_8_7_600.jpg
cs-410_8_7_155,cs-410,8,7,Topic,"00:10:37,620","00:10:41,720",155,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=637,"On the X axis I just illustrate lambda,",pic_cs-410_8_7_600.jpg
cs-410_8_7_156,cs-410,8,7,Topic,"00:10:41,720","00:10:44,260",156,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=641,as a one dimensional variable.,pic_cs-410_8_7_600.jpg
cs-410_8_7_157,cs-410,8,7,Topic,"00:10:44,260","00:10:49,360",157,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=644,"It's oversimplification, obviously,",pic_cs-410_8_7_600.jpg
cs-410_8_7_158,cs-410,8,7,Topic,"00:10:49,360","00:10:53,370",158,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=649,And the Y axis shows the probability,pic_cs-410_8_7_600.jpg
cs-410_8_7_159,cs-410,8,7,Topic,"00:10:53,370","00:10:57,780",159,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=653,This probability obviously depends,pic_cs-410_8_7_600.jpg
cs-410_8_7_160,cs-410,8,7,Topic,"00:10:57,780","00:11:01,480",160,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=657,So that's why it varies as you,pic_cs-410_8_7_600.jpg
cs-410_8_7_161,cs-410,8,7,Topic,"00:11:01,480","00:11:04,830",161,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=661,What we're interested here,pic_cs-410_8_7_660.jpg
cs-410_8_7_162,cs-410,8,7,Topic,"00:11:05,880","00:11:09,259",162,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=665,That would maximize the probability,pic_cs-410_8_7_660.jpg
cs-410_8_7_163,cs-410,8,7,Topic,"00:11:10,440","00:11:15,470",163,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=670,"So this would be, then,",pic_cs-410_8_7_660.jpg
cs-410_8_7_164,cs-410,8,7,Topic,"00:11:15,470","00:11:17,040",164,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=675,"And these parameters,",pic_cs-410_8_7_660.jpg
cs-410_8_7_165,cs-410,8,7,Topic,"00:11:17,040","00:11:21,720",165,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=677,note that are precisely what we,pic_cs-410_8_7_660.jpg
cs-410_8_7_166,cs-410,8,7,Topic,"00:11:21,720","00:11:25,405",166,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=681,So we'd treat these parameters,pic_cs-410_8_7_660.jpg
cs-410_8_7_167,cs-410,8,7,Topic,"00:11:25,405","00:11:28,046",167,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=685,the output of the data mining algorithm.,pic_cs-410_8_7_660.jpg
cs-410_8_7_168,cs-410,8,7,Topic,"00:11:28,046","00:11:32,966",168,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=688,So this is the general idea of using,pic_cs-410_8_7_660.jpg
cs-410_8_7_169,cs-410,8,7,Topic,"00:11:32,966","00:11:38,231",169,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=692,a generative model for text mining.,pic_cs-410_8_7_660.jpg
cs-410_8_7_170,cs-410,8,7,Topic,"00:11:38,231","00:11:42,762",170,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=698,"First, we design a model with",pic_cs-410_8_7_660.jpg
cs-410_8_7_171,cs-410,8,7,Topic,"00:11:42,762","00:11:44,804",171,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=702,the data as well as we can.,pic_cs-410_8_7_660.jpg
cs-410_8_7_172,cs-410,8,7,Topic,"00:11:44,804","00:11:47,207",172,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=704,"After we have fit the data,",pic_cs-410_8_7_660.jpg
cs-410_8_7_173,cs-410,8,7,Topic,"00:11:47,207","00:11:48,827",173,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=707,We will use the specific,pic_cs-410_8_7_660.jpg
cs-410_8_7_174,cs-410,8,7,Topic,"00:11:48,827","00:11:50,910",174,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=708,those would be the output,pic_cs-410_8_7_660.jpg
cs-410_8_7_175,cs-410,8,7,Topic,"00:11:50,910","00:11:55,880",175,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=710,And we'll treat those as actually,pic_cs-410_8_7_660.jpg
cs-410_8_7_176,cs-410,8,7,Topic,"00:11:55,880","00:11:59,460",176,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=715,By varying the model of course we,pic_cs-410_8_7_660.jpg
cs-410_8_7_177,cs-410,8,7,Topic,"00:11:59,460","00:12:03,840",177,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=719,"So to summarize, we introduced",pic_cs-410_8_7_660.jpg
cs-410_8_7_178,cs-410,8,7,Topic,"00:12:03,840","00:12:09,020",178,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=723,namely representing as word distribution,pic_cs-410_8_7_720.jpg
cs-410_8_7_179,cs-410,8,7,Topic,"00:12:09,020","00:12:14,039",179,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=729,multiple words to describe a complicated,pic_cs-410_8_7_720.jpg
cs-410_8_7_180,cs-410,8,7,Topic,"00:12:14,039","00:12:19,390",180,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=734,weights on words so we have more than,pic_cs-410_8_7_720.jpg
cs-410_8_7_181,cs-410,8,7,Topic,"00:12:19,390","00:12:23,390",181,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=739,"We talked about the task of topic mining,",pic_cs-410_8_7_720.jpg
cs-410_8_7_182,cs-410,8,7,Topic,"00:12:23,390","00:12:26,430",182,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=743,When we define a topic as distribution.,pic_cs-410_8_7_720.jpg
cs-410_8_7_183,cs-410,8,7,Topic,"00:12:26,430","00:12:30,140",183,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=746,So the importer is a clashing of text,pic_cs-410_8_7_720.jpg
cs-410_8_7_184,cs-410,8,7,Topic,"00:12:30,140","00:12:33,000",184,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=750,a vocabulary set and,pic_cs-410_8_7_720.jpg
cs-410_8_7_185,cs-410,8,7,Topic,"00:12:33,000","00:12:35,470",185,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=753,Each is a word distribution and,pic_cs-410_8_7_720.jpg
cs-410_8_7_186,cs-410,8,7,Topic,"00:12:35,470","00:12:38,730",186,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=755,also the coverage of all,pic_cs-410_8_7_720.jpg
cs-410_8_7_187,cs-410,8,7,Topic,"00:12:38,730","00:12:43,870",187,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=758,And these are formally represented,pic_cs-410_8_7_720.jpg
cs-410_8_7_188,cs-410,8,7,Topic,"00:12:43,870","00:12:48,710",188,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=763,And we have two constraints here for,pic_cs-410_8_7_720.jpg
cs-410_8_7_189,cs-410,8,7,Topic,"00:12:48,710","00:12:53,320",189,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=768,The first is the constraints,pic_cs-410_8_7_720.jpg
cs-410_8_7_190,cs-410,8,7,Topic,"00:12:53,320","00:12:56,820",190,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=773,In each worded distribution,pic_cs-410_8_7_720.jpg
cs-410_8_7_191,cs-410,8,7,Topic,"00:12:56,820","00:12:59,400",191,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=776,"must sum to 1,",pic_cs-410_8_7_720.jpg
cs-410_8_7_192,cs-410,8,7,Topic,"00:12:59,400","00:13:03,960",192,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=779,The second constraint is on,pic_cs-410_8_7_720.jpg
cs-410_8_7_193,cs-410,8,7,Topic,"00:13:03,960","00:13:08,600",193,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=783,A document is not allowed to recover,pic_cs-410_8_7_780.jpg
cs-410_8_7_194,cs-410,8,7,Topic,"00:13:08,600","00:13:10,200",194,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=788,we are discovering.,pic_cs-410_8_7_780.jpg
cs-410_8_7_195,cs-410,8,7,Topic,"00:13:10,200","00:13:17,220",195,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=790,"So, the coverage of each of these k",pic_cs-410_8_7_780.jpg
cs-410_8_7_196,cs-410,8,7,Topic,"00:13:17,220","00:13:21,580",196,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=797,We also introduce a general idea of using,pic_cs-410_8_7_780.jpg
cs-410_8_7_197,cs-410,8,7,Topic,"00:13:21,580","00:13:27,920",197,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=801,"And the idea here is, first we're design",pic_cs-410_8_7_780.jpg
cs-410_8_7_198,cs-410,8,7,Topic,"00:13:27,920","00:13:30,780",198,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=807,We simply assume that they,pic_cs-410_8_7_780.jpg
cs-410_8_7_199,cs-410,8,7,Topic,"00:13:30,780","00:13:34,730",199,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=810,And inside the model we embed some,pic_cs-410_8_7_780.jpg
cs-410_8_7_200,cs-410,8,7,Topic,"00:13:34,730","00:13:35,650",200,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=814,denoted by lambda.,pic_cs-410_8_7_780.jpg
cs-410_8_7_201,cs-410,8,7,Topic,"00:13:36,770","00:13:40,605",201,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=816,And then we can infer the most,pic_cs-410_8_7_780.jpg
cs-410_8_7_202,cs-410,8,7,Topic,"00:13:40,605","00:13:41,935",202,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=820,given a particular data set.,pic_cs-410_8_7_780.jpg
cs-410_8_7_203,cs-410,8,7,Topic,"00:13:43,095","00:13:48,975",203,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=823,And we can then take the lambda star as,pic_cs-410_8_7_780.jpg
cs-410_8_7_204,cs-410,8,7,Topic,"00:13:48,975","00:13:49,495",204,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=828,our problem.,pic_cs-410_8_7_780.jpg
cs-410_8_7_205,cs-410,8,7,Topic,"00:13:50,555","00:13:53,115",205,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=830,And we can adjust,pic_cs-410_8_7_780.jpg
cs-410_8_7_206,cs-410,8,7,Topic,"00:13:53,115","00:13:58,855",206,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=833,the parameters to discover various,pic_cs-410_8_7_780.jpg
cs-410_8_7_207,cs-410,8,7,Topic,"00:13:58,855","00:14:04,999",207,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=838,As you will see later,pic_cs-410_8_7_780.jpg
cs-410_8_7_208,cs-410,8,7,Topic,"00:14:04,999","00:14:14,999",208,https://www.coursera.org/learn/cs-410/lecture/ai3kj?t=844,[MUSIC],pic_cs-410_8_7_840.jpg
cs-410_8_8_1,cs-410,8,8,Probabilistic,"00:00:00,025","00:00:07,147",1,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=0,[SOUND],pic_cs-410_8_8_0.jpg
cs-410_8_8_2,cs-410,8,8,Probabilistic,"00:00:07,147","00:00:09,992",2,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=7,lecture is about the Overview,pic_cs-410_8_8_0.jpg
cs-410_8_8_3,cs-410,8,8,Probabilistic,"00:00:09,992","00:00:12,001",3,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=9,which cover proper,pic_cs-410_8_8_0.jpg
cs-410_8_8_4,cs-410,8,8,Probabilistic,"00:00:12,001","00:00:15,906",4,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=12,In this lecture we're going to give,pic_cs-410_8_8_0.jpg
cs-410_8_8_5,cs-410,8,8,Probabilistic,"00:00:15,906","00:00:21,320",5,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=15,a overview of Statical Language Models.,pic_cs-410_8_8_0.jpg
cs-410_8_8_6,cs-410,8,8,Probabilistic,"00:00:21,320","00:00:24,320",6,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=21,These models are general models that cover,pic_cs-410_8_8_0.jpg
cs-410_8_8_7,cs-410,8,8,Probabilistic,"00:00:24,320","00:00:28,120",7,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=24,probabilistic topic models,pic_cs-410_8_8_0.jpg
cs-410_8_8_8,cs-410,8,8,Probabilistic,"00:00:28,120","00:00:30,530",8,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=28,"So first off,",pic_cs-410_8_8_0.jpg
cs-410_8_8_9,cs-410,8,8,Probabilistic,"00:00:31,780","00:00:36,070",9,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=31,A Statistical Language Model is,pic_cs-410_8_8_0.jpg
cs-410_8_8_10,cs-410,8,8,Probabilistic,"00:00:36,070","00:00:37,870",10,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=36,over word sequences.,pic_cs-410_8_8_0.jpg
cs-410_8_8_11,cs-410,8,8,Probabilistic,"00:00:37,870","00:00:41,520",11,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=37,"So, for example,",pic_cs-410_8_8_0.jpg
cs-410_8_8_12,cs-410,8,8,Probabilistic,"00:00:41,520","00:00:44,480",12,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=41,today is Wednesday a probability of .001.,pic_cs-410_8_8_0.jpg
cs-410_8_8_13,cs-410,8,8,Probabilistic,"00:00:44,480","00:00:49,040",13,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=44,"It might give today Wednesday is, which",pic_cs-410_8_8_0.jpg
cs-410_8_8_14,cs-410,8,8,Probabilistic,"00:00:49,040","00:00:53,560",14,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=49,"is a non-grammatical sentence, a very,",pic_cs-410_8_8_0.jpg
cs-410_8_8_15,cs-410,8,8,Probabilistic,"00:00:54,580","00:00:56,170",15,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=54,"And similarly another sentence,",pic_cs-410_8_8_0.jpg
cs-410_8_8_16,cs-410,8,8,Probabilistic,"00:00:56,170","00:01:01,430",16,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=56,the eigenvalue is positive might,pic_cs-410_8_8_0.jpg
cs-410_8_8_17,cs-410,8,8,Probabilistic,"00:01:01,430","00:01:06,200",17,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=61,So as you can see such a distribution,pic_cs-410_8_8_60.jpg
cs-410_8_8_18,cs-410,8,8,Probabilistic,"00:01:06,200","00:01:09,830",18,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=66,It depends on the Context of Discussion.,pic_cs-410_8_8_60.jpg
cs-410_8_8_19,cs-410,8,8,Probabilistic,"00:01:09,830","00:01:15,370",19,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=69,Some Word Sequences might have higher,pic_cs-410_8_8_60.jpg
cs-410_8_8_20,cs-410,8,8,Probabilistic,"00:01:15,370","00:01:19,060",20,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=75,Sequence of Words might have different,pic_cs-410_8_8_60.jpg
cs-410_8_8_21,cs-410,8,8,Probabilistic,"00:01:20,490","00:01:24,870",21,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=80,And so this suggests that such a,pic_cs-410_8_8_60.jpg
cs-410_8_8_22,cs-410,8,8,Probabilistic,"00:01:26,960","00:01:31,440",22,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=86,such a model can also be regarded,pic_cs-410_8_8_60.jpg
cs-410_8_8_23,cs-410,8,8,Probabilistic,"00:01:31,440","00:01:32,520",23,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=91,generating text.,pic_cs-410_8_8_60.jpg
cs-410_8_8_24,cs-410,8,8,Probabilistic,"00:01:33,880","00:01:42,370",24,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=93,And that just means we can view text,pic_cs-410_8_8_60.jpg
cs-410_8_8_25,cs-410,8,8,Probabilistic,"00:01:42,370","00:01:49,225",25,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=102,"For this reason,",pic_cs-410_8_8_60.jpg
cs-410_8_8_26,cs-410,8,8,Probabilistic,"00:01:49,225","00:01:54,310",26,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=109,"So, now given a model we can then",pic_cs-410_8_8_60.jpg
cs-410_8_8_27,cs-410,8,8,Probabilistic,"00:01:54,310","00:01:59,790",27,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=114,"So, for example, based on the distribution",pic_cs-410_8_8_60.jpg
cs-410_8_8_28,cs-410,8,8,Probabilistic,"00:01:59,790","00:02:04,240",28,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=119,when matter it say assemble,pic_cs-410_8_8_60.jpg
cs-410_8_8_29,cs-410,8,8,Probabilistic,"00:02:04,240","00:02:07,130",29,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=124,because it has a relative,pic_cs-410_8_8_120.jpg
cs-410_8_8_30,cs-410,8,8,Probabilistic,"00:02:07,130","00:02:10,100",30,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=127,We might often get such a sequence.,pic_cs-410_8_8_120.jpg
cs-410_8_8_31,cs-410,8,8,Probabilistic,"00:02:10,100","00:02:14,470",31,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=130,We might also get the item,pic_cs-410_8_8_120.jpg
cs-410_8_8_32,cs-410,8,8,Probabilistic,"00:02:14,470","00:02:19,120",32,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=134,with a smaller probability and,pic_cs-410_8_8_120.jpg
cs-410_8_8_33,cs-410,8,8,Probabilistic,"00:02:19,120","00:02:22,940",33,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=139,get today is Wednesday because,pic_cs-410_8_8_120.jpg
cs-410_8_8_34,cs-410,8,8,Probabilistic,"00:02:24,650","00:02:28,960",34,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=144,"So in general, in order to categorize such",pic_cs-410_8_8_120.jpg
cs-410_8_8_35,cs-410,8,8,Probabilistic,"00:02:28,960","00:02:33,940",35,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=148,values for,pic_cs-410_8_8_120.jpg
cs-410_8_8_36,cs-410,8,8,Probabilistic,"00:02:33,940","00:02:37,827",36,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=153,"Obviously, it's impossible",pic_cs-410_8_8_120.jpg
cs-410_8_8_37,cs-410,8,8,Probabilistic,"00:02:37,827","00:02:42,540",37,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=157,impossible to enumerate all of,pic_cs-410_8_8_120.jpg
cs-410_8_8_38,cs-410,8,8,Probabilistic,"00:02:42,540","00:02:49,300",38,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=162,"So in practice, we will have to",pic_cs-410_8_8_120.jpg
cs-410_8_8_39,cs-410,8,8,Probabilistic,"00:02:49,300","00:02:52,710",39,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=169,"So, the simplest language model is",pic_cs-410_8_8_120.jpg
cs-410_8_8_40,cs-410,8,8,Probabilistic,"00:02:52,710","00:02:57,270",40,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=172,"In such a case, it was simply a the text",pic_cs-410_8_8_120.jpg
cs-410_8_8_41,cs-410,8,8,Probabilistic,"00:02:57,270","00:03:01,830",41,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=177,is generated by generating,pic_cs-410_8_8_120.jpg
cs-410_8_8_42,cs-410,8,8,Probabilistic,"00:03:02,980","00:03:06,660",42,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=182,"But in general, the words may",pic_cs-410_8_8_180.jpg
cs-410_8_8_43,cs-410,8,8,Probabilistic,"00:03:06,660","00:03:11,020",43,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=186,"But after we make this assumption, we can",pic_cs-410_8_8_180.jpg
cs-410_8_8_44,cs-410,8,8,Probabilistic,"00:03:12,230","00:03:16,700",44,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=192,"Basically, now the probability of",pic_cs-410_8_8_180.jpg
cs-410_8_8_45,cs-410,8,8,Probabilistic,"00:03:16,700","00:03:21,500",45,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=196,will be just the product of,pic_cs-410_8_8_180.jpg
cs-410_8_8_46,cs-410,8,8,Probabilistic,"00:03:24,850","00:03:26,210",46,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=204,"So for such a model,",pic_cs-410_8_8_180.jpg
cs-410_8_8_47,cs-410,8,8,Probabilistic,"00:03:26,210","00:03:30,470",47,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=206,we have as many parameters as,pic_cs-410_8_8_180.jpg
cs-410_8_8_48,cs-410,8,8,Probabilistic,"00:03:30,470","00:03:35,260",48,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=210,"So here we assume we have n words,",pic_cs-410_8_8_180.jpg
cs-410_8_8_49,cs-410,8,8,Probabilistic,"00:03:35,260","00:03:36,590",49,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=215,One for each word.,pic_cs-410_8_8_180.jpg
cs-410_8_8_50,cs-410,8,8,Probabilistic,"00:03:36,590","00:03:38,700",50,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=216,And then some to 1.,pic_cs-410_8_8_180.jpg
cs-410_8_8_51,cs-410,8,8,Probabilistic,"00:03:38,700","00:03:43,010",51,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=218,"So, now we assume that",pic_cs-410_8_8_180.jpg
cs-410_8_8_52,cs-410,8,8,Probabilistic,"00:03:43,010","00:03:46,220",52,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=223,drawn according to this word distribution.,pic_cs-410_8_8_180.jpg
cs-410_8_8_53,cs-410,8,8,Probabilistic,"00:03:46,220","00:03:50,870",53,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=226,"That just means,",pic_cs-410_8_8_180.jpg
cs-410_8_8_54,cs-410,8,8,Probabilistic,"00:03:50,870","00:03:52,360",54,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=230,then eventually we'll get a text.,pic_cs-410_8_8_180.jpg
cs-410_8_8_55,cs-410,8,8,Probabilistic,"00:03:53,690","00:03:56,163",55,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=233,"So for example, now again,",pic_cs-410_8_8_180.jpg
cs-410_8_8_56,cs-410,8,8,Probabilistic,"00:03:56,163","00:04:02,050",56,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=236,we can try to assemble words,pic_cs-410_8_8_180.jpg
cs-410_8_8_57,cs-410,8,8,Probabilistic,"00:04:02,050","00:04:05,110",57,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=242,We might get Wednesday often or,pic_cs-410_8_8_240.jpg
cs-410_8_8_58,cs-410,8,8,Probabilistic,"00:04:06,610","00:04:11,910",58,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=246,And some other words like eigenvalue,pic_cs-410_8_8_240.jpg
cs-410_8_8_59,cs-410,8,8,Probabilistic,"00:04:11,910","00:04:19,370",59,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=251,"But with this, we actually can",pic_cs-410_8_8_240.jpg
cs-410_8_8_60,cs-410,8,8,Probabilistic,"00:04:19,370","00:04:25,980",60,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=259,"every sequence, even though our model",pic_cs-410_8_8_240.jpg
cs-410_8_8_61,cs-410,8,8,Probabilistic,"00:04:25,980","00:04:27,780",61,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=265,And this is because of the independence.,pic_cs-410_8_8_240.jpg
cs-410_8_8_62,cs-410,8,8,Probabilistic,"00:04:27,780","00:04:32,970",62,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=267,"So specifically, we can compute",pic_cs-410_8_8_240.jpg
cs-410_8_8_63,cs-410,8,8,Probabilistic,"00:04:34,010","00:04:37,740",63,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=274,Because it's just a product,pic_cs-410_8_8_240.jpg
cs-410_8_8_64,cs-410,8,8,Probabilistic,"00:04:37,740","00:04:42,000",64,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=277,"the probability of is, and",pic_cs-410_8_8_240.jpg
cs-410_8_8_65,cs-410,8,8,Probabilistic,"00:04:42,000","00:04:45,380",65,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=282,"For example,",pic_cs-410_8_8_240.jpg
cs-410_8_8_66,cs-410,8,8,Probabilistic,"00:04:45,380","00:04:49,650",66,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=285,multiply these numbers together you get,pic_cs-410_8_8_240.jpg
cs-410_8_8_67,cs-410,8,8,Probabilistic,"00:04:49,650","00:04:55,900",67,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=289,"So as you can see, with N probabilities,",pic_cs-410_8_8_240.jpg
cs-410_8_8_68,cs-410,8,8,Probabilistic,"00:04:55,900","00:05:02,670",68,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=295,can characterize the probability situation,pic_cs-410_8_8_240.jpg
cs-410_8_8_69,cs-410,8,8,Probabilistic,"00:05:02,670","00:05:06,100",69,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=302,"And so, this is a very simple model.",pic_cs-410_8_8_300.jpg
cs-410_8_8_70,cs-410,8,8,Probabilistic,"00:05:06,100","00:05:07,890",70,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=306,Ignore the word order.,pic_cs-410_8_8_300.jpg
cs-410_8_8_71,cs-410,8,8,Probabilistic,"00:05:07,890","00:05:12,290",71,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=307,"So it may not be, in fact, in some",pic_cs-410_8_8_300.jpg
cs-410_8_8_72,cs-410,8,8,Probabilistic,"00:05:12,290","00:05:15,410",72,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=312,where you may care about,pic_cs-410_8_8_300.jpg
cs-410_8_8_73,cs-410,8,8,Probabilistic,"00:05:15,410","00:05:18,310",73,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=315,But it turns out to be,pic_cs-410_8_8_300.jpg
cs-410_8_8_74,cs-410,8,8,Probabilistic,"00:05:18,310","00:05:20,950",74,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=318,many tasks that involve topic analysis.,pic_cs-410_8_8_300.jpg
cs-410_8_8_75,cs-410,8,8,Probabilistic,"00:05:20,950","00:05:24,590",75,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=320,And that's also what,pic_cs-410_8_8_300.jpg
cs-410_8_8_76,cs-410,8,8,Probabilistic,"00:05:24,590","00:05:31,000",76,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=324,"So when we have a model, we generally have",pic_cs-410_8_8_300.jpg
cs-410_8_8_77,cs-410,8,8,Probabilistic,"00:05:31,000","00:05:38,520",77,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=331,"One is, given a model, how likely are we",pic_cs-410_8_8_300.jpg
cs-410_8_8_78,cs-410,8,8,Probabilistic,"00:05:38,520","00:05:41,890",78,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=338,"That is,",pic_cs-410_8_8_300.jpg
cs-410_8_8_79,cs-410,8,8,Probabilistic,"00:05:41,890","00:05:44,400",79,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=341,The other is the Estimation Process.,pic_cs-410_8_8_300.jpg
cs-410_8_8_80,cs-410,8,8,Probabilistic,"00:05:44,400","00:05:49,940",80,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=344,"And that, is to think of",pic_cs-410_8_8_300.jpg
cs-410_8_8_81,cs-410,8,8,Probabilistic,"00:05:49,940","00:05:53,510",81,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=349,some observe the data and we're,pic_cs-410_8_8_300.jpg
cs-410_8_8_82,cs-410,8,8,Probabilistic,"00:05:53,510","00:05:56,110",82,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=353,Let's first talk about the sampling.,pic_cs-410_8_8_300.jpg
cs-410_8_8_83,cs-410,8,8,Probabilistic,"00:05:56,110","00:06:02,480",83,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=356,"So, here I show two examples of Water",pic_cs-410_8_8_300.jpg
cs-410_8_8_84,cs-410,8,8,Probabilistic,"00:06:02,480","00:06:04,760",84,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=362,The first one has higher probabilities for,pic_cs-410_8_8_360.jpg
cs-410_8_8_85,cs-410,8,8,Probabilistic,"00:06:04,760","00:06:08,530",85,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=364,"words like a text mining association,",pic_cs-410_8_8_360.jpg
cs-410_8_8_86,cs-410,8,8,Probabilistic,"00:06:10,120","00:06:16,030",86,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=370,Now this signals a topic about text mining,pic_cs-410_8_8_360.jpg
cs-410_8_8_87,cs-410,8,8,Probabilistic,"00:06:16,030","00:06:21,970",87,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=376,"such a distribution, we tend to see words",pic_cs-410_8_8_360.jpg
cs-410_8_8_88,cs-410,8,8,Probabilistic,"00:06:23,710","00:06:27,460",88,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=383,"So in this case,",pic_cs-410_8_8_360.jpg
cs-410_8_8_89,cs-410,8,8,Probabilistic,"00:06:27,460","00:06:30,560",89,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=387,what is the probability of,pic_cs-410_8_8_360.jpg
cs-410_8_8_90,cs-410,8,8,Probabilistic,"00:06:30,560","00:06:36,610",90,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=390,"Then, we likely will see text that",pic_cs-410_8_8_360.jpg
cs-410_8_8_91,cs-410,8,8,Probabilistic,"00:06:36,610","00:06:42,110",91,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=396,"Of course, the text that we",pic_cs-410_8_8_360.jpg
cs-410_8_8_92,cs-410,8,8,Probabilistic,"00:06:42,110","00:06:45,150",92,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=402,This distribution is unlikely coherent.,pic_cs-410_8_8_360.jpg
cs-410_8_8_93,cs-410,8,8,Probabilistic,"00:06:45,150","00:06:49,079",93,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=405,"Although, the probability",pic_cs-410_8_8_360.jpg
cs-410_8_8_94,cs-410,8,8,Probabilistic,"00:06:49,079","00:06:53,535",94,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=409,[INAUDIBLE] publishing,pic_cs-410_8_8_360.jpg
cs-410_8_8_95,cs-410,8,8,Probabilistic,"00:06:53,535","00:06:59,090",95,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=413,non-zero assuming that no word has,pic_cs-410_8_8_360.jpg
cs-410_8_8_96,cs-410,8,8,Probabilistic,"00:06:59,090","00:07:02,590",96,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=419,"And that just means,",pic_cs-410_8_8_360.jpg
cs-410_8_8_97,cs-410,8,8,Probabilistic,"00:07:02,590","00:07:06,560",97,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=422,text documents including very,pic_cs-410_8_8_420.jpg
cs-410_8_8_98,cs-410,8,8,Probabilistic,"00:07:07,830","00:07:09,660",98,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=427,"Now, the second distribution show,",pic_cs-410_8_8_420.jpg
cs-410_8_8_99,cs-410,8,8,Probabilistic,"00:07:09,660","00:07:14,310",99,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=429,"on the bottom, has different than",pic_cs-410_8_8_420.jpg
cs-410_8_8_100,cs-410,8,8,Probabilistic,"00:07:14,310","00:07:17,940",100,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=434,"So food [INAUDIBLE] healthy [INAUDIBLE],",pic_cs-410_8_8_420.jpg
cs-410_8_8_101,cs-410,8,8,Probabilistic,"00:07:17,940","00:07:20,380",101,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=437,So this clearly indicates,pic_cs-410_8_8_420.jpg
cs-410_8_8_102,cs-410,8,8,Probabilistic,"00:07:20,380","00:07:23,190",102,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=440,In this case it's probably about health.,pic_cs-410_8_8_420.jpg
cs-410_8_8_103,cs-410,8,8,Probabilistic,"00:07:23,190","00:07:26,460",103,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=443,So if we sample a word,pic_cs-410_8_8_420.jpg
cs-410_8_8_104,cs-410,8,8,Probabilistic,"00:07:26,460","00:07:31,390",104,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=446,then the probability of observing a text,pic_cs-410_8_8_420.jpg
cs-410_8_8_105,cs-410,8,8,Probabilistic,"00:07:32,830","00:07:37,020",105,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=452,"On the other hand, the probability of",pic_cs-410_8_8_420.jpg
cs-410_8_8_106,cs-410,8,8,Probabilistic,"00:07:37,020","00:07:40,400",106,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=457,"nutrition paper would be high,",pic_cs-410_8_8_420.jpg
cs-410_8_8_107,cs-410,8,8,Probabilistic,"00:07:41,510","00:07:48,113",107,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=461,"So that just means, given a particular",pic_cs-410_8_8_420.jpg
cs-410_8_8_108,cs-410,8,8,Probabilistic,"00:07:48,113","00:07:51,830",108,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=468,Now let's look at,pic_cs-410_8_8_420.jpg
cs-410_8_8_109,cs-410,8,8,Probabilistic,"00:07:51,830","00:07:54,910",109,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=471,"In this case, we're going to assume",pic_cs-410_8_8_420.jpg
cs-410_8_8_110,cs-410,8,8,Probabilistic,"00:07:54,910","00:07:57,410",110,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=474,I will know exactly what,pic_cs-410_8_8_420.jpg
cs-410_8_8_111,cs-410,8,8,Probabilistic,"00:07:57,410","00:07:59,715",111,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=477,"In this case,",pic_cs-410_8_8_420.jpg
cs-410_8_8_112,cs-410,8,8,Probabilistic,"00:07:59,715","00:08:06,980",112,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=479,"In fact, it's abstract of the paper,",pic_cs-410_8_8_420.jpg
cs-410_8_8_113,cs-410,8,8,Probabilistic,"00:08:06,980","00:08:10,960",113,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=486,And I've shown some counts,pic_cs-410_8_8_480.jpg
cs-410_8_8_114,cs-410,8,8,Probabilistic,"00:08:12,550","00:08:16,880",114,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=492,"Now, if we ask the question,",pic_cs-410_8_8_480.jpg
cs-410_8_8_115,cs-410,8,8,Probabilistic,"00:08:17,950","00:08:22,440",115,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=497,Language Model that has been,pic_cs-410_8_8_480.jpg
cs-410_8_8_116,cs-410,8,8,Probabilistic,"00:08:22,440","00:08:26,400",116,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=502,Assuming that the text is observed,pic_cs-410_8_8_480.jpg
cs-410_8_8_117,cs-410,8,8,Probabilistic,"00:08:26,400","00:08:28,920",117,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=506,what's our best guess,pic_cs-410_8_8_480.jpg
cs-410_8_8_118,cs-410,8,8,Probabilistic,"00:08:30,740","00:08:35,510",118,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=510,"Okay, so the problem now is just to",pic_cs-410_8_8_480.jpg
cs-410_8_8_119,cs-410,8,8,Probabilistic,"00:08:35,510","00:08:36,490",119,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=515,As I've shown here.,pic_cs-410_8_8_480.jpg
cs-410_8_8_120,cs-410,8,8,Probabilistic,"00:08:37,560","00:08:38,370",120,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=517,So what do you think?,pic_cs-410_8_8_480.jpg
cs-410_8_8_121,cs-410,8,8,Probabilistic,"00:08:38,370","00:08:39,610",121,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=518,What would be your guess?,pic_cs-410_8_8_480.jpg
cs-410_8_8_122,cs-410,8,8,Probabilistic,"00:08:40,680","00:08:45,590",122,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=520,Would you guess text has,pic_cs-410_8_8_480.jpg
cs-410_8_8_123,cs-410,8,8,Probabilistic,"00:08:45,590","00:08:47,180",123,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=525,a relatively large probability?,pic_cs-410_8_8_480.jpg
cs-410_8_8_124,cs-410,8,8,Probabilistic,"00:08:48,360","00:08:50,310",124,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=528,What about query?,pic_cs-410_8_8_480.jpg
cs-410_8_8_125,cs-410,8,8,Probabilistic,"00:08:50,310","00:08:53,200",125,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=530,"Well, your guess probably",pic_cs-410_8_8_480.jpg
cs-410_8_8_126,cs-410,8,8,Probabilistic,"00:08:53,200","00:08:56,516",126,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=533,how many times we have observed,pic_cs-410_8_8_480.jpg
cs-410_8_8_127,cs-410,8,8,Probabilistic,"00:08:56,516","00:09:00,550",127,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=536,And if you think about it for a moment.,pic_cs-410_8_8_480.jpg
cs-410_8_8_128,cs-410,8,8,Probabilistic,"00:09:00,550","00:09:04,960",128,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=540,"And if you are like many others,",pic_cs-410_8_8_540.jpg
cs-410_8_8_129,cs-410,8,8,Probabilistic,"00:09:04,960","00:09:10,140",129,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=544,"well, text has a probability of 10",pic_cs-410_8_8_540.jpg
cs-410_8_8_130,cs-410,8,8,Probabilistic,"00:09:10,140","00:09:15,040",130,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=550,the text 10 times in the text,pic_cs-410_8_8_540.jpg
cs-410_8_8_131,cs-410,8,8,Probabilistic,"00:09:15,040","00:09:19,640",131,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=555,"And similarly, mining has 5 out of 100.",pic_cs-410_8_8_540.jpg
cs-410_8_8_132,cs-410,8,8,Probabilistic,"00:09:19,640","00:09:25,180",132,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=559,And query has a relatively small,pic_cs-410_8_8_540.jpg
cs-410_8_8_133,cs-410,8,8,Probabilistic,"00:09:25,180","00:09:27,130",133,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=565,So it's 1 out of 100.,pic_cs-410_8_8_540.jpg
cs-410_8_8_134,cs-410,8,8,Probabilistic,"00:09:27,130","00:09:32,220",134,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=567,"Right, so that, intuitively,",pic_cs-410_8_8_540.jpg
cs-410_8_8_135,cs-410,8,8,Probabilistic,"00:09:32,220","00:09:36,440",135,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=572,"But the question is, is this our best",pic_cs-410_8_8_540.jpg
cs-410_8_8_136,cs-410,8,8,Probabilistic,"00:09:37,840","00:09:40,000",136,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=577,"Of course,",pic_cs-410_8_8_540.jpg
cs-410_8_8_137,cs-410,8,8,Probabilistic,"00:09:40,000","00:09:45,070",137,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=580,"we have to define what do we mean by best,",pic_cs-410_8_8_540.jpg
cs-410_8_8_138,cs-410,8,8,Probabilistic,"00:09:45,070","00:09:50,540",138,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=585,it turns out that our,pic_cs-410_8_8_540.jpg
cs-410_8_8_139,cs-410,8,8,Probabilistic,"00:09:50,540","00:09:54,680",139,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=590,In some sense and this is called,pic_cs-410_8_8_540.jpg
cs-410_8_8_140,cs-410,8,8,Probabilistic,"00:09:54,680","00:10:00,789",140,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=594,"And it's the best thing that, it will give",pic_cs-410_8_8_540.jpg
cs-410_8_8_141,cs-410,8,8,Probabilistic,"00:10:01,960","00:10:05,740",141,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=601,"Meaning that, if you change",pic_cs-410_8_8_600.jpg
cs-410_8_8_142,cs-410,8,8,Probabilistic,"00:10:05,740","00:10:10,760",142,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=605,then the probability of the observed,pic_cs-410_8_8_600.jpg
cs-410_8_8_143,cs-410,8,8,Probabilistic,"00:10:10,760","00:10:13,952",143,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=610,And this is called,pic_cs-410_8_8_600.jpg
cs-410_8_8_144,cs-410,8,8,Probabilistic,"00:10:13,952","00:10:23,952",144,https://www.coursera.org/learn/cs-410/lecture/KaYeS?t=613,[MUSIC],pic_cs-410_8_8_600.jpg
cs-410_8_9_1,cs-410,8,9,Probabilistic,"00:00:00,025","00:00:05,683",1,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=0,[SOUND] This lecture is a continued,pic_cs-410_8_9_0.jpg
cs-410_8_9_2,cs-410,8,9,Probabilistic,"00:00:05,683","00:00:13,370",2,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=5,discussion of probabilistic topic models.,pic_cs-410_8_9_0.jpg
cs-410_8_9_3,cs-410,8,9,Probabilistic,"00:00:13,370","00:00:19,990",3,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=13,"In this lecture, we're going to continue",pic_cs-410_8_9_0.jpg
cs-410_8_9_4,cs-410,8,9,Probabilistic,"00:00:19,990","00:00:24,970",4,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=19,We're going to talk about,pic_cs-410_8_9_0.jpg
cs-410_8_9_5,cs-410,8,9,Probabilistic,"00:00:24,970","00:00:28,300",5,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=24,are interested in just mining,pic_cs-410_8_9_0.jpg
cs-410_8_9_6,cs-410,8,9,Probabilistic,"00:00:30,880","00:00:35,910",6,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=30,"So in this simple setup,",pic_cs-410_8_9_0.jpg
cs-410_8_9_7,cs-410,8,9,Probabilistic,"00:00:35,910","00:00:41,060",7,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=35,one document and,pic_cs-410_8_9_0.jpg
cs-410_8_9_8,cs-410,8,9,Probabilistic,"00:00:41,060","00:00:44,810",8,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=41,So this is the simplest,pic_cs-410_8_9_0.jpg
cs-410_8_9_9,cs-410,8,9,Probabilistic,"00:00:44,810","00:00:49,921",9,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=44,"The input now no longer has k,",pic_cs-410_8_9_0.jpg
cs-410_8_9_10,cs-410,8,9,Probabilistic,"00:00:49,921","00:00:55,670",10,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=49,know there is only one topic and the,pic_cs-410_8_9_0.jpg
cs-410_8_9_11,cs-410,8,9,Probabilistic,"00:00:55,670","00:01:00,738",11,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=55,"In the output,",pic_cs-410_8_9_0.jpg
cs-410_8_9_12,cs-410,8,9,Probabilistic,"00:01:00,738","00:01:06,150",12,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=60,we assumed that the document,pic_cs-410_8_9_60.jpg
cs-410_8_9_13,cs-410,8,9,Probabilistic,"00:01:06,150","00:01:10,532",13,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=66,So the main goal is just to discover,pic_cs-410_8_9_60.jpg
cs-410_8_9_14,cs-410,8,9,Probabilistic,"00:01:10,532","00:01:12,930",14,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=70,"this single topic, as shown here.",pic_cs-410_8_9_60.jpg
cs-410_8_9_15,cs-410,8,9,Probabilistic,"00:01:14,770","00:01:19,275",15,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=74,"As always, when we think about using a",pic_cs-410_8_9_60.jpg
cs-410_8_9_16,cs-410,8,9,Probabilistic,"00:01:19,275","00:01:24,280",16,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=79,we start with thinking about what,pic_cs-410_8_9_60.jpg
cs-410_8_9_17,cs-410,8,9,Probabilistic,"00:01:24,280","00:01:28,880",17,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=84,from what perspective we're going to,pic_cs-410_8_9_60.jpg
cs-410_8_9_18,cs-410,8,9,Probabilistic,"00:01:28,880","00:01:32,268",18,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=88,And then we're going to,pic_cs-410_8_9_60.jpg
cs-410_8_9_19,cs-410,8,9,Probabilistic,"00:01:32,268","00:01:36,520",19,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=92,"the generating of the data,",pic_cs-410_8_9_60.jpg
cs-410_8_9_20,cs-410,8,9,Probabilistic,"00:01:36,520","00:01:41,310",20,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=96,Where our perspective just means we want,pic_cs-410_8_9_60.jpg
cs-410_8_9_21,cs-410,8,9,Probabilistic,"00:01:41,310","00:01:45,700",21,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=101,"the data, so that the model will",pic_cs-410_8_9_60.jpg
cs-410_8_9_22,cs-410,8,9,Probabilistic,"00:01:45,700","00:01:48,770",22,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=105,discovering the knowledge that we want.,pic_cs-410_8_9_60.jpg
cs-410_8_9_23,cs-410,8,9,Probabilistic,"00:01:48,770","00:01:54,210",23,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=108,And then we'll be thinking,pic_cs-410_8_9_60.jpg
cs-410_8_9_24,cs-410,8,9,Probabilistic,"00:01:54,210","00:02:00,480",24,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=114,write down the microfunction to,pic_cs-410_8_9_60.jpg
cs-410_8_9_25,cs-410,8,9,Probabilistic,"00:02:00,480","00:02:04,860",25,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=120,a data point will be,pic_cs-410_8_9_120.jpg
cs-410_8_9_26,cs-410,8,9,Probabilistic,"00:02:05,900","00:02:10,370",26,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=125,And the likelihood function will have,pic_cs-410_8_9_120.jpg
cs-410_8_9_27,cs-410,8,9,Probabilistic,"00:02:10,370","00:02:15,780",27,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=130,And then we argue our interest in,pic_cs-410_8_9_120.jpg
cs-410_8_9_28,cs-410,8,9,Probabilistic,"00:02:15,780","00:02:21,680",28,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=135,by maximizing the likelihood which will,pic_cs-410_8_9_120.jpg
cs-410_8_9_29,cs-410,8,9,Probabilistic,"00:02:21,680","00:02:26,710",29,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=141,These estimator parameters,pic_cs-410_8_9_120.jpg
cs-410_8_9_30,cs-410,8,9,Probabilistic,"00:02:26,710","00:02:31,640",30,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=146,"of the mining hours,",pic_cs-410_8_9_120.jpg
cs-410_8_9_31,cs-410,8,9,Probabilistic,"00:02:31,640","00:02:35,320",31,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=151,parameters as the knowledge,pic_cs-410_8_9_120.jpg
cs-410_8_9_32,cs-410,8,9,Probabilistic,"00:02:35,320","00:02:39,690",32,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=155,So let's look at these steps for,pic_cs-410_8_9_120.jpg
cs-410_8_9_33,cs-410,8,9,Probabilistic,"00:02:39,690","00:02:45,970",33,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=159,Later we'll look at this procedure for,pic_cs-410_8_9_120.jpg
cs-410_8_9_34,cs-410,8,9,Probabilistic,"00:02:45,970","00:02:50,170",34,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=165,"So our data, in this case is, just",pic_cs-410_8_9_120.jpg
cs-410_8_9_35,cs-410,8,9,Probabilistic,"00:02:50,170","00:02:52,520",35,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=170,Each word here is denoted by x sub i.,pic_cs-410_8_9_120.jpg
cs-410_8_9_36,cs-410,8,9,Probabilistic,"00:02:52,520","00:02:56,800",36,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=172,Our model is a Unigram language model.,pic_cs-410_8_9_120.jpg
cs-410_8_9_37,cs-410,8,9,Probabilistic,"00:02:56,800","00:03:03,420",37,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=176,A word distribution that we hope to,pic_cs-410_8_9_120.jpg
cs-410_8_9_38,cs-410,8,9,Probabilistic,"00:03:03,420","00:03:08,950",38,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=183,So we will have as many parameters as many,pic_cs-410_8_9_180.jpg
cs-410_8_9_39,cs-410,8,9,Probabilistic,"00:03:09,950","00:03:14,580",39,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=189,And for convenience we're,pic_cs-410_8_9_180.jpg
cs-410_8_9_40,cs-410,8,9,Probabilistic,"00:03:14,580","00:03:18,270",40,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=194,denote the probability of word w sub i.,pic_cs-410_8_9_180.jpg
cs-410_8_9_41,cs-410,8,9,Probabilistic,"00:03:20,450","00:03:23,384",41,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=200,And obviously these theta,pic_cs-410_8_9_180.jpg
cs-410_8_9_42,cs-410,8,9,Probabilistic,"00:03:24,480","00:03:27,110",42,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=204,Now what does a likelihood,pic_cs-410_8_9_180.jpg
cs-410_8_9_43,cs-410,8,9,Probabilistic,"00:03:27,110","00:03:30,970",43,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=207,"Well, this is just the probability",pic_cs-410_8_9_180.jpg
cs-410_8_9_44,cs-410,8,9,Probabilistic,"00:03:30,970","00:03:31,948",44,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=210,that given such a model.,pic_cs-410_8_9_180.jpg
cs-410_8_9_45,cs-410,8,9,Probabilistic,"00:03:31,948","00:03:36,920",45,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=211,Because we assume the independence in,pic_cs-410_8_9_180.jpg
cs-410_8_9_46,cs-410,8,9,Probabilistic,"00:03:36,920","00:03:41,010",46,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=216,the document will be just a product,pic_cs-410_8_9_180.jpg
cs-410_8_9_47,cs-410,8,9,Probabilistic,"00:03:42,790","00:03:46,900",47,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=222,And since some word might,pic_cs-410_8_9_180.jpg
cs-410_8_9_48,cs-410,8,9,Probabilistic,"00:03:46,900","00:03:51,070",48,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=226,So we can also rewrite this,pic_cs-410_8_9_180.jpg
cs-410_8_9_49,cs-410,8,9,Probabilistic,"00:03:52,580","00:03:58,550",49,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=232,"So in this line, we have rewritten",pic_cs-410_8_9_180.jpg
cs-410_8_9_50,cs-410,8,9,Probabilistic,"00:03:58,550","00:04:05,360",50,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=238,over all the unique words in,pic_cs-410_8_9_180.jpg
cs-410_8_9_51,cs-410,8,9,Probabilistic,"00:04:05,360","00:04:09,170",51,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=245,Now this is different,pic_cs-410_8_9_240.jpg
cs-410_8_9_52,cs-410,8,9,Probabilistic,"00:04:09,170","00:04:13,990",52,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=249,"Well, the product is over different",pic_cs-410_8_9_240.jpg
cs-410_8_9_53,cs-410,8,9,Probabilistic,"00:04:15,040","00:04:19,694",53,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=255,"Now when we do this transformation,",pic_cs-410_8_9_240.jpg
cs-410_8_9_54,cs-410,8,9,Probabilistic,"00:04:19,694","00:04:24,120",54,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=259,introduce a counter function here.,pic_cs-410_8_9_240.jpg
cs-410_8_9_55,cs-410,8,9,Probabilistic,"00:04:24,120","00:04:29,395",55,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=264,This denotes the count of,pic_cs-410_8_9_240.jpg
cs-410_8_9_56,cs-410,8,9,Probabilistic,"00:04:29,395","00:04:33,390",56,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=269,similarly this is the count,pic_cs-410_8_9_240.jpg
cs-410_8_9_57,cs-410,8,9,Probabilistic,"00:04:33,390","00:04:37,890",57,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=273,because these words might,pic_cs-410_8_9_240.jpg
cs-410_8_9_58,cs-410,8,9,Probabilistic,"00:04:37,890","00:04:40,459",58,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=277,You can also see if a word did,pic_cs-410_8_9_240.jpg
cs-410_8_9_59,cs-410,8,9,Probabilistic,"00:04:41,810","00:04:46,790",59,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=281,"It will have a zero count, therefore",pic_cs-410_8_9_240.jpg
cs-410_8_9_60,cs-410,8,9,Probabilistic,"00:04:46,790","00:04:50,410",60,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=286,So this is a very useful form of,pic_cs-410_8_9_240.jpg
cs-410_8_9_61,cs-410,8,9,Probabilistic,"00:04:50,410","00:04:55,060",61,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=290,writing down the likelihood function,pic_cs-410_8_9_240.jpg
cs-410_8_9_62,cs-410,8,9,Probabilistic,"00:04:55,060","00:05:01,230",62,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=295,"So I want you to pay attention to this,",pic_cs-410_8_9_240.jpg
cs-410_8_9_63,cs-410,8,9,Probabilistic,"00:05:01,230","00:05:07,120",63,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=301,It's just to change the product over all,pic_cs-410_8_9_300.jpg
cs-410_8_9_64,cs-410,8,9,Probabilistic,"00:05:07,120","00:05:12,013",64,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=307,"So in the end, of course, we'll use",pic_cs-410_8_9_300.jpg
cs-410_8_9_65,cs-410,8,9,Probabilistic,"00:05:12,013","00:05:14,512",65,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=312,function and it would look like this.,pic_cs-410_8_9_300.jpg
cs-410_8_9_66,cs-410,8,9,Probabilistic,"00:05:14,512","00:05:19,468",66,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=314,"Next, we're going to find",pic_cs-410_8_9_300.jpg
cs-410_8_9_67,cs-410,8,9,Probabilistic,"00:05:19,468","00:05:24,530",67,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=319,of these words that would maximize,pic_cs-410_8_9_300.jpg
cs-410_8_9_68,cs-410,8,9,Probabilistic,"00:05:24,530","00:05:30,539",68,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=324,So now lets take a look at the maximum,pic_cs-410_8_9_300.jpg
cs-410_8_9_69,cs-410,8,9,Probabilistic,"00:05:32,520","00:05:35,870",69,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=332,This line is copied from,pic_cs-410_8_9_300.jpg
cs-410_8_9_70,cs-410,8,9,Probabilistic,"00:05:35,870","00:05:37,340",70,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=335,It's just our likelihood function.,pic_cs-410_8_9_300.jpg
cs-410_8_9_71,cs-410,8,9,Probabilistic,"00:05:38,590","00:05:43,950",71,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=338,So our goal is to maximize,pic_cs-410_8_9_300.jpg
cs-410_8_9_72,cs-410,8,9,Probabilistic,"00:05:43,950","00:05:46,210",72,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=343,We will find it often easy to,pic_cs-410_8_9_300.jpg
cs-410_8_9_73,cs-410,8,9,Probabilistic,"00:05:47,310","00:05:51,110",73,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=347,maximize the local likelihood,pic_cs-410_8_9_300.jpg
cs-410_8_9_74,cs-410,8,9,Probabilistic,"00:05:51,110","00:05:56,531",74,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=351,And this is purely for,pic_cs-410_8_9_300.jpg
cs-410_8_9_75,cs-410,8,9,Probabilistic,"00:05:56,531","00:06:03,698",75,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=356,the logarithm transformation our function,pic_cs-410_8_9_300.jpg
cs-410_8_9_76,cs-410,8,9,Probabilistic,"00:06:03,698","00:06:10,704",76,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=363,And we also have constraints,pic_cs-410_8_9_360.jpg
cs-410_8_9_77,cs-410,8,9,Probabilistic,"00:06:10,704","00:06:16,743",77,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=370,The sum makes it easier to take,pic_cs-410_8_9_360.jpg
cs-410_8_9_78,cs-410,8,9,Probabilistic,"00:06:16,743","00:06:21,022",78,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=376,finding the optimal,pic_cs-410_8_9_360.jpg
cs-410_8_9_79,cs-410,8,9,Probabilistic,"00:06:21,022","00:06:27,349",79,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=381,"So please take a look at this sum again,",pic_cs-410_8_9_360.jpg
cs-410_8_9_80,cs-410,8,9,Probabilistic,"00:06:27,349","00:06:32,434",80,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=387,And this is a form of,pic_cs-410_8_9_360.jpg
cs-410_8_9_81,cs-410,8,9,Probabilistic,"00:06:32,434","00:06:38,430",81,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=392,"see later also,",pic_cs-410_8_9_360.jpg
cs-410_8_9_82,cs-410,8,9,Probabilistic,"00:06:38,430","00:06:42,340",82,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=398,So it's a sum over all,pic_cs-410_8_9_360.jpg
cs-410_8_9_83,cs-410,8,9,Probabilistic,"00:06:42,340","00:06:48,105",83,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=402,And inside the sum there is,pic_cs-410_8_9_360.jpg
cs-410_8_9_84,cs-410,8,9,Probabilistic,"00:06:48,105","00:06:54,980",84,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=408,And this is macroed by,pic_cs-410_8_9_360.jpg
cs-410_8_9_85,cs-410,8,9,Probabilistic,"00:06:55,990","00:06:57,920",85,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=415,So let's see how we can,pic_cs-410_8_9_360.jpg
cs-410_8_9_86,cs-410,8,9,Probabilistic,"00:06:58,920","00:07:04,030",86,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=418,Now at this point the problem is purely a,pic_cs-410_8_9_360.jpg
cs-410_8_9_87,cs-410,8,9,Probabilistic,"00:07:04,030","00:07:11,360",87,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=424,to just the find the optimal solution,pic_cs-410_8_9_420.jpg
cs-410_8_9_88,cs-410,8,9,Probabilistic,"00:07:11,360","00:07:14,694",88,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=431,The objective function is,pic_cs-410_8_9_420.jpg
cs-410_8_9_89,cs-410,8,9,Probabilistic,"00:07:14,694","00:07:18,621",89,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=434,the constraint is that all these,pic_cs-410_8_9_420.jpg
cs-410_8_9_90,cs-410,8,9,Probabilistic,"00:07:18,621","00:07:23,234",90,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=438,"So, one way to solve the problem is",pic_cs-410_8_9_420.jpg
cs-410_8_9_91,cs-410,8,9,Probabilistic,"00:07:24,520","00:07:29,040",91,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=444,Now this command is beyond,pic_cs-410_8_9_420.jpg
cs-410_8_9_92,cs-410,8,9,Probabilistic,"00:07:29,040","00:07:33,670",92,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=449,since Lagrange multiplier is a very,pic_cs-410_8_9_420.jpg
cs-410_8_9_93,cs-410,8,9,Probabilistic,"00:07:33,670","00:07:37,940",93,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=453,"to just give a brief introduction to this,",pic_cs-410_8_9_420.jpg
cs-410_8_9_94,cs-410,8,9,Probabilistic,"00:07:39,720","00:07:43,857",94,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=459,So in this approach we will,pic_cs-410_8_9_420.jpg
cs-410_8_9_95,cs-410,8,9,Probabilistic,"00:07:43,857","00:07:49,887",95,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=463,And this function will combine,pic_cs-410_8_9_420.jpg
cs-410_8_9_96,cs-410,8,9,Probabilistic,"00:07:49,887","00:07:55,392",96,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=469,with another term that,pic_cs-410_8_9_420.jpg
cs-410_8_9_97,cs-410,8,9,Probabilistic,"00:07:55,392","00:07:59,980",97,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=475,"we introduce Lagrange multiplier here,",pic_cs-410_8_9_420.jpg
cs-410_8_9_98,cs-410,8,9,Probabilistic,"00:07:59,980","00:08:04,978",98,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=479,"lambda, so it's an additional parameter.",pic_cs-410_8_9_420.jpg
cs-410_8_9_99,cs-410,8,9,Probabilistic,"00:08:04,978","00:08:10,432",99,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=484,"Now, the idea of this approach is just to",pic_cs-410_8_9_480.jpg
cs-410_8_9_100,cs-410,8,9,Probabilistic,"00:08:10,432","00:08:14,800",100,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=490,"in some sense,",pic_cs-410_8_9_480.jpg
cs-410_8_9_101,cs-410,8,9,Probabilistic,"00:08:14,800","00:08:18,318",101,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=494,Now we are just interested in,pic_cs-410_8_9_480.jpg
cs-410_8_9_102,cs-410,8,9,Probabilistic,"00:08:19,460","00:08:24,022",102,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=499,"As you may recall from calculus,",pic_cs-410_8_9_480.jpg
cs-410_8_9_103,cs-410,8,9,Probabilistic,"00:08:24,022","00:08:29,910",103,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=504,would be achieved when,pic_cs-410_8_9_480.jpg
cs-410_8_9_104,cs-410,8,9,Probabilistic,"00:08:29,910","00:08:31,673",104,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=509,This is a necessary condition.,pic_cs-410_8_9_480.jpg
cs-410_8_9_105,cs-410,8,9,Probabilistic,"00:08:31,673","00:08:33,182",105,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=511,"It's not sufficient, though.",pic_cs-410_8_9_480.jpg
cs-410_8_9_106,cs-410,8,9,Probabilistic,"00:08:33,182","00:08:38,205",106,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=513,So if we do that you will,pic_cs-410_8_9_480.jpg
cs-410_8_9_107,cs-410,8,9,Probabilistic,"00:08:38,205","00:08:42,785",107,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=518,with respect to theta i,pic_cs-410_8_9_480.jpg
cs-410_8_9_108,cs-410,8,9,Probabilistic,"00:08:42,785","00:08:50,815",108,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=522,And this part comes from the derivative,pic_cs-410_8_9_480.jpg
cs-410_8_9_109,cs-410,8,9,Probabilistic,"00:08:50,815","00:08:55,390",109,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=530,this lambda is simply taken from here.,pic_cs-410_8_9_480.jpg
cs-410_8_9_110,cs-410,8,9,Probabilistic,"00:08:55,390","00:09:00,178",110,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=535,And when we set it to zero we can,pic_cs-410_8_9_480.jpg
cs-410_8_9_111,cs-410,8,9,Probabilistic,"00:09:00,178","00:09:05,610",111,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=540,easily see theta sub i is,pic_cs-410_8_9_540.jpg
cs-410_8_9_112,cs-410,8,9,Probabilistic,"00:09:06,820","00:09:09,900",112,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=546,Since we know all the theta,pic_cs-410_8_9_540.jpg
cs-410_8_9_113,cs-410,8,9,Probabilistic,"00:09:09,900","00:09:12,423",113,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=549,"we can plug this into this constraint,",pic_cs-410_8_9_540.jpg
cs-410_8_9_114,cs-410,8,9,Probabilistic,"00:09:12,423","00:09:15,600",114,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=552,And this will allow us to solve for,pic_cs-410_8_9_540.jpg
cs-410_8_9_115,cs-410,8,9,Probabilistic,"00:09:16,630","00:09:20,840",115,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=556,And this is just a net,pic_cs-410_8_9_540.jpg
cs-410_8_9_116,cs-410,8,9,Probabilistic,"00:09:20,840","00:09:27,350",116,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=560,And this further allows us to then,pic_cs-410_8_9_540.jpg
cs-410_8_9_117,cs-410,8,9,Probabilistic,"00:09:27,350","00:09:31,380",117,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=567,"eventually, to find the optimal",pic_cs-410_8_9_540.jpg
cs-410_8_9_118,cs-410,8,9,Probabilistic,"00:09:31,380","00:09:37,280",118,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=571,And if you look at this formula it turns,pic_cs-410_8_9_540.jpg
cs-410_8_9_119,cs-410,8,9,Probabilistic,"00:09:37,280","00:09:43,089",119,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=577,because this is just the normalized,pic_cs-410_8_9_540.jpg
cs-410_8_9_120,cs-410,8,9,Probabilistic,"00:09:43,089","00:09:47,751",120,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=583,which is also a sum of all,pic_cs-410_8_9_540.jpg
cs-410_8_9_121,cs-410,8,9,Probabilistic,"00:09:47,751","00:09:52,157",121,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=587,"So, after all this mess, after all,",pic_cs-410_8_9_540.jpg
cs-410_8_9_122,cs-410,8,9,Probabilistic,"00:09:52,157","00:09:59,044",122,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=592,we have just obtained something,pic_cs-410_8_9_540.jpg
cs-410_8_9_123,cs-410,8,9,Probabilistic,"00:09:59,044","00:10:04,415",123,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=599,this will be just our,pic_cs-410_8_9_540.jpg
cs-410_8_9_124,cs-410,8,9,Probabilistic,"00:10:04,415","00:10:10,338",124,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=604,maximize the data by,pic_cs-410_8_9_600.jpg
cs-410_8_9_125,cs-410,8,9,Probabilistic,"00:10:10,338","00:10:16,419",125,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=610,mass as possible to all,pic_cs-410_8_9_600.jpg
cs-410_8_9_126,cs-410,8,9,Probabilistic,"00:10:16,419","00:10:21,408",126,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=616,And you might also notice that this is,pic_cs-410_8_9_600.jpg
cs-410_8_9_127,cs-410,8,9,Probabilistic,"00:10:21,408","00:10:23,450",127,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=621,raised estimator.,pic_cs-410_8_9_600.jpg
cs-410_8_9_128,cs-410,8,9,Probabilistic,"00:10:23,450","00:10:29,333",128,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=623,"In general, the estimator would be to",pic_cs-410_8_9_600.jpg
cs-410_8_9_129,cs-410,8,9,Probabilistic,"00:10:29,333","00:10:35,050",129,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=629,the counts have to be done in a particular,pic_cs-410_8_9_600.jpg
cs-410_8_9_130,cs-410,8,9,Probabilistic,"00:10:35,050","00:10:41,730",130,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=635,So this is basically an analytical,pic_cs-410_8_9_600.jpg
cs-410_8_9_131,cs-410,8,9,Probabilistic,"00:10:41,730","00:10:46,303",131,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=641,"In general though, when the likelihood",pic_cs-410_8_9_600.jpg
cs-410_8_9_132,cs-410,8,9,Probabilistic,"00:10:46,303","00:10:50,919",132,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=646,going to be able to solve the optimization,pic_cs-410_8_9_600.jpg
cs-410_8_9_133,cs-410,8,9,Probabilistic,"00:10:50,919","00:10:55,134",133,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=650,Instead we have to use some,pic_cs-410_8_9_600.jpg
cs-410_8_9_134,cs-410,8,9,Probabilistic,"00:10:55,134","00:10:58,787",134,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=655,"we're going to see such cases later, also.",pic_cs-410_8_9_600.jpg
cs-410_8_9_135,cs-410,8,9,Probabilistic,"00:10:58,787","00:11:02,385",135,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=658,So if you imagine what would we,pic_cs-410_8_9_600.jpg
cs-410_8_9_136,cs-410,8,9,Probabilistic,"00:11:02,385","00:11:07,146",136,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=662,likelihood estimator to estimate one,pic_cs-410_8_9_660.jpg
cs-410_8_9_137,cs-410,8,9,Probabilistic,"00:11:07,146","00:11:09,903",137,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=667,Let's imagine this document,pic_cs-410_8_9_660.jpg
cs-410_8_9_138,cs-410,8,9,Probabilistic,"00:11:09,903","00:11:16,277",138,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=669,"Now, what you might see is",pic_cs-410_8_9_660.jpg
cs-410_8_9_139,cs-410,8,9,Probabilistic,"00:11:16,277","00:11:20,555",139,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=676,"On the top, you will see the high",pic_cs-410_8_9_660.jpg
cs-410_8_9_140,cs-410,8,9,Probabilistic,"00:11:20,555","00:11:23,710",140,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=680,"common words,",pic_cs-410_8_9_660.jpg
cs-410_8_9_141,cs-410,8,9,Probabilistic,"00:11:23,710","00:11:27,742",141,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=683,And this will be followed by,pic_cs-410_8_9_660.jpg
cs-410_8_9_142,cs-410,8,9,Probabilistic,"00:11:27,742","00:11:31,622",142,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=687,"characterize the topic well like text,",pic_cs-410_8_9_660.jpg
cs-410_8_9_143,cs-410,8,9,Probabilistic,"00:11:31,622","00:11:36,275",143,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=691,"And then in the end,",pic_cs-410_8_9_660.jpg
cs-410_8_9_144,cs-410,8,9,Probabilistic,"00:11:36,275","00:11:40,017",144,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=696,words that are not really,pic_cs-410_8_9_660.jpg
cs-410_8_9_145,cs-410,8,9,Probabilistic,"00:11:40,017","00:11:44,320",145,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=700,they might be extraneously,pic_cs-410_8_9_660.jpg
cs-410_8_9_146,cs-410,8,9,Probabilistic,"00:11:44,320","00:11:49,590",146,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=704,"As a topic representation,",pic_cs-410_8_9_660.jpg
cs-410_8_9_147,cs-410,8,9,Probabilistic,"00:11:49,590","00:11:52,452",147,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=709,That because the high probability,pic_cs-410_8_9_660.jpg
cs-410_8_9_148,cs-410,8,9,Probabilistic,"00:11:52,452","00:11:55,310",148,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=712,they are not really,pic_cs-410_8_9_660.jpg
cs-410_8_9_149,cs-410,8,9,Probabilistic,"00:11:55,310","00:11:58,280",149,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=715,So my question is how can we,pic_cs-410_8_9_660.jpg
cs-410_8_9_150,cs-410,8,9,Probabilistic,"00:11:59,720","00:12:02,680",150,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=719,Now this is the topic of the next module.,pic_cs-410_8_9_660.jpg
cs-410_8_9_151,cs-410,8,9,Probabilistic,"00:12:02,680","00:12:06,913",151,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=722,We're going to talk about how to use,pic_cs-410_8_9_720.jpg
cs-410_8_9_152,cs-410,8,9,Probabilistic,"00:12:06,913","00:12:08,077",152,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=726,these common words.,pic_cs-410_8_9_720.jpg
cs-410_8_9_153,cs-410,8,9,Probabilistic,"00:12:08,077","00:12:18,077",153,https://www.coursera.org/learn/cs-410/lecture/lCSNo?t=728,[MUSIC],pic_cs-410_8_9_720.jpg
cs-410_9_1_1,cs-410,9,1,Probabilistic,"00:00:00,171","00:00:04,190",1,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=0,[MUSIC],pic_cs-410_9_1_0.jpg
cs-410_9_1_2,cs-410,9,1,Probabilistic,"00:00:06,708","00:00:10,470",2,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=6,This lecture is about the mixture,pic_cs-410_9_1_0.jpg
cs-410_9_1_3,cs-410,9,1,Probabilistic,"00:00:11,900","00:00:16,280",3,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=11,In this lecture we will continue,pic_cs-410_9_1_0.jpg
cs-410_9_1_4,cs-410,9,1,Probabilistic,"00:00:16,280","00:00:20,950",4,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=16,"In particular, what we introduce",pic_cs-410_9_1_0.jpg
cs-410_9_1_5,cs-410,9,1,Probabilistic,"00:00:20,950","00:00:24,230",5,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=20,This is a slide that,pic_cs-410_9_1_0.jpg
cs-410_9_1_6,cs-410,9,1,Probabilistic,"00:00:24,230","00:00:29,189",6,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=24,Where we talked about how to,pic_cs-410_9_1_0.jpg
cs-410_9_1_7,cs-410,9,1,Probabilistic,"00:00:29,189","00:00:34,271",7,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=29,words that we have on top of for,pic_cs-410_9_1_0.jpg
cs-410_9_1_8,cs-410,9,1,Probabilistic,"00:00:36,540","00:00:38,440",8,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=36,"So if you want to solve the problem,",pic_cs-410_9_1_0.jpg
cs-410_9_1_9,cs-410,9,1,Probabilistic,"00:00:38,440","00:00:44,090",9,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=38,it would be useful to think about,pic_cs-410_9_1_0.jpg
cs-410_9_1_10,cs-410,9,1,Probabilistic,"00:00:44,090","00:00:49,570",10,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=44,"Well, this obviously because these",pic_cs-410_9_1_0.jpg
cs-410_9_1_11,cs-410,9,1,Probabilistic,"00:00:49,570","00:00:52,730",11,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=49,we are using a maximum,pic_cs-410_9_1_0.jpg
cs-410_9_1_12,cs-410,9,1,Probabilistic,"00:00:52,730","00:00:56,170",12,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=52,Then the estimate obviously would,pic_cs-410_9_1_0.jpg
cs-410_9_1_13,cs-410,9,1,Probabilistic,"00:00:56,170","00:00:59,284",13,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=56,these words in order to,pic_cs-410_9_1_0.jpg
cs-410_9_1_14,cs-410,9,1,Probabilistic,"00:00:59,284","00:01:03,390",14,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=59,"So, in order to get rid of them that",pic_cs-410_9_1_0.jpg
cs-410_9_1_15,cs-410,9,1,Probabilistic,"00:01:03,390","00:01:04,030",15,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=63,differently here.,pic_cs-410_9_1_60.jpg
cs-410_9_1_16,cs-410,9,1,Probabilistic,"00:01:05,740","00:01:09,290",16,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=65,In particular we'll have,pic_cs-410_9_1_60.jpg
cs-410_9_1_17,cs-410,9,1,Probabilistic,"00:01:09,290","00:01:12,300",17,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=69,doesn't have to explain all,pic_cs-410_9_1_60.jpg
cs-410_9_1_18,cs-410,9,1,Probabilistic,"00:01:12,300","00:01:13,620",18,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=72,"What were going to say is that,",pic_cs-410_9_1_60.jpg
cs-410_9_1_19,cs-410,9,1,Probabilistic,"00:01:13,620","00:01:19,760",19,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=73,these common words should not be,pic_cs-410_9_1_60.jpg
cs-410_9_1_20,cs-410,9,1,Probabilistic,"00:01:19,760","00:01:25,750",20,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=79,So one natural way to solve the problem is,pic_cs-410_9_1_60.jpg
cs-410_9_1_21,cs-410,9,1,Probabilistic,"00:01:25,750","00:01:29,350",21,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=85,to account for just these common words.,pic_cs-410_9_1_60.jpg
cs-410_9_1_22,cs-410,9,1,Probabilistic,"00:01:29,350","00:01:33,940",22,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=89,"This way, the two distributions can be",pic_cs-410_9_1_60.jpg
cs-410_9_1_23,cs-410,9,1,Probabilistic,"00:01:33,940","00:01:38,390",23,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=93,And we'll let the other model which,pic_cs-410_9_1_60.jpg
cs-410_9_1_24,cs-410,9,1,Probabilistic,"00:01:38,390","00:01:40,700",24,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=98,to generate the common words.,pic_cs-410_9_1_60.jpg
cs-410_9_1_25,cs-410,9,1,Probabilistic,"00:01:40,700","00:01:47,040",25,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=100,This way our target topic theta,pic_cs-410_9_1_60.jpg
cs-410_9_1_26,cs-410,9,1,Probabilistic,"00:01:47,040","00:01:51,439",26,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=107,the common handle words that are,pic_cs-410_9_1_60.jpg
cs-410_9_1_27,cs-410,9,1,Probabilistic,"00:01:52,880","00:01:54,310",27,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=112,"So, how does this work?",pic_cs-410_9_1_60.jpg
cs-410_9_1_28,cs-410,9,1,Probabilistic,"00:01:54,310","00:01:58,210",28,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=114,"Well, it is just a small",pic_cs-410_9_1_60.jpg
cs-410_9_1_29,cs-410,9,1,Probabilistic,"00:01:58,210","00:02:01,050",29,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=118,where we have just one distribution.,pic_cs-410_9_1_60.jpg
cs-410_9_1_30,cs-410,9,1,Probabilistic,"00:02:01,050","00:02:02,870",30,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=121,"Since we now have two distributions,",pic_cs-410_9_1_120.jpg
cs-410_9_1_31,cs-410,9,1,Probabilistic,"00:02:02,870","00:02:07,810",31,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=122,we have to decide which distribution,pic_cs-410_9_1_120.jpg
cs-410_9_1_32,cs-410,9,1,Probabilistic,"00:02:07,810","00:02:12,670",32,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=127,Each word will still be a sample,pic_cs-410_9_1_120.jpg
cs-410_9_1_33,cs-410,9,1,Probabilistic,"00:02:13,730","00:02:16,940",33,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=133,Text data is still,pic_cs-410_9_1_120.jpg
cs-410_9_1_34,cs-410,9,1,Probabilistic,"00:02:16,940","00:02:20,770",34,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=136,"Namely, look at the generating",pic_cs-410_9_1_120.jpg
cs-410_9_1_35,cs-410,9,1,Probabilistic,"00:02:20,770","00:02:23,300",35,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=140,eventually we generate a lot of words.,pic_cs-410_9_1_120.jpg
cs-410_9_1_36,cs-410,9,1,Probabilistic,"00:02:23,300","00:02:24,840",36,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=143,"When we generate the word,",pic_cs-410_9_1_120.jpg
cs-410_9_1_37,cs-410,9,1,Probabilistic,"00:02:24,840","00:02:29,820",37,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=144,"however, we're going to first decide",pic_cs-410_9_1_120.jpg
cs-410_9_1_38,cs-410,9,1,Probabilistic,"00:02:29,820","00:02:34,910",38,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=149,And this is controlled by another,pic_cs-410_9_1_120.jpg
cs-410_9_1_39,cs-410,9,1,Probabilistic,"00:02:34,910","00:02:39,639",39,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=154,theta sub d and,pic_cs-410_9_1_120.jpg
cs-410_9_1_40,cs-410,9,1,Probabilistic,"00:02:41,850","00:02:47,170",40,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=161,So this is a probability of enacting,pic_cs-410_9_1_120.jpg
cs-410_9_1_41,cs-410,9,1,Probabilistic,"00:02:47,170","00:02:51,150",41,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=167,This is the probability of,pic_cs-410_9_1_120.jpg
cs-410_9_1_42,cs-410,9,1,Probabilistic,"00:02:52,150","00:02:54,500",42,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=172,of distribution denoted by theta sub B.,pic_cs-410_9_1_120.jpg
cs-410_9_1_43,cs-410,9,1,Probabilistic,"00:02:55,500","00:02:59,890",43,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=175,On this case I just give example,pic_cs-410_9_1_120.jpg
cs-410_9_1_44,cs-410,9,1,Probabilistic,"00:02:59,890","00:03:03,800",44,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=179,"So you're going to basically flip a coin,",pic_cs-410_9_1_120.jpg
cs-410_9_1_45,cs-410,9,1,Probabilistic,"00:03:03,800","00:03:05,740",45,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=183,to decide what you want to use.,pic_cs-410_9_1_180.jpg
cs-410_9_1_46,cs-410,9,1,Probabilistic,"00:03:05,740","00:03:09,850",46,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=185,But in general these probabilities,pic_cs-410_9_1_180.jpg
cs-410_9_1_47,cs-410,9,1,Probabilistic,"00:03:09,850","00:03:15,590",47,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=189,So you might bias toward using,pic_cs-410_9_1_180.jpg
cs-410_9_1_48,cs-410,9,1,Probabilistic,"00:03:15,590","00:03:19,960",48,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=195,So now the process of generating a word,pic_cs-410_9_1_180.jpg
cs-410_9_1_49,cs-410,9,1,Probabilistic,"00:03:19,960","00:03:26,500",49,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=199,Based on these probabilities choosing,pic_cs-410_9_1_180.jpg
cs-410_9_1_50,cs-410,9,1,Probabilistic,"00:03:26,500","00:03:31,920",50,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=206,"shows up as head, which means we're going",pic_cs-410_9_1_180.jpg
cs-410_9_1_51,cs-410,9,1,Probabilistic,"00:03:31,920","00:03:37,620",51,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=211,Then we're going to use this word,pic_cs-410_9_1_180.jpg
cs-410_9_1_52,cs-410,9,1,Probabilistic,"00:03:37,620","00:03:40,649",52,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=217,Otherwise we might be,pic_cs-410_9_1_180.jpg
cs-410_9_1_53,cs-410,9,1,Probabilistic,"00:03:41,680","00:03:45,530",53,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=221,And we're going to use the background,pic_cs-410_9_1_180.jpg
cs-410_9_1_54,cs-410,9,1,Probabilistic,"00:03:46,910","00:03:51,330",54,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=226,"So in such a case,",pic_cs-410_9_1_180.jpg
cs-410_9_1_55,cs-410,9,1,Probabilistic,"00:03:51,330","00:03:54,630",55,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=231,associated with the use,pic_cs-410_9_1_180.jpg
cs-410_9_1_56,cs-410,9,1,Probabilistic,"00:03:54,630","00:03:59,420",56,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=234,But we can still think of this as,pic_cs-410_9_1_180.jpg
cs-410_9_1_57,cs-410,9,1,Probabilistic,"00:03:59,420","00:04:01,220",57,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=239,And such a model is,pic_cs-410_9_1_180.jpg
cs-410_9_1_58,cs-410,9,1,Probabilistic,"00:04:02,760","00:04:03,860",58,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=242,So now let's see.,pic_cs-410_9_1_240.jpg
cs-410_9_1_59,cs-410,9,1,Probabilistic,"00:04:03,860","00:04:07,020",59,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=243,"In this case, what's the probability",pic_cs-410_9_1_240.jpg
cs-410_9_1_60,cs-410,9,1,Probabilistic,"00:04:07,020","00:04:10,460",60,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=247,Now here I showed some words.,pic_cs-410_9_1_240.jpg
cs-410_9_1_61,cs-410,9,1,Probabilistic,"00:04:10,460","00:04:12,280",61,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=250,"like ""the"" and ""text"".",pic_cs-410_9_1_240.jpg
cs-410_9_1_62,cs-410,9,1,Probabilistic,"00:04:12,280","00:04:13,820",62,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=252,"So as in all cases,",pic_cs-410_9_1_240.jpg
cs-410_9_1_63,cs-410,9,1,Probabilistic,"00:04:13,820","00:04:17,910",63,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=253,once we setup a model we are interested,pic_cs-410_9_1_240.jpg
cs-410_9_1_64,cs-410,9,1,Probabilistic,"00:04:17,910","00:04:19,550",64,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=257,"The basic question is, so",pic_cs-410_9_1_240.jpg
cs-410_9_1_65,cs-410,9,1,Probabilistic,"00:04:19,550","00:04:23,040",65,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=259,what's the probability of,pic_cs-410_9_1_240.jpg
cs-410_9_1_66,cs-410,9,1,Probabilistic,"00:04:23,040","00:04:27,870",66,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=263,Now we know that the word can be observed,pic_cs-410_9_1_240.jpg
cs-410_9_1_67,cs-410,9,1,Probabilistic,"00:04:27,870","00:04:29,840",67,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=267,we have to consider two cases.,pic_cs-410_9_1_240.jpg
cs-410_9_1_68,cs-410,9,1,Probabilistic,"00:04:29,840","00:04:32,660",68,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=269,Therefore it's a sum over these two cases.,pic_cs-410_9_1_240.jpg
cs-410_9_1_69,cs-410,9,1,Probabilistic,"00:04:34,410","00:04:40,040",69,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=274,The first case is to use the topic for,pic_cs-410_9_1_240.jpg
cs-410_9_1_70,cs-410,9,1,Probabilistic,"00:04:40,040","00:04:46,150",70,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=280,And in such a case then,pic_cs-410_9_1_240.jpg
cs-410_9_1_71,cs-410,9,1,Probabilistic,"00:04:46,150","00:04:48,550",71,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=286,which is the probability,pic_cs-410_9_1_240.jpg
cs-410_9_1_72,cs-410,9,1,Probabilistic,"00:04:48,550","00:04:53,760",72,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=288,multiplied by the probability of actually,pic_cs-410_9_1_240.jpg
cs-410_9_1_73,cs-410,9,1,Probabilistic,"00:04:53,760","00:04:56,970",73,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=293,Both events must happen,pic_cs-410_9_1_240.jpg
cs-410_9_1_74,cs-410,9,1,Probabilistic,"00:04:56,970","00:05:02,050",74,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=296,We first must have choosing,pic_cs-410_9_1_240.jpg
cs-410_9_1_75,cs-410,9,1,Probabilistic,"00:05:02,050","00:05:07,650",75,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=302,we also have to actually have sampled,pic_cs-410_9_1_300.jpg
cs-410_9_1_76,cs-410,9,1,Probabilistic,"00:05:07,650","00:05:11,100",76,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=307,"And similarly,",pic_cs-410_9_1_300.jpg
cs-410_9_1_77,cs-410,9,1,Probabilistic,"00:05:11,100","00:05:13,880",77,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=311,a different way of generally,pic_cs-410_9_1_300.jpg
cs-410_9_1_78,cs-410,9,1,Probabilistic,"00:05:15,190","00:05:20,970",78,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=315,Now obviously the probability of,pic_cs-410_9_1_300.jpg
cs-410_9_1_79,cs-410,9,1,Probabilistic,"00:05:20,970","00:05:25,040",79,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=320,So we also can see the two,pic_cs-410_9_1_300.jpg
cs-410_9_1_80,cs-410,9,1,Probabilistic,"00:05:25,040","00:05:29,720",80,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=325,"And in each case, it's a product of the",pic_cs-410_9_1_300.jpg
cs-410_9_1_81,cs-410,9,1,Probabilistic,"00:05:29,720","00:05:34,530",81,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=329,is multiplied by the probability of,pic_cs-410_9_1_300.jpg
cs-410_9_1_82,cs-410,9,1,Probabilistic,"00:05:35,640","00:05:38,890",82,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=335,"Now whether you will see,",pic_cs-410_9_1_300.jpg
cs-410_9_1_83,cs-410,9,1,Probabilistic,"00:05:38,890","00:05:43,940",83,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=338,So might want to make sure that you have,pic_cs-410_9_1_300.jpg
cs-410_9_1_84,cs-410,9,1,Probabilistic,"00:05:43,940","00:05:48,130",84,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=343,And you should convince yourself that,pic_cs-410_9_1_300.jpg
cs-410_9_1_85,cs-410,9,1,Probabilistic,"00:05:48,130","00:05:49,940",85,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=348,obsolete text.,pic_cs-410_9_1_300.jpg
cs-410_9_1_86,cs-410,9,1,Probabilistic,"00:05:49,940","00:05:52,010",86,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=349,So to summarize what we observed here.,pic_cs-410_9_1_300.jpg
cs-410_9_1_87,cs-410,9,1,Probabilistic,"00:05:52,010","00:05:57,270",87,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=352,The probability of a word from,pic_cs-410_9_1_300.jpg
cs-410_9_1_88,cs-410,9,1,Probabilistic,"00:05:57,270","00:05:59,500",88,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=357,of different ways of generating the word.,pic_cs-410_9_1_300.jpg
cs-410_9_1_89,cs-410,9,1,Probabilistic,"00:06:00,610","00:06:01,990",89,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=360,"In each case,",pic_cs-410_9_1_360.jpg
cs-410_9_1_90,cs-410,9,1,Probabilistic,"00:06:01,990","00:06:07,898",90,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=361,it's a product of the probability,pic_cs-410_9_1_360.jpg
cs-410_9_1_91,cs-410,9,1,Probabilistic,"00:06:07,898","00:06:12,320",91,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=367,Multiplied by the probability of,pic_cs-410_9_1_360.jpg
cs-410_9_1_92,cs-410,9,1,Probabilistic,"00:06:12,320","00:06:14,010",92,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=372,from that component of the model.,pic_cs-410_9_1_360.jpg
cs-410_9_1_93,cs-410,9,1,Probabilistic,"00:06:14,010","00:06:20,940",93,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=374,And this is something quite general and,pic_cs-410_9_1_360.jpg
cs-410_9_1_94,cs-410,9,1,Probabilistic,"00:06:20,940","00:06:23,825",94,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=380,So the basic idea of a mixture,pic_cs-410_9_1_360.jpg
cs-410_9_1_95,cs-410,9,1,Probabilistic,"00:06:23,825","00:06:28,820",95,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=383,thesetwo distributions,pic_cs-410_9_1_360.jpg
cs-410_9_1_96,cs-410,9,1,Probabilistic,"00:06:28,820","00:06:32,810",96,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=388,So I used a box to bring all,pic_cs-410_9_1_360.jpg
cs-410_9_1_97,cs-410,9,1,Probabilistic,"00:06:32,810","00:06:36,200",97,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=392,So if you view this,pic_cs-410_9_1_360.jpg
cs-410_9_1_98,cs-410,9,1,Probabilistic,"00:06:36,200","00:06:38,610",98,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=396,it's just like any other generative model.,pic_cs-410_9_1_360.jpg
cs-410_9_1_99,cs-410,9,1,Probabilistic,"00:06:38,610","00:06:41,260",99,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=398,It would just give us,pic_cs-410_9_1_360.jpg
cs-410_9_1_100,cs-410,9,1,Probabilistic,"00:06:42,850","00:06:47,310",100,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=402,But the way that determines this,pic_cs-410_9_1_360.jpg
cs-410_9_1_101,cs-410,9,1,Probabilistic,"00:06:47,310","00:06:48,840",101,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=407,when we have just one distribution.,pic_cs-410_9_1_360.jpg
cs-410_9_1_102,cs-410,9,1,Probabilistic,"00:06:50,050","00:06:54,710",102,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=410,And this is basically a more,pic_cs-410_9_1_360.jpg
cs-410_9_1_103,cs-410,9,1,Probabilistic,"00:06:54,710","00:06:57,710",103,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=414,So the more complicated is more,pic_cs-410_9_1_360.jpg
cs-410_9_1_104,cs-410,9,1,Probabilistic,"00:06:57,710","00:06:58,740",104,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=417,And it's called a mixture model.,pic_cs-410_9_1_360.jpg
cs-410_9_1_105,cs-410,9,1,Probabilistic,"00:07:00,460","00:07:04,450",105,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=420,So as I just said we can treat,pic_cs-410_9_1_420.jpg
cs-410_9_1_106,cs-410,9,1,Probabilistic,"00:07:04,450","00:07:08,450",106,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=424,And it's often useful to think of,pic_cs-410_9_1_420.jpg
cs-410_9_1_107,cs-410,9,1,Probabilistic,"00:07:08,450","00:07:10,140",107,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=428,The illustration that,pic_cs-410_9_1_420.jpg
cs-410_9_1_108,cs-410,9,1,Probabilistic,"00:07:10,140","00:07:14,210",108,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=430,"which is dimmer now, is just",pic_cs-410_9_1_420.jpg
cs-410_9_1_109,cs-410,9,1,Probabilistic,"00:07:14,210","00:07:18,390",109,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=434,"So mathematically,",pic_cs-410_9_1_420.jpg
cs-410_9_1_110,cs-410,9,1,Probabilistic,"00:07:18,390","00:07:21,690",110,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=438,to just define the following,pic_cs-410_9_1_420.jpg
cs-410_9_1_111,cs-410,9,1,Probabilistic,"00:07:21,690","00:07:25,820",111,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=441,Where the probability of a word is,pic_cs-410_9_1_420.jpg
cs-410_9_1_112,cs-410,9,1,Probabilistic,"00:07:26,840","00:07:28,830",112,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=446,of generating the word.,pic_cs-410_9_1_420.jpg
cs-410_9_1_113,cs-410,9,1,Probabilistic,"00:07:28,830","00:07:32,800",113,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=448,And the form you are seeing now,pic_cs-410_9_1_420.jpg
cs-410_9_1_114,cs-410,9,1,Probabilistic,"00:07:32,800","00:07:36,680",114,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=452,what you have seen in,pic_cs-410_9_1_420.jpg
cs-410_9_1_115,cs-410,9,1,Probabilistic,"00:07:36,680","00:07:41,150",115,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=456,Well I just use the symbol,pic_cs-410_9_1_420.jpg
cs-410_9_1_116,cs-410,9,1,Probabilistic,"00:07:41,150","00:07:46,330",116,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=461,you can still see this is,pic_cs-410_9_1_420.jpg
cs-410_9_1_117,cs-410,9,1,Probabilistic,"00:07:46,330","00:07:47,560",117,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=466,Right?,pic_cs-410_9_1_420.jpg
cs-410_9_1_118,cs-410,9,1,Probabilistic,"00:07:47,560","00:07:53,080",118,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=467,And this sum is due to the fact that the,pic_cs-410_9_1_420.jpg
cs-410_9_1_119,cs-410,9,1,Probabilistic,"00:07:53,080","00:07:55,070",119,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=473,two ways in this case.,pic_cs-410_9_1_420.jpg
cs-410_9_1_120,cs-410,9,1,Probabilistic,"00:07:55,070","00:08:00,330",120,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=475,"And inside a sum,",pic_cs-410_9_1_420.jpg
cs-410_9_1_121,cs-410,9,1,Probabilistic,"00:08:00,330","00:08:05,720",121,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=480,And the two terms are first,pic_cs-410_9_1_480.jpg
cs-410_9_1_122,cs-410,9,1,Probabilistic,"00:08:05,720","00:08:07,280",122,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=485,"like of D Second,",pic_cs-410_9_1_480.jpg
cs-410_9_1_123,cs-410,9,1,Probabilistic,"00:08:07,280","00:08:12,730",123,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=487,the probability of actually observing,pic_cs-410_9_1_480.jpg
cs-410_9_1_124,cs-410,9,1,Probabilistic,"00:08:12,730","00:08:18,770",124,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=492,So this is a very general description,pic_cs-410_9_1_480.jpg
cs-410_9_1_125,cs-410,9,1,Probabilistic,"00:08:18,770","00:08:23,020",125,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=498,I just want to make sure,pic_cs-410_9_1_480.jpg
cs-410_9_1_126,cs-410,9,1,Probabilistic,"00:08:23,020","00:08:27,154",126,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=503,this because this is really the basis for,pic_cs-410_9_1_480.jpg
cs-410_9_1_127,cs-410,9,1,Probabilistic,"00:08:28,480","00:08:31,350",127,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=508,So now once we setup model.,pic_cs-410_9_1_480.jpg
cs-410_9_1_128,cs-410,9,1,Probabilistic,"00:08:31,350","00:08:34,310",128,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=511,We can write down that like,pic_cs-410_9_1_480.jpg
cs-410_9_1_129,cs-410,9,1,Probabilistic,"00:08:34,310","00:08:37,720",129,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=514,"The next question is,",pic_cs-410_9_1_480.jpg
cs-410_9_1_130,cs-410,9,1,Probabilistic,"00:08:37,720","00:08:40,080",130,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=517,or what to do with the parameters.,pic_cs-410_9_1_480.jpg
cs-410_9_1_131,cs-410,9,1,Probabilistic,"00:08:40,080","00:08:41,540",131,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=520,Given the data.,pic_cs-410_9_1_480.jpg
cs-410_9_1_132,cs-410,9,1,Probabilistic,"00:08:41,540","00:08:42,860",132,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=521,"Well, in general,",pic_cs-410_9_1_480.jpg
cs-410_9_1_133,cs-410,9,1,Probabilistic,"00:08:42,860","00:08:47,410",133,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=522,we can use some of the text data,pic_cs-410_9_1_480.jpg
cs-410_9_1_134,cs-410,9,1,Probabilistic,"00:08:47,410","00:08:50,470",134,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=527,And this estimation would allow us to,pic_cs-410_9_1_480.jpg
cs-410_9_1_135,cs-410,9,1,Probabilistic,"00:08:50,470","00:08:55,350",135,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=530,discover the interesting,pic_cs-410_9_1_480.jpg
cs-410_9_1_136,cs-410,9,1,Probabilistic,"00:08:55,350","00:08:58,450",136,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=535,"So you, in this case, what do we discover?",pic_cs-410_9_1_480.jpg
cs-410_9_1_137,cs-410,9,1,Probabilistic,"00:08:58,450","00:09:01,120",137,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=538,"Well, these are presented",pic_cs-410_9_1_480.jpg
cs-410_9_1_138,cs-410,9,1,Probabilistic,"00:09:01,120","00:09:03,320",138,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=541,we will have two kinds of parameters.,pic_cs-410_9_1_540.jpg
cs-410_9_1_139,cs-410,9,1,Probabilistic,"00:09:03,320","00:09:07,400",139,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=543,"One is the two worded distributions,",pic_cs-410_9_1_540.jpg
cs-410_9_1_140,cs-410,9,1,Probabilistic,"00:09:07,400","00:09:10,380",140,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=547,the other is the coverage,pic_cs-410_9_1_540.jpg
cs-410_9_1_141,cs-410,9,1,Probabilistic,"00:09:12,560","00:09:14,340",141,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=552,The coverage of each topic.,pic_cs-410_9_1_540.jpg
cs-410_9_1_142,cs-410,9,1,Probabilistic,"00:09:14,340","00:09:17,630",142,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=554,And this is determined by,pic_cs-410_9_1_540.jpg
cs-410_9_1_143,cs-410,9,1,Probabilistic,"00:09:17,630","00:09:22,310",143,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=557,"probability of theta, so this is to one.",pic_cs-410_9_1_540.jpg
cs-410_9_1_144,cs-410,9,1,Probabilistic,"00:09:22,310","00:09:25,040",144,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=562,"Now, what's interesting is",pic_cs-410_9_1_540.jpg
cs-410_9_1_145,cs-410,9,1,Probabilistic,"00:09:25,040","00:09:29,540",145,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=565,cases like when we send one of,pic_cs-410_9_1_540.jpg
cs-410_9_1_146,cs-410,9,1,Probabilistic,"00:09:29,540","00:09:32,770",146,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=569,"Well with the other, with the zero right?",pic_cs-410_9_1_540.jpg
cs-410_9_1_147,cs-410,9,1,Probabilistic,"00:09:32,770","00:09:35,150",147,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=572,And if you look at,pic_cs-410_9_1_540.jpg
cs-410_9_1_148,cs-410,9,1,Probabilistic,"00:09:36,320","00:09:40,640",148,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=576,it will then degenerate to the special,pic_cs-410_9_1_540.jpg
cs-410_9_1_149,cs-410,9,1,Probabilistic,"00:09:40,640","00:09:46,290",149,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=580,Okay so you can easily verify that by,pic_cs-410_9_1_540.jpg
cs-410_9_1_150,cs-410,9,1,Probabilistic,"00:09:46,290","00:09:47,940",150,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=586,the other is Zero.,pic_cs-410_9_1_540.jpg
cs-410_9_1_151,cs-410,9,1,Probabilistic,"00:09:49,130","00:09:53,290",151,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=589,"So in this sense,",pic_cs-410_9_1_540.jpg
cs-410_9_1_152,cs-410,9,1,Probabilistic,"00:09:53,290","00:09:56,490",152,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=593,the previous model where we,pic_cs-410_9_1_540.jpg
cs-410_9_1_153,cs-410,9,1,Probabilistic,"00:09:56,490","00:09:58,740",153,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=596,It can cover that as a special case.,pic_cs-410_9_1_540.jpg
cs-410_9_1_154,cs-410,9,1,Probabilistic,"00:09:59,960","00:10:05,340",154,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=599,"So to summarize, we talked about the",pic_cs-410_9_1_540.jpg
cs-410_9_1_155,cs-410,9,1,Probabilistic,"00:10:05,340","00:10:09,110",155,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=605,the data we're considering,pic_cs-410_9_1_600.jpg
cs-410_9_1_156,cs-410,9,1,Probabilistic,"00:10:09,110","00:10:13,420",156,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=609,And the model is a mixture,pic_cs-410_9_1_600.jpg
cs-410_9_1_157,cs-410,9,1,Probabilistic,"00:10:13,420","00:10:16,830",157,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=613,"two unigram LM models,",pic_cs-410_9_1_600.jpg
cs-410_9_1_158,cs-410,9,1,Probabilistic,"00:10:16,830","00:10:22,810",158,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=616,which is intended to denote the topic of,pic_cs-410_9_1_600.jpg
cs-410_9_1_159,cs-410,9,1,Probabilistic,"00:10:22,810","00:10:28,500",159,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=622,representing a background topic that,pic_cs-410_9_1_600.jpg
cs-410_9_1_160,cs-410,9,1,Probabilistic,"00:10:28,500","00:10:32,840",160,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=628,words because common words would be,pic_cs-410_9_1_600.jpg
cs-410_9_1_161,cs-410,9,1,Probabilistic,"00:10:33,950","00:10:36,870",161,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=633,So the parameters can,pic_cs-410_9_1_600.jpg
cs-410_9_1_162,cs-410,9,1,Probabilistic,"00:10:36,870","00:10:40,380",162,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=636,Lambda which I show here you can again,pic_cs-410_9_1_600.jpg
cs-410_9_1_163,cs-410,9,1,Probabilistic,"00:10:41,560","00:10:45,520",163,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=641,think about the question about how many,pic_cs-410_9_1_600.jpg
cs-410_9_1_164,cs-410,9,1,Probabilistic,"00:10:45,520","00:10:50,920",164,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=645,This is usually a good exercise to do,pic_cs-410_9_1_600.jpg
cs-410_9_1_165,cs-410,9,1,Probabilistic,"00:10:50,920","00:10:56,470",165,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=650,depth and to have a complete understanding,pic_cs-410_9_1_600.jpg
cs-410_9_1_166,cs-410,9,1,Probabilistic,"00:10:56,470","00:10:58,700",166,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=656,"And we have mixing weights,",pic_cs-410_9_1_600.jpg
cs-410_9_1_167,cs-410,9,1,Probabilistic,"00:10:59,790","00:11:02,340",167,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=659,So what does a likelihood,pic_cs-410_9_1_600.jpg
cs-410_9_1_168,cs-410,9,1,Probabilistic,"00:11:02,340","00:11:06,620",168,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=662,"Well, it looks very similar",pic_cs-410_9_1_660.jpg
cs-410_9_1_169,cs-410,9,1,Probabilistic,"00:11:06,620","00:11:09,100",169,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=666,"So for the document,",pic_cs-410_9_1_660.jpg
cs-410_9_1_170,cs-410,9,1,Probabilistic,"00:11:09,100","00:11:14,260",170,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=669,first it's a product over all the words in,pic_cs-410_9_1_660.jpg
cs-410_9_1_171,cs-410,9,1,Probabilistic,"00:11:14,260","00:11:20,200",171,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=674,The only difference is that inside here,pic_cs-410_9_1_660.jpg
cs-410_9_1_172,cs-410,9,1,Probabilistic,"00:11:20,200","00:11:24,420",172,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=680,So you might have recalled before,pic_cs-410_9_1_660.jpg
cs-410_9_1_173,cs-410,9,1,Probabilistic,"00:11:25,420","00:11:30,610",173,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=685,But now we have this sum,pic_cs-410_9_1_660.jpg
cs-410_9_1_174,cs-410,9,1,Probabilistic,"00:11:30,610","00:11:34,800",174,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=690,And because of the mixture model we,pic_cs-410_9_1_660.jpg
cs-410_9_1_175,cs-410,9,1,Probabilistic,"00:11:34,800","00:11:37,640",175,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=694,choosing that particular,pic_cs-410_9_1_660.jpg
cs-410_9_1_176,cs-410,9,1,Probabilistic,"00:11:39,530","00:11:44,470",176,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=699,And so,pic_cs-410_9_1_660.jpg
cs-410_9_1_177,cs-410,9,1,Probabilistic,"00:11:44,470","00:11:49,800",177,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=704,by using a product over all the unique,pic_cs-410_9_1_660.jpg
cs-410_9_1_178,cs-410,9,1,Probabilistic,"00:11:49,800","00:11:52,878",178,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=709,having that product over all,pic_cs-410_9_1_660.jpg
cs-410_9_1_179,cs-410,9,1,Probabilistic,"00:11:52,878","00:11:57,582",179,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=712,And this form where we look at,pic_cs-410_9_1_660.jpg
cs-410_9_1_180,cs-410,9,1,Probabilistic,"00:11:57,582","00:12:04,720",180,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=717,a commutative that formed for computing,pic_cs-410_9_1_660.jpg
cs-410_9_1_181,cs-410,9,1,Probabilistic,"00:12:04,720","00:12:09,965",181,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=724,"And the maximum likelihood estimator is,",pic_cs-410_9_1_720.jpg
cs-410_9_1_182,cs-410,9,1,Probabilistic,"00:12:09,965","00:12:15,290",182,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=729,just to find the parameters that would,pic_cs-410_9_1_720.jpg
cs-410_9_1_183,cs-410,9,1,Probabilistic,"00:12:15,290","00:12:18,940",183,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=735,And the constraints here,pic_cs-410_9_1_720.jpg
cs-410_9_1_184,cs-410,9,1,Probabilistic,"00:12:18,940","00:12:24,125",184,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=738,One is what are probabilities in each,pic_cs-410_9_1_720.jpg
cs-410_9_1_185,cs-410,9,1,Probabilistic,"00:12:24,125","00:12:29,142",185,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=744,[INAUDIBLE] must sum to 1 the other is,pic_cs-410_9_1_720.jpg
cs-410_9_1_186,cs-410,9,1,Probabilistic,"00:12:29,142","00:12:35,343",186,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=749,the choice of each,pic_cs-410_9_1_720.jpg
cs-410_9_1_187,cs-410,9,1,Probabilistic,"00:12:35,343","00:12:39,799",187,https://www.coursera.org/learn/cs-410/lecture/EbbsO?t=755,[MUSIC],pic_cs-410_9_1_720.jpg
cs-410_9_2_1,cs-410,9,2,Probabilistic,"00:00:06,710","00:00:11,380",1,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=6,This lecture is about,pic_cs-410_9_2_0.jpg
cs-410_9_2_2,cs-410,9,2,Probabilistic,"00:00:11,380","00:00:13,890",2,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=11,"In this lecture, we're",pic_cs-410_9_2_0.jpg
cs-410_9_2_3,cs-410,9,2,Probabilistic,"00:00:13,890","00:00:15,990",3,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=13,discussing probabilistic,pic_cs-410_9_2_0.jpg
cs-410_9_2_4,cs-410,9,2,Probabilistic,"00:00:15,990","00:00:18,075",4,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=15,"In particular, we're going",pic_cs-410_9_2_0.jpg
cs-410_9_2_5,cs-410,9,2,Probabilistic,"00:00:18,075","00:00:21,490",5,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=18,estimate the parameters,pic_cs-410_9_2_0.jpg
cs-410_9_2_6,cs-410,9,2,Probabilistic,"00:00:21,920","00:00:24,780",6,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=21,So let's first look,pic_cs-410_9_2_0.jpg
cs-410_9_2_7,cs-410,9,2,Probabilistic,"00:00:24,780","00:00:26,760",7,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=24,"for using a mixture model,",pic_cs-410_9_2_0.jpg
cs-410_9_2_8,cs-410,9,2,Probabilistic,"00:00:26,760","00:00:29,040",8,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=26,and we hope to effect out,pic_cs-410_9_2_0.jpg
cs-410_9_2_9,cs-410,9,2,Probabilistic,"00:00:29,040","00:00:32,610",9,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=29,the background words from,pic_cs-410_9_2_0.jpg
cs-410_9_2_10,cs-410,9,2,Probabilistic,"00:00:32,610","00:00:35,565",10,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=32,So the idea is to assume that,pic_cs-410_9_2_0.jpg
cs-410_9_2_11,cs-410,9,2,Probabilistic,"00:00:35,565","00:00:39,510",11,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=35,the text data actually,pic_cs-410_9_2_0.jpg
cs-410_9_2_12,cs-410,9,2,Probabilistic,"00:00:39,510","00:00:44,910",12,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=39,One kind is from,pic_cs-410_9_2_0.jpg
cs-410_9_2_13,cs-410,9,2,Probabilistic,"00:00:44,910","00:00:48,555",13,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=44,"so the ""is"", ""we"" etc.",pic_cs-410_9_2_0.jpg
cs-410_9_2_14,cs-410,9,2,Probabilistic,"00:00:48,555","00:00:50,675",14,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=48,The other kind is from,pic_cs-410_9_2_0.jpg
cs-410_9_2_15,cs-410,9,2,Probabilistic,"00:00:50,675","00:00:54,900",15,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=50,our topic word distribution,pic_cs-410_9_2_0.jpg
cs-410_9_2_16,cs-410,9,2,Probabilistic,"00:00:55,210","00:00:58,565",16,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=55,So in order to solve,pic_cs-410_9_2_0.jpg
cs-410_9_2_17,cs-410,9,2,Probabilistic,"00:00:58,565","00:01:01,175",17,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=58,"factoring out background words,",pic_cs-410_9_2_0.jpg
cs-410_9_2_18,cs-410,9,2,Probabilistic,"00:01:01,175","00:01:05,555",18,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=61,we can set up our mixture,pic_cs-410_9_2_60.jpg
cs-410_9_2_19,cs-410,9,2,Probabilistic,"00:01:05,555","00:01:07,160",19,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=65,We are going to assume that,pic_cs-410_9_2_60.jpg
cs-410_9_2_20,cs-410,9,2,Probabilistic,"00:01:07,160","00:01:08,735",20,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=67,we already know the parameters,pic_cs-410_9_2_60.jpg
cs-410_9_2_21,cs-410,9,2,Probabilistic,"00:01:08,735","00:01:12,110",21,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=68,of all the values for,pic_cs-410_9_2_60.jpg
cs-410_9_2_22,cs-410,9,2,Probabilistic,"00:01:12,110","00:01:15,065",22,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=72,all the parameters in,pic_cs-410_9_2_60.jpg
cs-410_9_2_23,cs-410,9,2,Probabilistic,"00:01:15,065","00:01:19,610",23,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=75,the word distribution of,pic_cs-410_9_2_60.jpg
cs-410_9_2_24,cs-410,9,2,Probabilistic,"00:01:19,610","00:01:24,020",24,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=79,So this is a case of,pic_cs-410_9_2_60.jpg
cs-410_9_2_25,cs-410,9,2,Probabilistic,"00:01:24,020","00:01:26,300",25,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=84,some model so that we,pic_cs-410_9_2_60.jpg
cs-410_9_2_26,cs-410,9,2,Probabilistic,"00:01:26,300","00:01:29,450",26,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=86,embedded the unknown variables,pic_cs-410_9_2_60.jpg
cs-410_9_2_27,cs-410,9,2,Probabilistic,"00:01:29,450","00:01:31,175",27,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=89,but we're going to,pic_cs-410_9_2_60.jpg
cs-410_9_2_28,cs-410,9,2,Probabilistic,"00:01:31,175","00:01:32,810",28,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=91,We're going to assume,pic_cs-410_9_2_60.jpg
cs-410_9_2_29,cs-410,9,2,Probabilistic,"00:01:32,810","00:01:34,550",29,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=92,others and this is,pic_cs-410_9_2_60.jpg
cs-410_9_2_30,cs-410,9,2,Probabilistic,"00:01:34,550","00:01:35,795",30,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=94,a powerful way of,pic_cs-410_9_2_60.jpg
cs-410_9_2_31,cs-410,9,2,Probabilistic,"00:01:35,795","00:01:39,110",31,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=95,customizing a model,pic_cs-410_9_2_60.jpg
cs-410_9_2_32,cs-410,9,2,Probabilistic,"00:01:39,110","00:01:41,915",32,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=99,"Now you can imagine, we",pic_cs-410_9_2_60.jpg
cs-410_9_2_33,cs-410,9,2,Probabilistic,"00:01:41,915","00:01:44,915",33,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=101,we also don't know the,pic_cs-410_9_2_60.jpg
cs-410_9_2_34,cs-410,9,2,Probabilistic,"00:01:44,915","00:01:47,630",34,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=104,"but in this case,",pic_cs-410_9_2_60.jpg
cs-410_9_2_35,cs-410,9,2,Probabilistic,"00:01:47,630","00:01:51,635",35,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=107,precisely those high probability,pic_cs-410_9_2_60.jpg
cs-410_9_2_36,cs-410,9,2,Probabilistic,"00:01:51,635","00:01:56,600",36,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=111,So we assume the background,pic_cs-410_9_2_60.jpg
cs-410_9_2_37,cs-410,9,2,Probabilistic,"00:01:56,600","00:01:58,850",37,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=116,"The problem here is,",pic_cs-410_9_2_60.jpg
cs-410_9_2_38,cs-410,9,2,Probabilistic,"00:01:58,850","00:02:02,840",38,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=118,how can we adjust the Theta sub,pic_cs-410_9_2_60.jpg
cs-410_9_2_39,cs-410,9,2,Probabilistic,"00:02:02,840","00:02:05,270",39,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=122,the probability of,pic_cs-410_9_2_120.jpg
cs-410_9_2_40,cs-410,9,2,Probabilistic,"00:02:05,270","00:02:08,675",40,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=125,here and we assume all the,pic_cs-410_9_2_120.jpg
cs-410_9_2_41,cs-410,9,2,Probabilistic,"00:02:08,675","00:02:11,780",41,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=128,"Now, although we",pic_cs-410_9_2_120.jpg
cs-410_9_2_42,cs-410,9,2,Probabilistic,"00:02:11,780","00:02:13,040",42,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=131,heuristically to try to,pic_cs-410_9_2_120.jpg
cs-410_9_2_43,cs-410,9,2,Probabilistic,"00:02:13,040","00:02:15,395",43,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=133,factor out these,pic_cs-410_9_2_120.jpg
cs-410_9_2_44,cs-410,9,2,Probabilistic,"00:02:15,395","00:02:18,470",44,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=135,it's unclear whether if,pic_cs-410_9_2_120.jpg
cs-410_9_2_45,cs-410,9,2,Probabilistic,"00:02:18,470","00:02:20,860",45,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=138,we use maximum,pic_cs-410_9_2_120.jpg
cs-410_9_2_46,cs-410,9,2,Probabilistic,"00:02:20,860","00:02:24,500",46,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=140,we will actually end up having,pic_cs-410_9_2_120.jpg
cs-410_9_2_47,cs-410,9,2,Probabilistic,"00:02:24,500","00:02:27,320",47,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=144,"the common words like ""the"" will",pic_cs-410_9_2_120.jpg
cs-410_9_2_48,cs-410,9,2,Probabilistic,"00:02:27,320","00:02:30,470",48,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=147,be indeed having smaller,pic_cs-410_9_2_120.jpg
cs-410_9_2_49,cs-410,9,2,Probabilistic,"00:02:30,470","00:02:33,950",49,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=150,"So now, in this case,",pic_cs-410_9_2_120.jpg
cs-410_9_2_50,cs-410,9,2,Probabilistic,"00:02:33,950","00:02:37,420",50,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=153,it turns out that,pic_cs-410_9_2_120.jpg
cs-410_9_2_51,cs-410,9,2,Probabilistic,"00:02:37,420","00:02:41,000",51,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=157,When we set up the probabilistic,pic_cs-410_9_2_120.jpg
cs-410_9_2_52,cs-410,9,2,Probabilistic,"00:02:41,000","00:02:43,009",52,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=161,when we use maximum,pic_cs-410_9_2_120.jpg
cs-410_9_2_53,cs-410,9,2,Probabilistic,"00:02:43,009","00:02:47,180",53,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=163,we will end up having,pic_cs-410_9_2_120.jpg
cs-410_9_2_54,cs-410,9,2,Probabilistic,"00:02:47,180","00:02:48,950",54,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=167,the common words,pic_cs-410_9_2_120.jpg
cs-410_9_2_55,cs-410,9,2,Probabilistic,"00:02:48,950","00:02:53,075",55,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=168,out by the use of,pic_cs-410_9_2_120.jpg
cs-410_9_2_56,cs-410,9,2,Probabilistic,"00:02:53,075","00:02:56,599",56,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=173,"So to understand why this is so,",pic_cs-410_9_2_120.jpg
cs-410_9_2_57,cs-410,9,2,Probabilistic,"00:02:56,599","00:03:00,425",57,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=176,it's useful to examine,pic_cs-410_9_2_120.jpg
cs-410_9_2_58,cs-410,9,2,Probabilistic,"00:03:00,425","00:03:03,655",58,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=180,So we're going to look,pic_cs-410_9_2_180.jpg
cs-410_9_2_59,cs-410,9,2,Probabilistic,"00:03:03,655","00:03:05,165",59,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=183,In order to understand,pic_cs-410_9_2_180.jpg
cs-410_9_2_60,cs-410,9,2,Probabilistic,"00:03:05,165","00:03:08,450",60,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=185,some interesting behaviors,pic_cs-410_9_2_180.jpg
cs-410_9_2_61,cs-410,9,2,Probabilistic,"00:03:08,450","00:03:11,450",61,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=188,the observed patterns,pic_cs-410_9_2_180.jpg
cs-410_9_2_62,cs-410,9,2,Probabilistic,"00:03:11,450","00:03:15,005",62,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=191,generalizable to mixture,pic_cs-410_9_2_180.jpg
cs-410_9_2_63,cs-410,9,2,Probabilistic,"00:03:15,005","00:03:18,020",63,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=195,but it's much easier to,pic_cs-410_9_2_180.jpg
cs-410_9_2_64,cs-410,9,2,Probabilistic,"00:03:18,020","00:03:21,455",64,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=198,we use a very simple case,pic_cs-410_9_2_180.jpg
cs-410_9_2_65,cs-410,9,2,Probabilistic,"00:03:21,455","00:03:24,020",65,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=201,"So specifically in this case,",pic_cs-410_9_2_180.jpg
cs-410_9_2_66,cs-410,9,2,Probabilistic,"00:03:24,020","00:03:26,105",66,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=204,let's assume that,pic_cs-410_9_2_180.jpg
cs-410_9_2_67,cs-410,9,2,Probabilistic,"00:03:26,105","00:03:29,345",67,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=206,choosing each of the two models,pic_cs-410_9_2_180.jpg
cs-410_9_2_68,cs-410,9,2,Probabilistic,"00:03:29,345","00:03:30,650",68,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=209,So we're going to flip,pic_cs-410_9_2_180.jpg
cs-410_9_2_69,cs-410,9,2,Probabilistic,"00:03:30,650","00:03:33,860",69,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=210,a fair coin to decide,pic_cs-410_9_2_180.jpg
cs-410_9_2_70,cs-410,9,2,Probabilistic,"00:03:33,860","00:03:36,530",70,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=213,"Furthermore, we are going",pic_cs-410_9_2_180.jpg
cs-410_9_2_71,cs-410,9,2,Probabilistic,"00:03:36,530","00:03:39,140",71,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=216,"precisely to words,",pic_cs-410_9_2_180.jpg
cs-410_9_2_72,cs-410,9,2,Probabilistic,"00:03:39,140","00:03:44,015",72,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=219,"Obviously, this is",pic_cs-410_9_2_180.jpg
cs-410_9_2_73,cs-410,9,2,Probabilistic,"00:03:44,015","00:03:45,810",73,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=224,"of the actual text,",pic_cs-410_9_2_180.jpg
cs-410_9_2_74,cs-410,9,2,Probabilistic,"00:03:45,810","00:03:48,000",74,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=225,"but again, it is useful",pic_cs-410_9_2_180.jpg
cs-410_9_2_75,cs-410,9,2,Probabilistic,"00:03:48,000","00:03:52,865",75,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=228,to examine the behavior,pic_cs-410_9_2_180.jpg
cs-410_9_2_76,cs-410,9,2,Probabilistic,"00:03:52,865","00:03:55,710",76,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=232,"So we further assume that,",pic_cs-410_9_2_180.jpg
cs-410_9_2_77,cs-410,9,2,Probabilistic,"00:03:55,710","00:03:58,520",77,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=235,the background model gives,pic_cs-410_9_2_180.jpg
cs-410_9_2_78,cs-410,9,2,Probabilistic,"00:03:58,520","00:04:02,840",78,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=238,"the word ""the"" and ""text"" 0.1.",pic_cs-410_9_2_180.jpg
cs-410_9_2_79,cs-410,9,2,Probabilistic,"00:04:02,840","00:04:07,070",79,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=242,"Now, let's also assume that",pic_cs-410_9_2_240.jpg
cs-410_9_2_80,cs-410,9,2,Probabilistic,"00:04:07,070","00:04:09,995",80,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=247,The document has just two words,pic_cs-410_9_2_240.jpg
cs-410_9_2_81,cs-410,9,2,Probabilistic,"00:04:09,995","00:04:11,570",81,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=249,"So now, let's write down",pic_cs-410_9_2_240.jpg
cs-410_9_2_82,cs-410,9,2,Probabilistic,"00:04:11,570","00:04:13,610",82,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=251,the likelihood function,pic_cs-410_9_2_240.jpg
cs-410_9_2_83,cs-410,9,2,Probabilistic,"00:04:13,610","00:04:16,220",83,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=253,"First, what's the probability",pic_cs-410_9_2_240.jpg
cs-410_9_2_84,cs-410,9,2,Probabilistic,"00:04:16,220","00:04:18,995",84,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=256,"of ""text"" and what's the",pic_cs-410_9_2_240.jpg
cs-410_9_2_85,cs-410,9,2,Probabilistic,"00:04:18,995","00:04:20,920",85,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=258,"I hope by this point,",pic_cs-410_9_2_240.jpg
cs-410_9_2_86,cs-410,9,2,Probabilistic,"00:04:20,920","00:04:23,045",86,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=260,you will be able,pic_cs-410_9_2_240.jpg
cs-410_9_2_87,cs-410,9,2,Probabilistic,"00:04:23,045","00:04:26,615",87,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=263,"So the probability of ""text"" is",pic_cs-410_9_2_240.jpg
cs-410_9_2_88,cs-410,9,2,Probabilistic,"00:04:26,615","00:04:30,275",88,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=266,basically a sum of,pic_cs-410_9_2_240.jpg
cs-410_9_2_89,cs-410,9,2,Probabilistic,"00:04:30,275","00:04:32,240",89,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=270,corresponds to each of,pic_cs-410_9_2_240.jpg
cs-410_9_2_90,cs-410,9,2,Probabilistic,"00:04:32,240","00:04:34,760",90,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=272,the water distribution and,pic_cs-410_9_2_240.jpg
cs-410_9_2_91,cs-410,9,2,Probabilistic,"00:04:34,760","00:04:38,930",91,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=274,it accounts for the two ways,pic_cs-410_9_2_240.jpg
cs-410_9_2_92,cs-410,9,2,Probabilistic,"00:04:38,930","00:04:41,570",92,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=278,"Inside each case, we have",pic_cs-410_9_2_240.jpg
cs-410_9_2_93,cs-410,9,2,Probabilistic,"00:04:41,570","00:04:43,820",93,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=281,the probability of choosing,pic_cs-410_9_2_240.jpg
cs-410_9_2_94,cs-410,9,2,Probabilistic,"00:04:43,820","00:04:46,940",94,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=283,0.5 multiplied by the probability,pic_cs-410_9_2_240.jpg
cs-410_9_2_95,cs-410,9,2,Probabilistic,"00:04:46,940","00:04:49,895",95,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=286,"of observing ""text""",pic_cs-410_9_2_240.jpg
cs-410_9_2_96,cs-410,9,2,Probabilistic,"00:04:49,895","00:04:53,450",96,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=289,"Similarly, ""the"" would",pic_cs-410_9_2_240.jpg
cs-410_9_2_97,cs-410,9,2,Probabilistic,"00:04:53,450","00:04:55,250",97,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=293,the same form just as it,pic_cs-410_9_2_240.jpg
cs-410_9_2_98,cs-410,9,2,Probabilistic,"00:04:55,250","00:04:58,205",98,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=295,was different exactly,pic_cs-410_9_2_240.jpg
cs-410_9_2_99,cs-410,9,2,Probabilistic,"00:04:58,205","00:05:01,130",99,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=298,"So naturally,",pic_cs-410_9_2_240.jpg
cs-410_9_2_100,cs-410,9,2,Probabilistic,"00:05:01,130","00:05:03,170",100,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=301,is just the product of the two.,pic_cs-410_9_2_300.jpg
cs-410_9_2_101,cs-410,9,2,Probabilistic,"00:05:03,170","00:05:07,685",101,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=303,"So it's very easy to see that,",pic_cs-410_9_2_300.jpg
cs-410_9_2_102,cs-410,9,2,Probabilistic,"00:05:07,685","00:05:10,070",102,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=307,once you understand,pic_cs-410_9_2_300.jpg
cs-410_9_2_103,cs-410,9,2,Probabilistic,"00:05:10,070","00:05:12,470",103,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=310,each word and which,pic_cs-410_9_2_300.jpg
cs-410_9_2_104,cs-410,9,2,Probabilistic,"00:05:12,470","00:05:14,930",104,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=312,important to understand what's,pic_cs-410_9_2_300.jpg
cs-410_9_2_105,cs-410,9,2,Probabilistic,"00:05:14,930","00:05:16,505",105,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=314,exactly the probability of,pic_cs-410_9_2_300.jpg
cs-410_9_2_106,cs-410,9,2,Probabilistic,"00:05:16,505","00:05:19,190",106,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=316,observing each word from,pic_cs-410_9_2_300.jpg
cs-410_9_2_107,cs-410,9,2,Probabilistic,"00:05:19,190","00:05:21,860",107,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=319,"Now, the interesting",pic_cs-410_9_2_300.jpg
cs-410_9_2_108,cs-410,9,2,Probabilistic,"00:05:21,860","00:05:25,280",108,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=321,how can we then optimize,pic_cs-410_9_2_300.jpg
cs-410_9_2_109,cs-410,9,2,Probabilistic,"00:05:25,280","00:05:27,575",109,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=325,"Well, you will notice that,",pic_cs-410_9_2_300.jpg
cs-410_9_2_110,cs-410,9,2,Probabilistic,"00:05:27,575","00:05:29,165",110,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=327,there are only two variables.,pic_cs-410_9_2_300.jpg
cs-410_9_2_111,cs-410,9,2,Probabilistic,"00:05:29,165","00:05:31,385",111,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=329,They are precisely,pic_cs-410_9_2_300.jpg
cs-410_9_2_112,cs-410,9,2,Probabilistic,"00:05:31,385","00:05:33,800",112,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=331,of the two words,pic_cs-410_9_2_300.jpg
cs-410_9_2_113,cs-410,9,2,Probabilistic,"00:05:33,800","00:05:38,320",113,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=333,by Theta sub d. This is,pic_cs-410_9_2_300.jpg
cs-410_9_2_114,cs-410,9,2,Probabilistic,"00:05:38,320","00:05:40,430",114,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=338,all the other,pic_cs-410_9_2_300.jpg
cs-410_9_2_115,cs-410,9,2,Probabilistic,"00:05:40,430","00:05:45,300",115,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=340,"So now, the question is",pic_cs-410_9_2_300.jpg
cs-410_9_2_116,cs-410,9,2,Probabilistic,"00:05:45,300","00:05:46,610",116,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=345,So we have a simple expression,pic_cs-410_9_2_300.jpg
cs-410_9_2_117,cs-410,9,2,Probabilistic,"00:05:46,610","00:05:48,830",117,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=346,with two variables and we hope,pic_cs-410_9_2_300.jpg
cs-410_9_2_118,cs-410,9,2,Probabilistic,"00:05:48,830","00:05:50,990",118,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=348,to choose the values of,pic_cs-410_9_2_300.jpg
cs-410_9_2_119,cs-410,9,2,Probabilistic,"00:05:50,990","00:05:53,785",119,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=350,these two variables to,pic_cs-410_9_2_300.jpg
cs-410_9_2_120,cs-410,9,2,Probabilistic,"00:05:53,785","00:05:56,300",120,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=353,It's exercises that we have,pic_cs-410_9_2_300.jpg
cs-410_9_2_121,cs-410,9,2,Probabilistic,"00:05:56,300","00:05:59,480",121,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=356,seen some simple,pic_cs-410_9_2_300.jpg
cs-410_9_2_122,cs-410,9,2,Probabilistic,"00:05:59,480","00:06:03,200",122,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=359,and note that the two,pic_cs-410_9_2_300.jpg
cs-410_9_2_123,cs-410,9,2,Probabilistic,"00:06:03,200","00:06:05,465",123,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=363,So there's some constraint.,pic_cs-410_9_2_360.jpg
cs-410_9_2_124,cs-410,9,2,Probabilistic,"00:06:05,465","00:06:08,000",124,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=365,If there were,pic_cs-410_9_2_360.jpg
cs-410_9_2_125,cs-410,9,2,Probabilistic,"00:06:08,000","00:06:09,935",125,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=368,we will set both probabilities to,pic_cs-410_9_2_360.jpg
cs-410_9_2_126,cs-410,9,2,Probabilistic,"00:06:09,935","00:06:13,250",126,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=369,their maximum value which,pic_cs-410_9_2_360.jpg
cs-410_9_2_127,cs-410,9,2,Probabilistic,"00:06:13,250","00:06:14,675",127,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=373,but we can't do that,pic_cs-410_9_2_360.jpg
cs-410_9_2_128,cs-410,9,2,Probabilistic,"00:06:14,675","00:06:17,930",128,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=374,"because ""text"" and",pic_cs-410_9_2_360.jpg
cs-410_9_2_129,cs-410,9,2,Probabilistic,"00:06:17,930","00:06:21,025",129,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=377,We can't give those a,pic_cs-410_9_2_360.jpg
cs-410_9_2_130,cs-410,9,2,Probabilistic,"00:06:21,025","00:06:23,065",130,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=381,"So now the question is,",pic_cs-410_9_2_360.jpg
cs-410_9_2_131,cs-410,9,2,Probabilistic,"00:06:23,065","00:06:25,160",131,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=383,how should we allocate,pic_cs-410_9_2_360.jpg
cs-410_9_2_132,cs-410,9,2,Probabilistic,"00:06:25,160","00:06:27,995",132,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=385,the mass between the two words?,pic_cs-410_9_2_360.jpg
cs-410_9_2_133,cs-410,9,2,Probabilistic,"00:06:27,995","00:06:30,080",133,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=387,"Now, it will be useful to look at",pic_cs-410_9_2_360.jpg
cs-410_9_2_134,cs-410,9,2,Probabilistic,"00:06:30,080","00:06:33,175",134,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=390,this formula for,pic_cs-410_9_2_360.jpg
cs-410_9_2_135,cs-410,9,2,Probabilistic,"00:06:33,175","00:06:36,325",135,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=393,intuitively what,pic_cs-410_9_2_360.jpg
cs-410_9_2_136,cs-410,9,2,Probabilistic,"00:06:36,325","00:06:38,300",136,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=396,set these probabilities to,pic_cs-410_9_2_360.jpg
cs-410_9_2_137,cs-410,9,2,Probabilistic,"00:06:38,300","00:06:40,980",137,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=398,maximize the value,pic_cs-410_9_2_360.jpg
cs-410_9_2_138,cs-410,9,2,Probabilistic,"00:06:41,780","00:06:44,260",138,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=401,"If we look into this further,",pic_cs-410_9_2_360.jpg
cs-410_9_2_139,cs-410,9,2,Probabilistic,"00:06:44,260","00:06:46,250",139,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=404,then we'll see,pic_cs-410_9_2_360.jpg
cs-410_9_2_140,cs-410,9,2,Probabilistic,"00:06:46,250","00:06:50,120",140,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=406,of the two component,pic_cs-410_9_2_360.jpg
cs-410_9_2_141,cs-410,9,2,Probabilistic,"00:06:50,120","00:06:53,000",141,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=410,they will be,pic_cs-410_9_2_360.jpg
cs-410_9_2_142,cs-410,9,2,Probabilistic,"00:06:53,000","00:06:55,070",142,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=413,the probability of,pic_cs-410_9_2_360.jpg
cs-410_9_2_143,cs-410,9,2,Probabilistic,"00:06:55,070","00:06:57,440",143,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=415,is dictated by the maximum,pic_cs-410_9_2_360.jpg
cs-410_9_2_144,cs-410,9,2,Probabilistic,"00:06:57,440","00:07:00,815",144,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=417,but they're also,pic_cs-410_9_2_360.jpg
cs-410_9_2_145,cs-410,9,2,Probabilistic,"00:07:00,815","00:07:03,965",145,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=420,"In particular, they",pic_cs-410_9_2_420.jpg
cs-410_9_2_146,cs-410,9,2,Probabilistic,"00:07:03,965","00:07:06,650",146,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=423,the words and they,pic_cs-410_9_2_420.jpg
cs-410_9_2_147,cs-410,9,2,Probabilistic,"00:07:06,650","00:07:09,665",147,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=426,high probabilities on,pic_cs-410_9_2_420.jpg
cs-410_9_2_148,cs-410,9,2,Probabilistic,"00:07:09,665","00:07:11,585",148,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=429,this competition in some sense,pic_cs-410_9_2_420.jpg
cs-410_9_2_149,cs-410,9,2,Probabilistic,"00:07:11,585","00:07:14,240",149,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=431,or to gain advantage,pic_cs-410_9_2_420.jpg
cs-410_9_2_150,cs-410,9,2,Probabilistic,"00:07:14,240","00:07:17,150",150,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=434,"So again, looking at this",pic_cs-410_9_2_420.jpg
cs-410_9_2_151,cs-410,9,2,Probabilistic,"00:07:17,150","00:07:20,790",151,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=437,a constraint on,pic_cs-410_9_2_420.jpg
cs-410_9_2_152,cs-410,9,2,Probabilistic,"00:07:20,790","00:07:25,530",152,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=440,now if you look at,pic_cs-410_9_2_420.jpg
cs-410_9_2_153,cs-410,9,2,Probabilistic,"00:07:25,530","00:07:27,605",153,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=445,you might feel that,pic_cs-410_9_2_420.jpg
cs-410_9_2_154,cs-410,9,2,Probabilistic,"00:07:27,605","00:07:29,330",154,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=447,"the probability of ""text""",pic_cs-410_9_2_420.jpg
cs-410_9_2_155,cs-410,9,2,Probabilistic,"00:07:29,330","00:07:31,720",155,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=449,"to be somewhat larger than ""the"".",pic_cs-410_9_2_420.jpg
cs-410_9_2_156,cs-410,9,2,Probabilistic,"00:07:31,720","00:07:34,489",156,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=451,This intuition can,pic_cs-410_9_2_420.jpg
cs-410_9_2_157,cs-410,9,2,Probabilistic,"00:07:34,489","00:07:37,190",157,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=454,"by mathematical fact which is,",pic_cs-410_9_2_420.jpg
cs-410_9_2_158,cs-410,9,2,Probabilistic,"00:07:37,190","00:07:40,550",158,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=457,when the sum of,pic_cs-410_9_2_420.jpg
cs-410_9_2_159,cs-410,9,2,Probabilistic,"00:07:40,550","00:07:42,770",159,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=460,constant then the product of,pic_cs-410_9_2_420.jpg
cs-410_9_2_160,cs-410,9,2,Probabilistic,"00:07:42,770","00:07:45,205",160,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=462,them which is maximum,pic_cs-410_9_2_420.jpg
cs-410_9_2_161,cs-410,9,2,Probabilistic,"00:07:45,205","00:07:47,645",161,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=465,and this is a fact that,pic_cs-410_9_2_420.jpg
cs-410_9_2_162,cs-410,9,2,Probabilistic,"00:07:47,645","00:07:49,145",162,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=467,"Now, if we plug that in,",pic_cs-410_9_2_420.jpg
cs-410_9_2_163,cs-410,9,2,Probabilistic,"00:07:49,145","00:07:51,890",163,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=469,we will would mean,pic_cs-410_9_2_420.jpg
cs-410_9_2_164,cs-410,9,2,Probabilistic,"00:07:51,890","00:07:55,750",164,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=471,the two probabilities equal.,pic_cs-410_9_2_420.jpg
cs-410_9_2_165,cs-410,9,2,Probabilistic,"00:07:55,750","00:07:58,760",165,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=475,When we make them equal,pic_cs-410_9_2_420.jpg
cs-410_9_2_166,cs-410,9,2,Probabilistic,"00:07:58,760","00:08:01,790",166,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=478,the constraint that we can,pic_cs-410_9_2_420.jpg
cs-410_9_2_167,cs-410,9,2,Probabilistic,"00:08:01,790","00:08:04,730",167,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=481,and the solution is the,pic_cs-410_9_2_480.jpg
cs-410_9_2_168,cs-410,9,2,Probabilistic,"00:08:04,730","00:08:07,610",168,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=484,would be 0.9 and probability,pic_cs-410_9_2_480.jpg
cs-410_9_2_169,cs-410,9,2,Probabilistic,"00:08:07,610","00:08:09,125",169,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=487,"As you can see indeed,",pic_cs-410_9_2_480.jpg
cs-410_9_2_170,cs-410,9,2,Probabilistic,"00:08:09,125","00:08:11,840",170,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=489,the probability of text,pic_cs-410_9_2_480.jpg
cs-410_9_2_171,cs-410,9,2,Probabilistic,"00:08:11,840","00:08:14,120",171,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=491,"probability of ""the"" and",pic_cs-410_9_2_480.jpg
cs-410_9_2_172,cs-410,9,2,Probabilistic,"00:08:14,120","00:08:16,895",172,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=494,this is not the case when we,pic_cs-410_9_2_480.jpg
cs-410_9_2_173,cs-410,9,2,Probabilistic,"00:08:16,895","00:08:18,950",173,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=496,This is clearly because of,pic_cs-410_9_2_480.jpg
cs-410_9_2_174,cs-410,9,2,Probabilistic,"00:08:18,950","00:08:21,350",174,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=498,the use of the,pic_cs-410_9_2_480.jpg
cs-410_9_2_175,cs-410,9,2,Probabilistic,"00:08:21,350","00:08:23,645",175,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=501,assign a very high probability,pic_cs-410_9_2_480.jpg
cs-410_9_2_176,cs-410,9,2,Probabilistic,"00:08:23,645","00:08:26,250",176,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=503,"to ""the"" low",pic_cs-410_9_2_480.jpg
cs-410_9_2_177,cs-410,9,2,Probabilistic,"00:08:26,250","00:08:28,150",177,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=506,"If you look at the equation,",pic_cs-410_9_2_480.jpg
cs-410_9_2_178,cs-410,9,2,Probabilistic,"00:08:28,150","00:08:29,990",178,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=508,you will see obviously,pic_cs-410_9_2_480.jpg
cs-410_9_2_179,cs-410,9,2,Probabilistic,"00:08:29,990","00:08:34,180",179,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=509,some interaction of,pic_cs-410_9_2_480.jpg
cs-410_9_2_180,cs-410,9,2,Probabilistic,"00:08:34,180","00:08:37,100",180,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=514,"In particular, you will see in",pic_cs-410_9_2_480.jpg
cs-410_9_2_181,cs-410,9,2,Probabilistic,"00:08:37,100","00:08:39,695",181,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=517,order to make them equal and then,pic_cs-410_9_2_480.jpg
cs-410_9_2_182,cs-410,9,2,Probabilistic,"00:08:39,695","00:08:45,050",182,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=519,the probability assigned,pic_cs-410_9_2_480.jpg
cs-410_9_2_183,cs-410,9,2,Probabilistic,"00:08:45,050","00:08:47,360",183,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=525,be higher for a word that has,pic_cs-410_9_2_480.jpg
cs-410_9_2_184,cs-410,9,2,Probabilistic,"00:08:47,360","00:08:51,980",184,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=527,a smaller probability,pic_cs-410_9_2_480.jpg
cs-410_9_2_185,cs-410,9,2,Probabilistic,"00:08:52,290","00:08:56,420",185,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=532,This is obvious from,pic_cs-410_9_2_480.jpg
cs-410_9_2_186,cs-410,9,2,Probabilistic,"00:08:56,420","00:08:58,410",186,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=536,"because ""the"" background part is",pic_cs-410_9_2_480.jpg
cs-410_9_2_187,cs-410,9,2,Probabilistic,"00:08:58,410","00:09:00,500",187,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=538,"weak for ""text"" it's a small.",pic_cs-410_9_2_480.jpg
cs-410_9_2_188,cs-410,9,2,Probabilistic,"00:09:00,500","00:09:02,750",188,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=540,So in order to,pic_cs-410_9_2_540.jpg
cs-410_9_2_189,cs-410,9,2,Probabilistic,"00:09:02,750","00:09:05,620",189,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=542,we must make the probability,pic_cs-410_9_2_540.jpg
cs-410_9_2_190,cs-410,9,2,Probabilistic,"00:09:05,620","00:09:07,340",190,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=545,Theta sub d somewhat,pic_cs-410_9_2_540.jpg
cs-410_9_2_191,cs-410,9,2,Probabilistic,"00:09:07,340","00:09:10,955",191,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=547,larger so that the two sides,pic_cs-410_9_2_540.jpg
cs-410_9_2_192,cs-410,9,2,Probabilistic,"00:09:10,955","00:09:12,290",192,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=550,So this is in fact,pic_cs-410_9_2_540.jpg
cs-410_9_2_193,cs-410,9,2,Probabilistic,"00:09:12,290","00:09:17,075",193,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=552,a very general behavior,pic_cs-410_9_2_540.jpg
cs-410_9_2_194,cs-410,9,2,Probabilistic,"00:09:17,075","00:09:18,965",194,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=557,"That is, if one distribution",pic_cs-410_9_2_540.jpg
cs-410_9_2_195,cs-410,9,2,Probabilistic,"00:09:18,965","00:09:21,575",195,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=558,assigns a high probability,pic_cs-410_9_2_540.jpg
cs-410_9_2_196,cs-410,9,2,Probabilistic,"00:09:21,575","00:09:23,180",196,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=561,then the other distribution,pic_cs-410_9_2_540.jpg
cs-410_9_2_197,cs-410,9,2,Probabilistic,"00:09:23,180","00:09:25,240",197,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=563,would tend to do the opposite.,pic_cs-410_9_2_540.jpg
cs-410_9_2_198,cs-410,9,2,Probabilistic,"00:09:25,240","00:09:26,835",198,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=565,"Basically, it would discourage",pic_cs-410_9_2_540.jpg
cs-410_9_2_199,cs-410,9,2,Probabilistic,"00:09:26,835","00:09:28,080",199,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=566,other distributions to do the,pic_cs-410_9_2_540.jpg
cs-410_9_2_200,cs-410,9,2,Probabilistic,"00:09:28,080","00:09:31,655",200,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=568,same and this is to,pic_cs-410_9_2_540.jpg
cs-410_9_2_201,cs-410,9,2,Probabilistic,"00:09:31,655","00:09:34,340",201,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=571,we can account for all words.,pic_cs-410_9_2_540.jpg
cs-410_9_2_202,cs-410,9,2,Probabilistic,"00:09:34,340","00:09:36,150",202,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=574,"This also means that,",pic_cs-410_9_2_540.jpg
cs-410_9_2_203,cs-410,9,2,Probabilistic,"00:09:36,150","00:09:38,780",203,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=576,by using a background,pic_cs-410_9_2_540.jpg
cs-410_9_2_204,cs-410,9,2,Probabilistic,"00:09:38,780","00:09:41,515",204,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=578,fixed to assign high probabilities,pic_cs-410_9_2_540.jpg
cs-410_9_2_205,cs-410,9,2,Probabilistic,"00:09:41,515","00:09:43,689",205,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=581,we can indeed encourage,pic_cs-410_9_2_540.jpg
cs-410_9_2_206,cs-410,9,2,Probabilistic,"00:09:43,689","00:09:46,175",206,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=583,the unknown topic,pic_cs-410_9_2_540.jpg
cs-410_9_2_207,cs-410,9,2,Probabilistic,"00:09:46,175","00:09:49,855",207,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=586,assign smaller probabilities,pic_cs-410_9_2_540.jpg
cs-410_9_2_208,cs-410,9,2,Probabilistic,"00:09:49,855","00:09:54,125",208,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=589,"Instead, put more probability",pic_cs-410_9_2_540.jpg
cs-410_9_2_209,cs-410,9,2,Probabilistic,"00:09:54,125","00:09:56,360",209,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=594,that cannot be explained well by,pic_cs-410_9_2_540.jpg
cs-410_9_2_210,cs-410,9,2,Probabilistic,"00:09:56,360","00:09:58,865",210,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=596,the background,pic_cs-410_9_2_540.jpg
cs-410_9_2_211,cs-410,9,2,Probabilistic,"00:09:58,865","00:10:00,739",211,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=598,they have a very,pic_cs-410_9_2_540.jpg
cs-410_9_2_212,cs-410,9,2,Probabilistic,"00:10:00,739","00:10:04,100",212,https://www.coursera.org/learn/cs-410/lecture/QnGYn?t=600,from the background,pic_cs-410_9_2_600.jpg
cs-410_9_3_1,cs-410,9,3,Probabilistic,"00:00:06,170","00:00:08,340",1,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=6,This lecture is about,pic_cs-410_9_3_0.jpg
cs-410_9_3_2,cs-410,9,3,Probabilistic,"00:00:08,340","00:00:11,250",2,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=8,the expectation-maximization,pic_cs-410_9_3_0.jpg
cs-410_9_3_3,cs-410,9,3,Probabilistic,"00:00:11,250","00:00:13,380",3,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=11,also called the EM algorithm.,pic_cs-410_9_3_0.jpg
cs-410_9_3_4,cs-410,9,3,Probabilistic,"00:00:13,380","00:00:15,960",4,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=13,"In this lecture, we're",pic_cs-410_9_3_0.jpg
cs-410_9_3_5,cs-410,9,3,Probabilistic,"00:00:15,960","00:00:18,345",5,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=15,the discussion of,pic_cs-410_9_3_0.jpg
cs-410_9_3_6,cs-410,9,3,Probabilistic,"00:00:18,345","00:00:22,205",6,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=18,"In particular, we're going to",pic_cs-410_9_3_0.jpg
cs-410_9_3_7,cs-410,9,3,Probabilistic,"00:00:22,205","00:00:26,040",7,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=22,which is a family of,pic_cs-410_9_3_0.jpg
cs-410_9_3_8,cs-410,9,3,Probabilistic,"00:00:26,040","00:00:28,605",8,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=26,the maximum likelihood estimate,pic_cs-410_9_3_0.jpg
cs-410_9_3_9,cs-410,9,3,Probabilistic,"00:00:28,605","00:00:30,180",9,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=28,So this is now,pic_cs-410_9_3_0.jpg
cs-410_9_3_10,cs-410,9,3,Probabilistic,"00:00:30,180","00:00:33,330",10,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=30,familiar scenario of,pic_cs-410_9_3_0.jpg
cs-410_9_3_11,cs-410,9,3,Probabilistic,"00:00:33,330","00:00:34,620",11,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=33,"the mixture model, to try",pic_cs-410_9_3_0.jpg
cs-410_9_3_12,cs-410,9,3,Probabilistic,"00:00:34,620","00:00:36,225",12,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=34,to factor out,pic_cs-410_9_3_0.jpg
cs-410_9_3_13,cs-410,9,3,Probabilistic,"00:00:36,225","00:00:40,185",13,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=36,from one topic word,pic_cs-410_9_3_0.jpg
cs-410_9_3_14,cs-410,9,3,Probabilistic,"00:00:40,185","00:00:46,695",14,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=40,So we're interested in,pic_cs-410_9_3_0.jpg
cs-410_9_3_15,cs-410,9,3,Probabilistic,"00:00:46,695","00:00:49,715",15,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=46,and we're going to try to adjust,pic_cs-410_9_3_0.jpg
cs-410_9_3_16,cs-410,9,3,Probabilistic,"00:00:49,715","00:00:51,710",16,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=49,these probability values to,pic_cs-410_9_3_0.jpg
cs-410_9_3_17,cs-410,9,3,Probabilistic,"00:00:51,710","00:00:55,615",17,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=51,maximize the probability,pic_cs-410_9_3_0.jpg
cs-410_9_3_18,cs-410,9,3,Probabilistic,"00:00:55,615","00:00:57,050",18,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=55,Note that we assume that all,pic_cs-410_9_3_0.jpg
cs-410_9_3_19,cs-410,9,3,Probabilistic,"00:00:57,050","00:00:58,520",19,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=57,the other parameters are known.,pic_cs-410_9_3_0.jpg
cs-410_9_3_20,cs-410,9,3,Probabilistic,"00:00:58,520","00:01:00,875",20,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=58,So the only thing unknown is,pic_cs-410_9_3_0.jpg
cs-410_9_3_21,cs-410,9,3,Probabilistic,"00:01:00,875","00:01:04,430",21,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=60,the word probabilities,pic_cs-410_9_3_60.jpg
cs-410_9_3_22,cs-410,9,3,Probabilistic,"00:01:04,430","00:01:08,090",22,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=64,"In this lecture, we're",pic_cs-410_9_3_60.jpg
cs-410_9_3_23,cs-410,9,3,Probabilistic,"00:01:08,090","00:01:11,665",23,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=68,compute this maximum,pic_cs-410_9_3_60.jpg
cs-410_9_3_24,cs-410,9,3,Probabilistic,"00:01:11,665","00:01:15,275",24,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=71,"Now, let's start with the idea of",pic_cs-410_9_3_60.jpg
cs-410_9_3_25,cs-410,9,3,Probabilistic,"00:01:15,275","00:01:19,340",25,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=75,separating the words in,pic_cs-410_9_3_60.jpg
cs-410_9_3_26,cs-410,9,3,Probabilistic,"00:01:19,340","00:01:23,065",26,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=79,One group would be explained,pic_cs-410_9_3_60.jpg
cs-410_9_3_27,cs-410,9,3,Probabilistic,"00:01:23,065","00:01:25,490",27,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=83,The other group would,pic_cs-410_9_3_60.jpg
cs-410_9_3_28,cs-410,9,3,Probabilistic,"00:01:25,490","00:01:28,520",28,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=85,the unknown topic,pic_cs-410_9_3_60.jpg
cs-410_9_3_29,cs-410,9,3,Probabilistic,"00:01:28,520","00:01:31,835",29,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=88,"After all, this is",pic_cs-410_9_3_60.jpg
cs-410_9_3_30,cs-410,9,3,Probabilistic,"00:01:31,835","00:01:33,590",30,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=91,But suppose we actually,pic_cs-410_9_3_60.jpg
cs-410_9_3_31,cs-410,9,3,Probabilistic,"00:01:33,590","00:01:36,230",31,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=93,know which word is from,pic_cs-410_9_3_60.jpg
cs-410_9_3_32,cs-410,9,3,Probabilistic,"00:01:36,230","00:01:38,505",32,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=96,"So that would mean, for example,",pic_cs-410_9_3_60.jpg
cs-410_9_3_33,cs-410,9,3,Probabilistic,"00:01:38,505","00:01:40,910",33,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=98,"these words the, is,",pic_cs-410_9_3_60.jpg
cs-410_9_3_34,cs-410,9,3,Probabilistic,"00:01:40,910","00:01:42,740",34,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=100,and we are known to,pic_cs-410_9_3_60.jpg
cs-410_9_3_35,cs-410,9,3,Probabilistic,"00:01:42,740","00:01:45,275",35,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=102,be from this background,pic_cs-410_9_3_60.jpg
cs-410_9_3_36,cs-410,9,3,Probabilistic,"00:01:45,275","00:01:48,410",36,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=105,"On the other hand, the",pic_cs-410_9_3_60.jpg
cs-410_9_3_37,cs-410,9,3,Probabilistic,"00:01:48,410","00:01:50,840",37,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=108,clustering etc are known to be,pic_cs-410_9_3_60.jpg
cs-410_9_3_38,cs-410,9,3,Probabilistic,"00:01:50,840","00:01:53,790",38,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=110,from the topic word distribution.,pic_cs-410_9_3_60.jpg
cs-410_9_3_39,cs-410,9,3,Probabilistic,"00:01:53,790","00:01:55,250",39,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=113,"If you can see the color,",pic_cs-410_9_3_60.jpg
cs-410_9_3_40,cs-410,9,3,Probabilistic,"00:01:55,250","00:01:57,140",40,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=115,then these are shown in blue.,pic_cs-410_9_3_60.jpg
cs-410_9_3_41,cs-410,9,3,Probabilistic,"00:01:57,140","00:01:59,420",41,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=117,These blue words are then,pic_cs-410_9_3_60.jpg
cs-410_9_3_42,cs-410,9,3,Probabilistic,"00:01:59,420","00:02:02,585",42,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=119,assumed that to be from,pic_cs-410_9_3_60.jpg
cs-410_9_3_43,cs-410,9,3,Probabilistic,"00:02:02,585","00:02:07,010",43,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=122,If we already know how,pic_cs-410_9_3_120.jpg
cs-410_9_3_44,cs-410,9,3,Probabilistic,"00:02:07,010","00:02:08,750",44,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=127,then the problem of estimating,pic_cs-410_9_3_120.jpg
cs-410_9_3_45,cs-410,9,3,Probabilistic,"00:02:08,750","00:02:11,315",45,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=128,the word distribution,pic_cs-410_9_3_120.jpg
cs-410_9_3_46,cs-410,9,3,Probabilistic,"00:02:11,315","00:02:13,655",46,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=131,If you think about,pic_cs-410_9_3_120.jpg
cs-410_9_3_47,cs-410,9,3,Probabilistic,"00:02:13,655","00:02:16,205",47,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=133,"you'll realize that, well,",pic_cs-410_9_3_120.jpg
cs-410_9_3_48,cs-410,9,3,Probabilistic,"00:02:16,205","00:02:20,180",48,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=136,we can simply take,pic_cs-410_9_3_120.jpg
cs-410_9_3_49,cs-410,9,3,Probabilistic,"00:02:20,180","00:02:21,680",49,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=140,to be from this word,pic_cs-410_9_3_120.jpg
cs-410_9_3_50,cs-410,9,3,Probabilistic,"00:02:21,680","00:02:24,185",50,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=141,distribution theta sub d,pic_cs-410_9_3_120.jpg
cs-410_9_3_51,cs-410,9,3,Probabilistic,"00:02:24,185","00:02:25,940",51,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=144,So indeed this problem would be,pic_cs-410_9_3_120.jpg
cs-410_9_3_52,cs-410,9,3,Probabilistic,"00:02:25,940","00:02:27,920",52,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=145,very easy to solve if we had,pic_cs-410_9_3_120.jpg
cs-410_9_3_53,cs-410,9,3,Probabilistic,"00:02:27,920","00:02:30,320",53,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=147,known which words are from,pic_cs-410_9_3_120.jpg
cs-410_9_3_54,cs-410,9,3,Probabilistic,"00:02:30,320","00:02:33,110",54,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=150,"which a distribution precisely,",pic_cs-410_9_3_120.jpg
cs-410_9_3_55,cs-410,9,3,Probabilistic,"00:02:33,110","00:02:35,515",55,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=153,and this is in fact,pic_cs-410_9_3_120.jpg
cs-410_9_3_56,cs-410,9,3,Probabilistic,"00:02:35,515","00:02:38,525",56,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=155,making this model no,pic_cs-410_9_3_120.jpg
cs-410_9_3_57,cs-410,9,3,Probabilistic,"00:02:38,525","00:02:40,550",57,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=158,because we can already observe,pic_cs-410_9_3_120.jpg
cs-410_9_3_58,cs-410,9,3,Probabilistic,"00:02:40,550","00:02:42,560",58,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=160,which distribution has been,pic_cs-410_9_3_120.jpg
cs-410_9_3_59,cs-410,9,3,Probabilistic,"00:02:42,560","00:02:44,900",59,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=162,used to generate,pic_cs-410_9_3_120.jpg
cs-410_9_3_60,cs-410,9,3,Probabilistic,"00:02:44,900","00:02:46,730",60,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=164,So we actually go back to,pic_cs-410_9_3_120.jpg
cs-410_9_3_61,cs-410,9,3,Probabilistic,"00:02:46,730","00:02:50,755",61,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=166,the single word,pic_cs-410_9_3_120.jpg
cs-410_9_3_62,cs-410,9,3,Probabilistic,"00:02:50,755","00:02:52,290",62,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=170,In this case let's call,pic_cs-410_9_3_120.jpg
cs-410_9_3_63,cs-410,9,3,Probabilistic,"00:02:52,290","00:02:58,895",63,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=172,these words that are,pic_cs-410_9_3_120.jpg
cs-410_9_3_64,cs-410,9,3,Probabilistic,"00:02:58,895","00:03:01,115",64,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=178,"a pseudo document of d prime,",pic_cs-410_9_3_120.jpg
cs-410_9_3_65,cs-410,9,3,Probabilistic,"00:03:01,115","00:03:04,880",65,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=181,and now all we need to,pic_cs-410_9_3_180.jpg
cs-410_9_3_66,cs-410,9,3,Probabilistic,"00:03:04,880","00:03:10,325",66,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=184,these words counts,pic_cs-410_9_3_180.jpg
cs-410_9_3_67,cs-410,9,3,Probabilistic,"00:03:10,325","00:03:13,420",67,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=190,That's fairly straightforward.,pic_cs-410_9_3_180.jpg
cs-410_9_3_68,cs-410,9,3,Probabilistic,"00:03:13,420","00:03:17,000",68,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=193,It's just dictated by the,pic_cs-410_9_3_180.jpg
cs-410_9_3_69,cs-410,9,3,Probabilistic,"00:03:17,000","00:03:21,470",69,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=197,"Now, this idea however",pic_cs-410_9_3_180.jpg
cs-410_9_3_70,cs-410,9,3,Probabilistic,"00:03:21,470","00:03:23,630",70,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=201,we in practice don't really,pic_cs-410_9_3_180.jpg
cs-410_9_3_71,cs-410,9,3,Probabilistic,"00:03:23,630","00:03:26,224",71,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=203,know which word is from,pic_cs-410_9_3_180.jpg
cs-410_9_3_72,cs-410,9,3,Probabilistic,"00:03:26,224","00:03:29,690",72,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=206,but this gives us,pic_cs-410_9_3_180.jpg
cs-410_9_3_73,cs-410,9,3,Probabilistic,"00:03:29,690","00:03:33,775",73,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=209,can guess which word is,pic_cs-410_9_3_180.jpg
cs-410_9_3_74,cs-410,9,3,Probabilistic,"00:03:33,775","00:03:37,245",74,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=213,Specifically given,pic_cs-410_9_3_180.jpg
cs-410_9_3_75,cs-410,9,3,Probabilistic,"00:03:37,245","00:03:41,200",75,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=217,can we infer the distribution,pic_cs-410_9_3_180.jpg
cs-410_9_3_76,cs-410,9,3,Probabilistic,"00:03:41,200","00:03:44,000",76,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=221,So let's assume that we actually,pic_cs-410_9_3_180.jpg
cs-410_9_3_77,cs-410,9,3,Probabilistic,"00:03:44,000","00:03:47,390",77,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=224,know tentative probabilities for,pic_cs-410_9_3_180.jpg
cs-410_9_3_78,cs-410,9,3,Probabilistic,"00:03:47,390","00:03:49,940",78,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=227,these words in theta sub d.,pic_cs-410_9_3_180.jpg
cs-410_9_3_79,cs-410,9,3,Probabilistic,"00:03:49,940","00:03:51,980",79,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=229,So now all the parameters,pic_cs-410_9_3_180.jpg
cs-410_9_3_80,cs-410,9,3,Probabilistic,"00:03:51,980","00:03:54,305",80,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=231,"are known for this mixture model,",pic_cs-410_9_3_180.jpg
cs-410_9_3_81,cs-410,9,3,Probabilistic,"00:03:54,305","00:03:58,660",81,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=234,and now let's consider,pic_cs-410_9_3_180.jpg
cs-410_9_3_82,cs-410,9,3,Probabilistic,"00:03:58,660","00:04:02,875",82,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=238,"So the question is, do you",pic_cs-410_9_3_180.jpg
cs-410_9_3_83,cs-410,9,3,Probabilistic,"00:04:02,875","00:04:05,120",83,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=242,having been generated from,pic_cs-410_9_3_240.jpg
cs-410_9_3_84,cs-410,9,3,Probabilistic,"00:04:05,120","00:04:08,245",84,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=245,theta sub d or from,pic_cs-410_9_3_240.jpg
cs-410_9_3_85,cs-410,9,3,Probabilistic,"00:04:08,245","00:04:10,670",85,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=248,"So in other words,",pic_cs-410_9_3_240.jpg
cs-410_9_3_86,cs-410,9,3,Probabilistic,"00:04:10,670","00:04:14,225",86,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=250,which distribution has been,pic_cs-410_9_3_240.jpg
cs-410_9_3_87,cs-410,9,3,Probabilistic,"00:04:14,225","00:04:16,940",87,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=254,"Now, this inference process is",pic_cs-410_9_3_240.jpg
cs-410_9_3_88,cs-410,9,3,Probabilistic,"00:04:16,940","00:04:20,570",88,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=256,a typical Bayesian inference,pic_cs-410_9_3_240.jpg
cs-410_9_3_89,cs-410,9,3,Probabilistic,"00:04:20,570","00:04:24,650",89,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=260,some prior about,pic_cs-410_9_3_240.jpg
cs-410_9_3_90,cs-410,9,3,Probabilistic,"00:04:24,650","00:04:27,575",90,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=264,So can you see what,pic_cs-410_9_3_240.jpg
cs-410_9_3_91,cs-410,9,3,Probabilistic,"00:04:27,575","00:04:29,630",91,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=267,"Well, the prior here is",pic_cs-410_9_3_240.jpg
cs-410_9_3_92,cs-410,9,3,Probabilistic,"00:04:29,630","00:04:33,145",92,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=269,the probability of,pic_cs-410_9_3_240.jpg
cs-410_9_3_93,cs-410,9,3,Probabilistic,"00:04:33,145","00:04:37,925",93,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=273,So the prior is given by,pic_cs-410_9_3_240.jpg
cs-410_9_3_94,cs-410,9,3,Probabilistic,"00:04:37,925","00:04:39,405",94,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=277,"In this case, the prior",pic_cs-410_9_3_240.jpg
cs-410_9_3_95,cs-410,9,3,Probabilistic,"00:04:39,405","00:04:44,585",95,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=279,is saying that each model,pic_cs-410_9_3_240.jpg
cs-410_9_3_96,cs-410,9,3,Probabilistic,"00:04:44,585","00:04:47,900",96,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=284,but we can imagine perhaps a,pic_cs-410_9_3_240.jpg
cs-410_9_3_97,cs-410,9,3,Probabilistic,"00:04:47,900","00:04:52,100",97,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=287,So this is called a prior,pic_cs-410_9_3_240.jpg
cs-410_9_3_98,cs-410,9,3,Probabilistic,"00:04:52,100","00:04:54,170",98,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=292,which distribution has,pic_cs-410_9_3_240.jpg
cs-410_9_3_99,cs-410,9,3,Probabilistic,"00:04:54,170","00:04:57,740",99,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=294,a word before we even,pic_cs-410_9_3_240.jpg
cs-410_9_3_100,cs-410,9,3,Probabilistic,"00:04:57,740","00:05:00,895",100,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=297,So that's why we,pic_cs-410_9_3_240.jpg
cs-410_9_3_101,cs-410,9,3,Probabilistic,"00:05:00,895","00:05:03,600",101,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=300,"So if we don't observe the word,",pic_cs-410_9_3_300.jpg
cs-410_9_3_102,cs-410,9,3,Probabilistic,"00:05:03,600","00:05:05,705",102,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=303,we don't know what word,pic_cs-410_9_3_300.jpg
cs-410_9_3_103,cs-410,9,3,Probabilistic,"00:05:05,705","00:05:09,905",103,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=305,Our best guess is to say,pic_cs-410_9_3_300.jpg
cs-410_9_3_104,cs-410,9,3,Probabilistic,"00:05:09,905","00:05:12,595",104,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=309,All right. So it's,pic_cs-410_9_3_300.jpg
cs-410_9_3_105,cs-410,9,3,Probabilistic,"00:05:12,595","00:05:16,015",105,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=312,Now in Bayesian inference we,pic_cs-410_9_3_300.jpg
cs-410_9_3_106,cs-410,9,3,Probabilistic,"00:05:16,015","00:05:18,820",106,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=316,our belief after we have,pic_cs-410_9_3_300.jpg
cs-410_9_3_107,cs-410,9,3,Probabilistic,"00:05:18,820","00:05:20,230",107,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=318,So what is the evidence here?,pic_cs-410_9_3_300.jpg
cs-410_9_3_108,cs-410,9,3,Probabilistic,"00:05:20,230","00:05:24,295",108,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=320,"Well, the evidence",pic_cs-410_9_3_300.jpg
cs-410_9_3_109,cs-410,9,3,Probabilistic,"00:05:24,295","00:05:28,780",109,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=324,Now that we know we're,pic_cs-410_9_3_300.jpg
cs-410_9_3_110,cs-410,9,3,Probabilistic,"00:05:28,780","00:05:32,530",110,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=328,So text that can be,pic_cs-410_9_3_300.jpg
cs-410_9_3_111,cs-410,9,3,Probabilistic,"00:05:32,530","00:05:36,160",111,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=332,and if we use,pic_cs-410_9_3_300.jpg
cs-410_9_3_112,cs-410,9,3,Probabilistic,"00:05:36,160","00:05:41,424",112,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=336,Bayes rule to combine the,pic_cs-410_9_3_300.jpg
cs-410_9_3_113,cs-410,9,3,Probabilistic,"00:05:41,424","00:05:46,390",113,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=341,what we will end up,pic_cs-410_9_3_300.jpg
cs-410_9_3_114,cs-410,9,3,Probabilistic,"00:05:46,390","00:05:52,485",114,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=346,prior with the likelihood,pic_cs-410_9_3_300.jpg
cs-410_9_3_115,cs-410,9,3,Probabilistic,"00:05:52,485","00:05:54,650",115,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=352,which is basically,pic_cs-410_9_3_300.jpg
cs-410_9_3_116,cs-410,9,3,Probabilistic,"00:05:54,650","00:05:57,200",116,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=354,the word text from,pic_cs-410_9_3_300.jpg
cs-410_9_3_117,cs-410,9,3,Probabilistic,"00:05:57,200","00:06:00,830",117,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=357,We see that in both cases,pic_cs-410_9_3_300.jpg
cs-410_9_3_118,cs-410,9,3,Probabilistic,"00:06:00,830","00:06:03,880",118,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=360,Note that even in the background,pic_cs-410_9_3_360.jpg
cs-410_9_3_119,cs-410,9,3,Probabilistic,"00:06:03,880","00:06:06,900",119,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=363,it just has a very,pic_cs-410_9_3_360.jpg
cs-410_9_3_120,cs-410,9,3,Probabilistic,"00:06:06,970","00:06:12,805",120,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=366,So intuitively what would,pic_cs-410_9_3_360.jpg
cs-410_9_3_121,cs-410,9,3,Probabilistic,"00:06:12,805","00:06:15,195",121,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=372,"Now if you're like many others,",pic_cs-410_9_3_360.jpg
cs-410_9_3_122,cs-410,9,3,Probabilistic,"00:06:15,195","00:06:18,020",122,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=375,you are guess text,pic_cs-410_9_3_360.jpg
cs-410_9_3_123,cs-410,9,3,Probabilistic,"00:06:18,020","00:06:22,610",123,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=378,theta sub d. It's more likely,pic_cs-410_9_3_360.jpg
cs-410_9_3_124,cs-410,9,3,Probabilistic,"00:06:22,610","00:06:27,440",124,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=382,You will probably see that,pic_cs-410_9_3_360.jpg
cs-410_9_3_125,cs-410,9,3,Probabilistic,"00:06:27,440","00:06:32,720",125,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=387,a much higher probability,pic_cs-410_9_3_360.jpg
cs-410_9_3_126,cs-410,9,3,Probabilistic,"00:06:32,720","00:06:36,110",126,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=392,then by the background model,pic_cs-410_9_3_360.jpg
cs-410_9_3_127,cs-410,9,3,Probabilistic,"00:06:36,110","00:06:38,780",127,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=396,which has a very,pic_cs-410_9_3_360.jpg
cs-410_9_3_128,cs-410,9,3,Probabilistic,"00:06:38,780","00:06:41,380",128,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=398,"By this we're going to say, well,",pic_cs-410_9_3_360.jpg
cs-410_9_3_129,cs-410,9,3,Probabilistic,"00:06:41,380","00:06:43,670",129,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=401,text is more likely from,pic_cs-410_9_3_360.jpg
cs-410_9_3_130,cs-410,9,3,Probabilistic,"00:06:43,670","00:06:45,830",130,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=403,theta sub d. So you see,pic_cs-410_9_3_360.jpg
cs-410_9_3_131,cs-410,9,3,Probabilistic,"00:06:45,830","00:06:48,320",131,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=405,our guess of which,pic_cs-410_9_3_360.jpg
cs-410_9_3_132,cs-410,9,3,Probabilistic,"00:06:48,320","00:06:51,335",132,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=408,used to generate,pic_cs-410_9_3_360.jpg
cs-410_9_3_133,cs-410,9,3,Probabilistic,"00:06:51,335","00:06:54,170",133,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=411,how high the probability of,pic_cs-410_9_3_360.jpg
cs-410_9_3_134,cs-410,9,3,Probabilistic,"00:06:54,170","00:06:58,655",134,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=414,the text is in,pic_cs-410_9_3_360.jpg
cs-410_9_3_135,cs-410,9,3,Probabilistic,"00:06:58,655","00:07:01,190",135,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=418,"We can do, tend to guess",pic_cs-410_9_3_360.jpg
cs-410_9_3_136,cs-410,9,3,Probabilistic,"00:07:01,190","00:07:02,540",136,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=421,the distribution that gives us,pic_cs-410_9_3_420.jpg
cs-410_9_3_137,cs-410,9,3,Probabilistic,"00:07:02,540","00:07:04,355",137,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=422,"a word a higher probability,",pic_cs-410_9_3_420.jpg
cs-410_9_3_138,cs-410,9,3,Probabilistic,"00:07:04,355","00:07:08,565",138,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=424,and this is likely to,pic_cs-410_9_3_420.jpg
cs-410_9_3_139,cs-410,9,3,Probabilistic,"00:07:08,565","00:07:11,300",139,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=428,So we're going to choose,pic_cs-410_9_3_420.jpg
cs-410_9_3_140,cs-410,9,3,Probabilistic,"00:07:11,300","00:07:15,665",140,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=431,a word that has,pic_cs-410_9_3_420.jpg
cs-410_9_3_141,cs-410,9,3,Probabilistic,"00:07:15,665","00:07:17,825",141,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=435,"So in other words,",pic_cs-410_9_3_420.jpg
cs-410_9_3_142,cs-410,9,3,Probabilistic,"00:07:17,825","00:07:21,765",142,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=437,these two probabilities of,pic_cs-410_9_3_420.jpg
cs-410_9_3_143,cs-410,9,3,Probabilistic,"00:07:21,765","00:07:24,340",143,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=441,the word given by,pic_cs-410_9_3_420.jpg
cs-410_9_3_144,cs-410,9,3,Probabilistic,"00:07:24,340","00:07:30,440",144,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=444,But our guess must also,pic_cs-410_9_3_420.jpg
cs-410_9_3_145,cs-410,9,3,Probabilistic,"00:07:30,440","00:07:33,760",145,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=450,So we also need to,pic_cs-410_9_3_420.jpg
cs-410_9_3_146,cs-410,9,3,Probabilistic,"00:07:33,760","00:07:38,660",146,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=453,Why? Because imagine if we,pic_cs-410_9_3_420.jpg
cs-410_9_3_147,cs-410,9,3,Probabilistic,"00:07:38,660","00:07:41,210",147,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=458,we're going to say,pic_cs-410_9_3_420.jpg
cs-410_9_3_148,cs-410,9,3,Probabilistic,"00:07:41,210","00:07:44,210",148,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=461,a background model is,pic_cs-410_9_3_420.jpg
cs-410_9_3_149,cs-410,9,3,Probabilistic,"00:07:44,210","00:07:47,450",149,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=464,"Now, if you have that kind",pic_cs-410_9_3_420.jpg
cs-410_9_3_150,cs-410,9,3,Probabilistic,"00:07:47,450","00:07:49,370",150,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=467,then that would,pic_cs-410_9_3_420.jpg
cs-410_9_3_151,cs-410,9,3,Probabilistic,"00:07:49,370","00:07:51,665",151,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=469,"You might think,",pic_cs-410_9_3_420.jpg
cs-410_9_3_152,cs-410,9,3,Probabilistic,"00:07:51,665","00:07:54,740",152,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=471,maybe text could have been,pic_cs-410_9_3_420.jpg
cs-410_9_3_153,cs-410,9,3,Probabilistic,"00:07:54,740","00:07:57,890",153,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=474,Although the probability,pic_cs-410_9_3_420.jpg
cs-410_9_3_154,cs-410,9,3,Probabilistic,"00:07:57,890","00:08:00,520",154,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=477,the prior is very high.,pic_cs-410_9_3_420.jpg
cs-410_9_3_155,cs-410,9,3,Probabilistic,"00:08:00,520","00:08:03,450",155,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=480,"So in the end, we have",pic_cs-410_9_3_480.jpg
cs-410_9_3_156,cs-410,9,3,Probabilistic,"00:08:03,450","00:08:05,880",156,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=483,and the base formula provides us,pic_cs-410_9_3_480.jpg
cs-410_9_3_157,cs-410,9,3,Probabilistic,"00:08:05,880","00:08:09,135",157,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=485,a solid and principled way,pic_cs-410_9_3_480.jpg
cs-410_9_3_158,cs-410,9,3,Probabilistic,"00:08:09,135","00:08:12,890",158,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=489,of making this kind of,pic_cs-410_9_3_480.jpg
cs-410_9_3_159,cs-410,9,3,Probabilistic,"00:08:12,890","00:08:15,305",159,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=492,"So more specifically,",pic_cs-410_9_3_480.jpg
cs-410_9_3_160,cs-410,9,3,Probabilistic,"00:08:15,305","00:08:17,120",160,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=495,let's think about,pic_cs-410_9_3_480.jpg
cs-410_9_3_161,cs-410,9,3,Probabilistic,"00:08:17,120","00:08:19,220",161,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=497,this word has been generated in,pic_cs-410_9_3_480.jpg
cs-410_9_3_162,cs-410,9,3,Probabilistic,"00:08:19,220","00:08:22,535",162,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=499,"fact from from theta sub d. Well,",pic_cs-410_9_3_480.jpg
cs-410_9_3_163,cs-410,9,3,Probabilistic,"00:08:22,535","00:08:24,920",163,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=502,in order for texts,pic_cs-410_9_3_480.jpg
cs-410_9_3_164,cs-410,9,3,Probabilistic,"00:08:24,920","00:08:27,095",164,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=504,theta sub d two things,pic_cs-410_9_3_480.jpg
cs-410_9_3_165,cs-410,9,3,Probabilistic,"00:08:27,095","00:08:31,275",165,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=507,"First, the theta sub d",pic_cs-410_9_3_480.jpg
cs-410_9_3_166,cs-410,9,3,Probabilistic,"00:08:31,275","00:08:34,250",166,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=511,so we have the selection,pic_cs-410_9_3_480.jpg
cs-410_9_3_167,cs-410,9,3,Probabilistic,"00:08:34,250","00:08:37,145",167,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=514,"Secondly, we also have to",pic_cs-410_9_3_480.jpg
cs-410_9_3_168,cs-410,9,3,Probabilistic,"00:08:37,145","00:08:40,640",168,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=517,actually have observed text,pic_cs-410_9_3_480.jpg
cs-410_9_3_169,cs-410,9,3,Probabilistic,"00:08:40,640","00:08:43,160",169,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=520,So when we multiply,pic_cs-410_9_3_480.jpg
cs-410_9_3_170,cs-410,9,3,Probabilistic,"00:08:43,160","00:08:46,940",170,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=523,we get the probability,pic_cs-410_9_3_480.jpg
cs-410_9_3_171,cs-410,9,3,Probabilistic,"00:08:46,940","00:08:51,230",171,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=526,fact been generated from,pic_cs-410_9_3_480.jpg
cs-410_9_3_172,cs-410,9,3,Probabilistic,"00:08:51,230","00:08:54,005",172,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=531,"for the background model,",pic_cs-410_9_3_480.jpg
cs-410_9_3_173,cs-410,9,3,Probabilistic,"00:08:54,005","00:08:56,630",173,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=534,the probability of generating,pic_cs-410_9_3_480.jpg
cs-410_9_3_174,cs-410,9,3,Probabilistic,"00:08:56,630","00:09:00,075",174,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=536,text is another product,pic_cs-410_9_3_480.jpg
cs-410_9_3_175,cs-410,9,3,Probabilistic,"00:09:00,075","00:09:01,700",175,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=540,"Now, we also introduced",pic_cs-410_9_3_540.jpg
cs-410_9_3_176,cs-410,9,3,Probabilistic,"00:09:01,700","00:09:05,665",176,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=541,the latent variable,pic_cs-410_9_3_540.jpg
cs-410_9_3_177,cs-410,9,3,Probabilistic,"00:09:05,665","00:09:11,625",177,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=545,whether the word is from,pic_cs-410_9_3_540.jpg
cs-410_9_3_178,cs-410,9,3,Probabilistic,"00:09:11,625","00:09:13,470",178,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=551,"When z is zero,",pic_cs-410_9_3_540.jpg
cs-410_9_3_179,cs-410,9,3,Probabilistic,"00:09:13,470","00:09:18,350",179,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=553,it means it's from the topic,pic_cs-410_9_3_540.jpg
cs-410_9_3_180,cs-410,9,3,Probabilistic,"00:09:18,350","00:09:21,515",180,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=558,it means it's from,pic_cs-410_9_3_540.jpg
cs-410_9_3_181,cs-410,9,3,Probabilistic,"00:09:21,515","00:09:23,030",181,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=561,So now we have,pic_cs-410_9_3_540.jpg
cs-410_9_3_182,cs-410,9,3,Probabilistic,"00:09:23,030","00:09:26,660",182,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=563,the probability that text,pic_cs-410_9_3_540.jpg
cs-410_9_3_183,cs-410,9,3,Probabilistic,"00:09:26,660","00:09:29,600",183,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=566,Then we can simply normalize,pic_cs-410_9_3_540.jpg
cs-410_9_3_184,cs-410,9,3,Probabilistic,"00:09:29,600","00:09:33,605",184,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=569,them to have an estimate,pic_cs-410_9_3_540.jpg
cs-410_9_3_185,cs-410,9,3,Probabilistic,"00:09:33,605","00:09:36,185",185,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=573,that the word text is,pic_cs-410_9_3_540.jpg
cs-410_9_3_186,cs-410,9,3,Probabilistic,"00:09:36,185","00:09:42,135",186,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=576,from theta sub d or,pic_cs-410_9_3_540.jpg
cs-410_9_3_187,cs-410,9,3,Probabilistic,"00:09:42,135","00:09:46,100",187,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=582,"Then equivalently, the",pic_cs-410_9_3_540.jpg
cs-410_9_3_188,cs-410,9,3,Probabilistic,"00:09:46,100","00:09:50,905",188,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=586,zero given that,pic_cs-410_9_3_540.jpg
cs-410_9_3_189,cs-410,9,3,Probabilistic,"00:09:50,905","00:09:55,490",189,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=590,So this is application,pic_cs-410_9_3_540.jpg
cs-410_9_3_190,cs-410,9,3,Probabilistic,"00:09:55,490","00:09:59,419",190,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=595,But this step is very,pic_cs-410_9_3_540.jpg
cs-410_9_3_191,cs-410,9,3,Probabilistic,"00:09:59,419","00:10:04,060",191,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=599,the EM algorithm because,pic_cs-410_9_3_540.jpg
cs-410_9_3_192,cs-410,9,3,Probabilistic,"00:10:04,060","00:10:07,250",192,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=604,then we would be able to first,pic_cs-410_9_3_600.jpg
cs-410_9_3_193,cs-410,9,3,Probabilistic,"00:10:07,250","00:10:11,830",193,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=607,initialize the parameter values,pic_cs-410_9_3_600.jpg
cs-410_9_3_194,cs-410,9,3,Probabilistic,"00:10:11,830","00:10:17,155",194,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=611,and then we're going to take,pic_cs-410_9_3_600.jpg
cs-410_9_3_195,cs-410,9,3,Probabilistic,"00:10:17,155","00:10:20,825",195,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=617,Which distributing has been,pic_cs-410_9_3_600.jpg
cs-410_9_3_196,cs-410,9,3,Probabilistic,"00:10:20,825","00:10:23,960",196,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=620,and the initialized,pic_cs-410_9_3_600.jpg
cs-410_9_3_197,cs-410,9,3,Probabilistic,"00:10:23,960","00:10:25,310",197,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=623,allow us to have a complete,pic_cs-410_9_3_600.jpg
cs-410_9_3_198,cs-410,9,3,Probabilistic,"00:10:25,310","00:10:27,035",198,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=625,specification of,pic_cs-410_9_3_600.jpg
cs-410_9_3_199,cs-410,9,3,Probabilistic,"00:10:27,035","00:10:31,145",199,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=627,which further allows us to,pic_cs-410_9_3_600.jpg
cs-410_9_3_200,cs-410,9,3,Probabilistic,"00:10:31,145","00:10:36,815",200,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=631,which distribution is more,pic_cs-410_9_3_600.jpg
cs-410_9_3_201,cs-410,9,3,Probabilistic,"00:10:36,815","00:10:40,700",201,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=636,This prediction,pic_cs-410_9_3_600.jpg
cs-410_9_3_202,cs-410,9,3,Probabilistic,"00:10:40,700","00:10:44,690",202,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=640,to separate the words from,pic_cs-410_9_3_600.jpg
cs-410_9_3_203,cs-410,9,3,Probabilistic,"00:10:44,690","00:10:47,115",203,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=644,Although we can't,pic_cs-410_9_3_600.jpg
cs-410_9_3_204,cs-410,9,3,Probabilistic,"00:10:47,115","00:10:53,250",204,https://www.coursera.org/learn/cs-410/lecture/cMSgR?t=647,but we can separate them,pic_cs-410_9_3_600.jpg
cs-410_9_4_1,cs-410,9,4,Probabilistic,"00:00:00,012","00:00:08,224",1,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=0,[SOUND],pic_cs-410_9_4_0.jpg
cs-410_9_4_2,cs-410,9,4,Probabilistic,"00:00:08,224","00:00:12,538",2,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=8,this is indeed a general idea of,pic_cs-410_9_4_0.jpg
cs-410_9_4_3,cs-410,9,4,Probabilistic,"00:00:12,538","00:00:13,310",3,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=12,Algorithm.,pic_cs-410_9_4_0.jpg
cs-410_9_4_4,cs-410,9,4,Probabilistic,"00:00:14,640","00:00:19,210",4,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=14,So in all the EM algorithms we,pic_cs-410_9_4_0.jpg
cs-410_9_4_5,cs-410,9,4,Probabilistic,"00:00:19,210","00:00:21,970",5,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=19,to help us solve the problem more easily.,pic_cs-410_9_4_0.jpg
cs-410_9_4_6,cs-410,9,4,Probabilistic,"00:00:21,970","00:00:25,453",6,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=21,In our case the hidden variable,pic_cs-410_9_4_0.jpg
cs-410_9_4_7,cs-410,9,4,Probabilistic,"00:00:25,453","00:00:27,203",7,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=25,each occurrence of a word.,pic_cs-410_9_4_0.jpg
cs-410_9_4_8,cs-410,9,4,Probabilistic,"00:00:27,203","00:00:32,020",8,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=27,And this binary variable would,pic_cs-410_9_4_0.jpg
cs-410_9_4_9,cs-410,9,4,Probabilistic,"00:00:32,020","00:00:35,144",9,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=32,been generated from 0 sub d or 0 sub p.,pic_cs-410_9_4_0.jpg
cs-410_9_4_10,cs-410,9,4,Probabilistic,"00:00:35,144","00:00:38,420",10,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=35,And here we show some possible,pic_cs-410_9_4_0.jpg
cs-410_9_4_11,cs-410,9,4,Probabilistic,"00:00:38,420","00:00:43,470",11,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=38,"For example, for the it's from background,",pic_cs-410_9_4_0.jpg
cs-410_9_4_12,cs-410,9,4,Probabilistic,"00:00:43,470","00:00:45,105",12,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=43,And text on the other hand.,pic_cs-410_9_4_0.jpg
cs-410_9_4_13,cs-410,9,4,Probabilistic,"00:00:45,105","00:00:52,040",13,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=45,Is from the topic then it's zero for,pic_cs-410_9_4_0.jpg
cs-410_9_4_14,cs-410,9,4,Probabilistic,"00:00:53,260","00:00:58,915",14,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=53,"Now, of course, we don't observe these z",pic_cs-410_9_4_0.jpg
cs-410_9_4_15,cs-410,9,4,Probabilistic,"00:00:58,915","00:01:01,875",15,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=58,Values of z attaching to other words.,pic_cs-410_9_4_0.jpg
cs-410_9_4_16,cs-410,9,4,Probabilistic,"00:01:02,905","00:01:04,975",16,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=62,And that's why we call,pic_cs-410_9_4_60.jpg
cs-410_9_4_17,cs-410,9,4,Probabilistic,"00:01:06,135","00:01:08,905",17,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=66,"Now, the idea that we",pic_cs-410_9_4_60.jpg
cs-410_9_4_18,cs-410,9,4,Probabilistic,"00:01:08,905","00:01:12,930",18,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=68,predicting the word distribution that,pic_cs-410_9_4_60.jpg
cs-410_9_4_19,cs-410,9,4,Probabilistic,"00:01:12,930","00:01:18,840",19,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=72,"is it a predictor,",pic_cs-410_9_4_60.jpg
cs-410_9_4_20,cs-410,9,4,Probabilistic,"00:01:18,840","00:01:25,080",20,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=78,"And, so, the EM algorithm then,",pic_cs-410_9_4_60.jpg
cs-410_9_4_21,cs-410,9,4,Probabilistic,"00:01:25,080","00:01:30,060",21,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=85,"First, we'll initialize all",pic_cs-410_9_4_60.jpg
cs-410_9_4_22,cs-410,9,4,Probabilistic,"00:01:30,060","00:01:34,960",22,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=90,"In our case,",pic_cs-410_9_4_60.jpg
cs-410_9_4_23,cs-410,9,4,Probabilistic,"00:01:34,960","00:01:37,840",23,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=94,"of a word, given by theta sub d.",pic_cs-410_9_4_60.jpg
cs-410_9_4_24,cs-410,9,4,Probabilistic,"00:01:37,840","00:01:39,680",24,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=97,So this is an initial addition stage.,pic_cs-410_9_4_60.jpg
cs-410_9_4_25,cs-410,9,4,Probabilistic,"00:01:39,680","00:01:44,150",25,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=99,These initialized values would allow,pic_cs-410_9_4_60.jpg
cs-410_9_4_26,cs-410,9,4,Probabilistic,"00:01:44,150","00:01:48,510",26,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=104,"of these z values, so",pic_cs-410_9_4_60.jpg
cs-410_9_4_27,cs-410,9,4,Probabilistic,"00:01:48,510","00:01:53,580",27,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=108,We can't say for sure whether,pic_cs-410_9_4_60.jpg
cs-410_9_4_28,cs-410,9,4,Probabilistic,"00:01:53,580","00:01:55,090",28,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=113,But we can have our guess.,pic_cs-410_9_4_60.jpg
cs-410_9_4_29,cs-410,9,4,Probabilistic,"00:01:55,090","00:01:57,620",29,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=115,This is given by this formula.,pic_cs-410_9_4_60.jpg
cs-410_9_4_30,cs-410,9,4,Probabilistic,"00:01:57,620","00:01:59,710",30,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=117,It's called an E-step.,pic_cs-410_9_4_60.jpg
cs-410_9_4_31,cs-410,9,4,Probabilistic,"00:01:59,710","00:02:06,520",31,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=119,And so the algorithm would then try to,pic_cs-410_9_4_60.jpg
cs-410_9_4_32,cs-410,9,4,Probabilistic,"00:02:06,520","00:02:12,190",32,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=126,"After that, it would then invoke",pic_cs-410_9_4_120.jpg
cs-410_9_4_33,cs-410,9,4,Probabilistic,"00:02:12,190","00:02:17,490",33,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=132,In this step we simply take advantage,pic_cs-410_9_4_120.jpg
cs-410_9_4_34,cs-410,9,4,Probabilistic,"00:02:17,490","00:02:22,825",34,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=137,then just group words that are in,pic_cs-410_9_4_120.jpg
cs-410_9_4_35,cs-410,9,4,Probabilistic,"00:02:22,825","00:02:26,315",35,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=142,from that ground including this as well.,pic_cs-410_9_4_120.jpg
cs-410_9_4_36,cs-410,9,4,Probabilistic,"00:02:27,585","00:02:32,865",36,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=147,We can then normalize the count,pic_cs-410_9_4_120.jpg
cs-410_9_4_37,cs-410,9,4,Probabilistic,"00:02:32,865","00:02:35,479",37,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=152,to revise our estimate of the parameters.,pic_cs-410_9_4_120.jpg
cs-410_9_4_38,cs-410,9,4,Probabilistic,"00:02:36,590","00:02:42,310",38,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=156,So let me also illustrate,pic_cs-410_9_4_120.jpg
cs-410_9_4_39,cs-410,9,4,Probabilistic,"00:02:42,310","00:02:46,760",39,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=162,that are believed to have,pic_cs-410_9_4_120.jpg
cs-410_9_4_40,cs-410,9,4,Probabilistic,"00:02:46,760","00:02:50,010",40,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=166,"that's text, mining algorithm,",pic_cs-410_9_4_120.jpg
cs-410_9_4_41,cs-410,9,4,Probabilistic,"00:02:51,760","00:02:55,718",41,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=171,And we group them together to help us,pic_cs-410_9_4_120.jpg
cs-410_9_4_42,cs-410,9,4,Probabilistic,"00:02:55,718","00:03:01,170",42,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=175,re-estimate the parameters,pic_cs-410_9_4_120.jpg
cs-410_9_4_43,cs-410,9,4,Probabilistic,"00:03:01,170","00:03:05,120",43,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=181,So these will help us,pic_cs-410_9_4_180.jpg
cs-410_9_4_44,cs-410,9,4,Probabilistic,"00:03:06,170","00:03:09,970",44,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=186,Note that before we just set,pic_cs-410_9_4_180.jpg
cs-410_9_4_45,cs-410,9,4,Probabilistic,"00:03:09,970","00:03:15,670",45,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=189,"But with this guess, we will have",pic_cs-410_9_4_180.jpg
cs-410_9_4_46,cs-410,9,4,Probabilistic,"00:03:15,670","00:03:18,740",46,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=195,"Of course, we don't know exactly",pic_cs-410_9_4_180.jpg
cs-410_9_4_47,cs-410,9,4,Probabilistic,"00:03:18,740","00:03:24,850",47,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=198,So we're not going to really,pic_cs-410_9_4_180.jpg
cs-410_9_4_48,cs-410,9,4,Probabilistic,"00:03:24,850","00:03:26,800",48,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=204,But rather we're going to,pic_cs-410_9_4_180.jpg
cs-410_9_4_49,cs-410,9,4,Probabilistic,"00:03:26,800","00:03:27,980",49,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=206,And this is what happened here.,pic_cs-410_9_4_180.jpg
cs-410_9_4_50,cs-410,9,4,Probabilistic,"00:03:29,150","00:03:34,420",50,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=209,So we're going to adjust the count by,pic_cs-410_9_4_180.jpg
cs-410_9_4_51,cs-410,9,4,Probabilistic,"00:03:34,420","00:03:38,410",51,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=214,this word has been generated,pic_cs-410_9_4_180.jpg
cs-410_9_4_52,cs-410,9,4,Probabilistic,"00:03:39,840","00:03:42,580",52,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=219,"And you can see this,",pic_cs-410_9_4_180.jpg
cs-410_9_4_53,cs-410,9,4,Probabilistic,"00:03:42,580","00:03:46,630",53,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=222,"Well, this has come from here, right?",pic_cs-410_9_4_180.jpg
cs-410_9_4_54,cs-410,9,4,Probabilistic,"00:03:46,630","00:03:48,120",54,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=226,From the E-step.,pic_cs-410_9_4_180.jpg
cs-410_9_4_55,cs-410,9,4,Probabilistic,"00:03:48,120","00:03:52,472",55,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=228,So the EM Algorithm would,pic_cs-410_9_4_180.jpg
cs-410_9_4_56,cs-410,9,4,Probabilistic,"00:03:52,472","00:03:57,375",56,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=232,estimate of parameters by using,pic_cs-410_9_4_180.jpg
cs-410_9_4_57,cs-410,9,4,Probabilistic,"00:03:57,375","00:04:02,458",57,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=237,The E-step is to augment the data,pic_cs-410_9_4_180.jpg
cs-410_9_4_58,cs-410,9,4,Probabilistic,"00:04:02,458","00:04:05,910",58,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=242,And the M-step is to take advantage,pic_cs-410_9_4_240.jpg
cs-410_9_4_59,cs-410,9,4,Probabilistic,"00:04:05,910","00:04:08,660",59,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=245,of the additional information,pic_cs-410_9_4_240.jpg
cs-410_9_4_60,cs-410,9,4,Probabilistic,"00:04:08,660","00:04:13,467",60,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=248,To split the data accounts and,pic_cs-410_9_4_240.jpg
cs-410_9_4_61,cs-410,9,4,Probabilistic,"00:04:13,467","00:04:17,870",61,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=253,re-estimate our parameter.,pic_cs-410_9_4_240.jpg
cs-410_9_4_62,cs-410,9,4,Probabilistic,"00:04:17,870","00:04:22,400",62,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=257,And then once we have a new generation of,pic_cs-410_9_4_240.jpg
cs-410_9_4_63,cs-410,9,4,Probabilistic,"00:04:22,400","00:04:25,150",63,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=262,We are going the E-step again.,pic_cs-410_9_4_240.jpg
cs-410_9_4_64,cs-410,9,4,Probabilistic,"00:04:25,150","00:04:28,520",64,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=265,To improve our estimate,pic_cs-410_9_4_240.jpg
cs-410_9_4_65,cs-410,9,4,Probabilistic,"00:04:28,520","00:04:33,630",65,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=268,And then that would lead to another,pic_cs-410_9_4_240.jpg
cs-410_9_4_66,cs-410,9,4,Probabilistic,"00:04:34,770","00:04:37,910",66,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=274,For the word distribution,pic_cs-410_9_4_240.jpg
cs-410_9_4_67,cs-410,9,4,Probabilistic,"00:04:39,610","00:04:44,670",67,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=279,"Okay, so, as I said,",pic_cs-410_9_4_240.jpg
cs-410_9_4_68,cs-410,9,4,Probabilistic,"00:04:44,670","00:04:50,380",68,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=284,"is really the variable z, hidden variable,",pic_cs-410_9_4_240.jpg
cs-410_9_4_69,cs-410,9,4,Probabilistic,"00:04:50,380","00:04:55,200",69,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=290,this water is from the top water,pic_cs-410_9_4_240.jpg
cs-410_9_4_70,cs-410,9,4,Probabilistic,"00:04:56,810","00:05:00,780",70,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=296,"So, this slide has a lot of content and",pic_cs-410_9_4_240.jpg
cs-410_9_4_71,cs-410,9,4,Probabilistic,"00:05:00,780","00:05:03,850",71,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=300,Pause the reader to digest it.,pic_cs-410_9_4_300.jpg
cs-410_9_4_72,cs-410,9,4,Probabilistic,"00:05:03,850","00:05:07,300",72,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=303,But this basically captures,pic_cs-410_9_4_300.jpg
cs-410_9_4_73,cs-410,9,4,Probabilistic,"00:05:07,300","00:05:12,500",73,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=307,Start with initial values that,pic_cs-410_9_4_300.jpg
cs-410_9_4_74,cs-410,9,4,Probabilistic,"00:05:12,500","00:05:18,150",74,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=312,And then we invoke E-step followed,pic_cs-410_9_4_300.jpg
cs-410_9_4_75,cs-410,9,4,Probabilistic,"00:05:18,150","00:05:19,690",75,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=318,setting of parameters.,pic_cs-410_9_4_300.jpg
cs-410_9_4_76,cs-410,9,4,Probabilistic,"00:05:19,690","00:05:23,340",76,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=319,"And then we repeated this, so",pic_cs-410_9_4_300.jpg
cs-410_9_4_77,cs-410,9,4,Probabilistic,"00:05:23,340","00:05:27,060",77,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=323,that would gradually improve,pic_cs-410_9_4_300.jpg
cs-410_9_4_78,cs-410,9,4,Probabilistic,"00:05:27,060","00:05:30,050",78,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=327,As I will explain later,pic_cs-410_9_4_300.jpg
cs-410_9_4_79,cs-410,9,4,Probabilistic,"00:05:30,050","00:05:35,340",79,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=330,reaching a local maximum of,pic_cs-410_9_4_300.jpg
cs-410_9_4_80,cs-410,9,4,Probabilistic,"00:05:35,340","00:05:40,180",80,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=335,So lets take a look at the computation for,pic_cs-410_9_4_300.jpg
cs-410_9_4_81,cs-410,9,4,Probabilistic,"00:05:40,180","00:05:41,840",81,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=340,these formulas are the EM.,pic_cs-410_9_4_300.jpg
cs-410_9_4_82,cs-410,9,4,Probabilistic,"00:05:41,840","00:05:48,220",82,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=341,"Formulas that you see before, and",pic_cs-410_9_4_300.jpg
cs-410_9_4_83,cs-410,9,4,Probabilistic,"00:05:48,220","00:05:53,720",83,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=348,"here, like here, n,",pic_cs-410_9_4_300.jpg
cs-410_9_4_84,cs-410,9,4,Probabilistic,"00:05:53,720","00:05:56,040",84,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=353,Like here for example we have n plus one.,pic_cs-410_9_4_300.jpg
cs-410_9_4_85,cs-410,9,4,Probabilistic,"00:05:56,040","00:05:59,728",85,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=356,That means we have improved.,pic_cs-410_9_4_300.jpg
cs-410_9_4_86,cs-410,9,4,Probabilistic,"00:05:59,728","00:06:04,047",86,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=359,From here to here we have an improvement.,pic_cs-410_9_4_300.jpg
cs-410_9_4_87,cs-410,9,4,Probabilistic,"00:06:04,047","00:06:08,106",87,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=364,So in this setting we have assumed the two,pic_cs-410_9_4_360.jpg
cs-410_9_4_88,cs-410,9,4,Probabilistic,"00:06:08,106","00:06:09,689",88,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=368,the background model is null.,pic_cs-410_9_4_360.jpg
cs-410_9_4_89,cs-410,9,4,Probabilistic,"00:06:09,689","00:06:11,872",89,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=369,So what are the relevance,pic_cs-410_9_4_360.jpg
cs-410_9_4_90,cs-410,9,4,Probabilistic,"00:06:11,872","00:06:13,892",90,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=371,Well these are the word counts.,pic_cs-410_9_4_360.jpg
cs-410_9_4_91,cs-410,9,4,Probabilistic,"00:06:13,892","00:06:18,290",91,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=373,"So assume we have just four words,",pic_cs-410_9_4_360.jpg
cs-410_9_4_92,cs-410,9,4,Probabilistic,"00:06:18,290","00:06:22,680",92,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=378,And this is our background model that,pic_cs-410_9_4_360.jpg
cs-410_9_4_93,cs-410,9,4,Probabilistic,"00:06:22,680","00:06:23,380",93,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=382,words like the.,pic_cs-410_9_4_360.jpg
cs-410_9_4_94,cs-410,9,4,Probabilistic,"00:06:25,910","00:06:29,860",94,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=385,"And in the first iteration,",pic_cs-410_9_4_360.jpg
cs-410_9_4_95,cs-410,9,4,Probabilistic,"00:06:29,860","00:06:32,280",95,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=389,Well first we initialize all the values.,pic_cs-410_9_4_360.jpg
cs-410_9_4_96,cs-410,9,4,Probabilistic,"00:06:32,280","00:06:37,360",96,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=392,"So here, this probability that we're",pic_cs-410_9_4_360.jpg
cs-410_9_4_97,cs-410,9,4,Probabilistic,"00:06:37,360","00:06:38,890",97,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=397,distribution of all the words.,pic_cs-410_9_4_360.jpg
cs-410_9_4_98,cs-410,9,4,Probabilistic,"00:06:40,330","00:06:45,940",98,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=400,And then the E-step would give us a guess,pic_cs-410_9_4_360.jpg
cs-410_9_4_99,cs-410,9,4,Probabilistic,"00:06:45,940","00:06:48,470",99,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=405,That will generate each word.,pic_cs-410_9_4_360.jpg
cs-410_9_4_100,cs-410,9,4,Probabilistic,"00:06:48,470","00:06:51,450",100,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=408,We can see we have different,pic_cs-410_9_4_360.jpg
cs-410_9_4_101,cs-410,9,4,Probabilistic,"00:06:51,450","00:06:52,430",101,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=411,Why?,pic_cs-410_9_4_360.jpg
cs-410_9_4_102,cs-410,9,4,Probabilistic,"00:06:52,430","00:06:56,840",102,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=412,"Well, that's because these words have",pic_cs-410_9_4_360.jpg
cs-410_9_4_103,cs-410,9,4,Probabilistic,"00:06:56,840","00:07:00,020",103,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=416,So even though the two,pic_cs-410_9_4_360.jpg
cs-410_9_4_104,cs-410,9,4,Probabilistic,"00:07:00,020","00:07:05,320",104,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=420,And then our initial audition say uniform,pic_cs-410_9_4_420.jpg
cs-410_9_4_105,cs-410,9,4,Probabilistic,"00:07:05,320","00:07:09,270",105,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=425,"in the background of the distribution,",pic_cs-410_9_4_420.jpg
cs-410_9_4_106,cs-410,9,4,Probabilistic,"00:07:09,270","00:07:14,280",106,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=429,So these words are believed to,pic_cs-410_9_4_420.jpg
cs-410_9_4_107,cs-410,9,4,Probabilistic,"00:07:15,820","00:07:17,930",107,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=435,These on the other hand are less likely.,pic_cs-410_9_4_420.jpg
cs-410_9_4_108,cs-410,9,4,Probabilistic,"00:07:17,930","00:07:19,030",108,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=437,Probably from background.,pic_cs-410_9_4_420.jpg
cs-410_9_4_109,cs-410,9,4,Probabilistic,"00:07:20,620","00:07:23,040",109,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=440,"So once we have these z values,",pic_cs-410_9_4_420.jpg
cs-410_9_4_110,cs-410,9,4,Probabilistic,"00:07:23,040","00:07:28,810",110,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=443,we know in the M-step these probabilities,pic_cs-410_9_4_420.jpg
cs-410_9_4_111,cs-410,9,4,Probabilistic,"00:07:28,810","00:07:33,670",111,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=448,So four must be multiplied by this 0.33,pic_cs-410_9_4_420.jpg
cs-410_9_4_112,cs-410,9,4,Probabilistic,"00:07:33,670","00:07:38,190",112,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=453,in order to get the allocated,pic_cs-410_9_4_420.jpg
cs-410_9_4_113,cs-410,9,4,Probabilistic,"00:07:39,550","00:07:43,770",113,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=459,And this is done by this multiplication.,pic_cs-410_9_4_420.jpg
cs-410_9_4_114,cs-410,9,4,Probabilistic,"00:07:43,770","00:07:49,700",114,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=463,Note that if our guess says this,pic_cs-410_9_4_420.jpg
cs-410_9_4_115,cs-410,9,4,Probabilistic,"00:07:52,380","00:07:58,010",115,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=472,then we just get the full count,pic_cs-410_9_4_420.jpg
cs-410_9_4_116,cs-410,9,4,Probabilistic,"00:07:58,010","00:08:01,200",116,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=478,In general it's not going,pic_cs-410_9_4_420.jpg
cs-410_9_4_117,cs-410,9,4,Probabilistic,"00:08:01,200","00:08:06,760",117,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=481,So we're just going to get some percentage,pic_cs-410_9_4_480.jpg
cs-410_9_4_118,cs-410,9,4,Probabilistic,"00:08:06,760","00:08:09,550",118,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=486,Then we simply normalize these counts,pic_cs-410_9_4_480.jpg
cs-410_9_4_119,cs-410,9,4,Probabilistic,"00:08:09,550","00:08:13,170",119,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=489,to have a new generation,pic_cs-410_9_4_480.jpg
cs-410_9_4_120,cs-410,9,4,Probabilistic,"00:08:13,170","00:08:16,600",120,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=493,"So you can see, compare this with",pic_cs-410_9_4_480.jpg
cs-410_9_4_121,cs-410,9,4,Probabilistic,"00:08:18,330","00:08:23,060",121,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=498,So compare this with this one and,pic_cs-410_9_4_480.jpg
cs-410_9_4_122,cs-410,9,4,Probabilistic,"00:08:23,060","00:08:25,930",122,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=503,"Not only that, we also see some",pic_cs-410_9_4_480.jpg
cs-410_9_4_123,cs-410,9,4,Probabilistic,"00:08:25,930","00:08:30,110",123,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=505,words that are believed to have come from,pic_cs-410_9_4_480.jpg
cs-410_9_4_124,cs-410,9,4,Probabilistic,"00:08:30,110","00:08:31,400",124,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=510,"Like this one, text.",pic_cs-410_9_4_480.jpg
cs-410_9_4_125,cs-410,9,4,Probabilistic,"00:08:32,530","00:08:35,930",125,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=512,"And of course, this new generation of",pic_cs-410_9_4_480.jpg
cs-410_9_4_126,cs-410,9,4,Probabilistic,"00:08:35,930","00:08:42,680",126,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=515,adjust the inferred latent variable or,pic_cs-410_9_4_480.jpg
cs-410_9_4_127,cs-410,9,4,Probabilistic,"00:08:42,680","00:08:45,742",127,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=522,"So we have a new generation of values,",pic_cs-410_9_4_480.jpg
cs-410_9_4_128,cs-410,9,4,Probabilistic,"00:08:45,742","00:08:51,115",128,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=525,because of the E-step based on,pic_cs-410_9_4_480.jpg
cs-410_9_4_129,cs-410,9,4,Probabilistic,"00:08:51,115","00:08:56,343",129,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=531,And these new inferred values,pic_cs-410_9_4_480.jpg
cs-410_9_4_130,cs-410,9,4,Probabilistic,"00:08:56,343","00:09:03,166",130,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=536,another generation of the estimate,pic_cs-410_9_4_480.jpg
cs-410_9_4_131,cs-410,9,4,Probabilistic,"00:09:03,166","00:09:07,990",131,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=543,And so on and so forth so this is what,pic_cs-410_9_4_540.jpg
cs-410_9_4_132,cs-410,9,4,Probabilistic,"00:09:07,990","00:09:11,750",132,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=547,these probabilities,pic_cs-410_9_4_540.jpg
cs-410_9_4_133,cs-410,9,4,Probabilistic,"00:09:11,750","00:09:16,745",133,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=551,As you can see in the last row,pic_cs-410_9_4_540.jpg
cs-410_9_4_134,cs-410,9,4,Probabilistic,"00:09:16,745","00:09:20,985",134,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=556,and the likelihood is increasing,pic_cs-410_9_4_540.jpg
cs-410_9_4_135,cs-410,9,4,Probabilistic,"00:09:20,985","00:09:25,875",135,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=560,And note that these log-likelihood is,pic_cs-410_9_4_540.jpg
cs-410_9_4_136,cs-410,9,4,Probabilistic,"00:09:25,875","00:09:30,070",136,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=565,"between 0 and 1 when you take a logarithm,",pic_cs-410_9_4_540.jpg
cs-410_9_4_137,cs-410,9,4,Probabilistic,"00:09:30,070","00:09:33,180",137,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=570,"Now what's also interesting is,",pic_cs-410_9_4_540.jpg
cs-410_9_4_138,cs-410,9,4,Probabilistic,"00:09:33,180","00:09:36,600",138,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=573,And these are the inverted word split.,pic_cs-410_9_4_540.jpg
cs-410_9_4_139,cs-410,9,4,Probabilistic,"00:09:36,600","00:09:42,150",139,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=576,And these are the probabilities,pic_cs-410_9_4_540.jpg
cs-410_9_4_140,cs-410,9,4,Probabilistic,"00:09:42,150","00:09:47,980",140,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=582,"have come from one distribution, in this",pic_cs-410_9_4_540.jpg
cs-410_9_4_141,cs-410,9,4,Probabilistic,"00:09:47,980","00:09:50,580",141,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=587,And you might wonder whether,pic_cs-410_9_4_540.jpg
cs-410_9_4_142,cs-410,9,4,Probabilistic,"00:09:50,580","00:09:55,540",142,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=590,Because our main goal is to,pic_cs-410_9_4_540.jpg
cs-410_9_4_143,cs-410,9,4,Probabilistic,"00:09:55,540","00:09:57,400",143,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=595,So this is our primary goal.,pic_cs-410_9_4_540.jpg
cs-410_9_4_144,cs-410,9,4,Probabilistic,"00:09:57,400","00:10:00,900",144,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=597,We hope to have a more discriminative,pic_cs-410_9_4_540.jpg
cs-410_9_4_145,cs-410,9,4,Probabilistic,"00:10:00,900","00:10:04,400",145,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=600,But the last column is also bi-product.,pic_cs-410_9_4_600.jpg
cs-410_9_4_146,cs-410,9,4,Probabilistic,"00:10:04,400","00:10:07,170",146,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=604,This also can actually be very useful.,pic_cs-410_9_4_600.jpg
cs-410_9_4_147,cs-410,9,4,Probabilistic,"00:10:07,170","00:10:08,380",147,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=607,You can think about that.,pic_cs-410_9_4_600.jpg
cs-410_9_4_148,cs-410,9,4,Probabilistic,"00:10:08,380","00:10:10,220",148,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=608,"We want to use, is to for",pic_cs-410_9_4_600.jpg
cs-410_9_4_149,cs-410,9,4,Probabilistic,"00:10:10,220","00:10:16,080",149,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=610,example is to estimate to what extent this,pic_cs-410_9_4_600.jpg
cs-410_9_4_150,cs-410,9,4,Probabilistic,"00:10:16,080","00:10:18,165",150,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=616,"And this, when we add this up or",pic_cs-410_9_4_600.jpg
cs-410_9_4_151,cs-410,9,4,Probabilistic,"00:10:18,165","00:10:23,304",151,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=618,take the average we will kind of know to,pic_cs-410_9_4_600.jpg
cs-410_9_4_152,cs-410,9,4,Probabilistic,"00:10:23,304","00:10:27,823",152,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=623,versus content was that are not,pic_cs-410_9_4_600.jpg
cs-410_9_4_153,cs-410,9,4,Probabilistic,"00:10:27,823","00:10:37,823",153,https://www.coursera.org/learn/cs-410/lecture/f82s5?t=627,[MUSIC],pic_cs-410_9_4_600.jpg
cs-410_9_5_1,cs-410,9,5,Probabilistic,"00:00:07,553","00:00:12,636",1,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=7,"So, I just showed you that empirically",pic_cs-410_9_5_0.jpg
cs-410_9_5_2,cs-410,9,5,Probabilistic,"00:00:12,636","00:00:17,041",2,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=12,but theoretically it can also,pic_cs-410_9_5_0.jpg
cs-410_9_5_3,cs-410,9,5,Probabilistic,"00:00:17,041","00:00:19,295",3,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=17,converge to a local maximum.,pic_cs-410_9_5_0.jpg
cs-410_9_5_4,cs-410,9,5,Probabilistic,"00:00:19,295","00:00:24,925",4,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=19,So here's just an illustration of what,pic_cs-410_9_5_0.jpg
cs-410_9_5_5,cs-410,9,5,Probabilistic,"00:00:24,925","00:00:29,613",5,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=24,"This required more knowledge about that,",pic_cs-410_9_5_0.jpg
cs-410_9_5_6,cs-410,9,5,Probabilistic,"00:00:29,613","00:00:36,910",6,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=29,"some of that inequalities,",pic_cs-410_9_5_0.jpg
cs-410_9_5_7,cs-410,9,5,Probabilistic,"00:00:39,380","00:00:45,040",7,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=39,So here what you see is on the X,pic_cs-410_9_5_0.jpg
cs-410_9_5_8,cs-410,9,5,Probabilistic,"00:00:45,040","00:00:46,799",8,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=45,This is a parameter that we have.,pic_cs-410_9_5_0.jpg
cs-410_9_5_9,cs-410,9,5,Probabilistic,"00:00:46,799","00:00:49,714",9,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=46,On the y axis we see,pic_cs-410_9_5_0.jpg
cs-410_9_5_10,cs-410,9,5,Probabilistic,"00:00:49,714","00:00:57,171",10,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=49,So this curve is the original,pic_cs-410_9_5_0.jpg
cs-410_9_5_11,cs-410,9,5,Probabilistic,"00:00:57,171","00:01:04,110",11,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=57,and this is the one that,pic_cs-410_9_5_0.jpg
cs-410_9_5_12,cs-410,9,5,Probabilistic,"00:01:04,110","00:01:06,630",12,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=64,And we hope to find a c0 value,pic_cs-410_9_5_60.jpg
cs-410_9_5_13,cs-410,9,5,Probabilistic,"00:01:06,630","00:01:11,480",13,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=66,But in the case of Mitsumoto we can,pic_cs-410_9_5_60.jpg
cs-410_9_5_14,cs-410,9,5,Probabilistic,"00:01:11,480","00:01:12,470",14,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=71,to the problem.,pic_cs-410_9_5_60.jpg
cs-410_9_5_15,cs-410,9,5,Probabilistic,"00:01:12,470","00:01:14,698",15,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=72,"So, we have to resolve",pic_cs-410_9_5_60.jpg
cs-410_9_5_16,cs-410,9,5,Probabilistic,"00:01:14,698","00:01:16,457",16,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=74,the EM algorithm is such an algorithm.,pic_cs-410_9_5_60.jpg
cs-410_9_5_17,cs-410,9,5,Probabilistic,"00:01:16,457","00:01:17,850",17,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=76,It's a Hill-Climb algorithm.,pic_cs-410_9_5_60.jpg
cs-410_9_5_18,cs-410,9,5,Probabilistic,"00:01:17,850","00:01:22,490",18,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=77,That would mean you start,pic_cs-410_9_5_60.jpg
cs-410_9_5_19,cs-410,9,5,Probabilistic,"00:01:22,490","00:01:26,260",19,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=82,"Let's say you start from here,",pic_cs-410_9_5_60.jpg
cs-410_9_5_20,cs-410,9,5,Probabilistic,"00:01:26,260","00:01:32,090",20,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=86,And then you try to improve,pic_cs-410_9_5_60.jpg
cs-410_9_5_21,cs-410,9,5,Probabilistic,"00:01:32,090","00:01:35,420",21,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=92,another point where you can,pic_cs-410_9_5_60.jpg
cs-410_9_5_22,cs-410,9,5,Probabilistic,"00:01:35,420","00:01:37,630",22,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=95,So that's the ideal hill climbing.,pic_cs-410_9_5_60.jpg
cs-410_9_5_23,cs-410,9,5,Probabilistic,"00:01:37,630","00:01:43,030",23,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=97,"And in the EM algorithm, the way we",pic_cs-410_9_5_60.jpg
cs-410_9_5_24,cs-410,9,5,Probabilistic,"00:01:43,030","00:01:46,940",24,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=103,"First, we'll fix a lower",pic_cs-410_9_5_60.jpg
cs-410_9_5_25,cs-410,9,5,Probabilistic,"00:01:46,940","00:01:48,628",25,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=106,So this is the lower bound.,pic_cs-410_9_5_60.jpg
cs-410_9_5_26,cs-410,9,5,Probabilistic,"00:01:48,628","00:01:49,128",26,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=108,See here.,pic_cs-410_9_5_60.jpg
cs-410_9_5_27,cs-410,9,5,Probabilistic,"00:01:51,010","00:01:57,560",27,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=111,"And once we fit the lower bound,",pic_cs-410_9_5_60.jpg
cs-410_9_5_28,cs-410,9,5,Probabilistic,"00:01:57,560","00:01:59,420",28,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=117,"And of course, the reason why this works,",pic_cs-410_9_5_60.jpg
cs-410_9_5_29,cs-410,9,5,Probabilistic,"00:01:59,420","00:02:02,850",29,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=119,is because the lower bound,pic_cs-410_9_5_60.jpg
cs-410_9_5_30,cs-410,9,5,Probabilistic,"00:02:02,850","00:02:05,780",30,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=122,So we know our current guess is here.,pic_cs-410_9_5_120.jpg
cs-410_9_5_31,cs-410,9,5,Probabilistic,"00:02:05,780","00:02:11,530",31,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=125,"And by maximizing the lower bound,",pic_cs-410_9_5_120.jpg
cs-410_9_5_32,cs-410,9,5,Probabilistic,"00:02:11,530","00:02:12,030",32,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=131,To here.,pic_cs-410_9_5_120.jpg
cs-410_9_5_33,cs-410,9,5,Probabilistic,"00:02:13,300","00:02:14,650",33,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=133,Right?,pic_cs-410_9_5_120.jpg
cs-410_9_5_34,cs-410,9,5,Probabilistic,"00:02:14,650","00:02:20,150",34,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=134,And we can then map to the original,pic_cs-410_9_5_120.jpg
cs-410_9_5_35,cs-410,9,5,Probabilistic,"00:02:20,150","00:02:25,600",35,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=140,"Because it's a lower bound, we are",pic_cs-410_9_5_120.jpg
cs-410_9_5_36,cs-410,9,5,Probabilistic,"00:02:25,600","00:02:30,570",36,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=145,Because we improve our lower bound and,pic_cs-410_9_5_120.jpg
cs-410_9_5_37,cs-410,9,5,Probabilistic,"00:02:30,570","00:02:35,040",37,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=150,curve which is above this lower bound,pic_cs-410_9_5_120.jpg
cs-410_9_5_38,cs-410,9,5,Probabilistic,"00:02:36,310","00:02:39,090",38,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=156,So we already know it's,pic_cs-410_9_5_120.jpg
cs-410_9_5_39,cs-410,9,5,Probabilistic,"00:02:39,090","00:02:42,440",39,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=159,So we definitely improve this,pic_cs-410_9_5_120.jpg
cs-410_9_5_40,cs-410,9,5,Probabilistic,"00:02:42,440","00:02:47,253",40,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=162,which is above this lower bound.,pic_cs-410_9_5_120.jpg
cs-410_9_5_41,cs-410,9,5,Probabilistic,"00:02:47,253","00:02:49,770",41,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=167,"So, in our example,",pic_cs-410_9_5_120.jpg
cs-410_9_5_42,cs-410,9,5,Probabilistic,"00:02:49,770","00:02:53,520",42,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=169,the current guess is parameter value,pic_cs-410_9_5_120.jpg
cs-410_9_5_43,cs-410,9,5,Probabilistic,"00:02:53,520","00:02:57,660",43,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=173,And then the next guess is,pic_cs-410_9_5_120.jpg
cs-410_9_5_44,cs-410,9,5,Probabilistic,"00:02:57,660","00:03:01,110",44,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=177,From this illustration you,pic_cs-410_9_5_120.jpg
cs-410_9_5_45,cs-410,9,5,Probabilistic,"00:03:01,110","00:03:03,620",45,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=181,is always better than the current guess.,pic_cs-410_9_5_180.jpg
cs-410_9_5_46,cs-410,9,5,Probabilistic,"00:03:03,620","00:03:06,930",46,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=183,"Unless it has reached the maximum,",pic_cs-410_9_5_180.jpg
cs-410_9_5_47,cs-410,9,5,Probabilistic,"00:03:06,930","00:03:08,008",47,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=186,So the two would be equal.,pic_cs-410_9_5_180.jpg
cs-410_9_5_48,cs-410,9,5,Probabilistic,"00:03:08,008","00:03:12,821",48,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=188,"So, the E-step is basically",pic_cs-410_9_5_180.jpg
cs-410_9_5_49,cs-410,9,5,Probabilistic,"00:03:12,821","00:03:17,650",49,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=192,to compute this lower bound.,pic_cs-410_9_5_180.jpg
cs-410_9_5_50,cs-410,9,5,Probabilistic,"00:03:17,650","00:03:22,061",50,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=197,We don't directly just compute,pic_cs-410_9_5_180.jpg
cs-410_9_5_51,cs-410,9,5,Probabilistic,"00:03:22,061","00:03:25,452",51,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=202,we compute the length of,pic_cs-410_9_5_180.jpg
cs-410_9_5_52,cs-410,9,5,Probabilistic,"00:03:25,452","00:03:28,990",52,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=205,these are basically a part,pic_cs-410_9_5_180.jpg
cs-410_9_5_53,cs-410,9,5,Probabilistic,"00:03:28,990","00:03:31,150",53,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=208,This helps determine the lower bound.,pic_cs-410_9_5_180.jpg
cs-410_9_5_54,cs-410,9,5,Probabilistic,"00:03:31,150","00:03:34,460",54,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=211,The M-step on the other hand is,pic_cs-410_9_5_180.jpg
cs-410_9_5_55,cs-410,9,5,Probabilistic,"00:03:34,460","00:03:37,480",55,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=214,It allows us to move,pic_cs-410_9_5_180.jpg
cs-410_9_5_56,cs-410,9,5,Probabilistic,"00:03:37,480","00:03:41,460",56,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=217,And that's why EM algorithm is guaranteed,pic_cs-410_9_5_180.jpg
cs-410_9_5_57,cs-410,9,5,Probabilistic,"00:03:42,490","00:03:46,720",57,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=222,"Now, as you can imagine,",pic_cs-410_9_5_180.jpg
cs-410_9_5_58,cs-410,9,5,Probabilistic,"00:03:46,720","00:03:50,100",58,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=226,we also have to repeat the EM,pic_cs-410_9_5_180.jpg
cs-410_9_5_59,cs-410,9,5,Probabilistic,"00:03:50,100","00:03:54,340",59,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=230,In order to figure out which one,pic_cs-410_9_5_180.jpg
cs-410_9_5_60,cs-410,9,5,Probabilistic,"00:03:54,340","00:03:59,070",60,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=234,And this actually in general is a,pic_cs-410_9_5_180.jpg
cs-410_9_5_61,cs-410,9,5,Probabilistic,"00:03:59,070","00:04:02,689",61,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=239,So here for,pic_cs-410_9_5_180.jpg
cs-410_9_5_62,cs-410,9,5,Probabilistic,"00:04:02,689","00:04:06,223",62,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=242,then we gradually just,pic_cs-410_9_5_240.jpg
cs-410_9_5_63,cs-410,9,5,Probabilistic,"00:04:06,223","00:04:11,227",63,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=246,"So, that's not optimal, and",pic_cs-410_9_5_240.jpg
cs-410_9_5_64,cs-410,9,5,Probabilistic,"00:04:11,227","00:04:16,575",64,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=251,so the only way to climb up to this gear,pic_cs-410_9_5_240.jpg
cs-410_9_5_65,cs-410,9,5,Probabilistic,"00:04:16,575","00:04:22,767",65,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=256,"So, in the EM algorithm, we generally",pic_cs-410_9_5_240.jpg
cs-410_9_5_66,cs-410,9,5,Probabilistic,"00:04:22,767","00:04:27,880",66,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=262,or have some other way to determine,pic_cs-410_9_5_240.jpg
cs-410_9_5_67,cs-410,9,5,Probabilistic,"00:04:29,840","00:04:34,320",67,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=269,To summarize in this lecture we,pic_cs-410_9_5_240.jpg
cs-410_9_5_68,cs-410,9,5,Probabilistic,"00:04:34,320","00:04:38,683",68,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=274,This is a general algorithm for computing,pic_cs-410_9_5_240.jpg
cs-410_9_5_69,cs-410,9,5,Probabilistic,"00:04:38,683","00:04:42,153",69,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=278,"kinds of models, so",pic_cs-410_9_5_240.jpg
cs-410_9_5_70,cs-410,9,5,Probabilistic,"00:04:42,153","00:04:46,468",70,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=282,"And it's a hill-climbing algorithm, so it",pic_cs-410_9_5_240.jpg
cs-410_9_5_71,cs-410,9,5,Probabilistic,"00:04:46,468","00:04:48,250",71,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=286,it will depend on initial points.,pic_cs-410_9_5_240.jpg
cs-410_9_5_72,cs-410,9,5,Probabilistic,"00:04:49,770","00:04:55,414",72,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=289,The general idea is that we will have,pic_cs-410_9_5_240.jpg
cs-410_9_5_73,cs-410,9,5,Probabilistic,"00:04:55,414","00:05:00,270",73,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=295,In the E-step we roughly [INAUDIBLE],pic_cs-410_9_5_240.jpg
cs-410_9_5_74,cs-410,9,5,Probabilistic,"00:05:00,270","00:05:05,560",74,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=300,of useful hidden variables that we,pic_cs-410_9_5_300.jpg
cs-410_9_5_75,cs-410,9,5,Probabilistic,"00:05:05,560","00:05:10,056",75,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=305,"In our case, this is the distribution",pic_cs-410_9_5_300.jpg
cs-410_9_5_76,cs-410,9,5,Probabilistic,"00:05:10,056","00:05:15,750",76,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=310,In the M-step then we would exploit,pic_cs-410_9_5_300.jpg
cs-410_9_5_77,cs-410,9,5,Probabilistic,"00:05:15,750","00:05:20,790",77,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=315,"it easier to estimate the distribution,",pic_cs-410_9_5_300.jpg
cs-410_9_5_78,cs-410,9,5,Probabilistic,"00:05:20,790","00:05:24,860",78,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=320,Here improve is guaranteed in,pic_cs-410_9_5_300.jpg
cs-410_9_5_79,cs-410,9,5,Probabilistic,"00:05:24,860","00:05:30,240",79,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=324,Note that it's not necessary that we,pic_cs-410_9_5_300.jpg
cs-410_9_5_80,cs-410,9,5,Probabilistic,"00:05:30,240","00:05:35,260",80,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=330,parameter value even though the likelihood,pic_cs-410_9_5_300.jpg
cs-410_9_5_81,cs-410,9,5,Probabilistic,"00:05:35,260","00:05:40,370",81,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=335,There are some properties that have to,pic_cs-410_9_5_300.jpg
cs-410_9_5_82,cs-410,9,5,Probabilistic,"00:05:40,370","00:05:44,640",82,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=340,also to convert into some stable value.,pic_cs-410_9_5_300.jpg
cs-410_9_5_83,cs-410,9,5,Probabilistic,"00:05:47,500","00:05:50,790",83,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=347,Now here data augmentation,pic_cs-410_9_5_300.jpg
cs-410_9_5_84,cs-410,9,5,Probabilistic,"00:05:50,790","00:05:51,360",84,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=350,"That means,",pic_cs-410_9_5_300.jpg
cs-410_9_5_85,cs-410,9,5,Probabilistic,"00:05:51,360","00:05:54,830",85,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=351,we're not going to just say exactly,pic_cs-410_9_5_300.jpg
cs-410_9_5_86,cs-410,9,5,Probabilistic,"00:05:54,830","00:05:59,390",86,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=354,But we're going to have a probability,pic_cs-410_9_5_300.jpg
cs-410_9_5_87,cs-410,9,5,Probabilistic,"00:05:59,390","00:06:01,140",87,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=359,these hidden variables.,pic_cs-410_9_5_300.jpg
cs-410_9_5_88,cs-410,9,5,Probabilistic,"00:06:01,140","00:06:05,990",88,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=361,So this causes a split of counts,pic_cs-410_9_5_360.jpg
cs-410_9_5_89,cs-410,9,5,Probabilistic,"00:06:07,430","00:06:12,783",89,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=367,And in our case we'll split the word,pic_cs-410_9_5_360.jpg
cs-410_9_5_90,cs-410,9,5,Probabilistic,"00:06:12,783","00:06:22,783",90,https://www.coursera.org/learn/cs-410/lecture/naLsv?t=372,[MUSIC],pic_cs-410_9_5_360.jpg
cs-410_9_6_1,cs-410,9,6,Probabilistic,"00:00:00,012","00:00:07,295",1,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=0,[SOUND],pic_cs-410_9_6_0.jpg
cs-410_9_6_2,cs-410,9,6,Probabilistic,"00:00:07,295","00:00:11,390",2,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=7,lecture is about probabilistic and,pic_cs-410_9_6_0.jpg
cs-410_9_6_3,cs-410,9,6,Probabilistic,"00:00:12,710","00:00:18,000",3,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=12,In this lecture we're going to introduce,pic_cs-410_9_6_0.jpg
cs-410_9_6_4,cs-410,9,6,Probabilistic,"00:00:18,000","00:00:18,770",4,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=18,often called PLSA.,pic_cs-410_9_6_0.jpg
cs-410_9_6_5,cs-410,9,6,Probabilistic,"00:00:18,770","00:00:26,060",5,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=18,"This is the most basic topic model,",pic_cs-410_9_6_0.jpg
cs-410_9_6_6,cs-410,9,6,Probabilistic,"00:00:26,060","00:00:30,890",6,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=26,Now this kind of models,pic_cs-410_9_6_0.jpg
cs-410_9_6_7,cs-410,9,6,Probabilistic,"00:00:30,890","00:00:34,560",7,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=30,mine multiple topics from text documents.,pic_cs-410_9_6_0.jpg
cs-410_9_6_8,cs-410,9,6,Probabilistic,"00:00:34,560","00:00:39,410",8,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=34,And PRSA is one of the most basic,pic_cs-410_9_6_0.jpg
cs-410_9_6_9,cs-410,9,6,Probabilistic,"00:00:39,410","00:00:43,800",9,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=39,So let's first examine this power,pic_cs-410_9_6_0.jpg
cs-410_9_6_10,cs-410,9,6,Probabilistic,"00:00:43,800","00:00:47,710",10,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=43,Here I show a sample article which is,pic_cs-410_9_6_0.jpg
cs-410_9_6_11,cs-410,9,6,Probabilistic,"00:00:48,830","00:00:51,100",11,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=48,And I show some simple topics.,pic_cs-410_9_6_0.jpg
cs-410_9_6_12,cs-410,9,6,Probabilistic,"00:00:51,100","00:00:55,870",12,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=51,"For example government response,",pic_cs-410_9_6_0.jpg
cs-410_9_6_13,cs-410,9,6,Probabilistic,"00:00:55,870","00:00:57,420",13,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=55,Donation and the background.,pic_cs-410_9_6_0.jpg
cs-410_9_6_14,cs-410,9,6,Probabilistic,"00:00:59,260","00:01:04,070",14,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=59,You can see in the article we use,pic_cs-410_9_6_0.jpg
cs-410_9_6_15,cs-410,9,6,Probabilistic,"00:01:05,150","00:01:09,540",15,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=65,So we first for example see there's,pic_cs-410_9_6_60.jpg
cs-410_9_6_16,cs-410,9,6,Probabilistic,"00:01:09,540","00:01:14,740",16,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=69,this is followed by discussion of flooding,pic_cs-410_9_6_60.jpg
cs-410_9_6_17,cs-410,9,6,Probabilistic,"00:01:14,740","00:01:17,440",17,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=74,We also see background,pic_cs-410_9_6_60.jpg
cs-410_9_6_18,cs-410,9,6,Probabilistic,"00:01:18,840","00:01:23,740",18,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=78,So the overall of topic analysis here,pic_cs-410_9_6_60.jpg
cs-410_9_6_19,cs-410,9,6,Probabilistic,"00:01:23,740","00:01:28,250",19,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=83,"the text, to segment the topics,",pic_cs-410_9_6_60.jpg
cs-410_9_6_20,cs-410,9,6,Probabilistic,"00:01:28,250","00:01:33,820",20,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=88,"distribution and to figure out first,",pic_cs-410_9_6_60.jpg
cs-410_9_6_21,cs-410,9,6,Probabilistic,"00:01:33,820","00:01:36,420",21,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=93,How do we know there's a topic,pic_cs-410_9_6_60.jpg
cs-410_9_6_22,cs-410,9,6,Probabilistic,"00:01:36,420","00:01:39,020",22,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=96,There's a topic about a flood in the city.,pic_cs-410_9_6_60.jpg
cs-410_9_6_23,cs-410,9,6,Probabilistic,"00:01:39,020","00:01:41,850",23,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=99,So these are the tasks,pic_cs-410_9_6_60.jpg
cs-410_9_6_24,cs-410,9,6,Probabilistic,"00:01:42,870","00:01:46,110",24,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=102,If we had discovered these,pic_cs-410_9_6_60.jpg
cs-410_9_6_25,cs-410,9,6,Probabilistic,"00:01:46,110","00:01:50,030",25,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=106,"as you see here,",pic_cs-410_9_6_60.jpg
cs-410_9_6_26,cs-410,9,6,Probabilistic,"00:01:50,030","00:01:54,390",26,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=110,"Then you can do a lot of things,",pic_cs-410_9_6_60.jpg
cs-410_9_6_27,cs-410,9,6,Probabilistic,"00:01:54,390","00:01:59,800",27,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=114,"of the topics,",pic_cs-410_9_6_60.jpg
cs-410_9_6_28,cs-410,9,6,Probabilistic,"00:01:59,800","00:02:04,220",28,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=119,So the formal definition of problem of,pic_cs-410_9_6_60.jpg
cs-410_9_6_29,cs-410,9,6,Probabilistic,"00:02:04,220","00:02:04,870",29,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=124,shown here.,pic_cs-410_9_6_120.jpg
cs-410_9_6_30,cs-410,9,6,Probabilistic,"00:02:04,870","00:02:09,270",30,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=124,And this is after a slide that you,pic_cs-410_9_6_120.jpg
cs-410_9_6_31,cs-410,9,6,Probabilistic,"00:02:09,270","00:02:14,100",31,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=129,"So the input is a collection, the number",pic_cs-410_9_6_120.jpg
cs-410_9_6_32,cs-410,9,6,Probabilistic,"00:02:14,100","00:02:15,060",32,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=134,of course the text data.,pic_cs-410_9_6_120.jpg
cs-410_9_6_33,cs-410,9,6,Probabilistic,"00:02:16,300","00:02:18,760",33,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=136,And then the output is of two kinds.,pic_cs-410_9_6_120.jpg
cs-410_9_6_34,cs-410,9,6,Probabilistic,"00:02:18,760","00:02:21,720",34,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=138,"One is the topic category,",pic_cs-410_9_6_120.jpg
cs-410_9_6_35,cs-410,9,6,Probabilistic,"00:02:21,720","00:02:22,520",35,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=141,Theta i's.,pic_cs-410_9_6_120.jpg
cs-410_9_6_36,cs-410,9,6,Probabilistic,"00:02:22,520","00:02:24,790",36,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=142,Each theta i is a word distribution.,pic_cs-410_9_6_120.jpg
cs-410_9_6_37,cs-410,9,6,Probabilistic,"00:02:24,790","00:02:28,160",37,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=144,"And second, it's the topic coverage for",pic_cs-410_9_6_120.jpg
cs-410_9_6_38,cs-410,9,6,Probabilistic,"00:02:28,160","00:02:30,130",38,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=148,These are pi sub i j's.,pic_cs-410_9_6_120.jpg
cs-410_9_6_39,cs-410,9,6,Probabilistic,"00:02:30,130","00:02:33,490",39,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=150,And they tell us which document it covers.,pic_cs-410_9_6_120.jpg
cs-410_9_6_40,cs-410,9,6,Probabilistic,"00:02:33,490","00:02:35,440",40,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=153,Which topic to what extent.,pic_cs-410_9_6_120.jpg
cs-410_9_6_41,cs-410,9,6,Probabilistic,"00:02:35,440","00:02:37,960",41,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=155,So we hope to generate these as output.,pic_cs-410_9_6_120.jpg
cs-410_9_6_42,cs-410,9,6,Probabilistic,"00:02:37,960","00:02:41,350",42,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=157,Because there are many useful,pic_cs-410_9_6_120.jpg
cs-410_9_6_43,cs-410,9,6,Probabilistic,"00:02:42,880","00:02:47,100",43,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=162,So the idea of PLSA is,pic_cs-410_9_6_120.jpg
cs-410_9_6_44,cs-410,9,6,Probabilistic,"00:02:47,100","00:02:50,660",44,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=167,the two component mixture model,pic_cs-410_9_6_120.jpg
cs-410_9_6_45,cs-410,9,6,Probabilistic,"00:02:50,660","00:02:54,760",45,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=170,The only difference is that we,pic_cs-410_9_6_120.jpg
cs-410_9_6_46,cs-410,9,6,Probabilistic,"00:02:54,760","00:02:57,960",46,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=174,"Otherwise, it is essentially the same.",pic_cs-410_9_6_120.jpg
cs-410_9_6_47,cs-410,9,6,Probabilistic,"00:02:57,960","00:03:03,730",47,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=177,So here I illustrate how we can generate,pic_cs-410_9_6_120.jpg
cs-410_9_6_48,cs-410,9,6,Probabilistic,"00:03:03,730","00:03:06,490",48,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=183,naturally in all cases,pic_cs-410_9_6_180.jpg
cs-410_9_6_49,cs-410,9,6,Probabilistic,"00:03:06,490","00:03:11,310",49,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=186,of Probabilistic modelling would want,pic_cs-410_9_6_180.jpg
cs-410_9_6_50,cs-410,9,6,Probabilistic,"00:03:11,310","00:03:13,400",50,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=191,"So we would also ask the question,",pic_cs-410_9_6_180.jpg
cs-410_9_6_51,cs-410,9,6,Probabilistic,"00:03:13,400","00:03:18,200",51,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=193,what's the probability of observing,pic_cs-410_9_6_180.jpg
cs-410_9_6_52,cs-410,9,6,Probabilistic,"00:03:18,200","00:03:19,470",52,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=198,Now if you look at this picture and,pic_cs-410_9_6_180.jpg
cs-410_9_6_53,cs-410,9,6,Probabilistic,"00:03:19,470","00:03:21,840",53,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=199,compare this with the picture,pic_cs-410_9_6_180.jpg
cs-410_9_6_54,cs-410,9,6,Probabilistic,"00:03:21,840","00:03:25,580",54,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=201,you will see the only difference is,pic_cs-410_9_6_180.jpg
cs-410_9_6_55,cs-410,9,6,Probabilistic,"00:03:26,940","00:03:32,900",55,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=206,"So, before we have just one topic,",pic_cs-410_9_6_180.jpg
cs-410_9_6_56,cs-410,9,6,Probabilistic,"00:03:32,900","00:03:35,990",56,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=212,But now we have more topics.,pic_cs-410_9_6_180.jpg
cs-410_9_6_57,cs-410,9,6,Probabilistic,"00:03:35,990","00:03:38,260",57,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=215,"Specifically, we have k topics now.",pic_cs-410_9_6_180.jpg
cs-410_9_6_58,cs-410,9,6,Probabilistic,"00:03:38,260","00:03:43,930",58,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=218,All these are topics that we assume,pic_cs-410_9_6_180.jpg
cs-410_9_6_59,cs-410,9,6,Probabilistic,"00:03:43,930","00:03:49,450",59,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=223,So the consequence is that our switch for,pic_cs-410_9_6_180.jpg
cs-410_9_6_60,cs-410,9,6,Probabilistic,"00:03:49,450","00:03:51,210",60,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=229,Before it's just a two way switch.,pic_cs-410_9_6_180.jpg
cs-410_9_6_61,cs-410,9,6,Probabilistic,"00:03:51,210","00:03:53,420",61,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=231,We can think of it as flipping a coin.,pic_cs-410_9_6_180.jpg
cs-410_9_6_62,cs-410,9,6,Probabilistic,"00:03:53,420","00:03:55,110",62,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=233,But now we have multiple ways.,pic_cs-410_9_6_180.jpg
cs-410_9_6_63,cs-410,9,6,Probabilistic,"00:03:55,110","00:03:59,660",63,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=235,First we can flip a coin to decide,pic_cs-410_9_6_180.jpg
cs-410_9_6_64,cs-410,9,6,Probabilistic,"00:03:59,660","00:04:06,913",64,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=239,So it's the background lambda,pic_cs-410_9_6_180.jpg
cs-410_9_6_65,cs-410,9,6,Probabilistic,"00:04:06,913","00:04:11,490",65,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=246,1 minus lambda sub B gives,pic_cs-410_9_6_240.jpg
cs-410_9_6_66,cs-410,9,6,Probabilistic,"00:04:11,490","00:04:16,300",66,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=251,actually choosing a non-background topic.,pic_cs-410_9_6_240.jpg
cs-410_9_6_67,cs-410,9,6,Probabilistic,"00:04:16,300","00:04:17,860",67,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=256,"After we have made this decision,",pic_cs-410_9_6_240.jpg
cs-410_9_6_68,cs-410,9,6,Probabilistic,"00:04:17,860","00:04:24,750",68,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=257,we have to make another decision to,pic_cs-410_9_6_240.jpg
cs-410_9_6_69,cs-410,9,6,Probabilistic,"00:04:24,750","00:04:26,480",69,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=264,So there are K way switch here.,pic_cs-410_9_6_240.jpg
cs-410_9_6_70,cs-410,9,6,Probabilistic,"00:04:26,480","00:04:30,120",70,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=266,"And this is characterized by pi,",pic_cs-410_9_6_240.jpg
cs-410_9_6_71,cs-410,9,6,Probabilistic,"00:04:31,450","00:04:33,775",71,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=271,This is just the difference of designs.,pic_cs-410_9_6_240.jpg
cs-410_9_6_72,cs-410,9,6,Probabilistic,"00:04:33,775","00:04:36,745",72,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=273,Which is a little bit more complicated.,pic_cs-410_9_6_240.jpg
cs-410_9_6_73,cs-410,9,6,Probabilistic,"00:04:36,745","00:04:40,655",73,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=276,But once we decide which distribution to,pic_cs-410_9_6_240.jpg
cs-410_9_6_74,cs-410,9,6,Probabilistic,"00:04:40,655","00:04:45,145",74,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=280,just generate a word by using one of,pic_cs-410_9_6_240.jpg
cs-410_9_6_75,cs-410,9,6,Probabilistic,"00:04:46,885","00:04:50,920",75,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=286,So now lets look at the question,pic_cs-410_9_6_240.jpg
cs-410_9_6_76,cs-410,9,6,Probabilistic,"00:04:50,920","00:04:55,780",76,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=290,So what's the probability of observing,pic_cs-410_9_6_240.jpg
cs-410_9_6_77,cs-410,9,6,Probabilistic,"00:04:55,780","00:04:57,250",77,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=295,What do you think?,pic_cs-410_9_6_240.jpg
cs-410_9_6_78,cs-410,9,6,Probabilistic,"00:04:57,250","00:05:01,150",78,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=297,Now we've seen this,pic_cs-410_9_6_240.jpg
cs-410_9_6_79,cs-410,9,6,Probabilistic,"00:05:01,150","00:05:05,210",79,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=301,"if you can recall, it's generally a sum.",pic_cs-410_9_6_300.jpg
cs-410_9_6_80,cs-410,9,6,Probabilistic,"00:05:05,210","00:05:08,540",80,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=305,Of all the different possibilities,pic_cs-410_9_6_300.jpg
cs-410_9_6_81,cs-410,9,6,Probabilistic,"00:05:08,540","00:05:14,260",81,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=308,So let's first look at how the word can,pic_cs-410_9_6_300.jpg
cs-410_9_6_82,cs-410,9,6,Probabilistic,"00:05:14,260","00:05:18,340",82,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=314,"Well, the probability that the word is",pic_cs-410_9_6_300.jpg
cs-410_9_6_83,cs-410,9,6,Probabilistic,"00:05:18,340","00:05:22,700",83,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=318,is lambda multiplied by the probability,pic_cs-410_9_6_300.jpg
cs-410_9_6_84,cs-410,9,6,Probabilistic,"00:05:22,700","00:05:24,200",84,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=322,"Model, right.",pic_cs-410_9_6_300.jpg
cs-410_9_6_85,cs-410,9,6,Probabilistic,"00:05:24,200","00:05:25,150",85,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=324,Two things must happen.,pic_cs-410_9_6_300.jpg
cs-410_9_6_86,cs-410,9,6,Probabilistic,"00:05:25,150","00:05:28,270",86,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=325,"First, we have to have",pic_cs-410_9_6_300.jpg
cs-410_9_6_87,cs-410,9,6,Probabilistic,"00:05:28,270","00:05:31,730",87,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=328,"and that's the probability of lambda,",pic_cs-410_9_6_300.jpg
cs-410_9_6_88,cs-410,9,6,Probabilistic,"00:05:31,730","00:05:36,330",88,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=331,"Then second, we must have actually",pic_cs-410_9_6_300.jpg
cs-410_9_6_89,cs-410,9,6,Probabilistic,"00:05:36,330","00:05:39,161",89,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=336,and that's probability,pic_cs-410_9_6_300.jpg
cs-410_9_6_90,cs-410,9,6,Probabilistic,"00:05:40,220","00:05:41,790",90,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=340,"Okay, so similarly,",pic_cs-410_9_6_300.jpg
cs-410_9_6_91,cs-410,9,6,Probabilistic,"00:05:41,790","00:05:46,020",91,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=341,we can figure out the probability of,pic_cs-410_9_6_300.jpg
cs-410_9_6_92,cs-410,9,6,Probabilistic,"00:05:46,020","00:05:48,530",92,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=346,Like the topic theta sub k.,pic_cs-410_9_6_300.jpg
cs-410_9_6_93,cs-410,9,6,Probabilistic,"00:05:48,530","00:05:51,890",93,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=348,Now notice that here's,pic_cs-410_9_6_300.jpg
cs-410_9_6_94,cs-410,9,6,Probabilistic,"00:05:51,890","00:05:57,023",94,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=351,And that's because of the choice,pic_cs-410_9_6_300.jpg
cs-410_9_6_95,cs-410,9,6,Probabilistic,"00:05:57,023","00:06:00,630",95,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=357,only happens if two things happen.,pic_cs-410_9_6_300.jpg
cs-410_9_6_96,cs-410,9,6,Probabilistic,"00:06:00,630","00:06:04,020",96,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=360,One is we decide not to,pic_cs-410_9_6_360.jpg
cs-410_9_6_97,cs-410,9,6,Probabilistic,"00:06:04,020","00:06:07,630",97,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=364,"So, that's a probability",pic_cs-410_9_6_360.jpg
cs-410_9_6_98,cs-410,9,6,Probabilistic,"00:06:07,630","00:06:13,290",98,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=367,"Second, we also have to actually choose",pic_cs-410_9_6_360.jpg
cs-410_9_6_99,cs-410,9,6,Probabilistic,"00:06:13,290","00:06:16,000",99,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=373,"So that's probability of theta sub K,",pic_cs-410_9_6_360.jpg
cs-410_9_6_100,cs-410,9,6,Probabilistic,"00:06:17,900","00:06:21,460",100,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=377,"And similarly, the probability of",pic_cs-410_9_6_360.jpg
cs-410_9_6_101,cs-410,9,6,Probabilistic,"00:06:21,460","00:06:26,480",101,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=381,The topic and the first topic,pic_cs-410_9_6_360.jpg
cs-410_9_6_102,cs-410,9,6,Probabilistic,"00:06:26,480","00:06:27,250",102,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=386,And so,pic_cs-410_9_6_360.jpg
cs-410_9_6_103,cs-410,9,6,Probabilistic,"00:06:27,250","00:06:32,480",103,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=387,in the end the probability of observing,pic_cs-410_9_6_360.jpg
cs-410_9_6_104,cs-410,9,6,Probabilistic,"00:06:32,480","00:06:38,080",104,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=392,And I have to stress again this is a very,pic_cs-410_9_6_360.jpg
cs-410_9_6_105,cs-410,9,6,Probabilistic,"00:06:38,080","00:06:44,150",105,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=398,really key to understanding all the topic,pic_cs-410_9_6_360.jpg
cs-410_9_6_106,cs-410,9,6,Probabilistic,"00:06:44,150","00:06:47,410",106,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=404,So make sure that you really,pic_cs-410_9_6_360.jpg
cs-410_9_6_107,cs-410,9,6,Probabilistic,"00:06:49,410","00:06:53,390",107,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=409,of w is indeed the sum of these terms.,pic_cs-410_9_6_360.jpg
cs-410_9_6_108,cs-410,9,6,Probabilistic,"00:06:56,540","00:07:00,620",108,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=416,"So, next,",pic_cs-410_9_6_360.jpg
cs-410_9_6_109,cs-410,9,6,Probabilistic,"00:07:00,620","00:07:05,250",109,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=420,we would be interested in,pic_cs-410_9_6_420.jpg
cs-410_9_6_110,cs-410,9,6,Probabilistic,"00:07:05,250","00:07:07,250",110,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=425,"All right, so to estimate the parameters.",pic_cs-410_9_6_420.jpg
cs-410_9_6_111,cs-410,9,6,Probabilistic,"00:07:07,250","00:07:07,760",111,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=427,"But firstly,",pic_cs-410_9_6_420.jpg
cs-410_9_6_112,cs-410,9,6,Probabilistic,"00:07:07,760","00:07:13,510",112,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=427,let's put all these together to have the,pic_cs-410_9_6_420.jpg
cs-410_9_6_113,cs-410,9,6,Probabilistic,"00:07:13,510","00:07:19,010",113,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=433,The first line shows the probability of a,pic_cs-410_9_6_420.jpg
cs-410_9_6_114,cs-410,9,6,Probabilistic,"00:07:19,010","00:07:20,980",114,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=439,And this is an important,pic_cs-410_9_6_420.jpg
cs-410_9_6_115,cs-410,9,6,Probabilistic,"00:07:22,560","00:07:24,250",115,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=442,So let's take a closer look at this.,pic_cs-410_9_6_420.jpg
cs-410_9_6_116,cs-410,9,6,Probabilistic,"00:07:24,250","00:07:27,430",116,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=444,This actually commands all,pic_cs-410_9_6_420.jpg
cs-410_9_6_117,cs-410,9,6,Probabilistic,"00:07:27,430","00:07:29,280",117,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=447,So first of all we see lambda sub b here.,pic_cs-410_9_6_420.jpg
cs-410_9_6_118,cs-410,9,6,Probabilistic,"00:07:29,280","00:07:31,539",118,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=449,This represents a percentage,pic_cs-410_9_6_420.jpg
cs-410_9_6_119,cs-410,9,6,Probabilistic,"00:07:32,610","00:07:35,560",119,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=452,that we believe exist in the text data.,pic_cs-410_9_6_420.jpg
cs-410_9_6_120,cs-410,9,6,Probabilistic,"00:07:35,560","00:07:39,220",120,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=455,And this can be a known value,pic_cs-410_9_6_420.jpg
cs-410_9_6_121,cs-410,9,6,Probabilistic,"00:07:41,180","00:07:43,380",121,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=461,"Second, we see the background",pic_cs-410_9_6_420.jpg
cs-410_9_6_122,cs-410,9,6,Probabilistic,"00:07:43,380","00:07:45,210",122,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=463,typically we also assume this is known.,pic_cs-410_9_6_420.jpg
cs-410_9_6_123,cs-410,9,6,Probabilistic,"00:07:45,210","00:07:48,000",123,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=465,"We can use a large collection of text, or",pic_cs-410_9_6_420.jpg
cs-410_9_6_124,cs-410,9,6,Probabilistic,"00:07:48,000","00:07:51,780",124,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=468,use all the text that we have available,pic_cs-410_9_6_420.jpg
cs-410_9_6_125,cs-410,9,6,Probabilistic,"00:07:52,890","00:07:55,008",125,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=472,Now next in the next stop this formula.,pic_cs-410_9_6_420.jpg
cs-410_9_6_126,cs-410,9,6,Probabilistic,"00:07:55,008","00:07:57,960",126,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=475,[COUGH] Excuse me.,pic_cs-410_9_6_420.jpg
cs-410_9_6_127,cs-410,9,6,Probabilistic,"00:07:57,960","00:08:00,160",127,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=477,You see two interesting,pic_cs-410_9_6_420.jpg
cs-410_9_6_128,cs-410,9,6,Probabilistic,"00:08:00,160","00:08:01,886",128,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=480,those are the most important parameters.,pic_cs-410_9_6_480.jpg
cs-410_9_6_129,cs-410,9,6,Probabilistic,"00:08:01,886","00:08:04,690",129,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=481,That we are.,pic_cs-410_9_6_480.jpg
cs-410_9_6_130,cs-410,9,6,Probabilistic,"00:08:04,690","00:08:06,190",130,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=484,So one is pi's.,pic_cs-410_9_6_480.jpg
cs-410_9_6_131,cs-410,9,6,Probabilistic,"00:08:06,190","00:08:10,060",131,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=486,And these are the coverage,pic_cs-410_9_6_480.jpg
cs-410_9_6_132,cs-410,9,6,Probabilistic,"00:08:11,280","00:08:15,310",132,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=491,And the other is word distributions,pic_cs-410_9_6_480.jpg
cs-410_9_6_133,cs-410,9,6,Probabilistic,"00:08:18,530","00:08:23,780",133,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=498,"So the next line,",pic_cs-410_9_6_480.jpg
cs-410_9_6_134,cs-410,9,6,Probabilistic,"00:08:23,780","00:08:26,280",134,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=503,in to calculate,pic_cs-410_9_6_480.jpg
cs-410_9_6_135,cs-410,9,6,Probabilistic,"00:08:26,280","00:08:29,720",135,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=506,"This is, again, of the familiar",pic_cs-410_9_6_480.jpg
cs-410_9_6_136,cs-410,9,6,Probabilistic,"00:08:29,720","00:08:32,050",136,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=509,you have a count of,pic_cs-410_9_6_480.jpg
cs-410_9_6_137,cs-410,9,6,Probabilistic,"00:08:32,050","00:08:35,100",137,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=512,And then log of a probability.,pic_cs-410_9_6_480.jpg
cs-410_9_6_138,cs-410,9,6,Probabilistic,"00:08:35,100","00:08:39,040",138,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=515,Now it's a little bit more,pic_cs-410_9_6_480.jpg
cs-410_9_6_139,cs-410,9,6,Probabilistic,"00:08:39,040","00:08:43,890",139,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=519,"Because now we have more components,",pic_cs-410_9_6_480.jpg
cs-410_9_6_140,cs-410,9,6,Probabilistic,"00:08:43,890","00:08:47,750",140,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=523,And then this line is just,pic_cs-410_9_6_480.jpg
cs-410_9_6_141,cs-410,9,6,Probabilistic,"00:08:47,750","00:08:51,130",141,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=527,"And it's very similar, just accounting for",pic_cs-410_9_6_480.jpg
cs-410_9_6_142,cs-410,9,6,Probabilistic,"00:08:52,470","00:08:54,060",142,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=532,So what are the unknown parameters?,pic_cs-410_9_6_480.jpg
cs-410_9_6_143,cs-410,9,6,Probabilistic,"00:08:54,060","00:08:55,960",143,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=534,I already said that there are two kinds.,pic_cs-410_9_6_480.jpg
cs-410_9_6_144,cs-410,9,6,Probabilistic,"00:08:55,960","00:08:59,150",144,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=535,"One is coverage,",pic_cs-410_9_6_480.jpg
cs-410_9_6_145,cs-410,9,6,Probabilistic,"00:08:59,150","00:09:02,350",145,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=539,"Again, it's a useful exercise for",pic_cs-410_9_6_480.jpg
cs-410_9_6_146,cs-410,9,6,Probabilistic,"00:09:02,350","00:09:04,730",146,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=542,Exactly how many,pic_cs-410_9_6_540.jpg
cs-410_9_6_147,cs-410,9,6,Probabilistic,"00:09:05,750","00:09:07,940",147,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=545,How many unknown parameters are there?,pic_cs-410_9_6_540.jpg
cs-410_9_6_148,cs-410,9,6,Probabilistic,"00:09:07,940","00:09:08,680",148,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=547,"Now, try and",pic_cs-410_9_6_540.jpg
cs-410_9_6_149,cs-410,9,6,Probabilistic,"00:09:08,680","00:09:13,090",149,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=548,think out that question will help you,pic_cs-410_9_6_540.jpg
cs-410_9_6_150,cs-410,9,6,Probabilistic,"00:09:13,090","00:09:17,760",150,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=553,And will also allow you to understand,pic_cs-410_9_6_540.jpg
cs-410_9_6_151,cs-410,9,6,Probabilistic,"00:09:17,760","00:09:20,430",151,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=557,when use PLSA to analyze text data?,pic_cs-410_9_6_540.jpg
cs-410_9_6_152,cs-410,9,6,Probabilistic,"00:09:20,430","00:09:22,480",152,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=560,And these are precisely,pic_cs-410_9_6_540.jpg
cs-410_9_6_153,cs-410,9,6,Probabilistic,"00:09:24,480","00:09:28,200",153,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=564,So after we have obtained,pic_cs-410_9_6_540.jpg
cs-410_9_6_154,cs-410,9,6,Probabilistic,"00:09:28,200","00:09:30,820",154,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=568,the next is to worry about,pic_cs-410_9_6_540.jpg
cs-410_9_6_155,cs-410,9,6,Probabilistic,"00:09:32,050","00:09:34,770",155,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=572,"And we can do the usual think,",pic_cs-410_9_6_540.jpg
cs-410_9_6_156,cs-410,9,6,Probabilistic,"00:09:34,770","00:09:40,190",156,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=574,"So again, it's a constrained optimization",pic_cs-410_9_6_540.jpg
cs-410_9_6_157,cs-410,9,6,Probabilistic,"00:09:40,190","00:09:44,350",157,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=580,Only that we have a collection of text and,pic_cs-410_9_6_540.jpg
cs-410_9_6_158,cs-410,9,6,Probabilistic,"00:09:44,350","00:09:48,655",158,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=584,"And we still have two constraints,",pic_cs-410_9_6_540.jpg
cs-410_9_6_159,cs-410,9,6,Probabilistic,"00:09:48,655","00:09:50,145",159,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=588,One is the word distributions.,pic_cs-410_9_6_540.jpg
cs-410_9_6_160,cs-410,9,6,Probabilistic,"00:09:51,245","00:09:56,525",160,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=591,All the words must have probabilities,pic_cs-410_9_6_540.jpg
cs-410_9_6_161,cs-410,9,6,Probabilistic,"00:09:56,525","00:09:59,975",161,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=596,The other is the topic,pic_cs-410_9_6_540.jpg
cs-410_9_6_162,cs-410,9,6,Probabilistic,"00:09:59,975","00:10:05,200",162,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=599,a document will have to cover,pic_cs-410_9_6_540.jpg
cs-410_9_6_163,cs-410,9,6,Probabilistic,"00:10:05,200","00:10:08,820",163,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=605,the probability of covering each,pic_cs-410_9_6_600.jpg
cs-410_9_6_164,cs-410,9,6,Probabilistic,"00:10:08,820","00:10:13,190",164,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=608,So at this point though it's basically,pic_cs-410_9_6_600.jpg
cs-410_9_6_165,cs-410,9,6,Probabilistic,"00:10:13,190","00:10:16,370",165,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=613,you just need to figure out,pic_cs-410_9_6_600.jpg
cs-410_9_6_166,cs-410,9,6,Probabilistic,"00:10:16,370","00:10:18,670",166,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=616,There's a function with many variables.,pic_cs-410_9_6_600.jpg
cs-410_9_6_167,cs-410,9,6,Probabilistic,"00:10:18,670","00:10:22,481",167,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=618,and we need to just figure,pic_cs-410_9_6_600.jpg
cs-410_9_6_168,cs-410,9,6,Probabilistic,"00:10:22,481","00:10:26,397",168,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=622,variables to make the function,pic_cs-410_9_6_600.jpg
cs-410_9_6_169,cs-410,9,6,Probabilistic,"00:10:26,397","00:10:36,397",169,https://www.coursera.org/learn/cs-410/lecture/N5cBh?t=626,>> [MUSIC],pic_cs-410_9_6_600.jpg
cs-410_9_7_1,cs-410,9,7,Probabilistic,"00:00:00,025","00:00:05,631",1,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=0,[SOUND] So,pic_cs-410_9_7_0.jpg
cs-410_9_7_2,cs-410,9,7,Probabilistic,"00:00:05,631","00:00:10,816",2,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=5,"PLSA to of LDA and to motivate that,",pic_cs-410_9_7_0.jpg
cs-410_9_7_3,cs-410,9,7,Probabilistic,"00:00:10,816","00:00:17,145",3,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=10,we need to talk about some,pic_cs-410_9_7_0.jpg
cs-410_9_7_4,cs-410,9,7,Probabilistic,"00:00:17,145","00:00:21,085",4,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=17,"First, it's not really a generative model",pic_cs-410_9_7_0.jpg
cs-410_9_7_5,cs-410,9,7,Probabilistic,"00:00:21,085","00:00:22,335",5,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=21,a new document.,pic_cs-410_9_7_0.jpg
cs-410_9_7_6,cs-410,9,7,Probabilistic,"00:00:22,335","00:00:26,670",6,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=22,"You can see why, and that's because the",pic_cs-410_9_7_0.jpg
cs-410_9_7_7,cs-410,9,7,Probabilistic,"00:00:26,670","00:00:31,180",7,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=26,but the pis are tied to the document,pic_cs-410_9_7_0.jpg
cs-410_9_7_8,cs-410,9,7,Probabilistic,"00:00:31,180","00:00:33,790",8,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=31,So we can't compute the pis for,pic_cs-410_9_7_0.jpg
cs-410_9_7_9,cs-410,9,7,Probabilistic,"00:00:34,810","00:00:39,030",9,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=34,"And there's some heuristic workaround,",pic_cs-410_9_7_0.jpg
cs-410_9_7_10,cs-410,9,7,Probabilistic,"00:00:39,030","00:00:42,990",10,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=39,"Secondly, it has many parameters, and I've",pic_cs-410_9_7_0.jpg
cs-410_9_7_11,cs-410,9,7,Probabilistic,"00:00:42,990","00:00:47,170",11,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=42,"exactly there are in PLSA, and",pic_cs-410_9_7_0.jpg
cs-410_9_7_12,cs-410,9,7,Probabilistic,"00:00:47,170","00:00:49,750",12,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=47,That means that model is very complex.,pic_cs-410_9_7_0.jpg
cs-410_9_7_13,cs-410,9,7,Probabilistic,"00:00:49,750","00:00:53,010",13,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=49,And this also means that there,pic_cs-410_9_7_0.jpg
cs-410_9_7_14,cs-410,9,7,Probabilistic,"00:00:53,010","00:00:55,090",14,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=53,it's prone to overfitting.,pic_cs-410_9_7_0.jpg
cs-410_9_7_15,cs-410,9,7,Probabilistic,"00:00:55,090","00:01:01,569",15,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=55,And that means it's very hard to,pic_cs-410_9_7_0.jpg
cs-410_9_7_16,cs-410,9,7,Probabilistic,"00:01:02,630","00:01:05,830",16,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=62,And that we are representing,pic_cs-410_9_7_60.jpg
cs-410_9_7_17,cs-410,9,7,Probabilistic,"00:01:05,830","00:01:09,590",17,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=65,"And in terms of explaining future data,",pic_cs-410_9_7_60.jpg
cs-410_9_7_18,cs-410,9,7,Probabilistic,"00:01:09,590","00:01:13,260",18,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=69,it will overfit the training data,pic_cs-410_9_7_60.jpg
cs-410_9_7_19,cs-410,9,7,Probabilistic,"00:01:13,260","00:01:18,010",19,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=73,The model is so flexible to fit precisely,pic_cs-410_9_7_60.jpg
cs-410_9_7_20,cs-410,9,7,Probabilistic,"00:01:18,010","00:01:22,980",20,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=78,And then it doesn't allow us to generalize,pic_cs-410_9_7_60.jpg
cs-410_9_7_21,cs-410,9,7,Probabilistic,"00:01:23,990","00:01:28,530",21,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=83,This however is not a necessary problem,pic_cs-410_9_7_60.jpg
cs-410_9_7_22,cs-410,9,7,Probabilistic,"00:01:28,530","00:01:32,150",22,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=88,only interested in hitting,pic_cs-410_9_7_60.jpg
cs-410_9_7_23,cs-410,9,7,Probabilistic,"00:01:32,150","00:01:36,980",23,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=92,We are not always interested in modern,pic_cs-410_9_7_60.jpg
cs-410_9_7_24,cs-410,9,7,Probabilistic,"00:01:36,980","00:01:40,490",24,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=96,"or if we would care about the generality,",pic_cs-410_9_7_60.jpg
cs-410_9_7_25,cs-410,9,7,Probabilistic,"00:01:42,330","00:01:46,860",25,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=102,"So LDA is proposing to improve that,",pic_cs-410_9_7_60.jpg
cs-410_9_7_26,cs-410,9,7,Probabilistic,"00:01:46,860","00:01:51,470",26,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=106,PLSA a generative model by imposing,pic_cs-410_9_7_60.jpg
cs-410_9_7_27,cs-410,9,7,Probabilistic,"00:01:51,470","00:01:56,130",27,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=111,Dirichlet is just a special distribution,pic_cs-410_9_7_60.jpg
cs-410_9_7_28,cs-410,9,7,Probabilistic,"00:01:56,130","00:02:00,120",28,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=116,"So in this sense, LDA is just",pic_cs-410_9_7_60.jpg
cs-410_9_7_29,cs-410,9,7,Probabilistic,"00:02:00,120","00:02:02,290",29,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=120,the parameters are now,pic_cs-410_9_7_120.jpg
cs-410_9_7_30,cs-410,9,7,Probabilistic,"00:02:02,290","00:02:05,570",30,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=122,You will see there are many,pic_cs-410_9_7_120.jpg
cs-410_9_7_31,cs-410,9,7,Probabilistic,"00:02:05,570","00:02:09,260",31,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=125,you can achieve the same goal as PLSA for,pic_cs-410_9_7_120.jpg
cs-410_9_7_32,cs-410,9,7,Probabilistic,"00:02:09,260","00:02:15,130",32,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=129,It means it can compute the top coverage,pic_cs-410_9_7_120.jpg
cs-410_9_7_33,cs-410,9,7,Probabilistic,"00:02:15,130","00:02:17,440",33,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=135,"However, there's no.",pic_cs-410_9_7_120.jpg
cs-410_9_7_34,cs-410,9,7,Probabilistic,"00:02:17,440","00:02:21,660",34,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=137,Why are the parameters for,pic_cs-410_9_7_120.jpg
cs-410_9_7_35,cs-410,9,7,Probabilistic,"00:02:21,660","00:02:26,530",35,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=141,there are fewer parameters and,pic_cs-410_9_7_120.jpg
cs-410_9_7_36,cs-410,9,7,Probabilistic,"00:02:26,530","00:02:29,650",36,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=146,"word distributions,",pic_cs-410_9_7_120.jpg
cs-410_9_7_37,cs-410,9,7,Probabilistic,"00:02:29,650","00:02:34,300",37,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=149,of influence of these variables because,pic_cs-410_9_7_120.jpg
cs-410_9_7_38,cs-410,9,7,Probabilistic,"00:02:34,300","00:02:38,190",38,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=154,So the influence part again,pic_cs-410_9_7_120.jpg
cs-410_9_7_39,cs-410,9,7,Probabilistic,"00:02:38,190","00:02:41,770",39,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=158,So essentially they are doing something,pic_cs-410_9_7_120.jpg
cs-410_9_7_40,cs-410,9,7,Probabilistic,"00:02:41,770","00:02:48,110",40,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=161,LDA is a more elegant way of looking,pic_cs-410_9_7_120.jpg
cs-410_9_7_41,cs-410,9,7,Probabilistic,"00:02:48,110","00:02:52,810",41,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=168,So let's see how we can,pic_cs-410_9_7_120.jpg
cs-410_9_7_42,cs-410,9,7,Probabilistic,"00:02:52,810","00:02:56,360",42,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=172,a standard PLSA to have LDA.,pic_cs-410_9_7_120.jpg
cs-410_9_7_43,cs-410,9,7,Probabilistic,"00:02:56,360","00:02:59,753",43,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=176,Now a full treatment of LDA is,pic_cs-410_9_7_120.jpg
cs-410_9_7_44,cs-410,9,7,Probabilistic,"00:02:59,753","00:03:03,285",44,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=179,we just don't have time to go in,pic_cs-410_9_7_120.jpg
cs-410_9_7_45,cs-410,9,7,Probabilistic,"00:03:03,285","00:03:07,040",45,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=183,"But here, I just want to give you",pic_cs-410_9_7_180.jpg
cs-410_9_7_46,cs-410,9,7,Probabilistic,"00:03:07,040","00:03:08,590",46,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=187,"what it enables, all right.",pic_cs-410_9_7_180.jpg
cs-410_9_7_47,cs-410,9,7,Probabilistic,"00:03:08,590","00:03:10,831",47,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=188,So this is the picture of LDA.,pic_cs-410_9_7_180.jpg
cs-410_9_7_48,cs-410,9,7,Probabilistic,"00:03:10,831","00:03:14,940",48,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=190,"Now, I remove the background",pic_cs-410_9_7_180.jpg
cs-410_9_7_49,cs-410,9,7,Probabilistic,"00:03:15,960","00:03:19,960",49,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=195,"Now, in this model, all these",pic_cs-410_9_7_180.jpg
cs-410_9_7_50,cs-410,9,7,Probabilistic,"00:03:19,960","00:03:22,220",50,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=199,we do not impose any prior.,pic_cs-410_9_7_180.jpg
cs-410_9_7_51,cs-410,9,7,Probabilistic,"00:03:22,220","00:03:28,650",51,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=202,So these word distributions are now,pic_cs-410_9_7_180.jpg
cs-410_9_7_52,cs-410,9,7,Probabilistic,"00:03:28,650","00:03:32,490",52,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=208,"So these are word distributions, so here.",pic_cs-410_9_7_180.jpg
cs-410_9_7_53,cs-410,9,7,Probabilistic,"00:03:32,490","00:03:35,520",53,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=212,And the other set of parameters are pis.,pic_cs-410_9_7_180.jpg
cs-410_9_7_54,cs-410,9,7,Probabilistic,"00:03:35,520","00:03:37,470",54,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=215,And we would present it as a vector also.,pic_cs-410_9_7_180.jpg
cs-410_9_7_55,cs-410,9,7,Probabilistic,"00:03:37,470","00:03:40,760",55,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=217,And this is more convenient,pic_cs-410_9_7_180.jpg
cs-410_9_7_56,cs-410,9,7,Probabilistic,"00:03:40,760","00:03:44,040",56,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=220,And we have one vector for each document.,pic_cs-410_9_7_180.jpg
cs-410_9_7_57,cs-410,9,7,Probabilistic,"00:03:44,040","00:03:48,820",57,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=224,"And in this case, in theta,",pic_cs-410_9_7_180.jpg
cs-410_9_7_58,cs-410,9,7,Probabilistic,"00:03:50,140","00:03:53,470",58,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=230,"Now, the difference between LDA and",pic_cs-410_9_7_180.jpg
cs-410_9_7_59,cs-410,9,7,Probabilistic,"00:03:53,470","00:03:58,390",59,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=233,"PLSA is that in LDA, we're not going",pic_cs-410_9_7_180.jpg
cs-410_9_7_60,cs-410,9,7,Probabilistic,"00:03:58,390","00:04:02,170",60,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=238,"Instead, we're going to force them to",pic_cs-410_9_7_180.jpg
cs-410_9_7_61,cs-410,9,7,Probabilistic,"00:04:03,400","00:04:04,900",61,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=243,"So more specifically,",pic_cs-410_9_7_240.jpg
cs-410_9_7_62,cs-410,9,7,Probabilistic,"00:04:04,900","00:04:09,760",62,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=244,they will be drawn from two Dirichlet,pic_cs-410_9_7_240.jpg
cs-410_9_7_63,cs-410,9,7,Probabilistic,"00:04:09,760","00:04:12,880",63,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=249,the Dirichlet distribution is,pic_cs-410_9_7_240.jpg
cs-410_9_7_64,cs-410,9,7,Probabilistic,"00:04:12,880","00:04:16,600",64,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=252,So it gives us a probability of,pic_cs-410_9_7_240.jpg
cs-410_9_7_65,cs-410,9,7,Probabilistic,"00:04:16,600","00:04:19,190",65,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=256,"Take, for example, pis, right.",pic_cs-410_9_7_240.jpg
cs-410_9_7_66,cs-410,9,7,Probabilistic,"00:04:19,190","00:04:25,100",66,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=259,So this Dirichlet distribution tells,pic_cs-410_9_7_240.jpg
cs-410_9_7_67,cs-410,9,7,Probabilistic,"00:04:25,100","00:04:29,390",67,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=265,And this distribution in itself is,pic_cs-410_9_7_240.jpg
cs-410_9_7_68,cs-410,9,7,Probabilistic,"00:04:29,390","00:04:30,040",68,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=269,of alphas.,pic_cs-410_9_7_240.jpg
cs-410_9_7_69,cs-410,9,7,Probabilistic,"00:04:31,790","00:04:35,130",69,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=271,"Depending on the alphas, we can",pic_cs-410_9_7_240.jpg
cs-410_9_7_70,cs-410,9,7,Probabilistic,"00:04:35,130","00:04:39,650",70,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=275,ways but with full certain choices of,pic_cs-410_9_7_240.jpg
cs-410_9_7_71,cs-410,9,7,Probabilistic,"00:04:39,650","00:04:40,230",71,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=279,"For example,",pic_cs-410_9_7_240.jpg
cs-410_9_7_72,cs-410,9,7,Probabilistic,"00:04:40,230","00:04:45,910",72,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=280,you might favor the choice of a relatively,pic_cs-410_9_7_240.jpg
cs-410_9_7_73,cs-410,9,7,Probabilistic,"00:04:45,910","00:04:51,090",73,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=285,Or you might favor generating,pic_cs-410_9_7_240.jpg
cs-410_9_7_74,cs-410,9,7,Probabilistic,"00:04:51,090","00:04:53,000",74,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=291,and this is controlled by alpha.,pic_cs-410_9_7_240.jpg
cs-410_9_7_75,cs-410,9,7,Probabilistic,"00:04:53,000","00:04:56,892",75,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=293,"And similarly here, the topic or",pic_cs-410_9_7_240.jpg
cs-410_9_7_76,cs-410,9,7,Probabilistic,"00:04:56,892","00:05:01,470",76,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=296,from another Dirichlet,pic_cs-410_9_7_240.jpg
cs-410_9_7_77,cs-410,9,7,Probabilistic,"00:05:01,470","00:05:04,450",77,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=301,"And note that here,",pic_cs-410_9_7_300.jpg
cs-410_9_7_78,cs-410,9,7,Probabilistic,"00:05:04,450","00:05:10,260",78,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=304,corresponding to our inference on,pic_cs-410_9_7_300.jpg
cs-410_9_7_79,cs-410,9,7,Probabilistic,"00:05:10,260","00:05:10,940",79,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=310,"Whereas here,",pic_cs-410_9_7_300.jpg
cs-410_9_7_80,cs-410,9,7,Probabilistic,"00:05:10,940","00:05:16,670",80,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=310,beta has n values corresponding to,pic_cs-410_9_7_300.jpg
cs-410_9_7_81,cs-410,9,7,Probabilistic,"00:05:17,700","00:05:22,740",81,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=317,"Now once we impose this price, then",pic_cs-410_9_7_300.jpg
cs-410_9_7_82,cs-410,9,7,Probabilistic,"00:05:22,740","00:05:27,667",82,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=322,And we start with joined pis from,pic_cs-410_9_7_300.jpg
cs-410_9_7_83,cs-410,9,7,Probabilistic,"00:05:27,667","00:05:32,380",83,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=327,the Dirichlet distribution and,pic_cs-410_9_7_300.jpg
cs-410_9_7_84,cs-410,9,7,Probabilistic,"00:05:35,370","00:05:40,990",84,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=335,"And then, we're going to use the pi",pic_cs-410_9_7_300.jpg
cs-410_9_7_85,cs-410,9,7,Probabilistic,"00:05:40,990","00:05:45,750",85,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=340,"to use, and this is of course",pic_cs-410_9_7_300.jpg
cs-410_9_7_86,cs-410,9,7,Probabilistic,"00:05:47,250","00:05:51,580",86,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=347,"And similar here, we're not going",pic_cs-410_9_7_300.jpg
cs-410_9_7_87,cs-410,9,7,Probabilistic,"00:05:51,580","00:05:56,900",87,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=351,"Instead, we're going to draw one",pic_cs-410_9_7_300.jpg
cs-410_9_7_88,cs-410,9,7,Probabilistic,"00:05:56,900","00:06:01,960",88,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=356,"And then from this,",pic_cs-410_9_7_300.jpg
cs-410_9_7_89,cs-410,9,7,Probabilistic,"00:06:01,960","00:06:04,739",89,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=361,And the rest is very similar to the.,pic_cs-410_9_7_360.jpg
cs-410_9_7_90,cs-410,9,7,Probabilistic,"00:06:04,739","00:06:07,550",90,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=364,The likelihood function now,pic_cs-410_9_7_360.jpg
cs-410_9_7_91,cs-410,9,7,Probabilistic,"00:06:07,550","00:06:12,130",91,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=367,But there's a close connection between the,pic_cs-410_9_7_360.jpg
cs-410_9_7_92,cs-410,9,7,Probabilistic,"00:06:12,130","00:06:15,240",92,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=372,So I'm going to illustrate,pic_cs-410_9_7_360.jpg
cs-410_9_7_93,cs-410,9,7,Probabilistic,"00:06:15,240","00:06:16,090",93,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=375,"So in the top,",pic_cs-410_9_7_360.jpg
cs-410_9_7_94,cs-410,9,7,Probabilistic,"00:06:16,090","00:06:20,730",94,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=376,you see PLSA likelihood function,pic_cs-410_9_7_360.jpg
cs-410_9_7_95,cs-410,9,7,Probabilistic,"00:06:20,730","00:06:22,760",95,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=380,It's copied from previous slide.,pic_cs-410_9_7_360.jpg
cs-410_9_7_96,cs-410,9,7,Probabilistic,"00:06:22,760","00:06:25,820",96,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=382,Only that I dropped the background for,pic_cs-410_9_7_360.jpg
cs-410_9_7_97,cs-410,9,7,Probabilistic,"00:06:27,160","00:06:32,100",97,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=387,So in the LDA formulas you,pic_cs-410_9_7_360.jpg
cs-410_9_7_98,cs-410,9,7,Probabilistic,"00:06:32,100","00:06:34,970",98,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=392,You see the first equation,pic_cs-410_9_7_360.jpg
cs-410_9_7_99,cs-410,9,7,Probabilistic,"00:06:34,970","00:06:39,140",99,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=394,And this is the probability of generating,pic_cs-410_9_7_360.jpg
cs-410_9_7_100,cs-410,9,7,Probabilistic,"00:06:40,690","00:06:45,440",100,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=400,And this formula is a sum of all,pic_cs-410_9_7_360.jpg
cs-410_9_7_101,cs-410,9,7,Probabilistic,"00:06:45,440","00:06:50,230",101,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=405,Inside a sum is a product of,pic_cs-410_9_7_360.jpg
cs-410_9_7_102,cs-410,9,7,Probabilistic,"00:06:50,230","00:06:54,080",102,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=410,multiplied by the probability of,pic_cs-410_9_7_360.jpg
cs-410_9_7_103,cs-410,9,7,Probabilistic,"00:06:55,180","00:06:59,100",103,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=415,"So this is a very important formula,",pic_cs-410_9_7_360.jpg
cs-410_9_7_104,cs-410,9,7,Probabilistic,"00:06:59,100","00:07:02,800",104,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=419,And this is actually the core,pic_cs-410_9_7_360.jpg
cs-410_9_7_105,cs-410,9,7,Probabilistic,"00:07:02,800","00:07:06,760",105,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=422,And you might see other topic models,pic_cs-410_9_7_420.jpg
cs-410_9_7_106,cs-410,9,7,Probabilistic,"00:07:06,760","00:07:08,230",106,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=426,And they all rely on this.,pic_cs-410_9_7_420.jpg
cs-410_9_7_107,cs-410,9,7,Probabilistic,"00:07:08,230","00:07:11,040",107,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=428,So it's very important to understand this.,pic_cs-410_9_7_420.jpg
cs-410_9_7_108,cs-410,9,7,Probabilistic,"00:07:11,040","00:07:15,140",108,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=431,And this gives us a probability of,pic_cs-410_9_7_420.jpg
cs-410_9_7_109,cs-410,9,7,Probabilistic,"00:07:15,140","00:07:20,930",109,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=435,"Now, next in the probability of",pic_cs-410_9_7_420.jpg
cs-410_9_7_110,cs-410,9,7,Probabilistic,"00:07:20,930","00:07:26,710",110,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=440,"component in the LDA formula, but the LDA",pic_cs-410_9_7_420.jpg
cs-410_9_7_111,cs-410,9,7,Probabilistic,"00:07:26,710","00:07:32,930",111,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=446,And that's to account for,pic_cs-410_9_7_420.jpg
cs-410_9_7_112,cs-410,9,7,Probabilistic,"00:07:32,930","00:07:39,180",112,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=452,So they are drawn from the original,pic_cs-410_9_7_420.jpg
cs-410_9_7_113,cs-410,9,7,Probabilistic,"00:07:39,180","00:07:43,210",113,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=459,"That's why we have to take an integral,",pic_cs-410_9_7_420.jpg
cs-410_9_7_114,cs-410,9,7,Probabilistic,"00:07:43,210","00:07:48,374",114,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=463,could possibly draw from,pic_cs-410_9_7_420.jpg
cs-410_9_7_115,cs-410,9,7,Probabilistic,"00:07:48,374","00:07:52,910",115,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=468,And similarly in the likelihood for,pic_cs-410_9_7_420.jpg
cs-410_9_7_116,cs-410,9,7,Probabilistic,"00:07:52,910","00:07:56,570",116,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=472,"we also see further components added,",pic_cs-410_9_7_420.jpg
cs-410_9_7_117,cs-410,9,7,Probabilistic,"00:07:58,190","00:07:58,760",117,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=478,Right?,pic_cs-410_9_7_420.jpg
cs-410_9_7_118,cs-410,9,7,Probabilistic,"00:07:58,760","00:08:03,345",118,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=478,So basically in the area we're just,pic_cs-410_9_7_420.jpg
cs-410_9_7_119,cs-410,9,7,Probabilistic,"00:08:03,345","00:08:08,306",119,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=483,the uncertainties and we added of course,pic_cs-410_9_7_480.jpg
cs-410_9_7_120,cs-410,9,7,Probabilistic,"00:08:08,306","00:08:11,480",120,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=488,"the choice of this parameters,",pic_cs-410_9_7_480.jpg
cs-410_9_7_121,cs-410,9,7,Probabilistic,"00:08:12,910","00:08:15,276",121,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=492,So this is a likelihood function for LDA.,pic_cs-410_9_7_480.jpg
cs-410_9_7_122,cs-410,9,7,Probabilistic,"00:08:15,276","00:08:19,760",122,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=495,"Now, next to this, let's talk about the",pic_cs-410_9_7_480.jpg
cs-410_9_7_123,cs-410,9,7,Probabilistic,"00:08:19,760","00:08:23,730",123,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=499,Now the parameters can be now estimated,pic_cs-410_9_7_480.jpg
cs-410_9_7_124,cs-410,9,7,Probabilistic,"00:08:23,730","00:08:25,280",124,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=503,maximum likelihood estimate for LDA.,pic_cs-410_9_7_480.jpg
cs-410_9_7_125,cs-410,9,7,Probabilistic,"00:08:25,280","00:08:31,270",125,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=505,Now you might think about how many,pic_cs-410_9_7_480.jpg
cs-410_9_7_126,cs-410,9,7,Probabilistic,"00:08:31,270","00:08:35,050",126,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=511,You'll see there're a fewer parameters,pic_cs-410_9_7_480.jpg
cs-410_9_7_127,cs-410,9,7,Probabilistic,"00:08:35,050","00:08:37,850",127,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=515,parameters are alphas and the betas.,pic_cs-410_9_7_480.jpg
cs-410_9_7_128,cs-410,9,7,Probabilistic,"00:08:37,850","00:08:41,330",128,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=517,So we can use the maximum likelihood,pic_cs-410_9_7_480.jpg
cs-410_9_7_129,cs-410,9,7,Probabilistic,"00:08:41,330","00:08:45,510",129,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=521,"Of course, it's more complicated because",pic_cs-410_9_7_480.jpg
cs-410_9_7_130,cs-410,9,7,Probabilistic,"00:08:45,510","00:08:46,890",130,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=525,more complicated.,pic_cs-410_9_7_480.jpg
cs-410_9_7_131,cs-410,9,7,Probabilistic,"00:08:46,890","00:08:51,740",131,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=526,But what's also important,pic_cs-410_9_7_480.jpg
cs-410_9_7_132,cs-410,9,7,Probabilistic,"00:08:51,740","00:08:56,350",132,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=531,parameters that we are interested,pic_cs-410_9_7_480.jpg
cs-410_9_7_133,cs-410,9,7,Probabilistic,"00:08:56,350","00:09:00,240",133,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=536,the coverage are no,pic_cs-410_9_7_480.jpg
cs-410_9_7_134,cs-410,9,7,Probabilistic,"00:09:00,240","00:09:04,110",134,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=540,In this case we have to,pic_cs-410_9_7_540.jpg
cs-410_9_7_135,cs-410,9,7,Probabilistic,"00:09:04,110","00:09:09,700",135,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=544,posterior inference to compute them based,pic_cs-410_9_7_540.jpg
cs-410_9_7_136,cs-410,9,7,Probabilistic,"00:09:09,700","00:09:13,900",136,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=549,"Unfortunately, this",pic_cs-410_9_7_540.jpg
cs-410_9_7_137,cs-410,9,7,Probabilistic,"00:09:13,900","00:09:17,570",137,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=553,So we generally have to resort,pic_cs-410_9_7_540.jpg
cs-410_9_7_138,cs-410,9,7,Probabilistic,"00:09:18,720","00:09:24,220",138,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=558,And there are many methods available for,pic_cs-410_9_7_540.jpg
cs-410_9_7_139,cs-410,9,7,Probabilistic,"00:09:24,220","00:09:29,100",139,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=564,see them when you use different tool kits,pic_cs-410_9_7_540.jpg
cs-410_9_7_140,cs-410,9,7,Probabilistic,"00:09:30,800","00:09:35,120",140,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=570,these different extensions of LDA.,pic_cs-410_9_7_540.jpg
cs-410_9_7_141,cs-410,9,7,Probabilistic,"00:09:35,120","00:09:39,210",141,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=575,"Now here we, of course, can't give",pic_cs-410_9_7_540.jpg
cs-410_9_7_142,cs-410,9,7,Probabilistic,"00:09:39,210","00:09:43,189",142,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=579,just know that they are computed based in,pic_cs-410_9_7_540.jpg
cs-410_9_7_143,cs-410,9,7,Probabilistic,"00:09:43,189","00:09:50,386",143,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=583,inference by using,pic_cs-410_9_7_540.jpg
cs-410_9_7_144,cs-410,9,7,Probabilistic,"00:09:50,386","00:09:53,820",144,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=590,"But our math [INAUDIBLE],",pic_cs-410_9_7_540.jpg
cs-410_9_7_145,cs-410,9,7,Probabilistic,"00:09:53,820","00:09:57,900",145,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=593,"in some of our math list,",pic_cs-410_9_7_540.jpg
cs-410_9_7_146,cs-410,9,7,Probabilistic,"00:09:57,900","00:10:02,720",146,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=597,"And, especially when we use",pic_cs-410_9_7_540.jpg
cs-410_9_7_147,cs-410,9,7,Probabilistic,"00:10:02,720","00:10:06,260",147,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=602,then the algorithm looks very,pic_cs-410_9_7_600.jpg
cs-410_9_7_148,cs-410,9,7,Probabilistic,"00:10:06,260","00:10:08,800",148,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=606,"So in the end,",pic_cs-410_9_7_600.jpg
cs-410_9_7_149,cs-410,9,7,Probabilistic,"00:10:10,660","00:10:14,950",149,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=610,So to summarize our discussion,pic_cs-410_9_7_600.jpg
cs-410_9_7_150,cs-410,9,7,Probabilistic,"00:10:14,950","00:10:17,340",150,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=614,these models provide,pic_cs-410_9_7_600.jpg
cs-410_9_7_151,cs-410,9,7,Probabilistic,"00:10:17,340","00:10:22,300",151,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=617,way of mining and analyzing topics,pic_cs-410_9_7_600.jpg
cs-410_9_7_152,cs-410,9,7,Probabilistic,"00:10:22,300","00:10:27,010",152,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=622,The best basic task setup is,pic_cs-410_9_7_600.jpg
cs-410_9_7_153,cs-410,9,7,Probabilistic,"00:10:27,010","00:10:29,540",153,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=627,we're going to output the k topics.,pic_cs-410_9_7_600.jpg
cs-410_9_7_154,cs-410,9,7,Probabilistic,"00:10:29,540","00:10:32,610",154,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=629,Each topic is characterized,pic_cs-410_9_7_600.jpg
cs-410_9_7_155,cs-410,9,7,Probabilistic,"00:10:32,610","00:10:36,999",155,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=632,And we're going to also output proportions,pic_cs-410_9_7_600.jpg
cs-410_9_7_156,cs-410,9,7,Probabilistic,"00:10:38,990","00:10:45,320",156,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=638,"And PLSA is the basic topic model, and",pic_cs-410_9_7_600.jpg
cs-410_9_7_157,cs-410,9,7,Probabilistic,"00:10:45,320","00:10:48,310",157,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=645,And this is often adequate for,pic_cs-410_9_7_600.jpg
cs-410_9_7_158,cs-410,9,7,Probabilistic,"00:10:48,310","00:10:51,800",158,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=648,That's why we spend a lot of,pic_cs-410_9_7_600.jpg
cs-410_9_7_159,cs-410,9,7,Probabilistic,"00:10:53,190","00:10:57,050",159,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=653,Now LDA improves over,pic_cs-410_9_7_600.jpg
cs-410_9_7_160,cs-410,9,7,Probabilistic,"00:10:57,050","00:11:00,650",160,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=657,This has led to theoretically,pic_cs-410_9_7_600.jpg
cs-410_9_7_161,cs-410,9,7,Probabilistic,"00:11:00,650","00:11:05,740",161,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=660,"However, in practice, LDA and",pic_cs-410_9_7_660.jpg
cs-410_9_7_162,cs-410,9,7,Probabilistic,"00:11:05,740","00:11:10,890",162,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=665,in practice PLSA and LDA would work,pic_cs-410_9_7_660.jpg
cs-410_9_7_163,cs-410,9,7,Probabilistic,"00:11:12,290","00:11:16,140",163,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=672,Now here are some suggested readings if,pic_cs-410_9_7_660.jpg
cs-410_9_7_164,cs-410,9,7,Probabilistic,"00:11:16,140","00:11:19,340",164,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=676,First is a nice review of,pic_cs-410_9_7_660.jpg
cs-410_9_7_165,cs-410,9,7,Probabilistic,"00:11:20,490","00:11:25,610",165,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=680,The second has a discussion about how,pic_cs-410_9_7_660.jpg
cs-410_9_7_166,cs-410,9,7,Probabilistic,"00:11:25,610","00:11:29,840",166,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=685,Now I've shown you some distributions and,pic_cs-410_9_7_660.jpg
cs-410_9_7_167,cs-410,9,7,Probabilistic,"00:11:29,840","00:11:31,690",167,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=689,But what exactly is a topic?,pic_cs-410_9_7_660.jpg
cs-410_9_7_168,cs-410,9,7,Probabilistic,"00:11:31,690","00:11:35,600",168,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=691,Can we use phrases to label the topic?,pic_cs-410_9_7_660.jpg
cs-410_9_7_169,cs-410,9,7,Probabilistic,"00:11:35,600","00:11:37,720",169,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=695,To make it the more easy to understand and,pic_cs-410_9_7_660.jpg
cs-410_9_7_170,cs-410,9,7,Probabilistic,"00:11:37,720","00:11:40,480",170,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=697,this paper is about the techniques for,pic_cs-410_9_7_660.jpg
cs-410_9_7_171,cs-410,9,7,Probabilistic,"00:11:40,480","00:11:45,820",171,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=700,The third one is empirical comparison,pic_cs-410_9_7_660.jpg
cs-410_9_7_172,cs-410,9,7,Probabilistic,"00:11:45,820","00:11:48,985",172,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=705,The conclusion is that they,pic_cs-410_9_7_660.jpg
cs-410_9_7_173,cs-410,9,7,Probabilistic,"00:11:48,985","00:11:58,985",173,https://www.coursera.org/learn/cs-410/lecture/HKe8K?t=708,[MUSIC],pic_cs-410_9_7_660.jpg
cs-410_10_1_1,cs-410,10,1,Text,"00:00:00,000","00:00:06,538",1,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=0,[MUSIC],pic_cs-410_10_1_0.jpg
cs-410_10_1_2,cs-410,10,1,Text,"00:00:06,538","00:00:10,520",2,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=6,This lecture is about,pic_cs-410_10_1_0.jpg
cs-410_10_1_3,cs-410,10,1,Text,"00:00:12,830","00:00:18,240",3,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=12,So far we have talked about multiple,pic_cs-410_10_1_0.jpg
cs-410_10_1_4,cs-410,10,1,Text,"00:00:18,240","00:00:21,430",4,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=18,how do we know which,pic_cs-410_10_1_0.jpg
cs-410_10_1_5,cs-410,10,1,Text,"00:00:22,950","00:00:25,540",5,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=22,So this has to do with evaluation.,pic_cs-410_10_1_0.jpg
cs-410_10_1_6,cs-410,10,1,Text,"00:00:25,540","00:00:28,030",6,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=25,Now to talk about evaluation one must,pic_cs-410_10_1_0.jpg
cs-410_10_1_7,cs-410,10,1,Text,"00:00:28,030","00:00:31,480",7,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=28,go back to the clustering bias that,pic_cs-410_10_1_0.jpg
cs-410_10_1_8,cs-410,10,1,Text,"00:00:32,560","00:00:36,770",8,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=32,Because two objects can be similar,pic_cs-410_10_1_0.jpg
cs-410_10_1_9,cs-410,10,1,Text,"00:00:37,780","00:00:41,910",9,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=37,we must clearly specify,pic_cs-410_10_1_0.jpg
cs-410_10_1_10,cs-410,10,1,Text,"00:00:41,910","00:00:46,200",10,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=41,"Without that, the problem of",pic_cs-410_10_1_0.jpg
cs-410_10_1_11,cs-410,10,1,Text,"00:00:46,200","00:00:51,740",11,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=46,So this perspective is also,pic_cs-410_10_1_0.jpg
cs-410_10_1_12,cs-410,10,1,Text,"00:00:51,740","00:00:53,350",12,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=51,"If you look at this slide, and",pic_cs-410_10_1_0.jpg
cs-410_10_1_13,cs-410,10,1,Text,"00:00:53,350","00:00:58,830",13,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=53,you can see we have two different,pic_cs-410_10_1_0.jpg
cs-410_10_1_14,cs-410,10,1,Text,"00:00:58,830","00:01:03,640",14,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=58,"if you ask a question, which one is",pic_cs-410_10_1_0.jpg
cs-410_10_1_15,cs-410,10,1,Text,"00:01:03,640","00:01:07,860",15,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=63,"You actually see, there's no way to answer",pic_cs-410_10_1_60.jpg
cs-410_10_1_16,cs-410,10,1,Text,"00:01:07,860","00:01:13,420",16,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=67,"we'd like to cluster based on shapes,",pic_cs-410_10_1_60.jpg
cs-410_10_1_17,cs-410,10,1,Text,"00:01:13,420","00:01:17,480",17,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=73,And that's precisely why,pic_cs-410_10_1_60.jpg
cs-410_10_1_18,cs-410,10,1,Text,"00:01:17,480","00:01:18,680",18,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=77,crucial for evaluation.,pic_cs-410_10_1_60.jpg
cs-410_10_1_19,cs-410,10,1,Text,"00:01:19,710","00:01:23,530",19,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=79,"In general,",pic_cs-410_10_1_60.jpg
cs-410_10_1_20,cs-410,10,1,Text,"00:01:23,530","00:01:27,580",20,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=83,"one is direct evaluation, and",pic_cs-410_10_1_60.jpg
cs-410_10_1_21,cs-410,10,1,Text,"00:01:27,580","00:01:29,310",21,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=87,"So in direct evaluation,",pic_cs-410_10_1_60.jpg
cs-410_10_1_22,cs-410,10,1,Text,"00:01:29,310","00:01:34,060",22,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=89,"we want to answer the following questions,",pic_cs-410_10_1_60.jpg
cs-410_10_1_23,cs-410,10,1,Text,"00:01:34,060","00:01:37,150",23,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=94,clusters to the ideal clusters,pic_cs-410_10_1_60.jpg
cs-410_10_1_24,cs-410,10,1,Text,"00:01:38,580","00:01:43,040",24,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=98,So the closeness here can be assessed,pic_cs-410_10_1_60.jpg
cs-410_10_1_25,cs-410,10,1,Text,"00:01:44,420","00:01:50,330",25,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=104,from multiple perspectives and,pic_cs-410_10_1_60.jpg
cs-410_10_1_26,cs-410,10,1,Text,"00:01:50,330","00:01:55,320",26,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=110,"of cluster result in multiple angles,",pic_cs-410_10_1_60.jpg
cs-410_10_1_27,cs-410,10,1,Text,"00:01:56,790","00:02:04,010",27,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=116,Now we also want to quantify,pic_cs-410_10_1_60.jpg
cs-410_10_1_28,cs-410,10,1,Text,"00:02:04,010","00:02:08,500",28,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=124,us to easily compare different measures,pic_cs-410_10_1_120.jpg
cs-410_10_1_29,cs-410,10,1,Text,"00:02:09,870","00:02:14,630",29,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=129,"And finally, you can see, in this case,",pic_cs-410_10_1_120.jpg
cs-410_10_1_30,cs-410,10,1,Text,"00:02:15,660","00:02:21,660",30,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=135,"by using humans, basically humans",pic_cs-410_10_1_120.jpg
cs-410_10_1_31,cs-410,10,1,Text,"00:02:21,660","00:02:24,260",31,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=141,desire to clustering bias.,pic_cs-410_10_1_120.jpg
cs-410_10_1_32,cs-410,10,1,Text,"00:02:24,260","00:02:25,610",32,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=144,"Now, how do we do that exactly?",pic_cs-410_10_1_120.jpg
cs-410_10_1_33,cs-410,10,1,Text,"00:02:25,610","00:02:27,519",33,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=145,"Well, the general procedure",pic_cs-410_10_1_120.jpg
cs-410_10_1_34,cs-410,10,1,Text,"00:02:28,590","00:02:33,290",34,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=148,Given a test set which consists,pic_cs-410_10_1_120.jpg
cs-410_10_1_35,cs-410,10,1,Text,"00:02:33,290","00:02:37,100",35,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=153,we can have humans to create,pic_cs-410_10_1_120.jpg
cs-410_10_1_36,cs-410,10,1,Text,"00:02:37,100","00:02:42,390",36,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=157,we're going to ask humans to partition,pic_cs-410_10_1_120.jpg
cs-410_10_1_37,cs-410,10,1,Text,"00:02:42,390","00:02:48,490",37,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=162,And they will use their judgments based,pic_cs-410_10_1_120.jpg
cs-410_10_1_38,cs-410,10,1,Text,"00:02:48,490","00:02:54,470",38,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=168,to generate what they think are the best,pic_cs-410_10_1_120.jpg
cs-410_10_1_39,cs-410,10,1,Text,"00:02:54,470","00:02:59,990",39,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=174,used to compare with the system generated,pic_cs-410_10_1_120.jpg
cs-410_10_1_40,cs-410,10,1,Text,"00:03:01,380","00:03:05,700",40,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=181,"And ideally, we want the system results",pic_cs-410_10_1_180.jpg
cs-410_10_1_41,cs-410,10,1,Text,"00:03:05,700","00:03:08,650",41,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=185,"results, but in general,",pic_cs-410_10_1_180.jpg
cs-410_10_1_42,cs-410,10,1,Text,"00:03:08,650","00:03:12,590",42,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=188,So we would like to then quantify the,pic_cs-410_10_1_180.jpg
cs-410_10_1_43,cs-410,10,1,Text,"00:03:12,590","00:03:15,410",43,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=192,clusters and the gold standard clusters.,pic_cs-410_10_1_180.jpg
cs-410_10_1_44,cs-410,10,1,Text,"00:03:15,410","00:03:20,110",44,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=195,And this similarity can also be measure,pic_cs-410_10_1_180.jpg
cs-410_10_1_45,cs-410,10,1,Text,"00:03:20,110","00:03:26,750",45,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=200,give us various meshes to quantitatively,pic_cs-410_10_1_180.jpg
cs-410_10_1_46,cs-410,10,1,Text,"00:03:26,750","00:03:34,140",46,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=206,And some of the commonly used measures,pic_cs-410_10_1_180.jpg
cs-410_10_1_47,cs-410,10,1,Text,"00:03:34,140","00:03:40,015",47,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=214,a cluster has a similar object from,pic_cs-410_10_1_180.jpg
cs-410_10_1_48,cs-410,10,1,Text,"00:03:40,015","00:03:45,545",48,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=220,And normalized mutual information,pic_cs-410_10_1_180.jpg
cs-410_10_1_49,cs-410,10,1,Text,"00:03:45,545","00:03:50,485",49,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=225,which basically measures,pic_cs-410_10_1_180.jpg
cs-410_10_1_50,cs-410,10,1,Text,"00:03:50,485","00:03:54,935",50,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=230,cluster of object in the system generally.,pic_cs-410_10_1_180.jpg
cs-410_10_1_51,cs-410,10,1,Text,"00:03:54,935","00:04:00,225",51,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=234,How well can you predict the cluster,pic_cs-410_10_1_180.jpg
cs-410_10_1_52,cs-410,10,1,Text,"00:04:00,225","00:04:01,440",52,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=240,vice versa?,pic_cs-410_10_1_240.jpg
cs-410_10_1_53,cs-410,10,1,Text,"00:04:01,440","00:04:06,315",53,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=241,"And mutual information captures, the",pic_cs-410_10_1_240.jpg
cs-410_10_1_54,cs-410,10,1,Text,"00:04:06,315","00:04:11,100",54,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=246,and normalized mutual information is often,pic_cs-410_10_1_240.jpg
cs-410_10_1_55,cs-410,10,1,Text,"00:04:11,100","00:04:15,300",55,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=251,used for quantifying the similarity for,pic_cs-410_10_1_240.jpg
cs-410_10_1_56,cs-410,10,1,Text,"00:04:15,300","00:04:19,220",56,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=255,"this evaluation purpose,",pic_cs-410_10_1_240.jpg
cs-410_10_1_57,cs-410,10,1,Text,"00:04:21,340","00:04:24,405",57,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=261,Now again a thorough discussion,pic_cs-410_10_1_240.jpg
cs-410_10_1_58,cs-410,10,1,Text,"00:04:24,405","00:04:28,810",58,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=264,these evaluation issues would be,pic_cs-410_10_1_240.jpg
cs-410_10_1_59,cs-410,10,1,Text,"00:04:29,820","00:04:34,730",59,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=269,I've suggested some reading in,pic_cs-410_10_1_240.jpg
cs-410_10_1_60,cs-410,10,1,Text,"00:04:34,730","00:04:35,840",60,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=274,at to know more about that.,pic_cs-410_10_1_240.jpg
cs-410_10_1_61,cs-410,10,1,Text,"00:04:36,950","00:04:39,830",61,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=276,So here I just want to,pic_cs-410_10_1_240.jpg
cs-410_10_1_62,cs-410,10,1,Text,"00:04:39,830","00:04:43,830",62,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=279,that would allow you to think about how,pic_cs-410_10_1_240.jpg
cs-410_10_1_63,cs-410,10,1,Text,"00:04:43,830","00:04:48,120",63,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=283,The second way to evaluate text,pic_cs-410_10_1_240.jpg
cs-410_10_1_64,cs-410,10,1,Text,"00:04:48,120","00:04:52,540",64,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=288,"So in this case the question to answer is,",pic_cs-410_10_1_240.jpg
cs-410_10_1_65,cs-410,10,1,Text,"00:04:52,540","00:04:55,120",65,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=292,the intended applications?,pic_cs-410_10_1_240.jpg
cs-410_10_1_66,cs-410,10,1,Text,"00:04:55,120","00:04:59,730",66,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=295,Now this of course is application,pic_cs-410_10_1_240.jpg
cs-410_10_1_67,cs-410,10,1,Text,"00:04:59,730","00:05:05,390",67,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=299,usefulness is going to depend,pic_cs-410_10_1_240.jpg
cs-410_10_1_68,cs-410,10,1,Text,"00:05:07,140","00:05:11,180",68,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=307,"In this case, the clustering bias is",pic_cs-410_10_1_300.jpg
cs-410_10_1_69,cs-410,10,1,Text,"00:05:11,180","00:05:12,850",69,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=311,"as well, so",pic_cs-410_10_1_300.jpg
cs-410_10_1_70,cs-410,10,1,Text,"00:05:12,850","00:05:17,790",70,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=312,what counts as a best cluster result,pic_cs-410_10_1_300.jpg
cs-410_10_1_71,cs-410,10,1,Text,"00:05:19,098","00:05:25,120",71,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=319,Now procedure wise we also would create,pic_cs-410_10_1_300.jpg
cs-410_10_1_72,cs-410,10,1,Text,"00:05:25,120","00:05:29,860",72,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=325,the intended application to quantify,pic_cs-410_10_1_300.jpg
cs-410_10_1_73,cs-410,10,1,Text,"00:05:32,880","00:05:38,870",73,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=332,"In this case,",pic_cs-410_10_1_300.jpg
cs-410_10_1_74,cs-410,10,1,Text,"00:05:38,870","00:05:43,960",74,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=338,clustering to some application so we often,pic_cs-410_10_1_300.jpg
cs-410_10_1_75,cs-410,10,1,Text,"00:05:45,040","00:05:47,670",75,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=345,This could be the current system for,pic_cs-410_10_1_300.jpg
cs-410_10_1_76,cs-410,10,1,Text,"00:05:47,670","00:05:51,820",76,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=347,then you hope to add,pic_cs-410_10_1_300.jpg
cs-410_10_1_77,cs-410,10,1,Text,"00:05:51,820","00:05:55,340",77,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=351,the baseline system could be using,pic_cs-410_10_1_300.jpg
cs-410_10_1_78,cs-410,10,1,Text,"00:05:55,340","00:05:59,940",78,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=355,And then what you are trying,pic_cs-410_10_1_300.jpg
cs-410_10_1_79,cs-410,10,1,Text,"00:05:59,940","00:06:03,110",79,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=359,you hope to have better,pic_cs-410_10_1_300.jpg
cs-410_10_1_80,cs-410,10,1,Text,"00:06:03,110","00:06:06,911",80,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=363,So in any case you have a baseline system,pic_cs-410_10_1_360.jpg
cs-410_10_1_81,cs-410,10,1,Text,"00:06:06,911","00:06:10,110",81,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=366,algorithm to the baseline system,pic_cs-410_10_1_360.jpg
cs-410_10_1_82,cs-410,10,1,Text,"00:06:11,740","00:06:14,870",82,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=371,And then we have to compare the,pic_cs-410_10_1_360.jpg
cs-410_10_1_83,cs-410,10,1,Text,"00:06:14,870","00:06:18,660",83,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=374,the baseline system in terms,pic_cs-410_10_1_360.jpg
cs-410_10_1_84,cs-410,10,1,Text,"00:06:18,660","00:06:19,940",84,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=378,that particular application.,pic_cs-410_10_1_360.jpg
cs-410_10_1_85,cs-410,10,1,Text,"00:06:21,070","00:06:25,620",85,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=381,So in this case we call it indirect,pic_cs-410_10_1_360.jpg
cs-410_10_1_86,cs-410,10,1,Text,"00:06:25,620","00:06:30,040",86,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=385,explicit assessment of,pic_cs-410_10_1_360.jpg
cs-410_10_1_87,cs-410,10,1,Text,"00:06:30,040","00:06:35,470",87,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=390,rather it's to assess the contribution,pic_cs-410_10_1_360.jpg
cs-410_10_1_88,cs-410,10,1,Text,"00:06:37,350","00:06:40,570",88,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=397,"So, to summarize text clustering,",pic_cs-410_10_1_360.jpg
cs-410_10_1_89,cs-410,10,1,Text,"00:06:41,950","00:06:46,750",89,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=401,it's a very useful unsupervised,pic_cs-410_10_1_360.jpg
cs-410_10_1_90,cs-410,10,1,Text,"00:06:46,750","00:06:52,860",90,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=406,it's particularly useful for obtaining,pic_cs-410_10_1_360.jpg
cs-410_10_1_91,cs-410,10,1,Text,"00:06:52,860","00:06:56,960",91,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=412,And this is often needed,pic_cs-410_10_1_360.jpg
cs-410_10_1_92,cs-410,10,1,Text,"00:06:56,960","00:06:59,910",92,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=416,this is often the first step when,pic_cs-410_10_1_360.jpg
cs-410_10_1_93,cs-410,10,1,Text,"00:07:01,720","00:07:06,610",93,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=421,The second application or,pic_cs-410_10_1_420.jpg
cs-410_10_1_94,cs-410,10,1,Text,"00:07:06,610","00:07:09,800",94,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=426,discover interesting clustering,pic_cs-410_10_1_420.jpg
cs-410_10_1_95,cs-410,10,1,Text,"00:07:09,800","00:07:11,660",95,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=429,these structures can be very meaningful.,pic_cs-410_10_1_420.jpg
cs-410_10_1_96,cs-410,10,1,Text,"00:07:13,330","00:07:18,752",96,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=433,There are many approaches that can,pic_cs-410_10_1_420.jpg
cs-410_10_1_97,cs-410,10,1,Text,"00:07:18,752","00:07:25,590",97,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=438,we discussed model based approaches and,pic_cs-410_10_1_420.jpg
cs-410_10_1_98,cs-410,10,1,Text,"00:07:25,590","00:07:30,390",98,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=445,"In general, strong clusters tend to",pic_cs-410_10_1_420.jpg
cs-410_10_1_99,cs-410,10,1,Text,"00:07:30,390","00:07:35,350",99,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=450,Also the effectiveness of a method,pic_cs-410_10_1_420.jpg
cs-410_10_1_100,cs-410,10,1,Text,"00:07:35,350","00:07:40,400",100,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=455,"clustering bias is captured appropriately,",pic_cs-410_10_1_420.jpg
cs-410_10_1_101,cs-410,10,1,Text,"00:07:40,400","00:07:45,150",101,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=460,"the right generating model, the model",pic_cs-410_10_1_420.jpg
cs-410_10_1_102,cs-410,10,1,Text,"00:07:45,150","00:07:49,460",102,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=465,the right similarity function,pic_cs-410_10_1_420.jpg
cs-410_10_1_103,cs-410,10,1,Text,"00:07:49,460","00:07:53,730",103,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=469,Deciding the optimal number of customers,pic_cs-410_10_1_420.jpg
cs-410_10_1_104,cs-410,10,1,Text,"00:07:53,730","00:07:59,510",104,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=473,"order cluster methods, and that's",pic_cs-410_10_1_420.jpg
cs-410_10_1_105,cs-410,10,1,Text,"00:07:59,510","00:08:03,885",105,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=479,and there's no training there how to guide,pic_cs-410_10_1_420.jpg
cs-410_10_1_106,cs-410,10,1,Text,"00:08:05,125","00:08:08,515",106,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=485,Now sometimes you may see some methods,pic_cs-410_10_1_480.jpg
cs-410_10_1_107,cs-410,10,1,Text,"00:08:08,515","00:08:13,575",107,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=488,"the number of clusters, but",pic_cs-410_10_1_480.jpg
cs-410_10_1_108,cs-410,10,1,Text,"00:08:13,575","00:08:18,135",108,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=493,application of clustering bias there and,pic_cs-410_10_1_480.jpg
cs-410_10_1_109,cs-410,10,1,Text,"00:08:18,135","00:08:23,900",109,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=498,Without clearly defining a clustering,pic_cs-410_10_1_480.jpg
cs-410_10_1_110,cs-410,10,1,Text,"00:08:23,900","00:08:30,860",110,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=503,"the optimal number of cluster is what,",pic_cs-410_10_1_480.jpg
cs-410_10_1_111,cs-410,10,1,Text,"00:08:31,920","00:08:35,650",111,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=511,And I should also say sometimes we,pic_cs-410_10_1_480.jpg
cs-410_10_1_112,cs-410,10,1,Text,"00:08:35,650","00:08:39,320",112,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=515,"the number of clusters, for example,",pic_cs-410_10_1_480.jpg
cs-410_10_1_113,cs-410,10,1,Text,"00:08:39,320","00:08:43,340",113,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=519,then obviously you don't want,pic_cs-410_10_1_480.jpg
cs-410_10_1_114,cs-410,10,1,Text,"00:08:43,340","00:08:45,870",114,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=523,the number can be dictated,pic_cs-410_10_1_480.jpg
cs-410_10_1_115,cs-410,10,1,Text,"00:08:46,880","00:08:53,470",115,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=526,"In other situations, we might be",pic_cs-410_10_1_480.jpg
cs-410_10_1_116,cs-410,10,1,Text,"00:08:53,470","00:08:59,440",116,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=533,to assess whether we've got a good number,pic_cs-410_10_1_480.jpg
cs-410_10_1_117,cs-410,10,1,Text,"00:08:59,440","00:09:04,330",117,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=539,"And to do that,",pic_cs-410_10_1_480.jpg
cs-410_10_1_118,cs-410,10,1,Text,"00:09:04,330","00:09:06,170",118,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=544,watch how well you can fit the data.,pic_cs-410_10_1_540.jpg
cs-410_10_1_119,cs-410,10,1,Text,"00:09:07,430","00:09:10,740",119,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=547,In general when you add a more components,pic_cs-410_10_1_540.jpg
cs-410_10_1_120,cs-410,10,1,Text,"00:09:10,740","00:09:13,100",120,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=550,"the data better because you, you don't,",pic_cs-410_10_1_540.jpg
cs-410_10_1_121,cs-410,10,1,Text,"00:09:13,100","00:09:17,350",121,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=553,you can always set the probability,pic_cs-410_10_1_540.jpg
cs-410_10_1_122,cs-410,10,1,Text,"00:09:17,350","00:09:23,930",122,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=557,So you can't in general fit the data,pic_cs-410_10_1_540.jpg
cs-410_10_1_123,cs-410,10,1,Text,"00:09:23,930","00:09:28,350",123,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=563,is as you add more components would you be,pic_cs-410_10_1_540.jpg
cs-410_10_1_124,cs-410,10,1,Text,"00:09:28,350","00:09:33,890",124,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=568,of the data and that can be used to,pic_cs-410_10_1_540.jpg
cs-410_10_1_125,cs-410,10,1,Text,"00:09:33,890","00:09:36,610",125,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=573,And finally evaluation,pic_cs-410_10_1_540.jpg
cs-410_10_1_126,cs-410,10,1,Text,"00:09:36,610","00:09:41,560",126,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=576,this kind can be done both directly and,pic_cs-410_10_1_540.jpg
cs-410_10_1_127,cs-410,10,1,Text,"00:09:41,560","00:09:46,590",127,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=581,do both in order to get a good sense,pic_cs-410_10_1_540.jpg
cs-410_10_1_128,cs-410,10,1,Text,"00:09:46,590","00:09:52,206",128,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=586,So here's some suggested reading and,pic_cs-410_10_1_540.jpg
cs-410_10_1_129,cs-410,10,1,Text,"00:09:52,206","00:09:58,838",129,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=592,to better understand how the matches,pic_cs-410_10_1_540.jpg
cs-410_10_1_130,cs-410,10,1,Text,"00:09:58,838","00:10:08,838",130,https://www.coursera.org/learn/cs-410/lecture/FcCdt?t=598,[MUSIC],pic_cs-410_10_1_540.jpg
cs-410_10_2_1,cs-410,10,2,Text,"00:00:00,192","00:00:03,512",1,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=0,[SOUND],pic_cs-410_10_2_0.jpg
cs-410_10_2_2,cs-410,10,2,Text,"00:00:06,614","00:00:09,660",2,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=6,This lecture is about text categorization.,pic_cs-410_10_2_0.jpg
cs-410_10_2_3,cs-410,10,2,Text,"00:00:11,360","00:00:15,320",3,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=11,"In this lecture, we're going to",pic_cs-410_10_2_0.jpg
cs-410_10_2_4,cs-410,10,2,Text,"00:00:16,390","00:00:21,320",4,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=16,This is a very important technique for,pic_cs-410_10_2_0.jpg
cs-410_10_2_5,cs-410,10,2,Text,"00:00:22,470","00:00:27,035",5,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=22,It is relevant to discovery,pic_cs-410_10_2_0.jpg
cs-410_10_2_6,cs-410,10,2,Text,"00:00:27,035","00:00:29,134",6,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=27,knowledge as shown here.,pic_cs-410_10_2_0.jpg
cs-410_10_2_7,cs-410,10,2,Text,"00:00:29,134","00:00:33,380",7,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=29,"First, it's related to topic mining and",pic_cs-410_10_2_0.jpg
cs-410_10_2_8,cs-410,10,2,Text,"00:00:33,380","00:00:36,060",8,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=33,"And, that's because it has to do with",pic_cs-410_10_2_0.jpg
cs-410_10_2_9,cs-410,10,2,Text,"00:00:36,060","00:00:40,970",9,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=36,analyzing text to data based,pic_cs-410_10_2_0.jpg
cs-410_10_2_10,cs-410,10,2,Text,"00:00:40,970","00:00:46,239",10,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=40,"Secondly, it's also related to",pic_cs-410_10_2_0.jpg
cs-410_10_2_11,cs-410,10,2,Text,"00:00:46,239","00:00:51,941",11,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=46,which has to do with discovery knowledge,pic_cs-410_10_2_0.jpg
cs-410_10_2_12,cs-410,10,2,Text,"00:00:51,941","00:00:56,301",12,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=51,"Because we can categorize the authors,",pic_cs-410_10_2_0.jpg
cs-410_10_2_13,cs-410,10,2,Text,"00:00:56,301","00:01:01,813",13,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=56,based on the content of the articles,pic_cs-410_10_2_0.jpg
cs-410_10_2_14,cs-410,10,2,Text,"00:01:01,813","00:01:06,611",14,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=61,"We can, in general,",pic_cs-410_10_2_60.jpg
cs-410_10_2_15,cs-410,10,2,Text,"00:01:06,611","00:01:10,800",15,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=66,based on the content that they produce.,pic_cs-410_10_2_60.jpg
cs-410_10_2_16,cs-410,10,2,Text,"00:01:12,300","00:01:16,720",16,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=72,"Finally, it's also related",pic_cs-410_10_2_60.jpg
cs-410_10_2_17,cs-410,10,2,Text,"00:01:16,720","00:01:21,760",17,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=76,"Because, we can often use text",pic_cs-410_10_2_60.jpg
cs-410_10_2_18,cs-410,10,2,Text,"00:01:21,760","00:01:26,180",18,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=81,variables in the real world that,pic_cs-410_10_2_60.jpg
cs-410_10_2_19,cs-410,10,2,Text,"00:01:27,230","00:01:32,490",19,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=87,"And so, this is a very important",pic_cs-410_10_2_60.jpg
cs-410_10_2_20,cs-410,10,2,Text,"00:01:34,820","00:01:37,860",20,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=94,This is the overall plan for,pic_cs-410_10_2_60.jpg
cs-410_10_2_21,cs-410,10,2,Text,"00:01:37,860","00:01:40,750",21,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=97,"First, we're going to talk about",pic_cs-410_10_2_60.jpg
cs-410_10_2_22,cs-410,10,2,Text,"00:01:40,750","00:01:44,510",22,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=100,why we're interested in,pic_cs-410_10_2_60.jpg
cs-410_10_2_23,cs-410,10,2,Text,"00:01:44,510","00:01:47,920",23,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=104,"And now, we're going to talk about",pic_cs-410_10_2_60.jpg
cs-410_10_2_24,cs-410,10,2,Text,"00:01:47,920","00:01:50,780",24,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=107,how to evaluate,pic_cs-410_10_2_60.jpg
cs-410_10_2_25,cs-410,10,2,Text,"00:01:50,780","00:01:56,140",25,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=110,"So, the problem of text",pic_cs-410_10_2_60.jpg
cs-410_10_2_26,cs-410,10,2,Text,"00:01:56,140","00:02:03,461",26,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=116,We're given a set of predefined categories,pic_cs-410_10_2_60.jpg
cs-410_10_2_27,cs-410,10,2,Text,"00:02:03,461","00:02:07,462",27,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=123,"And often,",pic_cs-410_10_2_120.jpg
cs-410_10_2_28,cs-410,10,2,Text,"00:02:07,462","00:02:12,519",28,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=127,training set of labeled text,pic_cs-410_10_2_120.jpg
cs-410_10_2_29,cs-410,10,2,Text,"00:02:12,519","00:02:17,810",29,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=132,objects have already been,pic_cs-410_10_2_120.jpg
cs-410_10_2_30,cs-410,10,2,Text,"00:02:17,810","00:02:23,040",30,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=137,"And then, the task is to classify",pic_cs-410_10_2_120.jpg
cs-410_10_2_31,cs-410,10,2,Text,"00:02:23,040","00:02:26,320",31,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=143,more of these predefined categories.,pic_cs-410_10_2_120.jpg
cs-410_10_2_32,cs-410,10,2,Text,"00:02:26,320","00:02:29,139",32,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=146,"So, the picture on this",pic_cs-410_10_2_120.jpg
cs-410_10_2_33,cs-410,10,2,Text,"00:02:30,270","00:02:32,120",33,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=150,"When we do text categorization,",pic_cs-410_10_2_120.jpg
cs-410_10_2_34,cs-410,10,2,Text,"00:02:32,120","00:02:37,630",34,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=152,we have a lot of text objects to be,pic_cs-410_10_2_120.jpg
cs-410_10_2_35,cs-410,10,2,Text,"00:02:37,630","00:02:43,820",35,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=157,"the system will, in general,",pic_cs-410_10_2_120.jpg
cs-410_10_2_36,cs-410,10,2,Text,"00:02:43,820","00:02:49,110",36,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=163,As shown on the right and,pic_cs-410_10_2_120.jpg
cs-410_10_2_37,cs-410,10,2,Text,"00:02:49,110","00:02:54,280",37,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=169,and we often assume the availability,pic_cs-410_10_2_120.jpg
cs-410_10_2_38,cs-410,10,2,Text,"00:02:54,280","00:02:59,060",38,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=174,these are the documents that,pic_cs-410_10_2_120.jpg
cs-410_10_2_39,cs-410,10,2,Text,"00:02:59,060","00:03:01,660",39,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=179,And these examples are very important for,pic_cs-410_10_2_120.jpg
cs-410_10_2_40,cs-410,10,2,Text,"00:03:01,660","00:03:06,110",40,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=181,helping the system to learn,pic_cs-410_10_2_180.jpg
cs-410_10_2_41,cs-410,10,2,Text,"00:03:06,110","00:03:10,180",41,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=186,"And, this would further help",pic_cs-410_10_2_180.jpg
cs-410_10_2_42,cs-410,10,2,Text,"00:03:11,280","00:03:16,560",42,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=191,the categories of new text,pic_cs-410_10_2_180.jpg
cs-410_10_2_43,cs-410,10,2,Text,"00:03:16,560","00:03:20,950",43,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=196,"So, here are some specific",pic_cs-410_10_2_180.jpg
cs-410_10_2_44,cs-410,10,2,Text,"00:03:20,950","00:03:26,140",44,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=200,"And in fact, there are many examples,",pic_cs-410_10_2_180.jpg
cs-410_10_2_45,cs-410,10,2,Text,"00:03:27,230","00:03:33,000",45,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=207,"So first, text objects can vary,",pic_cs-410_10_2_180.jpg
cs-410_10_2_46,cs-410,10,2,Text,"00:03:33,000","00:03:36,730",46,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=213,"or a passage, or a sentence,",pic_cs-410_10_2_180.jpg
cs-410_10_2_47,cs-410,10,2,Text,"00:03:36,730","00:03:41,400",47,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=216,"As in the case of clustering, the units",pic_cs-410_10_2_180.jpg
cs-410_10_2_48,cs-410,10,2,Text,"00:03:41,400","00:03:44,090",48,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=221,this creates a lot of possibilities.,pic_cs-410_10_2_180.jpg
cs-410_10_2_49,cs-410,10,2,Text,"00:03:44,090","00:03:46,690",49,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=224,"Secondly, categories can also vary.",pic_cs-410_10_2_180.jpg
cs-410_10_2_50,cs-410,10,2,Text,"00:03:46,690","00:03:49,880",50,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=226,"Allocate in general,",pic_cs-410_10_2_180.jpg
cs-410_10_2_51,cs-410,10,2,Text,"00:03:49,880","00:03:51,560",51,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=229,One is internal categories.,pic_cs-410_10_2_180.jpg
cs-410_10_2_52,cs-410,10,2,Text,"00:03:51,560","00:03:55,890",52,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=231,These are categories that,pic_cs-410_10_2_180.jpg
cs-410_10_2_53,cs-410,10,2,Text,"00:03:55,890","00:04:00,850",53,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=235,"For example, topic categories or",pic_cs-410_10_2_180.jpg
cs-410_10_2_54,cs-410,10,2,Text,"00:04:00,850","00:04:04,810",54,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=240,they generally have to do with,pic_cs-410_10_2_240.jpg
cs-410_10_2_55,cs-410,10,2,Text,"00:04:04,810","00:04:06,930",55,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=244,throughout the categorization,pic_cs-410_10_2_240.jpg
cs-410_10_2_56,cs-410,10,2,Text,"00:04:08,210","00:04:13,430",56,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=248,The other kind is external categories,pic_cs-410_10_2_240.jpg
cs-410_10_2_57,cs-410,10,2,Text,"00:04:13,430","00:04:16,120",57,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=253,associated with the text object.,pic_cs-410_10_2_240.jpg
cs-410_10_2_58,cs-410,10,2,Text,"00:04:16,120","00:04:17,630",58,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=256,"For example,",pic_cs-410_10_2_240.jpg
cs-410_10_2_59,cs-410,10,2,Text,"00:04:17,630","00:04:22,810",59,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=257,authors are entities associated,pic_cs-410_10_2_240.jpg
cs-410_10_2_60,cs-410,10,2,Text,"00:04:22,810","00:04:28,340",60,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=262,"And so, we can use their content in",pic_cs-410_10_2_240.jpg
cs-410_10_2_61,cs-410,10,2,Text,"00:04:28,340","00:04:31,670",61,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=268,"which part, for example, and",pic_cs-410_10_2_240.jpg
cs-410_10_2_62,cs-410,10,2,Text,"00:04:33,540","00:04:38,048",62,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=273,"Or, we can have any",pic_cs-410_10_2_240.jpg
cs-410_10_2_63,cs-410,10,2,Text,"00:04:38,048","00:04:43,147",63,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=278,associate with text data,pic_cs-410_10_2_240.jpg
cs-410_10_2_64,cs-410,10,2,Text,"00:04:43,147","00:04:47,788",64,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=283,connection between the entity and,pic_cs-410_10_2_240.jpg
cs-410_10_2_65,cs-410,10,2,Text,"00:04:47,788","00:04:54,025",65,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=287,"For example, we might collect a lot",pic_cs-410_10_2_240.jpg
cs-410_10_2_66,cs-410,10,2,Text,"00:04:54,025","00:04:58,073",66,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=294,"a lot of reviews about a product,",pic_cs-410_10_2_240.jpg
cs-410_10_2_67,cs-410,10,2,Text,"00:04:58,073","00:05:04,770",67,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=298,this text data can help us infer,pic_cs-410_10_2_240.jpg
cs-410_10_2_68,cs-410,10,2,Text,"00:05:04,770","00:05:07,770",68,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=304,"In that case, we can treat this",pic_cs-410_10_2_300.jpg
cs-410_10_2_69,cs-410,10,2,Text,"00:05:07,770","00:05:09,921",69,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=307,We can categorize restaurants or,pic_cs-410_10_2_300.jpg
cs-410_10_2_70,cs-410,10,2,Text,"00:05:09,921","00:05:13,924",70,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=309,categorize products based on,pic_cs-410_10_2_300.jpg
cs-410_10_2_71,cs-410,10,2,Text,"00:05:13,924","00:05:17,245",71,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=313,"So, this is an example for",pic_cs-410_10_2_300.jpg
cs-410_10_2_72,cs-410,10,2,Text,"00:05:17,245","00:05:20,400",72,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=317,Here are some specific,pic_cs-410_10_2_300.jpg
cs-410_10_2_73,cs-410,10,2,Text,"00:05:20,400","00:05:25,110",73,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=320,News categorization is very,pic_cs-410_10_2_300.jpg
cs-410_10_2_74,cs-410,10,2,Text,"00:05:25,110","00:05:30,009",74,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=325,News agencies would like,pic_cs-410_10_2_300.jpg
cs-410_10_2_75,cs-410,10,2,Text,"00:05:30,009","00:05:35,672",75,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=330,categories to categorize,pic_cs-410_10_2_300.jpg
cs-410_10_2_76,cs-410,10,2,Text,"00:05:35,672","00:05:39,824",76,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=335,"And, these virtual article",pic_cs-410_10_2_300.jpg
cs-410_10_2_77,cs-410,10,2,Text,"00:05:39,824","00:05:43,650",77,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=339,"For example, in the biomedical domain,",pic_cs-410_10_2_300.jpg
cs-410_10_2_78,cs-410,10,2,Text,"00:05:43,650","00:05:47,930",78,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=343,"MeSH stands for Medical Subject Heading,",pic_cs-410_10_2_300.jpg
cs-410_10_2_79,cs-410,10,2,Text,"00:05:49,090","00:05:52,490",79,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=349,characterize content of,pic_cs-410_10_2_300.jpg
cs-410_10_2_80,cs-410,10,2,Text,"00:05:54,590","00:05:59,860",80,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=354,Another example of application is spam,pic_cs-410_10_2_300.jpg
cs-410_10_2_81,cs-410,10,2,Text,"00:05:59,860","00:06:04,940",81,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=359,"So, we often have a spam filter",pic_cs-410_10_2_300.jpg
cs-410_10_2_82,cs-410,10,2,Text,"00:06:04,940","00:06:10,260",82,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=364,to help us distinguish spams,pic_cs-410_10_2_360.jpg
cs-410_10_2_83,cs-410,10,2,Text,"00:06:10,260","00:06:13,000",83,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=370,this is clearly a binary,pic_cs-410_10_2_360.jpg
cs-410_10_2_84,cs-410,10,2,Text,"00:06:14,500","00:06:18,460",84,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=374,Sentiment categorization of,pic_cs-410_10_2_360.jpg
cs-410_10_2_85,cs-410,10,2,Text,"00:06:18,460","00:06:23,120",85,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=378,another kind of applications where we,pic_cs-410_10_2_360.jpg
cs-410_10_2_86,cs-410,10,2,Text,"00:06:23,120","00:06:26,380",86,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=383,negative or positive and,pic_cs-410_10_2_360.jpg
cs-410_10_2_87,cs-410,10,2,Text,"00:06:27,460","00:06:32,820",87,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=387,"So, you can have send them to categories,",pic_cs-410_10_2_360.jpg
cs-410_10_2_88,cs-410,10,2,Text,"00:06:35,520","00:06:39,480",88,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=395,Another application is automatic,pic_cs-410_10_2_360.jpg
cs-410_10_2_89,cs-410,10,2,Text,"00:06:39,480","00:06:43,750",89,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=399,you might want to automatically sort your,pic_cs-410_10_2_360.jpg
cs-410_10_2_90,cs-410,10,2,Text,"00:06:43,750","00:06:47,320",90,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=403,one application of text categorization,pic_cs-410_10_2_360.jpg
cs-410_10_2_91,cs-410,10,2,Text,"00:06:48,370","00:06:52,580",91,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=408,The results are another important kind,pic_cs-410_10_2_360.jpg
cs-410_10_2_92,cs-410,10,2,Text,"00:06:52,580","00:06:55,910",92,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=412,"to the right person to handle,",pic_cs-410_10_2_360.jpg
cs-410_10_2_93,cs-410,10,2,Text,"00:06:55,910","00:07:01,890",93,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=415,email messaging is generally routed,pic_cs-410_10_2_360.jpg
cs-410_10_2_94,cs-410,10,2,Text,"00:07:01,890","00:07:05,820",94,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=421,Different people tend to handle,pic_cs-410_10_2_420.jpg
cs-410_10_2_95,cs-410,10,2,Text,"00:07:05,820","00:07:11,220",95,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=425,"And in many cases, a person would manually",pic_cs-410_10_2_420.jpg
cs-410_10_2_96,cs-410,10,2,Text,"00:07:11,220","00:07:15,231",96,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=431,"But, if you can imagine,",pic_cs-410_10_2_420.jpg
cs-410_10_2_97,cs-410,10,2,Text,"00:07:15,231","00:07:18,794",97,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=435,text categorization system,pic_cs-410_10_2_420.jpg
cs-410_10_2_98,cs-410,10,2,Text,"00:07:18,794","00:07:24,969",98,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=438,"And, this is a class file, the incoming",pic_cs-410_10_2_420.jpg
cs-410_10_2_99,cs-410,10,2,Text,"00:07:24,969","00:07:31,265",99,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=444,where each category actually corresponds,pic_cs-410_10_2_420.jpg
cs-410_10_2_100,cs-410,10,2,Text,"00:07:31,265","00:07:35,975",100,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=451,"And finally, author attribution, as I just",pic_cs-410_10_2_420.jpg
cs-410_10_2_101,cs-410,10,2,Text,"00:07:35,975","00:07:39,759",101,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=455,it's another example of using text,pic_cs-410_10_2_420.jpg
cs-410_10_2_102,cs-410,10,2,Text,"00:07:41,480","00:07:42,960",102,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=461,some other entities.,pic_cs-410_10_2_420.jpg
cs-410_10_2_103,cs-410,10,2,Text,"00:07:42,960","00:07:46,890",103,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=462,"And, there are also many variants",pic_cs-410_10_2_420.jpg
cs-410_10_2_104,cs-410,10,2,Text,"00:07:46,890","00:07:50,980",104,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=466,"And so, first, we have the simplest case,",pic_cs-410_10_2_420.jpg
cs-410_10_2_105,cs-410,10,2,Text,"00:07:50,980","00:07:52,990",105,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=470,where there are only two categories.,pic_cs-410_10_2_420.jpg
cs-410_10_2_106,cs-410,10,2,Text,"00:07:52,990","00:07:57,660",106,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=472,"And, there are many examples like that,",pic_cs-410_10_2_420.jpg
cs-410_10_2_107,cs-410,10,2,Text,"00:07:59,040","00:08:03,600",107,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=479,Applications with one distinguishing,pic_cs-410_10_2_420.jpg
cs-410_10_2_108,cs-410,10,2,Text,"00:08:03,600","00:08:04,940",108,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=483,documents for a particular query.,pic_cs-410_10_2_480.jpg
cs-410_10_2_109,cs-410,10,2,Text,"00:08:06,040","00:08:12,330",109,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=486,Spam filtering just distinguishing spams,pic_cs-410_10_2_480.jpg
cs-410_10_2_110,cs-410,10,2,Text,"00:08:12,330","00:08:16,800",110,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=492,"Sometimes, classifications of",pic_cs-410_10_2_480.jpg
cs-410_10_2_111,cs-410,10,2,Text,"00:08:16,800","00:08:17,800",111,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=496,positive and a negative.,pic_cs-410_10_2_480.jpg
cs-410_10_2_112,cs-410,10,2,Text,"00:08:19,120","00:08:22,650",112,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=499,A more general case would be K-category,pic_cs-410_10_2_480.jpg
cs-410_10_2_113,cs-410,10,2,Text,"00:08:22,650","00:08:26,755",113,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=502,"many applications like that,",pic_cs-410_10_2_480.jpg
cs-410_10_2_114,cs-410,10,2,Text,"00:08:26,755","00:08:30,155",114,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=506,"So, topic categorization is often",pic_cs-410_10_2_480.jpg
cs-410_10_2_115,cs-410,10,2,Text,"00:08:30,155","00:08:31,935",115,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=510,multiple topics.,pic_cs-410_10_2_480.jpg
cs-410_10_2_116,cs-410,10,2,Text,"00:08:31,935","00:08:36,205",116,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=511,Email routing would be another example,pic_cs-410_10_2_480.jpg
cs-410_10_2_117,cs-410,10,2,Text,"00:08:36,205","00:08:39,322",117,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=516,if you route the email to,pic_cs-410_10_2_480.jpg
cs-410_10_2_118,cs-410,10,2,Text,"00:08:39,322","00:08:44,550",118,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=519,then there are multiple,pic_cs-410_10_2_480.jpg
cs-410_10_2_119,cs-410,10,2,Text,"00:08:44,550","00:08:48,212",119,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=524,"So, in all these cases, there are more",pic_cs-410_10_2_480.jpg
cs-410_10_2_120,cs-410,10,2,Text,"00:08:49,272","00:08:52,382",120,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=529,Another variation is to have,pic_cs-410_10_2_480.jpg
cs-410_10_2_121,cs-410,10,2,Text,"00:08:52,382","00:08:54,442",121,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=532,where categories form a hierarchy.,pic_cs-410_10_2_480.jpg
cs-410_10_2_122,cs-410,10,2,Text,"00:08:54,442","00:08:56,602",122,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=534,"Again, topical hierarchy is very common.",pic_cs-410_10_2_480.jpg
cs-410_10_2_123,cs-410,10,2,Text,"00:08:58,232","00:09:00,742",123,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=538,Yet another variation is,pic_cs-410_10_2_480.jpg
cs-410_10_2_124,cs-410,10,2,Text,"00:09:00,742","00:09:04,550",124,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=540,That's when you have multiple,pic_cs-410_10_2_540.jpg
cs-410_10_2_125,cs-410,10,2,Text,"00:09:04,550","00:09:08,150",125,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=544,then you hope to kind of,pic_cs-410_10_2_540.jpg
cs-410_10_2_126,cs-410,10,2,Text,"00:09:08,150","00:09:13,340",126,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=548,Further leverage the dependency of,pic_cs-410_10_2_540.jpg
cs-410_10_2_127,cs-410,10,2,Text,"00:09:13,340","00:09:15,250",127,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=553,each individual task.,pic_cs-410_10_2_540.jpg
cs-410_10_2_128,cs-410,10,2,Text,"00:09:15,250","00:09:19,870",128,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=555,Among all these binary categorizations,pic_cs-410_10_2_540.jpg
cs-410_10_2_129,cs-410,10,2,Text,"00:09:19,870","00:09:25,170",129,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=559,part of it also is because it's simple and,pic_cs-410_10_2_540.jpg
cs-410_10_2_130,cs-410,10,2,Text,"00:09:25,170","00:09:31,000",130,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=565,it can actually be used to perform,pic_cs-410_10_2_540.jpg
cs-410_10_2_131,cs-410,10,2,Text,"00:09:31,000","00:09:34,839",131,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=571,"For example, a K-category",pic_cs-410_10_2_540.jpg
cs-410_10_2_132,cs-410,10,2,Text,"00:09:34,839","00:09:38,665",132,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=574,performed by using binary categorization.,pic_cs-410_10_2_540.jpg
cs-410_10_2_133,cs-410,10,2,Text,"00:09:40,075","00:09:43,405",133,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=580,"Basically, we can look at",pic_cs-410_10_2_540.jpg
cs-410_10_2_134,cs-410,10,2,Text,"00:09:43,405","00:09:49,385",134,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=583,then the binary categorization problem,pic_cs-410_10_2_540.jpg
cs-410_10_2_135,cs-410,10,2,Text,"00:09:49,385","00:09:52,005",135,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=589,"not, meaning in other categories.",pic_cs-410_10_2_540.jpg
cs-410_10_2_136,cs-410,10,2,Text,"00:09:53,485","00:09:59,820",136,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=593,"And, the hierarchical categorization",pic_cs-410_10_2_540.jpg
cs-410_10_2_137,cs-410,10,2,Text,"00:09:59,820","00:10:04,300",137,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=599,doing flat categorization at each level.,pic_cs-410_10_2_540.jpg
cs-410_10_2_138,cs-410,10,2,Text,"00:10:04,300","00:10:07,000",138,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=604,"So, we have, first, we categorize",pic_cs-410_10_2_600.jpg
cs-410_10_2_139,cs-410,10,2,Text,"00:10:07,000","00:10:09,140",139,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=607,"a small number of high-level categories,",pic_cs-410_10_2_600.jpg
cs-410_10_2_140,cs-410,10,2,Text,"00:10:09,140","00:10:13,740",140,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=609,"and inside each category, we have further",pic_cs-410_10_2_600.jpg
cs-410_10_2_141,cs-410,10,2,Text,"00:10:15,000","00:10:16,728",141,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=615,"So, why is text categorization important?",pic_cs-410_10_2_600.jpg
cs-410_10_2_142,cs-410,10,2,Text,"00:10:16,728","00:10:21,464",142,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=616,"Well, I already showed that you,",pic_cs-410_10_2_600.jpg
cs-410_10_2_143,cs-410,10,2,Text,"00:10:21,464","00:10:23,244",143,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=621,there are several reasons.,pic_cs-410_10_2_600.jpg
cs-410_10_2_144,cs-410,10,2,Text,"00:10:23,244","00:10:28,891",144,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=623,One is text categorization helps enrich,pic_cs-410_10_2_600.jpg
cs-410_10_2_145,cs-410,10,2,Text,"00:10:28,891","00:10:34,970",145,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=628,more understanding of text data that's,pic_cs-410_10_2_600.jpg
cs-410_10_2_146,cs-410,10,2,Text,"00:10:34,970","00:10:38,738",146,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=634,"So, now with categorization text can",pic_cs-410_10_2_600.jpg
cs-410_10_2_147,cs-410,10,2,Text,"00:10:38,738","00:10:47,310",147,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=638,The keyword conditions that's often,pic_cs-410_10_2_600.jpg
cs-410_10_2_148,cs-410,10,2,Text,"00:10:47,310","00:10:52,455",148,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=647,But we can now also add categories and,pic_cs-410_10_2_600.jpg
cs-410_10_2_149,cs-410,10,2,Text,"00:10:55,485","00:11:00,085",149,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=655,Semantic categories assigned can also,pic_cs-410_10_2_600.jpg
cs-410_10_2_150,cs-410,10,2,Text,"00:11:00,085","00:11:01,145",150,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=660,application.,pic_cs-410_10_2_660.jpg
cs-410_10_2_151,cs-410,10,2,Text,"00:11:01,145","00:11:07,869",151,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=661,"So, for example, semantic categories",pic_cs-410_10_2_660.jpg
cs-410_10_2_152,cs-410,10,2,Text,"00:11:07,869","00:11:12,248",152,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=667,other attribution might,pic_cs-410_10_2_660.jpg
cs-410_10_2_153,cs-410,10,2,Text,"00:11:12,248","00:11:18,118",153,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=672,Another example is when semantic,pic_cs-410_10_2_660.jpg
cs-410_10_2_154,cs-410,10,2,Text,"00:11:18,118","00:11:24,660",154,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=678,of text content and this is another case,pic_cs-410_10_2_660.jpg
cs-410_10_2_155,cs-410,10,2,Text,"00:11:25,950","00:11:30,940",155,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=685,"For example, if we want to know",pic_cs-410_10_2_660.jpg
cs-410_10_2_156,cs-410,10,2,Text,"00:11:32,010","00:11:37,830",156,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=692,could first categorize the opinions,pic_cs-410_10_2_660.jpg
cs-410_10_2_157,cs-410,10,2,Text,"00:11:37,830","00:11:42,730",157,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=697,"as positive or negative and then, that",pic_cs-410_10_2_660.jpg
cs-410_10_2_158,cs-410,10,2,Text,"00:11:42,730","00:11:47,810",158,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=702,"the sentiment, and it would tell us about",pic_cs-410_10_2_660.jpg
cs-410_10_2_159,cs-410,10,2,Text,"00:11:47,810","00:11:52,680",159,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=707,the 70% of the views are positive and,pic_cs-410_10_2_660.jpg
cs-410_10_2_160,cs-410,10,2,Text,"00:11:53,810","00:11:56,865",160,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=713,"So, without doing categorization,",pic_cs-410_10_2_660.jpg
cs-410_10_2_161,cs-410,10,2,Text,"00:11:56,865","00:12:02,402",161,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=716,it will be much harder to aggregate,pic_cs-410_10_2_660.jpg
cs-410_10_2_162,cs-410,10,2,Text,"00:12:02,402","00:12:07,468",162,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=722,way of coding text in some sense,pic_cs-410_10_2_720.jpg
cs-410_10_2_163,cs-410,10,2,Text,"00:12:07,468","00:12:13,640",163,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=727,"And, sometimes you may see in some",pic_cs-410_10_2_720.jpg
cs-410_10_2_164,cs-410,10,2,Text,"00:12:13,640","00:12:18,704",164,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=733,"called a text coded,",pic_cs-410_10_2_720.jpg
cs-410_10_2_165,cs-410,10,2,Text,"00:12:18,704","00:12:22,316",165,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=738,The second kind of reasons is to use text,pic_cs-410_10_2_720.jpg
cs-410_10_2_166,cs-410,10,2,Text,"00:12:22,316","00:12:27,024",166,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=742,categorization to infer,pic_cs-410_10_2_720.jpg
cs-410_10_2_167,cs-410,10,2,Text,"00:12:27,024","00:12:31,950",167,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=747,and text categories allows,pic_cs-410_10_2_720.jpg
cs-410_10_2_168,cs-410,10,2,Text,"00:12:31,950","00:12:36,950",168,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=751,of such entities that,pic_cs-410_10_2_720.jpg
cs-410_10_2_169,cs-410,10,2,Text,"00:12:36,950","00:12:41,140",169,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=756,"So, this means we can",pic_cs-410_10_2_720.jpg
cs-410_10_2_170,cs-410,10,2,Text,"00:12:41,140","00:12:44,090",170,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=761,to discover knowledge about the world.,pic_cs-410_10_2_720.jpg
cs-410_10_2_171,cs-410,10,2,Text,"00:12:44,090","00:12:48,370",171,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=764,"In general, as long as we can associate",pic_cs-410_10_2_720.jpg
cs-410_10_2_172,cs-410,10,2,Text,"00:12:48,370","00:12:53,600",172,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=768,we can always the text of data to help,pic_cs-410_10_2_720.jpg
cs-410_10_2_173,cs-410,10,2,Text,"00:12:53,600","00:12:54,502",173,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=773,"So, it's used for",pic_cs-410_10_2_720.jpg
cs-410_10_2_174,cs-410,10,2,Text,"00:12:54,502","00:12:59,380",174,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=774,single information network that will,pic_cs-410_10_2_720.jpg
cs-410_10_2_175,cs-410,10,2,Text,"00:12:59,380","00:13:03,750",175,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=779,The obvious entities that can be,pic_cs-410_10_2_720.jpg
cs-410_10_2_176,cs-410,10,2,Text,"00:13:03,750","00:13:08,340",176,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=783,"But, you can also imagine the author's",pic_cs-410_10_2_780.jpg
cs-410_10_2_177,cs-410,10,2,Text,"00:13:08,340","00:13:14,090",177,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=788,other things can be actually,pic_cs-410_10_2_780.jpg
cs-410_10_2_178,cs-410,10,2,Text,"00:13:14,090","00:13:18,860",178,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=794,"Once we have made the connection, then we",pic_cs-410_10_2_780.jpg
cs-410_10_2_179,cs-410,10,2,Text,"00:13:18,860","00:13:23,520",179,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=798,"So, this is a general way to allow",pic_cs-410_10_2_780.jpg
cs-410_10_2_180,cs-410,10,2,Text,"00:13:23,520","00:13:26,890",180,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=803,the text categorization to discover,pic_cs-410_10_2_780.jpg
cs-410_10_2_181,cs-410,10,2,Text,"00:13:26,890","00:13:32,330",181,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=806,"Very useful, especially in big text",pic_cs-410_10_2_780.jpg
cs-410_10_2_182,cs-410,10,2,Text,"00:13:32,330","00:13:38,150",182,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=812,just using text data as extra sets,pic_cs-410_10_2_780.jpg
cs-410_10_2_183,cs-410,10,2,Text,"00:13:38,150","00:13:43,930",183,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=818,to infer certain decision factors,pic_cs-410_10_2_780.jpg
cs-410_10_2_184,cs-410,10,2,Text,"00:13:43,930","00:13:45,710",184,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=823,"Specifically with text, for example,",pic_cs-410_10_2_780.jpg
cs-410_10_2_185,cs-410,10,2,Text,"00:13:45,710","00:13:49,220",185,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=825,we can also think of examples of,pic_cs-410_10_2_780.jpg
cs-410_10_2_186,cs-410,10,2,Text,"00:13:49,220","00:13:53,190",186,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=829,"For example, discovery of",pic_cs-410_10_2_780.jpg
cs-410_10_2_187,cs-410,10,2,Text,"00:13:53,190","00:13:59,460",187,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=833,"And, this can be done by categorizing",pic_cs-410_10_2_780.jpg
cs-410_10_2_188,cs-410,10,2,Text,"00:14:00,680","00:14:05,566",188,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=840,Another example is to predict the party,pic_cs-410_10_2_840.jpg
cs-410_10_2_189,cs-410,10,2,Text,"00:14:05,566","00:14:07,314",189,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=845,on the political speech.,pic_cs-410_10_2_840.jpg
cs-410_10_2_190,cs-410,10,2,Text,"00:14:07,314","00:14:11,967",190,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=847,"And, this is again an example",pic_cs-410_10_2_840.jpg
cs-410_10_2_191,cs-410,10,2,Text,"00:14:11,967","00:14:15,146",191,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=851,some knowledge about the real world.,pic_cs-410_10_2_840.jpg
cs-410_10_2_192,cs-410,10,2,Text,"00:14:15,146","00:14:19,265",192,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=855,"In nature,",pic_cs-410_10_2_840.jpg
cs-410_10_2_193,cs-410,10,2,Text,"00:14:19,265","00:14:24,980",193,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=859,that's as we defined and,pic_cs-410_10_2_840.jpg
cs-410_10_2_194,cs-410,10,2,Text,"00:14:24,980","00:14:34,980",194,https://www.coursera.org/learn/cs-410/lecture/gJTFA?t=864,[MUSIC],pic_cs-410_10_2_840.jpg
cs-410_11_1_1,cs-410,11,1,Text,"00:00:00,025","00:00:06,573",1,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=0,[SOUND] This lecture is,pic_cs-410_11_1_0.jpg
cs-410_11_1_2,cs-410,11,1,Text,"00:00:06,573","00:00:12,439",2,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=6,of evaluation of text categorization.,pic_cs-410_11_1_0.jpg
cs-410_11_1_3,cs-410,11,1,Text,"00:00:12,439","00:00:18,302",3,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=12,Earlier we have introduced measures that,pic_cs-410_11_1_0.jpg
cs-410_11_1_4,cs-410,11,1,Text,"00:00:18,302","00:00:19,920",4,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=18,recall.,pic_cs-410_11_1_0.jpg
cs-410_11_1_5,cs-410,11,1,Text,"00:00:19,920","00:00:26,090",5,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=19,For each category and each document,pic_cs-410_11_1_0.jpg
cs-410_11_1_6,cs-410,11,1,Text,"00:00:27,680","00:00:32,530",6,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=27,further examine how to combine the,pic_cs-410_11_1_0.jpg
cs-410_11_1_7,cs-410,11,1,Text,"00:00:32,530","00:00:36,980",7,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=32,"different documents how to aggregate them,",pic_cs-410_11_1_0.jpg
cs-410_11_1_8,cs-410,11,1,Text,"00:00:36,980","00:00:41,220",8,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=36,You see on the title here I indicated,pic_cs-410_11_1_0.jpg
cs-410_11_1_9,cs-410,11,1,Text,"00:00:41,220","00:00:46,190",9,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=41,this is in contrast to micro average,pic_cs-410_11_1_0.jpg
cs-410_11_1_10,cs-410,11,1,Text,"00:00:47,750","00:00:53,710",10,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=47,"So, again, for each category we're going",pic_cs-410_11_1_0.jpg
cs-410_11_1_11,cs-410,11,1,Text,"00:00:53,710","00:00:59,880",11,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=53,for example category c1 we have,pic_cs-410_11_1_0.jpg
cs-410_11_1_12,cs-410,11,1,Text,"00:00:59,880","00:01:06,380",12,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=59,And similarly we can do that for category,pic_cs-410_11_1_0.jpg
cs-410_11_1_13,cs-410,11,1,Text,"00:01:06,380","00:01:11,050",13,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=66,Now once we compute that and,pic_cs-410_11_1_60.jpg
cs-410_11_1_14,cs-410,11,1,Text,"00:01:11,050","00:01:13,840",14,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=71,example we can aggregate,pic_cs-410_11_1_60.jpg
cs-410_11_1_15,cs-410,11,1,Text,"00:01:13,840","00:01:17,610",15,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=73,"For all the categories, for",pic_cs-410_11_1_60.jpg
cs-410_11_1_16,cs-410,11,1,Text,"00:01:17,610","00:01:24,160",16,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=77,And this is often very useful to summarize,pic_cs-410_11_1_60.jpg
cs-410_11_1_17,cs-410,11,1,Text,"00:01:24,160","00:01:26,780",17,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=84,And aggregation can be,pic_cs-410_11_1_60.jpg
cs-410_11_1_18,cs-410,11,1,Text,"00:01:26,780","00:01:32,550",18,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=86,"Again as I said, in a case when you",pic_cs-410_11_1_60.jpg
cs-410_11_1_19,cs-410,11,1,Text,"00:01:32,550","00:01:36,630",19,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=92,it's always good to think about what's,pic_cs-410_11_1_60.jpg
cs-410_11_1_20,cs-410,11,1,Text,"00:01:36,630","00:01:41,750",20,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=96,"For example, we can consider arithmetic",pic_cs-410_11_1_60.jpg
cs-410_11_1_21,cs-410,11,1,Text,"00:01:41,750","00:01:46,180",21,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=101,"you can use geometric mean,",pic_cs-410_11_1_60.jpg
cs-410_11_1_22,cs-410,11,1,Text,"00:01:46,180","00:01:50,540",22,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=106,"Depending on the way you aggregate,",pic_cs-410_11_1_60.jpg
cs-410_11_1_23,cs-410,11,1,Text,"00:01:50,540","00:01:54,370",23,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=110,"in terms of which method works better,",pic_cs-410_11_1_60.jpg
cs-410_11_1_24,cs-410,11,1,Text,"00:01:54,370","00:02:00,860",24,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=114,differences and choosing the right one or,pic_cs-410_11_1_60.jpg
cs-410_11_1_25,cs-410,11,1,Text,"00:02:00,860","00:02:03,770",25,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=120,So the difference fore example,pic_cs-410_11_1_120.jpg
cs-410_11_1_26,cs-410,11,1,Text,"00:02:03,770","00:02:08,360",26,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=123,geometrically is that the arithmetically,pic_cs-410_11_1_120.jpg
cs-410_11_1_27,cs-410,11,1,Text,"00:02:08,360","00:02:12,170",27,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=128,values whereas geometrically would,pic_cs-410_11_1_120.jpg
cs-410_11_1_28,cs-410,11,1,Text,"00:02:12,170","00:02:16,940",28,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=132,Base and so whether you are want,pic_cs-410_11_1_120.jpg
cs-410_11_1_29,cs-410,11,1,Text,"00:02:16,940","00:02:22,040",29,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=136,high values would be a question,pic_cs-410_11_1_120.jpg
cs-410_11_1_30,cs-410,11,1,Text,"00:02:22,040","00:02:24,720",30,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=142,similar we can do that for,pic_cs-410_11_1_120.jpg
cs-410_11_1_31,cs-410,11,1,Text,"00:02:24,720","00:02:29,400",31,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=144,So that's how we can generate the overall,pic_cs-410_11_1_120.jpg
cs-410_11_1_32,cs-410,11,1,Text,"00:02:31,660","00:02:36,990",32,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=151,Now we can do the same for aggregation,pic_cs-410_11_1_120.jpg
cs-410_11_1_33,cs-410,11,1,Text,"00:02:36,990","00:02:40,300",33,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=156,So it's exactly the same situation for,pic_cs-410_11_1_120.jpg
cs-410_11_1_34,cs-410,11,1,Text,"00:02:40,300","00:02:42,340",34,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=160,"Precision, recall, and F.",pic_cs-410_11_1_120.jpg
cs-410_11_1_35,cs-410,11,1,Text,"00:02:42,340","00:02:47,130",35,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=162,And then after we have completed,pic_cs-410_11_1_120.jpg
cs-410_11_1_36,cs-410,11,1,Text,"00:02:47,130","00:02:51,590",36,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=167,we're going to aggregate them to generate,pic_cs-410_11_1_120.jpg
cs-410_11_1_37,cs-410,11,1,Text,"00:02:51,590","00:02:52,490",37,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=171,overall F score.,pic_cs-410_11_1_120.jpg
cs-410_11_1_38,cs-410,11,1,Text,"00:02:53,510","00:02:57,380",38,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=173,"These are, again, examining",pic_cs-410_11_1_120.jpg
cs-410_11_1_39,cs-410,11,1,Text,"00:02:57,380","00:03:00,390",39,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=177,Which one's more useful will,pic_cs-410_11_1_120.jpg
cs-410_11_1_40,cs-410,11,1,Text,"00:03:00,390","00:03:06,180",40,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=180,"In general, it's beneficial to look at",pic_cs-410_11_1_180.jpg
cs-410_11_1_41,cs-410,11,1,Text,"00:03:06,180","00:03:10,850",41,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=186,And especially if you compare different,pic_cs-410_11_1_180.jpg
cs-410_11_1_42,cs-410,11,1,Text,"00:03:10,850","00:03:16,370",42,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=190,it might reveal which method,pic_cs-410_11_1_180.jpg
cs-410_11_1_43,cs-410,11,1,Text,"00:03:16,370","00:03:19,830",43,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=196,in what situations and,pic_cs-410_11_1_180.jpg
cs-410_11_1_44,cs-410,11,1,Text,"00:03:19,830","00:03:23,070",44,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=199,Understanding the strands of a method or,pic_cs-410_11_1_180.jpg
cs-410_11_1_45,cs-410,11,1,Text,"00:03:23,070","00:03:25,100",45,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=203,this provides further insight for,pic_cs-410_11_1_180.jpg
cs-410_11_1_46,cs-410,11,1,Text,"00:03:28,260","00:03:32,180",46,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=208,"So as I mentioned,",pic_cs-410_11_1_180.jpg
cs-410_11_1_47,cs-410,11,1,Text,"00:03:32,180","00:03:35,890",47,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=212,in contrast to the macro average,pic_cs-410_11_1_180.jpg
cs-410_11_1_48,cs-410,11,1,Text,"00:03:35,890","00:03:41,110",48,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=215,"In this case, what we do is you",pic_cs-410_11_1_180.jpg
cs-410_11_1_49,cs-410,11,1,Text,"00:03:41,110","00:03:44,380",49,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=221,and then compute the precision and recall.,pic_cs-410_11_1_180.jpg
cs-410_11_1_50,cs-410,11,1,Text,"00:03:45,460","00:03:50,480",50,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=225,So we can compute the overall,pic_cs-410_11_1_180.jpg
cs-410_11_1_51,cs-410,11,1,Text,"00:03:50,480","00:03:55,832",51,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=230,"how many cases are in true positive,",pic_cs-410_11_1_180.jpg
cs-410_11_1_52,cs-410,11,1,Text,"00:03:55,832","00:04:01,660",52,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=235,"etc, it's computing the values",pic_cs-410_11_1_180.jpg
cs-410_11_1_53,cs-410,11,1,Text,"00:04:01,660","00:04:04,090",53,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=241,and then we can compute the precision and,pic_cs-410_11_1_240.jpg
cs-410_11_1_54,cs-410,11,1,Text,"00:04:06,060","00:04:10,296",54,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=246,"In contrast, in macro-averaging, we're",pic_cs-410_11_1_240.jpg
cs-410_11_1_55,cs-410,11,1,Text,"00:04:10,296","00:04:16,070",55,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=250,And then aggregate over these categories,pic_cs-410_11_1_240.jpg
cs-410_11_1_56,cs-410,11,1,Text,"00:04:16,070","00:04:19,950",56,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=256,then aggregate all the documents but,pic_cs-410_11_1_240.jpg
cs-410_11_1_57,cs-410,11,1,Text,"00:04:21,130","00:04:24,660",57,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=261,Now this would be very similar to,pic_cs-410_11_1_240.jpg
cs-410_11_1_58,cs-410,11,1,Text,"00:04:24,660","00:04:26,390",58,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=264,"used earlier, and",pic_cs-410_11_1_240.jpg
cs-410_11_1_59,cs-410,11,1,Text,"00:04:26,390","00:04:31,270",59,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=266,one problem here of course to treat all,pic_cs-410_11_1_240.jpg
cs-410_11_1_60,cs-410,11,1,Text,"00:04:32,400","00:04:34,990",60,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=272,And this may not be desirable.,pic_cs-410_11_1_240.jpg
cs-410_11_1_61,cs-410,11,1,Text,"00:04:36,310","00:04:39,160",61,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=276,But it may be a property for,pic_cs-410_11_1_240.jpg
cs-410_11_1_62,cs-410,11,1,Text,"00:04:39,160","00:04:45,570",62,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=279,"especially if we associate the, for",pic_cs-410_11_1_240.jpg
cs-410_11_1_63,cs-410,11,1,Text,"00:04:45,570","00:04:50,090",63,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=285,"Then we can actually compute for example,",pic_cs-410_11_1_240.jpg
cs-410_11_1_64,cs-410,11,1,Text,"00:04:50,090","00:04:55,140",64,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=290,Where you associate the different cost or,pic_cs-410_11_1_240.jpg
cs-410_11_1_65,cs-410,11,1,Text,"00:04:56,210","00:04:59,620",65,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=296,so there could be variations of these,pic_cs-410_11_1_240.jpg
cs-410_11_1_66,cs-410,11,1,Text,"00:04:59,620","00:05:06,398",66,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=299,But in general macro average tends to,pic_cs-410_11_1_240.jpg
cs-410_11_1_67,cs-410,11,1,Text,"00:05:06,398","00:05:13,889",67,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=306,just because it might reflect the need for,pic_cs-410_11_1_300.jpg
cs-410_11_1_68,cs-410,11,1,Text,"00:05:14,890","00:05:20,620",68,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=314,on each category or performance on each,pic_cs-410_11_1_300.jpg
cs-410_11_1_69,cs-410,11,1,Text,"00:05:20,620","00:05:27,210",69,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=320,"But macro averaging and micro averaging,",pic_cs-410_11_1_300.jpg
cs-410_11_1_70,cs-410,11,1,Text,"00:05:27,210","00:05:32,780",70,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=327,and you might see both reported in,pic_cs-410_11_1_300.jpg
cs-410_11_1_71,cs-410,11,1,Text,"00:05:32,780","00:05:36,750",71,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=332,Also sometimes categorization,pic_cs-410_11_1_300.jpg
cs-410_11_1_72,cs-410,11,1,Text,"00:05:36,750","00:05:39,290",72,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=336,be evaluated from ranking prospective.,pic_cs-410_11_1_300.jpg
cs-410_11_1_73,cs-410,11,1,Text,"00:05:40,400","00:05:43,990",73,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=340,And this is because categorization,pic_cs-410_11_1_300.jpg
cs-410_11_1_74,cs-410,11,1,Text,"00:05:43,990","00:05:49,610",74,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=343,often indeed passed it to a human for,pic_cs-410_11_1_300.jpg
cs-410_11_1_75,cs-410,11,1,Text,"00:05:49,610","00:05:53,300",75,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=349,"For example, it might be passed",pic_cs-410_11_1_300.jpg
cs-410_11_1_76,cs-410,11,1,Text,"00:05:53,300","00:05:58,810",76,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=353,"For example, news articles can be tempted",pic_cs-410_11_1_300.jpg
cs-410_11_1_77,cs-410,11,1,Text,"00:05:58,810","00:06:01,040",77,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=358,then human editors would,pic_cs-410_11_1_300.jpg
cs-410_11_1_78,cs-410,11,1,Text,"00:06:02,680","00:06:07,500",78,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=362,And all the email messages might be,pic_cs-410_11_1_360.jpg
cs-410_11_1_79,cs-410,11,1,Text,"00:06:07,500","00:06:09,890",79,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=367,handling in the help desk.,pic_cs-410_11_1_360.jpg
cs-410_11_1_80,cs-410,11,1,Text,"00:06:09,890","00:06:14,090",80,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=369,And in such a case the categorizations,pic_cs-410_11_1_360.jpg
cs-410_11_1_81,cs-410,11,1,Text,"00:06:14,090","00:06:18,600",81,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=374,the task for,pic_cs-410_11_1_360.jpg
cs-410_11_1_82,cs-410,11,1,Text,"00:06:19,690","00:06:25,360",82,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=379,"So, in this case the results",pic_cs-410_11_1_360.jpg
cs-410_11_1_83,cs-410,11,1,Text,"00:06:26,370","00:06:32,450",83,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=386,and if the system can't give a score,pic_cs-410_11_1_360.jpg
cs-410_11_1_84,cs-410,11,1,Text,"00:06:32,450","00:06:39,830",84,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=392,confidence then we can use the scores,pic_cs-410_11_1_360.jpg
cs-410_11_1_85,cs-410,11,1,Text,"00:06:39,830","00:06:44,660",85,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=399,"then evaluate the results as a rank list,",pic_cs-410_11_1_360.jpg
cs-410_11_1_86,cs-410,11,1,Text,"00:06:44,660","00:06:47,990",86,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=404,Evaluation where you rank,pic_cs-410_11_1_360.jpg
cs-410_11_1_87,cs-410,11,1,Text,"00:06:49,040","00:06:53,840",87,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=409,So for example a discovery of,pic_cs-410_11_1_360.jpg
cs-410_11_1_88,cs-410,11,1,Text,"00:06:55,790","00:07:00,140",88,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=415,based on ranking emails for,pic_cs-410_11_1_360.jpg
cs-410_11_1_89,cs-410,11,1,Text,"00:07:00,140","00:07:04,660",89,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=420,And this is useful if you want people,pic_cs-410_11_1_420.jpg
cs-410_11_1_90,cs-410,11,1,Text,"00:07:04,660","00:07:05,770",90,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=424,"spam, right?",pic_cs-410_11_1_420.jpg
cs-410_11_1_91,cs-410,11,1,Text,"00:07:05,770","00:07:10,170",91,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=425,The person would then take,pic_cs-410_11_1_420.jpg
cs-410_11_1_92,cs-410,11,1,Text,"00:07:10,170","00:07:14,770",92,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=430,then verify whether this is indeed a spam.,pic_cs-410_11_1_420.jpg
cs-410_11_1_93,cs-410,11,1,Text,"00:07:14,770","00:07:19,180",93,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=434,So to reflect the utility for,pic_cs-410_11_1_420.jpg
cs-410_11_1_94,cs-410,11,1,Text,"00:07:19,180","00:07:23,860",94,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=439,better to evaluate Ranking Chris and this,pic_cs-410_11_1_420.jpg
cs-410_11_1_95,cs-410,11,1,Text,"00:07:25,020","00:07:27,650",95,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=445,And in such a case often,pic_cs-410_11_1_420.jpg
cs-410_11_1_96,cs-410,11,1,Text,"00:07:27,650","00:07:31,810",96,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=447,better formulated as a ranking problem,pic_cs-410_11_1_420.jpg
cs-410_11_1_97,cs-410,11,1,Text,"00:07:31,810","00:07:35,545",97,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=451,"So for example, ranking documents in",pic_cs-410_11_1_420.jpg
cs-410_11_1_98,cs-410,11,1,Text,"00:07:35,545","00:07:39,255",98,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=455,"as a binary categorization problem,",pic_cs-410_11_1_420.jpg
cs-410_11_1_99,cs-410,11,1,Text,"00:07:39,255","00:07:43,505",99,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=459,are useful to users from those that,pic_cs-410_11_1_420.jpg
cs-410_11_1_100,cs-410,11,1,Text,"00:07:43,505","00:07:47,045",100,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=463,"frame this as a ranking problem,",pic_cs-410_11_1_420.jpg
cs-410_11_1_101,cs-410,11,1,Text,"00:07:47,045","00:07:50,540",101,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=467,That's because people tend,pic_cs-410_11_1_420.jpg
cs-410_11_1_102,cs-410,11,1,Text,"00:07:52,160","00:07:56,420",102,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=472,ranking evaluation more reflects,pic_cs-410_11_1_420.jpg
cs-410_11_1_103,cs-410,11,1,Text,"00:07:58,180","00:08:02,230",103,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=478,"So to summarize categorization evaluation,",pic_cs-410_11_1_420.jpg
cs-410_11_1_104,cs-410,11,1,Text,"00:08:02,230","00:08:05,220",104,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=482,first evaluation is always very,pic_cs-410_11_1_480.jpg
cs-410_11_1_105,cs-410,11,1,Text,"00:08:05,220","00:08:06,090",105,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=485,So get it right.,pic_cs-410_11_1_480.jpg
cs-410_11_1_106,cs-410,11,1,Text,"00:08:07,200","00:08:10,120",106,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=487,"If you don't get it right,",pic_cs-410_11_1_480.jpg
cs-410_11_1_107,cs-410,11,1,Text,"00:08:10,120","00:08:14,160",107,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=490,And you might be misled to believe,pic_cs-410_11_1_480.jpg
cs-410_11_1_108,cs-410,11,1,Text,"00:08:14,160","00:08:15,810",108,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=494,which is in fact not true.,pic_cs-410_11_1_480.jpg
cs-410_11_1_109,cs-410,11,1,Text,"00:08:15,810","00:08:17,460",109,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=495,So it's very important to get it right.,pic_cs-410_11_1_480.jpg
cs-410_11_1_110,cs-410,11,1,Text,"00:08:18,880","00:08:22,270",110,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=498,Measures must also reflect,pic_cs-410_11_1_480.jpg
cs-410_11_1_111,cs-410,11,1,Text,"00:08:22,270","00:08:24,100",111,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=502,a particular application.,pic_cs-410_11_1_480.jpg
cs-410_11_1_112,cs-410,11,1,Text,"00:08:24,100","00:08:25,760",112,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=504,"For example, in spam filtering and",pic_cs-410_11_1_480.jpg
cs-410_11_1_113,cs-410,11,1,Text,"00:08:25,760","00:08:29,670",113,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=505,news categorization the results,pic_cs-410_11_1_480.jpg
cs-410_11_1_114,cs-410,11,1,Text,"00:08:30,680","00:08:33,760",114,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=510,So then we would need to,pic_cs-410_11_1_480.jpg
cs-410_11_1_115,cs-410,11,1,Text,"00:08:33,760","00:08:35,490",115,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=513,design measures appropriately.,pic_cs-410_11_1_480.jpg
cs-410_11_1_116,cs-410,11,1,Text,"00:08:36,650","00:08:41,660",116,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=516,We generally need to consider how will the,pic_cs-410_11_1_480.jpg
cs-410_11_1_117,cs-410,11,1,Text,"00:08:41,660","00:08:43,630",117,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=521,and think from a user's perspective.,pic_cs-410_11_1_480.jpg
cs-410_11_1_118,cs-410,11,1,Text,"00:08:43,630","00:08:46,220",118,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=523,What quality is important?,pic_cs-410_11_1_480.jpg
cs-410_11_1_119,cs-410,11,1,Text,"00:08:46,220","00:08:47,880",119,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=526,What aspect of quality is important?,pic_cs-410_11_1_480.jpg
cs-410_11_1_120,cs-410,11,1,Text,"00:08:49,240","00:08:52,440",120,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=529,Sometimes there are trade offs between,pic_cs-410_11_1_480.jpg
cs-410_11_1_121,cs-410,11,1,Text,"00:08:52,440","00:08:57,610",121,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=532,recall and so we need to know for this,pic_cs-410_11_1_480.jpg
cs-410_11_1_122,cs-410,11,1,Text,"00:08:57,610","00:08:58,860",122,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=537,or high precision is more important.,pic_cs-410_11_1_480.jpg
cs-410_11_1_123,cs-410,11,1,Text,"00:08:59,910","00:09:03,570",123,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=539,Ideally we associate the different cost,pic_cs-410_11_1_480.jpg
cs-410_11_1_124,cs-410,11,1,Text,"00:09:03,570","00:09:06,810",124,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=543,And this of course has to be designed,pic_cs-410_11_1_540.jpg
cs-410_11_1_125,cs-410,11,1,Text,"00:09:08,140","00:09:12,950",125,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=548,Some commonly used measures for relative,pic_cs-410_11_1_540.jpg
cs-410_11_1_126,cs-410,11,1,Text,"00:09:12,950","00:09:17,268",126,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=552,"Classification accuracy, it's very",pic_cs-410_11_1_540.jpg
cs-410_11_1_127,cs-410,11,1,Text,"00:09:17,268","00:09:22,230",127,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=557,[INAUDIBLE] preceding [INAUDIBLE],pic_cs-410_11_1_540.jpg
cs-410_11_1_128,cs-410,11,1,Text,"00:09:22,230","00:09:27,266",128,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=562,"report characterizing performances,",pic_cs-410_11_1_540.jpg
cs-410_11_1_129,cs-410,11,1,Text,"00:09:27,266","00:09:32,440",129,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=567,[INAUDIBLE] like a [INAUDIBLE] Per,pic_cs-410_11_1_540.jpg
cs-410_11_1_130,cs-410,11,1,Text,"00:09:32,440","00:09:37,790",130,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=572,"take a average of all of them, different",pic_cs-410_11_1_540.jpg
cs-410_11_1_131,cs-410,11,1,Text,"00:09:37,790","00:09:42,910",131,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=577,"In general, you want to look at the",pic_cs-410_11_1_540.jpg
cs-410_11_1_132,cs-410,11,1,Text,"00:09:42,910","00:09:46,970",132,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=582,particular applications some perspectives,pic_cs-410_11_1_540.jpg
cs-410_11_1_133,cs-410,11,1,Text,"00:09:46,970","00:09:50,120",133,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=586,diagnoses and,pic_cs-410_11_1_540.jpg
cs-410_11_1_134,cs-410,11,1,Text,"00:09:50,120","00:09:54,920",134,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=590,It's generally useful to look at,pic_cs-410_11_1_540.jpg
cs-410_11_1_135,cs-410,11,1,Text,"00:09:54,920","00:10:00,220",135,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=594,to see subtle differences between methods,pic_cs-410_11_1_540.jpg
cs-410_11_1_136,cs-410,11,1,Text,"00:10:00,220","00:10:03,100",136,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=600,from which you can obtain sight for,pic_cs-410_11_1_600.jpg
cs-410_11_1_137,cs-410,11,1,Text,"00:10:04,670","00:10:07,340",137,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=604,Finally sometimes ranking,pic_cs-410_11_1_600.jpg
cs-410_11_1_138,cs-410,11,1,Text,"00:10:07,340","00:10:11,590",138,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=607,be careful sometimes categorization has,pic_cs-410_11_1_600.jpg
cs-410_11_1_139,cs-410,11,1,Text,"00:10:11,590","00:10:16,390",139,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=611,and there're machine running methods for,pic_cs-410_11_1_600.jpg
cs-410_11_1_140,cs-410,11,1,Text,"00:10:17,480","00:10:19,990",140,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=617,So here are two suggested readings.,pic_cs-410_11_1_600.jpg
cs-410_11_1_141,cs-410,11,1,Text,"00:10:19,990","00:10:25,120",141,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=619,One is some chapters of this book where,pic_cs-410_11_1_600.jpg
cs-410_11_1_142,cs-410,11,1,Text,"00:10:25,120","00:10:27,090",142,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=625,evaluation measures.,pic_cs-410_11_1_600.jpg
cs-410_11_1_143,cs-410,11,1,Text,"00:10:27,090","00:10:31,916",143,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=627,The second is a paper about,pic_cs-410_11_1_600.jpg
cs-410_11_1_144,cs-410,11,1,Text,"00:10:31,916","00:10:33,759",144,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=631,text categorization and,pic_cs-410_11_1_600.jpg
cs-410_11_1_145,cs-410,11,1,Text,"00:10:33,759","00:10:39,738",145,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=633,it also has an excellent discussion of,pic_cs-410_11_1_600.jpg
cs-410_11_1_146,cs-410,11,1,Text,"00:10:39,738","00:10:49,738",146,https://www.coursera.org/learn/cs-410/lecture/kgKI9?t=639,[MUSIC],pic_cs-410_11_1_600.jpg
cs-410_12_1_1,cs-410,12,1,Opinion,"00:00:00,160","00:00:07,187",1,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=0,[SOUND],pic_cs-410_12_1_0.jpg
cs-410_12_1_2,cs-410,12,1,Opinion,"00:00:07,187","00:00:09,830",2,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=7,lecture is about,pic_cs-410_12_1_0.jpg
cs-410_12_1_3,cs-410,12,1,Opinion,"00:00:11,010","00:00:14,400",3,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=11,Contextual text mining,pic_cs-410_12_1_0.jpg
cs-410_12_1_4,cs-410,12,1,Opinion,"00:00:14,400","00:00:18,880",4,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=14,kinds of knowledge that we mine from,pic_cs-410_12_1_0.jpg
cs-410_12_1_5,cs-410,12,1,Opinion,"00:00:18,880","00:00:23,580",5,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=18,It's related to topic mining because you,pic_cs-410_12_1_0.jpg
cs-410_12_1_6,cs-410,12,1,Opinion,"00:00:23,580","00:00:25,260",6,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=23,like time or location.,pic_cs-410_12_1_0.jpg
cs-410_12_1_7,cs-410,12,1,Opinion,"00:00:25,260","00:00:29,720",7,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=25,"And similarly, we can make opinion",pic_cs-410_12_1_0.jpg
cs-410_12_1_8,cs-410,12,1,Opinion,"00:00:29,720","00:00:32,590",8,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=29,making opinions connected to context.,pic_cs-410_12_1_0.jpg
cs-410_12_1_9,cs-410,12,1,Opinion,"00:00:34,090","00:00:38,320",9,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=34,It's related to text based prediction,pic_cs-410_12_1_0.jpg
cs-410_12_1_10,cs-410,12,1,Opinion,"00:00:38,320","00:00:43,690",10,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=38,data with text data to derive,pic_cs-410_12_1_0.jpg
cs-410_12_1_11,cs-410,12,1,Opinion,"00:00:43,690","00:00:45,110",11,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=43,the prediction problem.,pic_cs-410_12_1_0.jpg
cs-410_12_1_12,cs-410,12,1,Opinion,"00:00:45,110","00:00:49,290",12,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=45,"So more specifically, why are we",pic_cs-410_12_1_0.jpg
cs-410_12_1_13,cs-410,12,1,Opinion,"00:00:49,290","00:00:54,510",13,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=49,"Well, that's first because text",pic_cs-410_12_1_0.jpg
cs-410_12_1_14,cs-410,12,1,Opinion,"00:00:54,510","00:01:01,740",14,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=54,And this can include direct context such,pic_cs-410_12_1_0.jpg
cs-410_12_1_15,cs-410,12,1,Opinion,"00:01:01,740","00:01:06,260",15,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=61,"So, the direct context can grow",pic_cs-410_12_1_60.jpg
cs-410_12_1_16,cs-410,12,1,Opinion,"00:01:06,260","00:01:10,030",16,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=66,"location, authors, and",pic_cs-410_12_1_60.jpg
cs-410_12_1_17,cs-410,12,1,Opinion,"00:01:10,030","00:01:12,710",17,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=70,And they're almost always available to us.,pic_cs-410_12_1_60.jpg
cs-410_12_1_18,cs-410,12,1,Opinion,"00:01:14,280","00:01:19,690",18,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=74,Indirect context refers to additional,pic_cs-410_12_1_60.jpg
cs-410_12_1_19,cs-410,12,1,Opinion,"00:01:19,690","00:01:24,440",19,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=79,"So for example, from office,",pic_cs-410_12_1_60.jpg
cs-410_12_1_20,cs-410,12,1,Opinion,"00:01:24,440","00:01:29,280",20,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=84,context such as social network of,pic_cs-410_12_1_60.jpg
cs-410_12_1_21,cs-410,12,1,Opinion,"00:01:30,300","00:01:34,028",21,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=90,Such information is not in general,pic_cs-410_12_1_60.jpg
cs-410_12_1_22,cs-410,12,1,Opinion,"00:01:34,028","00:01:37,350",22,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=94,"through the process, we can connect them.",pic_cs-410_12_1_60.jpg
cs-410_12_1_23,cs-410,12,1,Opinion,"00:01:37,350","00:01:41,080",23,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=97,There could be other text,pic_cs-410_12_1_60.jpg
cs-410_12_1_24,cs-410,12,1,Opinion,"00:01:41,080","00:01:46,760",24,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=101,as this one through the other text can,pic_cs-410_12_1_60.jpg
cs-410_12_1_25,cs-410,12,1,Opinion,"00:01:46,760","00:01:51,400",25,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=106,"So in general, any related data",pic_cs-410_12_1_60.jpg
cs-410_12_1_26,cs-410,12,1,Opinion,"00:01:51,400","00:01:53,830",26,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=111,So there could be removed or,pic_cs-410_12_1_60.jpg
cs-410_12_1_27,cs-410,12,1,Opinion,"00:01:55,460","00:01:56,870",27,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=115,And so what's the use?,pic_cs-410_12_1_60.jpg
cs-410_12_1_28,cs-410,12,1,Opinion,"00:01:56,870","00:01:59,100",28,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=116,What is text context used for?,pic_cs-410_12_1_60.jpg
cs-410_12_1_29,cs-410,12,1,Opinion,"00:01:59,100","00:02:05,880",29,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=119,"Well, context can be used to partition",pic_cs-410_12_1_60.jpg
cs-410_12_1_30,cs-410,12,1,Opinion,"00:02:05,880","00:02:11,490",30,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=125,It can almost allow us to partition,pic_cs-410_12_1_120.jpg
cs-410_12_1_31,cs-410,12,1,Opinion,"00:02:11,490","00:02:14,180",31,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=131,And this is very important,pic_cs-410_12_1_120.jpg
cs-410_12_1_32,cs-410,12,1,Opinion,"00:02:14,180","00:02:18,060",32,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=134,us to do interesting comparative analyses.,pic_cs-410_12_1_120.jpg
cs-410_12_1_33,cs-410,12,1,Opinion,"00:02:18,060","00:02:21,622",33,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=138,"It also in general,",pic_cs-410_12_1_120.jpg
cs-410_12_1_34,cs-410,12,1,Opinion,"00:02:21,622","00:02:23,770",34,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=141,if we associate the text with context.,pic_cs-410_12_1_120.jpg
cs-410_12_1_35,cs-410,12,1,Opinion,"00:02:25,290","00:02:30,540",35,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=145,So here's illustration of how context,pic_cs-410_12_1_120.jpg
cs-410_12_1_36,cs-410,12,1,Opinion,"00:02:30,540","00:02:35,520",36,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=150,can be regarded as interesting,pic_cs-410_12_1_120.jpg
cs-410_12_1_37,cs-410,12,1,Opinion,"00:02:35,520","00:02:39,449",37,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=155,So here I just showed some research,pic_cs-410_12_1_120.jpg
cs-410_12_1_38,cs-410,12,1,Opinion,"00:02:41,740","00:02:43,122",38,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=161,"On different venues,",pic_cs-410_12_1_120.jpg
cs-410_12_1_39,cs-410,12,1,Opinion,"00:02:43,122","00:02:48,150",39,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=163,different conference names here listed on,pic_cs-410_12_1_120.jpg
cs-410_12_1_40,cs-410,12,1,Opinion,"00:02:49,640","00:02:53,080",40,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=169,Now such text data can be partitioned,pic_cs-410_12_1_120.jpg
cs-410_12_1_41,cs-410,12,1,Opinion,"00:02:53,080","00:02:55,480",41,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=173,in many interesting ways,pic_cs-410_12_1_120.jpg
cs-410_12_1_42,cs-410,12,1,Opinion,"00:02:56,860","00:03:01,620",42,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=176,So the context here just includes time and,pic_cs-410_12_1_120.jpg
cs-410_12_1_43,cs-410,12,1,Opinion,"00:03:01,620","00:03:04,930",43,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=181,But perhaps we can include,pic_cs-410_12_1_180.jpg
cs-410_12_1_44,cs-410,12,1,Opinion,"00:03:06,480","00:03:08,890",44,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=186,But let's see how we can partition,pic_cs-410_12_1_180.jpg
cs-410_12_1_45,cs-410,12,1,Opinion,"00:03:08,890","00:03:12,840",45,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=188,"First, we can treat each",pic_cs-410_12_1_180.jpg
cs-410_12_1_46,cs-410,12,1,Opinion,"00:03:12,840","00:03:17,987",46,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=192,"So in this case, a paper ID and the,",pic_cs-410_12_1_180.jpg
cs-410_12_1_47,cs-410,12,1,Opinion,"00:03:17,987","00:03:22,575",47,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=197,It's independent.,pic_cs-410_12_1_180.jpg
cs-410_12_1_48,cs-410,12,1,Opinion,"00:03:22,575","00:03:27,825",48,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=202,But we can also treat all the papers,pic_cs-410_12_1_180.jpg
cs-410_12_1_49,cs-410,12,1,Opinion,"00:03:27,825","00:03:32,530",49,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=207,this is only possible because,pic_cs-410_12_1_180.jpg
cs-410_12_1_50,cs-410,12,1,Opinion,"00:03:32,530","00:03:34,605",50,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=212,And we can partition data in this way.,pic_cs-410_12_1_180.jpg
cs-410_12_1_51,cs-410,12,1,Opinion,"00:03:34,605","00:03:38,012",51,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=214,This would allow us to compare topics for,pic_cs-410_12_1_180.jpg
cs-410_12_1_52,cs-410,12,1,Opinion,"00:03:39,840","00:03:42,745",52,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=219,"Similarly, we can partition",pic_cs-410_12_1_180.jpg
cs-410_12_1_53,cs-410,12,1,Opinion,"00:03:42,745","00:03:47,740",53,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=222,We can get all the SIGIR papers and,pic_cs-410_12_1_180.jpg
cs-410_12_1_54,cs-410,12,1,Opinion,"00:03:47,740","00:03:51,039",54,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=227,"Or compare SIGIR papers with KDD papers,",pic_cs-410_12_1_180.jpg
cs-410_12_1_55,cs-410,12,1,Opinion,"00:03:52,700","00:03:58,740",55,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=232,We can also partition the data to obtain,pic_cs-410_12_1_180.jpg
cs-410_12_1_56,cs-410,12,1,Opinion,"00:03:58,740","00:04:03,340",56,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=238,"and that of course,",pic_cs-410_12_1_180.jpg
cs-410_12_1_57,cs-410,12,1,Opinion,"00:04:03,340","00:04:08,240",57,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=243,And this would allow us to then,pic_cs-410_12_1_240.jpg
cs-410_12_1_58,cs-410,12,1,Opinion,"00:04:08,240","00:04:12,390",58,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=248,another set of papers written,pic_cs-410_12_1_240.jpg
cs-410_12_1_59,cs-410,12,1,Opinion,"00:04:13,910","00:04:17,810",59,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=253,Or we can obtain a set of,pic_cs-410_12_1_240.jpg
cs-410_12_1_60,cs-410,12,1,Opinion,"00:04:17,810","00:04:21,890",60,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=257,this can be compared with,pic_cs-410_12_1_240.jpg
cs-410_12_1_61,cs-410,12,1,Opinion,"00:04:21,890","00:04:25,310",61,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=261,And note that these,pic_cs-410_12_1_240.jpg
cs-410_12_1_62,cs-410,12,1,Opinion,"00:04:25,310","00:04:28,860",62,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=265,intersected with each other to generate,pic_cs-410_12_1_240.jpg
cs-410_12_1_63,cs-410,12,1,Opinion,"00:04:29,890","00:04:34,280",63,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=269,"And so in general, this enables",pic_cs-410_12_1_240.jpg
cs-410_12_1_64,cs-410,12,1,Opinion,"00:04:34,280","00:04:35,890",64,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=274,different context as needed.,pic_cs-410_12_1_240.jpg
cs-410_12_1_65,cs-410,12,1,Opinion,"00:04:37,150","00:04:40,300",65,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=277,"And in particular,",pic_cs-410_12_1_240.jpg
cs-410_12_1_66,cs-410,12,1,Opinion,"00:04:40,300","00:04:43,620",66,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=280,And this often gives us,pic_cs-410_12_1_240.jpg
cs-410_12_1_67,cs-410,12,1,Opinion,"00:04:43,620","00:04:49,350",67,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=283,"For example, comparing topics over time,",pic_cs-410_12_1_240.jpg
cs-410_12_1_68,cs-410,12,1,Opinion,"00:04:49,350","00:04:53,130",68,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=289,Comparing topics in different,pic_cs-410_12_1_240.jpg
cs-410_12_1_69,cs-410,12,1,Opinion,"00:04:53,130","00:04:55,230",69,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=293,about the two contexts.,pic_cs-410_12_1_240.jpg
cs-410_12_1_70,cs-410,12,1,Opinion,"00:04:55,230","00:04:59,680",70,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=295,So there are many interesting questions,pic_cs-410_12_1_240.jpg
cs-410_12_1_71,cs-410,12,1,Opinion,"00:04:59,680","00:05:01,780",71,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=299,Here I list some very specific ones.,pic_cs-410_12_1_240.jpg
cs-410_12_1_72,cs-410,12,1,Opinion,"00:05:01,780","00:05:05,300",72,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=301,"For example, what topics have",pic_cs-410_12_1_300.jpg
cs-410_12_1_73,cs-410,12,1,Opinion,"00:05:05,300","00:05:07,420",73,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=305,recently in data mining research?,pic_cs-410_12_1_300.jpg
cs-410_12_1_74,cs-410,12,1,Opinion,"00:05:07,420","00:05:08,570",74,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=307,"Now to answer this question,",pic_cs-410_12_1_300.jpg
cs-410_12_1_75,cs-410,12,1,Opinion,"00:05:08,570","00:05:11,724",75,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=308,obviously we need to analyze,pic_cs-410_12_1_300.jpg
cs-410_12_1_76,cs-410,12,1,Opinion,"00:05:13,815","00:05:17,455",76,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=313,So time is context in this case.,pic_cs-410_12_1_300.jpg
cs-410_12_1_77,cs-410,12,1,Opinion,"00:05:17,455","00:05:20,675",77,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=317,Is there any difference in the responses,pic_cs-410_12_1_300.jpg
cs-410_12_1_78,cs-410,12,1,Opinion,"00:05:20,675","00:05:22,885",78,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=320,"to the event, to any event?",pic_cs-410_12_1_300.jpg
cs-410_12_1_79,cs-410,12,1,Opinion,"00:05:22,885","00:05:25,515",79,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=322,So this is a very broad,pic_cs-410_12_1_300.jpg
cs-410_12_1_80,cs-410,12,1,Opinion,"00:05:25,515","00:05:28,635",80,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=325,"In this case of course,",pic_cs-410_12_1_300.jpg
cs-410_12_1_81,cs-410,12,1,Opinion,"00:05:28,635","00:05:31,260",81,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=328,What are the common research,pic_cs-410_12_1_300.jpg
cs-410_12_1_82,cs-410,12,1,Opinion,"00:05:31,260","00:05:34,110",82,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=331,"In this case, authors can be the context.",pic_cs-410_12_1_300.jpg
cs-410_12_1_83,cs-410,12,1,Opinion,"00:05:34,110","00:05:38,250",83,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=334,Is there any difference in the research,pic_cs-410_12_1_300.jpg
cs-410_12_1_84,cs-410,12,1,Opinion,"00:05:38,250","00:05:39,920",84,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=338,those outside?,pic_cs-410_12_1_300.jpg
cs-410_12_1_85,cs-410,12,1,Opinion,"00:05:39,920","00:05:43,990",85,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=339,"Now in this case,",pic_cs-410_12_1_300.jpg
cs-410_12_1_86,cs-410,12,1,Opinion,"00:05:43,990","00:05:46,030",86,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=343,their affiliation and location.,pic_cs-410_12_1_300.jpg
cs-410_12_1_87,cs-410,12,1,Opinion,"00:05:47,810","00:05:51,700",87,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=347,So this goes beyond just,pic_cs-410_12_1_300.jpg
cs-410_12_1_88,cs-410,12,1,Opinion,"00:05:51,700","00:05:55,520",88,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=351,We need to look at the additional,pic_cs-410_12_1_300.jpg
cs-410_12_1_89,cs-410,12,1,Opinion,"00:05:55,520","00:05:58,580",89,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=355,Is there any difference in the opinions,pic_cs-410_12_1_300.jpg
cs-410_12_1_90,cs-410,12,1,Opinion,"00:05:58,580","00:06:00,720",90,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=358,one social network and another?,pic_cs-410_12_1_300.jpg
cs-410_12_1_91,cs-410,12,1,Opinion,"00:06:00,720","00:06:04,870",91,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=360,"In this case, the social network of",pic_cs-410_12_1_360.jpg
cs-410_12_1_92,cs-410,12,1,Opinion,"00:06:06,128","00:06:10,250",92,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=366,Other topics in news data that,pic_cs-410_12_1_360.jpg
cs-410_12_1_93,cs-410,12,1,Opinion,"00:06:10,250","00:06:11,470",93,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=370,stock prices.,pic_cs-410_12_1_360.jpg
cs-410_12_1_94,cs-410,12,1,Opinion,"00:06:11,470","00:06:16,060",94,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=371,"In this case, we can use a time series",pic_cs-410_12_1_360.jpg
cs-410_12_1_95,cs-410,12,1,Opinion,"00:06:17,230","00:06:20,780",95,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=377,What issues mattered in the 2012,pic_cs-410_12_1_360.jpg
cs-410_12_1_96,cs-410,12,1,Opinion,"00:06:20,780","00:06:21,410",96,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=380,presidential election?,pic_cs-410_12_1_360.jpg
cs-410_12_1_97,cs-410,12,1,Opinion,"00:06:21,410","00:06:26,350",97,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=381,"Now in this case,",pic_cs-410_12_1_360.jpg
cs-410_12_1_98,cs-410,12,1,Opinion,"00:06:26,350","00:06:29,005",98,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=386,"So, as you can see,",pic_cs-410_12_1_360.jpg
cs-410_12_1_99,cs-410,12,1,Opinion,"00:06:29,005","00:06:34,911",99,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=389,"Basically, contextual text mining",pic_cs-410_12_1_360.jpg
cs-410_12_1_100,cs-410,12,1,Opinion,"00:06:34,911","00:06:44,911",100,https://www.coursera.org/learn/cs-410/lecture/dkntE?t=394,[MUSIC],pic_cs-410_12_1_360.jpg
cs-410_12_2_1,cs-410,12,2,Opinion,"00:00:06,910","00:00:09,570",1,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=6,This lecture is a summary,pic_cs-410_12_2_0.jpg
cs-410_12_2_2,cs-410,12,2,Opinion,"00:00:10,810","00:00:14,512",2,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=10,"First, let's revisit the topics",pic_cs-410_12_2_0.jpg
cs-410_12_2_3,cs-410,12,2,Opinion,"00:00:14,512","00:00:18,662",3,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=14,"In the beginning, we talked about",pic_cs-410_12_2_0.jpg
cs-410_12_2_4,cs-410,12,2,Opinion,"00:00:18,662","00:00:21,025",4,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=18,how it can enrich text representation.,pic_cs-410_12_2_0.jpg
cs-410_12_2_5,cs-410,12,2,Opinion,"00:00:21,025","00:00:26,207",5,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=21,We then talked about how to mine,pic_cs-410_12_2_0.jpg
cs-410_12_2_6,cs-410,12,2,Opinion,"00:00:26,207","00:00:29,434",6,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=26,"natural language used to express the,",pic_cs-410_12_2_0.jpg
cs-410_12_2_7,cs-410,12,2,Opinion,"00:00:29,434","00:00:33,270",7,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=29,what's observing the world in text and,pic_cs-410_12_2_0.jpg
cs-410_12_2_8,cs-410,12,2,Opinion,"00:00:34,320","00:00:38,410",8,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=34,"In particular, we talked about",pic_cs-410_12_2_0.jpg
cs-410_12_2_9,cs-410,12,2,Opinion,"00:00:38,410","00:00:42,300",9,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=38,We then talked about how,pic_cs-410_12_2_0.jpg
cs-410_12_2_10,cs-410,12,2,Opinion,"00:00:42,300","00:00:45,130",10,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=42,How to discover topics and analyze them.,pic_cs-410_12_2_0.jpg
cs-410_12_2_11,cs-410,12,2,Opinion,"00:00:47,580","00:00:51,314",11,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=47,This can be regarded as,pic_cs-410_12_2_0.jpg
cs-410_12_2_12,cs-410,12,2,Opinion,"00:00:51,314","00:00:55,747",12,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=51,and then we talked about how to mine,pic_cs-410_12_2_0.jpg
cs-410_12_2_13,cs-410,12,2,Opinion,"00:00:55,747","00:01:00,988",13,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=55,"particularly talk about the, how to",pic_cs-410_12_2_0.jpg
cs-410_12_2_14,cs-410,12,2,Opinion,"00:01:00,988","00:01:06,048",14,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=60,"And finally, we will talk about",pic_cs-410_12_2_60.jpg
cs-410_12_2_15,cs-410,12,2,Opinion,"00:01:06,048","00:01:11,204",15,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=66,do with predicting values of other real,pic_cs-410_12_2_60.jpg
cs-410_12_2_16,cs-410,12,2,Opinion,"00:01:11,204","00:01:16,270",16,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=71,"And in discussing this, we will also",pic_cs-410_12_2_60.jpg
cs-410_12_2_17,cs-410,12,2,Opinion,"00:01:16,270","00:01:21,421",17,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=76,which can contribute additional,pic_cs-410_12_2_60.jpg
cs-410_12_2_18,cs-410,12,2,Opinion,"00:01:21,421","00:01:25,425",18,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=81,and also can provide context for,pic_cs-410_12_2_60.jpg
cs-410_12_2_19,cs-410,12,2,Opinion,"00:01:25,425","00:01:30,110",19,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=85,in particular we talked about how,pic_cs-410_12_2_60.jpg
cs-410_12_2_20,cs-410,12,2,Opinion,"00:01:33,240","00:01:39,078",20,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=93,So here are the key high-level,pic_cs-410_12_2_60.jpg
cs-410_12_2_21,cs-410,12,2,Opinion,"00:01:39,078","00:01:41,670",21,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=99,I going to go over these major topics and,pic_cs-410_12_2_60.jpg
cs-410_12_2_22,cs-410,12,2,Opinion,"00:01:41,670","00:01:46,400",22,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=101,point out what are the key take-away,pic_cs-410_12_2_60.jpg
cs-410_12_2_23,cs-410,12,2,Opinion,"00:01:47,560","00:01:50,630",23,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=107,First the NLP and text representation.,pic_cs-410_12_2_60.jpg
cs-410_12_2_24,cs-410,12,2,Opinion,"00:01:53,530","00:01:56,840",24,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=113,You should realize that NLP,pic_cs-410_12_2_60.jpg
cs-410_12_2_25,cs-410,12,2,Opinion,"00:01:56,840","00:02:01,510",25,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=116,any text replication because it,pic_cs-410_12_2_60.jpg
cs-410_12_2_26,cs-410,12,2,Opinion,"00:02:01,510","00:02:05,060",26,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=121,The more NLP the better text,pic_cs-410_12_2_120.jpg
cs-410_12_2_27,cs-410,12,2,Opinion,"00:02:05,060","00:02:08,500",27,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=125,And this further enables more,pic_cs-410_12_2_120.jpg
cs-410_12_2_28,cs-410,12,2,Opinion,"00:02:08,500","00:02:11,710",28,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=128,"to discover deeper knowledge,",pic_cs-410_12_2_120.jpg
cs-410_12_2_29,cs-410,12,2,Opinion,"00:02:12,950","00:02:17,510",29,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=132,"However, the current estate of art",pic_cs-410_12_2_120.jpg
cs-410_12_2_30,cs-410,12,2,Opinion,"00:02:17,510","00:02:19,130",30,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=137,still not robust enough.,pic_cs-410_12_2_120.jpg
cs-410_12_2_31,cs-410,12,2,Opinion,"00:02:19,130","00:02:23,586",31,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=139,"So, as an result,",pic_cs-410_12_2_120.jpg
cs-410_12_2_32,cs-410,12,2,Opinion,"00:02:23,586","00:02:26,960",32,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=143,tend to be based on world [INAUDIBLE].,pic_cs-410_12_2_120.jpg
cs-410_12_2_33,cs-410,12,2,Opinion,"00:02:26,960","00:02:30,710",33,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=146,And tend to rely a lot,pic_cs-410_12_2_120.jpg
cs-410_12_2_34,cs-410,12,2,Opinion,"00:02:30,710","00:02:33,520",34,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=150,as we've discussed in this course.,pic_cs-410_12_2_120.jpg
cs-410_12_2_35,cs-410,12,2,Opinion,"00:02:33,520","00:02:39,700",35,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=153,And you may recall we've mostly,pic_cs-410_12_2_120.jpg
cs-410_12_2_36,cs-410,12,2,Opinion,"00:02:39,700","00:02:42,478",36,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=159,And we've relied a lot on,pic_cs-410_12_2_120.jpg
cs-410_12_2_37,cs-410,12,2,Opinion,"00:02:42,478","00:02:45,202",37,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=162,statistical learning,pic_cs-410_12_2_120.jpg
cs-410_12_2_38,cs-410,12,2,Opinion,"00:02:47,790","00:02:52,771",38,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=167,In word-association mining and,pic_cs-410_12_2_120.jpg
cs-410_12_2_39,cs-410,12,2,Opinion,"00:02:52,771","00:02:56,282",39,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=172,we are introduced the two concepts for,pic_cs-410_12_2_120.jpg
cs-410_12_2_40,cs-410,12,2,Opinion,"00:02:56,282","00:03:02,835",40,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=176,"complementary relations of words,",pic_cs-410_12_2_120.jpg
cs-410_12_2_41,cs-410,12,2,Opinion,"00:03:02,835","00:03:08,130",41,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=182,These are actually very general,pic_cs-410_12_2_180.jpg
cs-410_12_2_42,cs-410,12,2,Opinion,"00:03:08,130","00:03:14,330",42,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=188,If you take it as meaning,pic_cs-410_12_2_180.jpg
cs-410_12_2_43,cs-410,12,2,Opinion,"00:03:14,330","00:03:18,840",43,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=194,context in the sequence and elements,pic_cs-410_12_2_180.jpg
cs-410_12_2_44,cs-410,12,2,Opinion,"00:03:18,840","00:03:24,090",44,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=198,And these relations might be also,pic_cs-410_12_2_180.jpg
cs-410_12_2_45,cs-410,12,2,Opinion,"00:03:25,810","00:03:29,810",45,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=205,We also talked a lot about,pic_cs-410_12_2_180.jpg
cs-410_12_2_46,cs-410,12,2,Opinion,"00:03:29,810","00:03:34,350",46,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=209,discuss how to discover,pic_cs-410_12_2_180.jpg
cs-410_12_2_47,cs-410,12,2,Opinion,"00:03:34,350","00:03:38,390",47,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=214,the context of words discover,pic_cs-410_12_2_180.jpg
cs-410_12_2_48,cs-410,12,2,Opinion,"00:03:38,390","00:03:39,858",48,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=218,"At that point level,",pic_cs-410_12_2_180.jpg
cs-410_12_2_49,cs-410,12,2,Opinion,"00:03:39,858","00:03:44,437",49,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=219,we talked about representing text,pic_cs-410_12_2_180.jpg
cs-410_12_2_50,cs-410,12,2,Opinion,"00:03:44,437","00:03:48,638",50,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=224,And we talked about some retrieval,pic_cs-410_12_2_180.jpg
cs-410_12_2_51,cs-410,12,2,Opinion,"00:03:48,638","00:03:52,995",51,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=228,measuring similarity of text and,pic_cs-410_12_2_180.jpg
cs-410_12_2_52,cs-410,12,2,Opinion,"00:03:52,995","00:03:55,193",52,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=232,"tf-idf weighting, et cetera.",pic_cs-410_12_2_180.jpg
cs-410_12_2_53,cs-410,12,2,Opinion,"00:03:55,193","00:03:59,480",53,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=235,And this part is well-connected,pic_cs-410_12_2_180.jpg
cs-410_12_2_54,cs-410,12,2,Opinion,"00:03:59,480","00:04:02,330",54,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=239,There are other techniques that,pic_cs-410_12_2_180.jpg
cs-410_12_2_55,cs-410,12,2,Opinion,"00:04:03,890","00:04:08,650",55,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=243,The next point is about,pic_cs-410_12_2_240.jpg
cs-410_12_2_56,cs-410,12,2,Opinion,"00:04:08,650","00:04:12,770",56,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=248,we introduce some information,pic_cs-410_12_2_240.jpg
cs-410_12_2_57,cs-410,12,2,Opinion,"00:04:12,770","00:04:15,170",57,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=252,"conditional entropy,",pic_cs-410_12_2_240.jpg
cs-410_12_2_58,cs-410,12,2,Opinion,"00:04:15,170","00:04:18,293",58,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=255,These are not only very useful for,pic_cs-410_12_2_240.jpg
cs-410_12_2_59,cs-410,12,2,Opinion,"00:04:18,293","00:04:23,680",59,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=258,"measuring the co-occurrences of words,",pic_cs-410_12_2_240.jpg
cs-410_12_2_60,cs-410,12,2,Opinion,"00:04:23,680","00:04:26,940",60,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=263,"analyzing other kind of data, and",pic_cs-410_12_2_240.jpg
cs-410_12_2_61,cs-410,12,2,Opinion,"00:04:26,940","00:04:29,600",61,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=266,feature selection in text,pic_cs-410_12_2_240.jpg
cs-410_12_2_62,cs-410,12,2,Opinion,"00:04:30,920","00:04:34,460",62,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=270,"So this is another important concept,",pic_cs-410_12_2_240.jpg
cs-410_12_2_63,cs-410,12,2,Opinion,"00:04:35,480","00:04:38,640",63,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=275,And then we talked about,pic_cs-410_12_2_240.jpg
cs-410_12_2_64,cs-410,12,2,Opinion,"00:04:38,640","00:04:41,570",64,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=278,that's where we introduce in,pic_cs-410_12_2_240.jpg
cs-410_12_2_65,cs-410,12,2,Opinion,"00:04:41,570","00:04:45,960",65,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=281,We spent a lot of time to,pic_cs-410_12_2_240.jpg
cs-410_12_2_66,cs-410,12,2,Opinion,"00:04:45,960","00:04:52,930",66,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=285,"PLSA in detail and this is, those are the",pic_cs-410_12_2_240.jpg
cs-410_12_2_67,cs-410,12,2,Opinion,"00:04:52,930","00:04:56,190",67,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=292,"Theoretically, a more opinion model, but",pic_cs-410_12_2_240.jpg
cs-410_12_2_68,cs-410,12,2,Opinion,"00:04:56,190","00:05:01,460",68,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=296,we did not have enough time to really,pic_cs-410_12_2_240.jpg
cs-410_12_2_69,cs-410,12,2,Opinion,"00:05:02,960","00:05:06,600",69,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=302,"But in practice,",pic_cs-410_12_2_300.jpg
cs-410_12_2_70,cs-410,12,2,Opinion,"00:05:06,600","00:05:09,520",70,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=306,it's simpler to implement and,pic_cs-410_12_2_300.jpg
cs-410_12_2_71,cs-410,12,2,Opinion,"00:05:11,520","00:05:15,930",71,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=311,In this part of Wilson videos is some,pic_cs-410_12_2_300.jpg
cs-410_12_2_72,cs-410,12,2,Opinion,"00:05:15,930","00:05:20,410",72,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=315,"know, one is generative model,",pic_cs-410_12_2_300.jpg
cs-410_12_2_73,cs-410,12,2,Opinion,"00:05:20,410","00:05:23,630",73,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=320,modeling text data and,pic_cs-410_12_2_300.jpg
cs-410_12_2_74,cs-410,12,2,Opinion,"00:05:24,740","00:05:30,250",74,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=324,And we talked about the maximum life,pic_cs-410_12_2_300.jpg
cs-410_12_2_75,cs-410,12,2,Opinion,"00:05:30,250","00:05:35,290",75,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=330,solving the problem of,pic_cs-410_12_2_300.jpg
cs-410_12_2_76,cs-410,12,2,Opinion,"00:05:35,290","00:05:38,720",76,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=335,"So, these are all general techniques",pic_cs-410_12_2_300.jpg
cs-410_12_2_77,cs-410,12,2,Opinion,"00:05:38,720","00:05:39,840",77,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=338,in other scenarios as well.,pic_cs-410_12_2_300.jpg
cs-410_12_2_78,cs-410,12,2,Opinion,"00:05:40,940","00:05:45,020",78,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=340,Then we talked about the text,pic_cs-410_12_2_300.jpg
cs-410_12_2_79,cs-410,12,2,Opinion,"00:05:45,020","00:05:50,450",79,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=345,Those are two important building blocks,pic_cs-410_12_2_300.jpg
cs-410_12_2_80,cs-410,12,2,Opinion,"00:05:50,450","00:05:56,110",80,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=350,In text with clustering we talked,pic_cs-410_12_2_300.jpg
cs-410_12_2_81,cs-410,12,2,Opinion,"00:05:56,110","00:06:02,400",81,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=356,using a slightly different mixture module,pic_cs-410_12_2_300.jpg
cs-410_12_2_82,cs-410,12,2,Opinion,"00:06:02,400","00:06:07,060",82,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=362,and we then also prefer to,pic_cs-410_12_2_360.jpg
cs-410_12_2_83,cs-410,12,2,Opinion,"00:06:07,060","00:06:10,000",83,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=367,approaches to test for cuss word.,pic_cs-410_12_2_360.jpg
cs-410_12_2_84,cs-410,12,2,Opinion,"00:06:11,340","00:06:15,350",84,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=371,In categorization we also talk,pic_cs-410_12_2_360.jpg
cs-410_12_2_85,cs-410,12,2,Opinion,"00:06:15,350","00:06:19,390",85,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=375,One is generative classifies,pic_cs-410_12_2_360.jpg
cs-410_12_2_86,cs-410,12,2,Opinion,"00:06:20,690","00:06:24,870",86,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=380,infer the condition of or,pic_cs-410_12_2_360.jpg
cs-410_12_2_87,cs-410,12,2,Opinion,"00:06:24,870","00:06:28,250",87,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=384,in deeper we'll introduce you should,pic_cs-410_12_2_360.jpg
cs-410_12_2_88,cs-410,12,2,Opinion,"00:06:29,280","00:06:36,160",88,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=389,"This is the practical use for technique,",pic_cs-410_12_2_360.jpg
cs-410_12_2_89,cs-410,12,2,Opinion,"00:06:37,210","00:06:41,010",89,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=397,We also introduce the some,pic_cs-410_12_2_360.jpg
cs-410_12_2_90,cs-410,12,2,Opinion,"00:06:41,010","00:06:45,300",90,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=401,"particularly logistical regression,",pic_cs-410_12_2_360.jpg
cs-410_12_2_91,cs-410,12,2,Opinion,"00:06:45,300","00:06:49,030",91,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=405,"They also very important, they are very",pic_cs-410_12_2_360.jpg
cs-410_12_2_92,cs-410,12,2,Opinion,"00:06:49,030","00:06:50,490",92,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=409,text capitalization as well.,pic_cs-410_12_2_360.jpg
cs-410_12_2_93,cs-410,12,2,Opinion,"00:06:52,370","00:06:57,100",93,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=412,"In both parts, we'll also discuss",pic_cs-410_12_2_360.jpg
cs-410_12_2_94,cs-410,12,2,Opinion,"00:06:57,100","00:07:03,110",94,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=417,Evaluation is quite important because if,pic_cs-410_12_2_360.jpg
cs-410_12_2_95,cs-410,12,2,Opinion,"00:07:03,110","00:07:07,430",95,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=423,reflect the volatility of the method then,pic_cs-410_12_2_420.jpg
cs-410_12_2_96,cs-410,12,2,Opinion,"00:07:07,430","00:07:10,530",96,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=427,its very important to,pic_cs-410_12_2_420.jpg
cs-410_12_2_97,cs-410,12,2,Opinion,"00:07:10,530","00:07:15,420",97,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=430,And we talked about variation of,pic_cs-410_12_2_420.jpg
cs-410_12_2_98,cs-410,12,2,Opinion,"00:07:15,420","00:07:16,550",98,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=435,specific measures.,pic_cs-410_12_2_420.jpg
cs-410_12_2_99,cs-410,12,2,Opinion,"00:07:18,530","00:07:21,725",99,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=438,Then we talked about the sentiment,pic_cs-410_12_2_420.jpg
cs-410_12_2_100,cs-410,12,2,Opinion,"00:07:21,725","00:07:25,053",100,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=441,that's where we introduced,pic_cs-410_12_2_420.jpg
cs-410_12_2_101,cs-410,12,2,Opinion,"00:07:25,053","00:07:29,681",101,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=445,And although it's a special,pic_cs-410_12_2_420.jpg
cs-410_12_2_102,cs-410,12,2,Opinion,"00:07:29,681","00:07:34,932",102,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=449,we talked about how to extend or,pic_cs-410_12_2_420.jpg
cs-410_12_2_103,cs-410,12,2,Opinion,"00:07:34,932","00:07:41,261",103,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=454,by using more sophisticated features that,pic_cs-410_12_2_420.jpg
cs-410_12_2_104,cs-410,12,2,Opinion,"00:07:41,261","00:07:46,240",104,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=461,We did a review of some common use for,pic_cs-410_12_2_420.jpg
cs-410_12_2_105,cs-410,12,2,Opinion,"00:07:46,240","00:07:50,836",105,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=466,then we also talked about how to,pic_cs-410_12_2_420.jpg
cs-410_12_2_106,cs-410,12,2,Opinion,"00:07:50,836","00:07:55,511",106,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=470,"in sentiment classification, and",pic_cs-410_12_2_420.jpg
cs-410_12_2_107,cs-410,12,2,Opinion,"00:07:55,511","00:08:00,822",107,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=475,logistical regression then we also talked,pic_cs-410_12_2_420.jpg
cs-410_12_2_108,cs-410,12,2,Opinion,"00:08:00,822","00:08:05,104",108,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=480,This is an unsupervised way of using,pic_cs-410_12_2_480.jpg
cs-410_12_2_109,cs-410,12,2,Opinion,"00:08:05,104","00:08:07,280",109,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=485,review data in more detail.,pic_cs-410_12_2_480.jpg
cs-410_12_2_110,cs-410,12,2,Opinion,"00:08:07,280","00:08:12,650",110,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=487,"In particular, it allows us to",pic_cs-410_12_2_480.jpg
cs-410_12_2_111,cs-410,12,2,Opinion,"00:08:14,580","00:08:18,490",111,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=494,a reviewer on different,pic_cs-410_12_2_480.jpg
cs-410_12_2_112,cs-410,12,2,Opinion,"00:08:18,490","00:08:20,998",112,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=498,So given text reviews,pic_cs-410_12_2_480.jpg
cs-410_12_2_113,cs-410,12,2,Opinion,"00:08:20,998","00:08:24,503",113,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=500,the method allows even further,pic_cs-410_12_2_480.jpg
cs-410_12_2_114,cs-410,12,2,Opinion,"00:08:24,503","00:08:26,781",114,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=504,"And it also allows us to infer,",pic_cs-410_12_2_480.jpg
cs-410_12_2_115,cs-410,12,2,Opinion,"00:08:26,781","00:08:30,638",115,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=506,the viewers laying their,pic_cs-410_12_2_480.jpg
cs-410_12_2_116,cs-410,12,2,Opinion,"00:08:30,638","00:08:35,740",116,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=510,which aspects are more important to,pic_cs-410_12_2_480.jpg
cs-410_12_2_117,cs-410,12,2,Opinion,"00:08:35,740","00:08:39,140",117,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=515,And this enables a lot of,pic_cs-410_12_2_480.jpg
cs-410_12_2_118,cs-410,12,2,Opinion,"00:08:41,330","00:08:46,260",118,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=521,"Finally, in the discussion of prediction,",pic_cs-410_12_2_480.jpg
cs-410_12_2_119,cs-410,12,2,Opinion,"00:08:46,260","00:08:50,340",119,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=526,"of text and non text data, as they",pic_cs-410_12_2_480.jpg
cs-410_12_2_120,cs-410,12,2,Opinion,"00:08:51,960","00:08:57,070",120,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=531,We particularly talked about how text data,pic_cs-410_12_2_480.jpg
cs-410_12_2_121,cs-410,12,2,Opinion,"00:08:58,100","00:09:01,863",121,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=538,In the case of using non-text,pic_cs-410_12_2_480.jpg
cs-410_12_2_122,cs-410,12,2,Opinion,"00:09:01,863","00:09:04,565",122,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=541,we talked about,pic_cs-410_12_2_540.jpg
cs-410_12_2_123,cs-410,12,2,Opinion,"00:09:04,565","00:09:08,921",123,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=544,We introduced the contextual PLSA as a,pic_cs-410_12_2_540.jpg
cs-410_12_2_124,cs-410,12,2,Opinion,"00:09:08,921","00:09:13,354",124,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=548,to allows us to incorporate the context,pic_cs-410_12_2_540.jpg
cs-410_12_2_125,cs-410,12,2,Opinion,"00:09:13,354","00:09:18,328",125,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=553,And this is a general way to allow us,pic_cs-410_12_2_540.jpg
cs-410_12_2_126,cs-410,12,2,Opinion,"00:09:18,328","00:09:20,248",126,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=558,of patterns in text data.,pic_cs-410_12_2_540.jpg
cs-410_12_2_127,cs-410,12,2,Opinion,"00:09:20,248","00:09:24,750",127,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=560,"We also introduced the net PLSA,",pic_cs-410_12_2_540.jpg
cs-410_12_2_128,cs-410,12,2,Opinion,"00:09:24,750","00:09:30,550",128,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=564,network in general of text,pic_cs-410_12_2_540.jpg
cs-410_12_2_129,cs-410,12,2,Opinion,"00:09:31,950","00:09:36,520",129,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=571,And finally we talk about how,pic_cs-410_12_2_540.jpg
cs-410_12_2_130,cs-410,12,2,Opinion,"00:09:36,520","00:09:40,560",130,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=576,mine potentially causal,pic_cs-410_12_2_540.jpg
cs-410_12_2_131,cs-410,12,2,Opinion,"00:09:43,110","00:09:46,560",131,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=583,"Now, in the other way of using text to",pic_cs-410_12_2_540.jpg
cs-410_12_2_132,cs-410,12,2,Opinion,"00:09:47,990","00:09:51,470",132,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=587,help interpret patterns,pic_cs-410_12_2_540.jpg
cs-410_12_2_133,cs-410,12,2,Opinion,"00:09:51,470","00:09:57,300",133,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=591,we did not really discuss anything in,pic_cs-410_12_2_540.jpg
cs-410_12_2_134,cs-410,12,2,Opinion,"00:09:57,300","00:10:02,670",134,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=597,I should stress that that's after a very,pic_cs-410_12_2_540.jpg
cs-410_12_2_135,cs-410,12,2,Opinion,"00:10:02,670","00:10:06,700",135,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=602,if you want to build a practical,pic_cs-410_12_2_600.jpg
cs-410_12_2_136,cs-410,12,2,Opinion,"00:10:06,700","00:10:10,730",136,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=606,because understanding and,pic_cs-410_12_2_600.jpg
cs-410_12_2_137,cs-410,12,2,Opinion,"00:10:13,870","00:10:18,560",137,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=613,So this is a summary of the key,pic_cs-410_12_2_600.jpg
cs-410_12_2_138,cs-410,12,2,Opinion,"00:10:18,560","00:10:22,710",138,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=618,I hope these will be very,pic_cs-410_12_2_600.jpg
cs-410_12_2_139,cs-410,12,2,Opinion,"00:10:22,710","00:10:27,010",139,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=622,text mining applications or to you for,pic_cs-410_12_2_600.jpg
cs-410_12_2_140,cs-410,12,2,Opinion,"00:10:27,010","00:10:31,100",140,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=627,And this should provide a good basis for,pic_cs-410_12_2_600.jpg
cs-410_12_2_141,cs-410,12,2,Opinion,"00:10:31,100","00:10:33,580",141,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=631,to know more about more of allowance for,pic_cs-410_12_2_600.jpg
cs-410_12_2_142,cs-410,12,2,Opinion,"00:10:33,580","00:10:37,320",142,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=633,other organisms or,pic_cs-410_12_2_600.jpg
cs-410_12_2_143,cs-410,12,2,Opinion,"00:10:40,320","00:10:43,760",143,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=640,"So to know more about this topic,",pic_cs-410_12_2_600.jpg
cs-410_12_2_144,cs-410,12,2,Opinion,"00:10:43,760","00:10:47,519",144,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=643,I would suggest you to look,pic_cs-410_12_2_600.jpg
cs-410_12_2_145,cs-410,12,2,Opinion,"00:10:48,550","00:10:51,820",145,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=648,And during this short period,pic_cs-410_12_2_600.jpg
cs-410_12_2_146,cs-410,12,2,Opinion,"00:10:51,820","00:10:57,830",146,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=651,"we could only touch the basic concepts,",pic_cs-410_12_2_600.jpg
cs-410_12_2_147,cs-410,12,2,Opinion,"00:10:57,830","00:11:03,390",147,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=657,we emphasize the coverage,pic_cs-410_12_2_600.jpg
cs-410_12_2_148,cs-410,12,2,Opinion,"00:11:03,390","00:11:09,128",148,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=663,And this is after the cost,pic_cs-410_12_2_660.jpg
cs-410_12_2_149,cs-410,12,2,Opinion,"00:11:09,128","00:11:15,062",149,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=669,in many cases we omit the discussion,pic_cs-410_12_2_660.jpg
cs-410_12_2_150,cs-410,12,2,Opinion,"00:11:15,062","00:11:19,240",150,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=675,So to learn more about the subject,pic_cs-410_12_2_660.jpg
cs-410_12_2_151,cs-410,12,2,Opinion,"00:11:19,240","00:11:22,120",151,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=679,about the natural language process,pic_cs-410_12_2_660.jpg
cs-410_12_2_152,cs-410,12,2,Opinion,"00:11:22,120","00:11:24,200",152,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=682,all text based applications.,pic_cs-410_12_2_660.jpg
cs-410_12_2_153,cs-410,12,2,Opinion,"00:11:24,200","00:11:28,790",153,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=684,"The more NLP you can do, the better",pic_cs-410_12_2_660.jpg
cs-410_12_2_154,cs-410,12,2,Opinion,"00:11:28,790","00:11:32,520",154,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=688,then the deeper knowledge,pic_cs-410_12_2_660.jpg
cs-410_12_2_155,cs-410,12,2,Opinion,"00:11:32,520","00:11:34,010",155,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=692,So this is very important.,pic_cs-410_12_2_660.jpg
cs-410_12_2_156,cs-410,12,2,Opinion,"00:11:37,010","00:11:39,910",156,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=697,The second area you should look into,pic_cs-410_12_2_660.jpg
cs-410_12_2_157,cs-410,12,2,Opinion,"00:11:41,120","00:11:45,090",157,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=701,And these techniques are now,pic_cs-410_12_2_660.jpg
cs-410_12_2_158,cs-410,12,2,Opinion,"00:11:46,160","00:11:49,970",158,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=706,not just text analysis applications but,pic_cs-410_12_2_660.jpg
cs-410_12_2_159,cs-410,12,2,Opinion,"00:11:49,970","00:11:55,310",159,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=709,A lot of NLP techniques are nowadays,pic_cs-410_12_2_660.jpg
cs-410_12_2_160,cs-410,12,2,Opinion,"00:11:56,900","00:12:00,790",160,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=716,"So, they are very important",pic_cs-410_12_2_660.jpg
cs-410_12_2_161,cs-410,12,2,Opinion,"00:12:00,790","00:12:04,570",161,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=720,to also understanding some,pic_cs-410_12_2_720.jpg
cs-410_12_2_162,cs-410,12,2,Opinion,"00:12:04,570","00:12:08,220",162,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=724,naturally they will provide more tools for,pic_cs-410_12_2_720.jpg
cs-410_12_2_163,cs-410,12,2,Opinion,"00:12:09,770","00:12:13,930",163,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=729,"Now, a particularly interesting area,",pic_cs-410_12_2_720.jpg
cs-410_12_2_164,cs-410,12,2,Opinion,"00:12:13,930","00:12:17,640",164,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=733,called deep learning has attracted,pic_cs-410_12_2_720.jpg
cs-410_12_2_165,cs-410,12,2,Opinion,"00:12:17,640","00:12:21,110",165,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=737,It has also shown promise,pic_cs-410_12_2_720.jpg
cs-410_12_2_166,cs-410,12,2,Opinion,"00:12:21,110","00:12:26,660",166,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=741,"especially in speech and vision, and",pic_cs-410_12_2_720.jpg
cs-410_12_2_167,cs-410,12,2,Opinion,"00:12:26,660","00:12:30,820",167,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=746,"So, for example, recently there has",pic_cs-410_12_2_720.jpg
cs-410_12_2_168,cs-410,12,2,Opinion,"00:12:30,820","00:12:34,330",168,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=750,segment analysis to,pic_cs-410_12_2_720.jpg
cs-410_12_2_169,cs-410,12,2,Opinion,"00:12:34,330","00:12:38,320",169,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=754,So that's one example of [INAUDIBLE],pic_cs-410_12_2_720.jpg
cs-410_12_2_170,cs-410,12,2,Opinion,"00:12:38,320","00:12:40,050",170,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=758,but that's also very important.,pic_cs-410_12_2_720.jpg
cs-410_12_2_171,cs-410,12,2,Opinion,"00:12:41,390","00:12:45,400",171,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=761,And the other area that has emerged,pic_cs-410_12_2_720.jpg
cs-410_12_2_172,cs-410,12,2,Opinion,"00:12:45,400","00:12:50,720",172,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=765,"baring technique, where they can",pic_cs-410_12_2_720.jpg
cs-410_12_2_173,cs-410,12,2,Opinion,"00:12:50,720","00:12:55,210",173,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=770,And then these better recognitions will,pic_cs-410_12_2_720.jpg
cs-410_12_2_174,cs-410,12,2,Opinion,"00:12:55,210","00:12:55,820",174,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=775,"As you can see,",pic_cs-410_12_2_720.jpg
cs-410_12_2_175,cs-410,12,2,Opinion,"00:12:55,820","00:13:01,230",175,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=775,this provides directly a way to discover,pic_cs-410_12_2_720.jpg
cs-410_12_2_176,cs-410,12,2,Opinion,"00:13:01,230","00:13:06,600",176,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=781,"And results that people have got,",pic_cs-410_12_2_780.jpg
cs-410_12_2_177,cs-410,12,2,Opinion,"00:13:06,600","00:13:10,360",177,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=786,That's another promising technique,pic_cs-410_12_2_780.jpg
cs-410_12_2_178,cs-410,12,2,Opinion,"00:13:12,510","00:13:16,290",178,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=792,"but, of course,",pic_cs-410_12_2_780.jpg
cs-410_12_2_179,cs-410,12,2,Opinion,"00:13:16,290","00:13:20,970",179,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=796,would lead to practical useful techniques,pic_cs-410_12_2_780.jpg
cs-410_12_2_180,cs-410,12,2,Opinion,"00:13:20,970","00:13:25,172",180,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=800,technologies is still an open,pic_cs-410_12_2_780.jpg
cs-410_12_2_181,cs-410,12,2,Opinion,"00:13:25,172","00:13:28,000",181,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=805,And no serious evaluation,pic_cs-410_12_2_780.jpg
cs-410_12_2_182,cs-410,12,2,Opinion,"00:13:28,000","00:13:32,310",182,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=808,"In, for example, examining",pic_cs-410_12_2_780.jpg
cs-410_12_2_183,cs-410,12,2,Opinion,"00:13:32,310","00:13:34,990",183,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=812,other than word similarity and,pic_cs-410_12_2_780.jpg
cs-410_12_2_184,cs-410,12,2,Opinion,"00:13:36,710","00:13:39,650",184,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=816,"But nevertheless,",pic_cs-410_12_2_780.jpg
cs-410_12_2_185,cs-410,12,2,Opinion,"00:13:39,650","00:13:43,520",185,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=819,that surely will make impact,pic_cs-410_12_2_780.jpg
cs-410_12_2_186,cs-410,12,2,Opinion,"00:13:43,520","00:13:46,860",186,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=823,So its very important to,pic_cs-410_12_2_780.jpg
cs-410_12_2_187,cs-410,12,2,Opinion,"00:13:46,860","00:13:50,780",187,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=826,Statistical learning is also the key to,pic_cs-410_12_2_780.jpg
cs-410_12_2_188,cs-410,12,2,Opinion,"00:13:50,780","00:13:55,180",188,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=830,for many big data applications and we did,pic_cs-410_12_2_780.jpg
cs-410_12_2_189,cs-410,12,2,Opinion,"00:13:55,180","00:13:59,994",189,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=835,component but this is mostly about,pic_cs-410_12_2_780.jpg
cs-410_12_2_190,cs-410,12,2,Opinion,"00:13:59,994","00:14:05,050",190,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=839,techniques and this is another reason,pic_cs-410_12_2_780.jpg
cs-410_12_2_191,cs-410,12,2,Opinion,"00:14:07,350","00:14:11,730",191,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=847,We also suggest that you learn more about,pic_cs-410_12_2_840.jpg
cs-410_12_2_192,cs-410,12,2,Opinion,"00:14:11,730","00:14:16,610",192,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=851,general data mining algorithms can always,pic_cs-410_12_2_840.jpg
cs-410_12_2_193,cs-410,12,2,Opinion,"00:14:16,610","00:14:21,660",193,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=856,regarded as as special,pic_cs-410_12_2_840.jpg
cs-410_12_2_194,cs-410,12,2,Opinion,"00:14:23,520","00:14:26,030",194,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=863,So there are many applications,pic_cs-410_12_2_840.jpg
cs-410_12_2_195,cs-410,12,2,Opinion,"00:14:26,030","00:14:30,510",195,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=866,"In particular for example, pattern",pic_cs-410_12_2_840.jpg
cs-410_12_2_196,cs-410,12,2,Opinion,"00:14:30,510","00:14:35,860",196,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=870,the interesting features for test analysis,pic_cs-410_12_2_840.jpg
cs-410_12_2_197,cs-410,12,2,Opinion,"00:14:35,860","00:14:40,940",197,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=875,that mining techniques can also be used,pic_cs-410_12_2_840.jpg
cs-410_12_2_198,cs-410,12,2,Opinion,"00:14:42,360","00:14:44,980",198,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=882,So these are all good to know.,pic_cs-410_12_2_840.jpg
cs-410_12_2_199,cs-410,12,2,Opinion,"00:14:44,980","00:14:49,050",199,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=884,In order to develop effective,pic_cs-410_12_2_840.jpg
cs-410_12_2_200,cs-410,12,2,Opinion,"00:14:49,050","00:14:52,860",200,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=889,"And finally, we also recommend you to",pic_cs-410_12_2_840.jpg
cs-410_12_2_201,cs-410,12,2,Opinion,"00:14:52,860","00:14:55,930",201,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=892,"information retrieval, of search engines.",pic_cs-410_12_2_840.jpg
cs-410_12_2_202,cs-410,12,2,Opinion,"00:14:55,930","00:15:00,403",202,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=895,This is especially important if you,pic_cs-410_12_2_840.jpg
cs-410_12_2_203,cs-410,12,2,Opinion,"00:15:00,403","00:15:02,750",203,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=900,application systems.,pic_cs-410_12_2_900.jpg
cs-410_12_2_204,cs-410,12,2,Opinion,"00:15:02,750","00:15:05,950",204,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=902,And a search ending would,pic_cs-410_12_2_900.jpg
cs-410_12_2_205,cs-410,12,2,Opinion,"00:15:05,950","00:15:08,632",205,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=905,component in any text-based applications.,pic_cs-410_12_2_900.jpg
cs-410_12_2_206,cs-410,12,2,Opinion,"00:15:08,632","00:15:13,910",206,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=908,And that's because texts data,pic_cs-410_12_2_900.jpg
cs-410_12_2_207,cs-410,12,2,Opinion,"00:15:13,910","00:15:19,330",207,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=913,So humans are at the best position,pic_cs-410_12_2_900.jpg
cs-410_12_2_208,cs-410,12,2,Opinion,"00:15:19,330","00:15:24,910",208,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=919,it's important to have human in the loop,pic_cs-410_12_2_900.jpg
cs-410_12_2_209,cs-410,12,2,Opinion,"00:15:24,910","00:15:29,870",209,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=924,it can in particular help text,pic_cs-410_12_2_900.jpg
cs-410_12_2_210,cs-410,12,2,Opinion,"00:15:29,870","00:15:35,099",210,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=929,One is through effectively reduce,pic_cs-410_12_2_900.jpg
cs-410_12_2_211,cs-410,12,2,Opinion,"00:15:35,099","00:15:40,158",211,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=935,a small collection with the most,pic_cs-410_12_2_900.jpg
cs-410_12_2_212,cs-410,12,2,Opinion,"00:15:40,158","00:15:42,627",212,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=940,the particular interpretation.,pic_cs-410_12_2_900.jpg
cs-410_12_2_213,cs-410,12,2,Opinion,"00:15:42,627","00:15:47,901",213,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=942,So the other is to provide a way to,pic_cs-410_12_2_900.jpg
cs-410_12_2_214,cs-410,12,2,Opinion,"00:15:47,901","00:15:51,521",214,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=947,and this has to do with,pic_cs-410_12_2_900.jpg
cs-410_12_2_215,cs-410,12,2,Opinion,"00:15:51,521","00:15:54,853",215,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=951,"Once we discover some knowledge,",pic_cs-410_12_2_900.jpg
cs-410_12_2_216,cs-410,12,2,Opinion,"00:15:54,853","00:15:57,370",216,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=954,not the discovery is really reliable.,pic_cs-410_12_2_900.jpg
cs-410_12_2_217,cs-410,12,2,Opinion,"00:15:57,370","00:16:00,000",217,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=957,So we need to go back to,pic_cs-410_12_2_900.jpg
cs-410_12_2_218,cs-410,12,2,Opinion,"00:16:00,000","00:16:02,380",218,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=960,And that is why the search,pic_cs-410_12_2_960.jpg
cs-410_12_2_219,cs-410,12,2,Opinion,"00:16:04,070","00:16:08,040",219,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=964,"Moreover, some techniques",pic_cs-410_12_2_960.jpg
cs-410_12_2_220,cs-410,12,2,Opinion,"00:16:08,040","00:16:13,380",220,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=968,"for example BM25, vector space and",pic_cs-410_12_2_960.jpg
cs-410_12_2_221,cs-410,12,2,Opinion,"00:16:13,380","00:16:16,400",221,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=973,"We only mention some of them,",pic_cs-410_12_2_960.jpg
cs-410_12_2_222,cs-410,12,2,Opinion,"00:16:16,400","00:16:20,500",222,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=976,text retrieval you'll see that there,pic_cs-410_12_2_960.jpg
cs-410_12_2_223,cs-410,12,2,Opinion,"00:16:20,500","00:16:25,030",223,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=980,Another technique that it's used for,pic_cs-410_12_2_960.jpg
cs-410_12_2_224,cs-410,12,2,Opinion,"00:16:25,030","00:16:28,450",224,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=985,response of search engine to a user's,pic_cs-410_12_2_960.jpg
cs-410_12_2_225,cs-410,12,2,Opinion,"00:16:28,450","00:16:32,150",225,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=988,very useful for building efficient,pic_cs-410_12_2_960.jpg
cs-410_12_2_226,cs-410,12,2,Opinion,"00:16:35,160","00:16:39,830",226,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=995,"So, finally, I want to remind",pic_cs-410_12_2_960.jpg
cs-410_12_2_227,cs-410,12,2,Opinion,"00:16:39,830","00:16:43,900",227,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=999,harnessing big text data that I showed,pic_cs-410_12_2_960.jpg
cs-410_12_2_228,cs-410,12,2,Opinion,"00:16:45,350","00:16:48,970",228,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1005,"So in general, to deal with",pic_cs-410_12_2_960.jpg
cs-410_12_2_229,cs-410,12,2,Opinion,"00:16:48,970","00:16:51,760",229,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1008,"we need two kinds text,",pic_cs-410_12_2_960.jpg
cs-410_12_2_230,cs-410,12,2,Opinion,"00:16:53,380","00:16:58,040",230,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1013,"And text retrieval, as I explained,",pic_cs-410_12_2_960.jpg
cs-410_12_2_231,cs-410,12,2,Opinion,"00:16:58,040","00:17:02,930",231,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1018,a small amount of most relevant data for,pic_cs-410_12_2_960.jpg
cs-410_12_2_232,cs-410,12,2,Opinion,"00:17:02,930","00:17:07,240",232,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1022,"providing knowledge provenance,",pic_cs-410_12_2_1020.jpg
cs-410_12_2_233,cs-410,12,2,Opinion,"00:17:07,240","00:17:12,060",233,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1027,Text mining has to do with further,pic_cs-410_12_2_1020.jpg
cs-410_12_2_234,cs-410,12,2,Opinion,"00:17:12,060","00:17:16,460",234,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1032,the actionable knowledge that can be,pic_cs-410_12_2_1020.jpg
cs-410_12_2_235,cs-410,12,2,Opinion,"00:17:16,460","00:17:18,510",235,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1036,many other tasks.,pic_cs-410_12_2_1020.jpg
cs-410_12_2_236,cs-410,12,2,Opinion,"00:17:18,510","00:17:20,530",236,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1038,So this course covers text mining.,pic_cs-410_12_2_1020.jpg
cs-410_12_2_237,cs-410,12,2,Opinion,"00:17:20,530","00:17:24,020",237,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1040,And there's a companion course,pic_cs-410_12_2_1020.jpg
cs-410_12_2_238,cs-410,12,2,Opinion,"00:17:24,020","00:17:27,130",238,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1044,Search Engines that covers text retrieval.,pic_cs-410_12_2_1020.jpg
cs-410_12_2_239,cs-410,12,2,Opinion,"00:17:27,130","00:17:32,040",239,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1047,"If you haven't taken that course,",pic_cs-410_12_2_1020.jpg
cs-410_12_2_240,cs-410,12,2,Opinion,"00:17:32,040","00:17:37,490",240,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1052,especially if you are interested,pic_cs-410_12_2_1020.jpg
cs-410_12_2_241,cs-410,12,2,Opinion,"00:17:37,490","00:17:42,138",241,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1057,And taking both courses will give you,pic_cs-410_12_2_1020.jpg
cs-410_12_2_242,cs-410,12,2,Opinion,"00:17:42,138","00:17:43,708",242,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1062,building such a system.,pic_cs-410_12_2_1020.jpg
cs-410_12_2_243,cs-410,12,2,Opinion,"00:17:43,708","00:17:49,250",243,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1063,So in [INAUDIBLE],pic_cs-410_12_2_1020.jpg
cs-410_12_2_244,cs-410,12,2,Opinion,"00:17:49,250","00:17:51,050",244,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1069,taking this course.,pic_cs-410_12_2_1020.jpg
cs-410_12_2_245,cs-410,12,2,Opinion,"00:17:51,050","00:17:57,915",245,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1071,I hope you have learned useful knowledge,pic_cs-410_12_2_1020.jpg
cs-410_12_2_246,cs-410,12,2,Opinion,"00:17:57,915","00:18:02,185",246,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1077,As you see from our discussions,pic_cs-410_12_2_1020.jpg
cs-410_12_2_247,cs-410,12,2,Opinion,"00:18:02,185","00:18:06,235",247,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1082,this kind of techniques and,pic_cs-410_12_2_1080.jpg
cs-410_12_2_248,cs-410,12,2,Opinion,"00:18:06,235","00:18:10,910",248,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1086,So I hope you can use what you have,pic_cs-410_12_2_1080.jpg
cs-410_12_2_249,cs-410,12,2,Opinion,"00:18:10,910","00:18:15,550",249,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1090,applications will benefit society and,pic_cs-410_12_2_1080.jpg
cs-410_12_2_250,cs-410,12,2,Opinion,"00:18:15,550","00:18:20,759",250,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1095,the research community to discover new,pic_cs-410_12_2_1080.jpg
cs-410_12_2_251,cs-410,12,2,Opinion,"00:18:20,759","00:18:21,259",251,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1100,Thank you.,pic_cs-410_12_2_1080.jpg
cs-410_12_2_252,cs-410,12,2,Opinion,"00:18:21,259","00:18:31,259",252,https://www.coursera.org/learn/cs-410/lecture/OxeOx?t=1101,[MUSIC],pic_cs-410_12_2_1080.jpg
