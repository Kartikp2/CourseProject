[SOUND]
Hello welcome to CS410 DSO Text Information Systems.
>> This lecture is about Natural Language
This is an online course offered by University of Illinois at Urbana-Champaign.
of Content Analysis.
My name is ChengXiang Zhai.
As you see from this picture,
I also have a nickname, Cheng.
this is really the first step
I'm a professor of Computer Science at the University of Illinois at Urbana-Champaign.
Text data are in natural languages.
I'm the instructor of this course.
So computers have to understand
This first lecture is just an introduction to the course.
in order to make use of the data.
Let's first to start with some motivation.
So that's the topic of this lecture.
The problem we are trying to address in this course is how to harness big text data.
We're going to cover three things.
Text data is all kinds of data in
First, what is natural
the form of natural languages such as English and Chinese.
which is the main technique for processing
Now, this kind of data is everywhere and also growing very quickly.
The second is the state of
For example, you can find all kinds of
natural language processing.
web pages on the Internet and the number of pages is growing quickly.
Finally we're going to cover the relation
You can also find the blogs articles, news articles,
text retrieval.
or Emails and other kind of documents and enterprise environment.
First, what is NLP?
Of course, we will also have a lot of scientific literature in text form.
Well the best way to explain it
And nowadays, social media has been growing quickly.
a text in a foreign language
So we now see, tweets and other social media data also in the form of text.
Now what do you have to do in
All such text data encode a lot of useful knowledge about the world
This is basically what
because it's in some sense the data reported by human census about the observe the world.
So looking at the simple sentence like
So we can analyzed this kind of data to discover a lot of useful knowledge.
We don't have any problems
Especially the knowledge about the human opinions or preferences.
But imagine what the computer would
So this kind of data is very useful and we can use
Well in general,
computational methods to turn such data into useful knowledge,
First, it would have to know dog
which can then be further used in many applications.
So this is called lexical analysis,
The main techniques for making this happen include the Text Retrieval and Text Mining.
we need to figure out the syntactic
And these are the main techniques that we will cover in this course.
So that's the first step.
Logically, in order to make use a lot of text data.
After that, we're going to figure
We would first do text retrieval,
So for example, here it shows that A and
and that's due to a large set of text data
the dog would go together
into a smaller but much more relevant set of data,
And we won't have dog and is to go first.
that we actually need for a particular problem.
And there are some structures
And this step, is usually implemented by
But this structure shows what we might
using text retrieval techniques that involve humans in
try to interpret the sentence.
the loop to find and locate the most relevant documents to a particular problem.
Some words would go together first, and
Once we find the relevant documents,
then they will go together
the next step is to do text mining,
So here we show we have noun phrases
which is to further analyze the found of relevant documents to
then verbal phrases.
discover useful knowledge to extract
Finally we have a sentence.
the knowledge that can be directly used in application,
And you get this structure.
especially in a application such as decision making.
We need to do something called
These two steps corresponding to Text Retrieval and Text Mining Techniques,
And we may have a parser
that we will cover in this course.
that would automatically
Based on this picture that I show you,
At this point you would know
this course is designed to leverage to corresponding books that I've offered on Coursera,
still you don't know
which cover Text Retrieval and Text Mining respectively.
So we have to go further
The first book is called,
In our mind we usually can map
Text Retrieval and Search Engines.
such a sentence to what we already
The second book is called,
For example, you might imagine
the Text Mining and Analytics.
There's a boy and
These two books have
But for a computer would have
comprehensive lecture videos that cover
We'd use a symbol (d1) to denote a dog.
basic concepts and principles and
And (b)1 can denote a boy and
methods for text retrieval and text mining respectively.
Now there is also a chasing
So, this online course,
we have a relationship chasing
will leverage all those lecture videos and also
So this is how a computer would obtain
the online quizzes that are provided through the Coursera platform.
Now from this representation we could
But in addition to that,
and we might indeed naturally think of
we also need to add additional components in order to explore applications because
this is called inference.
those books have covered a general techniques without a necessary discussing in depths
So for example, if you believe
how those techniques are using applications.
this person might be scared,
So, to make the coverage more complete in CS410,
you can see computers could also
we were adding two additional components as shown here.
So this is some extra knowledge
That is of course Products and Technology Review.
some understanding of the text.
Both are meant to give the students freedom to choose a particular way to apply some of
You can even go further to understand
the techniques that you have learned in those two books to solve
So this has to do as a use of language.
a real world problem or to further learn about a particular topic.
This is called pragmatic analysis.
In the project, you will have opportunities to apply the knowledge that you have
In order to understand the speak
learned in those two books to solve a real world problem in an integrated manner.
We say something to
In technical review, you can leverage what you have
There's some purpose there.
learned to learn even more about either tool kit
And this has to do with
or a particular technology that can be
In this case the person who said
used to extend your knowledge about how to solve a problem.
this sentence might be reminding
The format of the course thus is a mixture of online videos plus
That could be one possible intent.
some high engagement module which mostly consists of
To reach this level of
our interactions through forums as I will explain later.
all of these steps and
And also, we will have a lot of interactions to help you
these steps in order to completely
finish the course Project and Technology Review.
Yet we humans have no trouble
There are a number of goals that we
we instantly would get everything.
have kept in mind when designing this course and I want to share with
There is a reason for that.
you these goals because they would help you understand
That's because we have a large
why the course has this particular format and
we can use common sense knowledge
why you are asked to do certain components of the tasks.
Computers unfortunately are hard
The first goal is we want to emphasize both theory and practice.
They don't have such a knowledge base.
Theory is obviously very important because the basic concepts and
They are still incapable of doing
general principles covering the theoretical part would be applicable to all applications.
so that makes natural language
That means, they are not just used for
But the fundamental reason why natural
today's applications but they can be used for solving future problems as well so,
computers is simply because natural
they are more general.
computers.
Therefore, they have long lasting impact and utility value.
Natural languages are designed for
And this is achieved by having you to watch all the lecture videos,
There are other languages designed for
and then to use quizzes and exams to make sure
For example, programming languages.
that you have mastered all the basic ideas,
Those are harder for us, right?
basic concepts and general principles and those general methods.
So natural languages is designed to
Practical skills are also very important,
As a result,
and because of specific practical skills can be immediately useful for solving
because we assume everyone
the problems today and maybe you will be able to
We also keep a lot of ambiguities because
resolve the problems that you encountered in your job.
know how to decipher an ambiguous word
And this is achieved by having you to do Programming assignments,
There's no need to demand different
where you would be able to use existing tool kits,
We could overload the same word with
to look into some algorithms in depths and to use some of
Because of these reasons this makes every
the algorithms to have some sense about how they work or how to improve them.
difficult for computers,
And finally, of course it's very important to integrate the theory and practice.
And common sense and reasoning is
And this will be done by,
So let me give you some
having you to work on Course projects.
Consider the word level ambiguity.
The second goal is to Personalized Learning,
The same word can have
because every one of you has a different in need and different in preference,
For example design can be a noun or
and you will have a different in schedules as well,
The word of root may
because many of you are full time workers, perhaps.
So square root in math sense or
And so, Personalized Learning is very important to ensure
You might be able to think
everyone to receive the best education.
There are also syntactical ambiguities.
So to achieve this goal,
For example, the main topic of this
we have intentionally designed
can actually be interpreted in two
the deadlines and the format of the course to be flexible so that,
Think for a moment and
you can have a lot of freedom to do self-paced learning.
We usually think of this as
For example, you can watch over the lecture videos at anytime that you want to.
but you could also think of this as do
Watch it, watch that.
So this is an example
And you can also finish all the quizzes pretty much
What we have different is
anytime and you can do the assignment,
applied to the same sequence of words.
program assigns particularly again in a flexible way.
Another common example of an ambiguous
In addition to self paced learning,
A man saw a boy with a telescope.
you would also have a choices of topics for project and the technology review.
Now in this case the question is,
And you will be able to work on a topic that's most interesting to you,
This is called a prepositional
for both course project and technology review.
PP attachment ambiguity.
Finally, we also have the goal of
Now we generally don't have a problem with
collaborative learning because this would maximize the efficiency of learning.
background knowledge to help
Our common goal is to help everyone learn maximum amount of knowledge,
Another example of difficulty
while spending hopefully minimum amount of effort.
So think about the sentence John
So, this is a common goal that we should all work toward,
The question here is does
and we should help each other achieve this goal.
So again this is something that
To promote collaborative learning and to facilitate collaborative learning,
the context to figure out.
we will use forum-based interactions to enable all of us,
Finally, presupposition
including many of you from different time zones,
Consider the sentence,
to interact with each other effectively.
Now this obviously implies
Another way to support a collaborative learning is to have you to work in groups,
So imagine a computer wants to understand
to finish cost projects and technology reviews.
It would have to use a lot of
So, both technology reviews and
It also would have to maintain a large
products can be done by working together with others in a group.
words and how they are connected to our
So, overall the format is as follows: first,
So this is why it's very difficult.
you will have to watch the lecture videos of those two MOOCs,
So as a result, we are steep not perfect,
and this is also shown here on this slide,
in fact far from perfect in understanding
and then you also need to take quizzes.
So this slide sort of gains a simplified
And these quizzes are given in a weekly manner.
We can do part of speech
But as I said, the deadlines are actually flexible.
I showed 97% accuracy here.
So, you should be able to finish quizzes as soon as you
Now this number is obviously
finish the corresponding lecture videos.
don't take this literally.
These quizzes are two.
This just shows that we
There are also two kinds of quizzes.
But it's still not perfect.
There are practice quizzes that you can use to help you understand some concepts.
In terms of parsing,
And there are also test quizzes.
That means we can get noun phrase
Those are of course to make sure that you have indeed mastered the materials.
or some segment of the sentence, and
Then there are two exams.
this dude correct them in
Those two exams will be given at the end of each MOOC corresponding.
And in some evaluation results,
So, the first exam will be given at the end of the first MOOC on Test Retrieval,
accuracy in terms of partial
and the second one will be given at the end of the second MOOC on Test Mining.
Again, I have to say these numbers
And then, during this period of watching these videos,
In some other datasets,
you will be also working on programming assignments.
Most of the existing work has been
Then at the end of the semester,
And so a lot of these numbers are more or
we'll leave about two weeks time for you to work intensively on the course project,
Think about social media data,
although you will be working on the course project throughout the semester.
In terms of a semantical analysis,
But most of the time will be in the end of the semester.
we are far from being able to do
Now, the course project will be down sequentially by following multiple steps.
But we have some techniques
The first is for you to select a topic.
do partial understanding of the sentence.
You can select the topic from a list of topics that
So I could mention some of them.
we provide or you can propose your topic,
For example, we have techniques that can
and then we'll ask you to submit
relations mentioned in text articles.
a short proposal to be more specific about what you want to work on.
For example,
And then, in the middle of the semester,
locations, organizations, etc in text.
you will be asked to provide
So this is called entity extraction.
a short progress report so that we can check
We may be able to recognize the relations.
your progress and we can provide help in a timely manner.
For example,
And then, at the end of the semester,
this person met that person or
you will deliver two things: one is a software and its documentation.
Such relations can be extracted by using
You upload that to a public website so that it's available to people.
the computer current
The second one is a tutorial presentation,
They're not perfect but
a short presentation to explain how your software can be used by people.
Some entities are harder than others.
There is also a technology review component which is mandatory and
We can also do word sense
this component will be correlated based on completion.
We have to figure out whether this word in
Everyone is required to finish this component.
in another context the computer could
Now, this component is designed to give you
Again, it's not perfect, but
an opportunity to explore further any topic that you're interested in.
We can also do sentiment analysis,
This can be in that examination of a tool kit that you are interested in,
meaning, to figure out whether
perhaps a tool that you have used in the course project,
This is especially useful for
or a comparison of multiple tools that can do similar things,
So these are examples
and you might have looked into multiple tools,
And they help us to obtain partial
and you have some opportunity to compare them and to figure out which one is the best.
It's not giving us a complete
Such a writing would be helpful for others to understand how to
this sentence.
use a tool kit or which one to choose.
But it would still help us gain
Of course, those of you who are interested in the methods,
And these can be useful.
you can also go in-depth to examine a certain type
In terms of inference,
of methods and write a brief review of them,
probably because of the general difficulty
or looking to a cutting edge topic in your research and write
This is a general challenge
a review of the recent papers about the topic.
Now that's probably also because
Now, we hope that you might be able to align
representation for
the technology review with your course project so that the two will
So this is hard.
be kind of synergistic in that technology review will help
Yet in some domains perhaps,
you learn more about the topic related to your course project,
restrictions on the word uses, you may be
while the course project will give you also a motivation for doing
But in general we can not
an in-depth study of the topic via technology review.
Speech act analysis is also
Of course, we will try to help you finish all these tasks,
we can only do that analysis for
and we'll do our best to help you.
So this roughly gives you some
That means, the TAs and I will interact with you in various ways to help you,
And then we also talk a little
and one way is a synchronous question answering and discussing via forums.
and so we can't even do 100%
And the other way is to have
Now this looks like a simple task, but
synchronous weekly office hours and that will be down by using video teleconferencing.
think about the example here,
The grading will be done as follows: 25% of your grade will be based on the quizzes,
have different syntactic categories if you
all the quizzes for the two MOOCs;
It's not that easy to figure
30% will be based on the two exams,
It's also hard to do
those two exams will be proctored exams;
And again, the same sentence
25% will be based on the programming assignments,
This ambiguity can be very hard to
there will be multiple programming assignments throughout the semester.
where you have to use a lot of knowledge
The remaining 20% will be based on your course project and that's for
from the background, in order to figure
the distribution as follows: 5% is for topic selection,
So although the sentence looks very
5% is for proposal,
And in cases when the sentence is
and another 5% for progress report,
five prepositional phrases, and there
65% in most of the grade of the course project will be based on software deposit,
It's also harder to do precise
your deliverable for the project.
So here's an example.
And finally, 20% is based on the tutorial presentation.
In the sentence "John owns a restaurant."
Now, most of these components in the course project will be graded based on completion.
The word own,
Indeed, a lot of tasks you see in programming assignments are graded in
it's very hard to precisely describe
the same way and that is because we believe we have
So as a result we have a robust and
designed the program assignments and course project in such a way
Natural Language Processing techniques
that you will be able to learn a lot by simply going through these tasks.
In a shallow way,
So, this is the effective way of learning by
For example, parts of speech tagging or a
doing and that's why we choose to grade based on completion.
And those are not deep understanding,
That also means you have a lot of control over these grades,
because we're not really understanding
because you can ensure that you finish
On the other hand of the deep
all these tasks and then you will get most of these grades.
up well, meaning that they would
The only part that will be based on the quality of solution is the tutorial presentation.
And if you don't restrict
This is actually not just based on the quality of
the use of words, then these
your tutorial presenting but rather based on whether
They may work well based on machine
you are software actually work as you propose.
that are similar to the training data
So, does it provide all the functions that you propose?
But they generally wouldn't work well on
Does it really work?
the training data.
And this will likely affect some of the grade for tutorial presentation,
So this pretty much summarizes the state
meaning that if a software for some reason doesn't really pass our test,
Of course, within such a short amount
then we would deduct the points from here.
a complete view of NLP,
As I said, the technology review is not actually contributing to you grade,
And I'd expect to see multiple courses on
but it's a metric component and it will be graded based on completion.
But because of its relevance to the topic
We also provide up to 5% of
you to know the background in case
extra credit based on your participation in the forum discussions.
So what does that mean for Text Retrieval?
And this is to encourage you to help
Well, in Text Retrieval we
each other and particularly by answering questions posed by others.
It's very hard to restrict
We will be able to use log data from the forum to give you credit,
And we also are often dealing
extra credit, and these extra credit points will be actually added to the regular points.
So that means The NLP techniques must
So that will allow you to actually increase
And that just implies today we can only
your grade as will be shown here in more detail.
text retrieval.
And this is how we are going to determine your final letter grades.
In fact,
They are based on the grade points you have collected over
most search engines today use something
the semester using this map shown here on the slide.
Now, this is probably the simplest
Mostly it's five points for each bracket but not always.
That is to turn text data
And as you can see,
Meaning we'll keep individual words, but
if you have earned 5% extra credit and when adding this extra credit,
And we'll keep duplicated
will likely help you move your grades up by one bracket,
So this is called a bag
although it won't be more than one bracket.
When you represent text in this way,
So, we feel that this fixed mapping would give you complete control over your grade,
That just makes it harder to understand
so you can monitor your grade over the semester and kind of assess
because we've lost the order.
your progress and also to adjust your schedule accordingly.
But yet this representation tends
So, this is a visualization of your workload.
most search tasks.
Horizontally, we show the timeline,
And this was partly because the search
from the first day of instruction to the last day of instruction.
If you see matching of some of
And then vertically, you can see there are mainly six tasks for you over the semester.
chances are that that document is about
First, you will spend most of your time on watching the lecture videos,
So in comparison of some other tasks, for
and this is why we have a thick black line
example, machine translation would require
there that shows that most of
Otherwise the translation would be wrong.
your effort probably will be spent on watching lecture videos.
So in comparison such tasks
And then, you will be taking 12 quizzes,
Such a representation is often sufficient
and these will be spreading over most of the semester,
the major search engines today,
corresponding to about 12 weeks when you are expected to watch those lecture videos.
Of course, I put in parentheses but
Now, we want to emphasize that you should spend most of your time to
that are not answered well by
watch videos and make sure you understand the materials before you take quizzes,
they do require the replantation that
and this will ensure that you don't leave any holes.
That would require more natural
If you do it the other way and by using quizzes to guide you through the lecture videos,
There was another reason why we
then you might leave some holes,
NLP techniques in modern search engines.
and that might hurt your performance on exams.
And that's because some
There will be two proctored exams that will be
naturally solved the problem of NLP.
given in the middle of the semester and also later in the semester,
So one example is word
at the end of the two box.
Think about a word like Java.
So, this is your third task.
It could mean coffee or
The fourth task is programming assignments,
If you look at the word anome,
and this will be,
when the user uses the word in the query,
again, through the entire semester,
For example, I'm looking for
actually not really all the weeks in the semester,
When I have applet there,
but most of the weeks,
And that contest can help us
because we don't want you to use up the last two weeks,
which Java is referring
which we reserve for you to work on course project.
Because those documents would
And the course project is the fifth component.
If Java occurs in that
And finally, you need to finish technology review.
then you would never match applet or
Now, although most of the work of
So this is the case when
the project is expected to be done at the end of the semester,
naturally achieve the goal of word.
you will actually start working on it from the very beginning.
Another example is some technique called
And, soon after the semester starts,
feedback which we will talk about
we will ask you to think about the topics and you will discuss
This technique would allow us to add
the topics and then form teams to submit the proposal.
those additional words could
But of course, during the first 12 weeks,
And these words can help matching
you will be most occupied by the lecture videos, quizzes,
have not occurred.
and your assignments for programming.
So this achieves, to some extent,
So, that's why you will likely have more time to work on the project in the end.
So those techniques also helped us
The technology review is kind of designed to extend your knowledge based on your project.
bypass some of the difficulties
But of course, you have the complete freedom
However, in the long run we still need
to choose whatever topic that you want to review.
techniques in order to improve the
So, you don't have to tie it to the project.
And it's particularly needed for
And so, we imagine that you would start
Or for question and answering.
working on it a little bit after you have decided the project,
Google has recently launched a knowledge
the topic, so that you can decide whether you want to tie
that goal, because knowledge graph would
the technology review with your project.
And this goes beyond the simple
So, it's good to keep this picture in mind throughout
And such technique should help us
the semester because you might have irregular schedule sometimes.
significantly, although this is the open
For example, you might find that you are very busy in the middle of the semester,
In sum, in this lecture we
then this picture can help you adjust your schedule and the degree you
we've talked about the state
probably want to then work more on some of the tasks at the beginning of the semester,
What we can do, what we cannot do.
so that you don't overwhelm yourself in the middle of the semester.
And finally, we also explain why
And similarly, if you expect to be very busy the end of the semester,
remains the dominant replantation
you might want to start working on the project much earlier.
even though deeper NLP would be needed for
So, I hope this picture will be always in your mind,
If you want to know more, you can take
and you will be able to adjust your schedule accordingly
I only cited one here and
and to particularly work on tasks proactively,
Thanks.
in case you anticipate any busy time period.
[MUSIC]
Some of you have already taken maybe both MOOCs.
Now, if you are one of them,
then I think naturally,
you will have more time to work on other problems in this course.
So, you should take advantage of this
to proactively finish some of the tasks much more quickly.
In particular, since you have already watched those videos and then you can
simply review them and then you try to work on the quizzes
so that you can finish all the quizzes much more quickly.
You may be able to finish most of the programming assignments quickly too.
This is not only going to be helpful for you
in the sense that you will have a lot of time to work on course project, but also,
it would allow you to help others by answering
their questions on forums or helping them in other way,
like teaming up with them to work on the project.
However, I should also say that there are a few tasks that are,
unfortunately, have very fixed time that you cannot really work on in advance.
For example, the two exams will be scheduled on
some particular dates that would have no flexibility for you to work on earlier.
There may be some tasks in the programming assignments that have to be synchronized.
For example, we might run competition of
some tasks and there may be some synchronization that's needed,
but we would like to minimize the dependency so that you
could hopefully work on many of those tasks as early as you can.
And of course, we encourage you to use more time to finish
a more challenging course project or to finish
a higher quality technology review.
Now, we rely a lot on forum discussion,
and this is because we are in different time zones
and it's very hard to find a time that works well for everyone.
But forum has important advantages
of being able to accommodate everyone in the discussion.
So, this will be the primary way of our interactions and engagement,
and in particular, we will be using Piazza,
which is a forum that we have used for many other courses,
and it has proven to be a useful forum with a lot of useful functions.
The second advantage of forum is it would also enable you
to ask your questions as soon as you have a question.
And therefore, we can hopefully accelerate question answering and
so that you can have your question answered quickly on
the forum without waiting until office hour.
Finally, we hope that the forum discussions would help us
identify difficult concepts in those lectures,
so that we can focus on discussing these concepts or
explaining these concepts that are hard to understand in our office hours.
This would make better use of our office hours to help all of you.
Because of these reasons,
so the protocol of question answering will be as follows,
and we want to emphasize that it's important to
follow this protocol so that we can make effective use of our office hours,
so that you can get your questions answered more quickly,
so that you can all help each other in learning.
So, first, as soon as you have a question or issue to discuss,
post it immediately on the forum.
And this has a number of advantages, and first is,
it will give you the opportunity to have the question answered quickly because often,
your question may be answered by your peers or may be answered by a teacher or by me.
So, by posting the question immediately,
you have a better chance of getting the question answered quickly,
so that you won't have to wait.
And secondly, your question might also help others because sometimes
other students may have a similar question or may not
realize that they have encountered this question,
but you just articulated this question.
So, this is helpful for your peers as well.
And discussing of this question is often also a good way to learn.
However, if your question is not answered in a timely manner on the forum,
or addressed adequately from your perspective,
then you should email the question to all of us including me and TAs.
Please use a subject line that contains the keyword CS410DSO all together.
Of course, your subject line,
you can contain other keywords based on your question.
And then, if you don't receive a reply
from us by email in a timely manner, join an office-hour.
Now, we would do our best to reply to your emails but then,
depending on the number of emails,
depending on our own schedule,
we may not be able to always reply to your emails in a timely manner.
Of course, we would do our best to respond quickly but if we can't,
then you should come to one of our office hours.
We would try to schedule our office hours in different time slots during the day.
For example, we would have some time slot in the morning,
or late morning, or early afternoon,
and then another time slot in the evening.
And this is so that,
we can hopefully accommodate different time zones,
because the same slot of time may not be equally convenient with all of you.
So, we'll do our best again,
to diversify the time slots,
both in terms of the time in a day,
and also in terms of the days in a week.
The format of office hours is as follows, and this again,
is based on our need to make effective use and
the efficient use of our limited office hours to help you in the best way.
So, we will hold weekly office hours,
as I said, and we'll publish those time slots.
And the office hours will be given by using video-teleconference,
and particularly using Zoom,
and its used for system that has worked well.
And you can join or leave an office hour at any time and that means you can join late,
or you can leave early,
and it's very flexible.
So, don't feel that you always have to come at the beginning of the office hour.
Feel free to stop by in the last ten minutes,
if that's the best time for you.
And again, by taking advantage of the forum discussion, hopefully,
by the time of going to office hour,
we will only need to deal with some of the relatively difficult questions.
And the priority we will use
is to give the highest priority to any issues that have already been posted on forums,
but have not been resolved even after some email communications.
Those are clearly the toughest issues
because it has been posted on forum without a good answer,
and then was emailed to us and still not satisfactory solved.
So, we would have to give those issues the highest priority,
that means if there are such issues being raised during office hour,
those issues would be given the first priority.
After that, we'll look at the any other unresolved issues on forums.
That's why it's very important for you to post the issue on the forum first,
and only after resolving those issues that have already been posted
on forums would we take
other questions or issues that have not yet been posted on forums.
And of course, you should not
hesitate to bring any questions that you have or any issues you want to discuss.
It's just that, we want to have a policy to prioritize the issues that we want to handle,
so that we can provide the maximum benefit to all of
you by using our office hours efficiently.
Finally, I want to just say something
in general about how to get the most out of this course.
Perhaps, the most important advice is to plan ahead based on your own schedule,
because many of you are very busy.
So, you want to kind of take a look at
the picture that I showed you earlier about the tasks,
and then consider your own schedule.
Try to imagine which periods will be a relatively busy period
for you and identify what tasks you're supposed to work on in that period,
and try to finish those tasks earlier,
so that you don't overlap with yourself during that busy period.
And of course, at any time,
please let us know how we can help, again,
by using the forums as the first step.
And another thing to mention is also to allocate a sufficient time
for the preparation of two proctored exams because they will be given only once.
That means, you only have one chance to take each exam,
so you want to really prepare well for them.
However, those exams are mostly to confirm that you have indeed mastered the materials.
So, they will have similar questions to the quiz questions that you have already seen.
And in fact, some questions may be exactly the same as the questions in the quizzes.
And we do that because this would allow you
to have some sense about what the questions might look like in the exams.
So, they should not be much surprise if you have actually worked on
all the questions in those quizzes and have made
sure that you have understood the answers to those questions.
Of course, if you can,
try to complete the quizzes and program assignments ahead of time.
This would be to your advantage because you can now raise
your questions earlier that would allow you to have
more time to get your questions answered.
Therefore, by the time when you take the quiz,
then you would have all the questions resolved.
And it also would allow you to actively help others and to discuss any of
the problems that you have encountered that would help you earn the extra credit also.
The second advice is to post questions on forum immediately and
whenever you have difficulty understanding any part of the course materials.
And again, I want to emphasize the immediate action of posting any issue,
any question on the forums.
We would do our best to resolve those issues through the forums.
And it's the effective way to also engage the peers to help each other.
So, do not hesitate to post questions and we
won't to penalize you for posting many questions.
In fact, we'll reward that perhaps,
because that's one way to contribute to the forum discussion.
Finally, you should leverage collaborative learning,
this is kind of related to you posting your questions on forums,
and again, to actively participate in the forum discussion.
You will actually learn a lot from reading other people's posts,
even if you know the answers because they are
often opinions expressed about those materials.
So, we do hope that you have active discussion on
forums to help each other and to help each other, particularly,
to save time, to understand the difficult concepts,
to do well in the quizzes and exams,
and to finish assignments smoothly.
And finally, we do encourage you to help each other understand materials too.
So, we encourage you to answer others' questions,
and the system will record those answers.
We'll have a statistics about that,
and then we'll use that to give extra credit.
So, this was just the overall introduction to the course.
For more information, you can visit the course website on Coursera.
We hope you enjoy this course,
and I look forward to working with you. Thank you.
[SOUND]
In this lecture,
In the previous lecture, we talked about
We explained that the state of the are
are still not good enough to process
in a robust manner.
As a result,
bag of words remains very popular in
In this lecture, we're going to talk
help users get access to the text data.
This is also important step to convert
That are actually needed
So the main question we'll address here,
can a text information system, help users
We're going to cover two complimentary
And then we're going to talk about
querying versus browsing.
So first push versus pull.
These are two different ways connect
at the right time.
The difference is which
which party takes the initiative.
In the pull mode,
the users take the initiative to
And in this case, a user typically would
For example,
then browse the results to
So this is usually appropriate for
satisfying a user's ad
An ad hoc information need is
For example, you want to buy a product so
you suddenly have a need to read
But after you have cracked information,
You generally no longer
it's a temporary information need.
In such a case, it's very hard for
it's more proper for
that's why search engines are very useful.
Today because many people have many
So as we're speaking Google is probably
And those are all, or mostly adequate.
Information needs.
So this is a pull mode.
In contrast in the push mode in
to push the information to the user or
So in this case this is usually
Now this would be appropriate if.
The user has a stable information.
For example you may have a research
that interest tends to stay for a while.
So, it's rather stable.
Your hobby is another example of.
A stable information need is such a case
can learn your interest, and
If the system hasn't seen any
the system could then take the initiative
So, for example, a news filter or
news recommended system could
identify interesting news to you and
This mode of information access may be
has good knowledge about the users need
So for example, when you search for
a search engine might infer you might be
Formation.
And they would recommend the information
example, of an advertisement
So this is about the two high level
Now let's look at the pull
In the pull mode, we can further
Querying versus browsing.
In querying,
Typical the keyword query, and
the search engine system would
And this works well when the user knows
So if you know exactly
you tend to know the right keywords.
And then query works very well,
But we also know that sometimes
When you don't know the right
you want to browse information
You use because browsing
So in this case, in the case of browsing,
into the relevant information
supported by the structures of documents.
So the system would maintain
then the user could follow
So this really works well when the user
or the user doesn't know what
Or simply because the user finds it
So even if a user knows what query to
to search for information.
It's still harder to enter the query.
In such a case, again,
The relationship between browsing and
imagine you're site seeing.
Imagine if you're touring a city.
Now if you know the exact
Taking a taxi there is
You can go directly to the site.
But if you don't know the exact address,
Or you can take a taxi to a nearby
It turns out that we do exactly
If you know exactly what you
use the right keywords in your query
That's usually the fastest way to do,
But what if you don't know
Well, you clearly probably won't so well.
You will not related pages.
And then, you need to also walk
meaning by following the links or
You can then finally get
If you want to learn about again.
You will likely do a lot of browsing so
just like you are looking around in
interesting attractions
[INAUDIBLE].
So this analogy also tells us that
query, but we don't really have
And this is because in order
we need a map to guide us,
Of Chicago,
through the city of Chicago, you need a
So how to construct such a topical
research question that might bring us
more interesting browsing experience
So, to summarize this lecture,
we've talked about the two high level
Push tends to be supported by
Pull tends to be supported
Of course, in the sophisticated
we should combine the two.
In the pull mode, we can further this
Again we generally want to combine
so that you can support
If you want to know more about
push, you can read this article.
This give excellent discussion of the
information retrieval.
Here informational filtering is similar
the push mode of information access.
[MUSIC]
[MUSIC]
This lecture is about
This picture shows our overall plan for
In the last lecture, we talked about
We talked about push versus pull.
Such engines are the main tools for
Starting from this lecture,
we're going to talk about the how
So first it's about
We're going to talk about
First, we define Text Retrieval.
Second we're going to make a comparison
the related task Database Retrieval.
Finally, we're going to talk about
Document Ranking as two strategies for
So what is Text Retrieval?
It should be a task that's familiar for
the most of us because we're using
So text retrieval is basically a task
where the system would respond to
Basically, it's for supporting a query
as one way to implement the poll
So the situation is the following.
You have a collection of
These documents could be all
all the literature articles
Or maybe all the text
A user will typically give a query to
And then, the system would return
Relevant documents refer to those
the user who typed in the query.
All this task is a phone call
But literally information retrieval would
non-textual information as well,
It's worth noting that
of information retrieval in
video can be retrieved by
So for example,
match a user's query was
This problem is also
And the technology is often called
If you ever take a course in databases it
will be useful to pause
think about the differences between
Now these two tasks
But, there are some important differences.
So, spend a moment to think about
Think about the data, and the information
those that are managed
Think about the different between
database system versus queries that
And then finally think about the answers.
What's the difference between the two?
Okay, so if we think about the information
we will see that in text retrieval.
The data is unstructured, it's free text.
But in databases, they are structured data
to tell you this column is the names
The unstructured text is not obvious
what are the names of people
Because of this difference, we also see
ambiguous and we talk about that in the
But they don't tend to have
The results important
this is partly due to the difference
So test queries tend to be ambiguous.
Whereas in their research,
Think about a SQL query that would clearly
So it has very well-defined semantics.
Keyword queries or electronic queries tend
to be incomplete,
specify what documents
Whereas complete specification for
And because of these differences,
Being the case of text retrieval, we're
In the database search,
match records with the sequel
Now in the case of text retrieval,
to the query is not very well specified,
So it's unclear what should be
And this has very important consequences,
textual retrieval is
So this is a problem because
then we can not mathematically prove one
That also means we must rely
involving users to know
And that's why we have.
You need more than one lectures
Because this is very important topic for
Without knowing how to evaluate heroism
whether we have got the better or
So now let's look at
So, this slide shows a formal formulation
First, we have our vocabulary set, which
Now here,
in reality, on the web,
We have texts that are in
But here for simplicity, we just
As the techniques used for retrieving
less similar to the techniques used for
although there is important difference,
Next, we have the query,
And so here, you can see
the query is defined as
Each q sub i is a word in the vocabulary.
A document is defined in the same way,
And here,
Now typically, the documents
But there are also cases where
So you can think about what
I hope you can think of Twitter search.
Tweets are very short.
But in general,
Now, then we have
and this collection can be very large.
So think about the web.
It could be very large.
And then the goal of text retrieval
the documents, which we denote by R'(q),
And this in general, a subset of all
Unfortunately, this set of relevant
and user-dependent in the sense that,
in by different users, they expect
The query given to us by
on which document should be in this set.
And indeed, the user is generally
be in this set, especially in the case
large, the user doesn't have complete
So the best search system
an approximation of this
So we denote it by R'(q).
So formerly,
R'(q) approximation of
So how can we do that?
Now imagine if you are now asked
What would you do?
Now think for a moment.
Right, so these are your input.
The query, the documents.
And then you are to compute
which is a set of documents that
So, how would you solve the problem?
Now in general,
The first strategy is we do a document
going to have a binary classification
That's a function that
query as input, and then give a zero or
one as output to indicate whether this
So in this case, you can see the document.
The relevant document is set,
It basically, all the documents that
So in this case,
you can see the system must have decide
Basically, it has to say
And this is called absolute relevance.
Basically, it needs to know
useful to the user.
Alternatively, there's another
Now in this case,
the system is not going to make a call
But rather the system is going to
That would simply give us a value
that would indicate which
So it's not going to make a call whether
But rather it would say which
So this function then can be
then we're going to let
when the user looks at the document.
So we have a threshold theta
documents should be in
And we're going to assume
are ranked above the threshold
these are the documents that
And theta is a cutoff
So here we've got some collaboration
because we don't really make a cutoff.
And the user kind of helped
So in this case,
if one document is more
And that is, it only needs to
as opposed to absolute relevance.
Now you can probably already sense that
relative relevance would be easier to
Because in the first case,
we have to say exactly whether
And it turns out that ranking is indeed
So let's look at these two
So this picture shows how it works.
So on the left side,
we use the pluses to indicate
So we can see the true relevant
of true relevant documents, consists
And with the document selection function,
we're going to basically
relevant documents, and non-relevant ones.
Of course, the classified will not
So here we can see, in the approximation
we have got some number in the documents.
And similarly,
there is a relevant document that's
In the case of document ranking,
simply ranks all the documents in
And then, we're going to let the user
If the user wants to
then the user will scroll down some
But if the user only wants to
the user might stop at the top position.
So in this case, the user stops at d4.
So in fact, we have delivered
So as I said ranking is generally
because the classifier in the case of
Why?
Because the only clue
But the query may not be accurate in the
For example, you might expect relevant
topics by using specific vocabulary.
And as a result,
Because in the collection,
no others have discussed the topic
So in this case,
no relevant documents to return in
On the other hand,
for example, if the query
does not have sufficient descriptive
You may actually end up having of
thought these words my be sufficient
But, it turns out they
there are many distractions,
And so, this is a case of over delivery.
Unfortunately, it's very hard to find the
Why?
Because whether users looking for
not have a good knowledge about
And in that case, the user does not
vocabularies will be used in
So it's very hard for
a user to pre-specify the right
Even if the classifier is accurate,
relevant documents, because they
Relevance is often a matter of degree.
So we must prioritize these documents for
And note that this
because a user cannot
the user generally would have to
And therefore, it would make sense to
And that's what ranking is doing.
So for these reasons,
Now this preference also has
this is given by the probability
In the end of this lecture,
This principle says, returning a ranked
of probability that a document
is the optimal strategy under
First, the utility of
Is independent of the utility
Second, a user would be assumed to
Now it's easy to understand why these
Site for the ranking strategy.
Because if the documents are independent,
then we can evaluate the utility
And this would allow the computer
And then, we are going to rank these
The second assumption is to say that the
If the user is not going to follow
the documents sequentially, then obviously
So under these two assumptions, we can
is, in fact, the best that you could do.
Now, I've put one question here.
Do these two assumptions hold?
I suggest you to pause the lecture,
Now, can you think of
suggest these assumptions
Now, if you think for a moment,
you may realize none of
For example, in the case of
have documents that have similar or
If we look at each of them alone,
But if the user has already seen
generally not very useful for the user to
So clearly the utility
is dependent on other documents
In some other cases you might see
be useful to the user, but when three
They provide answers to
So this is a collective relevance and
the value of the document might
Sequential browsing generally would make
But even if you have a rank list,
users don't always just go strictly
They sometimes will look at the bottom for
And if you think about the more
we could possibly use like
Where you can put that additional
sequential browsing is a very
So the point here is that
none of these assumptions is
But probability ranking principle
ranking as a primary pattern for
And this has actually been the basis for
a lot of research work in
And many hours have been designed
despite that the assumptions
And we can address this problem
Of a ranked list, for example,
So to summarize this lecture,
the main points that you can
First, text retrieval is
And that means which algorithm is
Second, document ranking
And this will help users prioritize
And this is also to bypass the difficulty
Because we can get some help from users
it's more flexible.
So, this further suggests that the main
engine is the design
In other words, we need to define
on the query and document pair.
How we design such a function is the main
There are two suggested
The first is the classical paper on
The second one is a must-read for anyone
It's a classic IR book, which has
and results in early days up to
Chapter six of this book has
the Probability Ranking Principle and
[MUSIC]
[SOUND]
This lecture is a overview of
In the previous lecture, we introduced
We explained that the main problem
is the design of ranking function
In this lecture,
we will give an overview of different
So the problem is the following.
We have a query that has
the document that's also
And we hope to define a function f
that can compute a score based
So the main challenge you hear is with
can rank all the relevant documents
Clearly, this means our function
the likelihood that a document
That also means we have to have
In particular, in order to
we have to have a computational
And we achieve this goal by
which gives us
Now, over many decades,
researchers have designed many
And they fall into different categories.
First, one family of the models
Basically, we assume that if
the query than another document is,
then we will say the first document
So in this case,
the similarity between the query and
One well known example in this
which we will cover more in
A second kind of models
In this family of models, we follow a very
queries and documents are all
And we assume there is a binary
to indicate whether a document
We then define the score of document with
this random variable R is equal to 1,
There are different cases
One is classic probabilistic model,
yet another is divergence
In a later lecture, we will talk more
A third kind of model are based
So here the idea is to associate
and we can then quantify
show that the query
Finally, there is also a family of models
that are using axiomatic thinking.
Here, an idea is to define
hope a good retrieval function to satisfy.
So in this case, the problem is
that can satisfy all
Interestingly, although these different
in the end, the retrieval function
And these functions tend to
So now let's take a look at the common
and to examine some of the common
First, these models are all
of using bag of words to represent text,
we explained this in the natural
Bag of words representation remains
the search engines.
So with this assumption,
like a presidential campaign news
would be based on scores computed
And that means the score would
such as presidential, campaign, and news.
Here, we can see there
each corresponding to how well the
Inside of these functions,
So for example, one factor that
is how many times does the word
This is called a term frequency, or TF.
We might also denote as
In general, if the word occurs
then the value of this
Another factor is,
And this is to use the document length for
In general, if a term occurs in a long
document many times,
if it occurred the same number
Because in a long document, any term
Finally, there is this factor
That is, we also want to look at how
collection, and we call this document
And in some other models,
to characterize this information.
So here, I show the probability of
So all these are trying to characterize
the collection.
In general, matching a rare term in
to the overall score than
So this captures some of the main ideas
the art original models.
So now, a natural question is,
Now it turns out that many
So here are a list of
that are generally regarded as
pivoted length normalization,
When optimized,
And this was discussed in detail in this
Among all these,
It's most likely that this has been used
and you will also often see this
And we'll talk more about this
So, to summarize, the main points made
of a good ranking function pre-requires a
we achieve this goal by designing
Second, many models are equally effective,
Researchers are still active and
trying to find a truly
Finally, the state of the art
to rely on the following ideas.
First, bag of words representation.
Second, TF and
Such information is used in
the overall contribution of matching
These are often combined in interesting
exactly they are combined to rank
There are two suggested additional
The first is a paper where you can
comparison of multiple
The second is a book with
review of different retrieval models.
[MUSIC]
[SOUND]
This lecture is about the
We're going to give
In the last lecture, we talked about
a retrieval model, which would give
In this lecture, we're going to
designing a ramping function called
And we're going to give a brief
Vector space model is a special case of
similarity based models
Which means we assume relevance
between the document and the query.
Now whether is this assumption
But in order to solve the search problem,
we have to convert the vague notion
definition that can be implemented
So in this process,
This is the first assumption
Basically, we assume that if a document
another document.
Then the first document will be assumed it
And this is the basis for
Again, it's questionable whether this is
As we will see later there
The basic idea of vectors for
base retrieval model is actually
Imagine a high dimensional space where
So here I issue a three dimensional
programming, library and presidential.
So each term here defines one dimension.
Now we can consider vectors in this,
And we're going to assume
the query will be placed
So for example, on document might
Now this means this document
presidential, but
What does this mean in terms
That just means we're going to look at
this vector.
We're going to ignore everything else.
Basically, what we see here is only
Of course,
For example, the orders of
that's because we assume that
So with this presentation
d1 simply suggests a [INAUDIBLE] library.
Now this is different from another
a different vector, d2 here.
Now in this case, the document that
it doesn't talk about presidential.
So what does this remind you?
Well you can probably guess the topic
the library is software lab library.
So this shows that by using
we can actually capture the differences
Now you can also imagine
For example,
that might be a presidential program.
And in fact we can place all
And they will be pointing
And similarly,
we're going to place our query also
And then we're going to measure the
every document vector.
So in this case for example,
we can easily see d2 seems to be
And therefore,
So this is basically the main
So to be more precise,
vector space model is a framework.
In this framework,
First, we represent a document and
So here a term can be any basic concept.
For example, a word or a phrase or
Those are just sequence of
Each term is assumed that will
Therefore n terms in our vocabulary,
A query vector would consist
corresponding to the weights
Each document vector is also similar.
It has a number of elements and
indicating the weight of
Here, you can see,
Therefore, they are N elements
each corresponding to the weight
So the relevance in this case
will be assumed to be the similarity
Therefore, our ranking function
between the query vector and
Now if I ask you to write a program
in a search engine.
You would realize that
We haven't said a lot of things in detail,
therefore it's impossible to actually
That's why I said, this is a framework.
And this has to be refined
suggest a particular ranking function
So what does this framework not say?
Well, it actually hasn't said many things
that would be required in order
First, it did not say how we should define
We clearly assume
Otherwise, there will be redundancy.
For example, if two synonyms or somehow
Then they would be defining
that would clearly cause redundancy here.
Or all the emphasizing of
because it would be as if
when you actually matched
Secondly, it did not say how we
the query in this space.
Basically that show you some examples
But where exactly should the vector for
So this is equivalent to how
How do you compute the lose
This is a very important question,
because term weight in the query vector
So depending on how you assign the weight,
you might prefer some terms
Similarly, the total word in
It indicates how well the term
If you got it wrong then you clearly
Finally, how to define the similarity
So these questions must be addressed
function that we can actually
So how do we solve these problems
is the main topic of the next lecture.
[MUSIC]
In this lecture we're going to talk
about how to instantiate
that we can get very
So this is to continue the discussion
which is one particular approach
And we're going to talk about how
the the vector space
instantiate the framework to derive
And we're going to cover the symbolist
So as we discussed in
the vector space model
And this didn't say.
As we discussed in the previous lecture,
It does not say many things.
So, for example,
here it shows that it did not say
It also did not say how we place
It did not say how we place a query
And, finally, it did not say how we
the query vector and the document vector.
So you can imagine,
we have to say specifically
What is exactly xi?
And what is exactly yi?
This will determine where
where we place a query vector.
And, of course,
we also need to say exactly what
So if we can provide a definition
the dimensions and these xi's or
queries and document, then we will be
query vectors in this well defined space.
And then,
then we'll have a well
So let's see how we can do that and
Actually, I would suggest you to
spend a couple minutes to think about.
Suppose you are asked
You have come up with the idea of vector
out how to compute these vectors exactly,
What would you do?
So, think for a couple of minutes,
So, let's think about some simplest ways
First, how do we define the dimension?
Well, the obvious choice is to use
each word in our vocabulary
And show that there are N
Therefore, there are N dimensions.
Each word defines one dimension.
And this is basically
Now let's look at how we
Again here, the simplest strategy is to
use a Bit Vector to represent
And that means each element, xi and
yi will be taking a value
When it's 1,
it means the corresponding word is
When it's 0,
So you can imagine if the user
then the query vector will only
The document vector,
But it will also have many zeros since
Many words don't really
Many words will only occasionally
A lot of words will be absent
So now we have placed the documents and
Let's look at how we
So, a commonly used similarity
The Dot Product of two
the sum of the products of the
So, here we see that it's
So, here.
And then, x2 multiplied by y2.
And then, finally, xn multiplied by yn.
And then, we take a sum here.
So that's a Dot Product.
Now, we can represent this in a more
So this is only one of the many different
So, now we see that we have
we have defined the vectors, and we have
So now we finally have the simplest
on the bit vector [INAUDIBLE] dot product
And the formula looks like this.
So this is our formula.
And that's actually a particular retrieval
Now we can finally implement this
and then rank the documents for query.
Now, at this point you should
to think about how we can
So, we have gone through the process
using a vector space model.
And then,
vectors in the vector space, and
So in the end, we've got a specific
Now, the next step is to think about
actually makes sense, right?
Can we expect this function
when we used it to rank documents for
So it's worth thinking about what is
So, in the end, we'll get a number.
But what does this number mean?
Is it meaningful?
So, spend a couple minutes
And, of course,
the general question here is do you
Would it actually work well?
So, again,
Is it actually meaningful?
Does it mean something?
This is related to how well
So, in order to assess
vector space model actually works well,
So, here I show some sample documents and
The query is news about
And we have five documents here.
They cover different terms in the query.
And if you look at these documents for
some documents are probably relevant, and
Now, if I asked you to rank these
This is basically our ideal ranking.
When humans can examine the documents,
Now, so think for a moment,
And perhaps by pausing the lecture.
So I think most of you would
better than others because they
They match news,
So, it looks like these documents
They should be ranked on top.
And the other three d2, d1, and
So we can also say d4 and
d1, d2 and d5 are non-relevant.
So now let's see if our simplest
or could do something closer.
So, let's first think about
to score documents.
All right.
Here I show two documents, d1 and d3.
And we have the query also here.
In the vector space model, of course we
these documents and the query.
Now, I showed the vocabulary here as well.
So these are the end dimensions
So what do you think is the vector for
Note that we're assuming
to indicate whether a term is absent or
So these are zero,1 bit vectors.
So what do you think is the query vector?
Well, the query has four words here.
So for these four words,
And for the rest, there will be zeros.
Now, what about the documents?
It's the same.
So d1 has two rows, news and about.
So, there are two 1's here,
Similarly, so now that we
have the two vectors,
And we're going to use Do Product.
So you can see when we use Dot Product,
we just multiply the corresponding
So these two will be formal product,
and these two will
and these two will generate yet
Now you can easily see if we do that,
these zeroes because whenever we have
So when we take a sum
then the zero entries will be gone.
As long as you have one zero,
So, in the fact, we're just
In this case, we have seen two,
So what does that mean?
Well, that means this number, or
is simply the count of how many unique
Because if a term is matched in the
If it's not, then there will
Similarly, if the document has a term but
there will be a zero in the query vector.
So those don't count.
So, as a result,
measures how many unique query
This is how we interpret this score.
Now, we can also take a look at d3.
In this case, you can see the result
distinctive query words news, presidential
Now in this case, this seems
And this simplest vector
So that looks pretty good.
However, if we examine this model in
So, here I'm going to show all
And you can easily verify they're
counting the number of unique query
Now note that this measure
It basically means if a document
then the document will be
And that seems to make sense.
The only problem is here we can note that
And they tied with a 3 as a score.
So, that's a problem because if you look
should be ranked above d3 because
d3 only mentions the presidential once,
In the case of d3,
But d4 is clearly above
Another problem is that d2 and
But if you look at the three words
it matched the news, about and campaign.
But in the case of d3, it matched news,
So intuitively this reads better
is more important than matching about,
even though about and
So intuitively,
But this model doesn't do that.
So that means this model
We have to solve these problems.
To summarize,
in this lecture we talked about how
We mainly need to do three things.
One is to define the dimension.
The second is to decide how to place
and to also place a query in
And third is to define
particularly the query vector and
We also talked about various simple way
Indeed, that's probably the simplest
In this case,
We use a zero, 1 bit vector to
In this case, we basically only care
We ignore the frequency.
And we use the Dot Product
And with such a instantiation,
we showed that the scoring
a document based on the number of distinct
We also showed that such a simple vector
we need to improve it.
And this is a topic that we're
[MUSIC]
[SOUND]
In this lecture, we are going to talk about how
of the vector space model.
This is a continued discussion
We're going to focus on how to improve
In the previous lecture,
you have seen that with simple
we can come up with a simple scoring
an account of how many unique query
We also have seen that this function
In particular,
they will all get the same score because
But intuitively we would like
d2 is really not relevant.
So the problem here is that this function
First, we would like to give
matched presidential more times than d3.
Second, intuitively, matching presidential
matching about, because about is a very
It doesn't really carry that much content.
So in this lecture,
let's see how we can improve the model
It's worth thinking at this point
If we look back at assumptions we have
space model,
is really coming from
In particular, it has to do with how we
So then naturally,
we have to revisit those assumptions.
Perhaps we will have to use different ways
In particular, we have to place
So let's see how we can improve this.
One natural thought is in order to
the document,
we should consider the term frequency
In order to consider the difference
term occurred multiple times and one
we have to consider the term frequency,
In the simplest model, we only modeled
We ignored the actual number of times
So let's add this back.
So we're going to then
a vector with term frequency as element.
So that is to say, now the elements
the document vector will not be 0 or
instead they will be the counts of
So this would bring in additional
this can be seen as more accurate
So now let's see what the formula
representation.
So as you'll see on this slide,
And so the formula looks
In fact, it looks identical.
But inside the sum, of course,
They are now the counts of word i in
the query and in the document.
Now at this point I also suggest you
just to think about how we can interpret
It's doing something very similar
But because of the change of the vector,
now the new score has
Can you see the difference?
And it has to do with the consideration
the same term in a document.
More importantly, we would like to know
of the simplest vector space model.
So let's look at this example again.
So suppose we change the vector
Now let's look at these
The query vector is the same
exactly once in the query.
So the vector is still a 01 vector.
And in fact, d2 is also essentially
because none of these words
As a result,
The same is true for d3,
But d4 would be different, because
So the ending for presidential in the
As a result, now the score for
It's a 4 now.
So this means by using term frequency,
we can now rank d4 above d2 and
So this solved the problem with d4.
But we can also see that d2 and
They still have identical scores,
So how can we fix this problem?
Intuitively, we would like
matching presidential than matching about.
But how can we solve
Is there any way to determine
more importantly and
About is such a word which does not
We can essentially ignore that.
We sometimes call such
Those are generally very frequent and
Matching it doesn't really mean anything.
But computationally how
So again, I encourage you to
Can you came up with any statistical
presidential from about?
Now if you think about it for a moment,
you'll realize that one difference is
So if you count the occurrence of
then we will see that about has much
which tends to occur
So this idea suggests
the global statistics of terms or
some other information
the element of about in
At the same time,
the weight of presidential
If we can do that, then we can
score to be less than 3 while
Then we would be able to
So how can we do this systematically?
Again, we can rely on
And in this case, the particular idea
Now we have seen document
the modern retrieval functions.
We discussed this in a previous lecture.
So here is the specific way of using it.
Document frequency is the count of
Here we say inverse document frequency
that doesn't occur in many documents.
And so the way to incorporate this
is then to modify the frequency
the IDF of the corresponding word,
If we can do that,
which generally have a lower IDF, and
reward rare words,
So more specifically,
the IDF can be defined as
where M is the total number of documents
document frequency, the total number
Now if you plot this
then you would see the curve
In general, you can see it
a low DF word, a rare word.
You can also see the maximum value
It would be interesting for you to think
this function.
This could be an interesting exercise.
Now the specific function
the heuristic to simply
But it turns out that this particular
Now whether there's a better
the open research question.
But it's also clear that if
like what's shown here with this line,
then it may not be as
In particular, you can see
and we somehow have
After this point, we're going to say these
They can be essentially ignored.
And this makes sense when
let's say a term occurs in more
then the term is unlikely very important
It's not very important
So with the standard IDF you can
they all have low weights.
There's no difference.
But if you look at
at this point that there
So intuitively we'd want to
of low DF words rather
Well, of course,
validated by using the empirically
And we have to use users to
So now let's see how
So now let's look at
Now without the IDF weighting before,
But with IDF weighting we
by multiplying with the IDF value.
For example,
in particular for about there's adjustment
which is smaller than the IDF
So if you look at these,
As a result, adjustment here would be
So if we score with these new vectors,
of course,
campaign, but the matching of
So now as a result of IDF weighting,
because it matched a rare word,
So this shows that the IDF
So how effective is this model in
Well, let's look at all these
These are the new scores
But how effective is this new weighting
So now let's see overall how effective
with TF-IDF weighting.
Here we show all the five documents
these are their scores.
Now we can see the scores for
the first four documents here
They are as we expected.
However, we also see a new
which did not have a very high score
now actually has a very high score.
In fact, it has the highest score here.
So this creates a new problem.
This is actually a common phenomenon
Basically, when you try
you tend to introduce other problems.
And that's why it's very tricky how
And what's the best ranking function
Researchers are still working on that.
But in the next few lectures we're going
ideas to further improve this model and
So to summarize this lecture, we've talked
model, and
the vector space model
So the improvement is mostly on
give high weight to a term that
infrequently in the whole collection.
And we have seen that this
looks better than the simplest
But it also still has some problems.
In the next lecture we're going to look at
[MUSIC]
[MUSIC]
In this lecture, we continue
In particular, we're going to
In the previous lecture,
we have derived a TF idea of weighting
And we have assumed that this model
these examples as shown on this slide,
d5, which has received a very high score.
Indeed, it has received the highest
But this document is intuitive and
In this lecture,
how we're going to use TF
Before we discuss the details,
this simple TF-IDF
And see why this document has
So this is the formula, and
then you will see it involves a sum
And inside the sum, each matched
And this weight is TF-IDF weighting.
So it has an idea of component,
One is the total number of documents
The other is the document of frequency.
This is the number of
This word w.
The other variables
involved in the formula include
W in the query, and
If you look at this document again,
the reason why it hasn't
it has a very high count of campaign.
So the count of campaign in this document
the other documents, and has contributed
So in treating the amount
this document, we need to somehow
of the matching of this
And if you think about the matching
you actually would realize,
we probably shouldn't reward
And by that I mean,
says a lot about
because it goes from zero
And that increase means a lot.
Once we see a word in the document,
it's very likely that the document
If we see a extra occurrence on
that is to go from one to two,
occurrence kind of confirmed that it's
Now we are more sure that this
But imagine we have seen, let's say,
Now, adding one extra occurrence is not
because we're already sure that
So if you're thinking this way, it seems
of a high count of a term, and
So this transformation function is
word into a term frequency weight for
So here I show in x axis that we'll count,
y axis I show the term frequency weight.
So in the previous breaking functions,
we actually have imprison rate
So for example,
we actually use such a transformation
Basically if the count is 0,
otherwise it would have a weight of 1.
It's flat.
Now, what about using
Well, that's a linear function, so it has
Now we have just seen that
So what we want is something like this.
So for example,
we can't have a sublinear
And this will control the influence
because it's going to lower its inference.
Yet, it will retain
Or we might want to even bend the curve
Now people have tried all these methods.
And they are indeed working better than
But so far, what works the best seems
called a BM25 transformation.
BM stands for best matching.
Now in this transformation,
And this k controls the upper
It's easy to see this
because if you look at the x divided by
then the numerator will never be able
So it's upper bounded by k+1.
Now, this is also difference between
a logarithm transformation.
Which it doesn't have upper bound.
Furthermore, one interesting property
we can actually simulate different
Including the two extremes
That is, the 0/1 bit transformation and
So for example, if we set k to 0,
the function value will be 1.
So we precisely recover
If you set k to very large
it's going to look more like
So in this sense,
It allows us to control
It also has a nice property
And this upper bound is useful to control
And so that we can prevent a spammer
of one term to spam all queries
In other words, this upper bound
terms would be counted when we aggregate
As I said, this transformation
So to summarize this lecture,
Sublinear TF Transformation,
capture the intuition of diminishing
It's also to avoid the dominance by
This BM25 transformation that we
It's so far one of the best-performing
It has upper bound, and so
Now if we're plugging this function into
Then we'd end up having
which has a BM25 TF component.
Now, this is already
the odd ranking function called BM25.
And we'll discuss how we can further
[MUSIC]
[SOUND]
This lecture is about
Document Length Normalization
In this lecture, we will continue
In particular, we're going to discuss the
So far in the lectures about the vector
signals from the document to assess
In particular,
The count of a tone in a document.
We have also considered it's
IDF, Inverse Document Frequency.
But we have not considered
So here I show two example documents,
D6 on the other hand, has a 5000 words.
If you look at the matching
we see that in d6, there are more
But one might reason that,
these query words in a scattered manner.
So maybe the topic of d6, is not
So, the discussion of the campaign
may have nothing to do with the managing
In general,
they would have a higher chance for
In fact, if you generate a long document
a distribution of words, then eventually
So in this sense, we should penalize on
better chance matching to any query, and
We also need to be careful in avoiding
On the one hand,
But on the other hand,
Now, the reasoning is because
different reasons.
In one case, the document may be
So for example, think about the vortex
It would use more words than
So, this is a case where we probably
long documents such as a full paper.
When we compare the matching
document with matching of
Then long papers in general,
have a higher chance of matching clearer
However, there is another case
that is when the document
Now consider another
where we simply concatenate a lot
In such a case, obviously, we don't want
Indeed, we probably don't want to penalize
So that's why, we need to be careful about
A method of that has been working well,
is called a pivoted length normalization.
And in this case,
the idea is to use the average document
That means we'll assume that for
the score is about right so
But if the document is longer
then there will be some penalization.
Whereas if it's a shorter,
So this is illustrated at
x-axis you can see the length of document.
On the y-axis, we show the normalizer.
In this case, the Pivoted Length
is seeing to be interpolation of 1 and
the normalize the document in length
So you can see here,
of the document by the average documents,
sense about how this document is
also gives us a benefit of not
We can measure the length by words or
Anyway, this normalizer
First we see that, if we set the parameter
So, there's no lens normalization at all.
So, b, in this sense,
Whereas, if we set b to a nonzero value,
All right, so
documents that are longer than
Whereas, the value of
would be smaller for shorter documents.
So in this sense,
long documents, and
The degree of penalization
because if we set b to a larger value,
There's even more penalization for
the short documents.
By adjusting b, which varies from 0 to 1,
we can control the degree
So, if we plug in this length
the vector space model, ranking functions
Then we will end up having
And these are in fact the state of
Let's take a look at each of them.
The first one is called a pivoted length
and a reference in [INAUDIBLE]
And here we see that, it's basically
the idea of component should
There is also a query term
And then, in the middle, there is
we see we use the double logarithm
this is to achieve
But we also put a document
Right, so this would cause
because the larger the denominator is,
And this is of course controlled
And you can see again, if b is set to 0
Okay, so this is one of the two most
The next one called a BM25 or Okapi,
is also similar in that it
and query IDF component here.
But in the middle,
As we explained,
and that does sublinear
In this case we have put the length
We're adjusting k but
because we put a normalizer
Therefore, again, if a document is longer
So you can see after we have gone through
and we have in the end reached
So, So far, we have talked about
mainly how to place the document
And, this has played an important role
the simple function.
But there are also other dimensions,
For example, can we further
the dimension of the Vector Space Model?
Now, we've just assumed that the bag
dimension as a word but obviously,
For example, a stemmed word, those
into the same root form, so
that computation and computing were all
We get those stop word removal.
This is to remove some very common words
We get use of phrases
We can even use later in
some clusters of words that represent the
We can also use smaller unit,
are sequences of and
However, in practice, people have found
phrases is still the most effective
So, this is still so far the most
And it's used in all major search engines.
I should also mention, that sometimes
domain specific tokenization.
And this is actually very important, as we
prevent us from matching them with each
In some languages like Chinese,
text to obtain word band rates because
A word might correspond to one
even three characters.
So, it's easier in English when we
In some other languages, we may need
figure a way out of what
There is also the possibility to
And so
one can imagine there are other measures.
For example, we can measure the cosine
Or we can use Euclidean distance measure.
And these are all possible, but
dot product seems still the best and
In fact that it's sufficiently general,
if you consider the possibilities
So, for example,
cosine measure can be thought of as the
That means, we first normalize each factor
That would be critical
I just mentioned that the BM25, seems to
But there has been also further
Although, none of these words have
So in one line work,
Here, F stands for field, and this is
So for example, you might consider
or body of the research article.
Or even anchor text on the web page,
links to other pages and
a proper way of different fields to help
When we use BM25 for such a document and
the obvious choice is to apply BM25 for
Basically, the idea of BM25F is
counts of terms in all the fields,
Now, this has advantage of avoiding over
Remember in the sublinear
the first occurrence is very important and
And if we do that for all the fields,
then the same term might have gained
But when we combine these
we just do the transformation one time.
At that time,
then the extra occurrences will not be
And this method has been working very well
The other line of extension
In this line,
over penalization of
So to address this problem,
We can simply add a small constant
But what's interesting is that,
doing such a small modification,
the problem of over penalization of
So the new formula called BM25+,
is empirically and
So to summarize all what we have
here are the major take away points.
First, in such a model,
Assuming that relevance of a document
basically proportional to the similarity
So naturally,
document must have been
And in this case, we will present them as
Where the dimensions are defined by words,
And we generally, need to use a lot of
We use some examples, which show
including Tf weighting and transformation.
And IDF weighting, and
These major heuristics are the most
to ensure such a general ranking function
And finally, BM25 and
to be the most effective formulas
Now I have to say that, I put BM25 in
in fact, the BM25 has been derived
So the reason why I've put it in
the ranking function actually has a nice
We can easily see,
it looks very much like a vector space
The second reason is because the original
And that form of IDF after
well as the standard IDF
So as effective retrieval function,
BM25 should probably use a heuristic
To make them even more look
There are some additional readings.
The first is, a paper about
It's an excellent example
analysis to suggest the need for
then further derive the length
The second, is the original paper
The third paper,
its extensions, particularly BM25 F.
And finally, in the last paper
BM25 to correct the over
[MUSIC]
[MUSIC]
This lecture is about the implementation
In this lecture we will discuss
retrieval method to build a search engine.
The main challenge is to
to enable a query to be answered very
This is a typical text
We can see the documents are first
get tokenized units, for example, words.
And then, these words, or
a indexer that will create a index,
the search engine to use
And the query would be going
So the Tokenizer would be
so that the text can be
The same units would be
The query's representation would
which would use the index to quickly
the documents and then ranking them.
The results will be given to the user.
And then the user can look at the results
explicit judgements of both
which documents are bad.
Or implicit feedback such as so that
End user will just look at the results,
skip some, and
So these interacting signals can be used
accuracy by assuming that viewed documents
So a search engine system then
The first part is the indexer, and
responds to the users query, and
Now typically, the Indexer is
you can pre-process the correct data and
to build the inventory index,
And this data structure can then be used
to process a user's query dynamically and
The feedback mechanism can be done online
The implementation of the indexer and
and this is the main topic of this
The feedback mechanism,
it depends on which method is used.
So that is usually done in
Let's first talk about the tokenizer.
Tokernization is a normalized lexical
so that semantically similar words
Now, in the language like English,
this will map all the inflectional
So for example, computer, computation, and
computing can all be matched
This way all these different forms of
Now normally, this is a good idea,
to increase the coverage of documents
But it's also not always beneficial,
because sometimes the subtlest
computation might still suggest the
But in most cases,
When we tokenize the text in some other
face some special challenges in segmenting
Because it's not obvious
there's no space to separate them.
So here of course, we have to use some
Once we do tokenization, then we would
convert the documents and do some data
The basic idea is to precompute
So the most commonly used index
And this has been used
to support basic search algorithms.
Sometimes the other indices, for example,
document index might be needed in order
And these kind of techniques
that they vary a lot according
To understand why we want to use
you to think about how you would
So if you want to use more time to
So think about how you can
that you can quickly respond
Where if you have thought
you might realize that where
the list of documents that match
In this way, you can basically
So when you see a term you can simply just
that term and return the list to the user.
So that's the fastest way to
Now the idea of the invert index
We're going to do pre-constructed
us to quickly find all the documents
So let's take a look at this example.
We have three documents here,
and these are the documents that you
Suppose that we want to create
Then we want to maintain a dictionary, in
each term and we're going to store
For example, the number of
the total number of code or
which means we would kind of duplicate
And so, for example, news,
this term occur in all
so the count of documents is three.
And you might also realize we needed this
for computing some statistics to
Can you think of that?
So what weighting heuristic
Well, that's the idea, right,
So, IDF is the property of a term,
So, with the document that count here,
either at this time, or
At random time when we see a query.
Now in addition to these basic statistics,
we'll also store all the documents
and these entries are stored
So in this case it matched
we store information about
This is the document id,
The tf is one for news, in the second
So from this list, we can get all
we can also know the frequency
So, if the query has just one word,
we have easily look up to this
go quicker into the postings to fetch
So, let's take a look at another term.
This time, let's take a look
This would occur in only one document,
So the document frequency is 1 but
So the frequency count is two, and
some other reachable method where
assess the popularity of
Similarly we'll have a pointer
and in this case,
the term occurred in just one document and
The document id is 3 and
So this is the basic
It's actually pretty simple, right?
With this structure we can easily fetch
And this will be the basis for
Now sometimes we also want to store
So in many of these cases the term
So there's only one position for
But in this case, the term occurred
Now the position information is very
the matching of query terms is
let's say, five words or ten words.
Or, whether the matching of the two query
That this can all be checked quickly
So, why is inverted index good for
Well, we just talked about the possibility
And that's very easy.
What about the multiple term queries?
Well let's first look at the some
A Boolean query is basically
So I want the value in the document
So that's one conjunctive query.
Or I want the web documents
That's a disjunctive query.
But how can we answer such
Well if you think a bit about it,
it would be obvious because
the documents that match term A and also
And then just take the intersection
Or to take the union to
So this is all very easy to answer.
It's going to be very quick.
Now what about the multi-term
We talked about the vector space model for
we will do a match such query with
And the score is based on
So in this case it's not
the scoring can be actually
Basically it's similar to
Basically, it's like A or B.
We take the union of all the documents
then we would aggregate the term weights.
So this is a basic idea of using inverted
And we're going to talk about
But for now,
let's just look at the question
Basically why is more efficient than
This is the obvious approach.
You can just compute a score for each
And this is a straightforward method but
this is going to be very slow imagine
If you do this then it will take
So the question now is why would
Well it has to do is the word
So, here's some common phenomena
There are some languages independent
And these patterns are basically
A few words like the common
we occur very, very frequently in text.
So they account for
But most words would occur just rarely.
There are many words that occur just once,
let's say, in a document or
And there are many such.
It's also true that the most
they have to be rare in another.
That means although the general
was observed in many cases that
may vary from context to context.
So this phenomena is characterized
This law says that the rank of a word
multiplied by the frequency of
So formally if we use F(w)
r(w) to denote the rank of a word.
Then this is the formula.
It basically says the same thing,
Where C is basically a constant and
alpha, that might be adjusted to
So if I plot the word
then you can see this more easily.
The x axis is basically the word rank.
This is r(w) and
Now this curve shows that the product
Now if you look at these words, we can see
In the middle,
These words tend to occur
they are not like those
And they are also not very rare.
So they tend to be often used in
queries and they also tend
These intermediate frequency words.
But if you look at the left
these are the highest frequency words.
They are covered very frequently.
They are usually words,
Those words are very, very frequent and
discriminated, and they are generally
So they are often removed and
So you can use pretty much just the kind
infer what words might be stop words.
Those are basically
And they also occupy a lot of
You can imagine the posting entries for
And then therefore,
if you can remove such words you can save
We also show the tail part,
Those words don't occur very frequently,
Those words are actually very useful for
search also, if a user happens to
But because they're rare,
aren't necessarily
But retain them would allow us to
They generally have very high IDF.
So what kind of data structures should
Well, it has two parts, right.
If you recall, we have a dictionary and
The dictionary has modest size, although
large but compare it with
And we also need to have fast
because we're going to look up
So therefore, we'd prefer to keep such
If the collection is not very large,
if the collection is very large
If the vocabulary size is very large,
So, in general that's how it goes.
So the data structures
storing dictionary,
There are structures like hash table, or
b-tree if we can't store
And then try to build a structure that
For postings they are huge.
And in general, we don't have to have
We generally would just look up
frequencies for all the documents
So would read those entries sequentially.
And therefore because it's large and
they have to stay on disc and they would
term frequency or
Now because they are very large,
Now this is not only to save disc space,
one benefit of compression, it It's
But it's also to help improving speed.
Can you see why?
Well, we know that input and
In comparison with the time taken by CPU.
So, CPU is much faster but
so by compressing the inverter index,
the entries, that we have the readings,
would be smaller, and
the amount of tracking IO and
Of course, we have to then do more
uncompress the data in the memory.
But as I said CPU is fast.
So over all we can still save time.
So compression here is both
to speed up the loading of the index.
[MUSIC]
[SOUND]
This lecture is about the inverted index
construction.
In this lecture, we will continue
In particular, we're going to discuss
The construction of the inverted index
very small.
It's very easy to construct a dictionary
The problem is that when our data
then we have to use some
And unfortunately, in most retrieval
And they generally cannot be
And there are many approaches to
method is quite common and
First, you collect the local termID,
Basically you will locate the terms
And then once you collect those accounts
So that you will be able to local
these are called rounds.
And then you write them into
then you merge in step 3.
Do pairwise merging of these runs, until
generate a single inverted index.
So this is an illustration of this method.
On the left you see some documents and
on the right we have a term lexicon and
These lexicons are to map string-based
terms into integer representations or
map back from integers to
The reason why we want our interest
IDs is because integers
For example,
array, and they are also easy to compress.
So this is one reason why we tend
so that we don't have to
So how does this approach work?
Well, it's very simple.
We're going to scan these
then parse the documents and
And in this stage we generally sort
because we process each
So we'll first encounter all
Therefore the document IDs
And this will be followed by document IDs
only just because we process
At some point,
that would have to write
Before we do that we 're going to sort
We can sort them and then this time
Note that here,
So all the entries that share the same
In this case,
that match term 1 would
And we're going to write this into
And would that allows you to
makes a batch of documents.
And we're going to do that for
So we're going to write a lot of
And then the next stage is
We're going to merge them and
Eventually, we will get
where the entries are sorted
And on the top, we're going to see
the documents that match term ID 1.
So this is basically, how we can do
Even though the data cannot be
Now, we mention earlier that
it's desirable to compress them.
So let's now take a little bit
Well the idea of compression in general,
leverage skewed distributions of values.
And we generally have to use
instead of the fixed-length
a program manager like C++.
And so how can we leverage
to compress these values?
Well in general, we will use few
words at the cost of using longer
So in our case, let's think about how
Now, if you can picture what
you will see in post things,
Those are the frequencies of
Now, if you think about it, what kind
You probably will be able to guess
far more frequently than large numbers.
Why?
Well, think about the distribution of
and many words occur just rarely so
Therefore, we can use fewer bits for
highly frequent integers and
that's cost of using more bits for
This is a trade off of course.
If the values are distributed to uniform,
but because we tend to see many small
We can save on average even though
we have to use a lot of bits.
What about the document IDs
Well they are not distributed
So how can we deal with that?
Well it turns out that we can
that is to store the difference
And we can imagine if a term has
there will be longest of document IDs.
So when we take the gap, and we take the
those gaps will be small.
So again, see a lot of small numbers.
Whereas if a term occurred
then the gap would be large,
So this creates some skewed distribution,
that would allow us to
This is also possible because
uncompress these document IDs,
Because we stored the difference and
document ID we have to first
And then we can add the difference to
the current document ID.
Now this was possible because we only
those document IDs.
Once we look up the term, we look up all
then we sequentially process them.
So it's very natural,
And there are many different methods for
So binary code is a commonly used
We use basically fixed glance in coding.
Unary code, gamma code, and
there are many other possibilities.
So let's look at some
Binary coding is really
that's a property for
The unary coding is a variable
In this case, integer this 1 will be
encoded as x -1, 1 bit followed by 0.
So for example, 3 will be encoded as 2,
whereas 5 will be encoded as 4,
So now you can imagine how many bits do we
So how many bits do you have to
Well exactly, we have to use 100 bits.
So it's the same number of bits
So this is very inefficient if you
Imagine if you occasionally see a number
So this only works well if you
no large numbers, mostly very
Now, how do you decode this code?
Now since these are variable
you can't just count how many bits and
You can't say 8-bits or 32-bits,
They are variable length, so
In this case for unary, you can see
Now you can easily see 0 would
So you just count up how many 1s you
You have finished one number,
Now we just saw that unary
In rewarding small numbers, and
if you occasionally can see a very
So what about some other
Well gamma coding's one of them and
in this method we can use unary coding for
a transform form of that.
So it's 1 plus the floor of log of x.
So the magnitude of this value is
So that's why we can afford
And so first I have the unary code for
And this would be followed by
And this basically the same uniform code,
And we're going to use this coder to code
And this is basically precisely
So the unary code are basically
well add one there and here.
But the remaining part
code through actually code the difference
between the x and this 2 to the log of x.
And it's easy to show that for this
difference we only need to use up
to this many bits and
And this is easy to understand,
if the difference is too large, then we
So here are some examples for
The first two digits are the unary code.
So this isn't for the value 2,
10 encodes 2 in unary coding.
And so that means the floor of
log of x is 1,
In code 1 plus the flow of log of x,
since this is two then we know that
So that 3 is still larger than 2 to the 1.
So the difference is 1, and
So that's why we have 101 for 3.
Now similarly 5 is encoded as 110,
And in this case the unary code in code 3.
And so this is a unary code 110 and
And that means we're going to
the 2 to the 2 and that's 1.
And so we now have again 1 at the end.
But this time we're going to use 2 bits,
because with this level
We could have more numbers a 5, 6, 7 they
So in order to differentiate them,
we have to use 2 bits in
So you can imagine 6 would be 10 here
It's also true that the form of
odd number of bits, and
That's the end of the unary code.
And before that or on the left side
And on the right side of this 0,
So how can you decode such code?
Well you again first do unary coding.
Once you hit 0, you have got the unary
many bits you have to read further
So this is how you can
There is also a delta code that's
that you replace the unary
So that's even less
in terms of wording the small integers.
So that means, it's okay if you
It's okay with delta code.
It's also fine with the gamma code,
And they are all operating of course,
at different degrees of favoring short or
And that also means they would be
But none of them is perfect for
And which method works the best would
in your dataset.
For inverted index compression,
people have found that gamma
So how to uncompress inverted index?
I will just talk about this.
Firstly, you decode
And we just I think discussed the how we
What about the document IDs that
Well, we're going to do
supposed the encoded I list is x1,
We first decode x1 to obtain
Then we can decode x2,
which is actually the difference between
So we have to add the decoder
the value of the ID at
So this is where you can
converting document IDs to integers.
And that allows us to do
And we just repeat until we
Every time we use the document ID in
the document ID in the next position.
[MUSIC]
[SOUND]
This lecture is about how to do faster
In this lecture, we're going to continue
In particular,
we're going to talk about how to support
So let's think about what a general
Now of course, the vector space
we can imagine many other retrieval
So the form of this
We see this scoring function
a query Q is defined as
that adjustment a function that
That I'll assume here at the end,
f sub d of d and f sub q of q.
These are adjustment factors
so they are at the level of a document and
So and then inside of this function,
we also see there's
So this is the main part
these as I just said of
the level of the whole document and
For example, document [INAUDIBLE] and
this aggregate punching would
Now inside this h function,
there are functions that
of the contribution of
So this g,
of a matched query term ti in document d.
And this h function would then
So for example,
but it can also be a product or it could
And then finally, this adjustment
the document level or query level
for example, document [INAUDIBLE].
So, this general form would cover
Let's look at how we can score documents
So, here's a general algorithm
First this query level and
document level factors can be
Of course, for the query we have to
document, for example,
And then, we maintain a score accumulator
An h is an aggregation function
So how do we do that?
For each period term we're going to
from the invert index.
This will give us all the documents
and that includes d1, f1 and so dn fn.
So each pair is a document ID and
Then for each entry d sub j and
of the term in this
We'll going to compute the function
weight of this term, so
we're computing the weight completion of
And then, we're going to update
this document and
accumulator that would
So this is basically a general
functions of this form by
Note that we don't have to
that didn't match any query term.
Well, this is why it's fast,
we only need to process the documents
In the end, then we're going to adjust
sub a and then we can sort.
So let's take a look
In this case, let's assume the scoring
it just takes the sum of t f, the role of
This simplification would help
It's very easy to extend the computation
the transformation of tf, or [INAUDIBLE]
So let's take a look at specific example,
and it show some entries of
Information occurred in four documents and
their frequencies are also there,
So let's see how the arrows works, so
and we fetch the first query then,
That's information, right?
And imagine we have all these
scores for these documents.
We can imagine there will be other but
then they will only be
So before we do any waiting of terms,
we don't even need a score of.
That comes actually we have these score
So lets fetch the interest from
information, that the first one.
So these four accumulators obviously
So, the first entry is d1 and 3,
3 is occurrences of
Since our scoring function assume that the
We just need to add a 3 to the score
the increase of score due to matching
And then, we go to the next entry,
then we add a 4 to the score
Of course, at this point, that we will
And so at this point, we allocated
and we add one, we allocate another
And then finally,
information occurred five
Okay, so this completes the processing of
information.
It processed all the contributions
four documents.
So now, our error will go to
So, we're going to fetch all
So, in this case,
there are three entries, and
The first is d2 and 3 and
that means security occur three
Well, we do exactly the same,
So, this time we're going to change the
allocated and
value which is a 4, so
D2 score is increased because the match
the security.
Go to the next entry, that's d4 and
d4 and again, we add 1 to d4 so
Finally, we process d5 and a 3.
Since we have not yet allocated a score
we're going to allocate 1 for d5,
So, those scores, of the last rule,
If our scoring function is just
Now, what if we, actually,
Well, we going to do the [INAUDIBLE]
So, to summarize this,
we first process the information
we processed all the entries
Then we process the security,
what should be the order of processing
It might make a difference especially
the score accumulators.
Let's say, we only want to keep
What do you think would be
Would you process a common term first or
The answers is we just go to who
A rare term would match a few documents,
be higher,
And then, it allows us to attach
So, it helps pruning
if we don't need so
So those are all heuristics for
Here you can also see how we can
So they can [INAUDIBLE] when we
each query time.
When we fetch the inverted index we
then we can compute IDF.
Or maybe perhaps the IDF value
when we indexed the documents.
At that time, we already computed
so all these can be done at this time.
So that would mean when we process
these words would be adjusted by the same
So this is the basic idea of using
it works well for all kinds of
And this generally,
most state of art retrieval functions.
So there are some tricks to
some general techniques
This is we just store some results of
when you see the same query,
Similarly, you can also slow the list
a popular term.
And if the query term is popular likely,
you will soon need to factor the inverted
So keeping it in the memory would help,
improving efficiency.
We can also keep only the most promising
doesn't want to examine so many documents.
We only need to return high
likely are ranked on the top.
For that purpose,
We don't have to store
At some point, we just keep
Another technique is to do parallel
really process in such a large
And you scale up to
the special techniques you
to distribute the storage
So here is a list of some text retrieval
You can find more information
And here, I listed your four here,
that can support a lot of applications and
You can use it to build a search
The downside is that it's not
the algorithms implemented they are also
Lemur or Indri is another
a nice support web
it has many advanced search algorithms and
Terrier is yet another toolkit
application capability and
So that's maybe in between Lemur or
maybe rather combining
so that's also useful tool kit.
MeTA is a toolkit that we will use for
the problem assignment and
a combination of both text retrieval
And so talking models are implement they
implemented in the toolkit as
So to summarize all the discussion
here are the major takeaway points.
Inverted index is the primary data
and that's the key to enable
And the basic idea is to preprocess
we want to do compression
So that we can save disk space and
processing of inverted index in general.
We talked about how to construct the
the memory.
And then we talk about faster search using
the invective index to accumulate a scores
And we exploit the Zipf's law to
that don't match any query term and
this algorithm can actually support
So these basic techniques
further scaling up using distributed file
Here are two additional readings you
you are interested in
The first one is a classical
o inverted index and
And how to,
any inputs of the space,
The second one is a newer textbook that
evaluating search engines.
[MUSIC]
[MUSIC]
This lecture is about Evaluation of
lectures, we have talked about
different kinds of ranking functions.
But how do we know which
In order to answer this question,
we have to compare them and that means we
So this is the main topic of this lecture.
First, lets think about why
I already give one reason.
That is, we have to use evaluation
works better.
Now this is very important for
Otherwise, we wouldn't know whether a new
In the beginning of this course, we talked
We compare it with data base retrieval.
There we mentioned that text retrieval
So evaluation must rely on users.
Which system works better,
So, this becomes a very
because how can we get users
How can we do a fair comparison
So just go back to the reasons for
I listed two reasons here.
The second reason, is basically what I
reason which is to assess the actual
Imagine you're building your
it would be interesting knowing how well
So in this case,
matches must reflect the utility to
And typically, this has to be
using the real search engine.
In the second case, or the second reason,
the measures actually all need to collated
Thus, they don't have to accurately
So the measure only needs to be good
And this is usually done
And this is the main idea that we'll
This has been very important for
for improving search
So let's talk about what to measure.
There are many aspects of searching
And here,
One, is effectiveness or accuracy.
How accurate are the search results?
In this case, we're measuring a system's
on top of non relevant ones.
The second, is efficiency.
How quickly can you get the results?
How much computing resources
In this case, we need to measure the space
The third aspect is usability.
Basically the question is,
Here, obviously, interfaces and
many other things also important and
Now in this course, we're going to
accuracy measures.
Because the efficiency and
usability dimensions are not
And so they are needed for
And there is also good coverage
But how to evaluate search
something unique to text retrieval and
The main idea that people have proposed
the text retrieval algorithm is called
This one actually was developed
It's a methodology for
Its sampling methodology that has
search engine evaluation.
But also for evaluating virtually
for example in natural language processing
is empirical to find, we typically
And today with the big data challenging
with the use of machine
This methodology has been very popular,
a search engine application in the 1960s.
So the basic idea of this approach is
define measures.
Once such a test collection is built,
again to test different algorithms.
And we're going to define measures
performance of a system and algorithm.
So how exactly will this work?
Well we can do have a sample collection of
the real document collection
We're going to also have a sample
This is a little simulator
Then, we'll have to have
These are judgments of which documents
Ideally, they have to be made by
Because those are the people that know
And finally, we have to have matches for
quantify how well our system's result
That would be constructed base
So this methodology is very useful for
because the test can be reused many times.
And it will also provide a fair
We have the same criteria or
same dataset to be used to
This allows us to compare
an old algorithm that was divided many
So this is the illustration of this works,
we need our queries that are showing here.
We have Q1, Q2 etc.
We also need the documents and
on the right side you will see
These are basically the binary judgments
So for example,
D2 is judged as being relevant as well,
And the Q1 etc.
These will be created by users.
Once we have these, and
And then if you have two systems,
then you can just run each
the documents and
Let's say if the queries Q1 and
Here I show R sub A as
So this is, remember we talked about
task of computing approximation
R sub A is system A's approximation here.
And R sub B is system B's
Now, let's take a look at these results.
So which is better?
Now imagine if a user,
Now let's take a look at the both results.
And there are some differences and
there are some documents that
But if you look at the results,
A is better in the sense that we don't
And among the three documents returned,
So that's good, it's precise.
On the other hand one council
because we've got all of
We've got three instead of two.
So which one is better and
Well, obviously this question
It depends on users as well.
You might even imagine for
If the user is not interested in
Right, in this case the user doesn't
see most of the relevant documents.
On the other hand,
to have as many random
For example, if you're doing a literature
and you might find that
So in the case, we will have to also
And we might need it to define multiple
perspectives of looking at the results.
[MUSIC]
[SOUND]
lecture is about the basic measures for
In this lecture,
measures to quantitatively
This is a slide that you have seen
about the Granville
We can have a test faction that consists
We can then run two systems on these
Their performance.
And we raise the question,
Is system A better or is system B better?
So let's now talk about how to
Suppose we have a total of 10 relevant
this query.
Now, the relevant judgments show on
And we have only seen 3 [INAUDIBLE] there,
But, we can imagine there are other Random
So now, intuitively,
A is better because it
And in particular we have seen
two of them are relevant but in system B,
we have five results and
So intuitively it looks like
And this infusion can be captured
where we simply compute to what extent
If you have 100% position,
that would mean that all
So in this case system A has
three System B has some
this shows that system
But we also talked about System B
would like to retrieve as many
So in that case we'll have to compare
retrieve and
This method uses the completeness
In your retrieval result.
So we just assume that there are ten
And here we've got two of them,
So the recall is 2 out of 10.
Whereas System B has called a 3,
Now we can see by recall
And these two measures turn out to
evaluating search engine.
And they are very important because
other test evaluation problems.
For example, if you look at
you tend to see precision recall numbers
Okay so, now let's define these
And these measures are to evaluate a set
we are considering that approximation
We can distinguish 4 cases depending
A document can be retrieved or
Because we are talking
A document can be also relevant or
not relevant depending on whether the user
So we can now have counts of documents in.
Each of the four categories again
documents that have been retrieved and
B for documents that are not retrieved but
No with this table then
As the ratio of the relevant
retrieved documents A to the total
So, this is just A divided
The sum of this column.
Singularly recall is defined by
So that's again to divide a by.
The sum of the row instead of the column.
All right, so we can see precision and
that's the number of
But we're going to use
Okay, so what would be an ideal result.
Well, you can easily see being
recall oil to be 1.0.
That means We have got 1% of
in our results, and all of the results
At least there's no single
In reality, however, high recall tends
And you can imagine why that's the case.
As you go down the to try to get as
you tend to encounter a lot of documents,
Note that this set can also
In the rest of this, that's why although
retrieve the documents, they are actually
They are the fundamental measures in
We often are interested in The precision
This means we look at how many documents
among the top ten results
Now, this is a very meaningful measure,
because it tells us how many relevant
On the first page of where they
So precision and recall
use them to further evaluate a search
We just said that there tends to be
so naturally it would be
And here's one method that's often used,
it's a [INAUDIBLE] mean of precision and
So, you can see at first, compute the.
Inverse of R and P here,
the 2 by using coefficients
And after some transformation you can
And in any case it just becomes
recall, and beta is a parameter,
It can control the emphasis
set beta to 1 We end up having a special
This is a popular measure that's often
And the formula looks very simple.
It's just this, here.
Now it's easy to see that if
larger recall than f
But, what's interesting is that
recall is captured
So, in order to understand that, we
can first look at the natural
using the symbol arithmetically
That would be likely the most natural way
If you want to think more,
So why is this not as good as F1?
Or what's the problem with this?
Now, if you think about
you can see this is
In this case,
In the case of a sum, the total value
that means if you have a very high P or
don't care about whether the other value
Now this is not desirable because one
We have perfect recall easily.
Can we imagine how?
It's probably very easy to
all the documents in the collection and
And this will give us 0.5 as the average.
But such results are clearly not
though the average using this
In contrast you can see F 1 would
recall are roughly That seminar, so
it would a case where you had
So this means f one encodes
Now this example shows
Methodology here.
But when you try to solve a problem you
let's say in this it's
But it's important not to
It's important to think whether you
And once you think about the multiple
difference, and then think about
In this case, if you think more carefully,
you will think that F1
Than the simple.
Although in other cases there
But in this case the seems not reasonable.
But if you don't pay attention
you might just take a easy way to
And here later, you will find that,
All right.
So this methodology is actually very
Try to think about the best solution.
Try to understand the problem very well,
know why you needed this measure, and why
And then use that to guide you in
To summarize, we talked about
are there retrievable
We also talk about the Recall.
Which addresses the question, have all of
These two, are the two,
They are used for
We talk about F measure as a way to
We also talked about the tradeoff
And this turns out to depend
we'll discuss this point
[MUSIC]
[MUSIC]
This lecture is about,
In this lecture, we will continue
In particular,
we are going to look at, how we can
In the previous lecture,
These are the two basic measures for,
quantitatively measuring
But, as we talked about, ranking, before,
we framed that the text of retrieval
So, we also need to evaluate the,
How can we use precision-recall
Well, naturally, we have to look after the
Because in the end, the approximation
given by a ranked list, is determined
Right?
the list of results, the user would,
that point would determine the set.
And then,
that we have to consider,
Without knowing where
then we have to consider, all
So, let's look at these positions.
Look at this slide, and
what if the user stops at the,
What's the precision-recall at this point?
What do you think?
Well, it's easy to see, that this document
We have, got one document,
What about the recall?
Well, note that, we're assuming that,
this query in the collection,
What if the user stops
Top two.
Well, the precision is the same,
And, the record is two out of ten.
What if the user stops
Well, this is interesting,
additional relevant document,
But the precision is lower,
what's exactly the precision?
Well, it's two out of three, right?
And, recall is the same, two out of ten.
So, when would see another point,
Now, if you look down the list,
we have, seeing another relevant document.
In this case D5, at that point, the,
three out of ten, and,
So, you can see, if we keep doing this,
And then, we will have
because there are eight documents,
And, the recall is a four out of ten.
Now, when can we get,
Well, in this list, we don't have it,
We don't know, where it is?
But, as convenience, we often assume that,
at all the, the othe,
all the other levels of recall,
So, of course,
the actual position would be higher,
in order to, have an easy way to,
compute another measure called Average
Now, I should also say, now, here you see,
we make these assumptions that
But, this is okay, for
And, this is for the relative comparison,
or actual, actual number deviates
As long as the deviation,
is not biased toward any particular
We can still,
And, this is important point,
When you compare different algorithms,
the key's to avoid any
And, as long as, you can avoid that.
It's okay, for you to do transformation
you can preserve the order.
Okay, so, we'll just talk about,
we can get a lot of precision-recall
So, now, you can imagine,
And, this just shows on the,
And, on the y-axis, we show the precision.
So, the precision line was marked as .1,
Right?
this is, the different, levels of recall.
And,, the y-axis also has,
So, we plot the, these, precision-recall
as points on this picture.
Now, we can further, and
As you'll see,
we assumed all the other, precision
And, that's why, they are down here,
And this, the actual curve probably will
discussed, it, it doesn't matter that
because this would be,
Okay, so, now that we,
how can we compare ranked to back list?
All right, so, that means,
And here, we show, two cases.
Where system A is showing red,
All right, so, which one is better?
I hope you can see,
Why?
see same level of recall here,
the precision point by system A is better,
So, there's no question.
In here, you can imagine, what does the
Well, it has to have perfect,
it has to be this line.
That would be the ideal system.
In general, the higher the curve is,
The problem is that,
This actually happens often.
Like, the two curves cross each other.
Now, in this case, which one is better?
What do you think?
Now, this is a real problem,
Suppose, you build a search engine,
that's shown here in blue, or system B.
And, you have come up with a new idea.
And, you test it.
And, the results are shown in red,
Now, your question is, is your new
Or more, practically,
you're already using, your, in your search
So, should we use system,
This is going to be a real decision,
If you make the replacement, the search
whereas, if you don't do that,
So, what do you do?
Now, if you want to spend more time
And, it's actually very
As I said, it's a real decision that you
search engine, or if you're working, for
Now, if you have thought about this for
a moment, you might realize that,
Now, some users might like a system A,
So, what's the difference here?
Well, the difference is just that,
in the, low level of recall,
There's a higher precision.
But in high recall region,
Now, so, that also means,
cares about the high recall, or
You can imagine, if someone is just going
want to find out something
Well, which one is better?
What do you think?
In this case, clearly, system B is better,
because the user is unlikely
The user doesn't care about high recall.
On the other hand,
where a user is doing you are,
You want to find, whether your idea ha,
In that case, you emphasize high recall.
So, you want to see,
Therefore, you might, favor, system A.
So, that means, which one is better?
That actually depends on users,
So, this means, you may not necessarily
that would accurately
You have to look at the overall picture.
Yet, as I said, when you have
whether you replace ours with another,
then you may have to actually come up with
Or, when we compare many different
one number to compare, them with, so, that
So, for all these reasons, it is desirable
So, how do we do that?
And, that,
So, here again it's
And, one way to summarize
this whole curve,
Right?
There are other ways to measure that,
this particular way of matching
has been used, since a long time ago for
basically, in this way, and
Basically, we're going to take a, a look
And then, look out for the precision.
So, we know, you know,
And, this is another,
Now, this, we don't count to this one,
because the recall level is the same,
this number, and that's precision at
So, we have all these, you know, added up.
These are the precisions
corresponding to retrieving the first
then, the third, that follows, et cetera.
Now, we missed the many relevant
we just, assume,
And then, finally, we take the average.
So, we divide it by ten, and
which is the total number of relevant
Note that here,
Which is a number retrieved
Now, imagine, if I divide by four,
Now, think about this, for a moment.
It's a common mistake that people,
Right, so, if we, we divide this by four,
In fact, that you are favoring a system,
documents, as in that case,
So, this would be, not a good matching.
So, note that this denomina,
the total number of relevant documents.
And, this will basically ,compute
And, this is the standard method,
Note that, it actually combines
But first, you know, we have
we also consider recall, because if missed
All right, so,
And furthermore, you can see this
of a position of a relevant document.
Let's say, if I move this relevant
it would increase this means,
Whereas, if I move any relevant document,
document down, then it would decrease,
So, this is a very good,
because it's a very sensitive to
It can tell, small differences
And, that is what we want,
sometimes one algorithm only works
And, we want to see this difference.
In contrast, if we look at
If we look at this, this whole set, well,
what, what's the precision,
Well, it's easy to see,
So, that precision is very meaningful,
So, that's pretty useful, right?
So, it's a meaningful measure,
But, if we use this measure to
because it wouldn't be sensitive to where
If I move them around the precision
Right.
this is not a good measure for
In contrast, the average precision
It can tell the difference of, different,
a difference in ranked list in,
[MUSIC]
[SOUND]
So average precision is computer for
just one.
one query.
different queries and this is to
Depending on the queries you use you
Right, so
If you use more queries then,
take the average of the average
So how can we do that?
Well, you can naturally.
Think of just doing arithmetic mean as we
always tend to, to think in, in this way.
So, this would give us what's called
In this case,
we take arithmetic mean of all the average
But as I just mentioned in
We call that.
We talked about the different ways
And we conclude that the arithmetic
But here it's the same.
We can also think about the alternative
Don't just automatically assume that,
Let's just also take the arithmetic
these queries.
Let's think about what's
If you think about the different ways,
probably be able to think about
And we call this kind of average a gMAP.
This is another way.
So now, once you think about
Of doing the same thing.
The natural question to ask is,
So.
So, do you use MAP or gMAP?
Again, that's important question.
Imagine you are again
by comparing the ways your old
Now you tested multiple topics.
Now you've got the average precision for
Now you are thinking of looking
You have to take the average.
But which, which strategy would you use?
Now first, you should also think about the
Can you think of scenarios where using
That is they would give different
And that also means depending on
Average of these average positions.
You will get different conclusions.
This makes the question
Right?
Well again, if you look at
Different ways of aggregating
You'll realize in arithmetic mean,
So what does large value here mean?
It means the query is relatively easy.
You can have a high pres,
Whereas gMAP tends to be
And those are the queries that
The average precision is low.
So if you think about the,
those difficult queries,
On the other hand, if you just want to.
Have improved a lot.
Over all the kinds of queries or
easy and you want to make the perfect and
So again, the answer depends on
their pref, their preferences.
So the point that here is to think
the same problem, and then compare them,
And which one makes more sense.
Often, when one of them might
another might make more sense
So it's important to pick out under
As a special case of the mean average
the case where there was precisely
And this happens often, for example,
Where you know a target page, let's
You have one relevant document there,
That's call a "known item search".
In that case,
Or in another application,
maybe there's only one answer.
Are there.
So if you rank the answers,
then your goal is to rank that one
So in this case, you can easily
will basically boil down
That is, 1 over r where r is the rank
So if that document is ranked
then it's 1 for reciprocal rank.
If it's ranked at the,
Et cetera.
And then we can also take a, a average
reciprocal rank over a set of topics, and
that would give us something
It's a very popular measure.
For no item search or, you know,
an problem where you have
Now again here, you can see this
And this r is basically
a user would have to make in order
If it's ranked on the top it's low effort
But if it's ranked at 100
read presumably 100 documents
So, in this sense r is also a meaningful
take the reciprocal of r,
So my natural question here
I imagine if you were to design
of a random system,
You might have thought about
After all,
But, think about if you take a average
Again it would make a difference.
Right, for one single topic, using r or
using 1 over r wouldn't
It's the same.
Larger r with corresponds
But the difference would only show when,
So again, think about the average of Mean
What's the difference?
Do you see any difference?
And would, would this difference
In our conclusion.
And this, it turns out that,
if you think about it, if you want to
then pause the video.
Basically, the difference is,
Again it will be dominated
So what are those values?
Those are basically large values that
That means the relevant items
And the sum that's also the average
Where those relevant documents
in the lower portion of the ranked.
But from a users perspective we care
So by taking this transformation
Here we emphasize more on
You know, think about
it would make a big difference, in 1 over
where and when won't make much
But if you use this there will
let's say 1,000, right.
So this is not the desirable.
On the other hand, a 1 and
So this is yet another case where there
thing and then you need to figure
So to summarize,
Can characterize the overall
And we emphasized that the actual
on how many top ranked results
Some users will examine more.
Than others.
An average person uses a standard measure
It combines precision and recall and
it's sensitive to the rank
[MUSIC]
[MUSIC]
This lecture is about how to evaluate
multiple levels of judgements.
In this lecture, we will continue
We're going to look at how to
when we have multiple
So far we have talked about
that means a document is judged as
But earlier, we also talk about
So we often can distinguish
those are very useful documents,
They are okay, they are useful perhaps.
And further from now, we're adding
So imagine you can have ratings for
Then, you would have
For example, here I show example of three
very relevant, 2 for marginally relevant,
Now, how do we evaluate the search
Obvious that the map doesn't work, average
recall doesn't work,
So let's look at some top ranked
Imagine the user would be mostly
And we marked the rating levels,
for these documents as shown here,
And we call these gain.
And the reason why we call it
that we are infusing is called the NDCG
So this gain, basically,
information a user can obtain by
So looking at the first document,
Looking at the non-relevant document
Looking at the moderator or
document the user would get 2 points,
So, this gain to each of the measures is
perspective.
Of course, if we assume the user
we're looking at the cutoff at 10,
And what's that?
Well, that's simply the sum of these,
So if the user stops after the position 1,
If the user looks at another document,
If the user looks at the more documents,
Of course this is at the cost of
So cumulative gain gives
much total gain the user would have if
Now, in NDCG, we also have another letter
So, why do we want to do discounting?
Well, if you look at this cumulative gain,
which is it did not consider the rank
So for example, looking at this sum here,
and we only know there is 1
1 marginally relevant document,
We don't really care
Ideally, we want these two to be ranked
But how can we capture that intuition?
Well we have to say, well this is 3 here
And that means the contribution
positions has to be
And this is the idea of discounting,
So we're going to to say, well, the first
because the user can be assumed
But the second one,
because there's a small possibility
So we divide this gain by
So log of 2,
And when we go to the third position,
because the normalizer is log of 3,
So when we take such a sum that a lower
that much as a highly ranked document.
So that means if you, for example,
this position, and this one, and then
for example very relevant
Imagine if you put the 3 here,
So it's not as good as if
So this is the idea of discounting.
Okay, so now at this point that we have
measuring the utility of this ranked
So are we happy with this?
Well, we can use this to rank systems.
Now, we still need to do a little bit more
in order to make this measure
And this is the last step, and by the way,
so this is the total sum of DCG,
So the last step is called N,
And if we do that,
So how do we do that?
Well, the idea here is we're
the ideal DCG at the same cutoff.
What is the ideal DCG?
Well, this is the DCG of an ideal ranking.
So imagine if we have 9 documents in
And that means in total we
Then our ideal rank lister would have put
So all these would have to be 3 and
Because that's the best we could
But all these positions would be 3.
Right?
So this would our ideal ranked list.
And then we had computed the DCG for
So this would be given by this
And so this ideal DCG would then
So here.
And this idea of DCG would
So you can imagine now,
compare the actual DCG with the best DCG
Now why do we want to do this?
Well, by doing this we'll map the DCG
So the best value, or the highest value,
That's when your rank list is,
otherwise, in general,
Now, what if we don't do that?
Well, you can see, this transformation,
doesn't really affect the relative
just one topic, because this ideal
so the ranking of systems based on
if you rank them based
The difference however is
Because if we don't do normalization,
different topics will have
For a topic like this one,
the DCG can get really high,
there are only two very relevant documents
Then the highest DCG that
such a topic would not be very high.
So again, we face the problem of
When we take an average,
we don't want the average to be
Those are, again, easy queries.
So, by doing the normalization,
making all the queries contribute
So, this is a idea of NDCG, it's used for
measuring a rank list based on multiple
In a more general way this
that can be applied to any ranked task
And the scale of the judgements
binary not only more than binary they
0, 5 or
And the main idea of this measure,
is to measure the total utility
So you always choose a cutoff and
And it would discount the contribution
And then finally,
it would do normalization to ensure
comparability across queries.
[MUSIC]
[SOUND].
This lecture is about some practical
evaluation of text retrieval systems.
In this lecture, we will continue
We'll cover some practical
in actual evaluation of
So, in order to create
we have to create a set of queries.
A set of documents and
It turns out that each is
First, the documents and
They must represent the real queries and
And we also have to use many queries and
many documents in order to
For the matching of relevant
We also need to ensure that there exists a
If a query has only one, that's
It's not very informative to
using such a query because there's not
So ideally, there should be more
the queries also should represent
In terms of relevance judgments,
complete judgments of all
Yet, minimizing human and
fault, because we have to use human
It's very labor intensive.
And as a result, it's impossible to
all the queries, especially considering
So this is actually a major challenge,
For measures, it's also challenging,
accurately reflect
We have to consider carefully
And then design measures to measure that.
If your measure is not
then your conclusion would be misled.
So it's very important.
So we're going to talk about
One is the statistical significance test.
And this also is a reason why
And the question here is how sure can
doesn't simply result from
So here are some sample results of
System B into different experiments.
And you can see in the bottom,
So the mean, if you look at the mean
of positions are exactly the same
So you can see this is 0.20,
And again here it's also 0.20 and
Yet, if you look at these exact average
If you look at these numbers in detail,
you would feel that you can trust
In the another case, in the other case,
So, why don't you take a look at all these
So, if you look at the average,
we can easily, say that well,
So, after all it's 0.40 and
this is twice as much as 0.20,
But if you look at these two experiments,
You will see that, we've been more
in experiment one.
In this case.
Because these numbers seem to be
Whereas in Experiment 2, we're not sure
after System A is better and
But yet if we look at only average,
So, what do you think?
How reliable is our conclusion,
Now in this case, intuitively,
But how can we quantitate
And this is why we need to do
So, the idea of the statistical
assess the variants across
If there is a big variance,
that means the results could fluctuate
Then we should believe that,
the results might change if we
Right, so this is then not so
if you have c high variance
So let's look at these results
So, here we show two different
One is a sign test where
If System B is better than System A,
When System A is better we
Using this case, if you see this,
We actually have four cases
But three cases of System A is better,
this is almost like a random results,
So if you just take a random
if you use plus to denote the head and
that could easily be the results of just
So, the fact that the average is
We can't reliably conclude that.
And this can be quantitatively
And that basically means
the probability that this result is
In this case, probability is 1.0.
It means it surely is
Now in Willcoxan test,
and we would be not only
we'll be also looking at
But we can draw a similar conclusion,
where you say it's very
To illustrate this, let's think
And this is called a now distribution.
We assume that the mean is zero here.
Lets say we started with
no difference between the two systems.
But we assume that because of random
we might observe a difference.
So the actual difference might
on the right side here, right?
So, and this curve kind of shows
actually observe values that
Now, so if we look at this picture then,
if a difference is observed here, then
the chance is very high that this is
We can define a region of
random fluctuation and
And in this then the observed may
But if you observe a value in this
then the difference is unlikely
All right, so there's a very small
such a difference just because
So in that case, we can then conclude
So System B is indeed better.
So this is the idea of
The takeaway message here is that you
jumping into a conclusion.
As in this case,
There are many different ways of doing
So now, let's talk about the other
as we said earlier,
completely unless it's
So the question is,
in the collection,
And the solution here is Pooling.
And this is a strategy that has been used
So the idea of Pooling is the following.
We would first choose a diverse
These are Text Retrieval systems.
And we hope these methods can help us
So the goal is to pick out
We want to make judgements on relevant
useful documents from users perspectives.
So then we're going to have
The K can vary from systems.
But the point is to ask them to suggest
And then we simply combine
to form a pool of documents for
To judge, so imagine you have many
We take the top-K documents,
Now, of course, there are many
many systems might have retrieved
So there will be some duplicate documents.
And there are also unique documents
So the idea of having diverse
set of ranking methods is to
And can include as many possible
And then, the users would,
the judgments on this data set, this pool.
And the other unjudged the documents are
Now if the pool is large enough,
But if the pool is not very large,
And we might use other
there are indeed other
And such a strategy is generally okay for
comparing systems that
That means if you participate
then it's unlikely that it
because the problematic
However, this is problematic for
evaluating a new system that may
In this case, a new system might
nominated some read only documents
So those documents might be
That's unfair.
So to summarize the whole part of textual
Because the problem is the empirically
don't rely on users, there's no way to
If we have in the property
we might misguide our research or
And we might just draw wrong conclusions.
And we have seen this is
So make sure to get it right for
The main methodology is the Cranfield
And they are the main paradigm used in
not just a search engine variation.
Map and nDCG are the two main
know about and they are appropriate for
You will see them often
Precision at 10 documents is easier
So that's also often useful.
What's not covered is some other
Where the system would mix two,
And then would show
Of course, the users don't see
The users would judge those results or
click on those documents in
In this case then, the search engine
see if one method has contributed
If the user tends to click on one,
then it suggests that
So this is what leverages the real users
It's called A-B Test and
the modern search engines or
Another way to evaluate IR or
textual retrieval is user studies and
I've put some references here
to know more about that.
So, there are three
These are three mini books about
in covering a broad review of
And it covers some of the things
they also have a lot of others to offer.
[MUSIC]
[SOUND]
lecture is about
In this lecture,
we're going to continue the discussion
We're going to look at another kind of
functions than the Vector Space Model
In probabilistic models,
based on the probability that this
In other words, we introduce
This is the variable R here.
And we also assume that the query and
the documents are all observations
Note that in the vector-based models,
here we assume they are the data
And so, the problem of retrieval becomes
In this category of models,
The classic probabilistic model has
which we discussed in in
because its a form is actually
In this lecture,
P class called a language
In particular, we're going to discuss
which is one of the most effective
There was also another line called
which has led to the PL2 function,
it's also one of the most effective
In query likelihood, our assumption
can be approximated by the probability
So intuitively, this probability just
And that is if a user likes document d,
the user enter query q ,in
So we assume that the user likes d,
And then we ask the question about how
from this user?
So this is the basic idea.
Now, to understand this idea,
the basic idea of
So here, I listed some imagined
relevance judgments of queries and
For example, in this line,
it shows that q1 is a query
And d1 is a document
And 1 means the user thinks
So this R here can be also approximated
engine can collect by watching how you
So in this case, let's say
So there's a 1 here.
Similarly, the user clicked on d2 also,
In other words,
On the other hand,
And d4 is non-relevant and then d5 is
And this part, maybe,
So this user typed in q1 and then found
so d1 is actually non-relevant.
In contrast, here we see it's relevant.
Or this could be the same query typed
But d2 is also relevant, etc.
And then here,
Now, we can imagine we
Now we can ask the question,
how can we then estimate
So how can we compute this
Well, intuitively that just means
if we look at all the entries
this particular q, how likely we'll
So basically that just means that
We can first count how many
d as a pair in this table and
we actually have also seen
And then, we just compute the ratio.
So let's take a look at
Suppose we are trying to compute this
What is the estimated probability?
Now, think about that.
You can pause the video if needed.
Try to take a look at the table.
And try to give your
Have you seen that,
we'll be looking at these two pairs?
And in both cases, well,
actually, in one of the cases, the user
So R = 1 in only one of the two cases.
In the other case, it's 0.
So that's one out of two.
What about the d1 and the d2?
Well, they are here, d1 and d2, d1 and d2,
in both cases, in this case, R = 1.
So it's a two out of two and
So you can see with this approach,
we can actually score these documents for
We now have a score for d1,
And we can simply rank them
so that's the basic idea
And you can see it makes a lot of sense,
it's going to rank d2 above
Because in all the cases,
The user clicked on this document.
So this also should show that
a search engine can learn a lot from
This is a simple example
with small amount of entries here we can
These probabilities would give us
might be more relevant or more useful
Now, of course, the problems that we
all the documents and
There would be a lot of unseen documents,
we have only collected the data from the
And there are even more unseen queries
queries will be typed in by users.
So obviously,
it to unseen queries or unseen documents.
Nevertheless, this shows the basic idea
it makes sense intuitively.
So what do we do in such a case when
unseen queries?
Well, the solutions that we have
So in this particular case called
we just approximate this by
p(q given d, R=1).
So in the condition part, we assume that
have seen that the user
And this part shows that
likely the user would
How likely we will see this
So note that here, we have made
Basically, we're going to do, assume that
has something to do with whether
In other words,
And that is a user formulates a query
Where if you just look at this
it's not obvious we
So what I really meant is that
probability to help us score,
probability will have to somehow
conditional probability without
Otherwise we would be having
by making this assumption,
and try to just model how the user
So this is how you can
that we can derive a specific
So let's look at how this model work for
And basically,
what we are going to do in this case
Which of these documents is most
document in the user's mind when
So we ask this question and we quantify
a conditional probability of observing
fact the imaginary relevant
Here you can see we've computed all
The likelihood of queries
Once we have these values,
we can then rank these documents
So to summarize, the general idea
risk model is to assume the we introduce
And then,
let the scoring function be defined
We also talked about approximating
And in this case we have a ranking
based on the probability of
And this probability should be interpreted
likes document d, would pose query q.
Now, the question of course is, how do
At this in general has to do with how
because q is a text.
And this has to do with a model
And these kind of models
So more specifically, we will be
conditional probability
If the user liked this document,
And in the next lecture we're going to do,
giving introduction to language
can model text that was a probable
[MUSIC]
[SOUND] This lecture is about
In this lecture,
we're going to give an introduction
This has to do with how do you model
So it's related to how we model
We're going to talk about
And then we're going to talk about the
language model, which also happens to be
And finally, what this class
What is a language model?
Well, it's just a probability
So here, I'll show one.
This model gives the sequence Today
It give Today Wednesday is a very,
very small probability
You can see the probabilities
sequences of words can vary
Therefore, it's clearly context dependent.
In ordinary conversation,
probably Today is Wednesday is most
Imagine in the context of
maybe the eigenvalue is positive,
This means it can be used to
The model can also be regarded
generating text.
And this is why it's also often
So what does that mean?
We can imagine this is a mechanism that's
visualised here as a stochastic system
So, we can ask for a sequence,
a sequence from the device if you want,
Today is Wednesday, but it could
So for example,
So in this sense,
a sample observed from
So, why is such a model useful?
Well, it's mainly because it can quantify
Where do uncertainties come from?
Well, one source is simply
that we discussed earlier in the lecture.
Another source is because we don't
we lack all the knowledge
In that case,
So let me show some examples of questions
that would have interesting
Given that we see John and feels,
as opposed to habit as the next
Now, obviously, this would be very useful
habit would have similar acoustic sound,
But, if we look at the language model,
we know that John feels happy would be
Another example, given that we
game once in a news article,
This obviously is related to text
Also, given that a user is
how likely would the user
Now, this is clearly related
that we discussed in the previous lecture.
So now,
called a unigram language model.
In such a case,
we assume that we generate a text by
So this means the probability of
the product of
Now normally,
So if you have single word in like
likely to observe model than if
So this assumption is not
we make this assumption
So now the model has precisely N
We have one probability for each word, and
So strictly speaking,
As I said,
drawn from this word distribution.
So for example,
the model to stochastically generate
So instead of giving a whole sequence,
like Today is Wednesday,
And we can get all kinds of words.
And we can assemble these
So that will still allow you
Today is Wednesday as the product
As you can see, even though we have not
it actually allows us to compute
this model now only needs N
That means if we specify
all the words, then the model's
Whereas if we don't make this assumption,
all kinds of combinations
So by making this assumption, it makes it
So let's see a specific example here.
Here I show two unigram language
And these are high probability
The first one clearly suggests
because the high probability
The second one is more related to health.
Now we can ask the question,
how likely were observe a particular
Now suppose we sample
Let's say we take the first distribution,
What words do you think would be
maybe mining maybe another word?
Even food,
which has a very small probability,
But in general, high probability
So we can imagine what general text
In fact, with small probability,
you might be able to actually generate
Now, it will actually be meaningful,
very small.
In an extreme case, you might
a text mining paper that would be
And in that case,
But it's a non-zero probability,
if we assume none of the words
Similarly from the second topic,
we can imagine we can generate
That doesn't mean we cannot generate this
We can, but the probability would be very,
generating a paper that can be accepted
So the point is that
we can talk about the probability of
Some texts will have higher
Now let's look at the problem
Suppose we now have available
In this case, many of the abstract or
we see these word counts here.
The total number of words is 100.
Now the question you ask here
We can ask the question which model,
which one of these distribution has
assuming that the text has been generated
So what would be your guess?
What we have to decide are what
would have.
Suppose the view for a second, and
If you're like a lot of people,
my best guess is text has a probability
seen text 10 times, and
So we simply normalize these counts.
And that's in fact the word justified, and
your intuition is consistent
And this is called the maximum
In this estimator,
of those that would give our observe
That means if we change these
observing the particular text
So you can see,
Basically, we just need to look at
and then divide it by the total number of
Normalize the frequency.
A consequence of this is,
of course, we're going to assign
If we have an observed word,
there will be no incentive to assign a
Why?
Because that would take away probability
And that obviously wouldn't maximize
the probability of this
But one has still question whether
Well, the answer depends on what kind
This estimator gives a best model
But if you are interested in a model
paper for this abstract, then you
So for thing,
of that article, so
even though they're not
So we're going to cover this
in this class in the query
So let's take a look at some possible
One use is simply to use
So here I show some general
We can use this text to
and the model might look like this.
Right, so on the top, we have those
etc., and then we'll see some
then some very,
This is a background language model.
It represents the frequency of
This is the background model.
Now let's look at another text,
we'll look at the computer
So we have a collection of
we do as mentioned again, we can just
where we simply normalize the frequencies.
Now in this case, we'll get
On the top, it looks similar because
they are very common.
But as we go down,
computer science,
And so although here, we might also see
we can imagine the probability here is
And we will see many other words here that
So you can see this distribution
the corresponding text.
We can look at even the smaller text.
So in this case,
Now if we do the same,
again the can be expected
The sooner we see text, mining,
these words have relatively
In contrast, in this distribution, the
So this means, again,
we can have a different model,
So we call this document
we call this collection language model.
And later, you will see how they're
But now,
Can we statistically find what words
Now how do we find such words?
Well, our first thought is that let's take
So we can take a look at all the documents
Let's build a language model.
We can see what words we see there.
Well, not surprisingly, we see these
So in this case, this language model gives
the word in the context of computer.
And these common words will
But we also see the computer itself and
software will have relatively
But if we just use this model,
we cannot just say all these words
So ultimately, what we'd like to
How can we do that?
It turns out that it's possible
But I suggest you think about that.
So how can we know what
so that we want to kind
What model will tell us that?
Well, maybe you can think about that.
So the background language model
It tells us what was
So if we use this background model,
we would know that these words
So it's not surprising to observe
Whereas computer has a very
it's very surprising that we have seen
the same is true for software.
So then we can use these two
the words that are related to computer.
For example, we can simply take the ratio
normalize the topic of language model
the background language model.
So if we do that, we take the ratio,
computer is ranked, and
program, all these words
Because they occur very frequently in the
the whole collection, whereas these common
In fact,
because they are not really
By taking the sample of text
we don't really see more occurrences
So this shows that even with
we can do some limited
So in this lecture,
which is basically a probability
We talked about the simplest language
which is also just a word distribution.
We talked about the two
One is we represent the topic in a
The other is we discover
In the next lecture, we're going to talk
design a retrieval function.
Here are two additional readings.
The first is a textbook on statistical
The second is an article that
language models with a lot of
[MUSIC]
[SOUND]
This lecture is about query likelihood,
probabilistic retrieval model.
In this lecture,
we continue the discussion of
In particular, we're going to talk about
In the query light holder retrieval model,
How like their user who likes a document
So in this case,
particular document about
Now we assume,
a basis to impose a query to try and
So again, imagine use a process
Where we assume that
assembling words from the document.
So for example, a user might
from this document and
And then the user would pick
that would be the second query word.
Now this of course is an assumption
how a user would pose a query.
Whether a user actually followed this
this assumption has allowed us to formerly
And this allows us to also not rely on
to use empirical data to
And this is why we can use this
retrieval function that we can
So as you see the assumption
word is independent of the sample.
And also each word is basically
So now let's see how this works exactly.
Well, since we are completing
then the probability here is just
which is a sequence of words.
And we make the assumption that each
So as a result, the probability
of the probability of each query word.
Now how do we compute
Well, based on the assumption that a word
is picked from the document
Now we know the probability of each word
word in the document.
So for example, the probability of
Would be just the count
divided by the total number of words
So with these assumptions we now have
We can use this to rank our documents.
So does this model work?
Let's take a look.
Here are some example documents
Suppose now the query is
we see the formula here on the top.
So how do we score this document?
Well, it's very simple.
We just count how many times do
how many times do we have seen campaigns,
And we see here 44, and
So that's 2 over the length of
the length of document 4 for
And similarly, we can get probabilities
Now if you look at these numbers or
scoring all these documents,
Because if we assume d3 and
then looks like a nominal rank d4
And as we would expect,
a TF query state, and so
However, if we try a different
presidential campaign update
Well what problem?
Well think about the update.
Now none of these documents
So according to our assumption that a user
generate a query, then the probability of
Would be 0.
So that causes a problem,
to have zero probability
Now why it's fine to have zero probability
It's not okay to have 0 for d3 and
d4 because now we no longer
What's worse?
We can't even distinguish them from d2.
So that's obviously not desirable.
Now when a [INAUDIBLE] has such result,
we should think about what
So we have to examine what
as we derive this ranking function.
Now is you examine those assumptions
what has caused this problem?
So take a moment to think about it.
What do you think is the reason why update
So if you think about this from the moment
have made an assumption
be drawn from the document
So in order to fix this, we have to
a word not necessarily from the document.
So that's the improved model.
An improvement here is to say that,
well instead of drawing
let's imagine that the user would actually
And so I show a model here.
And we assume that this document is
model.
Now, this model doesn't necessarily assign
we can assume this model does not
Now if we're thinking this way then
different.
Now the user has this model in mind
Although the model has to be
So the user can again generate
Namely, pick a word for example,
Now the difference is that this time
even though update doesn't
to potentially generate
So that a query was updated
So this would fix our problem.
And it's also reasonable because when our
in a more general way, that is unique
So how do we compute
If we make this sum wide
The first one is compute this model, and
For example, I've shown two pulse models
And then given a query like a data mining
just compute the likelihood of this query.
And by making independence
probability as a product of
We do this for both documents, and
then rank them.
So that's the basic idea of this
So more generally this ranking function
Here we assume that the query has n words,
w1 through wn, and
The ranking function is the probability
given that the user is
And this is assume it will be product of
This is based on independent assumption.
Now we actually often score
using log of the query likelihood
Now we do this to avoid
having a lot of small probabilities,
And this could cause under flow and we
the value in our algorithm function.
We maintain the order of these documents
And so if we take longer than
the product would become a sum
So the sum of all the query
that is one of the probability of
And then we can further rewrite
So in the first sum here, in this sum,
we have it over all the query words and
And in this sum we have a sum
But we put a counter here
Essentially we are only considering
because if a word is not in the query,
So we're still considering
But we're using a different form as
all the words in the vocabulary.
And of course, a word might occur
That's why we have a count here.
And then this part is log of
given by the document language model.
So you can see in this retrieval function,
we actually know the count
So the only thing that we don't know
Therefore, we have converted
include the problem of estimating
So that we can compute the probability of
And different estimation methods would
This is just like a different way to
which leads to a different ranking
Here different ways to
a different ranking function for
[MUSIC]
[SOUND]
lecture is about smoothing
In this lecture,
we're going to continue talking about
In particular,
language model in the query
So you have seen this slide
This is the ranking function
Here, we assume that the independence of
would look like the following where
And inside the sum there is a log
the document or document image model.
So the main task now is to estimate this
document language model as we
estimating this model would lead
So in this lecture, we're going to
So how do we estimate this language model?
Well the obvious choice would be
that we have seen before.
And that is we're going to normalize
And estimate the probability
This is a step function here.
Which means all of the words that have
the same frequency count will
This is another freedom to count,
Note that for words that have not
they will have 0 probability.
So we know this is just like the model
Where we assume that the use of
a formula to clear it.
And there's no chance of assembling any
we know that's not good.
So how do we improve this?
Well in order to assign
to words that have not been observed in
some probability mass from the words
So for example here, we have to take away
need some extra probability mass for
So all these probabilities must sum to 1.
So to make this transformation and to
by assigning non zero probabilities to
We have to do smoothing and
the estimate by considering
had been asking to write more words for
the document,
If you think about this factor
would be a more accurate than
Imagine you have seen an abstract
Let's say this document is abstract.
If we assume and see words in this
That would mean there's
a word outside the abstract
But imagine a user who is interested
The user might actually
that chapter to use as query.
So obviously,
author would have written
So smoothing of the language
to recover the model for
And then of course,
words that are not
So that's why smoothing is
So let's talk a little more about
The key question here is, what probability
And there are many different
One idea here, that's very useful for
word be proportional to its probability
That means if you don't observe
We're going to assume that its
by another reference language
It will tell us which unseen words
In the case of retrieval,
take the collection language model
That is to say, if you don't
we're going to assume that
would be proportional to the probability
So more formally,
we'll be estimating the probability
If the word is seen in
would be this counted the maximum
Otherwise, if the word is not seen in the
be proportional to the probability
And here the coefficient that offer is to
control the amount of probability
Obviously, all these
alpha sub d is constrained in some way.
So what if we plug in this
query likelihood ranking function?
This is what we will get.
In this formula, we have this
as a sum over all the query words and
those that we have written here as the sum
This is the sum of all
but not that we have a count
So in fact, we are just taking
This is now a common
because of its convenience
So this is as I said,
In our smoothing method,
are not observed in the method would have
Name it's four, this foru.
So we're going to do then,
One sum is over all the query words
That means that in this sum, all the words
have a non zero probability
Sorry, it's the non zero count
They all occur in the document.
And they also have to of course
So these are the query words
On the other hand, in this sum we
that are not all query was
So they occur in the query
they don't occur in the document.
In this case,
these words have this probability because
That here, these seen words
Now, we can go further by
as a difference of two other sums.
Basically, the first sum is
Now, we know that the original sum
This is over all the query words that
So here we pretend that they
So we take a sum over all the query words.
Obviously, this sum has extra
Because, here we're taking
There, it's not matched in the document.
So in order to make them equal, we will
And this is the sum over all the query
And this makes sense, because here
And then we subtract the query
That would give us the query that
And this is almost a reverse
And you might wonder why
Well, that's because if we do this,
then we have different forms
So now, you can see in this sum
the query was matching the document
Here we have another sum over the same set
But inside the sum, it's different.
But these two sums can clearly be merged.
So if we do that, we'll get another form
of the formula that looks like
And note that this is
Because here we combine
some of the query words matching in
And the other sum now is
And these two parts
because these are the probabilities
This formula is very interesting
the match the query terms.
And just like in the vector space model,
of terms that are in the intersection of
So it already looks a little bit
In fact, there's even more similarity
[MUSIC]
[SOUND]
So I showed you how we rewrite the query
like holder which is a function into
of this slide after if we make
the language model based on
Now if you look at this rewriting,
The first benefit is it helps us better
In particular, we're going to show that
with the collection language model would
and length normalization.
The second benefit is that
the query like holder more efficiently.
In particular we see that
is a sum over the match
So this is much better than if we
After we smooth the document the damage
for all the words.
So this new form of the formula is
It's also interesting to note that
the last term here is actually
Since our goal is to
the same query we can ignore this term for
Because it's going to be the same for
Ignoring it wouldn't affect
Inside the sum, we
also see that each matched query
And this weight actually
is very interesting because it
First we can already see it has
just like in the vector space model.
When we take a thought product,
we see the word frequency in
And so naturally this part would
element from the documented vector.
And here indeed we can see it actually
encodes a weight that has similar
I'll let you examine it, can you see it?
Can you see which part is capturing TF?
And which part is
So if want you can pause
So have you noticed that this P sub
in the sense that if a word occurs
then the s made through probability
So this means this term is really
Now have you also noticed that
is actually achieving the factor of IDF?
Why, because this is the popularity
But it's in the denominator, so if the
then the weight is actually smaller.
And this means a popular term.
We actually have a smaller weight and this
Only that we now have
Remember IDF has a logarithm
But here we have something different.
But intuitively it
Interestingly, we also have something
Again, can you see which factor is related
What I just say is that this term
This collection probability,
this term here is actually related
In particular, F of sub d might
So it encodes how much probability
How much smoothing do we want to do?
Intuitively, if a document is long,
then we need to do less smoothing because
We probably have observed all the words
But if the document is short then r of
We need to do more smoothing.
It's likey there are words that have
So this term appears to paralyze
other sub D would tend to be longer
But note that alpha sub d
this may not actually be necessary
The effect is not so clear yet.
But as we will see later, when we
it turns out that they do
Just like in TF-IDF weighting and
document length normalization
So, that's a very interesting
we don't even have to think about
We just need to assume that if we smooth
then we would have a formula that
documents length violation.
What's also interesting that we have
And see we have not heuristically
In fact, you can think about why
You look at the assumptions that
it's because we have used a logarithm
And we turned the product into a sum
that's why we have this logarithm.
Note that if only want to heuristically
IDF weighting, we don't necessary
Imagine if we drop this logarithm,
But what's nice with problem risk modeling
the logarithm function here.
And that's basically a fixed form
really have to heuristically design,
the logarithm the model probably won't
So a nice property of problem risk
assumptions and the probability rules
And the formula would have
And if we heuristically design
end up having such a specific formula.
So to summarize, we talked about the need
Otherwise it would give zero probability
that's not good for
It's also necessary, in general,
the model represent
The general idea of smoothing in retrieval
to give us some clue about which unseen
That is, the probability of an unseen
to its probability in the collection.
With this assumption, we've shown that we
query likelihood that has
document length normalization.
We also see that, through some rewriting,
the scoring of such a ranking function
matched query terms,
But, the actual ranking
automatically by the probability rules and
And like in the vector space model
think about the form of the function.
However, we still need to address
smooth the document and the model.
How exactly we should
model based on the connection
the maximum micro is made of and
[MUSIC]
[SOUND]
This lecture is about the specific
smoothing methods for language models
In this lecture, we will continue
information retrieval, particularly
And we're going to talk about specifically
such a retrieval function.
So this is a slide from a previous
likelihood ranking and smoothing
we add up having a retrieval function
So this is the retrieval function based on
You can see it's a sum of all
And inside its sum is the count
some weight for the term in the document.
We have t of i, the f weight here, and
So clearly if we want to implement this
we still need to figure
In particular, we're going to need to
of a word exactly and how do we set alpha.
So in order to answer this question,
smoothing methods, and
We're going to talk about
The first is simple linear
And this is also called
So the idea is actually very simple.
This picture shows how
language model by using
That gives us word counts normalized by
The idea of using this method
is to maximize the probability
As a result,
in the text, it's going to get
So the idea of smoothing, then,
where this word is not going to have
nonzero probability should
So we can note that network has
So in this approach what we do is we do
likelihood placement here and
is computed by the smoothing parameter
So this is a smoothing parameter.
The larger lambda is,
So by mixing them together,
we achieve the goal of assigning nonzero
So let's see how it works for
For example, if we compute
Now the maximum likelihood
that's going to be here.
But the collection probability is this.
So we'll just combine them
We can also see the word network,
now is getting a non-zero
And that's because the count is
But this part is nonzero, and
Now if you think about this and
sub d in this smoothing
Because that's remember the coefficient
of the word given by the collection
Okay, so
The second one is similar but
linear interpolation.
It's often called Dirichlet Prior,
So again here we face problem
an unseen word like network.
Again we will use the collection
we're going to combine them
The formula first can be seen as
likelihood estimate and
as in the J-M smoothing method.
Only that the coefficient now
but a dynamic coefficient in this form,
where mu is a parameter,
And you can see if we
the effect is that a long document would
Because a long document
therefore the coefficient
And so a long document would have
So this seems to make more sense
Of course,
that the two coefficients would sum to 1.
Now this is one way to
Basically, it means it's a dynamic
There is another way to understand
easier to remember, and
So it's easier to see how we can rewrite
Now in this form we can easily
the maximum likelihood estimate,
So normalize the count
So in this form we can see what we did is
So what does this mean?
Well, this is basically something related
the collection.
And we multiply that by the parameter mu.
And when we combine this
essentially we are adding
We pretend every word has
So the total count would be
the actual count of
As a result, in total we would
Why?
over all the words, then we'll see the
and that gives us just mu.
So this is the total number of
And so
So in this case, we can easily
add this as a pseudocount to this data.
Pretend we actually augment the data
defined by the collection language model.
As a result, we have more counts is that
the total counts for
And as a result, even if a word has zero
count here, then it would still have
So this is how this method works.
Let's also take a look at
So for text again we will
that we actually observe, but
And so the probability of
Naturally, the probability of
And so here you can also see
Can you see it?
If you want to think about it,
But you'll notice that this
So we can see, in this case,
alpha sub d does depend on the document,
because this length
whereas in the linear interpolation,
the J-M smoothing method,
[MUSIC]
[SOUND]
So let's plug in these model masses
into the ranking function to
This is a general smoothing.
So a general ranking function for
you have seen this before.
And now we have a very specific smoothing
So now let's see what what's a value for
And what's the value for p sub c here?
Right, so we may need to decide this
in order to figure out the exact
And we also need to figure
So let's see.
Well this ratio is basically this,
here, this is the probability
and this is the probability
in other words basically 11
this, so it's easy to see that.
This can be then rewritten as this.
Very simple.
So we can plug this into here.
And then here, what's the value for alpha?
What do you think?
So it would be just lambda, right?
And what would happen if we plug in
What can we say about this?
Does it depend on the document?
No, so it can be ignored.
Right?
So we'll end up having this
And in this case you can easy to see,
this a precisely a vector space
a sum over all the matched query terms,
What do you think is a element
Well it's this, right.
So that's our document left element.
And let's further examine what's
Well one plus this.
So it's going to be nonnegative,
it's going to be at least 1, right?
And these, this is a parameter,
And let's look at this.
Now this is a TF.
Now we see very clearly
And the larger the count is,
We also see IDF weighting,
And we see docking the lan's
So all these heuristics
What's interesting that
weighting function automatically
Whereas in the vector space model,
we had to go through those heuristic
And in this case note that
And when you see whether this
All right so what do you think
This is a math of document.
Total number of words,
given by the collection, right?
So this actually can be interpreted
If we're going to draw, a word,
And, we're going to draw as many as
If you do that,
would be precisely given
So, this ratio basically,
The actual count of the word in the
product if the word is in fact following
And if this counter is larger than
this ratio would be larger than one.
So that's actually a very
It's very natural and intuitive,
And this is one advantage of using
where we have made explicit assumptions.
And, we know precisely why
And, why we have these probabilities here.
And, we also have a formula that
does TF-IDF weighting and
Let's look at the,
It's very similar to
In this case,
that's different from
But the format looks very similar.
The form of the function
So we still have linear operation here.
And when we compute this ratio,
one will find that is that
And what's interesting here is that we
We're comparing the actual count.
Which is the expected account of the world
the collection world probability.
So note that it's interesting we don't
lighter in the JMs model.
All right so this of course
So you might wonder, so
Interestingly the docking lens
this would be plugged into this part.
As a result what we get is
this is again a sum over
And we're against the queer,
And you can interpret this as
but this is no longer
Because we have this part,
right?
So that just means if
we have to take a sum over
then do some adjustment of
But it's still, it's still clear
modulation because this lens
a longer document will
And we can also see it has tf here and
Only that this time the form of the
in JMs one.
But intuitively it still implements TFIDF
the form of the function is dictated
assumptions that we have made.
Now there are also
And that is, there's no guarantee
of the formula will actually work well.
So if we look about at this geo function,
rendition for example it's unclear whether
Unfortunately we can see here there
So we do have also the,
So we do have the sublinear
we do not intentionally do that.
That means there's no guarantee that
Suppose we don't have logarithm,
As we discussed before, perhaps
So that's an example of the gap
the relevance that we have to model,
which is really a subject
So it doesn't mean we cannot fix this.
For example, imagine if we did
So we can take a risk and
or we can even add double logarithm.
But then, it would mean that the function
So the consequence of
longer as predictable as
So, that's also why, for example,
still, open channel how to use
better model than the PM25.
In particular how do we use query
that would work consistently
Currently we still cannot do that.
Still interesting open question.
So to summarize this part, we've talked
Jelinek-Mercer which is doing the fixed
Dirichlet Prior this is what add a pseudo
interpolation in that the coefficient
In most cases we can see, by using these
reach a retrieval function where
So they are less heuristic.
Explaining the results also show
Also are very effective and they are
So this is a major advantage
where we don't have to do
Yet in the end that we naturally
doc length normalization.
Each of these functions also has
In this case of course we still need
There are also methods that can be
So overall,
we follow very different strategies
Yet, in the end, we end up uh,with
look very similar to
With some advantages in having
And then, the form dictated
Now, this also concludes our discussion of
And let's recall what
in order to derive the functions
Well we basically have made four
The first assumption is that the relevance
And the second assumption with med is, are
that allows us to decompose
into a product of probabilities
And then,
if a word is not seen,
its probability proportional to
That's a smoothing with
And finally, we made one of these
So we either used JM smoothing or
If we make these four assumptions
to take the form of the retrieval
Fortunately the function has a nice
weighting and document machine and
So in that sense,
these functions are less heuristic
And there are many extensions of this,
you can find the discussion of them in
[MUSIC]
[SOUND]
lecture is about the feedback
So in this lecture, we will continue with
In particular, we're going to talk
This is a diagram that shows
We can see the user would type in a query.
And then, the query would be
search engine, and
These results would be issued to the user.
Now, after the user has
the user can actually make judgements.
So for example, the user says,
this document is not very useful and
Now, this is called a relevance judgment
got some feedback information from
And this can be very useful to the system,
knowing what exactly is
So the feedback module would
also use the document collection
Typically it would involve
the system can now render the results
So this is called relevance feedback.
The feedback is based on relevance
Now, these judgements are reliable but
the users generally don't want to make
So the down side is that it involves
There's another form of feedback
blind feedback,
In this case, we can see once
in fact we don't have to invoke users.
So you can see there's
And we simply assume that the top
Let's say we have assumed
And then, we will then use this
and to improve the query.
Now, you might wonder,
how could this help if we simply
Well, you can imagine these top
similar to relevant documents
They look like relevant documents.
So it's possible to learn some related
In fact, you may recall that we
analyze what association, to learn
And there, what we did is we
all the documents that contain computer.
So imagine now the query
And then, the result will be those
And what we can do then is
They can match computer very well.
And we're going to count
And then, we're going to then use
the terms that are frequent in this set
So if we make a contrast between
is that related to terms
As we have seen before.
And these related words can then be added
And this would help us bring the documents
match other words like program and
So this is very effective for
But of course, pseudo-relevancy
We have to arbitrarily set a cut off.
So there's also something in
In this case,
we don't have to ask
Instead, we're going to observe how the
So in this case we'll look
So the user clicked on this one.
And the user viewed this one.
And the user skipped this one.
And the user viewed this one again.
Now, this also is a clue about whether
And we can even assume that we're
here in this document,
instead of the actual
The link they are saying web search
If the user tries to fetch this
we can assume these displayed
is interesting to you so
And this is called interesting feedback.
And we can, again,
This is a very important
Now, think about the Google and Bing and
they can collect a lot of user
So they would observe what documents
And this information is very valuable.
And they can use this to
So to summarize, we talked about
Relevant feedback where the user
It takes some user effort, but
We talk about the pseudo feedback where
will be relevant.
We don't have to involve the user
actually before we return
And the third is implicit feedback
Where we involve the users, but
the user doesn't have to make
Make judgement.
[MUSIC]
[SOUND]
lecture is about the feedback
In this lecture, we continue talking
Particularly, we're going to talk about
As we have discussed before,
of text retrieval system is removed from
We will have positive examples.
Those are the documents that
be charged with being relevant.
All the documents that
We also have negative examples.
Those are documents known
They can also be the documents
The general method in
feedback is to modify our query vector.
We want to place the query vector in
And what does that mean exactly?
Well, if we think about the query vector
something to the vector elements.
And in general,
Or we might just weight of old terms or
As a result, in general,
We often call this query expansion.
The most effective method in
is called the Rocchio Feedback, which was
So the idea is quite simple.
We illustrate this idea by
of all the documents in the collection and
So now we can see the query
and these are all the documents.
So when we use the query back there and
the most similar documents,
that these documents would be
And these process are relevant documents,
these are relevant documents,
And then these minuses are negative
So our goal here is trying to move
to improve the retrieval accuracy.
By looking at this diagram,
Where should we move the query vector so
that we can improve
Intuitively, where do you
If you want to think more,
If you think about this picture, you can
case you want the query vector to be as
That means ideally, you want to place
Or we want to move the query
Now so what exactly is this point?
Well, if you want these relevant
you want this to be in the center of
Because then if you draw
you'll get all these relevant documents.
So that means we can move the query
all the relevant document vectors.
And this is basically the idea of Rocchio.
Of course, you can consider
we want to move away from
Now your match that we're talking about
away from other vectors.
It just means that we have this formula.
Here you can see this is
this average basically is the centroid
When we take the average of these vectors,
then were computing
Similarly, this is the average of
So it's essentially of
And we have these three parameters here,
They are controlling
When we add these two vectors together,
we're moving the query vector
This is when we add them together.
When we subtracted this part,
we kind of move the query
So this is the main idea
And after we have done this,
we will get a new query vector which
This new query vector,
original query vector toward this
away from the non-relevant value.
Okay, so let's take a look at the example.
This is the example that
Only that I deemed that display
I only showed the vector
We have five documents here and we have
to read in the documents here, right.
And they're displayed in red.
And these are the term vectors.
Now I have just assumed some of weights.
A lot of terms,
Now these are negative arguments.
There are two here.
There is another one here.
Now in this Rocchio method, we first
And so let's see,
the positive documents, we simply just,
We just add this with this one
And then that's down here and
And then we're going to add
then just take the average.
And so we do this for all this.
In the end, what we have is this one.
This is the average vector of these two,
Let's also look at the centroid
This is basically the same.
We're going to take the average
And these are the corresponding
on and so forth.
So in the end, we have this one.
Now in the Rocchio feedback
these with the original
So now let's see how we
Well, that's basically this.
So we have a parameter alpha
query times weight that's one.
And now we have beta to control
centroid of the weight, that's 1.5.
That comes from here.
All right, so this goes here.
And we also have this negative
And this way, it has come from,
And we do exactly the same for
And this is our new vector.
And we're going to use this new query
You can imagine what would happen, right?
Because of the movement that this one
better because we moved
And it's going to penalize these black
So this is precisely what
Now of course if we apply this method in
and that is the original query has
But after we do query explaining and
merging, we'll have many times
So the calculation will
In practice,
only retain the terms
So let's talk about how we
I just mentioned that they're
Consider only a small number of
the centroid vector.
This is for efficiency concern.
I also said here that negative examples,
tend not to be very useful, especially
Now you can think about why.
One reason is because negative documents
directions.
So, when you take the average,
it doesn't really tell you where
Whereas positive documents
And they will point you to
So that also means that sometimes we don't
But note that in some cases, in difficult
negative feedback after is very useful.
Another thing is to avoid over-fitting.
That means we have to keep relatively
Why?
Because the sample that we see in
We don't want to overly
And the original query terms
Those terms are heightened by the user and
the user has decided that those
So in order to prevent
drifting, prevent topic drifting due to
We generally would have to keep a pretty
it was safe to do that.
And this is especially true for
Now, this method can be used for
both relevance feedback and
In the case of pseudo-feedback, the prime
value because the relevant examples
They're not as reliable as
In the case of relevance feedback,
So those parameters,
And the Rocchio Method is
It's still a very popular method for
[MUSIC]
[SOUND]
lecture is about the feedback in
In this lecture, we will continue the
In particular,
we're going to talk about the feedback
So we derive the query likelihood ranking
As a basic retrieval function,
But if we think about the feedback
use query likelihood to perform feedback,
a lot of times the feedback information is
But we assume the query has
from a language model in
It's kind of unnatural to sample
As a result, researchers proposed a way
and it's called Kullback-Leibler
And this model is actually going
retrieval function much
Yet this form of the language model
query likelihood, in the sense that it can
And in this case,
then feedback can be achieved through
This is very similar to Rocchio,
So let's see what is this
So on the top, what you see is a query
And then KL-divergence, or
retrieval model is basically to generalize
the frequency part here
So basically it's the difference given
by the probabilistic model here to
versus the count of query words there.
And this difference allows us to plug in
So this can be estimated
including using feedback information.
But this is called a KL-divergence,
this can be interpreted as matching
One is the query model,
One is the document
smooth them with a collection
And we are not going to talk
you'll find it in some references.
It's also called cross entropy because,
we ignore some terms in
we will end up having
And both are terms of information theory.
But anyway, for our purposes here,
you can just see the two
except that here we have a probability of
And here the sum is over all the words
also with the nonzero probability for
So it's kind of, again, a generalization
Now you can also easily see we can recover
by simply setting this query model to the
This is very easy to
into here you can eliminate this
And then you will get exactly like that.
So you can see the equivalence.
And that's also why this KL-divergence
of query likelihood, because we can cover
But it would also allow us
So this is how we can use the
The picture shows that we first
then we estimate a query language model,
This is often denoted by a D here.
But this basically means this is
because we compute a vector for the
the query, and
Only that these vectors are of special
And then we get the results and
Let's assume they are mostly
although we could also consider
So what we could do is, like in Rocchio,
model called the feedback
Again, this is going to be another vector
in Rocchio.
And then this model can be combined
a linear interpolation, and
just like, again, in Rocchio.
So here we can see the parameter alpha
If it's set to zero,
If it's set to one, we get full feedback
And this is generally not desirable,
So unless you are absolutely sure you
then the query terms are not important.
So of course, the main question here is,
This is the big question here, and
So here we will talk about
there are many approaches, of course.
This approach is based
I'm going to show you how it works.
This will use a generative mixture model.
So this picture shows that
the feedback model that
And the basis is the feedback documents.
Let's say we are observing
These are the clicked documents by users
or are simply top ranked documents
Now imagine how we can
these documents by using language model.
One approach is simply to assume
these documents are generated
As we did before, what we could do
here to here and
Now the question is whether this
Well, you can imagine the top
What do you think?
Well, those words would be common words.
As we always see in a language model,
the top ranked words are actually
So it's not very good for feedback,
words to our query when we interpolate
So this was not good, so
In particular, we are trying to
And we have seen actually one way
language model in the case of
the words that are related
We could do that and that would be
are going to talk about another approach
In this case, we're going to say well,
in these documents that should not
So now what we can do is to assume that,
those words are generated from
they will generate those words like the,
And if we use maximum likelihood estimate,
note that if all the words here
then this model is forced to assign
because it occurs so frequently here.
Note that in order to reduce its
another model, which is this one,
And in this case,
it's not appropriate to use the background
goal because this model would assign high
So in this approach, then,
we assume this machine that was generating
We have a source control up here.
Imagine we flip a coin here to
With probability of lambda,
we're going to use
And we're going to do that in
With probability of 1 minus lambda,
to use a known topic model, here,
And we're going to then
If we make this assumption and this whole
this a mixture model because there are two
And we actually don't know when
So again,
and we can still ask for words and it will
And of course, which word will show up
that distribution.
In addition,
because if you say lambda is very high and
always use the background distribution,
Then if you say, well, lambda is
So all of these
And then if you're thinking this way,
basically we can do exactly
We're going to use maximum likelihood
to estimate the parameters.
Basically we're going to
that we can best explain all the data.
The difference now is that we are not
But rather we are going to ask this whole
Because it has got some help
it doesn't have to assign high
As a result, it will then assign higher
are common here but
So those would be common here.
And if they're common, they would
according to a maximum
And if they are rare here,
much help from this background model.
As a result, this topic model
So the high probability words,
would be those that are common here but
So this is basically a little bit
But this would allow us to achieve the
are meaningless in the feedback.
So mathematically, what we have is
local likelihood,
And note that we also have another
we assume that the lambda denotes
So we are going to,
Let's say 50% of the words are noise or
And this can then be
If we assume this is fixed, then we only
just like in the simple
We have n parameters,
And then the likelihood
It's very similar to the global
except that inside the logarithm
And this sum is because we
And which one is used would depend on
But mathematically, this is the function
So this is just a function.
All the other values are known except for
So we can then choose this
this log likelihood,
the same idea as the maximum likelihood
We just have to solve this
We essentially would try all
that gives this whole thing
So it's a well-defined math problem.
Once we have done that, we obtain this
original query model to the feedback.
So here are some examples of
document collection.
And we do pseudo-feedback we just
we use this mixture model.
So the query is airport security.
What we do is we first retrieve ten
this is of course pseudo-feedback.
And then we're going to feed that
And these are the words
This is the probability of a word given
So in both cases you can see the highest
probability words include the very
So airport security, for example,
these query words still show up as high
because they occur frequently
But we also see beverage,
So these are relevant to this topic,
if combined with original query, can help
And also they can help us bring up
these other words, maybe, for example,
So this is how pseudo-feedback works.
It shows that this model really works and
What's also interesting is that if
you compare them,
when lambda is set to a small value,
And that means, well,
Remember, lambda confuses the probability
to generate the text.
If we don't rely much on background model,
we still have to use this topic model
Whereas if we set lambda
we will use the background model
Then there's no burden on
in the feedback documents
So as a result, the topic model
It contains all the relevant
So this can be added to the original
So to summarize,
in this lecture we have talked about
In general,
These examples can be assumed examples,
like assume the top ten documents
They could be based on user interactions,
like feedback based on clickthroughs or
We talked about the three major
pseudo feedback, and implicit feedback.
We talked about how to use Rocchio to
how to use query model estimation for
And we briefly talked about
There are many other methods.
For example,
the relevance model is a very effective
So you can read more about these
are listed at the end of this lecture.
So there are two additional readings here.
The first one is a book that
discussion of language models for
And the second one is a important research
paper that's about relevance
and it's a very effective way
[MUSIC]
This lecture is about Web Search.
In this lecture,
of the most important applications of
So let's first look at some
opportunities in web search.
Now, many informational
had been developed
So when the web was born,
those algorithms to major application
So naturally, there have to be some
search algorithms to address new
So here are some general challenges.
First, this is a scalability challenge.
How to handle the size of the web and
ensure completeness of
How to serve many users quickly and
And so that's one major challenge and
before the web was born the scale
The second problem is that there's
there are often spams.
The third challenge is
The new pages are constantly create and
so it makes it harder to
So these are some of the challenges
deal with high quality web searching.
On the other hand there are also some
leverage to include the search results.
There are many additional heuristics,
using links that we can
Now everything that we talked about
are general algorithms.
They can be applied to any search
On the other hand, they also don't take
of pages or documents in the specific
Web pages are linked with each other,
the linking is something
So, because of these challenges and
that have been developed for
One is parallel indexing and searching and
this is to address
In particular, Google's imaging of
has been very helpful in that aspect.
Second, there are techniques
addressing the problem of spams,
We'll have to prevent those spam
And there are also techniques
And we're going to use a lot
that it's not easy to spam the search
And the third line of techniques is link
analysis and these are techniques that can
allow us to improve such results
And in general in web searching,
ranking not just for link analysis.
But also exploring all kinds
anchor text that describes
So, here's a picture showing
Basically, this is the web on the left and
we're going to help this user to get
And the first component is a Crawler that
component is Indexer that would take
The third component there is a Retriever
answer user's query by talking
And then the search results will be given
show those results, it allows
So, we're going to talk about
First of all, we're going to talk about
software robot that would do something
To build a toy crawler is relatively easy,
because you just need to start
And then fetch pages from the web and
figure out new links.
And then add them to the priority que and
But to be able to real crawler
there are some complicated issues
For example robustness,
what if there's a trap that generates
that might attract your crawler to
to fetch dynamic generated pages?
The results of this issue
you don't want to overload one particular
you have to respect the robot
You also need to handle different
PDF files,
And you have to also
sometimes those are CGI scripts and
etc, and sometimes you have
they also create challenges.
And you ideally should also recognize
to duplicate those pages.
And finally, you may be interested
Those are URLs that may not be linked
the URL to a shorter path, you might
So what are the Major Crawling Strategies?
In general,
Breadth-First is most common because
You would not keep probing a particular
Also parallel crawling is very
easy to parallelize.
And there is some variations
and one interesting variation
In this case, we're going to crawl just
For example,
And this is typically going to
then you can use the query to get some
And then you can start it with those
The one channel in crawling,
is you will find the new
people probably are creating
And this is very challenging if
linked to any old pages.
If they are, then you can probably find
so these are also some interesting
And finally, we might face the scenario
repeated crawling, right.
Let's say,
and you first crawl a lot
But then,
in the future you just need
In general, you don't have to
It's not necessary.
So in this case, your goal is to
by using minimum resources
So, this is actually a very
and this is a open research question,
standard algorithms established yet
But in general, you can imagine,
So the two major factors that
first will this page
And do I have to quote this page again?
If the page is a static page and
you probably don't have to re-crawl it
will changed frequently.
On the other hand, if it's a sports score
you may need to re-crawl it and
The other factor to consider is,
If it is, then it means that
then thus it's more important to
Compared with another page that has
a year, then even though that
It's probably not that necessary to
not as urgent as to maintain the freshness
So to summarize, web search is one of
retrieval and there are some new
efficiency, quality information.
There are also new opportunities
layout, etc.
A crawler is an essential component
in general, you can find two scenarios.
One is initial crawling and
of the web if you are doing
focused crawling if you want to just
And then, there is another scenario that's
incremental crawling.
In this case,
try to use minimum resource
[MUSIC]
[SOUND]
This lecture is about the Web Indexing.
In this lecture, we will continue
we're going to talk about how
So once we crawl the web,
The next step is to use the indexer
In general, we can use the same
creating an index and that is what we
but there are there are new
For web scale indexing, and the two main
The index would be so large,
that it cannot actually fit into
So we have to store the data
Also, because the data is so
process the data in parallel, so
Now to address these challenges,
One is the Google File System that's
programmers manage files stored
The second is MapReduce.
This is a general software framework for
Hadoop is the most well known open
Now used in many applications.
So, this is the architecture
It uses a very simple centralized
management mechanism to manage
Files, so
look up a table to know where
The application client will then
that obtains specific locations of
And once the GFS file kind obtained
then the application client can talk
data actually sits directly, so
In the network.
So when this file system stores
with great fixed sizes of chunks, so
Many chunks.
Each chunk is 64 MB, so it's pretty big.
And that's appropriate for
These chunks are replicated
So this is something that the programmer
and it's all taken care
So from the application perspective,
the programmer would see this
And the programmer doesn't have to
can just invoke high level.
Operators to process the file.
And another feature is that the data
and chunk servers.
So it's efficient in this sense.
On top of the Google file system, Google
also proposed MapReduce as a general
Now, this is very useful to support
And so, this framework is,
Hiding a lot of low-level
As a result, the programmer can make
that can be run a large
So some of the low level details
the specific and network communications or
where the task are executed.
All these details are hidden
There is also a nice feature which
If one server is broken,
the server is down, and
Then the MapReduce mapper will know
So it automatically dispatches a task
And therefore, again the program
here's how MapReduce works.
The input data would be separated
Now what exactly is in the value
it's actually a fairly general framework
into different parts and each part
Each key value pair would be and
The program was right the map function,
And then the map function will
then generate a number of
Of course, the new key is usually
that's given to the map as input.
And these key value pairs
all the outputs of all the map
and then there will be for
And the result is that,
with the same key will be
So now we've got a pair of of a key and
So this would then be sent
Now, of course, each reduce function
so we will send these output values to
multiple reduce functions
A reduce function would then
a key in a set of values to produce
So these output values would
to form the final output.
And so, this is the general
Now the programmer only needs to write
Everything else is actually taken
So you can see the program really
And with such a framework, the input data
which is processing parallel first by map,
then being the process after
The much more reduced if I'm
the different keys and
So it achieves some,
it achieves the purpose of parallel
So let's take a look at a simple example.
And that's Word Counting.
The input is containing words,
and the output that we want to generate is
So it's the Word Count.
We know this kind of counting
assess the popularity of a word in
achieving a factor of IDF wading for
So how can we solve this problem?
Well, one natural thought is that,
done in parallel by simply counting
and then in the end we just
And that's precisely the idea of
We can parallelize on
So more specifically, we can assume
a key value pair that represents the line
So the first line, for
that is another word by word and
So this key value pair would
The Map Function then would just
And in this case,
Each world gets a count of one and
these are the output that you see here
So the map function is really
what the pseudocode looks
you see it simply needs to iterate
And then just collect the function
which means it would then send the word
The collector would then try to
different Map Functions, right?
So the function is very simple and
this function as a way to
Of course, the second line will be
which we will produce a single output.
Okay, now the output from the map
send it to a collector and the collector
So at this stage, you can see,
Each pair is a word and
So, once we see all these pairs.
Then we can sort them based on the key,
So we will collect all the counts
And similarly, we do that for other words.
Like Hadoop, Hello, etc.
So each word now is attached to
And these counts represent the occurrences
So now we have got a new pair of a key and
this pair will then be fed into reduce
would have to finish the job of counting
Now, it has all ready got all
all it needs to do is
So the reduce function here
You have a counter, and
That you'll see in this array.
And that,
And then finally, you output the P and
And that's precisely what we want as
So you can see,
To building an Invert index.
And if you think about it,
And we have already got a dictionary,
We have got the count.
But what's missing is
frequency counts of words
So we can modify this slightly to
here's one way to do that.
So in this case, we can assume the input
which denotes the document ID,
denoting the screen for that document,
And so, the map function would do
seen in the word campaign example.
It simply groups all the counts of
And it would then generate
Each key is a word, and
the value is the count of this word in
Now, you can easily see why we need to
in inverted index, we would like to
should keep track of it, and this can then
Now similarly another document D2
So in the end, again, there is a sorting
And then we will have just a key,
associated with all the documents
Or all the documents where java occurred.
And the counts, so
And this will be collected together.
And this will be, so
So now you can see the reduce function
an inverted index entry.
So it's just the word and all
the frequencies of the word
So all you need to do is
into a continuous chunk of data.
And this can be done
So basically the reduce function
Work.
And so, this is a pseudo-code for
[INAUDIBLE] that's construction.
Here we see two functions,
And a programmer would specify these two
And you can see basically they
In the case of map, it's going to count
the occurrences of a word
And it would output all the counts
So, this is the reduce function,
simply concatenates all the input
and then put them together as
So this is a very simple
it would allow us to construct an inverted
the data can be processed
And program doesn't have to
So this is how we can do parallel
So to summarize,
web scale indexing requires some
Standard traditional indexing techniques.
Mainly, we have to store
And this is usually done by using a filing
But this should be through a file system.
And secondly, it requires creating
large and takes long time to create
So if we can do it in parallel,
this is done by using
Note that both the GFS and
they can also support
[MUSIC]
[SOUND] This lecture is about
link analysis for web search.
In this lecture, we're going to talk
focusing on how to do link analysis and
The main topic of this lecture is to look
In the previous lecture we talked
Now that we have index, we want to see
The web.
Now standard IR models,
In fact,
improve, for supporting web search.
But they aren't sufficient.
And mainly for the following reasons.
First, on the web, we tend to have
example, people might search for
And this is different from
where people are primarily interested
So this kind of query is often
The purpose is to navigate into
So for such queries we might benefit
Secondly, documents have additional
are web format,
such as the layout, the title,
So this has provided opportunity to use
extra context information of
And finally,
That means we have to consider
the range in the algorithm.
This would give us a more robust way
any spammer to just manipulate the one
So as a result,
people have made a number of major
One line is to exploit
And that's the main topic of this lecture.
People have also proposed algorithms to
Feedback information the form of
in the category of feedback techniques and
In general in web search the ranking
algorithms to combine
Many of them are based on
as BM25 that we talked about [INAUDIBLE]
to provide additional features
but link information
they provide additional scoring signals.
So let's look at links in
So this is a snapshot of some
So we can see there are many links that
And in this case, you can also
a description of a link that's pointing
Now, this description text
Now if you think about this text,
because it provides some extra
So for example, if someone wants
the person might say the biggest
then the link to Amazon, right?
So, the description here after is very
the query box when they are looking for
And that's why it's very useful for
Suppose someone types in
biggest online bookstore.
All right the query would match
And then this actually
matching the page that's being
a entry page.
So if you match anchor text that
actually that provides good evidence for
So anchor text is very useful.
If you look at the bottom part of this
patterns of some links and these links
So for example,
on the right side you'll see this
Now that means many other pages
This shows that this page is quite useful.
On the left side you can see this
many other pages.
So this is a director page
actually see a lot of other pages.
So we can call the first
the second case half page, but this means
One is to provide extra text for matching.
The other is to provide some
to characterize how likely a page is
So people then of course and proposed
Google's PageRank which was the main
is a good example and
popularity, basically to score authority.
So the intuitions here are links
Now think about one page
this is very similar to one
So, of course then,
then we can assume this page
So that's a very good intuition.
Now PageRank is essentially to take
implement with the principal approach.
Intuitively, it is essentially doing
It just improves the simple
One it will consider indirect citations.
So that means you don't just look
You also look at what are those
If those pages themselves have a lot
In some sense,
But if those pages that
being pointed to by other pages they
then well, you don't get that much.
So that's the idea of
All right, so
you can also understand this idea by
If you're cited by let's say ten papers,
are just workshop papers or some papers
So although you've got ten in-links,
are cited by ten papers that themselves
And so in this case where we would
page does that.
The other idea is it's
Assume that basically every page is having
Essentially you are trying to
links that will link all
that you actually get the pseudo
The reason why they want to do that.
Is this will allow them
elegantly with linear algebra technique.
So, I think maybe the best
the PageRank is to think
probability of random surfer
[MUSIC]
[MUSIC]
So let's take a look at this in detail.
So in this random surfing
random surfer would choose
So this is a small graph here.
That's of course, over simplification
But let's say there are four
And let's assume that a random surfer or
And then the random
just randomly jumping to any page or
follow a link and
So if the random surfer is at d1,
then there is some probability that
Now there are two outlinks here,
the other is pointing to d4.
So the random surfer could pick any
But it also assumes that the random so
So the random surfing which decide
simply randomly jump
So if it does that, it would be able
though there's no link you actually,
So this is to assume that
Imagine a random surfer is
then we can ask the question how
would actually reach a particular
That's the average probability of
this probability is precisely
So the page rank score of
probability that the surfer
Now intuitively, this would basically
Because if a page has a lot of inlinks,
then it would have a higher
Because there will be more
follow a link to come to this page.
And this is why the random surfing model
actually captures the ID
Note that it also considers
Because if the page is that point then
That would mean the random surfer would
therefore, it increase
So this is just a nice way to capture
So mathematically, how can we compute this
we need to take a look at how this
So first of all let's take a look
And this is just metrics with
the random surfer would go
So each rule stands for a starting page.
For example, rule one would
to any of the other four pages from d1.
And here we see there are only
So this is because if you look at
There is no link from d1 or d2.
So we've got 0s for the first 2
columns and 0.5 for d3 and d4.
In general, the M in this matrix,
M sub ij is the probability
And obviously for each rule,
because the surfer would have to go to
So this is a transition metric.
Now how can we compute the probability
Well if you look at the surf
we can compute the probability
So here on the left hand side,
visiting page dj at time plus 1,
On the right hand side, you can see
of at page di at time t.
So you can see the subscript
that indicates that's the probability that
So the equation basically,
possibilities of reaching
What are these two possibilities?
Well one is through random surfing and
one is through following a link,
So the first part captures the probability
that the random surfer would reach
And you can see the random
with probability 1 minus
And so
But the main party is realist
that the surfer could have been at time t.
There are n pages so
Inside the sum is a product
One is the probability that the surfer
was at di at time t, that's p sub t of di.
The other is the transition
And so in order to reach this dj page,
the surfer must first be at di at time t.
And then also, would also have to
So the probability is the probability
the probability of going from that
The second part is a similar sum, the only
probability is a uniform
1 over n and
of reaching this page
So the form is exactly the same and
see on why PageRank is essentially assumed
If you think about this 1 over n as
that has all the elements being
Then you can see very clearly
because they are of the same form.
We can imagine there's a different
that uniform metrics where
And in this sense PageRank uses
ensuring that there's no zero entry
Now of course this is the time dependent
Now we can imagine, if we'll compute
the average of probabilities probably
without considering the time index.
So let's drop the time index and
Now this would give us any equations,
each page we have such equation.
And if you look at the what
there are also precisely n variables.
So this basically means,
n equations with n variables and
So basically, now the problem boils
And here, I also show
It's the vector p here equals a matrix or
the transpose of the matrix here and
Now, if you still remember some knowledge
and then you will realize, this is
When multiply the metrics by this vector,
this can be solved by
So because the equations here
on the back are basically
So you'll see the relation between the
And this iterative approach or
we simply start with s
And then we repeatedly
multiplying the metrics
I also show a concrete example here.
So you can see this now.
If we assume alpha is 0.2,
then with the example that
we have the original
That includes the graph, the actual links
metrics, uniform transition metrics
And we can combine them together with
metric that would be like this.
So essentially,
we can imagine now the web looks like
They're all virtual links
The page we're on now would just
then just computed the updating of this
p vector by using this
Now if you rewrite this
terms of individual equations,
And this is basically,
this particular pages and page score.
So you can also see if you want to compute
You basically multiply
and we'll take the third
And that will give us the value for
So this is how we updated the vector
these guys for this.
And then we just revise
set of scores and
So we just repeatedly apply this and
And when the matrix is like this,
it can be guaranteed to converge.
And at that point the we will just have
We typically go to sets of
So interestingly,
also interpreted as propagating
Or if you look at this formula and
can you imagine,
essentially propagating
I hope you will see that indeed,
we can imagine we have values
So we can have values here and
And then we're going to use these
And if you look at the equation here
to combine the scores of the pages that
So we'll look at all the pages
then combine this score and propagate the
To look at the scores that we present
surfer would be visiting the other
And then just do
the probability of reaching this page, d1.
So there are two interpretations here.
One is just the matrix multiplication.
We repeat the multiplying
The other is to just think
these scores repeatedly on the web.
So in practice, the combination of
Because the matrices is fast and there
So that you avoid actually
all those elements.
Sometimes you may also normalize the
different form of the equation, but
The results of this potential
In that case, if a page does not have
these pages would not sum to 1.
Basically, the probability of reaching the
1, mainly because we have lost
One would assume there's some probability
the links, but
And one possible solution is simply to use
and that could easily fix this.
Basically, that's to say alpha would
In that case,
randomly jump to another page
There are many extensions of PageRank, one
Note that PageRank doesn't merely
So we can make PageRank specific however.
So for example,
we can simply assume
The surfer is not randomly
Instead, he's going to jump to only those
For example, if the query is not sports
doing random jumping, it's going
By doing this, then we can buy
And then if you know the current
then you can use this specialized
That would be better than if you
PageRank is also a channel that can be
network analysis particularly for
You can imagine if you compute
social network, where a link
a relation, you would get some
[MUSIC]
[SOUND]
we talked about PageRank as
Now, we also looked at some other examples
So there is another algorithm called HITS,
that going to compute the scores for
The intuitions are pages that are widely
whereas pages that cite many
I think that the most interesting
is it's going to use
to kind of help improve the scoring for
And so here's the idea,
it was assumed that good
That means if you are cited by many
that inquiry says, you're an authority.
And similarly, good hubs are those
So if you pointed to a lot
then your hubs score would be increased.
So then you will have literally reinforced
some good hubs.
And so you have pointed to some good
whereas those authority
improved because they
And this is algorithms is also general it
network analysis.
So just briefly, here's how it works.
We first also construct a matrix, but this
matrix and
So if there's a link there's a 1,
Again, it's the same graph.
And then we're going to
as the sum of the authority scores of
So whether you are hub,
really depends on whether you are pointing
That's what it says in the first equation.
In the second equation,
as a sum of the hub scores of all
So whether you are good authority
pages that are pointing
So you can see this forms
Now, these three questions can be
So what we get here is then the hub
of the adjacency matrix and
and this is basically the first equation.
And similarly, the second equation
vector is equal to the product of
Now, these are just different ways
But what's interesting is that
you can also plug in the authority
So if you do that, you have actually
and you get the equations
The hubs score vector is
by a transpose multiplied
Similarly, we can do a transformation
just the authorities also.
So although we frame the problem
we can actually eliminate one of them to
Now, the difference between this and page
a multiplication of the adjacency
So this is different from page rank.
But mathematically, then we will
So in HITS,
Let's say, 1 for all these values, and
then we would iteratively apply
And this is equivalent to multiply
So the arrows of these is exactly
But here because the adjacency
So what we have to do is after each
this would allow us to
Otherwise they would grow larger and
And if we do that, and
That was the computer, the hubs scores,
And these scores can then be used in
So to summarize in this lecture, we have
In particular,
increase the text
And we also talk about the PageRank and
page anchor as two major
Both can generate scores for web pages
Note that PageRank and
So they have many applications in
[MUSIC]
[MUSIC]
This lecture is about
In this lecture, we are going to
In particular we're going to talk
to combine different features
So the question that we address in
many features to generate a single ranking
In the previous lectures we have talked
We have talked about some retrieval
They can generate a based this course for
And we also talked about the link
that can give additional scores
Now the question now is,
potentially many other
And this will be very useful for
accuracy, but also to improve
So that it's not easy for
a few features to promote a page.
So the general idea of learning
learning to combine this
on different features to generate
So we will assume that the given
we can define a number of features.
And these features can vary from
a score of the document with
a retrieval function such as BM25 or
of punitive commands from a machine or
It can also be a link based score like or
It can be also application of retrieval
Those are the types of descriptions
So, these can all the clues whether
We can even include a feature
has a tilde because this might be
So all these features can then be combined
The question is, of course.
How can we combine them?
In this approach,
that this document isn't relevant to this
So we can hypothesize this
that the probability of relevance
through a particular form of
These parameters can control
the influence of different
Now this is of course just an assumption.
Whether this assumption really
that's they have to empirically
But by hypothesizing that
features in the particular way, we can
the potential more powerful ranking
Naturally the next question is how
How do we know which features
and which features will have lower weight?
So this is the task of training or
in this approach what we will
Those are the data that have
that we already know
We already know which documents should
And this information can be based
this can also be approximated by just
where we can assume the clicked documents
clicked documents are relevant and
So in general with the fit
function to the training data
meaning that we will try to optimize it's
And we can adjust these parameters to see
how we can optimize the performance of
in terms of some measures such as MAP or
So the training date would
Each tuple has three elements, the query,
So it looks very much like our
about in the evaluation
[MUSIC]
[MUSIC]
So now let's take a look at the specific
Now, this is one of the many
it's one of the simplest methods.
And I choose this to explain
So in this approach, we simply assume
respect to a query is related to a linear
Here I used Xi to denote the feature.
So Xi of Q and D is a feature.
And we can have as many
And we assume that these features
And each feature is controlled
and this beta i is a parameter.
That's a weighting parameter.
A larger value would mean the feature
and it would contribute more
This specific form of the function
a transformation of
So this is the probability of relevance.
And we know that the probability of
And we could have just assumed that
this linear combination.
So we can do a linear regression.
But then, the value of this linear
So this transformation
to 1 range to the whole
you can verify it by yourself.
So this allows us then to connect
which is between 0 and 1 to a linear
And if we rewrite this into a probability
So on this equation, now we'll
And on the right hand side,
Now, this form is clearly nonnegative, and
it still involves a linear
And it's also clear that if this value is,
this is actually negative of the linear
If this value here is large,
then it would mean this value is small.
And therefore,
And that's we expect, that basically,
gives us a high value, then
So this is our hypothesis.
Again, this is not necessarily the best
way to connect these features with
So now we have this combination function.
The next task is to
that the function cache will be applied.
But without knowing the beta values,
So let's see how can
All right,
In this example, we have three features.
One is the BM25 score of the document and
One is the PageRank score of the document,
might not depend on the query.
We might have a topic-sensitive PageRank,
Otherwise, the general PageRank
And then we have BM25 score on
Now, these are then the feature values for
And in this case, the document is D1 and
Here's another training instance and
but in this case, it's not relevant.
This is an oversimplified case where
it's sufficient to illustrate the point.
So what we can do is we use
actually estimate the parameters.
Basically, we're going to
of the document based
That is, given that we observed
Can we predict the relevance here?
Now, of course, the prediction would be
And we hypothesize that the probability
features in this way.
So we are going to see, for what values of
What do we mean by predicting
Well, we just mean, in the first case, for
D1 this expression right here
In fact, we'll hope this
Why?
On the other hand,
we hope this value will be small, right.
Why?
Because it's a non-relevant document.
So now let's see how this can
And this is similar to expressing
only that we are not talking about
talking about the probability
So what's the probability
relevant if it has these feature values?
Well, this is just this expression.
We just need to plug in the Xi's.
So that's what we will get.
It's exactly like what we have seen above,
only that we replaced these
So for example, this 0.7 goes to here and
this 0.11 goes to here.
And these are different feature values,
and we combine them in
The beta values are still unknown.
But this gives us the probability
if we assume such a model.
Okay?
And we want to maximize this probability,
What do we do for the second document?
Well, we want to compute the probability
So this would mean we have to
since this expression is actually
So to compute the non-relevance
we just do 1 minus
Okay?
So this whole expression then
predicting these two relevance values.
One is 1 here, one is 0.
And this whole equation
observing a 1 here and observing a 0 here.
Of course, this probability
So then our goal is to adjust
thing reach its maximum,
So that means we're going to compute this.
The beta is just the parameter
maximize this whole likelihood expression.
And what it means is,
we're going to choose betas to
make this also as large as possible,
make this part as small as possible.
And this is precisely what we want.
So once we do the training,
So then this function
Once beta values are known, both this and
So for any new query and new document,
we can simply compute the features for
And then we just use this formula
And this scoring function can be used to
So that's the basic idea
[MUSIC]
[SOUND]
more of the Munster learning algorithms
they generally attempt to direct
Like a MAP or nDCG.
Note that the optimization object or
on the previous slide is not directly
By maximizing the prediction of one or
zero, we don't necessarily optimize
One can imagine that our
And let's say both are around 0.5.
So it's kind of in the middle of zero and
But the ranking can be wrong, so we might
So that won't be good from
even though function, it's not bad.
In contrast, we might have another
around the 0.9, it said.
And by the objective function,
But if we didn't get the order
that's actually a better result.
So these new, more advanced approaches
Of course, then the challenge is
be harder to solve.
And then, researchers have posed
and you can read more of the references at
Now, these learning ranked
So there accounts would be be applied
not just the retrieval problem.
So some people will go
computational advertising,
there are many others that you can
To summarize this lecture we
learning to combine much more
Actually the use of machine learning
in information retrieval has
So for example, the Rocchio feedback
was a machine learning approach
But the most recent use of machine
changes in the environment of
First, it's mostly freedom of
in the form of critical, such as
So the data can provide a lot of
machine learning methods can be
Secondly, it's also freedom by
and this is not only just
features available on the web that can
It's also because by combining them,
of ranking, so this is desired for
Modern search engines all use some
combine many features
this is a major feature of these
The topic of learning to rank is still
and so we can expect to see new results
perhaps.
Here are some additional readings
about how learning to rank at works and
[MUSIC]
[SOUND].
This lecture is about
In this lecture, we're going to talk
of web search and intelligent information
In order to further improve
it's important that to consider
So one particular trend could be to
customized search engines, and they
These vertical search engines can be
the current general search engines
users are a special group of users that
and then the search engine can be
And because of the customization,
So the search can be personalized,
because we have a better
Because of the restrictions with domain,
in handling the documents, because we can
For example, particular words may
So we can bypass the problem of ambiguity.
Another trend we can expect to see,
is the search engine will
It's like a lifetime learning or
very attractive because that means the
As more people are using it, the search
this is already happening,
because the search engines can learn
More users use it, and the quality
the popular queries that are typed in by
so this is sort of another
The third trend might be
bottles of information access.
So search, navigation, and
combined to form a full-fledged
And in the beginning of this course,
These are different modes of information
And similarly, in the pull mode, querying
And in fact we're doing that basically,
We are querying, sometimes browsing,
Sometimes we've got some
Although most of the cases the information
But in the future, you can imagine
multi-mode for information access, and
Another trend is that we might see systems
that try to go beyond the searches
After all, the reason why people want
to make a decision or perform a task.
For example consumers might search for
opinions about products in
choose a good product by, so
support the whole workflow of purchasing
In this era, after the common search
For example, you can sometimes look at the
you can just click on the button to go the
But it does not provide a,
For example, for researchers,
you might want to find the realm in
And then, there's no, not much support for
So, in general, I think,
So in the following few slides, I'll
specific ideas or thoughts that hopefully,
can help you in imagining new
Some of them might be already relevant
In general, we can think about any
information system, as we specified
And so
then we'll able to specify
And I call this
So basically the three questions you
what kind of data are you are managing and
Right there, this would help us
And there are many different ways
how you connect them,
So let me give you some examples.
On the top,
On the left side, you can see different
on the bottom,
Now imagine you can connect
So, for example, you can connect
the support search and
Well, that's web search, right?
What if we connect UIUC employees with
documents to support the search and
If you connect the scientist
to provide all kinds of service,
alert of new random documents or
or provide the task with support or
For example, we might be,
automatically generating
a research paper, and
Right?
we can imagine this would
If we connect the online shoppers
then we can help these people
So we can provide, for example data mining
to compare products, compare sentiment of
decision support to have them
Or we can connect customer service
and, and we can imagine a system
of these emails to find that the major
We can imagine a system we
by automatically generating
Maybe intelligently attach
if appropriate, if they detect that that's
then you might take this opportunity
Whereas if it's a complaint,
automatically generate some
tell the customer that he or she can
All of these are trying to help
So this shows that
It's just only restricted
So this picture shows the trend
it characterizes the, intelligent
You can see in the center, there's
to search a bag of words representation.
That means the current search engines
to users and mostly model
and sees the data through
So it's a very simple approximation of
But that's what the current system does.
It connects these three nodes
it only provides a basic search function
and it doesn't really understand that
Now, I showed some trends to push each
So think about the user node here, right?
So we can go beyond the keyword queries,
and then further model the user
the user's task environment,
Okay, so this is pushing for
And this is a major
in order to build intelligent
On the document side,
go beyond bag of words implementation
This means we'll recognize people's names,
And this is already feasible with
And Google is the reason
If you haven't heard of it,
And once we can get to that level without
it can enable the search engine
In the future we would like to have
knowledge representation where we
then the search engine would
So this calls for
perhaps this is more feasible for
It's easier to make progress
Now on the service side,
we see we need to go beyond the search of
So search is only one way to get access
systems and push and pull so different
But going beyond access,
we also need to help people digest the
and this step has to do with analysis
We have to find patterns or
real knowledge that can
actionable knowledge that can be used for
And furthermore the knowledge
improve productivity in finishing a task,
Right, so this is a trend.
And, and, and so basically,
in the future intelligent information
interactive task support.
Now I should also emphasize interactive
the combined intelligence of the users and
So we, we can get some help
And we don't have to assume the system
user, and the machine can collaborate in
then the combined intelligence
we can minimize the user's overall
So this is the big picture of future
and this hopefully can provide
how to make further innovations
[MUSIC]
[MUSIC]
This lecture is about
So far we have talked about a lot
We have talked about the problem
different methods for ranking,
how to evaluate a search engine, etc.
This is important because we know
the most important applications
And they are the most useful tools
data into a small set
Another reason why we spend so
is because many techniques used in search
Recommender Systems,
And so, overall, the two systems
And there are many techniques
So this is a slide that
when we talked about the two
Pull and the Push.
And we mentioned that recommender
users in the Push Mode, where the systems
the information to the user or
And this often works
stable information need
So a Recommender System is sometimes
it's because recommending useful
filtering out the the useless articles,
and so
And in all the cases the system
usually there's a dynamic source
that you have some knowledge
And then the system would make a decision
about whether this item is
then if it's interesting then the system
So the basic filtering question here is
Will U like item X?
And there are two ways to answer this
And one is look at what items U likes and
then we can see if X is
The other is to look at who likes X,
user looks like a one of those users,
And these strategies can be combined.
If we follow the first strategy and
look at item similarity in the case
then we're talking about a content-based
If we look at the second strategy, then,
we're user similarity and the technique
So, let's first look at
This is what the system would look like.
Inside the system, there will be
knowledge about the user's interests, and
It maintains this profile to keep
then there is a utility function
a nice plan utility
It helps the system decide
And then the accepted documents will
according to the classified.
There should be also an initialization
maybe from a user's specified keywords or
etc., and this would be to feed into
There is also typically a learning
users' feedback over time.
Now note that in this case typical
the system would have a lot more
If the user has taken a recommended item,
this a signal to indicate that
If the user discarded it,
And so such feedback can be a long term
And the system can collect a lot of
this then can then be used
Now what's the criteria for
How do we know this filtering
Now in this case we cannot use the ranking
because we can't afford waiting for
then rank the documents to
And so the system must make
to decide whether the item is
So in other words, we're trying
So in this case,
one common user strategy is to use
So here, I show linear utility function.
That's defined as for example three
you delivered, minus two multiplied by the
So in other words, we could kind of just
treat this as almost in a gambling game.
If you delete one good item,
you gain three dollars but if you deliver
And this utility function
how much money you are get by
And so it's clear that if you want
this strategy should be delivered
and minimize the delivery of bad articles.
That's obvious, right?
Now one interesting question here is
I just showed a three and
But one can ask the question,
So what do you think?
Do you think that's a reasonable choice?
What about the other choices?
So for example, we can have 10 and
What's the difference?
What do you think?
How would this utility function affect
Right, you can think of
(10, -1) + (1, -10), which one do
system to over do it and which one would
If you think about it you will see that
our good document you incur only a small
Intuitively, you would be
And you can try to deliver more in
And then we'll get a big reward.
So on the other hand,
you really don't get such a big prize
On the other hand, you will have
You can imagine that,
the system would be very reluctant
It has to be absolutely
So this utility function has to be
The three basic problems in content-based
first, it has to make
So it has to be a binary decision maker,
Given a text document and
it has to say yes or no, whether this
So that's a decision module, and
module as you have seen earlier and
And we have to initialize the system
text exclusion or
And the third model is
has to be able to learn from limited
counted them from the user about their
If we don't deliver document
be able to know whether
And we had accumulate a lot of documents
All these modules will have to be
So how can we deal with such a system?
And there are many different approaches.
Here we're going to talk about
a search engine for information filtering.
Again, here's why we've spent a lot of
Because it's actually not very hard
information filtering.
So here's the basic idea for
information filtering.
First, we can reuse a lot of
Right, so we know how to score
We're going to match the similarity
a document.
And then we can use a score threshold for
We do retrieval and then we kind of find
apply a threshold to see whether the
And if it's passing the threshold,
we're going to say it's relevant and
Another component that we have to add is,
we had used is the traditional feedback
And we know rock hill can be using for
And, but we have to develop a new
And we need to set it initially and
then we have to learn how to
So here's what the system
generalize the vector-space model for
So you can see the document vector could
already exists in a search engine
And the profile will be treated
the profile vector can be matched with
And then this score would be fed into a
no, and then the evaluation would be based
If it says yes and then the document
And then user could give some feedback.
The feedback information would be
to adjust the vector representation.
So the vector learning is essentially
feedback in the case of search.
The threshold of learning
that we need to talk
[MUSIC]
[SOUND]
There are some interesting challenges
in threshold for
So here I show the historical data that
so you can see the scores and
So the first one has a score of 36.5 and
The second one is not relevant and
Of course, we have a lot of documents for
because we have never
So as you can see here,
we only see the judgements of
So this is not a random sample,
It's kind of biased, so that creates
Secondly, there are in general very little
so it's also challenging for
typically they require more training data.
And in the extreme case at
labeled data as well.
The system there has to make a decision,
so that's a very difficult
Finally, there is also this issue of
Now, this means we also want
space a little bit and
interested in documents that
So in other words, we're going to
by testing whether the user might be
currently are not matching
So how do we do that?
Well, we could lower the threshold
deliver some near misses to the user
to see how the user would
And this is a tradeoff, because on
on the other hand,
because then you will over
So exploitation means you would
Let's say you know the user is
you don't want to deviate that much, but
if you don't deviate at all then you don't
You might miss opportunity to learn
So this is a dilemma.
And that's also a difficulty
Now, how do we solve these problems?
In general, I think one can use the
And this strategy is basically to optimize
just as you have seen
Right, so you can just compute
each candidate score threshold.
Pretend that, what if I cut at this point.
What if I cut at the different scoring
What's utility?
Since these are training data,
and we know that relevant status,
relevant status based on
So then we can just choose the threshold
on the training data.
But this of course, doesn't account for
And there is also the difficulty of
So, in general, we can only get the upper
because the threshold might
So, it's possible that this could
interesting to the user.
So how do we solve this problem?
Well, we generally, and
as I said we can low with this
So here's on particular approach
So the idea is falling.
So here I show a ranked list of all the
far, and
And on the y axis we show the utility,
how you specify the coefficients
we can then imagine, that depending on the
Suppose I cut at this position and
For example,
The optimal point,
when it will achieve the maximum utility
And there is also zero utility threshold.
You can see at this cutoff
What does that mean?
That means if I lower the threshold
The utility would be lower but
So it's not as high as
But it gives us as a safe point
as I have explained, it's desirable
So it's desirable to lower the threshold
So that means, in general, we want to set
Let's say we can use the alpha to control
the deviation from
So you can see the formula of the
of the zero utility threshold and
Now, the question is,
And when should we deviate more
Well, this can depend on multiple factors,
encourage this threshold
up to the zero point, and
we're not going to necessarily reach
Rather, we're going to use other
this specifically is as follows.
So there will be a beta parameter to
threshold and this can be based on can
to the training data let's say, and so
But what's more interesting
Here, and you can see in this formula,
gamma is controlling the inference
of the number of examples
So you can see in this formula as N which
becomes bigger, then it would
In other words, when these very
And that just means if we have seen few
examples we're not sure whether we
So we need to explore but as we have
many that have we feel that we
So this gives us a beta gamma for
The more examples we have seen
So the threshold would be closer
that's the basic idea of this approach.
This approach actually has been working
particularly effective.
And also can work on arbitrary utility
And explicitly addresses
it kind of uses the zero utility
exploration-exploitation tradeoff.
We're not never going to explore
So if you take the analogy of gambling,
you don't want to risk on losing money.
So it's a safe spend, really
And the problem is of course,
the zero utility lower boundary is also
course, more advance in machine learning
solving this problems and
So to summarize, there are two
filtering systems, one is content based,
and the other is collaborative filtering
We've covered content-based
In the next lecture, we will talk
In content-based filtering system,
several problems relative to
And such a system can actually be
by adding a threshold mechanism and
allow the system to learn from
[MUSIC]
This lecture is about
In this lecture we're going to continue
In particular, we're going to look at
You have seen this slide before when
answer the basic question,
In the previous lecture,
we looked at the item similarity,
In this lecture, we're going to
This is a different strategy,
So first, what is collaborative filtering?
It is to make filtering decisions for
individual user based on
And that is to say we will
preferences from that
So the general idea is the following.
Given a user u, we're going to first
And then we're going to
based on the preferences of
Now, the user similarity here can
the preferences on a common set of items.
Now here you can see the exact
We're going to look at the only the
So this means this
It can be applied to any items,
So this approach would work well
First, users with the same interest
Second, the users with similar preferences
So for example, if the interest of
then we can infer the user
So those who are interested in
probably all favor SIGIR papers.
That's an assumption that we make.
And if this assumption is true,
then it would help collaborative
We can also assume that if we see
then we can infer their interest
So in these simple examples,
in many cases such assumption
So another assumption we have to make
number of user preferences
So for example, if you see a lot
those indicate their
And if you have a lot of such data,
filtering can be very effective.
If not, there will be a problem, and
That means you don't have many
the system could not fully take advantage
So let's look at the filtering
So this picture shows that we are,
in general, considering a lot of users and
we're showing m users here, so U1 through.
And we're also considering
Let's say n objects in
And then we will assume that
objects and the user could for
For example, those items could be movies,
then the users would give
So what you see here is that we have
some combinations.
So some users have watched some movies,
they obviously won't be able
some users may actually
So this is in general a small symmetrics.
So many items and
And what's interesting here is we
of an element in this matrix
And that's after the essential question
we assume there's an unknown
That would map a pair of user and
And we have observed the sum
And we want to infer the value
other pairs that don't have
So this is very similar to other
know the values of the function
And we hope to predict the values of
this is a function approximation.
And how can we pick out the function
So this is the setup.
Now there are many approaches
In fact,
reason that there are special
major conference devoted to the problem.
[MUSIC]
[SOUND]
here we're going to talk
And that would be based on
then predicting the rating of and
object by an active user using the ratings
This is called a memory based approach
storing all the user information and
when we are considering a particular
retrieve the rating users or
And then try to use this
to predict the preference of this user.
So here is the general idea and
x sub i j denotes the rating
and n sub i is average rating
So this n i is needed because
we would like to normalize
So how do you do normalization?
Well, we're going to just subtract
Now, this is to normalize these ratings so
that the ratings from different
Because some users might be more generous,
ratings but some others might be
cannot be directly compared with each
So we need to do this normalization.
Another prediction of
by another user or
can be based on the average
So the user u sub a is the user that we
And we now are interested in
So we're interested in knowing how
How do we know that?
Where the idea here is to look at
have liked this object.
So mathematically this is to say
this user on this app object,
combination of the normalized
and in fact here,
But not all users contribute
and this is conjured by the weights.
So this weight controls the inference
of the user on the prediction.
And of course,
the similarity between ua and
The more similar they are,
user ui can make in predicting
So, the formula is extremely simple.
You can see,
And inside the sum we have their ratings,
their normalized ratings
The ratings need to be normalized in
And then these ratings
So you can imagine w of a and i is just
Now what's k here?
Well k is simply a normalizer.
It's just one over the sum of all
So this means, basically, if you consider
we have coefficients of weight that
And it's just a normalization strategy so
in the same range as these ratings
Right?
So this is basically the main idea
collaborative filtering.
Once we make this prediction,
back through the rating that
the user would actually make,
and this is to further
average rating of this user u
This would recover a meaningful rating for
So if this user is generous, then
and when we add that the rating will be
Now when you recommend an item to a user
because you are interested in
that's more meaningful.
But when they evaluate these
they typically assume that
these objects to be unknown and
then you compare the predicted
So, you do have access
But, then you pretend that you don't know,
then you compare your systems
In that case, obviously, the systems
the actual ratings of the user and
Okay so this is the memory based approach.
Now, of course,
if you want to write
you still face the problem of
Once you know the w function, then
So, indeed, there are many different ways
w, and specific approaches generally
So here are some possibilities and
you can imagine there
One popular approach is we use
This would be a sum over
And the formula is a standard
coefficient formula as shown here.
So this basically measures
to all give higher ratings to similar
Another measure is the cosine measure,
vectors as vectors in the vector space.
And then,
compute the cosine of
And this measure has been using the vector
So as you can imagine there are just
In all these cases, note that the user's
on items and we did not actually use
It didn't matter these items are,
they can be books, they can be products,
they can be text documents which
so this allows such approach to be
Now in some newer approaches of course,
we would like to use more
Clearly, we know more about the user,
So in the actual filtering system,
we could also combine that
We could use more context information,
that people are just starting, and
But, this memory based approach has
and it's easy to implement in
a starting point to see if the strategy
So, there are some obvious ways
mainly we would like to improve
And there are some practical
So for example,
What do you do with them?
Well, you can set them to default values
And that would be a simple solution.
But there are advanced approaches that
missing values, and then use predictive
So in fact that the memory based apology
So you get you have iterative approach
prediction and
then you can use the predictive values to
So this is a heuristic
And the strategy obviously would affect
just like any other heuristics would
Another idea which is actually very
have seen in text search is called
Now here the idea is to look at where
If the item is a popular item that
seen [INAUDIBLE] to people interested
interesting but if it's a rare item,
But these two users deal with this
And, that says more
It's kind of to emphasize
on items that are not
[MUSIC]
[SOUND]
to summarize our discussion of
the filtering task for
in some other sense,
So it's easy because
In this case the system takes initiative
The user doesn't really make any effort,
any recommendation is better than nothing.
All right.
items or useless documents.
If you can recommend
users generally will appreciate it,
However, filtering is actually much harder
make a binary decision and you can't
then you're going to see whether
You have to make a decision
Think about news filtering.
As soon as you see the news enough
interesting to the user.
If you wait for a few days, well, even if
the most relevant news, the utility is
Another reason why it's hard
if you think of this
Collaborative filtering, for
example, is purely based on
So if you don't have many ratings there's
And yeah I just mentioned
This is actually a very serious,
But of course there are strategies that
and there are different strategies that
You can use, for example, more user
instead of using the preferences
items give me additional information
And we also talk about two strategies for
One is content-based where
is collaborative filtering where
And they obviously can be
You can imagine they generally
So that would give us a hybrid
And we also could recall that we talked
about push versus pull as two strategies
And recommender system easy to
search engines are serving
Obviously the two should be combined,
The two have a system
with multiple mode information access.
So in the future we could anticipate such
And either,
there are a lot of new algorithms
In particular those new algorithms tend
Now the context here could be
could also be the context of the user.
Items.
The items are not the isolated.
They're connected in many ways.
The users might form
so there's a rich context there
really solve the problem well and
research area where also machine
Here are some additional readings in
the handbook called
has a collection of a lot
can give you an overview
approaches through recommender systems.
[MUSIC]
[NOISE]
This lecture is a summary of this course.
This map shows the major topics
And here are some key
First, we talked about natural
Here the main take-away messages
a foundation for text retrieval, but
the battle of wars is generally the main
And it's often sufficient before
obviously for
a deeper natural language
We then talked about the high
text access and
In pull we talked about
Now in general in future search engines,
to provide a math involved
And now we'll talk about a number of
We talked about the search problem.
And we framed that as a ranking problem.
And we talked about a number
We start with the overview
the probabilistic model and then we talked
We also later talked about
that's probabilistic model.
And here, many take-away message is that
look similar, and
Most important ones are TF-IDF weighting,
And the TF is often transformed through
And then we talked about how to
the main techniques that we talked about,
that we can prepare the system
And we talked about how to do a faster
And we then talked about how to
mainly introduced to
This was a very important
applied to many tasks.
We talked about the major
So, the most important measures for
are MAP, mean average precision,
accumulative gain and also precision and
And we then talked about
And we talked about the Rocchio
the mixture model and
Feedback is a very important
the opportunity of learning from
We then talked about Web search.
And here we talked about how
to solve the scalability issue in that
Then we talked about how to use linking
We talked about page rank and
hits as the major hours is to
We then talked about
This is the use of machine learning
improvement scoring.
Not only that the effectiveness can be
we can also improve the robustness of the.
The ranking function so that it's
It just some features to promote the page.
And finally we talked about
About the some major reactions
in the future in improving the count
And then finally we talked about
these are systems to
And we'll talk about the two approaches,
one is collaborative filtering and
Now, an obvious missing piece
in this picture is the user,
so user interface is also an important
Even though the current search interface
done a lot of studies of user interfaces
And this is the topic to that,
you can learn more by reading this book.
It's an excellent book about all kinds
If you want to know more about
you can also read some additional
In this short course we only
topics in text retrievals and
And these resources provide additional
they give a more thorough treatment of
And a main source is
that you can see a lot of short
or long tutorials.
They tend to provide a lot of
And there a lot of series that
One is information concepts,
One is human langauge technology.
And yet another is artificial
There are also some major journals and
tend to have a lot of research papers
And finally, for more information
tool kits, etc you can check out his URL.
So, if you have not taken the text
specialization series then naturally
As this picture shows,
we generally need two kinds of techniques.
One is text retrieval,
And these techniques will help us
relevant text data, which are actually
Now human plays important role in mining
written for humans to consume.
So involving humans in the process
in this course we have covered
access to the most relevant data.
These techniques are always so
to help provide prominence and
patterns that the user will
So, in general, the user would have
better understand the patterns.
So the text mining cause, or rather,
will be dealing with what to do once
So this is a second step in this
the text data into actionable knowledge.
And this has to do with helping users to
to find the patterns and
In text and such knowledge can
systems to help decision making or
So, if you have not taken that course,
that natural next step would
Thank you for taking this course.
I hope you had fun and
And I look forward to interacting
[MUSIC]
[SOUND]
this lecture we give an overview
First, let's define the term text mining,
The title of this course is
But the two terms text mining, and text
So we are not really going to
we're going to use them interchangeably.
But the reason that we have chosen to use
both terms in the title is because
if you look at the two phrases literally.
Mining emphasizes more on the process.
So it gives us a error rate
Analytics, on the other hand
or having a problem in mind.
We are going to look at text
But again as I said, we can treat
And I think in the literature
So we're not going to really
Both text mining and
want to turn text data into high quality
So in both cases, we
have the problem of dealing with
Turn these text data into something more
And here we distinguish
One is high-quality information,
Sometimes the boundary between
But I also want to say a little bit about
these two different angles of
In the case of high quality information,
concise information about the topic.
Which might be much easier for
For example, you might face
A more concise form of information
of the major opinions about
Positive about,
Now this kind of results are very useful
And so this is to minimize a human effort
The other kind of output
Here we emphasize the utility
knowledge we discover from text data.
It's actionable knowledge for some
For example, we might be able to determine
or a better choice for
Now, such an outcome could be
because a consumer can take the knowledge
So, in this case text mining supplies
But again, the two are not so
we don't necessarily have
Text mining is also
which is a essential component
Now, text retrieval refers to
a large amount of text data.
So I've taught another separate MOOC
Where we discussed various techniques for
If you have taken that MOOC,
And it will be useful To know
of understanding some of
But, if you have not taken that MOOC,
it's also fine because in this MOOC
going to repeat some of the key concepts
But they're at the high level and
they also explain the relation between
Text retrieval is very useful for
First, text retrieval can be
Meaning that it can help
a relatively small amount
Which is often what's needed for
And in this sense, text retrieval
Text retrieval is also needed for
And this roughly corresponds
mining as turning text data
Once we find the patterns in text data, or
actionable knowledge, we generally
By looking at the original text data.
So the users would have to have some text
text data to interpret the pattern or
to verify whether a pattern
So this is a high level introduction
and the relationship between
Next, let's talk about text
Now it's interesting to
generated by humans as subjective sensors.
So, this slide shows an analogy
And between humans as
physical sensors,
So in general a sensor would
It would sense some signal
then would report the signal as data,
For example, a thermometer would watch
then we report the temperature
Similarly, a geo sensor would sense
The location specification, for
example, in the form of longitude
A network sends over
or activities in the network and
Some digital format of data.
Similarly we can think of
That will observe the real world and
And then humans will express what they
So, in this sense, human is actually
sense what's happening in the world and
then express what's observed in the form
Now, looking at the text data in
able to integrate all
And that's indeed needed in
So here we are looking at
And in general we would Be
about our world that
And in general it will be dealing with
And of course the non-text data
And those non-text data can
Numerical data, categorical,
or multi-media data like video or speech.
So, these non text data are often
But text data is also very important,
mostly because they contain
And they often contain
especially preferences and
So, but by treating text data as
we can treat all this data
So the data mining problem is
turn all the data in your actionable
of it to change the real
So this means the data mining problem is
basically taking a lot of data as input
Inside of the data mining module,
we have a number of different
And this is because, for
we generally need different algorithms for
For example,
video data might require computer
And that would facilitate
And we also have a lot of general
to all kinds of data and those algorithms,
Although, for a particular kind of data,
we generally want to also
So this course will cover
are particularly useful for
[MUSIC]
[SOUND]
looking at the text mining problem more
similar to general data mining, except
And we're going to have text mining
into actionable knowledge that
especially for decision making, or
for completing whatever tasks that
Because, in general,
we also tend to have other kinds
So a more general picture would be
And for this reason we might be
non-text data.
And so in this course we're
but we're also going to also touch how do
non-text data.
With this problem definition we
the topics in text mining and analytics.
Now this slide shows the process of
More specifically, a human sensor or
human observer would look at
Different people would be looking at
they'll pay attention to different things.
The same person at different times might
of the observed world.
And so the humans are able to perceive
And that human, the sensor,
And that can be called the Observed World.
Of course, this would be different from
that the person has taken
Now the Observed World can be
entity-relation graphs or
using knowledge representation language.
But in general, this is basically what
And we don't really know what
But then the human would
observed using a natural language,
And the result is text data.
Of course a person could have used
she has observed.
In that case we might have text data of
The main goal of text mining
process of generating text data.
We hope to be able to uncover
Specifically, we can think about mining,
And that means by looking at text data
something about English, some usage
So this is one type of mining problems,
some knowledge about language which
If you look at the picture,
we can also then mine knowledge
And so this has much to do with
We're going to look at what the text
get the essence of it or
about a particular aspect of
For example, everything that has been
a particular entity.
And this can be regarded as mining content
to describe the observed world in
If you look further,
we can mine knowledge about this observer,
So this has also to do with
some properties of this person.
And these properties could
sentiment of the person.
And note that we distinguish
because text data can't describe what the
But the description can be also
in general, you can imagine the text
descriptions of the world plus
So that's why it's also possible to
do text mining to mine
Finally, if you look at the picture
then you can see we can certainly also
Right?
So indeed we can do text mining to
And this is often called
And we want to predict the value
So, this picture basically covered
multiple types of knowledge that
When we infer other
could also use some of the results from
mining text data as intermediate
For example,
after we mine the content of text data we
And that summary could be then used
to help us predict the variables
Now of course this is still generated
but I want to emphasize here that
to generate some features that can help
And that's why here we show the results of
some other mining tasks, including
mining knowledge about the observer,
In fact, when we have non-text data,
data to help prediction, and
In general, non-text data can be very
For example,
changes of stock prices based on
in social media, then this is an example
of using text data to predict
But in this case, obviously,
the historical stock price data would
And so that's an example of
useful for the prediction.
And we're going to combine both kinds
Now non-text data can be also used for
When we look at the text data alone,
we'll be mostly looking at the content
But text data generally also
For example, the time and the location
And these are useful context information.
And the context can provide interesting
For example, we might partition text
because of the availability of the time.
Now we can analyze text data in each
Similarly we can partition text
any meta data that's associated to
So, in this sense,
interesting angles or
And it can help us make context-sensitive
analysis of content or
the opinions about the observer or
We could analyze the sentiment
So this is a fairly general landscape of
In this course we're going to
We actually hope to cover
First we're going to cover
briefly because this has to do
this determines how we can represent
Second, we're going to talk about how to
And word associations is a form of use for
Third, we're going to talk about
And this is only one way to
it's a very useful ways
It's also one of the most useful
Then we're going to talk about
So this can be regarded as one example
And finally we're going to
problems where we try to predict some
So this slide also serves as
And we're going to use
the topics that we'll cover
[MUSIC]
[SOUND]
This lecture is about natural language
content analysis.
Natural language content analysis
So we're going to first talk about this.
And in particular,
natural language processing with
And this determines what algorithms can
We're going to take a look at the basic
And I'm going to explain these concepts
using a similar example
A dog is chasing a boy on the playground.
Now this is a very simple sentence.
When we read such a sentence
about it to get the meaning of it.
But when a computer has to
the computer has to go
First, the computer needs
how to segment the words in English.
And this is very easy,
And then the computer will need
syntactical categories.
So for example, dog is a noun,
And this is called a Lexical analysis.
In particular, tagging these words
is called a part-of-speech tagging.
After that the computer also needs to
these words.
So a and dog would form a noun phrase.
On the playground would be
And there is certain way for
them to create meaning.
Some other combinations
And this is called syntactical parsing, or
syntactical analysis,
The outcome is a parse tree
That tells us the structure
that we know how we can
But this is not semantics yet.
So in order to get the meaning we
these structures into some real world
So dog is a concept that we know,
So connecting these phrases
Now for a computer, would have to formally
So dog, d1 means d1 is a dog.
Boy, b1 means b1 refers to a boy etc.
And also represents the chasing
So, chasing is a predicate here with
three arguments, d1, b1, and p1.
Which is playground.
So this formal rendition of
Once we reach that level of understanding,
For example, if we assume there's a rule
the person can get scared, then we
This is the inferred meaning,
And finally, we might even further infer
what this sentence is requesting,
or why the person who say it in
And so, this has to do with
This is called speech act analysis or
Which first to the use of language.
So, in this case a person saying this
bring back the dog.
So this means when saying a sentence,
So the action here is to make a request.
Now, this slide clearly shows that
a sentence there are a lot of
Now, in general it's very hard for
especially if you would want
This is very difficult.
Now, the main reason why natural
it's because it's designed it will
As a result, for example,
Because we assume all of
there's no need to encode this knowledge.
That makes communication efficient.
We also keep a lot of ambiguities,
And this is again, because we assume we
So, there's no problem with
possibly different things
Yet for
because a computer does not have
So the computer will be confused indeed.
And this makes it hard for
Indeed, it makes it very hard for
every step in the slide
Ambiguity is a main killer.
Meaning that in every step
and the computer would have to
that decision can be very difficult
And in general,
we need common sense reasoning in order
And computers today don't yet have that.
That's why it's very hard for
computers to precisely understand
So here are some specific
Think about the world-level ambiguity.
A word like design can be a noun or
we've got ambiguous part of speech tag.
Root also has multiple meanings,
like in the square of, or
Syntactic ambiguity refers
of a sentence in terms structures.
So for example,
natural language processing can
So one is the ordinary meaning that we
will be getting as we're
So, it's processing of natural language.
But there's is also another
which is to say language
Now we don't generally have this problem,
the structure, the computer would have
Another classic example is a man
And this ambiguity lies in
This is called a prepositional
Meaning where to attach this
Should it modify the boy?
Or should it be modifying, saw, the verb.
Another problem is anaphora resolution.
In John persuaded Bill to buy a TV for
Does himself refer to John or Bill?
Presupposition is another difficulty.
He has quit smoking implies
we need to have such a knowledge in
Because of these problems, the state
techniques can not do anything perfectly.
Even for
we still can not solve the whole problem.
The accuracy that are listed here,
was just taken from some studies earlier.
And these studies obviously have to
the numbers here are not
take it out of the context of the data
But I show these numbers mainly to give
or how well we can do things like this.
It doesn't mean any data set
But, in general, we can do parsing speech
Parsing would be more difficult, but for
phrases correct, we can probably
But to get the complete parse tree
For semantic analysis, we can also do
particularly, extraction of entities and
For example, recognizing this is
this person and
We can also do word sense to some extent.
The occurrence of root in this sentence
Sentiment analysis is another aspect
That means we can tag the senses
it's talking about the product or
Inference, however, is very hard,
any big domain and if it's only
And that's a generally difficult
Speech act analysis is
we can only do this probably for
And with a lot of help from humans
the computers to learn from.
So the slide also shows that
computers are far from being able to
And that also explains why the text
Because we cannot rely on
computational methods to
Therefore, we have to use
A particular statistical machine learning
to try to get as much meaning
And, later you will see
many such algorithms
interesting model from text even though
Meaning of all the natural
[MUSIC]
[SOUND]
So here are some specific examples of what
we can't do today and
part of speech tagging is still
So in the example, he turned off the
the two offs actually have somewhat
categories and also its very difficult
Again, the example, a man saw a boy
be very difficult to parse
Precise deep semantic
For example, to define the meaning of own,
precisely is very difficult in
So the state of the off can
Robust and
general NLP tends to be shallow while
For this reason in this course,
general, shallow techniques for
mining text data and they are generally
So there are robust and
the in category of shallow analysis.
So such techniques have
applied to any text data in
But the downside is that, they don't
For that, we have to rely on
That typically would require
a lot of examples of analysis that would
machine learning techniques and learn from
So in practical applications, we generally
with the general statistical and
These can be applied to any text data.
And on top of that, we're going to use
to use supervised machine learning
especially for those important
to analyze text data more precisely.
But this course will cover
that generally,
So they're practically,
analysis techniques that require a lot of
So to summarize,
is the foundation for text mining.
So obviously, the better we
the better we can do text mining.
Computers today are far from being able
Deep NLP requires common sense
Thus, only working for
large scale text mining.
Shallow NLP based on statistical
is the main topic of this course and
they are generally applicable
They are in some sense also,
In practice,
we'll have humans for
[MUSIC]
This lecture is about the
In this lecture, we are going
to discuss textual
and discuss how natural
allow us to represent text
Let's take a look at this
We can represent this sentence
First, we can always
represent such a sentence
This is true for
when we store them
When we store a natural
as a string of characters,
we have perhaps the most general
since we always use
this approach to
But unfortunately, using
help us to do semantic analysis,
which is often needed
for many applications
The reason is because we're
So as a string,
we're going to keep
and these ASCII symbols.
We can perhaps count what's
the most frequent character
or the correlation
but we can't really
Yet, this is the most
text because we can use
this to represent any
If we try to do
a little bit more natural
by doing word segmentation,
then we can obtain a
but in the form of a
So here we see that
words like a dog is chasing etc.
Now with this level
we certainly can do
and this is mainly because
of human communication
so they are very powerful.
By identifying words, we can for
example easily count what are
the most frequent words in
this document or in
These words can be used to form
topics when we combine
and some words are positive,
some words negative, so we can
also do sentiment analysis.
So representing text data
opens up a lot of interesting
However, this level of
representation is slightly
of characters because in
it's actually not
all the word boundaries
you see text as a sequence of
characters with
So you'll have to rely on
some special techniques
In such a language,
we might make mistakes
So the sequence of
not as robust as
But in English, it's very
easy to obtain this level
so we can do that all the time.
Now, if we go further
to do naturally
we can add a part of speech tags.
Now once we do that,
we can count, for example,
the most frequent
nouns are associated with
So this opens up
a little bit more
for further analysis.
Note that I use a plus sign
representing text as a sequence
we don't necessarily replace
the original word
Instead, we add this as
an additional way of
so that now the data is
of words and a sequence
This enriches the
and thus also enables
If we go further, then we'll
often to obtain
Now this of course,
further open up
of, for example,
the writing styles or
If we go further for
then we might be able to
and we also can recognize
and playground as a location.
We can further analyze
dog is chasing the boy and
Now this will add
relations through
At this level,
then we can do even more
For example, now we
the most frequent person that's
mentioning this whole collection
or whenever you
you also tend to see mentioning
So this is a very
and it's also related to
the knowledge graph that
of that Google is doing as
a more semantic way of
However, it's also less robust
even syntactical analysis
always easy to identify
all the entities with
and we might make mistakes,
and relations are
and we might make mistakes.
So this makes this level of
yet it's very useful.
Now if we move further
then we can have predicates
With inference rules, we can
infer interesting derived
so that's very useful.
But unfortunately,
representation is even less
robust and we can make
mistakes and we can't do
that all the time for
Finally, speech acts would
of repetition of the intent
So in this case,
it might be a request.
So knowing that would
even more interesting
this observer or the author
What's the intention
What's scenarios? What kind
So this is another level
of analysis that would
So this picture shows
we generally see
natural language processing
Unfortunately,
require more human effort,
and they are less accurate.
That means there are mistakes.
So if we add an texts that are at
the levels that are
representing deeper
then we have to
So that also means it's
such deep analysis with
for example, sequence of words.
On the right side,
you'll see the arrow points
As we go down,
we are representation
to knowledge representation
and need for solving
Now this is desirable because as
we can represent text at
we can easily extract
That's the purpose
So there is a trade-off
here between doing
might have errors but would give
us direct knowledge that
Doing shallow analysis, which
is more robust but
give us the necessary deeper
I should also say that
humans and are meant to
So as a result, in
text-mining humans play
they are always in the loop.
Meaning that we should optimize
the collaboration of
So in that sense,
it's okay that computers
to have compute accurately
and the patterns
from text data can be
and humans can
to do more accurate analysis
by providing features
learning programs to make
[SOUND].
So, as we explained the different text
representation tends to
In particular,
more deeper analysis results
And that would open up a more
opportunities and
So, this table summarizes
So the first column shows
The second visualizes the generality
Meaning whether we can do this
all the text data or only some of them.
And the third column shows
And the final column shows some
can be achieved through this
So let's take a look at them.
So as a stream text can only be processed
It's very robust, it's general.
And there was still some interesting
at this level.
For example, compression of text.
Doesn't necessarily need to
Although knowing word boundaries
Word base repetition is a very
It's quite general and
relatively robust, indicating they
Such as word relation analysis,
And there are many applications that can
For example, thesaurus discovery has
And topic and
And there are, for example, people
might be interesting in knowing the major
And this can be the case
And scientists want to know what are the
Or customer service people might want to
customers by mining their e-mail messages.
And business intelligence
understanding consumers' opinions about
products to figure out what are the
And, in general, there are many
applications that can be enabled by
Now, moving down, we'll see we can
By adding syntactical structures,
syntactical graph analysis.
We can use graph mining algorithms
And some applications are related
For example,
stylistic analysis generally requires
We can also generate
And those are features that might help us
categories by looking at the structures
It can be more accurate.
For example,
different categories corresponding
You want to figure out which of
this article, then you generally need
When we add entities and relations,
then we can enable other techniques
answers, or information network and
And this analysis enable
For example,
discovery of all the knowledge and
You can also use this level representation
to integrate everything about
Finally, when we add logical predicates,
that would enable large inference,
And this can be very useful for
integrating analysis of
For example,
extracted the information from text,
A good of example of application in this
is a knowledge assistant for biologists.
And this program that can help a biologist
literature about a research problem such
And the computer can make inferences
about some of the hypothesis that
For example,
then the intelligent program can read the
doing compiling and
And then using a logic system to
to researchers questioning about what
So in order to support
we need to go as far as
Now, this course is covering techniques
And these techniques are general and
robust and that's more widely
In fact, in virtually all the text mining
representation and then techniques that
But obviously all these other
should be combined in order to support
So to summarize,
Text representation determines what
And there are multiple ways to
syntactic structures, entity-relation
And these different
be combined in real applications
For example, even if we cannot
of syntactic structures, we can state
And if we can recognize some entities,
So in general we want to
And when different levels
we can enable a richer analysis,
This course however focuses
Such techniques have also several
robust, so they are applicable
That's a big advantage over
more fragile natural language
Secondly, it does not require
sometimes, it does not
So that's, again, an important benefit,
because that means that you can apply
Third, these techniques are actually
effective form in implications.
Although not all of course
Now they are very effective
are invented by humans as basically
So they are actually quite sufficient for
So that makes this kind of word-based
And finally, such a word-based
by such a representation can be combined
So they're not competing with each other.
[MUSIC]
[SOUND] This lecture is
about the word association
mining and analysis.
In this lecture,
associations of words from text.
Now this is an example of knowledge
we can mine from text data.
Here's the outline.
We're going to first talk about
then explain why discovering such
we're going to talk about some general
In general there are two word
One is called a paradigmatic relation.
The other is syntagmatic relation.
A and B have paradigmatic relation
if they can be substituted for each other.
That means the two words that
would be in the same semantic class,
And we can in general
without affecting
That means we would still
For example, cat and dog, these two
because they are in
And in general,
the sentence would still be a valid
Similarly Monday and
The second kind of relation is
In this case, the two words that have this
So A and B have syntagmatic relation if
a sentence, that means these two
So for example, cat and sit are related
Similarly, car and
they can be combined with
However, in general, we can not
car with drive in the sentence
meaning that if we do that, the sentence
So this is different from
And these two relations are in fact so
generalized to capture basic relations
And definitely they can be
relations of any items in a language.
So, A and B don't have to be words and
And they can even be more complex
If you think about the general
then we can think about the units
Then we think of paradigmatic
are applied to units that tend to occur
or in a sequence of data
So they occur in similar locations
Syntagmatical relation on
co-occurrent elements that tend
So these two are complimentary and
And we're interested in discovering
Discovering such worded
First, such relations can be directly
tasks, and this is because this is part
So if you know these two words
and then you can help a lot of tasks.
And grammar learning can be also
Because if we can learn
then we form classes of words,
And if we learn syntagmatic relations,
the rules for putting together a larger
So we learn the structure and
Word relations can be also very useful for
many applications in text retrieval and
For example, in search and
associations to modify a query,
introduce additional related words into
It's often called a query expansion.
Or you can use related words to
to explore the information space.
Another application is to
automatically construct the top
We can have words as nodes and
A user could navigate from
find information in the information space.
Finally, such word associations can also
For example, we might be interested
negative opinions about the iPhone 6.
In order to do that, we can look at what
a feature word like battery in
Such a syntagmatical
show the detailed opinions
So, how can we discover such
Now, here are some intuitions
Now let's first look at
Here we essentially can take
So here you see some simple
You can see they generally
and that after all is the definition
On the right side you can kind
the context of cat and
I've taken away cat and
that you can see just the context.
Now, of course we can have different
For example, we can look at
part of this context.
So we can call this left context.
What words occur before we see cat or dog?
So, you can see in this case, clearly
You generally say his cat or my cat and
So that makes them similar
Similarly, if you look at the words
which we can call right context,
Of course, it's an extreme case,
And in general,
that can't follow cat and dog.
You can also even look
And that might include all
in sentences around this word.
And even in the general context, you also
So this was just a suggestion
relation by looking at
So, for example,
How similar are context of cat and
In contrast how similar are context
Now, intuitively,
the context of dog would
the context of cat and
That means, in the first case
between the context of cat and
the similarity between context of cat and
because they all not having a paradigmatic
relationship and imagine what words
It would be very different from
So this is the basic idea of what
What about the syntagmatic relation?
Well, here we're going to explore
again based on the definition
Here you see the same sample of text.
But here we're interested in knowing
with the verb eats and
And if you look at the right
you see,
I've taken away the word to its left and
also the word to its
And then we ask the question, what words
And what words tend to
Now thinking about this question
relations because syntagmatic relations
So the important question to ask for
whenever eats occurs,
So the question here has
are some other words that tend
Meaning that whenever you see eats
And if you don't see eats, probably,
So this intuition can help
Now again, consider example.
How helpful is occurrence of eats for
Right.
in a sentence would generally help us
And if we see eats occur in the sentence,
that should increase the chance
In contrast,
how helpful is the occurrence of eats for
Because eats and
knowing whether eats occurred
really help us predict the weather,
So this is in contrast to
This also helps explain that intuition
behind the methods of what
Mainly we need to capture the correlation
So to summarize the general ideas for
discovering word associations
For paradigmatic relation,
And then compute its context similarity.
We're going to assume the words
to have paradigmatic relation.
For syntagmatic relation, we will count
in a context, which can be a sentence,
And we're going to compare
their co-occurrences with
We're going to assume words
relatively low individual occurrences
because they attempt to occur together and
Note that the paradigmatic relation and
are actually closely related
related words tend to have syntagmatic
They tend to be associated
that suggests that we can also do join
So these general ideas can be
And the course won't cover all of them,
we will cover at least some of
discovering these relations.
[MUSIC]
[SOUND]
lecture is about
In this lecture we are going to talk about
association called
By definition,
related if they share a similar context.
Namely, they occur in
So naturally our idea of discovering such
of each word and then try to compute
So here is an example of
Here I have taken the word
you can see we are seeing some remaining
Now, we can do the same thing for
So in general we would like to capture
the similarity of the context of cat and
So now the question is how can we
then define the similarity function.
So first, we note that the context
So, they can be regarded as
document, but there are also different
For example, we can look at the word
We can call this context Left1 context.
All right, so in this case you
big, a, the, et cetera.
These are the words that can
So we say my cat, his cat,
Similarly, we can also collect the words
We can call this context Right1, and
here we see words like eats,
Or, more generally,
we can look at all the words in
Here, let's say we can take a window
We call this context Window8.
Now, of course, you can see all
so we'll have a bag of words in
Now, such a word based representation
an interesting way to define the
Because if you look at just
then we'll see words that share
and we kind of ignored the other words
So that gives us one perspective to
if we only use the Right1 context,
we will capture this narrative
Using both the Left1 and
the similarity with even
So in general, context may contain
my, that you see here, or
Tuesday, or
And this flexibility also allows us
different ways.
Sometimes this is useful,
as we might want to capture
That would give us loosely
Whereas if you use only the words
to the right of the word, then you
much related by their syntactical
So the general idea of discovering
is to compute the similarity
So here, for example,
dog based on the similarity
In general, we can combine all
And so the similarity function is,
a combination of similarities
And of course, we can also assign
similarities to allow us to focus
And this would be naturally
here the main idea for discovering
to computer the similarity
So next let's see how we exactly
Now to answer this question,
representation as vectors
Now those of you who have been
textual retrieval techniques would
been used frequently for
But here we also find it convenient
paradigmatic relation discovery.
So the idea of this
word in our vocabulary as defining one
So we have N words in
then we have N dimensions,
And on the bottom, you can see a frequency
and here we see where eats
ate occurred 3 times, et cetera.
So this vector can then be placed
So in general,
context of cat as one vector,
dog, might give us a different context,
And then we can measure
So by viewing context in
we convert the problem of
into the problem of computing
So the two questions that we
how to compute each vector, and
And the other question is how
Now in general, there are many approaches
most of them are developed for
And they have been shown to work well for
matching a query vector and
But we can adapt many of
of context documents for our purpose here.
So let's first look at
where we try to match
the expected overlap of words,
So the idea here is to represent
where each word has a weight
that a randomly picked word from
So in other words,
account of word wi in the context, and
this can be interpreted as
actually pick this word from d1
Now, of course these xi's would sum to one
and this means the vector is
actually probability of
So, the vector d2 can be also
this would give us then two probability
So, that addresses the problem
next let's see how we can define
Well, here, we simply define
vectors, and
of the corresponding
Now, it's interesting to see
actually has a nice interpretation,
Dot product, in fact that gives
randomly picked words from
That means if we try to pick a word
word from another context, we can then
If the two contexts are very similar,
see the two words picked from
If they are very different,
identical words being picked from
So this intuitively makes sense, right,
Now you might want to also take
see why this can be interpreted
two randomly picked words are identical.
So if you just stare at the formula
then you will see basically in each
we will see an overlap on
And where xi gives us a probability that
and yi gives us the probability
And when we pick the same
then we have an identical pick, right so.
That's one possible approach, EOWC,
Now as always, we would like to assess
Now of course, ultimately we have to
see if it gives us really
Really give us paradigmatical relations,
analytically we can also analyze
So first, as I said,
formula will give a higher score if there
So that's exactly what we want.
But if you analyze
then you also see there might
and specifically there
First, it might favor matching
over matching more distinct terms.
And that is because in the dot product,
element is shared by both contexts and
it might indeed make the score
where the two vectors actually have
But each term has a relatively low
Of course, this might be
But in our case, we should intuitively
more different terms in the context,
in saying that the two words
If you only rely on one term and
that's a little bit questionable,
Now the second problem is that it
So if you match a word like the and
it will be the same as
intuitively we know
surprising because the occurs everywhere.
So matching the is not as such
a word like eats,
So this is another
In the next chapter we are going to talk
[MUSIC]
In this lecture, we continue
discussing Paradigmatical
Earlier we introduced
Expected Overlap of
In this method, we
a word vector that represents
the probability of a
We measure the similarity
which can be interpreted as
randomly picked words from
the two contexts are identical.
We also discussed
The first is that
one frequent term very well over
matching more distinct terms.
It put too much emphasis on
The second is that it
Even a common word like
equally as content
So now we are
going to talk about how
More specifically, we're
some retrieval heuristics
These heuristics can effectively
as these problems also
when we match a query that
So to address the first problem,
we can use a sublinear
That is, we don't have to use
the raw frequency count of
a term to represent the context.
We can transform
that wouldn't emphasize so
To address the
we can put more weight
That is we can reward
This heuristic is called the IDF
term weighting in text retrieval.
IDF stands for
So now, we're going to talk about
the two heuristics
First let's talk about
That is to convert
a word in the document
that reflects our belief
about how important
So that will be
That's shown in the y-axis.
Now, in general, there are
Let's first look at
In this case, we're
any non-zero counts
one and the zero count
So with this mapping
all the frequencies will be
mapped to only two
The mapping function is shown
Now, this is naive
because it's not
However, this actually
emphasizing matching all
So it does not allow
a frequency of word to
Now, the approach
earlier in the expected
is a linear transformation.
We basically, take
So we use the raw count
That created the problem
that we just talked about namely;
it emphasize too much on just
Matching one frequent term
So we can have a lot
of other interesting
in between the two extremes,
and they generally form
So for example,
logarithm of the raw count,
and this will give us curve
that you are seeing here.
In this case, you can see
The high counts are
so the curve is a sublinear
the weight of
This is what we want,
terms from dominating
Now, there is also
another interesting
a BM25 transformation which
has been shown to be very
In this transformation, we have
a form that looks like this.
So it's k plus one multiplied
where k is a parameter,
x is the count,
the raw count of a word.
Now, the transformation
that it can actually go from
one extreme to the other
k. It also interesting
k plus one in this case.
So this puts
on high frequency terms,
because their weight would
As we vary k, if we can
So when k is set to zero,
we roughly have the 0,1 vector.
Whereas when we set k
it will behave more like
So this transformation
far the most effective
text retrieval and it also makes
sense for our problem setup.
So we just talked about how
overemphasizing a frequency term
Now let's look at
and that is how we can
Matching "the" is not surprising,
because "the" occurs everywhere.
But matching "eats"
So how can we address
Now in this case, we can
That's commonly
IDF stands for
Document frequency
of the total number of
documents that contain
So here we show that the IDF
a logarithm function
of documents that match a
So K is the number of
document frequency and M
here is the total number of
The IDF function is giving
meaning that it
The maximum value is
That's when the word occurred
So that's a very rare term,
the rare is term in
The lowest value you can
its maximum which would be M.
So that would be
close to zero in fact.
So this of course measure
is used in search where we
In our case, what would
Well, we can also
we can collect all the words
That is to say,
a word that's popular in
would also have a low IDF.
Because depending on the dataset,
we can construct the context
But in the end if a term is
very frequent in
then it will still be frequent
in the collective
So how can we add
improve our similarity function?
Well, here's one way
many other ways
But this is a reasonable way,
where we can adapt
for paradigmatical
In this case, we define the
elements representing
So in this
we take sum over all
normalize the weight of
of the weights of all the words.
This is to again ensure all the
xi's will sum to
So this would be very similar
in that this vector is
actually something similar
all the xi's will sum to one.
Now, the weight of BM25 for
If you compare this with
have a normalized count
So we only have this one
the total counts of words in
and that's what we had before.
But now with the BM25
we introduced something else.
First, of course,
this count is just to
achieve the sub-linear
But we also see we introduced
and this parameter is
although zero is also possible.
But this controls
and also controls to what extent
it simulates the
So this is one parameter,
but we also see there is
b, and this would be
This is a parameter to
In this case,
the normalization formula has
a average document lens here.
This is computed up
of the lenses of all the
In this case, all the lenses of
all the context of documents
So this average documents
will be a constant for
So it actually is only
affecting the effect
b, here because
But I kept it here because
for in retrieval where it would
give us a stabilized
But for our purpose,
this will be a constant so
the lens normalization
Now, with this definition then,
we have a new way to define
and we can compute
The difference is that
the high-frequency terms will now
have a somewhat lower weights.
This would help us control
the inference of
Now, the idea can be added
That means we'll
for matching each term.
So you may recall
all the possible words
overlap between the two contexts.
The x_i and the y_i
of picking the word
Therefore, it
we'll see a match on this word.
Now, IDF would give us
the importance of
A common word will be worth
So we emphasize more on
So with this modification,
then the new function will
likely address
Now, interestingly
this approach to discover
In general, when we re-brand
a context with a term vector,
we would likely see
some terms have high weights
and other terms have low weights.
Depending on how we assign
we might be able to
discover the words that
are strongly associated with
the candidate word
So let's take a look at
the term vector in
We have each x_i
defined as the normalized
Now, this weight alone only
reflects how frequent the word
But we can't just say
any frequent term in
the context that would
the candidate word because
many common words like 'the' will
occur frequently in
But if we apply IDF
we can then re-weight
That means the words that are
common like 'the'
So now the highest
those common terms because
Instead, those terms would
be the terms that are
but not frequent
So those are clearly the words
the context of the candidate
So for this reason,
the highly weighted terms in
can also be assumed to
be candidates for
Now, of course, this is
our approach for discovering
In the next lecture, we're
how to discover
But it clearly shows the relation
between discovering
Indeed they can be discovered in
a joint manner by leveraging
So to summarize,
paradigmatic relations is to
collect the context of
a candidate word to
This is typically represented
Then compute the similarity of
the corresponding
of two candidate words.
Then we can take
and treat them as having
These are the words that
There are many different ways to
implement this general idea.
We just talked about
More specifically, we
text retrieval models to help us
design effective
compute the
More specifically, we have used
the BM25 and IDF weighting
to discover
These approaches also represent
the state of the art in
Finally, syntagmatic relations
as a by-product when we discover
[SOUND].
This lecture is about the syntagmatic
In this lecture, we're going to continue
In particular, we're going to talk about
And we're going to start with
which is the basis for designing some
By definition,
syntagmatic relations hold between words
That means,
we tend to see the occurrence
So, take a more specific example, here.
We can ask the question,
whenever eats occurs,
Looking at the sentences on the left,
together with eats, like cat,
But if I take them out and
only show eats and some other words,
Can you predict what other words
Right so
other words are associated with eats.
If they are associated with eats,
More specifically our
any text segment which can be a sentence,
And then ask I the question,
absent in this segment?
Right here we ask about the word W.
Is W present or absent in this segment?
Now what's interesting is that
some words are actually easier
If you take a look at the three
unicorn, which one do you
Now if you think about it for
the is easier to predict because
So I can just say,
Unicorn is also relatively easy
And I can bet that it doesn't
But meat is somewhere in
And it makes it harder to predict because
or the segment, more accurately.
But it may also not occur in the sentence,
now let's study this
So the problem can be formally defined
as predicting the value of
Here we denote it by X sub w,
this random variable is associated
When the value of the variable is 1,
When it's 0, it means the word is absent.
And naturally, the probabilities for
because a word is either present or
There's no other choice.
So the intuition with this concept earlier
The more random this random variable is,
Now the question is how does one
a random variable like X sub w?
How in general, can we quantify
that's why we need a measure
this measure introduced in information
There is also some connection
that is beyond the scope of this course.
So for
as a function defined
In this case, it is a binary random
be easily generalized for
Now the function form looks like this,
there's the sum of all the possible
Inside the sum for each value we
that the random variable equals this
And note that there is also
Now entropy in general is non-negative.
And that can be mathematically proved.
So if we expand this sum, we'll see that
Where I explicitly plugged
And sometimes when we have 0 log of 0,
we would generally define that as 0,
So this is the entropy function.
And this function will
different distributions
And it clearly depends on the probability
that the random variable
If we plot this function against
the probability that the random
And then the function looks like this.
At the two ends,
equals 1 is very small or very large,
When it's 0.5 in the middle
Now if we plot the function
is taking a value of 0 and the function
would show exactly the same curve here,
And so that's because
the two probabilities are symmetric,
So an interesting question you
what kind of X does entropy
And we can in particular think
For example, in one case,
always takes a value of 1.
The probability is 1.
Or there's a random variable that
is equally likely taking a value of one or
So in this case the probability
Now which one has a higher entropy?
It's easier to look at the problem
using coin tossing.
So when we think about random
it gives us a random variable,
It can be head or tail.
So we can define a random variable
when the coin shows up as head,
So now we can compute the entropy
And this entropy indicates how
of a coin toss.
So we can think about the two cases.
One is a fair coin, it's completely fair.
The coin shows up as head or
So the two probabilities would be a half.
Right?
Another extreme case is
where the coin always shows up as heads.
So it's a completely biased coin.
Now let's think about
And if you plug in these values you can
For a fair coin we see the entropy
For the completely biased coin,
And that intuitively makes a lot of sense.
Because a fair coin is
Whereas a completely biased
We can always say, well, it's a head.
Because it is a head all the time.
So they can be shown on
So the fair coin corresponds to the middle
The completely biased coin
point where we have a probability
So, now let's see how we can use
Let's think about our problem is
absent in this segment.
Again, think about the three words,
Now we can assume high entropy
And so we now have a quantitative way to
Now if you look at the three words meat,
we clearly would expect meat to have
In fact if you look at the entropy of the,
Because it occurs everywhere.
So it's like a completely biased coin.
Therefore the entropy is zero.
[MUSIC]
[SOUND] This lecture is
relation discovery and
In this lecture,
we're going to continue the discussion
We're going to talk about the conditional
discovering syntagmatic relations.
Earlier, we talked about
how easy it is to predict the presence or
Now, we'll address
we assume that we know something
So now the question is, suppose we know
How would that help us
absence of water, like in meat?
And in particular, we want to
has helped us predict
And if we frame this using entrophy,
that would mean we are interested
the presence of eats could reduce
Or, reduce the entrophy
corresponding to the presence or
We can also ask as a question,
Would that also help us predict
These questions can be
concept called a conditioning entropy.
So to explain this concept, let's first
when we know nothing about the segment.
So we have these probabilities indicating
or it doesn't occur in the segment.
And we have an entropy function that
Now suppose we know eats is present, so
now we know the value of another
Now, that would change all
conditional probabilities.
Where we look at the presence or
given that we know eats
So as a result,
if we replace these probabilities
probabilities in the entropy function,
So this equation now here would be
the conditional entropy.
Conditional on the presence of eats.
So, you can see this is essentially
seen before, except that all
And this then tells us
after we have known eats
And of course, we can also define
the scenario where we don't see eats.
So if we know it did not occur in
entropy would capture the instances
So now,
we have the completed definition
Basically, we're going to consider both
and this gives us a probability
Basically, whether eats is present or
And this of course,
is the conditional entropy of
So if you expanded this entropy,
then you have the following equation.
Where you see the involvement of
Now in general, for any discrete
the conditional entropy is no larger
So basically, this is upper bound for
That means by knowing more
we want to be able to
We can only reduce uncertainty.
And that intuitively makes sense
it should always help
And cannot hurt
Now, what's interesting here is also to
value of this conditional entropy?
Now, we know that the maximum
But what about the minimum,
I hope you can reach the conclusion that
And it will be interesting to think about
So, let's see how we can use conditional
Now of course,
one way to measure
Because it tells us to what extent,
word given that we know the presence or
Now before we look at the intuition
syntagmatic relations, it's useful to
That is, the conditional entropy
So here,
we listed this conditional
So, it's here.
So, what is the value of this?
Now, this means we know where
And we hope to predict whether
And of course, this is 0 because
Once we know whether the word
we'll already know the answer
So this is zero.
And that's also when this conditional
So now, let's look at some other cases.
So this is a case of knowing the and
And this is a case of knowing eats and
Which one do you think is smaller?
No doubt smaller entropy means easier for
Which one do you think is higher?
Which one is not smaller?
Well, if you at the uncertainty,
the doesn't really tell
So knowing the occurrence of the doesn't
So it stays fairly close to
Whereas in the case of eats,
So knowing presence of eats or
would help us predict whether meat occurs.
So it can help us reduce entropy of meat.
So we should expect the sigma term, namely
And that means there is a stronger
So we now also know when
meat, then the conditional entropy
And for what kind of words
Well, that's when this stuff
And like the for example,
which is the entropy of meat itself.
So this suggests that when you
mining syntagmatic relations,
For each word W1, we're going to
And then, we can compute
We thought all the candidate was in
because we're out of favor,
Meaning that it helps us predict
And then, we're going to take the top ring
potential syntagmatic relations with W1.
Note that we need to use
The stresser can be the number
absolute value for
Now, this would allow us to mine the most
strongly correlated words with
But, this algorithm does not
that K syntagmatical relations
Because in order to do that, we have to
are comparable across different words.
In this case of discovering
a targeted word like W1, we only need
for W1, given different words.
And in this case, they are comparable.
All right.
So, the conditional entropy of W1, given
given W3 are comparable.
They all measure how hard
But, if we think about the two pairs,
where we share W2 in the same condition,
Then, the conditional entropies
You can think of about this question.
Why?
So why are they not comfortable?
Well, that was because they
Right?
the entropy of W1 and the entropy of W3.
And they have different upper bounds.
So we cannot really
So how do we address this problem?
Well later, we'll discuss, we can use
[MUSIC]
[SOUND].
This lecture is about the syntagmatic
In this lecture we are going to continue
In particular,
the concept in the information series,
how it can be used to discover
Before we talked about the problem
that is the conditional entropy
It is not really comparable, so
strong synagmatic relations
So now we are going to introduce mutual
in the information series
normalize the conditional entropy to make
In particular, mutual information
matches the entropy reduction
More specifically the question we
of an entropy of X can
So mathematically it can be
the original entropy of X, and
And you might see,
as reduction of entropy of
Now normally the two conditional
the entropy of Y given X are not equal,
the reduction of entropy by knowing
So, this quantity is called a Mutual
And this function has some interesting
This is easy to understand because
not going to be lower than the possibility
In other words, the conditional entropy
Knowing some information can
will not hurt us in predicting x.
The signal property is that it
entropy is not symmetrical,
the third property is that It
only if the two random variables
That means knowing one of them does not
this last property can be verified by
it reaches 0 if and
[INAUDIBLE] Y is exactly the same
So that means knowing why it did not
a Y are completely independent.
Now when we fix X to rank different
would give the same order as
because in the function here,
So ranking based on mutual entropy is
the conditional entropy of X given Y, but
the mutual information allows us to
So, that is why mutual information is
So, let us examine the intuition
Syntagmatical Relation Mining.
Now, the question we ask forcing
whenever "eats" occurs,
So this question can be framed as
which words have high mutual
so computer the missing information
And if we do that, and it is basically
we will see that words that
will have a high point.
Whereas words that are not related
For this, I will give some example here.
The mutual information between "eats" and
which is the same as between "meats" and
symmetrical is expected to be higher than
the, because knowing the does not
It is similar, and
the as well.
And you also can easily
information between a word and
which is equal to
so, because in this case the reduction is
maximum because knowing one allows
So the conditional entropy is zero,
therefore the mutual information
It is going to be larger, then are equal
In other words picking any other word and
the computer picking between eats and
You will not get any information larger
So now let us look at how to
Now in order to do that, we often
use a different form of mutual
rewrite the mutual information
Where we essentially see
called a KL-divergence or divergence.
This is another term
It measures the divergence
Now, if you look at the formula,
different values of the two random
mainly we are doing a comparison
The numerator has the joint,
actual observed the joint distribution
The bottom part or the denominator can be
interpreted as the expected joint
if they were independent because when
they are joined distribution is equal to
So this comparison will tell us whether
If they are indeed independent then we
but if the numerator is different
the two variables are not independent and
The sum is simply to take into
of the values of these
In our case, each random variable
zero or one, so
If we look at this form of mutual
information matches the divergence
from the expected distribution
The larger this divergence is, the higher
So now let us further look at what
involved in this formula
And here, this is all the probabilities
you to verify that.
Basically, we have first to
corresponding to the presence or
So, for w1,
They should sum to one, because a word
In the segment, and similarly for
the second word, we also have two
absences of this word, and
And finally, we have a lot of
the scenarios of co-occurrences of
And they sum to one because the two
possible scenarios.
Either they both occur, so
in that case both variables will have
There are two scenarios.
In these two cases one of the random
the other will be zero and finally we have
This is when the two variables
So these are the probabilities involved
over here.
Once we know how to calculate
we can easily calculate
It is also interesting to know that
constraint among these probabilities,
So in the previous slide,
that you have seen that
words sum to one and
that says the two words have these
but we also have some additional
For example, this one means if we add up
the probabilities that we observe
the probabilities when the first word
We get exactly the probability
In other words, when the word is observed.
When the first word is observed, and
there are only two scenarios, depending on
So, this probability captures the first
actually is also observed, and
this captures the second scenario
So, we only see the first word, and
it is easy to see the other equations
Now these equations allow us to
other probabilities, and
So more specifically,
a word is present, like in this case,
if we know the probability of
then we can easily compute
It is very easy to use this
we take care of the computation of
absence of each word.
Now let's look at
Let us assume that we also have available
the probability that
Now it is easy to see that we can
probabilities based on these.
Specifically for
the probability that the first word
because we know these probabilities in
equation we can compute the probability
Word.
And then finally,
by using this equation because
this is also known, and
So this can be easier to calculate.
So now this can be calculated.
So this slide shows that we only
these three probabilities
naming the presence of each word and the
[MUSIC]
[SOUND]
In general, we can use the empirical count
of events in the observed data
And a commonly used technique is
where we simply normalize
So if we do that, we can see, we can
For estimating the probability that
we simply normalize the count of
So let's first take
On the right side, you see a list of some,
These are segments.
And in some segments you see both words
both columns.
In some other cases only one will occur,
the other column has zero.
And in all, of course, in some other
so they are both zeros.
And for estimating these probabilities, we
So the three counts are first,
And that's the total number of
It's just as the ones in the column of W1.
We can count how many
The segment count is for word 2, and we
And these will give us the total
The third count is when both words occur.
So this time, we're going to count
And then, so this would give us
where we have seen both W1 and W2.
Once we have these counts,
which is the total number of segments, and
this will give us the probabilities that
Now, there is a small problem,
And in this case, we don't want a zero
a small sample and in general, we would
a [INAUDIBLE] to avoid any context.
So, to address this problem,
And that's basically to add some
and so that we don't get
Now, the best way to understand smoothing
data than we actually have, because we'll
I illustrated on the top,
And these pseudo-segments would
of these words so
Now, in particular we introduce
Each is weighted at one quarter.
And these represent the four different
So now each event,
at least one count or at least a non-zero
So, in the actual segments
it's okay if we haven't observed
So more specifically, you can see
ones in the two pseudo-segments,
We add them up, we get 0.5.
And similar to this,
pseudo-segment that indicates
And of course in the denominator we add
we add, in this case,
Each is weighed at one quarter so
So, that's why in the denominator
So, this basically concludes
four syntagmatic relation discoveries.
Now, so to summarize,
be discovered by measuring correlations
We've introduced the three
Entropy, which measures the uncertainty
Conditional entropy, which measures
And mutual information of X and Y,
due to knowing Y, or
They are the same.
So these three concepts are actually very
That's why we spent some time
But in particular,
discovering syntagmatic relations.
In particular,
discovering such a relation.
It allows us to have values
words that are comparable and
discover the strongest syntagmatic
Now, note that there is some relation
[INAUDIBLE] relation discovery.
So we already discussed the possibility
terms in the context to potentially
that have syntagmatic relations
But here, once we use mutual information
we can also represent the context with
So this would give us
the context of a word, like a cat.
And if we do the same for all the words,
compare the similarity between these
So this provides yet
paradigmatic relation discovery.
And so to summarize this whole part
We introduce two basic associations,
a syntagmatic relations.
These are fairly general, they apply
the units don't have to be words,
We introduced multiple statistical
mainly showing that pure
are variable for
And they can be combined to
These approaches can be applied
mostly because they are based
they can actually discover
We can also use different ways with
this would lead us to some interesting
For example, the context can be very
a sentence, or maybe paragraphs,
allows to discover different flavors
And similarly,
visual information to discover
We also have to define the segment, and
text window or a longer text article.
And this would give us different
These discovery associations can
in both information retrieval and
So here are some recommended readings,
The first is a book with
which is quite relevant to
The second is an article
statistical measures to
Those are phrases that
For example,
blue chip is not a chip that's blue.
And the paper has a discussion about some
The third one is a new paper on a unified
relations and a syntagmatical relations,
[SOUND]
[SOUND]
lecture is about topic mining and
We're going to talk about its
In this lecture we're going to talk
As you see on this road map,
mining knowledge about language,
word associations such as paradigmatic and
Now, starting from this lecture, we're
knowledge, which is content mining, and
trying to discover knowledge about
And we call that topic mining and
In this lecture, we're going to talk about
So first of all,
So topic is something that we
it's actually not that
Roughly speaking, topic is the main
And you can think of this as a theme or
It can also have different granularities.
For example,
A topic of article,
the topic of all the research articles
so different grand narratives of topics
Indeed, there are many applications that
they're analyzed then.
Here are some examples.
For example, we might be interested
users are talking about today?
Are they talking about NBA sports, or
are they talking about some
Or we are interested in
For example, one might be interested in
topics in data mining, and how are they
Now this involves discovery of topics
also we want to discover topics in
And then we can make a comparison.
We might also be also interested in
some products like the iPhone 6,
And this involves discovering
iPhone 6 and
Or perhaps we're interested in knowing
presidential election?
And all these have to do with discovering
and we're going to talk about a lot
In general we can view a topic as
So from text data we expect to
then these topics generally provide
And it tells us something about the world.
About a product, about a person etc.
Now when we have some non-text data,
then we can have more context for
For example, we might know the time
locations where the text
or the authors of the text, or
All such meta data, or
context variables can be associated
then we can use these context variables
For example, looking at topics over time,
whether there's a trending topic, or
Soon you are looking at topics
We might know some insights about
So that's why mining
Now, let's look at the tasks
In general, it would involve first
k topics.
And then we also would like to know, which
to what extent.
So for example, in document one, we
Topic 2 and
And other topics,
Document two, on the other hand,
but it did not cover Topic 1 at all, and
it also covers Topic k to some extent,
So now you can see there
sub-tasks, the first is to discover k
What are these k topics?
Okay, major topics in the text they are.
The second task is to figure out
to what extent.
So more formally,
First, we have, as input,
Here we can denote the text
denote text article as d i.
And, we generally also need to have
But there may be techniques that can
But in the techniques that we will
techniques, we often need to
Now the output would then be the k
in order as theta sub
Also we want to generate the coverage of
this is denoted by pi sub i j.
And pi sub ij is the probability
covering topic theta sub j.
So obviously for each document, we have
what extent the document covers,
And we can assume that these
Because a document won't be able to cover
other topics outside of the topics
So now, the question is, how do we define
Now this problem has not
until we define what is exactly theta.
So in the next few lectures,
we're going to talk about
[MUSIC]
[MUSIC]
This lecture is about topic mining and
We're going to talk about
This is a slide that you have
where we define the task of
We also raised the question, how do
So in this lecture, we're going to
that's our initial idea.
Our idea here is defining
A term can be a word or a phrase.
And in general,
So our first thought is just
For example, we might have terms
as you see here.
Now if we define a topic in this way,
we can then analyze the coverage
Here for example,
we might want to discover to what
And we found that 30% of the content
And 12% is about the travel, etc.
We might also discover document
So the coverage is zero, etc.
So now, of course,
topic mining and analysis,
One is to discover the topics.
And the second is to analyze coverage.
So let's first think
topics if we represent
So that means we need to mine k
Now there are, of course,
And we're going to talk about
which is also likely effective.
So first of all,
we're going to parse the text data in
Here candidate terms can be words or
Let's say the simplest solution is
These words then become candidate topics.
Then we're going to design a scoring
is as a topic.
So how can we design such a function?
Well there are many things
For example, we can use pure statistics
Intuitively, we would like to
meaning terms that can represent
So that would mean we want
However, if we simply use the frequency
then the highest scored terms
functional terms like the, etc.
Those terms occur very frequently English.
So we also want to avoid having
we want to penalize such words.
But in general, we would like to favor
not so frequent.
So a particular approach could be based
And TF stands for term frequency.
IDF stands for inverse document frequency.
We talked about some of these
ideas in the lectures about
So these are statistical methods,
meaning that the function is
So the scoring function
It can be applied to any language,
But when we apply such a approach
we might also be able to leverage
For example, in news we might favor
We might want to favor title
use the title to describe
If we're dealing with tweets,
which are invented to denote topics.
So naturally, hashtags can be good
Anyway, after we have this design
the k topical terms by simply picking
Now, of course,
we might encounter situation where the
They're semantically similar, or
So that's not desirable.
So we also want to have coverage over
So we would like to remove redundancy.
And one way to do that is
which is sometimes called a maximal
Basically, the idea is to go down
function and gradually take terms
The first term, of course, will be picked.
When we pick the next term, we're
been picked and try to avoid
So while we are considering
we are also considering
with respect to the terms
And with some thresholding,
the redundancy removal and
Okay, so
And those can be regarded as the topics
Next, let's think about how we're going
So looking at this picture,
these topics.
And now suppose you are give a document.
How should we pick out coverage
Well, one approach can be to simply
So for example, sports might have occurred
travel occurred twice, etc.
And then we can just normalize these
probability for each topic.
So in general, the formula would
all the terms that represent the topics.
And then simply normalize them so
topic in the document would add to one.
This forms a distribution of the topics
of different topics in the document.
Now, as always,
solving problem, we have to ask
Or is this the best way
So now let's examine this approach.
In general,
by using actual data sets and
Well, in this case let's take
And we have a text document that's
So in terms of the content,
But if we simply count these
we will find that the word sports
even though the content
So the count of sports is zero.
That means the coverage of sports
Now of course,
the document and
And that's okay.
But sports certainly is not okay because
So this estimate has problem.
What's worse, the term travel
So when we estimate the coverage
we have got a non-zero count.
So its estimated coverage
So this obviously is also not desirable.
So this simple example illustrates
First, when we count what
we also need to consider related words.
We can't simply just count
In this case, it did not occur at all.
But there are many related words
So we need to count
The second problem is that a word
So here it probably means
we can imagine it might also
So in that case, the star might actually
So we need to deal with that as well.
Finally, a main restriction of this
term to describe the topic, so it cannot
For example, a very specialized
describe by using just a word or
We need to use more words.
So this example illustrates
this approach of treating a term as topic.
First, it lacks expressive power.
Meaning that it can only represent
it cannot represent the complicated topics
Second, it's incomplete
meaning that the topic itself
It does not suggest what other
Even if we're talking about sports,
So it does not allow us to easily
conversion to coverage of this topic.
Finally, there is this problem
A topical term or
For example,
So in the next lecture,
about how to solve
[MUSIC]
This lecture is about Probabilistic Topic
In this lecture,
we're going to continue talking
We're going to introduce
So this is a slide that
where we discussed the problems
So, to solve these problems
more words to describe the topic.
And this will address the problem
When we have more words that we
that we can describe complicated topics.
To address the second problem we
This is what allows you to distinguish
to introduce semantically
Finally, to solve the problem of
ambiguous word, so
It turns out that all these can be done
And that's why we're going to spend a lot
So the basic idea here is that,
improve the replantation of
So what you see now is
Where we replanted each topic, it was just
But now we're going to use a word
So here you see that for sports.
We're going to use
theoretical speaking all
So for example, the high
game, basketball,
These are sports related terms.
And of course it would also give
like Trouble which might be
not so much related to topic.
In general we can imagine a non
And some words that are not read and
And these probabilities will sum to one.
So that it forms a distribution
Now intuitively, this distribution
words from the distribution, we tended
You can also see, as a very special case,
is concentrated in entirely on
And this basically degenerates
of a topic was just one word.
But as a distribution,
in general,
can model several differences
Similarly we can model Travel and Science
In the distribution for Travel we see top
Whereas in Science we see scientist,
genomics, and, you know,
Now that doesn't mean sports related terms
will necessarily have zero
In general we can imagine all of these
It's just that for a particular
very small probabilities.
Now you can also see there are some
When I say shared it just means even
you can still see one word
In this case I mark them in black.
So you can see travel, for example,
with different probabilities.
It has the highest probability for
But with much smaller probabilities for
And similarly, you can see a Star
Science with reasonably
Because they might be actually
So with this replantation it addresses the
First, it now uses multiple
So it allows us to describe
Second, it assigns weights to terms.
So now we can model several
And you can bring in related
Third, because we have probabilities for
we can disintegrate the sense of word.
In the text to decode
to address all these three problems with
So now of course our problem definition
The slight is very similar to what
added refinement for what our topic is.
Now each topic is word distribution,
that all the probabilities should sum to
So you see a constraint here.
And we still have another constraint
So all the Pi sub ij's must sum to one for
So how do we solve this problem?
Well, let's look at this problem
So we clearly specify it's input and
output and
Input of course is our text data.
C is our collection but we also generally
Or we hypothesize a number and
even though we don't know the exact
And V is the vocabulary that has
units would be treated as
In most cases we'll use words
And that means each word is a unique.
Now the output would consist of as first
Each theta I is a word distribution.
And we also want to know the coverage
So that's.
That the same pi ijs
So given a set of text data we would
all these coverages as you
Now of course there may be many
In theory, you can write the [INAUDIBLE]
but here we're going to introduce
a general way of solving this
And this is, in fact,
it's a principle way of using statistical
And here I dimmed the picture
in order to show the generation process.
So the idea of this approach is actually
So we design a probabilistic model
Of course,
The actual data aren't
So that gave us a probability
that you are seeing on this slide.
Given a particular model and
So this template of actually consists of
all the parameters that
And these parameters in general
the probability risk model.
Meaning that if you set these
it will give some data points
Now in this case of course,
more precisely topic mining problem
First of all we have theta i's which
a set of pis for each document.
And since we have n documents, so we have
The pi values will sum to one.
So this is to say that we
have these word distributions and
And then we can see how we can generate
So how do we model the data in this way?
And we assume that the data
drawn from such a model that
Now one interesting question here is to
think about how many
Now obviously we can already see
For pi's.
We also see k theta i's.
But each theta i is actually a set
It's a distribution of words.
So I leave this as an exercise for
you to figure out exactly how
Now once we set up the model then
Meaning that we can
infer the parameters based on the data.
In other words we would like to
Until we give our data set
I just said,
some data points will have higher
What we're interested in, here,
is what parameter values will give
So I also illustrate the problem
On the X axis I just illustrate lambda,
as a one dimensional variable.
It's oversimplification, obviously,
And the Y axis shows the probability
This probability obviously depends
So that's why it varies as you
What we're interested here
That would maximize the probability
So this would be, then,
And these parameters,
note that are precisely what we
So we'd treat these parameters
the output of the data mining algorithm.
So this is the general idea of using
a generative model for text mining.
First, we design a model with
the data as well as we can.
After we have fit the data,
We will use the specific
those would be the output
And we'll treat those as actually
By varying the model of course we
So to summarize, we introduced
namely representing as word distribution
multiple words to describe a complicated
weights on words so we have more than
We talked about the task of topic mining,
When we define a topic as distribution.
So the importer is a clashing of text
a vocabulary set and
Each is a word distribution and
also the coverage of all
And these are formally represented
And we have two constraints here for
The first is the constraints
In each worded distribution
must sum to 1,
The second constraint is on
A document is not allowed to recover
we are discovering.
So, the coverage of each of these k
We also introduce a general idea of using
And the idea here is, first we're design
We simply assume that they
And inside the model we embed some
denoted by lambda.
And then we can infer the most
given a particular data set.
And we can then take the lambda star as
our problem.
And we can adjust
the parameters to discover various
As you will see later
[MUSIC]
[SOUND]
lecture is about the Overview
which cover proper
In this lecture we're going to give
a overview of Statical Language Models.
These models are general models that cover
probabilistic topic models
So first off,
A Statistical Language Model is
over word sequences.
So, for example,
today is Wednesday a probability of .001.
It might give today Wednesday is, which
is a non-grammatical sentence, a very,
And similarly another sentence,
the eigenvalue is positive might
So as you can see such a distribution
It depends on the Context of Discussion.
Some Word Sequences might have higher
Sequence of Words might have different
And so this suggests that such a
such a model can also be regarded
generating text.
And that just means we can view text
For this reason,
So, now given a model we can then
So, for example, based on the distribution
when matter it say assemble
because it has a relative
We might often get such a sequence.
We might also get the item
with a smaller probability and
get today is Wednesday because
So in general, in order to categorize such
values for
Obviously, it's impossible
impossible to enumerate all of
So in practice, we will have to
So, the simplest language model is
In such a case, it was simply a the text
is generated by generating
But in general, the words may
But after we make this assumption, we can
Basically, now the probability of
will be just the product of
So for such a model,
we have as many parameters as
So here we assume we have n words,
One for each word.
And then some to 1.
So, now we assume that
drawn according to this word distribution.
That just means,
then eventually we'll get a text.
So for example, now again,
we can try to assemble words
We might get Wednesday often or
And some other words like eigenvalue
But with this, we actually can
every sequence, even though our model
And this is because of the independence.
So specifically, we can compute
Because it's just a product
the probability of is, and
For example,
multiply these numbers together you get
So as you can see, with N probabilities,
can characterize the probability situation
And so, this is a very simple model.
Ignore the word order.
So it may not be, in fact, in some
where you may care about
But it turns out to be
many tasks that involve topic analysis.
And that's also what
So when we have a model, we generally have
One is, given a model, how likely are we
That is,
The other is the Estimation Process.
And that, is to think of
some observe the data and we're
Let's first talk about the sampling.
So, here I show two examples of Water
The first one has higher probabilities for
words like a text mining association,
Now this signals a topic about text mining
such a distribution, we tend to see words
So in this case,
what is the probability of
Then, we likely will see text that
Of course, the text that we
This distribution is unlikely coherent.
Although, the probability
[INAUDIBLE] publishing
non-zero assuming that no word has
And that just means,
text documents including very
Now, the second distribution show,
on the bottom, has different than
So food [INAUDIBLE] healthy [INAUDIBLE],
So this clearly indicates
In this case it's probably about health.
So if we sample a word
then the probability of observing a text
On the other hand, the probability of
nutrition paper would be high,
So that just means, given a particular
Now let's look at
In this case, we're going to assume
I will know exactly what
In this case,
In fact, it's abstract of the paper,
And I've shown some counts
Now, if we ask the question,
Language Model that has been
Assuming that the text is observed
what's our best guess
Okay, so the problem now is just to
As I've shown here.
So what do you think?
What would be your guess?
Would you guess text has
a relatively large probability?
What about query?
Well, your guess probably
how many times we have observed
And if you think about it for a moment.
And if you are like many others,
well, text has a probability of 10
the text 10 times in the text
And similarly, mining has 5 out of 100.
And query has a relatively small
So it's 1 out of 100.
Right, so that, intuitively,
But the question is, is this our best
Of course,
we have to define what do we mean by best,
it turns out that our
In some sense and this is called
And it's the best thing that, it will give
Meaning that, if you change
then the probability of the observed
And this is called
[MUSIC]
[SOUND] This lecture is a continued
discussion of probabilistic topic models.
In this lecture, we're going to continue
We're going to talk about
are interested in just mining
So in this simple setup,
one document and
So this is the simplest
The input now no longer has k,
know there is only one topic and the
In the output,
we assumed that the document
So the main goal is just to discover
this single topic, as shown here.
As always, when we think about using a
we start with thinking about what
from what perspective we're going to
And then we're going to
the generating of the data,
Where our perspective just means we want
the data, so that the model will
discovering the knowledge that we want.
And then we'll be thinking
write down the microfunction to
a data point will be
And the likelihood function will have
And then we argue our interest in
by maximizing the likelihood which will
These estimator parameters
of the mining hours,
parameters as the knowledge
So let's look at these steps for
Later we'll look at this procedure for
So our data, in this case is, just
Each word here is denoted by x sub i.
Our model is a Unigram language model.
A word distribution that we hope to
So we will have as many parameters as many
And for convenience we're
denote the probability of word w sub i.
And obviously these theta
Now what does a likelihood
Well, this is just the probability
that given such a model.
Because we assume the independence in
the document will be just a product
And since some word might
So we can also rewrite this
So in this line, we have rewritten
over all the unique words in
Now this is different
Well, the product is over different
Now when we do this transformation,
introduce a counter function here.
This denotes the count of
similarly this is the count
because these words might
You can also see if a word did
It will have a zero count, therefore
So this is a very useful form of
writing down the likelihood function
So I want you to pay attention to this,
It's just to change the product over all
So in the end, of course, we'll use
function and it would look like this.
Next, we're going to find
of these words that would maximize
So now lets take a look at the maximum
This line is copied from
It's just our likelihood function.
So our goal is to maximize
We will find it often easy to
maximize the local likelihood
And this is purely for
the logarithm transformation our function
And we also have constraints
The sum makes it easier to take
finding the optimal
So please take a look at this sum again,
And this is a form of
see later also,
So it's a sum over all
And inside the sum there is
And this is macroed by
So let's see how we can
Now at this point the problem is purely a
to just the find the optimal solution
The objective function is
the constraint is that all these
So, one way to solve the problem is
Now this command is beyond
since Lagrange multiplier is a very
to just give a brief introduction to this,
So in this approach we will
And this function will combine
with another term that
we introduce Lagrange multiplier here,
lambda, so it's an additional parameter.
Now, the idea of this approach is just to
in some sense,
Now we are just interested in
As you may recall from calculus,
would be achieved when
This is a necessary condition.
It's not sufficient, though.
So if we do that you will
with respect to theta i
And this part comes from the derivative
this lambda is simply taken from here.
And when we set it to zero we can
easily see theta sub i is
Since we know all the theta
we can plug this into this constraint,
And this will allow us to solve for
And this is just a net
And this further allows us to then
eventually, to find the optimal
And if you look at this formula it turns
because this is just the normalized
which is also a sum of all
So, after all this mess, after all,
we have just obtained something
this will be just our
maximize the data by
mass as possible to all
And you might also notice that this is
raised estimator.
In general, the estimator would be to
the counts have to be done in a particular
So this is basically an analytical
In general though, when the likelihood
going to be able to solve the optimization
Instead we have to use some
we're going to see such cases later, also.
So if you imagine what would we
likelihood estimator to estimate one
Let's imagine this document
Now, what you might see is
On the top, you will see the high
common words,
And this will be followed by
characterize the topic well like text,
And then in the end,
words that are not really
they might be extraneously
As a topic representation,
That because the high probability
they are not really
So my question is how can we
Now this is the topic of the next module.
We're going to talk about how to use
these common words.
[MUSIC]
[MUSIC]
This lecture is about the mixture
In this lecture we will continue
In particular, what we introduce
This is a slide that
Where we talked about how to
words that we have on top of for
So if you want to solve the problem,
it would be useful to think about
Well, this obviously because these
we are using a maximum
Then the estimate obviously would
these words in order to
So, in order to get rid of them that
differently here.
In particular we'll have
doesn't have to explain all
What were going to say is that,
these common words should not be
So one natural way to solve the problem is
to account for just these common words.
This way, the two distributions can be
And we'll let the other model which
to generate the common words.
This way our target topic theta
the common handle words that are
So, how does this work?
Well, it is just a small
where we have just one distribution.
Since we now have two distributions,
we have to decide which distribution
Each word will still be a sample
Text data is still
Namely, look at the generating
eventually we generate a lot of words.
When we generate the word,
however, we're going to first decide
And this is controlled by another
theta sub d and
So this is a probability of enacting
This is the probability of
of distribution denoted by theta sub B.
On this case I just give example
So you're going to basically flip a coin,
to decide what you want to use.
But in general these probabilities
So you might bias toward using
So now the process of generating a word
Based on these probabilities choosing
shows up as head, which means we're going
Then we're going to use this word
Otherwise we might be
And we're going to use the background
So in such a case,
associated with the use
But we can still think of this as
And such a model is
So now let's see.
In this case, what's the probability
Now here I showed some words.
like "the" and "text".
So as in all cases,
once we setup a model we are interested
The basic question is, so
what's the probability of
Now we know that the word can be observed
we have to consider two cases.
Therefore it's a sum over these two cases.
The first case is to use the topic for
And in such a case then
which is the probability
multiplied by the probability of actually
Both events must happen
We first must have choosing
we also have to actually have sampled
And similarly,
a different way of generally
Now obviously the probability of
So we also can see the two
And in each case, it's a product of the
is multiplied by the probability of
Now whether you will see,
So might want to make sure that you have
And you should convince yourself that
obsolete text.
So to summarize what we observed here.
The probability of a word from
of different ways of generating the word.
In each case,
it's a product of the probability
Multiplied by the probability of
from that component of the model.
And this is something quite general and
So the basic idea of a mixture
thesetwo distributions
So I used a box to bring all
So if you view this
it's just like any other generative model.
It would just give us
But the way that determines this
when we have just one distribution.
And this is basically a more
So the more complicated is more
And it's called a mixture model.
So as I just said we can treat
And it's often useful to think of
The illustration that
which is dimmer now, is just
So mathematically,
to just define the following
Where the probability of a word is
of generating the word.
And the form you are seeing now
what you have seen in
Well I just use the symbol
you can still see this is
Right?
And this sum is due to the fact that the
two ways in this case.
And inside a sum,
And the two terms are first
like of D Second,
the probability of actually observing
So this is a very general description
I just want to make sure
this because this is really the basis for
So now once we setup model.
We can write down that like
The next question is,
or what to do with the parameters.
Given the data.
Well, in general,
we can use some of the text data
And this estimation would allow us to
discover the interesting
So you, in this case, what do we discover?
Well, these are presented
we will have two kinds of parameters.
One is the two worded distributions,
the other is the coverage
The coverage of each topic.
And this is determined by
probability of theta, so this is to one.
Now, what's interesting is
cases like when we send one of
Well with the other, with the zero right?
And if you look at
it will then degenerate to the special
Okay so you can easily verify that by
the other is Zero.
So in this sense,
the previous model where we
It can cover that as a special case.
So to summarize, we talked about the
the data we're considering
And the model is a mixture
two unigram LM models,
which is intended to denote the topic of
representing a background topic that
words because common words would be
So the parameters can
Lambda which I show here you can again
think about the question about how many
This is usually a good exercise to do
depth and to have a complete understanding
And we have mixing weights,
So what does a likelihood
Well, it looks very similar
So for the document,
first it's a product over all the words in
The only difference is that inside here
So you might have recalled before
But now we have this sum
And because of the mixture model we
choosing that particular
And so
by using a product over all the unique
having that product over all
And this form where we look at
a commutative that formed for computing
And the maximum likelihood estimator is,
just to find the parameters that would
And the constraints here
One is what are probabilities in each
[INAUDIBLE] must sum to 1 the other is
the choice of each
[MUSIC]
This lecture is about
In this lecture, we're
discussing probabilistic
In particular, we're going
estimate the parameters
So let's first look
for using a mixture model,
and we hope to effect out
the background words from
So the idea is to assume that
the text data actually
One kind is from
so the "is", "we" etc.
The other kind is from
our topic word distribution
So in order to solve
factoring out background words,
we can set up our mixture
We are going to assume that
we already know the parameters
of all the values for
all the parameters in
the word distribution of
So this is a case of
some model so that we
embedded the unknown variables
but we're going to
We're going to assume
others and this is
a powerful way of
customizing a model
Now you can imagine, we
we also don't know the
but in this case,
precisely those high probability
So we assume the background
The problem here is,
how can we adjust the Theta sub
the probability of
here and we assume all the
Now, although we
heuristically to try to
factor out these
it's unclear whether if
we use maximum
we will actually end up having
the common words like "the" will
be indeed having smaller
So now, in this case,
it turns out that
When we set up the probabilistic
when we use maximum
we will end up having
the common words
out by the use of
So to understand why this is so,
it's useful to examine
So we're going to look
In order to understand
some interesting behaviors
the observed patterns
generalizable to mixture
but it's much easier to
we use a very simple case
So specifically in this case,
let's assume that
choosing each of the two models
So we're going to flip
a fair coin to decide
Furthermore, we are going
precisely to words,
Obviously, this is
of the actual text,
but again, it is useful
to examine the behavior
So we further assume that,
the background model gives
the word "the" and "text" 0.1.
Now, let's also assume that
The document has just two words
So now, let's write down
the likelihood function
First, what's the probability
of "text" and what's the
I hope by this point,
you will be able
So the probability of "text" is
basically a sum of
corresponds to each of
the water distribution and
it accounts for the two ways
Inside each case, we have
the probability of choosing
0.5 multiplied by the probability
of observing "text"
Similarly, "the" would
the same form just as it
was different exactly
So naturally,
is just the product of the two.
So it's very easy to see that,
once you understand
each word and which
important to understand what's
exactly the probability of
observing each word from
Now, the interesting
how can we then optimize
Well, you will notice that,
there are only two variables.
They are precisely
of the two words
by Theta sub d. This is
all the other
So now, the question is
So we have a simple expression
with two variables and we hope
to choose the values of
these two variables to
It's exercises that we have
seen some simple
and note that the two
So there's some constraint.
If there were
we will set both probabilities to
their maximum value which
but we can't do that
because "text" and
We can't give those a
So now the question is,
how should we allocate
the mass between the two words?
Now, it will be useful to look at
this formula for
intuitively what
set these probabilities to
maximize the value
If we look into this further,
then we'll see
of the two component
they will be
the probability of
is dictated by the maximum
but they're also
In particular, they
the words and they
high probabilities on
this competition in some sense
or to gain advantage
So again, looking at this
a constraint on
now if you look at
you might feel that
the probability of "text"
to be somewhat larger than "the".
This intuition can
by mathematical fact which is,
when the sum of
constant then the product of
them which is maximum
and this is a fact that
Now, if we plug that in,
we will would mean
the two probabilities equal.
When we make them equal
the constraint that we can
and the solution is the
would be 0.9 and probability
As you can see indeed,
the probability of text
probability of "the" and
this is not the case when we
This is clearly because of
the use of the
assign a very high probability
to "the" low
If you look at the equation,
you will see obviously
some interaction of
In particular, you will see in
order to make them equal and then
the probability assigned
be higher for a word that has
a smaller probability
This is obvious from
because "the" background part is
weak for "text" it's a small.
So in order to
we must make the probability
Theta sub d somewhat
larger so that the two sides
So this is in fact
a very general behavior
That is, if one distribution
assigns a high probability
then the other distribution
would tend to do the opposite.
Basically, it would discourage
other distributions to do the
same and this is to
we can account for all words.
This also means that,
by using a background
fixed to assign high probabilities
we can indeed encourage
the unknown topic
assign smaller probabilities
Instead, put more probability
that cannot be explained well by
the background
they have a very
from the background
This lecture is about
the expectation-maximization
also called the EM algorithm.
In this lecture, we're
the discussion of
In particular, we're going to
which is a family of
the maximum likelihood estimate
So this is now
familiar scenario of
the mixture model, to try
to factor out
from one topic word
So we're interested in
and we're going to try to adjust
these probability values to
maximize the probability
Note that we assume that all
the other parameters are known.
So the only thing unknown is
the word probabilities
In this lecture, we're
compute this maximum
Now, let's start with the idea of
separating the words in
One group would be explained
The other group would
the unknown topic
After all, this is
But suppose we actually
know which word is from
So that would mean, for example,
these words the, is,
and we are known to
be from this background
On the other hand, the
clustering etc are known to be
from the topic word distribution.
If you can see the color,
then these are shown in blue.
These blue words are then
assumed that to be from
If we already know how
then the problem of estimating
the word distribution
If you think about
you'll realize that, well,
we can simply take
to be from this word
distribution theta sub d
So indeed this problem would be
very easy to solve if we had
known which words are from
which a distribution precisely,
and this is in fact
making this model no
because we can already observe
which distribution has been
used to generate
So we actually go back to
the single word
In this case let's call
these words that are
a pseudo document of d prime,
and now all we need to
these words counts
That's fairly straightforward.
It's just dictated by the
Now, this idea however
we in practice don't really
know which word is from
but this gives us
can guess which word is
Specifically given
can we infer the distribution
So let's assume that we actually
know tentative probabilities for
these words in theta sub d.
So now all the parameters
are known for this mixture model,
and now let's consider
So the question is, do you
having been generated from
theta sub d or from
So in other words,
which distribution has been
Now, this inference process is
a typical Bayesian inference
some prior about
So can you see what
Well, the prior here is
the probability of
So the prior is given by
In this case, the prior
is saying that each model
but we can imagine perhaps a
So this is called a prior
which distribution has
a word before we even
So that's why we
So if we don't observe the word,
we don't know what word
Our best guess is to say
All right. So it's
Now in Bayesian inference we
our belief after we have
So what is the evidence here?
Well, the evidence
Now that we know we're
So text that can be
and if we use
Bayes rule to combine the
what we will end up
prior with the likelihood
which is basically
the word text from
We see that in both cases
Note that even in the background
it just has a very
So intuitively what would
Now if you're like many others,
you are guess text
theta sub d. It's more likely
You will probably see that
a much higher probability
then by the background model
which has a very
By this we're going to say, well,
text is more likely from
theta sub d. So you see
our guess of which
used to generate
how high the probability of
the text is in
We can do, tend to guess
the distribution that gives us
a word a higher probability,
and this is likely to
So we're going to choose
a word that has
So in other words,
these two probabilities of
the word given by
But our guess must also
So we also need to
Why? Because imagine if we
we're going to say
a background model is
Now, if you have that kind
then that would
You might think,
maybe text could have been
Although the probability
the prior is very high.
So in the end, we have
and the base formula provides us
a solid and principled way
of making this kind of
So more specifically,
let's think about
this word has been generated in
fact from from theta sub d. Well,
in order for texts
theta sub d two things
First, the theta sub d
so we have the selection
Secondly, we also have to
actually have observed text
So when we multiply
we get the probability
fact been generated from
for the background model,
the probability of generating
text is another product
Now, we also introduced
the latent variable
whether the word is from
When z is zero,
it means it's from the topic
it means it's from
So now we have
the probability that text
Then we can simply normalize
them to have an estimate
that the word text is
from theta sub d or
Then equivalently, the
zero given that
So this is application
But this step is very
the EM algorithm because
then we would be able to first
initialize the parameter values
and then we're going to take
Which distributing has been
and the initialized
allow us to have a complete
specification of
which further allows us to
which distribution is more
This prediction
to separate the words from
Although we can't
but we can separate them
[SOUND]
this is indeed a general idea of
Algorithm.
So in all the EM algorithms we
to help us solve the problem more easily.
In our case the hidden variable
each occurrence of a word.
And this binary variable would
been generated from 0 sub d or 0 sub p.
And here we show some possible
For example, for the it's from background,
And text on the other hand.
Is from the topic then it's zero for
Now, of course, we don't observe these z
Values of z attaching to other words.
And that's why we call
Now, the idea that we
predicting the word distribution that
is it a predictor,
And, so, the EM algorithm then,
First, we'll initialize all
In our case,
of a word, given by theta sub d.
So this is an initial addition stage.
These initialized values would allow
of these z values, so
We can't say for sure whether
But we can have our guess.
This is given by this formula.
It's called an E-step.
And so the algorithm would then try to
After that, it would then invoke
In this step we simply take advantage
then just group words that are in
from that ground including this as well.
We can then normalize the count
to revise our estimate of the parameters.
So let me also illustrate
that are believed to have
that's text, mining algorithm,
And we group them together to help us
re-estimate the parameters
So these will help us
Note that before we just set
But with this guess, we will have
Of course, we don't know exactly
So we're not going to really
But rather we're going to
And this is what happened here.
So we're going to adjust the count by
this word has been generated
And you can see this,
Well, this has come from here, right?
From the E-step.
So the EM Algorithm would
estimate of parameters by using
The E-step is to augment the data
And the M-step is to take advantage
of the additional information
To split the data accounts and
re-estimate our parameter.
And then once we have a new generation of
We are going the E-step again.
To improve our estimate
And then that would lead to another
For the word distribution
Okay, so, as I said,
is really the variable z, hidden variable,
this water is from the top water
So, this slide has a lot of content and
Pause the reader to digest it.
But this basically captures
Start with initial values that
And then we invoke E-step followed
setting of parameters.
And then we repeated this, so
that would gradually improve
As I will explain later
reaching a local maximum of
So lets take a look at the computation for
these formulas are the EM.
Formulas that you see before, and
here, like here, n,
Like here for example we have n plus one.
That means we have improved.
From here to here we have an improvement.
So in this setting we have assumed the two
the background model is null.
So what are the relevance
Well these are the word counts.
So assume we have just four words,
And this is our background model that
words like the.
And in the first iteration,
Well first we initialize all the values.
So here, this probability that we're
distribution of all the words.
And then the E-step would give us a guess
That will generate each word.
We can see we have different
Why?
Well, that's because these words have
So even though the two
And then our initial audition say uniform
in the background of the distribution,
So these words are believed to
These on the other hand are less likely.
Probably from background.
So once we have these z values,
we know in the M-step these probabilities
So four must be multiplied by this 0.33
in order to get the allocated
And this is done by this multiplication.
Note that if our guess says this
then we just get the full count
In general it's not going
So we're just going to get some percentage
Then we simply normalize these counts
to have a new generation
So you can see, compare this with
So compare this with this one and
Not only that, we also see some
words that are believed to have come from
Like this one, text.
And of course, this new generation of
adjust the inferred latent variable or
So we have a new generation of values,
because of the E-step based on
And these new inferred values
another generation of the estimate
And so on and so forth so this is what
these probabilities
As you can see in the last row
and the likelihood is increasing
And note that these log-likelihood is
between 0 and 1 when you take a logarithm,
Now what's also interesting is,
And these are the inverted word split.
And these are the probabilities
have come from one distribution, in this
And you might wonder whether
Because our main goal is to
So this is our primary goal.
We hope to have a more discriminative
But the last column is also bi-product.
This also can actually be very useful.
You can think about that.
We want to use, is to for
example is to estimate to what extent this
And this, when we add this up or
take the average we will kind of know to
versus content was that are not
[MUSIC]
So, I just showed you that empirically
but theoretically it can also
converge to a local maximum.
So here's just an illustration of what
This required more knowledge about that,
some of that inequalities,
So here what you see is on the X
This is a parameter that we have.
On the y axis we see
So this curve is the original
and this is the one that
And we hope to find a c0 value
But in the case of Mitsumoto we can
to the problem.
So, we have to resolve
the EM algorithm is such an algorithm.
It's a Hill-Climb algorithm.
That would mean you start
Let's say you start from here,
And then you try to improve
another point where you can
So that's the ideal hill climbing.
And in the EM algorithm, the way we
First, we'll fix a lower
So this is the lower bound.
See here.
And once we fit the lower bound,
And of course, the reason why this works,
is because the lower bound
So we know our current guess is here.
And by maximizing the lower bound,
To here.
Right?
And we can then map to the original
Because it's a lower bound, we are
Because we improve our lower bound and
curve which is above this lower bound
So we already know it's
So we definitely improve this
which is above this lower bound.
So, in our example,
the current guess is parameter value
And then the next guess is
From this illustration you
is always better than the current guess.
Unless it has reached the maximum,
So the two would be equal.
So, the E-step is basically
to compute this lower bound.
We don't directly just compute
we compute the length of
these are basically a part
This helps determine the lower bound.
The M-step on the other hand is
It allows us to move
And that's why EM algorithm is guaranteed
Now, as you can imagine,
we also have to repeat the EM
In order to figure out which one
And this actually in general is a
So here for
then we gradually just
So, that's not optimal, and
so the only way to climb up to this gear
So, in the EM algorithm, we generally
or have some other way to determine
To summarize in this lecture we
This is a general algorithm for computing
kinds of models, so
And it's a hill-climbing algorithm, so it
it will depend on initial points.
The general idea is that we will have
In the E-step we roughly [INAUDIBLE]
of useful hidden variables that we
In our case, this is the distribution
In the M-step then we would exploit
it easier to estimate the distribution,
Here improve is guaranteed in
Note that it's not necessary that we
parameter value even though the likelihood
There are some properties that have to
also to convert into some stable value.
Now here data augmentation
That means,
we're not going to just say exactly
But we're going to have a probability
these hidden variables.
So this causes a split of counts
And in our case we'll split the word
[MUSIC]
[SOUND]
lecture is about probabilistic and
In this lecture we're going to introduce
often called PLSA.
This is the most basic topic model,
Now this kind of models
mine multiple topics from text documents.
And PRSA is one of the most basic
So let's first examine this power
Here I show a sample article which is
And I show some simple topics.
For example government response,
Donation and the background.
You can see in the article we use
So we first for example see there's
this is followed by discussion of flooding
We also see background
So the overall of topic analysis here
the text, to segment the topics,
distribution and to figure out first,
How do we know there's a topic
There's a topic about a flood in the city.
So these are the tasks
If we had discovered these
as you see here,
Then you can do a lot of things,
of the topics,
So the formal definition of problem of
shown here.
And this is after a slide that you
So the input is a collection, the number
of course the text data.
And then the output is of two kinds.
One is the topic category,
Theta i's.
Each theta i is a word distribution.
And second, it's the topic coverage for
These are pi sub i j's.
And they tell us which document it covers.
Which topic to what extent.
So we hope to generate these as output.
Because there are many useful
So the idea of PLSA is
the two component mixture model
The only difference is that we
Otherwise, it is essentially the same.
So here I illustrate how we can generate
naturally in all cases
of Probabilistic modelling would want
So we would also ask the question,
what's the probability of observing
Now if you look at this picture and
compare this with the picture
you will see the only difference is
So, before we have just one topic,
But now we have more topics.
Specifically, we have k topics now.
All these are topics that we assume
So the consequence is that our switch for
Before it's just a two way switch.
We can think of it as flipping a coin.
But now we have multiple ways.
First we can flip a coin to decide
So it's the background lambda
1 minus lambda sub B gives
actually choosing a non-background topic.
After we have made this decision,
we have to make another decision to
So there are K way switch here.
And this is characterized by pi,
This is just the difference of designs.
Which is a little bit more complicated.
But once we decide which distribution to
just generate a word by using one of
So now lets look at the question
So what's the probability of observing
What do you think?
Now we've seen this
if you can recall, it's generally a sum.
Of all the different possibilities
So let's first look at how the word can
Well, the probability that the word is
is lambda multiplied by the probability
Model, right.
Two things must happen.
First, we have to have
and that's the probability of lambda,
Then second, we must have actually
and that's probability
Okay, so similarly,
we can figure out the probability of
Like the topic theta sub k.
Now notice that here's
And that's because of the choice
only happens if two things happen.
One is we decide not to
So, that's a probability
Second, we also have to actually choose
So that's probability of theta sub K,
And similarly, the probability of
The topic and the first topic
And so
in the end the probability of observing
And I have to stress again this is a very
really key to understanding all the topic
So make sure that you really
of w is indeed the sum of these terms.
So, next,
we would be interested in
All right, so to estimate the parameters.
But firstly,
let's put all these together to have the
The first line shows the probability of a
And this is an important
So let's take a closer look at this.
This actually commands all
So first of all we see lambda sub b here.
This represents a percentage
that we believe exist in the text data.
And this can be a known value
Second, we see the background
typically we also assume this is known.
We can use a large collection of text, or
use all the text that we have available
Now next in the next stop this formula.
[COUGH] Excuse me.
You see two interesting
those are the most important parameters.
That we are.
So one is pi's.
And these are the coverage
And the other is word distributions
So the next line,
in to calculate
This is, again, of the familiar
you have a count of
And then log of a probability.
Now it's a little bit more
Because now we have more components,
And then this line is just
And it's very similar, just accounting for
So what are the unknown parameters?
I already said that there are two kinds.
One is coverage,
Again, it's a useful exercise for
Exactly how many
How many unknown parameters are there?
Now, try and
think out that question will help you
And will also allow you to understand
when use PLSA to analyze text data?
And these are precisely
So after we have obtained
the next is to worry about
And we can do the usual think,
So again, it's a constrained optimization
Only that we have a collection of text and
And we still have two constraints,
One is the word distributions.
All the words must have probabilities
The other is the topic
a document will have to cover
the probability of covering each
So at this point though it's basically
you just need to figure out
There's a function with many variables.
and we need to just figure
variables to make the function
>> [MUSIC]
[SOUND] So
PLSA to of LDA and to motivate that,
we need to talk about some
First, it's not really a generative model
a new document.
You can see why, and that's because the
but the pis are tied to the document
So we can't compute the pis for
And there's some heuristic workaround,
Secondly, it has many parameters, and I've
exactly there are in PLSA, and
That means that model is very complex.
And this also means that there
it's prone to overfitting.
And that means it's very hard to
And that we are representing
And in terms of explaining future data,
it will overfit the training data
The model is so flexible to fit precisely
And then it doesn't allow us to generalize
This however is not a necessary problem
only interested in hitting
We are not always interested in modern
or if we would care about the generality,
So LDA is proposing to improve that,
PLSA a generative model by imposing
Dirichlet is just a special distribution
So in this sense, LDA is just
the parameters are now
You will see there are many
you can achieve the same goal as PLSA for
It means it can compute the top coverage
However, there's no.
Why are the parameters for
there are fewer parameters and
word distributions,
of influence of these variables because
So the influence part again
So essentially they are doing something
LDA is a more elegant way of looking
So let's see how we can
a standard PLSA to have LDA.
Now a full treatment of LDA is
we just don't have time to go in
But here, I just want to give you
what it enables, all right.
So this is the picture of LDA.
Now, I remove the background
Now, in this model, all these
we do not impose any prior.
So these word distributions are now
So these are word distributions, so here.
And the other set of parameters are pis.
And we would present it as a vector also.
And this is more convenient
And we have one vector for each document.
And in this case, in theta,
Now, the difference between LDA and
PLSA is that in LDA, we're not going
Instead, we're going to force them to
So more specifically,
they will be drawn from two Dirichlet
the Dirichlet distribution is
So it gives us a probability of
Take, for example, pis, right.
So this Dirichlet distribution tells
And this distribution in itself is
of alphas.
Depending on the alphas, we can
ways but with full certain choices of
For example,
you might favor the choice of a relatively
Or you might favor generating
and this is controlled by alpha.
And similarly here, the topic or
from another Dirichlet
And note that here,
corresponding to our inference on
Whereas here,
beta has n values corresponding to
Now once we impose this price, then
And we start with joined pis from
the Dirichlet distribution and
And then, we're going to use the pi
to use, and this is of course
And similar here, we're not going
Instead, we're going to draw one
And then from this,
And the rest is very similar to the.
The likelihood function now
But there's a close connection between the
So I'm going to illustrate
So in the top,
you see PLSA likelihood function
It's copied from previous slide.
Only that I dropped the background for
So in the LDA formulas you
You see the first equation
And this is the probability of generating
And this formula is a sum of all
Inside a sum is a product of
multiplied by the probability of
So this is a very important formula,
And this is actually the core
And you might see other topic models
And they all rely on this.
So it's very important to understand this.
And this gives us a probability of
Now, next in the probability of
component in the LDA formula, but the LDA
And that's to account for
So they are drawn from the original
That's why we have to take an integral,
could possibly draw from
And similarly in the likelihood for
we also see further components added,
Right?
So basically in the area we're just
the uncertainties and we added of course
the choice of this parameters,
So this is a likelihood function for LDA.
Now, next to this, let's talk about the
Now the parameters can be now estimated
maximum likelihood estimate for LDA.
Now you might think about how many
You'll see there're a fewer parameters
parameters are alphas and the betas.
So we can use the maximum likelihood
Of course, it's more complicated because
more complicated.
But what's also important
parameters that we are interested
the coverage are no
In this case we have to
posterior inference to compute them based
Unfortunately, this
So we generally have to resort
And there are many methods available for
see them when you use different tool kits
these different extensions of LDA.
Now here we, of course, can't give
just know that they are computed based in
inference by using
But our math [INAUDIBLE],
in some of our math list,
And, especially when we use
then the algorithm looks very
So in the end,
So to summarize our discussion
these models provide
way of mining and analyzing topics
The best basic task setup is
we're going to output the k topics.
Each topic is characterized
And we're going to also output proportions
And PLSA is the basic topic model, and
And this is often adequate for
That's why we spend a lot of
Now LDA improves over
This has led to theoretically
However, in practice, LDA and
in practice PLSA and LDA would work
Now here are some suggested readings if
First is a nice review of
The second has a discussion about how
Now I've shown you some distributions and
But what exactly is a topic?
Can we use phrases to label the topic?
To make it the more easy to understand and
this paper is about the techniques for
The third one is empirical comparison
The conclusion is that they
[MUSIC]
[MUSIC]
This lecture is about
So far we have talked about multiple
how do we know which
So this has to do with evaluation.
Now to talk about evaluation one must
go back to the clustering bias that
Because two objects can be similar
we must clearly specify
Without that, the problem of
So this perspective is also
If you look at this slide, and
you can see we have two different
if you ask a question, which one is
You actually see, there's no way to answer
we'd like to cluster based on shapes,
And that's precisely why
crucial for evaluation.
In general,
one is direct evaluation, and
So in direct evaluation,
we want to answer the following questions,
clusters to the ideal clusters
So the closeness here can be assessed
from multiple perspectives and
of cluster result in multiple angles,
Now we also want to quantify
us to easily compare different measures
And finally, you can see, in this case,
by using humans, basically humans
desire to clustering bias.
Now, how do we do that exactly?
Well, the general procedure
Given a test set which consists
we can have humans to create
we're going to ask humans to partition
And they will use their judgments based
to generate what they think are the best
used to compare with the system generated
And ideally, we want the system results
results, but in general,
So we would like to then quantify the
clusters and the gold standard clusters.
And this similarity can also be measure
give us various meshes to quantitatively
And some of the commonly used measures
a cluster has a similar object from
And normalized mutual information
which basically measures
cluster of object in the system generally.
How well can you predict the cluster
vice versa?
And mutual information captures, the
and normalized mutual information is often
used for quantifying the similarity for
this evaluation purpose,
Now again a thorough discussion
these evaluation issues would be
I've suggested some reading in
at to know more about that.
So here I just want to
that would allow you to think about how
The second way to evaluate text
So in this case the question to answer is,
the intended applications?
Now this of course is application
usefulness is going to depend
In this case, the clustering bias is
as well, so
what counts as a best cluster result
Now procedure wise we also would create
the intended application to quantify
In this case,
clustering to some application so we often
This could be the current system for
then you hope to add
the baseline system could be using
And then what you are trying
you hope to have better
So in any case you have a baseline system
algorithm to the baseline system
And then we have to compare the
the baseline system in terms
that particular application.
So in this case we call it indirect
explicit assessment of
rather it's to assess the contribution
So, to summarize text clustering,
it's a very useful unsupervised
it's particularly useful for obtaining
And this is often needed
this is often the first step when
The second application or
discover interesting clustering
these structures can be very meaningful.
There are many approaches that can
we discussed model based approaches and
In general, strong clusters tend to
Also the effectiveness of a method
clustering bias is captured appropriately,
the right generating model, the model
the right similarity function
Deciding the optimal number of customers
order cluster methods, and that's
and there's no training there how to guide
Now sometimes you may see some methods
the number of clusters, but
application of clustering bias there and
Without clearly defining a clustering
the optimal number of cluster is what,
And I should also say sometimes we
the number of clusters, for example,
then obviously you don't want
the number can be dictated
In other situations, we might be
to assess whether we've got a good number
And to do that,
watch how well you can fit the data.
In general when you add a more components
the data better because you, you don't,
you can always set the probability
So you can't in general fit the data
is as you add more components would you be
of the data and that can be used to
And finally evaluation
this kind can be done both directly and
do both in order to get a good sense
So here's some suggested reading and
to better understand how the matches
[MUSIC]
[SOUND]
This lecture is about text categorization.
In this lecture, we're going to
This is a very important technique for
It is relevant to discovery
knowledge as shown here.
First, it's related to topic mining and
And, that's because it has to do with
analyzing text to data based
Secondly, it's also related to
which has to do with discovery knowledge
Because we can categorize the authors,
based on the content of the articles
We can, in general,
based on the content that they produce.
Finally, it's also related
Because, we can often use text
variables in the real world that
And so, this is a very important
This is the overall plan for
First, we're going to talk about
why we're interested in
And now, we're going to talk about
how to evaluate
So, the problem of text
We're given a set of predefined categories
And often,
training set of labeled text
objects have already been
And then, the task is to classify
more of these predefined categories.
So, the picture on this
When we do text categorization,
we have a lot of text objects to be
the system will, in general,
As shown on the right and
and we often assume the availability
these are the documents that
And these examples are very important for
helping the system to learn
And, this would further help
the categories of new text
So, here are some specific
And in fact, there are many examples,
So first, text objects can vary,
or a passage, or a sentence,
As in the case of clustering, the units
this creates a lot of possibilities.
Secondly, categories can also vary.
Allocate in general,
One is internal categories.
These are categories that
For example, topic categories or
they generally have to do with
throughout the categorization
The other kind is external categories
associated with the text object.
For example,
authors are entities associated
And so, we can use their content in
which part, for example, and
Or, we can have any
associate with text data
connection between the entity and
For example, we might collect a lot
a lot of reviews about a product,
this text data can help us infer
In that case, we can treat this
We can categorize restaurants or
categorize products based on
So, this is an example for
Here are some specific
News categorization is very
News agencies would like
categories to categorize
And, these virtual article
For example, in the biomedical domain,
MeSH stands for Medical Subject Heading,
characterize content of
Another example of application is spam
So, we often have a spam filter
to help us distinguish spams
this is clearly a binary
Sentiment categorization of
another kind of applications where we
negative or positive and
So, you can have send them to categories,
Another application is automatic
you might want to automatically sort your
one application of text categorization
The results are another important kind
to the right person to handle,
email messaging is generally routed
Different people tend to handle
And in many cases, a person would manually
But, if you can imagine,
text categorization system
And, this is a class file, the incoming
where each category actually corresponds
And finally, author attribution, as I just
it's another example of using text
some other entities.
And, there are also many variants
And so, first, we have the simplest case,
where there are only two categories.
And, there are many examples like that,
Applications with one distinguishing
documents for a particular query.
Spam filtering just distinguishing spams
Sometimes, classifications of
positive and a negative.
A more general case would be K-category
many applications like that,
So, topic categorization is often
multiple topics.
Email routing would be another example
if you route the email to
then there are multiple
So, in all these cases, there are more
Another variation is to have
where categories form a hierarchy.
Again, topical hierarchy is very common.
Yet another variation is
That's when you have multiple
then you hope to kind of
Further leverage the dependency of
each individual task.
Among all these binary categorizations
part of it also is because it's simple and
it can actually be used to perform
For example, a K-category
performed by using binary categorization.
Basically, we can look at
then the binary categorization problem
not, meaning in other categories.
And, the hierarchical categorization
doing flat categorization at each level.
So, we have, first, we categorize
a small number of high-level categories,
and inside each category, we have further
So, why is text categorization important?
Well, I already showed that you,
there are several reasons.
One is text categorization helps enrich
more understanding of text data that's
So, now with categorization text can
The keyword conditions that's often
But we can now also add categories and
Semantic categories assigned can also
application.
So, for example, semantic categories
other attribution might
Another example is when semantic
of text content and this is another case
For example, if we want to know
could first categorize the opinions
as positive or negative and then, that
the sentiment, and it would tell us about
the 70% of the views are positive and
So, without doing categorization,
it will be much harder to aggregate
way of coding text in some sense
And, sometimes you may see in some
called a text coded,
The second kind of reasons is to use text
categorization to infer
and text categories allows
of such entities that
So, this means we can
to discover knowledge about the world.
In general, as long as we can associate
we can always the text of data to help
So, it's used for
single information network that will
The obvious entities that can be
But, you can also imagine the author's
other things can be actually
Once we have made the connection, then we
So, this is a general way to allow
the text categorization to discover
Very useful, especially in big text
just using text data as extra sets
to infer certain decision factors
Specifically with text, for example,
we can also think of examples of
For example, discovery of
And, this can be done by categorizing
Another example is to predict the party
on the political speech.
And, this is again an example
some knowledge about the real world.
In nature,
that's as we defined and
[MUSIC]
[SOUND] This lecture is
of evaluation of text categorization.
Earlier we have introduced measures that
recall.
For each category and each document
further examine how to combine the
different documents how to aggregate them,
You see on the title here I indicated
this is in contrast to micro average
So, again, for each category we're going
for example category c1 we have
And similarly we can do that for category
Now once we compute that and
example we can aggregate
For all the categories, for
And this is often very useful to summarize
And aggregation can be
Again as I said, in a case when you
it's always good to think about what's
For example, we can consider arithmetic
you can use geometric mean,
Depending on the way you aggregate,
in terms of which method works better,
differences and choosing the right one or
So the difference fore example
geometrically is that the arithmetically
values whereas geometrically would
Base and so whether you are want
high values would be a question
similar we can do that for
So that's how we can generate the overall
Now we can do the same for aggregation
So it's exactly the same situation for
Precision, recall, and F.
And then after we have completed
we're going to aggregate them to generate
overall F score.
These are, again, examining
Which one's more useful will
In general, it's beneficial to look at
And especially if you compare different
it might reveal which method
in what situations and
Understanding the strands of a method or
this provides further insight for
So as I mentioned,
in contrast to the macro average
In this case, what we do is you
and then compute the precision and recall.
So we can compute the overall
how many cases are in true positive,
etc, it's computing the values
and then we can compute the precision and
In contrast, in macro-averaging, we're
And then aggregate over these categories
then aggregate all the documents but
Now this would be very similar to
used earlier, and
one problem here of course to treat all
And this may not be desirable.
But it may be a property for
especially if we associate the, for
Then we can actually compute for example,
Where you associate the different cost or
so there could be variations of these
But in general macro average tends to
just because it might reflect the need for
on each category or performance on each
But macro averaging and micro averaging,
and you might see both reported in
Also sometimes categorization
be evaluated from ranking prospective.
And this is because categorization
often indeed passed it to a human for
For example, it might be passed
For example, news articles can be tempted
then human editors would
And all the email messages might be
handling in the help desk.
And in such a case the categorizations
the task for
So, in this case the results
and if the system can't give a score
confidence then we can use the scores
then evaluate the results as a rank list,
Evaluation where you rank
So for example a discovery of
based on ranking emails for
And this is useful if you want people
spam, right?
The person would then take
then verify whether this is indeed a spam.
So to reflect the utility for
better to evaluate Ranking Chris and this
And in such a case often
better formulated as a ranking problem
So for example, ranking documents in
as a binary categorization problem,
are useful to users from those that
frame this as a ranking problem,
That's because people tend
ranking evaluation more reflects
So to summarize categorization evaluation,
first evaluation is always very
So get it right.
If you don't get it right,
And you might be misled to believe
which is in fact not true.
So it's very important to get it right.
Measures must also reflect
a particular application.
For example, in spam filtering and
news categorization the results
So then we would need to
design measures appropriately.
We generally need to consider how will the
and think from a user's perspective.
What quality is important?
What aspect of quality is important?
Sometimes there are trade offs between
recall and so we need to know for this
or high precision is more important.
Ideally we associate the different cost
And this of course has to be designed
Some commonly used measures for relative
Classification accuracy, it's very
[INAUDIBLE] preceding [INAUDIBLE]
report characterizing performances,
[INAUDIBLE] like a [INAUDIBLE] Per
take a average of all of them, different
In general, you want to look at the
particular applications some perspectives
diagnoses and
It's generally useful to look at
to see subtle differences between methods
from which you can obtain sight for
Finally sometimes ranking
be careful sometimes categorization has
and there're machine running methods for
So here are two suggested readings.
One is some chapters of this book where
evaluation measures.
The second is a paper about
text categorization and
it also has an excellent discussion of
[MUSIC]
[SOUND]
lecture is about
Contextual text mining
kinds of knowledge that we mine from
It's related to topic mining because you
like time or location.
And similarly, we can make opinion
making opinions connected to context.
It's related to text based prediction
data with text data to derive
the prediction problem.
So more specifically, why are we
Well, that's first because text
And this can include direct context such
So, the direct context can grow
location, authors, and
And they're almost always available to us.
Indirect context refers to additional
So for example, from office,
context such as social network of
Such information is not in general
through the process, we can connect them.
There could be other text
as this one through the other text can
So in general, any related data
So there could be removed or
And so what's the use?
What is text context used for?
Well, context can be used to partition
It can almost allow us to partition
And this is very important
us to do interesting comparative analyses.
It also in general,
if we associate the text with context.
So here's illustration of how context
can be regarded as interesting
So here I just showed some research
On different venues,
different conference names here listed on
Now such text data can be partitioned
in many interesting ways
So the context here just includes time and
But perhaps we can include
But let's see how we can partition
First, we can treat each
So in this case, a paper ID and the,
It's independent.
But we can also treat all the papers
this is only possible because
And we can partition data in this way.
This would allow us to compare topics for
Similarly, we can partition
We can get all the SIGIR papers and
Or compare SIGIR papers with KDD papers,
We can also partition the data to obtain
and that of course,
And this would allow us to then
another set of papers written
Or we can obtain a set of
this can be compared with
And note that these
intersected with each other to generate
And so in general, this enables
different context as needed.
And in particular,
And this often gives us
For example, comparing topics over time,
Comparing topics in different
about the two contexts.
So there are many interesting questions
Here I list some very specific ones.
For example, what topics have
recently in data mining research?
Now to answer this question,
obviously we need to analyze
So time is context in this case.
Is there any difference in the responses
to the event, to any event?
So this is a very broad
In this case of course,
What are the common research
In this case, authors can be the context.
Is there any difference in the research
those outside?
Now in this case,
their affiliation and location.
So this goes beyond just
We need to look at the additional
Is there any difference in the opinions
one social network and another?
In this case, the social network of
Other topics in news data that
stock prices.
In this case, we can use a time series
What issues mattered in the 2012
presidential election?
Now in this case,
So, as you can see,
Basically, contextual text mining
[MUSIC]
This lecture is a summary
First, let's revisit the topics
In the beginning, we talked about
how it can enrich text representation.
We then talked about how to mine
natural language used to express the,
what's observing the world in text and
In particular, we talked about
We then talked about how
How to discover topics and analyze them.
This can be regarded as
and then we talked about how to mine
particularly talk about the, how to
And finally, we will talk about
do with predicting values of other real
And in discussing this, we will also
which can contribute additional
and also can provide context for
in particular we talked about how
So here are the key high-level
I going to go over these major topics and
point out what are the key take-away
First the NLP and text representation.
You should realize that NLP
any text replication because it
The more NLP the better text
And this further enables more
to discover deeper knowledge,
However, the current estate of art
still not robust enough.
So, as an result,
tend to be based on world [INAUDIBLE].
And tend to rely a lot
as we've discussed in this course.
And you may recall we've mostly
And we've relied a lot on
statistical learning
In word-association mining and
we are introduced the two concepts for
complementary relations of words,
These are actually very general
If you take it as meaning
context in the sequence and elements
And these relations might be also
We also talked a lot about
discuss how to discover
the context of words discover
At that point level,
we talked about representing text
And we talked about some retrieval
measuring similarity of text and
tf-idf weighting, et cetera.
And this part is well-connected
There are other techniques that
The next point is about
we introduce some information
conditional entropy,
These are not only very useful for
measuring the co-occurrences of words,
analyzing other kind of data, and
feature selection in text
So this is another important concept,
And then we talked about
that's where we introduce in
We spent a lot of time to
PLSA in detail and this is, those are the
Theoretically, a more opinion model, but
we did not have enough time to really
But in practice,
it's simpler to implement and
In this part of Wilson videos is some
know, one is generative model,
modeling text data and
And we talked about the maximum life
solving the problem of
So, these are all general techniques
in other scenarios as well.
Then we talked about the text
Those are two important building blocks
In text with clustering we talked
using a slightly different mixture module
and we then also prefer to
approaches to test for cuss word.
In categorization we also talk
One is generative classifies
infer the condition of or
in deeper we'll introduce you should
This is the practical use for technique,
We also introduce the some
particularly logistical regression,
They also very important, they are very
text capitalization as well.
In both parts, we'll also discuss
Evaluation is quite important because if
reflect the volatility of the method then
its very important to
And we talked about variation of
specific measures.
Then we talked about the sentiment
that's where we introduced
And although it's a special
we talked about how to extend or
by using more sophisticated features that
We did a review of some common use for
then we also talked about how to
in sentiment classification, and
logistical regression then we also talked
This is an unsupervised way of using
review data in more detail.
In particular, it allows us to
a reviewer on different
So given text reviews
the method allows even further
And it also allows us to infer,
the viewers laying their
which aspects are more important to
And this enables a lot of
Finally, in the discussion of prediction,
of text and non text data, as they
We particularly talked about how text data
In the case of using non-text
we talked about
We introduced the contextual PLSA as a
to allows us to incorporate the context
And this is a general way to allow us
of patterns in text data.
We also introduced the net PLSA,
network in general of text
And finally we talk about how
mine potentially causal
Now, in the other way of using text to
help interpret patterns
we did not really discuss anything in
I should stress that that's after a very
if you want to build a practical
because understanding and
So this is a summary of the key
I hope these will be very
text mining applications or to you for
And this should provide a good basis for
to know more about more of allowance for
other organisms or
So to know more about this topic,
I would suggest you to look
And during this short period
we could only touch the basic concepts,
we emphasize the coverage
And this is after the cost
in many cases we omit the discussion
So to learn more about the subject
about the natural language process
all text based applications.
The more NLP you can do, the better
then the deeper knowledge
So this is very important.
The second area you should look into
And these techniques are now
not just text analysis applications but
A lot of NLP techniques are nowadays
So, they are very important
to also understanding some
naturally they will provide more tools for
Now, a particularly interesting area,
called deep learning has attracted
It has also shown promise
especially in speech and vision, and
So, for example, recently there has
segment analysis to
So that's one example of [INAUDIBLE]
but that's also very important.
And the other area that has emerged
baring technique, where they can
And then these better recognitions will
As you can see,
this provides directly a way to discover
And results that people have got,
That's another promising technique
but, of course,
would lead to practical useful techniques
technologies is still an open
And no serious evaluation
In, for example, examining
other than word similarity and
But nevertheless,
that surely will make impact
So its very important to
Statistical learning is also the key to
for many big data applications and we did
component but this is mostly about
techniques and this is another reason
We also suggest that you learn more about
general data mining algorithms can always
regarded as as special
So there are many applications
In particular for example, pattern
the interesting features for test analysis
that mining techniques can also be used
So these are all good to know.
In order to develop effective
And finally, we also recommend you to
information retrieval, of search engines.
This is especially important if you
application systems.
And a search ending would
component in any text-based applications.
And that's because texts data
So humans are at the best position
it's important to have human in the loop
it can in particular help text
One is through effectively reduce
a small collection with the most
the particular interpretation.
So the other is to provide a way to
and this has to do with
Once we discover some knowledge,
not the discovery is really reliable.
So we need to go back to
And that is why the search
Moreover, some techniques
for example BM25, vector space and
We only mention some of them,
text retrieval you'll see that there
Another technique that it's used for
response of search engine to a user's
very useful for building efficient
So, finally, I want to remind
harnessing big text data that I showed
So in general, to deal with
we need two kinds text,
And text retrieval, as I explained,
a small amount of most relevant data for
providing knowledge provenance,
Text mining has to do with further
the actionable knowledge that can be
many other tasks.
So this course covers text mining.
And there's a companion course
Search Engines that covers text retrieval.
If you haven't taken that course,
especially if you are interested
And taking both courses will give you
building such a system.
So in [INAUDIBLE]
taking this course.
I hope you have learned useful knowledge
As you see from our discussions
this kind of techniques and
So I hope you can use what you have
applications will benefit society and
the research community to discover new
Thank you.
[MUSIC]
