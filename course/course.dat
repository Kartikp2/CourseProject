[SOUND]
>> This lecture is about Natural Language
of Content Analysis.
As you see from this picture,
this is really the first step
Text data are in natural languages.
So computers have to understand
in order to make use of the data.
So that's the topic of this lecture.
We're going to cover three things.
First, what is natural
which is the main technique for processing
The second is the state of
natural language processing.
Finally we're going to cover the relation
text retrieval.
First, what is NLP?
Well the best way to explain it
a text in a foreign language
Now what do you have to do in
This is basically what
So looking at the simple sentence like
We don't have any problems
But imagine what the computer would
Well in general,
First, it would have to know dog
So this is called lexical analysis,
we need to figure out the syntactic
So that's the first step.
After that, we're going to figure
So for example, here it shows that A and
the dog would go together
And we won't have dog and is to go first.
And there are some structures
But this structure shows what we might
try to interpret the sentence.
Some words would go together first, and
then they will go together
So here we show we have noun phrases
then verbal phrases.
Finally we have a sentence.
And you get this structure.
We need to do something called
And we may have a parser
that would automatically
At this point you would know
still you don't know
So we have to go further
In our mind we usually can map
such a sentence to what we already
For example, you might imagine
There's a boy and
But for a computer would have
We'd use a symbol (d1) to denote a dog.
And (b)1 can denote a boy and
Now there is also a chasing
we have a relationship chasing
So this is how a computer would obtain
Now from this representation we could
and we might indeed naturally think of
this is called inference.
So for example, if you believe
this person might be scared,
you can see computers could also
So this is some extra knowledge
some understanding of the text.
You can even go further to understand
So this has to do as a use of language.
This is called pragmatic analysis.
In order to understand the speak
We say something to
There's some purpose there.
And this has to do with
In this case the person who said
this sentence might be reminding
That could be one possible intent.
To reach this level of
all of these steps and
these steps in order to completely
Yet we humans have no trouble
we instantly would get everything.
There is a reason for that.
That's because we have a large
we can use common sense knowledge
Computers unfortunately are hard
They don't have such a knowledge base.
They are still incapable of doing
so that makes natural language
But the fundamental reason why natural
computers is simply because natural
computers.
Natural languages are designed for
There are other languages designed for
For example, programming languages.
Those are harder for us, right?
So natural languages is designed to
As a result,
because we assume everyone
We also keep a lot of ambiguities because
know how to decipher an ambiguous word
There's no need to demand different
We could overload the same word with
Because of these reasons this makes every
difficult for computers,
And common sense and reasoning is
So let me give you some
Consider the word level ambiguity.
The same word can have
For example design can be a noun or
The word of root may
So square root in math sense or
You might be able to think
There are also syntactical ambiguities.
For example, the main topic of this
can actually be interpreted in two
Think for a moment and
We usually think of this as
but you could also think of this as do
So this is an example
What we have different is
applied to the same sequence of words.
Another common example of an ambiguous
A man saw a boy with a telescope.
Now in this case the question is,
This is called a prepositional
PP attachment ambiguity.
Now we generally don't have a problem with
background knowledge to help
Another example of difficulty
So think about the sentence John
The question here is does
So again this is something that
the context to figure out.
Finally, presupposition
Consider the sentence,
Now this obviously implies
So imagine a computer wants to understand
It would have to use a lot of
It also would have to maintain a large
words and how they are connected to our
So this is why it's very difficult.
So as a result, we are steep not perfect,
in fact far from perfect in understanding
So this slide sort of gains a simplified
We can do part of speech
I showed 97% accuracy here.
Now this number is obviously
don't take this literally.
This just shows that we
But it's still not perfect.
In terms of parsing,
That means we can get noun phrase
or some segment of the sentence, and
this dude correct them in
And in some evaluation results,
accuracy in terms of partial
Again, I have to say these numbers
In some other datasets,
Most of the existing work has been
And so a lot of these numbers are more or
Think about social media data,
In terms of a semantical analysis,
we are far from being able to do
But we have some techniques
do partial understanding of the sentence.
So I could mention some of them.
For example, we have techniques that can
relations mentioned in text articles.
For example,
locations, organizations, etc in text.
So this is called entity extraction.
We may be able to recognize the relations.
For example,
this person met that person or
Such relations can be extracted by using
the computer current
They're not perfect but
Some entities are harder than others.
We can also do word sense
We have to figure out whether this word in
in another context the computer could
Again, it's not perfect, but
We can also do sentiment analysis,
meaning, to figure out whether
This is especially useful for
So these are examples
And they help us to obtain partial
It's not giving us a complete
this sentence.
But it would still help us gain
And these can be useful.
In terms of inference,
probably because of the general difficulty
This is a general challenge
Now that's probably also because
representation for
So this is hard.
Yet in some domains perhaps,
restrictions on the word uses, you may be
But in general we can not
Speech act analysis is also
we can only do that analysis for
So this roughly gives you some
And then we also talk a little
and so we can't even do 100%
Now this looks like a simple task, but
think about the example here,
have different syntactic categories if you
It's not that easy to figure
It's also hard to do
And again, the same sentence
This ambiguity can be very hard to
where you have to use a lot of knowledge
from the background, in order to figure
So although the sentence looks very
And in cases when the sentence is
five prepositional phrases, and there
It's also harder to do precise
So here's an example.
In the sentence "John owns a restaurant."
The word own,
it's very hard to precisely describe
So as a result we have a robust and
Natural Language Processing techniques
In a shallow way,
For example, parts of speech tagging or a
And those are not deep understanding,
because we're not really understanding
On the other hand of the deep
up well, meaning that they would
And if you don't restrict
the use of words, then these
They may work well based on machine
that are similar to the training data
But they generally wouldn't work well on
the training data.
So this pretty much summarizes the state
Of course, within such a short amount
a complete view of NLP,
And I'd expect to see multiple courses on
But because of its relevance to the topic
you to know the background in case
So what does that mean for Text Retrieval?
Well, in Text Retrieval we
It's very hard to restrict
And we also are often dealing
So that means The NLP techniques must
And that just implies today we can only
text retrieval.
In fact,
most search engines today use something
Now, this is probably the simplest
That is to turn text data
Meaning we'll keep individual words, but
And we'll keep duplicated
So this is called a bag
When you represent text in this way,
That just makes it harder to understand
because we've lost the order.
But yet this representation tends
most search tasks.
And this was partly because the search
If you see matching of some of
chances are that that document is about
So in comparison of some other tasks, for
example, machine translation would require
Otherwise the translation would be wrong.
So in comparison such tasks
Such a representation is often sufficient
the major search engines today,
Of course, I put in parentheses but
that are not answered well by
they do require the replantation that
That would require more natural
There was another reason why we
NLP techniques in modern search engines.
And that's because some
naturally solved the problem of NLP.
So one example is word
Think about a word like Java.
It could mean coffee or
If you look at the word anome,
when the user uses the word in the query,
For example, I'm looking for
When I have applet there,
And that contest can help us
which Java is referring
Because those documents would
If Java occurs in that
then you would never match applet or
So this is the case when
naturally achieve the goal of word.
Another example is some technique called
feedback which we will talk about
This technique would allow us to add
those additional words could
And these words can help matching
have not occurred.
So this achieves, to some extent,
So those techniques also helped us
bypass some of the difficulties
However, in the long run we still need
techniques in order to improve the
And it's particularly needed for
Or for question and answering.
Google has recently launched a knowledge
that goal, because knowledge graph would
And this goes beyond the simple
And such technique should help us
significantly, although this is the open
In sum, in this lecture we
we've talked about the state
What we can do, what we cannot do.
And finally, we also explain why
remains the dominant replantation
even though deeper NLP would be needed for
If you want to know more, you can take
I only cited one here and
Thanks.
[MUSIC]
[SOUND]
lecture is the first one
In this lecture, we are going to
This is a very important technique for
In particular,
in this lecture we're going to start with
And that is, what is text clustering and
In the following lectures, we are going
How to evaluate the clustering results?
So what is text clustering?
Well, clustering actually is
data mining as you might have
The idea is to discover natural
In another words,
In our case, these objects are of course,
For example, they can be documents,
sentences, or websites, and then I'll
So let's see an example, well, here
I just used some shapes to denote
Now if I ask you, what are some natural
if you look at it and you might agree that
or their locations on this
So we got the three clusters in this case.
And they may not be so
these three clusters but it really depends
Maybe some of you have also seen
we might get different clusters.
And you'll see another example
But the main point of here is, the problem
And the problem lies in
And what do you mean by similar objects?
Now this problem has to be
clearly defined in order to have
And the problem is in general
that any two objects can be similar
So for example, this will kept
So are the two words similar?
Well, it depends on how if
properties of car and
if you look at them functionally,
can both be transportation tools.
So in that sense, they may be similar.
So as we can see, it really depends on
And so it ought to make
A user must define the perspective for
And we call this perspective
And when you define a clustering problem,
your perspective for
the similarity that will be
because otherwise,
one can have different
So let's look at the example here.
You are seeing some objects,
that are very similar to what you
if I ask you to group these objects,
feel there is more than here
For example, you might think, well, we
can steer a group by ships, so that would
However, you might also feel that,
well, maybe the objects can be
So that would give us a different way
the size and
So as you can see clearly here,
we'll get different clustering result.
So that also clearly tells us that in
we must use perspective.
Without perspective, it's very hard to
So there are many examples
And so for example, we can cluster
So in this case,
We may be able to cluster terms.
In this case, terms are objects.
And a cluster of terms can be used to
In fact, there's a topic models that you
give you cluster of terms in some
high probabilities from word distribution.
Another example is just to cluster any
sentences, or any segments that you can
For example, we might extract the order
let's say, by using a topic model.
Now once we've got those
cluster the segments that we've got to
discover interesting clusters that
So this is a case of combining text
And in general you will
can be accurate combined in
the goal of doing more sophisticated
We can also cluster fairly
I just mean text objects may
So for example, we might cluster websites.
Each website is actually
Similarly, we can also cluster articles
So we can trigger all the articles
clustering.
In this way, we might group authors
published papers or similar.
For the more text clusters will be for
That's because we can in general cluster
So more generally why is
Well, it's because it's a very
particularly exploratory text analysis.
And so a typical scenario is that
let's say all the email messages
all the literature articles, etc.
And then you hope to get a sense
the connection, so for example,
a sense about major topics,
representative documents
And clustering to help
We sometimes also want to link
And these objects might be
And in that case,
such a technique can help us remove
Sometimes they are about
by linking them together we can have
We may also used text clustering to create
we can create a hierarchy of structures
We may also use text clustering to induce
data when we cluster documents together,
And then we can say when
then the feature value would be one.
And if a document is not in this cluster,
And this helps provide additional
text classification as
So there are, in general,
And I just thought of
One is to cluster search results, for
example, [INAUDIBLE] search engine
that the user can see overall structure
And when the query's ambiguous this
the clusters likely represent
Another application is to understand the
their emails, right.
So in this case,
then find in the major
we can understand what are the major
[MUSIC]
[SOUND]
lecture is about the discriminative
In this lecture we're going to
do text categorization and
This is a slide that you have seen from
where we have shown that although
the generation of text data, from each
rule to eventually rewrite the scoring
And this scoring function is basically
of word features, where the feature values
are the log of probability ratios of
Now this kind of scoring function
function where we can in general
Of course the features don't
Their features can be other
And we mentioned that this is precisely
So, in this lecture we're going to
They try to model
labels given the data directly
to compute that interactively
So the general idea of logistic
the dependency of a binary
on some predictors that are denoted as X.
So here we have also changed the notation
to X for future values.
You may recall in the previous
FI to represent the future values.
And here we use the notation of X factor,
which is more common when we introduce
such discriminative algorithms.
So, X is our input.
It's a vector with n features and
And I will go with a model that dependency
these features.
So in our categorization problem when
theta 2, and we can use the Y value to
it means the category of the document,
Now, the goal here is the model, the
as opposed to model of the generation of
And another advantage of this
it would allow many other features
since we're not modeling
And we can plug in any
So this is potentially advantageous for
So more specifically,
assume the functional form of Y
And this is very closely
odds that I introduced in the Naive Bayes
of the two categories that you
So this is what I meant.
So in the case of Naive Bayes,
eventually we have reached
But here we actually
that we with the model our
probability of Y given X
directly as a function of these features.
So, most specifically we assume that the
the probability of Y equals
All right, so it's a function of x and
it's a linear combination of these feature
And it seems we know that
is one minus probability
this can be also written in this way.
So this is a log out ratio here.
And so in logistic regression,
we're basically assuming that
Okay my X is dependent on this linear
So it's just one of the many possible
But this particular form
it also has some nice properties.
So if we rewrite this equation to actually
In terms of X by getting rid of
and this is called a logistical function.
It's a transformation of X into Y,
on the right side here, so
into a range of values from 0 to 1.0,
And that's precisely what we want
And the function form looks like this.
So this is the basic idea
And it's a very useful classifier that
can be used to do a lot of classification
So as in all cases of model we would be
And in fact in all of the machine running
set up object and
then the next step is to
In general, we're going to adjust
Optimize the performance of
So in our case just assume we have
each pair is basically a future vector
Y is either 1 or 0.
So in our case we are interested
The conditional likelihood here is
basically to model why
so it's not like a moderate x, but
rather we're going to model this.
Note that this is a conditional
this is also precisely what we wanted For
Now so the likelihood function would be
And in each case,
this is the model of the probability of
So given a particular Xi, how likely
Of course, Yi could be 1 or
the function found here would vary
If it's a 1, we'll be taking this form.
And that's basically the logistic
But what about this, if it's 0?
Well, if it's 0, then we have to use
Now, how do we get this one?
Well, that's just a 1 minus
And you can easily see this.
Now the key point in here is that the
Yi, if it's a 1,
And if you think about when we
we're basically going to want this
When the label is 1, that means
But if the document is not,
and what's going to happen is
small as possible because this sum's 1.
When I maximize this one,
So you can see basically, if we maximize
to basically try to make the prediction on
So as another occasion, when you
basically you'll find a beta value,
a set of beta values that would
And this, again, then gives us
In this case,
Newton's method is a popular
there are other methods as well.
But in the end,
Once we have the beta values,
function to help us classify a document.
So what's the function?
Well, it's this one.
See, if we have all the beta values,
All we need is to compute the Xi for that
That will give us an estimated probability
Okay so, so much for
Let's also introduce another
called K-Nearest Neighbors.
Now in general, I should say there
a thorough introduction to all of them is
And you should take
read more about machine
Here, I just want to include the basic
used classifiers, since you might
So the second classifier is
In this approach,
the conditional probability of label
So the idea is to keep all
then once we see a text object that we
the K examples in the training set and
Basically, this is to find
the training data set.
So once we found the neighborhood and
we found the object that are close to the
and let's say we have found
That's why this method is
Then we're going to assign the category
Basically we're going to allow
the category of the objective that
Now that means if most of them have
one, they're going to say this
This approach can also be improved by
of a current object.
Basically, we can assume a closed
about the category of the subject.
So, we can give such a neighbor
And we can take away some of
But the general idea is look
then try to assess the category based
Intuitively, this makes a lot of sense.
But mathematically, this can also be
there's a conditional probability of
Now I'm going to explain this intuition in
emphasize that we do need a similarity
Note that in naive base class five,
And in logistical regression, we did not
either, but here we explicitly
Now this similarity function
us to inject any of our
Basically effective features
make the objects that are on the same
distinguishing objects
So the design of this similarity function
of the features in logistical
So let's illustrate how K-NN works.
Now suppose we have a lot
And I've colored them differently and
Now suppose we have a new object in
So according to this approach,
Now, let's first think of a special
the closest neighbor.
Now in this case, let's assume the closest
And so then we're going to say,
object that is in category of diamonds,
Then we're going to say, well,
we're going to assign the same
But let's also look at another possibility
so let's think about the four neighbors.
In this case, we're going to include a lot
pink, right?
So in this case now, we're going to
there are three neighbors
So if we take a vote,
then we'll conclude the object is
So this both illustrates how
also it illustrates some potential
Basically, the results might
k's an important parameter to optimize.
Now, you can intuitively imagine
around this object, and
a lot of neighbors who will
But if we have only a few,
So on the one hand,
And then we have more votes.
But on the other hand, as we try to find
on getting neighbors that are not
They might actually be far away
So although you get more neighbors but
helpful because they are not
So the parameter still has
And typically, you can optimize such
Basically, you're going to separate
then you're going to use one
the parameter k here or some other
And then you're going to assume
training that will be actually be
So as I mentioned,
K-NN can be actually regarded as estimate
an that's why we put this in the category
So the key assumption that we made in
of the label given the document
example probability of theta i
And that just means we're going to assume
all the documents in these region R here.
And suppose we draw a neighborhood and
since the data instances are very
the conditional distribution of the label
If these are very different
the probability of c doc given
So that's a very key assumption.
And that's actually important assumption
that would allow us to
But in reality,
whether this is true of course,
Because neighborhood is largely
If our similarity function captures
distributions then these
if our similarity function could
assumption would be a problem and
Okay, let's proceed with these assumption.
Then what we are saying is that,
in order to estimate the probability
We can try to estimate the probability of
Now, this has a benefit, of course,
of bringing additional data points to
And so this is precisely the idea of K-NN.
Basically now we can use
all the documents in this region
And I have even given a formula here where
this region and then normalize that by the
So the numerator that you see here,
is a counter of the documents in
Since these are training document and
We can simply count how many
How many times we have the same signs,
And then the denominator is just
documents in this region.
So this gives us a rough estimate of
neighborhood.
And we are going to assign
to our data object since
[MUSIC]
[MUSIC]
This lecture is about
Contextual Text Mining called Contextual
In this lecture, we're going to continue
And we're going to introduce Contextual
as exchanging of POS for
Recall that in contextual text mining
in consideration of the context so
that we can associate the topics with a
So in this approach, contextual
or CPLSA, the main idea is to
context variables into a generating model.
Recall that before when we generate
wIth some topics, and
But here, we're going to add context
and also the content of topics
Or in other words, we're going to let
the content of a topic.
The consequences that this will enable
Make the topics more interesting,
Because we can then have topics
specifically to a particular
For example, a particular time period.
As an extension of PLSA model,
CPLSA does the following changes.
Firstly it would model the conditional
That clearly suggests that the generation
and that allows us to bring
Secondly, it makes two specific
of topics on context.
One is to assume that depending on
periods or different locations, we assume
or different versions of word
And this assumption allows
variations of the same topic
The other is that we assume the topic
That means depending on the time or
location, we might cover
Again, this dependency
capture the association of
We can still use the EM algorithm to solve
And in this case, the estimated parameters
And in particular,
a lot of conditional probabilities
And this is what allows you
So this is the basic idea.
Now, we don't have time to
but there are references here that you
Here I just want to explain the high
Particularly I want to explain
Of text data that has context
So as you see here, we can assume
For example, some topics might represent
donation Or the city of New Orleans.
Now this example is in the context
that hit New Orleans.
Now as you can see we
views associated with each of the topics.
And these are shown as View 1,
Each view is a different
And these views are tied
For example, tied to the location Texas,
or the occupation of the author
Now, on the right side, now we assume
So the time is known to be July 2005.
The location is Texas, etc.
And such context information is
So we're not going to just model the text.
And so one idea here is to model
various content.
And this gives us different views
Now on the bottom you will see the theme
according to these context
of a location like Texas, people might
That's New Orleans.
That's visualized here.
But in a certain time period,
maybe Particular topic and
So this variation is
So to generate the searcher document With
And this view of course now could
Let's say, we have taken this
In the middle.
So now, we will have a specific
Now, you can see some probabilities
Now, once we have chosen a view,
now the situation will be very similar
We assume we have got word distribution
And then next, we will also choose
we're going to choose a particular
before is fixed in PLSA, and
Each document has just one
Now here, because we consider context, so
of Topics can vary depending on the
So, for example,
Let's say in this case we picked
Now with the coverage and
we can generate a document in
So what it means, we're going to
to choose one of these three topics.
Let's say we have picked the yellow topic.
Then we'll draw a word from this
Okay, so
And then next time we might
we'll get donate, etc.
Until we generate all the words.
And this is basically
So the main difference is
And the word distribution,
in other words we have extra switches
control the choices of different views
And naturally the model we have
But once we can estimate those
then we will be able to understand
or context specific coverages of topics.
And this is precisely what we
So here are some simple results.
From using such a model.
Not necessary exactly the same model,
So on this slide you see
comparing news articles about Iraq War and
Now we have about 30 articles on Iraq
And in this case,
It's covered in both sets of articles and
the differences of variations of
So in this case the context is explicitly
And we see the results here
theme that's corresponding to
And there is a common theme indicting that
It's a common topic covered
And that's indicated by the high
nations.
Now if you know the background,
this topic is indeed very
If you look at the column further and
two cells of word
collection specific variations
So it indicates that the Iraq War,
United Nations was more involved
the Afghanistan War it was more involved
It's a different variation of
So this shows that by
In this case different the walls or
We can have topical variations
to review the differences of coverage
Now similarly if you look at
it has to do with the killing of people,
it's not surprising if you know
All the wars involve killing of people,
imagine if you are not familiar
We have a lot of text articles, and
such a technique can reveal the common
It can be used to review common topics
If you look at of course in
you see variations of killing of people
And here is another example of results
obtained from blog articles
In this case,
the trends of topics over time.
And the top one shows just
One is oil price, and one is about
Now these topics are obtained from
And people talk about these topics.
And end up teaching to some other topics.
But the visualisation shows
we can have conditional
Given a topic.
So this allows us to plot
the curve is like what you're seeing here.
We see that, initially, the two
But later we see the topic of New Orleans
And this turns out to be
the time period when another hurricane,
And that apparently triggered more
The bottom curve shows
about flooding of the city by block
And it also shows some shift of
people's migrating from the state
So in this case we can see the time can
topics.
These are some additional
In this case it was about
And there was some criticism about
in the case of Hurricane Katrina.
And the discussion now is
And these visualizations show the coverage
And initially it's covered
in the South, but then gradually
But in week four,
we see a pattern that's very similar
And that's when again
So such a technique would allow
to examine their issues of topics.
And of course the moral
you can apply this to any
To review spatial temporal patterns.
His view found another application
where we look at the use of the model for
So here we're looking at the research
IR, particularly SIGIR papers.
And the topic we are focusing on
And you can see the top words with high
And then we hope to examine
One is a start of TREC, for
This is a major evaluation
government, and was launched in 1992 or
And that is known to have made a impact on
the topics of research
The other is the publication of
This is about a language model
It's also known to have made a high
So we hope to use this kind of
The idea here is simply to
And use these events to divide
For the event and
And then we can compare
The and the variations, etc.
So in this case,
retrieval models was mostly a vector
But the after Trec,
apparently the study of retrieval models
That seems to suggest some
example, email was used in
subtopical retrieval was another
On the bottom,
with the propagation of
Before, we have those classic
logic model, Boolean etc., but after 1998,
we see clear dominance of language
And we see words like language model,
So this technique here can use events as
Again the technique is generals so
you can use this to analyze
Here are some suggested readings.
The first is paper about simple staging of
It's to perform comparative
extract common topics shared
And there are variations
The second one is the main
Was a discussion of a lot of applications.
The third one has a lot of details
the Hurricane Katrina example.
[MUSIC]
[SOUND]
In this lecture, we are going to talk about how
of the vector space model.
This is a continued discussion
We're going to focus on how to improve
In the previous lecture,
you have seen that with simple
we can come up with a simple scoring
an account of how many unique query
We also have seen that this function
In particular,
they will all get the same score because
But intuitively we would like
d2 is really not relevant.
So the problem here is that this function
First, we would like to give
matched presidential more times than d3.
Second, intuitively, matching presidential
matching about, because about is a very
It doesn't really carry that much content.
So in this lecture,
let's see how we can improve the model
It's worth thinking at this point
If we look back at assumptions we have
space model,
is really coming from
In particular, it has to do with how we
So then naturally,
we have to revisit those assumptions.
Perhaps we will have to use different ways
In particular, we have to place
So let's see how we can improve this.
One natural thought is in order to
the document,
we should consider the term frequency
In order to consider the difference
term occurred multiple times and one
we have to consider the term frequency,
In the simplest model, we only modeled
We ignored the actual number of times
So let's add this back.
So we're going to then
a vector with term frequency as element.
So that is to say, now the elements
the document vector will not be 0 or
instead they will be the counts of
So this would bring in additional
this can be seen as more accurate
So now let's see what the formula
representation.
So as you'll see on this slide,
And so the formula looks
In fact, it looks identical.
But inside the sum, of course,
They are now the counts of word i in
the query and in the document.
Now at this point I also suggest you
just to think about how we can interpret
It's doing something very similar
But because of the change of the vector,
now the new score has
Can you see the difference?
And it has to do with the consideration
the same term in a document.
More importantly, we would like to know
of the simplest vector space model.
So let's look at this example again.
So suppose we change the vector
Now let's look at these
The query vector is the same
exactly once in the query.
So the vector is still a 01 vector.
And in fact, d2 is also essentially
because none of these words
As a result,
The same is true for d3,
But d4 would be different, because
So the ending for presidential in the
As a result, now the score for
It's a 4 now.
So this means by using term frequency,
we can now rank d4 above d2 and
So this solved the problem with d4.
But we can also see that d2 and
They still have identical scores,
So how can we fix this problem?
Intuitively, we would like
matching presidential than matching about.
But how can we solve
Is there any way to determine
more importantly and
About is such a word which does not
We can essentially ignore that.
We sometimes call such
Those are generally very frequent and
Matching it doesn't really mean anything.
But computationally how
So again, I encourage you to
Can you came up with any statistical
presidential from about?
Now if you think about it for a moment,
you'll realize that one difference is
So if you count the occurrence of
then we will see that about has much
which tends to occur
So this idea suggests
the global statistics of terms or
some other information
the element of about in
At the same time,
the weight of presidential
If we can do that, then we can
score to be less than 3 while
Then we would be able to
So how can we do this systematically?
Again, we can rely on
And in this case, the particular idea
Now we have seen document
the modern retrieval functions.
We discussed this in a previous lecture.
So here is the specific way of using it.
Document frequency is the count of
Here we say inverse document frequency
that doesn't occur in many documents.
And so the way to incorporate this
is then to modify the frequency
the IDF of the corresponding word,
If we can do that,
which generally have a lower IDF, and
reward rare words,
So more specifically,
the IDF can be defined as
where M is the total number of documents
document frequency, the total number
Now if you plot this
then you would see the curve
In general, you can see it
a low DF word, a rare word.
You can also see the maximum value
It would be interesting for you to think
this function.
This could be an interesting exercise.
Now the specific function
the heuristic to simply
But it turns out that this particular
Now whether there's a better
the open research question.
But it's also clear that if
like what's shown here with this line,
then it may not be as
In particular, you can see
and we somehow have
After this point, we're going to say these
They can be essentially ignored.
And this makes sense when
let's say a term occurs in more
then the term is unlikely very important
It's not very important
So with the standard IDF you can
they all have low weights.
There's no difference.
But if you look at
at this point that there
So intuitively we'd want to
of low DF words rather
Well, of course,
validated by using the empirically
And we have to use users to
So now let's see how
So now let's look at
Now without the IDF weighting before,
But with IDF weighting we
by multiplying with the IDF value.
For example,
in particular for about there's adjustment
which is smaller than the IDF
So if you look at these,
As a result, adjustment here would be
So if we score with these new vectors,
of course,
campaign, but the matching of
So now as a result of IDF weighting,
because it matched a rare word,
So this shows that the IDF
So how effective is this model in
Well, let's look at all these
These are the new scores
But how effective is this new weighting
So now let's see overall how effective
with TF-IDF weighting.
Here we show all the five documents
these are their scores.
Now we can see the scores for
the first four documents here
They are as we expected.
However, we also see a new
which did not have a very high score
now actually has a very high score.
In fact, it has the highest score here.
So this creates a new problem.
This is actually a common phenomenon
Basically, when you try
you tend to introduce other problems.
And that's why it's very tricky how
And what's the best ranking function
Researchers are still working on that.
But in the next few lectures we're going
ideas to further improve this model and
So to summarize this lecture, we've talked
model, and
the vector space model
So the improvement is mostly on
give high weight to a term that
infrequently in the whole collection.
And we have seen that this
looks better than the simplest
But it also still has some problems.
In the next lecture we're going to look at
[MUSIC]
[MUSIC]
This lecture is about Evaluation of
lectures, we have talked about
different kinds of ranking functions.
But how do we know which
In order to answer this question,
we have to compare them and that means we
So this is the main topic of this lecture.
First, lets think about why
I already give one reason.
That is, we have to use evaluation
works better.
Now this is very important for
Otherwise, we wouldn't know whether a new
In the beginning of this course, we talked
We compare it with data base retrieval.
There we mentioned that text retrieval
So evaluation must rely on users.
Which system works better,
So, this becomes a very
because how can we get users
How can we do a fair comparison
So just go back to the reasons for
I listed two reasons here.
The second reason, is basically what I
reason which is to assess the actual
Imagine you're building your
it would be interesting knowing how well
So in this case,
matches must reflect the utility to
And typically, this has to be
using the real search engine.
In the second case, or the second reason,
the measures actually all need to collated
Thus, they don't have to accurately
So the measure only needs to be good
And this is usually done
And this is the main idea that we'll
This has been very important for
for improving search
So let's talk about what to measure.
There are many aspects of searching
And here,
One, is effectiveness or accuracy.
How accurate are the search results?
In this case, we're measuring a system's
on top of non relevant ones.
The second, is efficiency.
How quickly can you get the results?
How much computing resources
In this case, we need to measure the space
The third aspect is usability.
Basically the question is,
Here, obviously, interfaces and
many other things also important and
Now in this course, we're going to
accuracy measures.
Because the efficiency and
usability dimensions are not
And so they are needed for
And there is also good coverage
But how to evaluate search
something unique to text retrieval and
The main idea that people have proposed
the text retrieval algorithm is called
This one actually was developed
It's a methodology for
Its sampling methodology that has
search engine evaluation.
But also for evaluating virtually
for example in natural language processing
is empirical to find, we typically
And today with the big data challenging
with the use of machine
This methodology has been very popular,
a search engine application in the 1960s.
So the basic idea of this approach is
define measures.
Once such a test collection is built,
again to test different algorithms.
And we're going to define measures
performance of a system and algorithm.
So how exactly will this work?
Well we can do have a sample collection of
the real document collection
We're going to also have a sample
This is a little simulator
Then, we'll have to have
These are judgments of which documents
Ideally, they have to be made by
Because those are the people that know
And finally, we have to have matches for
quantify how well our system's result
That would be constructed base
So this methodology is very useful for
because the test can be reused many times.
And it will also provide a fair
We have the same criteria or
same dataset to be used to
This allows us to compare
an old algorithm that was divided many
So this is the illustration of this works,
we need our queries that are showing here.
We have Q1, Q2 etc.
We also need the documents and
on the right side you will see
These are basically the binary judgments
So for example,
D2 is judged as being relevant as well,
And the Q1 etc.
These will be created by users.
Once we have these, and
And then if you have two systems,
then you can just run each
the documents and
Let's say if the queries Q1 and
Here I show R sub A as
So this is, remember we talked about
task of computing approximation
R sub A is system A's approximation here.
And R sub B is system B's
Now, let's take a look at these results.
So which is better?
Now imagine if a user,
Now let's take a look at the both results.
And there are some differences and
there are some documents that
But if you look at the results,
A is better in the sense that we don't
And among the three documents returned,
So that's good, it's precise.
On the other hand one council
because we've got all of
We've got three instead of two.
So which one is better and
Well, obviously this question
It depends on users as well.
You might even imagine for
If the user is not interested in
Right, in this case the user doesn't
see most of the relevant documents.
On the other hand,
to have as many random
For example, if you're doing a literature
and you might find that
So in the case, we will have to also
And we might need it to define multiple
perspectives of looking at the results.
[MUSIC]
[SOUND]
lecture is about
In this lecture,
we're going to continue the discussion
We're going to look at another kind of
functions than the Vector Space Model
In probabilistic models,
based on the probability that this
In other words, we introduce
This is the variable R here.
And we also assume that the query and
the documents are all observations
Note that in the vector-based models,
here we assume they are the data
And so, the problem of retrieval becomes
In this category of models,
The classic probabilistic model has
which we discussed in in
because its a form is actually
In this lecture,
P class called a language
In particular, we're going to discuss
which is one of the most effective
There was also another line called
which has led to the PL2 function,
it's also one of the most effective
In query likelihood, our assumption
can be approximated by the probability
So intuitively, this probability just
And that is if a user likes document d,
the user enter query q ,in
So we assume that the user likes d,
And then we ask the question about how
from this user?
So this is the basic idea.
Now, to understand this idea,
the basic idea of
So here, I listed some imagined
relevance judgments of queries and
For example, in this line,
it shows that q1 is a query
And d1 is a document
And 1 means the user thinks
So this R here can be also approximated
engine can collect by watching how you
So in this case, let's say
So there's a 1 here.
Similarly, the user clicked on d2 also,
In other words,
On the other hand,
And d4 is non-relevant and then d5 is
And this part, maybe,
So this user typed in q1 and then found
so d1 is actually non-relevant.
In contrast, here we see it's relevant.
Or this could be the same query typed
But d2 is also relevant, etc.
And then here,
Now, we can imagine we
Now we can ask the question,
how can we then estimate
So how can we compute this
Well, intuitively that just means
if we look at all the entries
this particular q, how likely we'll
So basically that just means that
We can first count how many
d as a pair in this table and
we actually have also seen
And then, we just compute the ratio.
So let's take a look at
Suppose we are trying to compute this
What is the estimated probability?
Now, think about that.
You can pause the video if needed.
Try to take a look at the table.
And try to give your
Have you seen that,
we'll be looking at these two pairs?
And in both cases, well,
actually, in one of the cases, the user
So R = 1 in only one of the two cases.
In the other case, it's 0.
So that's one out of two.
What about the d1 and the d2?
Well, they are here, d1 and d2, d1 and d2,
in both cases, in this case, R = 1.
So it's a two out of two and
So you can see with this approach,
we can actually score these documents for
We now have a score for d1,
And we can simply rank them
so that's the basic idea
And you can see it makes a lot of sense,
it's going to rank d2 above
Because in all the cases,
The user clicked on this document.
So this also should show that
a search engine can learn a lot from
This is a simple example
with small amount of entries here we can
These probabilities would give us
might be more relevant or more useful
Now, of course, the problems that we
all the documents and
There would be a lot of unseen documents,
we have only collected the data from the
And there are even more unseen queries
queries will be typed in by users.
So obviously,
it to unseen queries or unseen documents.
Nevertheless, this shows the basic idea
it makes sense intuitively.
So what do we do in such a case when
unseen queries?
Well, the solutions that we have
So in this particular case called
we just approximate this by
p(q given d, R=1).
So in the condition part, we assume that
have seen that the user
And this part shows that
likely the user would
How likely we will see this
So note that here, we have made
Basically, we're going to do, assume that
has something to do with whether
In other words,
And that is a user formulates a query
Where if you just look at this
it's not obvious we
So what I really meant is that
probability to help us score,
probability will have to somehow
conditional probability without
Otherwise we would be having
by making this assumption,
and try to just model how the user
So this is how you can
that we can derive a specific
So let's look at how this model work for
And basically,
what we are going to do in this case
Which of these documents is most
document in the user's mind when
So we ask this question and we quantify
a conditional probability of observing
fact the imaginary relevant
Here you can see we've computed all
The likelihood of queries
Once we have these values,
we can then rank these documents
So to summarize, the general idea
risk model is to assume the we introduce
And then,
let the scoring function be defined
We also talked about approximating
And in this case we have a ranking
based on the probability of
And this probability should be interpreted
likes document d, would pose query q.
Now, the question of course is, how do
At this in general has to do with how
because q is a text.
And this has to do with a model
And these kind of models
So more specifically, we will be
conditional probability
If the user liked this document,
And in the next lecture we're going to do,
giving introduction to language
can model text that was a probable
[MUSIC]
[SOUND]
lecture is about the feedback
So in this lecture, we will continue with
In particular, we're going to talk
This is a diagram that shows
We can see the user would type in a query.
And then, the query would be
search engine, and
These results would be issued to the user.
Now, after the user has
the user can actually make judgements.
So for example, the user says,
this document is not very useful and
Now, this is called a relevance judgment
got some feedback information from
And this can be very useful to the system,
knowing what exactly is
So the feedback module would
also use the document collection
Typically it would involve
the system can now render the results
So this is called relevance feedback.
The feedback is based on relevance
Now, these judgements are reliable but
the users generally don't want to make
So the down side is that it involves
There's another form of feedback
blind feedback,
In this case, we can see once
in fact we don't have to invoke users.
So you can see there's
And we simply assume that the top
Let's say we have assumed
And then, we will then use this
and to improve the query.
Now, you might wonder,
how could this help if we simply
Well, you can imagine these top
similar to relevant documents
They look like relevant documents.
So it's possible to learn some related
In fact, you may recall that we
analyze what association, to learn
And there, what we did is we
all the documents that contain computer.
So imagine now the query
And then, the result will be those
And what we can do then is
They can match computer very well.
And we're going to count
And then, we're going to then use
the terms that are frequent in this set
So if we make a contrast between
is that related to terms
As we have seen before.
And these related words can then be added
And this would help us bring the documents
match other words like program and
So this is very effective for
But of course, pseudo-relevancy
We have to arbitrarily set a cut off.
So there's also something in
In this case,
we don't have to ask
Instead, we're going to observe how the
So in this case we'll look
So the user clicked on this one.
And the user viewed this one.
And the user skipped this one.
And the user viewed this one again.
Now, this also is a clue about whether
And we can even assume that we're
here in this document,
instead of the actual
The link they are saying web search
If the user tries to fetch this
we can assume these displayed
is interesting to you so
And this is called interesting feedback.
And we can, again,
This is a very important
Now, think about the Google and Bing and
they can collect a lot of user
So they would observe what documents
And this information is very valuable.
And they can use this to
So to summarize, we talked about
Relevant feedback where the user
It takes some user effort, but
We talk about the pseudo feedback where
will be relevant.
We don't have to involve the user
actually before we return
And the third is implicit feedback
Where we involve the users, but
the user doesn't have to make
Make judgement.
[MUSIC]
[MUSIC]
This lecture is about
In this lecture, we are going to
In particular we're going to talk
to combine different features
So the question that we address in
many features to generate a single ranking
In the previous lectures we have talked
We have talked about some retrieval
They can generate a based this course for
And we also talked about the link
that can give additional scores
Now the question now is,
potentially many other
And this will be very useful for
accuracy, but also to improve
So that it's not easy for
a few features to promote a page.
So the general idea of learning
learning to combine this
on different features to generate
So we will assume that the given
we can define a number of features.
And these features can vary from
a score of the document with
a retrieval function such as BM25 or
of punitive commands from a machine or
It can also be a link based score like or
It can be also application of retrieval
Those are the types of descriptions
So, these can all the clues whether
We can even include a feature
has a tilde because this might be
So all these features can then be combined
The question is, of course.
How can we combine them?
In this approach,
that this document isn't relevant to this
So we can hypothesize this
that the probability of relevance
through a particular form of
These parameters can control
the influence of different
Now this is of course just an assumption.
Whether this assumption really
that's they have to empirically
But by hypothesizing that
features in the particular way, we can
the potential more powerful ranking
Naturally the next question is how
How do we know which features
and which features will have lower weight?
So this is the task of training or
in this approach what we will
Those are the data that have
that we already know
We already know which documents should
And this information can be based
this can also be approximated by just
where we can assume the clicked documents
clicked documents are relevant and
So in general with the fit
function to the training data
meaning that we will try to optimize it's
And we can adjust these parameters to see
how we can optimize the performance of
in terms of some measures such as MAP or
So the training date would
Each tuple has three elements, the query,
So it looks very much like our
about in the evaluation
[MUSIC]
[SOUND]
this lecture we give an overview
First, let's define the term text mining,
The title of this course is
But the two terms text mining, and text
So we are not really going to
we're going to use them interchangeably.
But the reason that we have chosen to use
both terms in the title is because
if you look at the two phrases literally.
Mining emphasizes more on the process.
So it gives us a error rate
Analytics, on the other hand
or having a problem in mind.
We are going to look at text
But again as I said, we can treat
And I think in the literature
So we're not going to really
Both text mining and
want to turn text data into high quality
So in both cases, we
have the problem of dealing with
Turn these text data into something more
And here we distinguish
One is high-quality information,
Sometimes the boundary between
But I also want to say a little bit about
these two different angles of
In the case of high quality information,
concise information about the topic.
Which might be much easier for
For example, you might face
A more concise form of information
of the major opinions about
Positive about,
Now this kind of results are very useful
And so this is to minimize a human effort
The other kind of output
Here we emphasize the utility
knowledge we discover from text data.
It's actionable knowledge for some
For example, we might be able to determine
or a better choice for
Now, such an outcome could be
because a consumer can take the knowledge
So, in this case text mining supplies
But again, the two are not so
we don't necessarily have
Text mining is also
which is a essential component
Now, text retrieval refers to
a large amount of text data.
So I've taught another separate MOOC
Where we discussed various techniques for
If you have taken that MOOC,
And it will be useful To know
of understanding some of
But, if you have not taken that MOOC,
it's also fine because in this MOOC
going to repeat some of the key concepts
But they're at the high level and
they also explain the relation between
Text retrieval is very useful for
First, text retrieval can be
Meaning that it can help
a relatively small amount
Which is often what's needed for
And in this sense, text retrieval
Text retrieval is also needed for
And this roughly corresponds
mining as turning text data
Once we find the patterns in text data, or
actionable knowledge, we generally
By looking at the original text data.
So the users would have to have some text
text data to interpret the pattern or
to verify whether a pattern
So this is a high level introduction
and the relationship between
Next, let's talk about text
Now it's interesting to
generated by humans as subjective sensors.
So, this slide shows an analogy
And between humans as
physical sensors,
So in general a sensor would
It would sense some signal
then would report the signal as data,
For example, a thermometer would watch
then we report the temperature
Similarly, a geo sensor would sense
The location specification, for
example, in the form of longitude
A network sends over
or activities in the network and
Some digital format of data.
Similarly we can think of
That will observe the real world and
And then humans will express what they
So, in this sense, human is actually
sense what's happening in the world and
then express what's observed in the form
Now, looking at the text data in
able to integrate all
And that's indeed needed in
So here we are looking at
And in general we would Be
about our world that
And in general it will be dealing with
And of course the non-text data
And those non-text data can
Numerical data, categorical,
or multi-media data like video or speech.
So, these non text data are often
But text data is also very important,
mostly because they contain
And they often contain
especially preferences and
So, but by treating text data as
we can treat all this data
So the data mining problem is
turn all the data in your actionable
of it to change the real
So this means the data mining problem is
basically taking a lot of data as input
Inside of the data mining module,
we have a number of different
And this is because, for
we generally need different algorithms for
For example,
video data might require computer
And that would facilitate
And we also have a lot of general
to all kinds of data and those algorithms,
Although, for a particular kind of data,
we generally want to also
So this course will cover
are particularly useful for
[MUSIC]
[SOUND].
This lecture is about the syntagmatic
In this lecture, we're going to continue
In particular, we're going to talk about
And we're going to start with
which is the basis for designing some
By definition,
syntagmatic relations hold between words
That means,
we tend to see the occurrence
So, take a more specific example, here.
We can ask the question,
whenever eats occurs,
Looking at the sentences on the left,
together with eats, like cat,
But if I take them out and
only show eats and some other words,
Can you predict what other words
Right so
other words are associated with eats.
If they are associated with eats,
More specifically our
any text segment which can be a sentence,
And then ask I the question,
absent in this segment?
Right here we ask about the word W.
Is W present or absent in this segment?
Now what's interesting is that
some words are actually easier
If you take a look at the three
unicorn, which one do you
Now if you think about it for
the is easier to predict because
So I can just say,
Unicorn is also relatively easy
And I can bet that it doesn't
But meat is somewhere in
And it makes it harder to predict because
or the segment, more accurately.
But it may also not occur in the sentence,
now let's study this
So the problem can be formally defined
as predicting the value of
Here we denote it by X sub w,
this random variable is associated
When the value of the variable is 1,
When it's 0, it means the word is absent.
And naturally, the probabilities for
because a word is either present or
There's no other choice.
So the intuition with this concept earlier
The more random this random variable is,
Now the question is how does one
a random variable like X sub w?
How in general, can we quantify
that's why we need a measure
this measure introduced in information
There is also some connection
that is beyond the scope of this course.
So for
as a function defined
In this case, it is a binary random
be easily generalized for
Now the function form looks like this,
there's the sum of all the possible
Inside the sum for each value we
that the random variable equals this
And note that there is also
Now entropy in general is non-negative.
And that can be mathematically proved.
So if we expand this sum, we'll see that
Where I explicitly plugged
And sometimes when we have 0 log of 0,
we would generally define that as 0,
So this is the entropy function.
And this function will
different distributions
And it clearly depends on the probability
that the random variable
If we plot this function against
the probability that the random
And then the function looks like this.
At the two ends,
equals 1 is very small or very large,
When it's 0.5 in the middle
Now if we plot the function
is taking a value of 0 and the function
would show exactly the same curve here,
And so that's because
the two probabilities are symmetric,
So an interesting question you
what kind of X does entropy
And we can in particular think
For example, in one case,
always takes a value of 1.
The probability is 1.
Or there's a random variable that
is equally likely taking a value of one or
So in this case the probability
Now which one has a higher entropy?
It's easier to look at the problem
using coin tossing.
So when we think about random
it gives us a random variable,
It can be head or tail.
So we can define a random variable
when the coin shows up as head,
So now we can compute the entropy
And this entropy indicates how
of a coin toss.
So we can think about the two cases.
One is a fair coin, it's completely fair.
The coin shows up as head or
So the two probabilities would be a half.
Right?
Another extreme case is
where the coin always shows up as heads.
So it's a completely biased coin.
Now let's think about
And if you plug in these values you can
For a fair coin we see the entropy
For the completely biased coin,
And that intuitively makes a lot of sense.
Because a fair coin is
Whereas a completely biased
We can always say, well, it's a head.
Because it is a head all the time.
So they can be shown on
So the fair coin corresponds to the middle
The completely biased coin
point where we have a probability
So, now let's see how we can use
Let's think about our problem is
absent in this segment.
Again, think about the three words,
Now we can assume high entropy
And so we now have a quantitative way to
Now if you look at the three words meat,
we clearly would expect meat to have
In fact if you look at the entropy of the,
Because it occurs everywhere.
So it's like a completely biased coin.
Therefore the entropy is zero.
[MUSIC]
[SOUND]
this is indeed a general idea of
Algorithm.
So in all the EM algorithms we
to help us solve the problem more easily.
In our case the hidden variable
each occurrence of a word.
And this binary variable would
been generated from 0 sub d or 0 sub p.
And here we show some possible
For example, for the it's from background,
And text on the other hand.
Is from the topic then it's zero for
Now, of course, we don't observe these z
Values of z attaching to other words.
And that's why we call
Now, the idea that we
predicting the word distribution that
is it a predictor,
And, so, the EM algorithm then,
First, we'll initialize all
In our case,
of a word, given by theta sub d.
So this is an initial addition stage.
These initialized values would allow
of these z values, so
We can't say for sure whether
But we can have our guess.
This is given by this formula.
It's called an E-step.
And so the algorithm would then try to
After that, it would then invoke
In this step we simply take advantage
then just group words that are in
from that ground including this as well.
We can then normalize the count
to revise our estimate of the parameters.
So let me also illustrate
that are believed to have
that's text, mining algorithm,
And we group them together to help us
re-estimate the parameters
So these will help us
Note that before we just set
But with this guess, we will have
Of course, we don't know exactly
So we're not going to really
But rather we're going to
And this is what happened here.
So we're going to adjust the count by
this word has been generated
And you can see this,
Well, this has come from here, right?
From the E-step.
So the EM Algorithm would
estimate of parameters by using
The E-step is to augment the data
And the M-step is to take advantage
of the additional information
To split the data accounts and
re-estimate our parameter.
And then once we have a new generation of
We are going the E-step again.
To improve our estimate
And then that would lead to another
For the word distribution
Okay, so, as I said,
is really the variable z, hidden variable,
this water is from the top water
So, this slide has a lot of content and
Pause the reader to digest it.
But this basically captures
Start with initial values that
And then we invoke E-step followed
setting of parameters.
And then we repeated this, so
that would gradually improve
As I will explain later
reaching a local maximum of
So lets take a look at the computation for
these formulas are the EM.
Formulas that you see before, and
here, like here, n,
Like here for example we have n plus one.
That means we have improved.
From here to here we have an improvement.
So in this setting we have assumed the two
the background model is null.
So what are the relevance
Well these are the word counts.
So assume we have just four words,
And this is our background model that
words like the.
And in the first iteration,
Well first we initialize all the values.
So here, this probability that we're
distribution of all the words.
And then the E-step would give us a guess
That will generate each word.
We can see we have different
Why?
Well, that's because these words have
So even though the two
And then our initial audition say uniform
in the background of the distribution,
So these words are believed to
These on the other hand are less likely.
Probably from background.
So once we have these z values,
we know in the M-step these probabilities
So four must be multiplied by this 0.33
in order to get the allocated
And this is done by this multiplication.
Note that if our guess says this
then we just get the full count
In general it's not going
So we're just going to get some percentage
Then we simply normalize these counts
to have a new generation
So you can see, compare this with
So compare this with this one and
Not only that, we also see some
words that are believed to have come from
Like this one, text.
And of course, this new generation of
adjust the inferred latent variable or
So we have a new generation of values,
because of the E-step based on
And these new inferred values
another generation of the estimate
And so on and so forth so this is what
these probabilities
As you can see in the last row
and the likelihood is increasing
And note that these log-likelihood is
between 0 and 1 when you take a logarithm,
Now what's also interesting is,
And these are the inverted word split.
And these are the probabilities
have come from one distribution, in this
And you might wonder whether
Because our main goal is to
So this is our primary goal.
We hope to have a more discriminative
But the last column is also bi-product.
This also can actually be very useful.
You can think about that.
We want to use, is to for
example is to estimate to what extent this
And this, when we add this up or
take the average we will kind of know to
versus content was that are not
[MUSIC]
[SOUND]
In this lecture,
In the previous lecture, we talked about
We explained that the state of the are
are still not good enough to process
in a robust manner.
As a result,
bag of words remains very popular in
In this lecture, we're going to talk
help users get access to the text data.
This is also important step to convert
That are actually needed
So the main question we'll address here,
can a text information system, help users
We're going to cover two complimentary
And then we're going to talk about
querying versus browsing.
So first push versus pull.
These are two different ways connect
at the right time.
The difference is which
which party takes the initiative.
In the pull mode,
the users take the initiative to
And in this case, a user typically would
For example,
then browse the results to
So this is usually appropriate for
satisfying a user's ad
An ad hoc information need is
For example, you want to buy a product so
you suddenly have a need to read
But after you have cracked information,
You generally no longer
it's a temporary information need.
In such a case, it's very hard for
it's more proper for
that's why search engines are very useful.
Today because many people have many
So as we're speaking Google is probably
And those are all, or mostly adequate.
Information needs.
So this is a pull mode.
In contrast in the push mode in
to push the information to the user or
So in this case this is usually
Now this would be appropriate if.
The user has a stable information.
For example you may have a research
that interest tends to stay for a while.
So, it's rather stable.
Your hobby is another example of.
A stable information need is such a case
can learn your interest, and
If the system hasn't seen any
the system could then take the initiative
So, for example, a news filter or
news recommended system could
identify interesting news to you and
This mode of information access may be
has good knowledge about the users need
So for example, when you search for
a search engine might infer you might be
Formation.
And they would recommend the information
example, of an advertisement
So this is about the two high level
Now let's look at the pull
In the pull mode, we can further
Querying versus browsing.
In querying,
Typical the keyword query, and
the search engine system would
And this works well when the user knows
So if you know exactly
you tend to know the right keywords.
And then query works very well,
But we also know that sometimes
When you don't know the right
you want to browse information
You use because browsing
So in this case, in the case of browsing,
into the relevant information
supported by the structures of documents.
So the system would maintain
then the user could follow
So this really works well when the user
or the user doesn't know what
Or simply because the user finds it
So even if a user knows what query to
to search for information.
It's still harder to enter the query.
In such a case, again,
The relationship between browsing and
imagine you're site seeing.
Imagine if you're touring a city.
Now if you know the exact
Taking a taxi there is
You can go directly to the site.
But if you don't know the exact address,
Or you can take a taxi to a nearby
It turns out that we do exactly
If you know exactly what you
use the right keywords in your query
That's usually the fastest way to do,
But what if you don't know
Well, you clearly probably won't so well.
You will not related pages.
And then, you need to also walk
meaning by following the links or
You can then finally get
If you want to learn about again.
You will likely do a lot of browsing so
just like you are looking around in
interesting attractions
[INAUDIBLE].
So this analogy also tells us that
query, but we don't really have
And this is because in order
we need a map to guide us,
Of Chicago,
through the city of Chicago, you need a
So how to construct such a topical
research question that might bring us
more interesting browsing experience
So, to summarize this lecture,
we've talked about the two high level
Push tends to be supported by
Pull tends to be supported
Of course, in the sophisticated
we should combine the two.
In the pull mode, we can further this
Again we generally want to combine
so that you can support
If you want to know more about
push, you can read this article.
This give excellent discussion of the
information retrieval.
Here informational filtering is similar
the push mode of information access.
[MUSIC]
[SOUND]
is a continuing discussion of Generative
In this lecture, we are going to continue
particularly, the Generative
So this is a slide that you have seen
the likelihood function for
distributions, being a two component
Now in this lecture, we're going to
Now if you look at the formula and think
you'll realize that all we need is to add
So you can just add more thetas and
thetas and the probabilities of
So this is precisely what we
the general presentation of the mixture
So as more cases would follow these
think about our data.
And so in this case our data
end documents denoted by d sub i,
and then we talk about the other models,
In this case, we design a mixture
It's a little bit different from the topic
We have a set of theta i's that
corresponding to the k
We have p of each theta i as
each of the k distributions
Now note that although our goal
we actually have used a more general
cluster and this as you will see later,
will allow us to assign
that has the highest probability of
So as a result,
properties, as you will see later.
So the model basically would make
the generation of a document.
We first choose a theta i according
then generate all the words in
Note that it's important that we
use this distribution all
This is very different from topic model.
So the likelihood function would
So you can take a look
we have used the different notation
here in the second line of this equation.
You are going to see now
to use unique word in the vocabulary,
instead of particular
So from X subject to W,
this change allows us to show
And you have seen this change also
it's basically still just a product of
And so
with the likelihood function, now we can
Here we can simply use
So that's just a standard
So all should be familiar to you now.
It's just a different model.
So after we have estimated parameters,
how can we then allocate
Well, let's take a look at
So we just repeated the parameters
Now if you think about what we can
we can actually get more information than
So theta i for example,
this is actually a by-product, it can help
If you look at the top
in this word distribution and they will
p of theta i can be interpreted as
tells us how likely the cluster would
The more likely a cluster is
we can assume the larger
Note that unlike in PLSA and
this probability of theta
Now you may recall that the topic
actually depends on d.
That means each document can have
but here we have a generic choice
But of course, even a particular document
topic is more likely to
So in that sense,
we can still have a document
So now let's look at the key problem
assigning clusters to documents.
So that's the computer c sub d here and
the range of one through k to indicate
Now first you might think about
that is to assign d to the cluster
that most likely has
So that means we're going to choose
gives d the highest probability.
In other words,
we see which distribution has the content
Intuitively that makes sense,
does not consider the size of clusters,
so a better way is to use
in this case the prior is p of theta i.
And together, that is, we're going to
the posterior probability of theta,
And if we choose theta .based
we would have the following formula that
And in this case, we're going to choose
that means a large cluster and
So we're going to favor
also consistent with the document.
And that intuitively makes
a document being a large cluster is
So this means once we can estimate
then we can easily solve
So next, we'll have to discuss how to
actually compute
[MUSIC]
[SOUND] This lecture is
Discriminative Classifiers for
So, in this lecture,
another Discriminative Classifier called
Which is a very popular
it has been also shown to be effective for
So to introduce this classifier,
let's also think about the simple
We have two topic categories,
And we want to classify documents
we're going to represent again
Now, the idea of this classifier is
here that you'll see and
it's very similar to what you have
And we're going to do also say
value is positive then we're going to
Otherwise, we're going to
So that makes 0 that is the decision
So, in generally hiding
corresponds to a hyper plain.
Now I've shown you a simple case of two
X2 and this case this corresponds
So, this is a line defined by
just three parameters here,
Now, this line is heading
it shows that as we increase X1,
So we know that beta one and beta two have
the other is positive.
So let's just assume that beta one is
Now, it's interesting to examine, then,
the data instances on
So, here, the data instance are visualized
diamonds for the other class.
Now, one question is to take a point
what's the value of this expression, or
So what do you think?
Basically, we're going to evaluate
And as we said, if this value's positive
one, and if it's negative,
Intuitively, this line separates these two
one side would be positive and the points
Our question is under the assumption
let's examine a particular
So what do you think is
Well, to examine the sine we can
And we can compare this with let's say,
value on the line, let's see,
While they have identical X1, but
Now, let's look at the sin
Well, we know this is a positive.
So, what that means is
this point should be higher
this point on the line that means
So we know in general of
the function's value will be positive and
you can also verify all the points
And so this is how this kind
linear separator can then separate
So, now the natural question is,
Now, I've get you one line here
And this line, of course, is determined
Different coefficients will
So, we could imagine there are other
Gamma, for example,
could give us another line that counts
Of course, there are also lines that won't
But, the question is,
separate both clauses,
In fact, you can imagine, there are many
So, the logistical regression classifier
some criteria to determine where this line
And uses a conditional likelihood
which line is the best.
But in SVM we're going to
determining which line is the best.
And this time,
the criteria is more tied to
So, the basic idea is to choose
So what is a margin?
So, I choose some dotted
the boundaries of those
And the margin is simply
the separator, and
So you can see the margin of this
you can also define
In order for
it has to be kind of in the middle
you don't want this separator to
that in intuition makes a lot of sense.
So this is basic idea of SVM.
We're going to choose a linear
Now on this slide,
that I'm not going to use beta
But instead, I'm going to use w although
don't be confused here.
W here is actually a width,
So I'm also using lowercase b to
And there are instances do
I also use the vector form
So we see a transpose of w vector
So b is a bias constant and w is a set of
We have m features and
that will represent as a vector.
And similarly, the data instance here,
is represented by also a feature
Xi is a feature value.
For example, word count and
Multiply these two vectors together,
we get the same form of the linear
It's just a different way
Now I use this way so that it's
people usually use when
This way you can better connect the slides
Okay, so when we maximize
it just means the boundary of
a few data points, and these are the data
So here illustrated are two support
the other class.
And these quotas define
you can imagine once we know which
center separator line will
So the other data points actually
And you can see if you change the other
the margin, so
Mainly affected by
Sorry, it's mainly affected
that's why it's called
Okay, so now the next question is,
how can we set it up to optimize the line?
How can we actually find the line or
Now this is equivalent to
b, because they will determine
So in the simplest case, the linear SVM
So again, let's recall that our classifier
have weights for all the features, and the
And the classifier will say X is in
Otherwise, it's going to say
So this is our assumption, our setup.
So in the linear SVM,
values to optimize the margins and
The training data would be basically
We have a set of training points
then we also know the corresponding label,
And here we define y i as two values, but
these values are not 0, 1 as you
positive 1, and they're corresponding to
Now you might wonder why we
1 instead of having -1, 1.
And this is purely for mathematical
So the goal of optimization first is
to make sure the labeling of
So that just means if y i,
is 1, we would like this
And here we just choose
But if you use another threshold,
into the parameter values b and
Now if, on the other hand, y i is -1,
then we want this classifier
in fact a negative value, and we want this
Now these are the two different instances,
How can we combine them together?
Now this is where it's convenient
the other category,
because it turns out that we can either
y i multiplied by the classifier value
And obviously when y i is just 1,
you see this is the same as
But when y i is -1, you also see that this
So this one actually captures both
and that's a convenient way of
What's our second goal?
Well, that's to maximize margin, so
we want to ensure that separator
But then, among all the cases
we also would like to choose the separator
Now the margin can be assumed to be
And so
us basically the sum of
So to have a small value for
it means all the w i's must be small.
So we've just assumed that
getting the data on the training
Now we also have the objective that's
and this is simply to minimize
and we often denote this by phi of w.
So now you can see this is
We have some variables to optimize,
b and we have some constraints.
These are linear constraints and
the objective function is
So this a quadratic program
and there are standard algorithm that
And once we solve the problem
And then this would give us
So we can then use this classifier
Now the previous formulation did not
but sometimes the data may not
That means that they may not
the previous slide where a line
And what would happen if
Well, the principle can stay.
We want to minimize the training error but
But in this case we have a soft margin,
because the data points may
So it turns out that we can easily
So what you see here is very similar
but we have introduced
And we in fact will have one for
this is going to model the error
But the optimization problem
So specifically,
you will see we have added something
First we have added some
that now we allow a Allow the classifier
to make some mistakes here.
So, this Xi i is allowed an error.
If we set Xi i to 0, then we go
We want every instance to
But, if we allow this to be non-zero,
In fact, if the length of the Xi i is very
So naturally,
So we want to then also
So, because Xi i needs to be minimized
And so, as a result,
we also add more to the original one,
by basically ensuring that we not
also minimize the errors, as you see here.
Here we simply take a sum
Each one has a Xi i to model
And when we combine them together,
we basically want to minimize
Now you see there's a parameter C here,
the trade-off between minimizing
If C is set to zero, you can see,
we go back to the original object function
We don't really optimize
then Xi i can be set to a very large value
That's not very good of course, so
C should be set to a non-zero value,
But when C is set to a very,
we'll see the object of the function will
and so the optimization of margin
So if that happens, what would happen is
then we will try to do our best to
then we're not going to
that affects the generalization factors
So it's also not good.
So in particular, this parameter C
And this is just like in the case of
to optimize a number of neighbors.
Here you need to optimize the C.
And this is, in general,
Basically, you look at
see what value C should be set to in
Now with this modification,
the problem is still quadratic programming
algorithm can be actually applied to solve
Again, once we have obtained
then we can have classifier that's
So that's the basic idea of SVM.
So to summarize the text
where we introduce the many methods,
Some are discriminative methods.
And these tend to perform
So there's still no clear winner,
And the performance might also
different problems.
And one reason is also because the feature
and these methods all require
And to design an effective feature set,
we need domain knowledge and humans
although there are new
algorithm representation learning
And another common thing
be performing similarly on the data set,
but with different mistakes.
And so,
then the mistakes they
So that means it's useful to
a particular problem and
because this can improve the robustness
So assemble approaches that
methods tend to be more robust and
Most techniques that we introduce
which is a very general method.
So that means that these methods can
categorization problem.
As long as we have humans to help
design features, then supervising machine
can be easily applied to those problems
allow us to characterize content
Or to predict the sum
variables that are associated
The computers, of course, here are trying
the features provided by human.
And as I said, there are many
they also optimize different object or
But in order to achieve good performance,
also plenty of training data.
So as a general rule, and if you can
and then provide more training data,
Performance is often much more
features than by the choice
So feature design tends to be more
classifier.
So, how do we design effective features?
Well, unfortunately,
So there's no really much
But we can do some analysis of
try to understand what kind of features
And in general, we can use a lot of domain
And another way to figure out
to do error analysis on
You could, for example,
look at which category tends to be
And you can use a confusion matrix
across categories.
And then,
see why the mistake has been made and
And this can allow you to obtain
So error analysis is very
that's where you can get the insights
And finally, we can leverage this
So, for example, feature selection is
about, but is very important.
And it has to do with trying to select the
train a full classifier.
Sometimes training a classifier will also
values.
There are also other ways
Of the model,
For example, the SVM actually tries
But you can further force some features,
force to use only a small
There are also techniques for
And that's to reduce a high dimensional
space typically by clustering
So metrics factorization
such a job, and this is some of the
the talking models that we'll discuss.
So talking morals like psa or
lda can actually help us reduce
Like imagine the words
But the can be matched to the topic
So a document can now be represented
as a vector of just k values
So we can let each topic define one
space instead of the original high
And this is often another way
Especially, we could also use the
such low dimensional structures.
And so, the original worth features
amazing dimension features or
to provide a multi resolution
Deep learning is a new technique that
It's particularly useful for
So deep learning refers to deep neural
where you can have intermediate
That it's highly non-linear transpire, and
some recent events that's allowed us to
And the technique has been shown to be
computer reasoning, and
It has shown some promise.
And one important advantage
relationship with the featured design,
learn intermediate replantations or
And this is very valuable for
for text recalibration.
Although in text domain, because words are
because these are human's imaging for
And they are generally sufficient for
If there's a need for
people would have invented a new word.
So because of this we think
text processing tends to be lower than for
And the speech revenue where
where the design that worked as features.
But people only still very promising for
complicated tasks.
Like a analysis it has
because it can provide that
Now regarding the training examples.
It's generally hard to get a lot of
human labor.
But there are also some
So one is to assume in some low quality
So, those can be called
For example, if you take reviews from the
So, to train a of categorizer,
And categorize these reviews
Then we could assume five star reviews
One star are negative.
But of course,
sometimes even five star reviews will also
sample is not all of that high quality,
Another idea is to exploit
there are techniques called
learning techniques that can allow you to
So, in other case it's easy to see
both text plus read and
So you can imagine, if you have a lot of
then you can actually do clustering
And then try to somehow
With the categories defined
where we already know which
So you can in fact use the Algorithm
That would allow you essentially also
You can think of this in another way.
Basically, we can use let's say a to
classify all of the unlabeled text
assume the high confidence Classification
Then you suddenly have more training
now know some are labeled as category one,
All though the label is not
then they can still be useful.
So let's assume they are actually training
with true training examples through
And so this idea is very powerful.
When the enabled data and
we might need to use other advanced
called domain adaptation or
This is when we can
Borrow some training examples from
Or, from a categorization password
that follow very different distribution
But basically,
then we need to be careful and
But yet, we can still want to use some
So for example,
give you Effective plus y for
But you can still learn something from
So there are mission learning techniques
Here's a suggested reading where you
more of the methods is
[MUSIC]
[MUSIC]
In this lecture, we continue
In particular, we're going to
In the previous lecture,
we have derived a TF idea of weighting
And we have assumed that this model
these examples as shown on this slide,
d5, which has received a very high score.
Indeed, it has received the highest
But this document is intuitive and
In this lecture,
how we're going to use TF
Before we discuss the details,
this simple TF-IDF
And see why this document has
So this is the formula, and
then you will see it involves a sum
And inside the sum, each matched
And this weight is TF-IDF weighting.
So it has an idea of component,
One is the total number of documents
The other is the document of frequency.
This is the number of
This word w.
The other variables
involved in the formula include
W in the query, and
If you look at this document again,
the reason why it hasn't
it has a very high count of campaign.
So the count of campaign in this document
the other documents, and has contributed
So in treating the amount
this document, we need to somehow
of the matching of this
And if you think about the matching
you actually would realize,
we probably shouldn't reward
And by that I mean,
says a lot about
because it goes from zero
And that increase means a lot.
Once we see a word in the document,
it's very likely that the document
If we see a extra occurrence on
that is to go from one to two,
occurrence kind of confirmed that it's
Now we are more sure that this
But imagine we have seen, let's say,
Now, adding one extra occurrence is not
because we're already sure that
So if you're thinking this way, it seems
of a high count of a term, and
So this transformation function is
word into a term frequency weight for
So here I show in x axis that we'll count,
y axis I show the term frequency weight.
So in the previous breaking functions,
we actually have imprison rate
So for example,
we actually use such a transformation
Basically if the count is 0,
otherwise it would have a weight of 1.
It's flat.
Now, what about using
Well, that's a linear function, so it has
Now we have just seen that
So what we want is something like this.
So for example,
we can't have a sublinear
And this will control the influence
because it's going to lower its inference.
Yet, it will retain
Or we might want to even bend the curve
Now people have tried all these methods.
And they are indeed working better than
But so far, what works the best seems
called a BM25 transformation.
BM stands for best matching.
Now in this transformation,
And this k controls the upper
It's easy to see this
because if you look at the x divided by
then the numerator will never be able
So it's upper bounded by k+1.
Now, this is also difference between
a logarithm transformation.
Which it doesn't have upper bound.
Furthermore, one interesting property
we can actually simulate different
Including the two extremes
That is, the 0/1 bit transformation and
So for example, if we set k to 0,
the function value will be 1.
So we precisely recover
If you set k to very large
it's going to look more like
So in this sense,
It allows us to control
It also has a nice property
And this upper bound is useful to control
And so that we can prevent a spammer
of one term to spam all queries
In other words, this upper bound
terms would be counted when we aggregate
As I said, this transformation
So to summarize this lecture,
Sublinear TF Transformation,
capture the intuition of diminishing
It's also to avoid the dominance by
This BM25 transformation that we
It's so far one of the best-performing
It has upper bound, and so
Now if we're plugging this function into
Then we'd end up having
which has a BM25 TF component.
Now, this is already
the odd ranking function called BM25.
And we'll discuss how we can further
[MUSIC]
[SOUND]
lecture is about the basic measures for
In this lecture,
measures to quantitatively
This is a slide that you have seen
about the Granville
We can have a test faction that consists
We can then run two systems on these
Their performance.
And we raise the question,
Is system A better or is system B better?
So let's now talk about how to
Suppose we have a total of 10 relevant
this query.
Now, the relevant judgments show on
And we have only seen 3 [INAUDIBLE] there,
But, we can imagine there are other Random
So now, intuitively,
A is better because it
And in particular we have seen
two of them are relevant but in system B,
we have five results and
So intuitively it looks like
And this infusion can be captured
where we simply compute to what extent
If you have 100% position,
that would mean that all
So in this case system A has
three System B has some
this shows that system
But we also talked about System B
would like to retrieve as many
So in that case we'll have to compare
retrieve and
This method uses the completeness
In your retrieval result.
So we just assume that there are ten
And here we've got two of them,
So the recall is 2 out of 10.
Whereas System B has called a 3,
Now we can see by recall
And these two measures turn out to
evaluating search engine.
And they are very important because
other test evaluation problems.
For example, if you look at
you tend to see precision recall numbers
Okay so, now let's define these
And these measures are to evaluate a set
we are considering that approximation
We can distinguish 4 cases depending
A document can be retrieved or
Because we are talking
A document can be also relevant or
not relevant depending on whether the user
So we can now have counts of documents in.
Each of the four categories again
documents that have been retrieved and
B for documents that are not retrieved but
No with this table then
As the ratio of the relevant
retrieved documents A to the total
So, this is just A divided
The sum of this column.
Singularly recall is defined by
So that's again to divide a by.
The sum of the row instead of the column.
All right, so we can see precision and
that's the number of
But we're going to use
Okay, so what would be an ideal result.
Well, you can easily see being
recall oil to be 1.0.
That means We have got 1% of
in our results, and all of the results
At least there's no single
In reality, however, high recall tends
And you can imagine why that's the case.
As you go down the to try to get as
you tend to encounter a lot of documents,
Note that this set can also
In the rest of this, that's why although
retrieve the documents, they are actually
They are the fundamental measures in
We often are interested in The precision
This means we look at how many documents
among the top ten results
Now, this is a very meaningful measure,
because it tells us how many relevant
On the first page of where they
So precision and recall
use them to further evaluate a search
We just said that there tends to be
so naturally it would be
And here's one method that's often used,
it's a [INAUDIBLE] mean of precision and
So, you can see at first, compute the.
Inverse of R and P here,
the 2 by using coefficients
And after some transformation you can
And in any case it just becomes
recall, and beta is a parameter,
It can control the emphasis
set beta to 1 We end up having a special
This is a popular measure that's often
And the formula looks very simple.
It's just this, here.
Now it's easy to see that if
larger recall than f
But, what's interesting is that
recall is captured
So, in order to understand that, we
can first look at the natural
using the symbol arithmetically
That would be likely the most natural way
If you want to think more,
So why is this not as good as F1?
Or what's the problem with this?
Now, if you think about
you can see this is
In this case,
In the case of a sum, the total value
that means if you have a very high P or
don't care about whether the other value
Now this is not desirable because one
We have perfect recall easily.
Can we imagine how?
It's probably very easy to
all the documents in the collection and
And this will give us 0.5 as the average.
But such results are clearly not
though the average using this
In contrast you can see F 1 would
recall are roughly That seminar, so
it would a case where you had
So this means f one encodes
Now this example shows
Methodology here.
But when you try to solve a problem you
let's say in this it's
But it's important not to
It's important to think whether you
And once you think about the multiple
difference, and then think about
In this case, if you think more carefully,
you will think that F1
Than the simple.
Although in other cases there
But in this case the seems not reasonable.
But if you don't pay attention
you might just take a easy way to
And here later, you will find that,
All right.
So this methodology is actually very
Try to think about the best solution.
Try to understand the problem very well,
know why you needed this measure, and why
And then use that to guide you in
To summarize, we talked about
are there retrievable
We also talk about the Recall.
Which addresses the question, have all of
These two, are the two,
They are used for
We talk about F measure as a way to
We also talked about the tradeoff
And this turns out to depend
we'll discuss this point
[MUSIC]
[SOUND] This lecture is about
In this lecture,
we're going to give an introduction
This has to do with how do you model
So it's related to how we model
We're going to talk about
And then we're going to talk about the
language model, which also happens to be
And finally, what this class
What is a language model?
Well, it's just a probability
So here, I'll show one.
This model gives the sequence Today
It give Today Wednesday is a very,
very small probability
You can see the probabilities
sequences of words can vary
Therefore, it's clearly context dependent.
In ordinary conversation,
probably Today is Wednesday is most
Imagine in the context of
maybe the eigenvalue is positive,
This means it can be used to
The model can also be regarded
generating text.
And this is why it's also often
So what does that mean?
We can imagine this is a mechanism that's
visualised here as a stochastic system
So, we can ask for a sequence,
a sequence from the device if you want,
Today is Wednesday, but it could
So for example,
So in this sense,
a sample observed from
So, why is such a model useful?
Well, it's mainly because it can quantify
Where do uncertainties come from?
Well, one source is simply
that we discussed earlier in the lecture.
Another source is because we don't
we lack all the knowledge
In that case,
So let me show some examples of questions
that would have interesting
Given that we see John and feels,
as opposed to habit as the next
Now, obviously, this would be very useful
habit would have similar acoustic sound,
But, if we look at the language model,
we know that John feels happy would be
Another example, given that we
game once in a news article,
This obviously is related to text
Also, given that a user is
how likely would the user
Now, this is clearly related
that we discussed in the previous lecture.
So now,
called a unigram language model.
In such a case,
we assume that we generate a text by
So this means the probability of
the product of
Now normally,
So if you have single word in like
likely to observe model than if
So this assumption is not
we make this assumption
So now the model has precisely N
We have one probability for each word, and
So strictly speaking,
As I said,
drawn from this word distribution.
So for example,
the model to stochastically generate
So instead of giving a whole sequence,
like Today is Wednesday,
And we can get all kinds of words.
And we can assemble these
So that will still allow you
Today is Wednesday as the product
As you can see, even though we have not
it actually allows us to compute
this model now only needs N
That means if we specify
all the words, then the model's
Whereas if we don't make this assumption,
all kinds of combinations
So by making this assumption, it makes it
So let's see a specific example here.
Here I show two unigram language
And these are high probability
The first one clearly suggests
because the high probability
The second one is more related to health.
Now we can ask the question,
how likely were observe a particular
Now suppose we sample
Let's say we take the first distribution,
What words do you think would be
maybe mining maybe another word?
Even food,
which has a very small probability,
But in general, high probability
So we can imagine what general text
In fact, with small probability,
you might be able to actually generate
Now, it will actually be meaningful,
very small.
In an extreme case, you might
a text mining paper that would be
And in that case,
But it's a non-zero probability,
if we assume none of the words
Similarly from the second topic,
we can imagine we can generate
That doesn't mean we cannot generate this
We can, but the probability would be very,
generating a paper that can be accepted
So the point is that
we can talk about the probability of
Some texts will have higher
Now let's look at the problem
Suppose we now have available
In this case, many of the abstract or
we see these word counts here.
The total number of words is 100.
Now the question you ask here
We can ask the question which model,
which one of these distribution has
assuming that the text has been generated
So what would be your guess?
What we have to decide are what
would have.
Suppose the view for a second, and
If you're like a lot of people,
my best guess is text has a probability
seen text 10 times, and
So we simply normalize these counts.
And that's in fact the word justified, and
your intuition is consistent
And this is called the maximum
In this estimator,
of those that would give our observe
That means if we change these
observing the particular text
So you can see,
Basically, we just need to look at
and then divide it by the total number of
Normalize the frequency.
A consequence of this is,
of course, we're going to assign
If we have an observed word,
there will be no incentive to assign a
Why?
Because that would take away probability
And that obviously wouldn't maximize
the probability of this
But one has still question whether
Well, the answer depends on what kind
This estimator gives a best model
But if you are interested in a model
paper for this abstract, then you
So for thing,
of that article, so
even though they're not
So we're going to cover this
in this class in the query
So let's take a look at some possible
One use is simply to use
So here I show some general
We can use this text to
and the model might look like this.
Right, so on the top, we have those
etc., and then we'll see some
then some very,
This is a background language model.
It represents the frequency of
This is the background model.
Now let's look at another text,
we'll look at the computer
So we have a collection of
we do as mentioned again, we can just
where we simply normalize the frequencies.
Now in this case, we'll get
On the top, it looks similar because
they are very common.
But as we go down,
computer science,
And so although here, we might also see
we can imagine the probability here is
And we will see many other words here that
So you can see this distribution
the corresponding text.
We can look at even the smaller text.
So in this case,
Now if we do the same,
again the can be expected
The sooner we see text, mining,
these words have relatively
In contrast, in this distribution, the
So this means, again,
we can have a different model,
So we call this document
we call this collection language model.
And later, you will see how they're
But now,
Can we statistically find what words
Now how do we find such words?
Well, our first thought is that let's take
So we can take a look at all the documents
Let's build a language model.
We can see what words we see there.
Well, not surprisingly, we see these
So in this case, this language model gives
the word in the context of computer.
And these common words will
But we also see the computer itself and
software will have relatively
But if we just use this model,
we cannot just say all these words
So ultimately, what we'd like to
How can we do that?
It turns out that it's possible
But I suggest you think about that.
So how can we know what
so that we want to kind
What model will tell us that?
Well, maybe you can think about that.
So the background language model
It tells us what was
So if we use this background model,
we would know that these words
So it's not surprising to observe
Whereas computer has a very
it's very surprising that we have seen
the same is true for software.
So then we can use these two
the words that are related to computer.
For example, we can simply take the ratio
normalize the topic of language model
the background language model.
So if we do that, we take the ratio,
computer is ranked, and
program, all these words
Because they occur very frequently in the
the whole collection, whereas these common
In fact,
because they are not really
By taking the sample of text
we don't really see more occurrences
So this shows that even with
we can do some limited
So in this lecture,
which is basically a probability
We talked about the simplest language
which is also just a word distribution.
We talked about the two
One is we represent the topic in a
The other is we discover
In the next lecture, we're going to talk
design a retrieval function.
Here are two additional readings.
The first is a textbook on statistical
The second is an article that
language models with a lot of
[MUSIC]
[SOUND]
lecture is about the feedback
In this lecture, we continue talking
Particularly, we're going to talk about
As we have discussed before,
of text retrieval system is removed from
We will have positive examples.
Those are the documents that
be charged with being relevant.
All the documents that
We also have negative examples.
Those are documents known
They can also be the documents
The general method in
feedback is to modify our query vector.
We want to place the query vector in
And what does that mean exactly?
Well, if we think about the query vector
something to the vector elements.
And in general,
Or we might just weight of old terms or
As a result, in general,
We often call this query expansion.
The most effective method in
is called the Rocchio Feedback, which was
So the idea is quite simple.
We illustrate this idea by
of all the documents in the collection and
So now we can see the query
and these are all the documents.
So when we use the query back there and
the most similar documents,
that these documents would be
And these process are relevant documents,
these are relevant documents,
And then these minuses are negative
So our goal here is trying to move
to improve the retrieval accuracy.
By looking at this diagram,
Where should we move the query vector so
that we can improve
Intuitively, where do you
If you want to think more,
If you think about this picture, you can
case you want the query vector to be as
That means ideally, you want to place
Or we want to move the query
Now so what exactly is this point?
Well, if you want these relevant
you want this to be in the center of
Because then if you draw
you'll get all these relevant documents.
So that means we can move the query
all the relevant document vectors.
And this is basically the idea of Rocchio.
Of course, you can consider
we want to move away from
Now your match that we're talking about
away from other vectors.
It just means that we have this formula.
Here you can see this is
this average basically is the centroid
When we take the average of these vectors,
then were computing
Similarly, this is the average of
So it's essentially of
And we have these three parameters here,
They are controlling
When we add these two vectors together,
we're moving the query vector
This is when we add them together.
When we subtracted this part,
we kind of move the query
So this is the main idea
And after we have done this,
we will get a new query vector which
This new query vector,
original query vector toward this
away from the non-relevant value.
Okay, so let's take a look at the example.
This is the example that
Only that I deemed that display
I only showed the vector
We have five documents here and we have
to read in the documents here, right.
And they're displayed in red.
And these are the term vectors.
Now I have just assumed some of weights.
A lot of terms,
Now these are negative arguments.
There are two here.
There is another one here.
Now in this Rocchio method, we first
And so let's see,
the positive documents, we simply just,
We just add this with this one
And then that's down here and
And then we're going to add
then just take the average.
And so we do this for all this.
In the end, what we have is this one.
This is the average vector of these two,
Let's also look at the centroid
This is basically the same.
We're going to take the average
And these are the corresponding
on and so forth.
So in the end, we have this one.
Now in the Rocchio feedback
these with the original
So now let's see how we
Well, that's basically this.
So we have a parameter alpha
query times weight that's one.
And now we have beta to control
centroid of the weight, that's 1.5.
That comes from here.
All right, so this goes here.
And we also have this negative
And this way, it has come from,
And we do exactly the same for
And this is our new vector.
And we're going to use this new query
You can imagine what would happen, right?
Because of the movement that this one
better because we moved
And it's going to penalize these black
So this is precisely what
Now of course if we apply this method in
and that is the original query has
But after we do query explaining and
merging, we'll have many times
So the calculation will
In practice,
only retain the terms
So let's talk about how we
I just mentioned that they're
Consider only a small number of
the centroid vector.
This is for efficiency concern.
I also said here that negative examples,
tend not to be very useful, especially
Now you can think about why.
One reason is because negative documents
directions.
So, when you take the average,
it doesn't really tell you where
Whereas positive documents
And they will point you to
So that also means that sometimes we don't
But note that in some cases, in difficult
negative feedback after is very useful.
Another thing is to avoid over-fitting.
That means we have to keep relatively
Why?
Because the sample that we see in
We don't want to overly
And the original query terms
Those terms are heightened by the user and
the user has decided that those
So in order to prevent
drifting, prevent topic drifting due to
We generally would have to keep a pretty
it was safe to do that.
And this is especially true for
Now, this method can be used for
both relevance feedback and
In the case of pseudo-feedback, the prime
value because the relevant examples
They're not as reliable as
In the case of relevance feedback,
So those parameters,
And the Rocchio Method is
It's still a very popular method for
[MUSIC]
[MUSIC]
So now let's take a look at the specific
Now, this is one of the many
it's one of the simplest methods.
And I choose this to explain
So in this approach, we simply assume
respect to a query is related to a linear
Here I used Xi to denote the feature.
So Xi of Q and D is a feature.
And we can have as many
And we assume that these features
And each feature is controlled
and this beta i is a parameter.
That's a weighting parameter.
A larger value would mean the feature
and it would contribute more
This specific form of the function
a transformation of
So this is the probability of relevance.
And we know that the probability of
And we could have just assumed that
this linear combination.
So we can do a linear regression.
But then, the value of this linear
So this transformation
to 1 range to the whole
you can verify it by yourself.
So this allows us then to connect
which is between 0 and 1 to a linear
And if we rewrite this into a probability
So on this equation, now we'll
And on the right hand side,
Now, this form is clearly nonnegative, and
it still involves a linear
And it's also clear that if this value is,
this is actually negative of the linear
If this value here is large,
then it would mean this value is small.
And therefore,
And that's we expect, that basically,
gives us a high value, then
So this is our hypothesis.
Again, this is not necessarily the best
way to connect these features with
So now we have this combination function.
The next task is to
that the function cache will be applied.
But without knowing the beta values,
So let's see how can
All right,
In this example, we have three features.
One is the BM25 score of the document and
One is the PageRank score of the document,
might not depend on the query.
We might have a topic-sensitive PageRank,
Otherwise, the general PageRank
And then we have BM25 score on
Now, these are then the feature values for
And in this case, the document is D1 and
Here's another training instance and
but in this case, it's not relevant.
This is an oversimplified case where
it's sufficient to illustrate the point.
So what we can do is we use
actually estimate the parameters.
Basically, we're going to
of the document based
That is, given that we observed
Can we predict the relevance here?
Now, of course, the prediction would be
And we hypothesize that the probability
features in this way.
So we are going to see, for what values of
What do we mean by predicting
Well, we just mean, in the first case, for
D1 this expression right here
In fact, we'll hope this
Why?
On the other hand,
we hope this value will be small, right.
Why?
Because it's a non-relevant document.
So now let's see how this can
And this is similar to expressing
only that we are not talking about
talking about the probability
So what's the probability
relevant if it has these feature values?
Well, this is just this expression.
We just need to plug in the Xi's.
So that's what we will get.
It's exactly like what we have seen above,
only that we replaced these
So for example, this 0.7 goes to here and
this 0.11 goes to here.
And these are different feature values,
and we combine them in
The beta values are still unknown.
But this gives us the probability
if we assume such a model.
Okay?
And we want to maximize this probability,
What do we do for the second document?
Well, we want to compute the probability
So this would mean we have to
since this expression is actually
So to compute the non-relevance
we just do 1 minus
Okay?
So this whole expression then
predicting these two relevance values.
One is 1 here, one is 0.
And this whole equation
observing a 1 here and observing a 0 here.
Of course, this probability
So then our goal is to adjust
thing reach its maximum,
So that means we're going to compute this.
The beta is just the parameter
maximize this whole likelihood expression.
And what it means is,
we're going to choose betas to
make this also as large as possible,
make this part as small as possible.
And this is precisely what we want.
So once we do the training,
So then this function
Once beta values are known, both this and
So for any new query and new document,
we can simply compute the features for
And then we just use this formula
And this scoring function can be used to
So that's the basic idea
[MUSIC]
[SOUND]
looking at the text mining problem more
similar to general data mining, except
And we're going to have text mining
into actionable knowledge that
especially for decision making, or
for completing whatever tasks that
Because, in general,
we also tend to have other kinds
So a more general picture would be
And for this reason we might be
non-text data.
And so in this course we're
but we're also going to also touch how do
non-text data.
With this problem definition we
the topics in text mining and analytics.
Now this slide shows the process of
More specifically, a human sensor or
human observer would look at
Different people would be looking at
they'll pay attention to different things.
The same person at different times might
of the observed world.
And so the humans are able to perceive
And that human, the sensor,
And that can be called the Observed World.
Of course, this would be different from
that the person has taken
Now the Observed World can be
entity-relation graphs or
using knowledge representation language.
But in general, this is basically what
And we don't really know what
But then the human would
observed using a natural language,
And the result is text data.
Of course a person could have used
she has observed.
In that case we might have text data of
The main goal of text mining
process of generating text data.
We hope to be able to uncover
Specifically, we can think about mining,
And that means by looking at text data
something about English, some usage
So this is one type of mining problems,
some knowledge about language which
If you look at the picture,
we can also then mine knowledge
And so this has much to do with
We're going to look at what the text
get the essence of it or
about a particular aspect of
For example, everything that has been
a particular entity.
And this can be regarded as mining content
to describe the observed world in
If you look further,
we can mine knowledge about this observer,
So this has also to do with
some properties of this person.
And these properties could
sentiment of the person.
And note that we distinguish
because text data can't describe what the
But the description can be also
in general, you can imagine the text
descriptions of the world plus
So that's why it's also possible to
do text mining to mine
Finally, if you look at the picture
then you can see we can certainly also
Right?
So indeed we can do text mining to
And this is often called
And we want to predict the value
So, this picture basically covered
multiple types of knowledge that
When we infer other
could also use some of the results from
mining text data as intermediate
For example,
after we mine the content of text data we
And that summary could be then used
to help us predict the variables
Now of course this is still generated
but I want to emphasize here that
to generate some features that can help
And that's why here we show the results of
some other mining tasks, including
mining knowledge about the observer,
In fact, when we have non-text data,
data to help prediction, and
In general, non-text data can be very
For example,
changes of stock prices based on
in social media, then this is an example
of using text data to predict
But in this case, obviously,
the historical stock price data would
And so that's an example of
useful for the prediction.
And we're going to combine both kinds
Now non-text data can be also used for
When we look at the text data alone,
we'll be mostly looking at the content
But text data generally also
For example, the time and the location
And these are useful context information.
And the context can provide interesting
For example, we might partition text
because of the availability of the time.
Now we can analyze text data in each
Similarly we can partition text
any meta data that's associated to
So, in this sense,
interesting angles or
And it can help us make context-sensitive
analysis of content or
the opinions about the observer or
We could analyze the sentiment
So this is a fairly general landscape of
In this course we're going to
We actually hope to cover
First we're going to cover
briefly because this has to do
this determines how we can represent
Second, we're going to talk about how to
And word associations is a form of use for
Third, we're going to talk about
And this is only one way to
it's a very useful ways
It's also one of the most useful
Then we're going to talk about
So this can be regarded as one example
And finally we're going to
problems where we try to predict some
So this slide also serves as
And we're going to use
the topics that we'll cover
[MUSIC]
[SOUND] This lecture is
relation discovery and
In this lecture,
we're going to continue the discussion
We're going to talk about the conditional
discovering syntagmatic relations.
Earlier, we talked about
how easy it is to predict the presence or
Now, we'll address
we assume that we know something
So now the question is, suppose we know
How would that help us
absence of water, like in meat?
And in particular, we want to
has helped us predict
And if we frame this using entrophy,
that would mean we are interested
the presence of eats could reduce
Or, reduce the entrophy
corresponding to the presence or
We can also ask as a question,
Would that also help us predict
These questions can be
concept called a conditioning entropy.
So to explain this concept, let's first
when we know nothing about the segment.
So we have these probabilities indicating
or it doesn't occur in the segment.
And we have an entropy function that
Now suppose we know eats is present, so
now we know the value of another
Now, that would change all
conditional probabilities.
Where we look at the presence or
given that we know eats
So as a result,
if we replace these probabilities
probabilities in the entropy function,
So this equation now here would be
the conditional entropy.
Conditional on the presence of eats.
So, you can see this is essentially
seen before, except that all
And this then tells us
after we have known eats
And of course, we can also define
the scenario where we don't see eats.
So if we know it did not occur in
entropy would capture the instances
So now,
we have the completed definition
Basically, we're going to consider both
and this gives us a probability
Basically, whether eats is present or
And this of course,
is the conditional entropy of
So if you expanded this entropy,
then you have the following equation.
Where you see the involvement of
Now in general, for any discrete
the conditional entropy is no larger
So basically, this is upper bound for
That means by knowing more
we want to be able to
We can only reduce uncertainty.
And that intuitively makes sense
it should always help
And cannot hurt
Now, what's interesting here is also to
value of this conditional entropy?
Now, we know that the maximum
But what about the minimum,
I hope you can reach the conclusion that
And it will be interesting to think about
So, let's see how we can use conditional
Now of course,
one way to measure
Because it tells us to what extent,
word given that we know the presence or
Now before we look at the intuition
syntagmatic relations, it's useful to
That is, the conditional entropy
So here,
we listed this conditional
So, it's here.
So, what is the value of this?
Now, this means we know where
And we hope to predict whether
And of course, this is 0 because
Once we know whether the word
we'll already know the answer
So this is zero.
And that's also when this conditional
So now, let's look at some other cases.
So this is a case of knowing the and
And this is a case of knowing eats and
Which one do you think is smaller?
No doubt smaller entropy means easier for
Which one do you think is higher?
Which one is not smaller?
Well, if you at the uncertainty,
the doesn't really tell
So knowing the occurrence of the doesn't
So it stays fairly close to
Whereas in the case of eats,
So knowing presence of eats or
would help us predict whether meat occurs.
So it can help us reduce entropy of meat.
So we should expect the sigma term, namely
And that means there is a stronger
So we now also know when
meat, then the conditional entropy
And for what kind of words
Well, that's when this stuff
And like the for example,
which is the entropy of meat itself.
So this suggests that when you
mining syntagmatic relations,
For each word W1, we're going to
And then, we can compute
We thought all the candidate was in
because we're out of favor,
Meaning that it helps us predict
And then, we're going to take the top ring
potential syntagmatic relations with W1.
Note that we need to use
The stresser can be the number
absolute value for
Now, this would allow us to mine the most
strongly correlated words with
But, this algorithm does not
that K syntagmatical relations
Because in order to do that, we have to
are comparable across different words.
In this case of discovering
a targeted word like W1, we only need
for W1, given different words.
And in this case, they are comparable.
All right.
So, the conditional entropy of W1, given
given W3 are comparable.
They all measure how hard
But, if we think about the two pairs,
where we share W2 in the same condition,
Then, the conditional entropies
You can think of about this question.
Why?
So why are they not comfortable?
Well, that was because they
Right?
the entropy of W1 and the entropy of W3.
And they have different upper bounds.
So we cannot really
So how do we address this problem?
Well later, we'll discuss, we can use
[MUSIC]
So, I just showed you that empirically
but theoretically it can also
converge to a local maximum.
So here's just an illustration of what
This required more knowledge about that,
some of that inequalities,
So here what you see is on the X
This is a parameter that we have.
On the y axis we see
So this curve is the original
and this is the one that
And we hope to find a c0 value
But in the case of Mitsumoto we can
to the problem.
So, we have to resolve
the EM algorithm is such an algorithm.
It's a Hill-Climb algorithm.
That would mean you start
Let's say you start from here,
And then you try to improve
another point where you can
So that's the ideal hill climbing.
And in the EM algorithm, the way we
First, we'll fix a lower
So this is the lower bound.
See here.
And once we fit the lower bound,
And of course, the reason why this works,
is because the lower bound
So we know our current guess is here.
And by maximizing the lower bound,
To here.
Right?
And we can then map to the original
Because it's a lower bound, we are
Because we improve our lower bound and
curve which is above this lower bound
So we already know it's
So we definitely improve this
which is above this lower bound.
So, in our example,
the current guess is parameter value
And then the next guess is
From this illustration you
is always better than the current guess.
Unless it has reached the maximum,
So the two would be equal.
So, the E-step is basically
to compute this lower bound.
We don't directly just compute
we compute the length of
these are basically a part
This helps determine the lower bound.
The M-step on the other hand is
It allows us to move
And that's why EM algorithm is guaranteed
Now, as you can imagine,
we also have to repeat the EM
In order to figure out which one
And this actually in general is a
So here for
then we gradually just
So, that's not optimal, and
so the only way to climb up to this gear
So, in the EM algorithm, we generally
or have some other way to determine
To summarize in this lecture we
This is a general algorithm for computing
kinds of models, so
And it's a hill-climbing algorithm, so it
it will depend on initial points.
The general idea is that we will have
In the E-step we roughly [INAUDIBLE]
of useful hidden variables that we
In our case, this is the distribution
In the M-step then we would exploit
it easier to estimate the distribution,
Here improve is guaranteed in
Note that it's not necessary that we
parameter value even though the likelihood
There are some properties that have to
also to convert into some stable value.
Now here data augmentation
That means,
we're not going to just say exactly
But we're going to have a probability
these hidden variables.
So this causes a split of counts
And in our case we'll split the word
[MUSIC]
[MUSIC]
This lecture is about
This picture shows our overall plan for
In the last lecture, we talked about
We talked about push versus pull.
Such engines are the main tools for
Starting from this lecture,
we're going to talk about the how
So first it's about
We're going to talk about
First, we define Text Retrieval.
Second we're going to make a comparison
the related task Database Retrieval.
Finally, we're going to talk about
Document Ranking as two strategies for
So what is Text Retrieval?
It should be a task that's familiar for
the most of us because we're using
So text retrieval is basically a task
where the system would respond to
Basically, it's for supporting a query
as one way to implement the poll
So the situation is the following.
You have a collection of
These documents could be all
all the literature articles
Or maybe all the text
A user will typically give a query to
And then, the system would return
Relevant documents refer to those
the user who typed in the query.
All this task is a phone call
But literally information retrieval would
non-textual information as well,
It's worth noting that
of information retrieval in
video can be retrieved by
So for example,
match a user's query was
This problem is also
And the technology is often called
If you ever take a course in databases it
will be useful to pause
think about the differences between
Now these two tasks
But, there are some important differences.
So, spend a moment to think about
Think about the data, and the information
those that are managed
Think about the different between
database system versus queries that
And then finally think about the answers.
What's the difference between the two?
Okay, so if we think about the information
we will see that in text retrieval.
The data is unstructured, it's free text.
But in databases, they are structured data
to tell you this column is the names
The unstructured text is not obvious
what are the names of people
Because of this difference, we also see
ambiguous and we talk about that in the
But they don't tend to have
The results important
this is partly due to the difference
So test queries tend to be ambiguous.
Whereas in their research,
Think about a SQL query that would clearly
So it has very well-defined semantics.
Keyword queries or electronic queries tend
to be incomplete,
specify what documents
Whereas complete specification for
And because of these differences,
Being the case of text retrieval, we're
In the database search,
match records with the sequel
Now in the case of text retrieval,
to the query is not very well specified,
So it's unclear what should be
And this has very important consequences,
textual retrieval is
So this is a problem because
then we can not mathematically prove one
That also means we must rely
involving users to know
And that's why we have.
You need more than one lectures
Because this is very important topic for
Without knowing how to evaluate heroism
whether we have got the better or
So now let's look at
So, this slide shows a formal formulation
First, we have our vocabulary set, which
Now here,
in reality, on the web,
We have texts that are in
But here for simplicity, we just
As the techniques used for retrieving
less similar to the techniques used for
although there is important difference,
Next, we have the query,
And so here, you can see
the query is defined as
Each q sub i is a word in the vocabulary.
A document is defined in the same way,
And here,
Now typically, the documents
But there are also cases where
So you can think about what
I hope you can think of Twitter search.
Tweets are very short.
But in general,
Now, then we have
and this collection can be very large.
So think about the web.
It could be very large.
And then the goal of text retrieval
the documents, which we denote by R'(q),
And this in general, a subset of all
Unfortunately, this set of relevant
and user-dependent in the sense that,
in by different users, they expect
The query given to us by
on which document should be in this set.
And indeed, the user is generally
be in this set, especially in the case
large, the user doesn't have complete
So the best search system
an approximation of this
So we denote it by R'(q).
So formerly,
R'(q) approximation of
So how can we do that?
Now imagine if you are now asked
What would you do?
Now think for a moment.
Right, so these are your input.
The query, the documents.
And then you are to compute
which is a set of documents that
So, how would you solve the problem?
Now in general,
The first strategy is we do a document
going to have a binary classification
That's a function that
query as input, and then give a zero or
one as output to indicate whether this
So in this case, you can see the document.
The relevant document is set,
It basically, all the documents that
So in this case,
you can see the system must have decide
Basically, it has to say
And this is called absolute relevance.
Basically, it needs to know
useful to the user.
Alternatively, there's another
Now in this case,
the system is not going to make a call
But rather the system is going to
That would simply give us a value
that would indicate which
So it's not going to make a call whether
But rather it would say which
So this function then can be
then we're going to let
when the user looks at the document.
So we have a threshold theta
documents should be in
And we're going to assume
are ranked above the threshold
these are the documents that
And theta is a cutoff
So here we've got some collaboration
because we don't really make a cutoff.
And the user kind of helped
So in this case,
if one document is more
And that is, it only needs to
as opposed to absolute relevance.
Now you can probably already sense that
relative relevance would be easier to
Because in the first case,
we have to say exactly whether
And it turns out that ranking is indeed
So let's look at these two
So this picture shows how it works.
So on the left side,
we use the pluses to indicate
So we can see the true relevant
of true relevant documents, consists
And with the document selection function,
we're going to basically
relevant documents, and non-relevant ones.
Of course, the classified will not
So here we can see, in the approximation
we have got some number in the documents.
And similarly,
there is a relevant document that's
In the case of document ranking,
simply ranks all the documents in
And then, we're going to let the user
If the user wants to
then the user will scroll down some
But if the user only wants to
the user might stop at the top position.
So in this case, the user stops at d4.
So in fact, we have delivered
So as I said ranking is generally
because the classifier in the case of
Why?
Because the only clue
But the query may not be accurate in the
For example, you might expect relevant
topics by using specific vocabulary.
And as a result,
Because in the collection,
no others have discussed the topic
So in this case,
no relevant documents to return in
On the other hand,
for example, if the query
does not have sufficient descriptive
You may actually end up having of
thought these words my be sufficient
But, it turns out they
there are many distractions,
And so, this is a case of over delivery.
Unfortunately, it's very hard to find the
Why?
Because whether users looking for
not have a good knowledge about
And in that case, the user does not
vocabularies will be used in
So it's very hard for
a user to pre-specify the right
Even if the classifier is accurate,
relevant documents, because they
Relevance is often a matter of degree.
So we must prioritize these documents for
And note that this
because a user cannot
the user generally would have to
And therefore, it would make sense to
And that's what ranking is doing.
So for these reasons,
Now this preference also has
this is given by the probability
In the end of this lecture,
This principle says, returning a ranked
of probability that a document
is the optimal strategy under
First, the utility of
Is independent of the utility
Second, a user would be assumed to
Now it's easy to understand why these
Site for the ranking strategy.
Because if the documents are independent,
then we can evaluate the utility
And this would allow the computer
And then, we are going to rank these
The second assumption is to say that the
If the user is not going to follow
the documents sequentially, then obviously
So under these two assumptions, we can
is, in fact, the best that you could do.
Now, I've put one question here.
Do these two assumptions hold?
I suggest you to pause the lecture,
Now, can you think of
suggest these assumptions
Now, if you think for a moment,
you may realize none of
For example, in the case of
have documents that have similar or
If we look at each of them alone,
But if the user has already seen
generally not very useful for the user to
So clearly the utility
is dependent on other documents
In some other cases you might see
be useful to the user, but when three
They provide answers to
So this is a collective relevance and
the value of the document might
Sequential browsing generally would make
But even if you have a rank list,
users don't always just go strictly
They sometimes will look at the bottom for
And if you think about the more
we could possibly use like
Where you can put that additional
sequential browsing is a very
So the point here is that
none of these assumptions is
But probability ranking principle
ranking as a primary pattern for
And this has actually been the basis for
a lot of research work in
And many hours have been designed
despite that the assumptions
And we can address this problem
Of a ranked list, for example,
So to summarize this lecture,
the main points that you can
First, text retrieval is
And that means which algorithm is
Second, document ranking
And this will help users prioritize
And this is also to bypass the difficulty
Because we can get some help from users
it's more flexible.
So, this further suggests that the main
engine is the design
In other words, we need to define
on the query and document pair.
How we design such a function is the main
There are two suggested
The first is the classical paper on
The second one is a must-read for anyone
It's a classic IR book, which has
and results in early days up to
Chapter six of this book has
the Probability Ranking Principle and
[MUSIC]
[SOUND]
This lecture is about text categorization.
In this lecture, we're going to
This is a very important technique for
It is relevant to discovery
knowledge as shown here.
First, it's related to topic mining and
And, that's because it has to do with
analyzing text to data based
Secondly, it's also related to
which has to do with discovery knowledge
Because we can categorize the authors,
based on the content of the articles
We can, in general,
based on the content that they produce.
Finally, it's also related
Because, we can often use text
variables in the real world that
And so, this is a very important
This is the overall plan for
First, we're going to talk about
why we're interested in
And now, we're going to talk about
how to evaluate
So, the problem of text
We're given a set of predefined categories
And often,
training set of labeled text
objects have already been
And then, the task is to classify
more of these predefined categories.
So, the picture on this
When we do text categorization,
we have a lot of text objects to be
the system will, in general,
As shown on the right and
and we often assume the availability
these are the documents that
And these examples are very important for
helping the system to learn
And, this would further help
the categories of new text
So, here are some specific
And in fact, there are many examples,
So first, text objects can vary,
or a passage, or a sentence,
As in the case of clustering, the units
this creates a lot of possibilities.
Secondly, categories can also vary.
Allocate in general,
One is internal categories.
These are categories that
For example, topic categories or
they generally have to do with
throughout the categorization
The other kind is external categories
associated with the text object.
For example,
authors are entities associated
And so, we can use their content in
which part, for example, and
Or, we can have any
associate with text data
connection between the entity and
For example, we might collect a lot
a lot of reviews about a product,
this text data can help us infer
In that case, we can treat this
We can categorize restaurants or
categorize products based on
So, this is an example for
Here are some specific
News categorization is very
News agencies would like
categories to categorize
And, these virtual article
For example, in the biomedical domain,
MeSH stands for Medical Subject Heading,
characterize content of
Another example of application is spam
So, we often have a spam filter
to help us distinguish spams
this is clearly a binary
Sentiment categorization of
another kind of applications where we
negative or positive and
So, you can have send them to categories,
Another application is automatic
you might want to automatically sort your
one application of text categorization
The results are another important kind
to the right person to handle,
email messaging is generally routed
Different people tend to handle
And in many cases, a person would manually
But, if you can imagine,
text categorization system
And, this is a class file, the incoming
where each category actually corresponds
And finally, author attribution, as I just
it's another example of using text
some other entities.
And, there are also many variants
And so, first, we have the simplest case,
where there are only two categories.
And, there are many examples like that,
Applications with one distinguishing
documents for a particular query.
Spam filtering just distinguishing spams
Sometimes, classifications of
positive and a negative.
A more general case would be K-category
many applications like that,
So, topic categorization is often
multiple topics.
Email routing would be another example
if you route the email to
then there are multiple
So, in all these cases, there are more
Another variation is to have
where categories form a hierarchy.
Again, topical hierarchy is very common.
Yet another variation is
That's when you have multiple
then you hope to kind of
Further leverage the dependency of
each individual task.
Among all these binary categorizations
part of it also is because it's simple and
it can actually be used to perform
For example, a K-category
performed by using binary categorization.
Basically, we can look at
then the binary categorization problem
not, meaning in other categories.
And, the hierarchical categorization
doing flat categorization at each level.
So, we have, first, we categorize
a small number of high-level categories,
and inside each category, we have further
So, why is text categorization important?
Well, I already showed that you,
there are several reasons.
One is text categorization helps enrich
more understanding of text data that's
So, now with categorization text can
The keyword conditions that's often
But we can now also add categories and
Semantic categories assigned can also
application.
So, for example, semantic categories
other attribution might
Another example is when semantic
of text content and this is another case
For example, if we want to know
could first categorize the opinions
as positive or negative and then, that
the sentiment, and it would tell us about
the 70% of the views are positive and
So, without doing categorization,
it will be much harder to aggregate
way of coding text in some sense
And, sometimes you may see in some
called a text coded,
The second kind of reasons is to use text
categorization to infer
and text categories allows
of such entities that
So, this means we can
to discover knowledge about the world.
In general, as long as we can associate
we can always the text of data to help
So, it's used for
single information network that will
The obvious entities that can be
But, you can also imagine the author's
other things can be actually
Once we have made the connection, then we
So, this is a general way to allow
the text categorization to discover
Very useful, especially in big text
just using text data as extra sets
to infer certain decision factors
Specifically with text, for example,
we can also think of examples of
For example, discovery of
And, this can be done by categorizing
Another example is to predict the party
on the political speech.
And, this is again an example
some knowledge about the real world.
In nature,
that's as we defined and
[MUSIC]
[SOUND] This lecture is
of evaluation of text categorization.
Earlier we have introduced measures that
recall.
For each category and each document
further examine how to combine the
different documents how to aggregate them,
You see on the title here I indicated
this is in contrast to micro average
So, again, for each category we're going
for example category c1 we have
And similarly we can do that for category
Now once we compute that and
example we can aggregate
For all the categories, for
And this is often very useful to summarize
And aggregation can be
Again as I said, in a case when you
it's always good to think about what's
For example, we can consider arithmetic
you can use geometric mean,
Depending on the way you aggregate,
in terms of which method works better,
differences and choosing the right one or
So the difference fore example
geometrically is that the arithmetically
values whereas geometrically would
Base and so whether you are want
high values would be a question
similar we can do that for
So that's how we can generate the overall
Now we can do the same for aggregation
So it's exactly the same situation for
Precision, recall, and F.
And then after we have completed
we're going to aggregate them to generate
overall F score.
These are, again, examining
Which one's more useful will
In general, it's beneficial to look at
And especially if you compare different
it might reveal which method
in what situations and
Understanding the strands of a method or
this provides further insight for
So as I mentioned,
in contrast to the macro average
In this case, what we do is you
and then compute the precision and recall.
So we can compute the overall
how many cases are in true positive,
etc, it's computing the values
and then we can compute the precision and
In contrast, in macro-averaging, we're
And then aggregate over these categories
then aggregate all the documents but
Now this would be very similar to
used earlier, and
one problem here of course to treat all
And this may not be desirable.
But it may be a property for
especially if we associate the, for
Then we can actually compute for example,
Where you associate the different cost or
so there could be variations of these
But in general macro average tends to
just because it might reflect the need for
on each category or performance on each
But macro averaging and micro averaging,
and you might see both reported in
Also sometimes categorization
be evaluated from ranking prospective.
And this is because categorization
often indeed passed it to a human for
For example, it might be passed
For example, news articles can be tempted
then human editors would
And all the email messages might be
handling in the help desk.
And in such a case the categorizations
the task for
So, in this case the results
and if the system can't give a score
confidence then we can use the scores
then evaluate the results as a rank list,
Evaluation where you rank
So for example a discovery of
based on ranking emails for
And this is useful if you want people
spam, right?
The person would then take
then verify whether this is indeed a spam.
So to reflect the utility for
better to evaluate Ranking Chris and this
And in such a case often
better formulated as a ranking problem
So for example, ranking documents in
as a binary categorization problem,
are useful to users from those that
frame this as a ranking problem,
That's because people tend
ranking evaluation more reflects
So to summarize categorization evaluation,
first evaluation is always very
So get it right.
If you don't get it right,
And you might be misled to believe
which is in fact not true.
So it's very important to get it right.
Measures must also reflect
a particular application.
For example, in spam filtering and
news categorization the results
So then we would need to
design measures appropriately.
We generally need to consider how will the
and think from a user's perspective.
What quality is important?
What aspect of quality is important?
Sometimes there are trade offs between
recall and so we need to know for this
or high precision is more important.
Ideally we associate the different cost
And this of course has to be designed
Some commonly used measures for relative
Classification accuracy, it's very
[INAUDIBLE] preceding [INAUDIBLE]
report characterizing performances,
[INAUDIBLE] like a [INAUDIBLE] Per
take a average of all of them, different
In general, you want to look at the
particular applications some perspectives
diagnoses and
It's generally useful to look at
to see subtle differences between methods
from which you can obtain sight for
Finally sometimes ranking
be careful sometimes categorization has
and there're machine running methods for
So here are two suggested readings.
One is some chapters of this book where
evaluation measures.
The second is a paper about
text categorization and
it also has an excellent discussion of
[MUSIC]
[SOUND]
This lecture is about
Document Length Normalization
In this lecture, we will continue
In particular, we're going to discuss the
So far in the lectures about the vector
signals from the document to assess
In particular,
The count of a tone in a document.
We have also considered it's
IDF, Inverse Document Frequency.
But we have not considered
So here I show two example documents,
D6 on the other hand, has a 5000 words.
If you look at the matching
we see that in d6, there are more
But one might reason that,
these query words in a scattered manner.
So maybe the topic of d6, is not
So, the discussion of the campaign
may have nothing to do with the managing
In general,
they would have a higher chance for
In fact, if you generate a long document
a distribution of words, then eventually
So in this sense, we should penalize on
better chance matching to any query, and
We also need to be careful in avoiding
On the one hand,
But on the other hand,
Now, the reasoning is because
different reasons.
In one case, the document may be
So for example, think about the vortex
It would use more words than
So, this is a case where we probably
long documents such as a full paper.
When we compare the matching
document with matching of
Then long papers in general,
have a higher chance of matching clearer
However, there is another case
that is when the document
Now consider another
where we simply concatenate a lot
In such a case, obviously, we don't want
Indeed, we probably don't want to penalize
So that's why, we need to be careful about
A method of that has been working well,
is called a pivoted length normalization.
And in this case,
the idea is to use the average document
That means we'll assume that for
the score is about right so
But if the document is longer
then there will be some penalization.
Whereas if it's a shorter,
So this is illustrated at
x-axis you can see the length of document.
On the y-axis, we show the normalizer.
In this case, the Pivoted Length
is seeing to be interpolation of 1 and
the normalize the document in length
So you can see here,
of the document by the average documents,
sense about how this document is
also gives us a benefit of not
We can measure the length by words or
Anyway, this normalizer
First we see that, if we set the parameter
So, there's no lens normalization at all.
So, b, in this sense,
Whereas, if we set b to a nonzero value,
All right, so
documents that are longer than
Whereas, the value of
would be smaller for shorter documents.
So in this sense,
long documents, and
The degree of penalization
because if we set b to a larger value,
There's even more penalization for
the short documents.
By adjusting b, which varies from 0 to 1,
we can control the degree
So, if we plug in this length
the vector space model, ranking functions
Then we will end up having
And these are in fact the state of
Let's take a look at each of them.
The first one is called a pivoted length
and a reference in [INAUDIBLE]
And here we see that, it's basically
the idea of component should
There is also a query term
And then, in the middle, there is
we see we use the double logarithm
this is to achieve
But we also put a document
Right, so this would cause
because the larger the denominator is,
And this is of course controlled
And you can see again, if b is set to 0
Okay, so this is one of the two most
The next one called a BM25 or Okapi,
is also similar in that it
and query IDF component here.
But in the middle,
As we explained,
and that does sublinear
In this case we have put the length
We're adjusting k but
because we put a normalizer
Therefore, again, if a document is longer
So you can see after we have gone through
and we have in the end reached
So, So far, we have talked about
mainly how to place the document
And, this has played an important role
the simple function.
But there are also other dimensions,
For example, can we further
the dimension of the Vector Space Model?
Now, we've just assumed that the bag
dimension as a word but obviously,
For example, a stemmed word, those
into the same root form, so
that computation and computing were all
We get those stop word removal.
This is to remove some very common words
We get use of phrases
We can even use later in
some clusters of words that represent the
We can also use smaller unit,
are sequences of and
However, in practice, people have found
phrases is still the most effective
So, this is still so far the most
And it's used in all major search engines.
I should also mention, that sometimes
domain specific tokenization.
And this is actually very important, as we
prevent us from matching them with each
In some languages like Chinese,
text to obtain word band rates because
A word might correspond to one
even three characters.
So, it's easier in English when we
In some other languages, we may need
figure a way out of what
There is also the possibility to
And so
one can imagine there are other measures.
For example, we can measure the cosine
Or we can use Euclidean distance measure.
And these are all possible, but
dot product seems still the best and
In fact that it's sufficiently general,
if you consider the possibilities
So, for example,
cosine measure can be thought of as the
That means, we first normalize each factor
That would be critical
I just mentioned that the BM25, seems to
But there has been also further
Although, none of these words have
So in one line work,
Here, F stands for field, and this is
So for example, you might consider
or body of the research article.
Or even anchor text on the web page,
links to other pages and
a proper way of different fields to help
When we use BM25 for such a document and
the obvious choice is to apply BM25 for
Basically, the idea of BM25F is
counts of terms in all the fields,
Now, this has advantage of avoiding over
Remember in the sublinear
the first occurrence is very important and
And if we do that for all the fields,
then the same term might have gained
But when we combine these
we just do the transformation one time.
At that time,
then the extra occurrences will not be
And this method has been working very well
The other line of extension
In this line,
over penalization of
So to address this problem,
We can simply add a small constant
But what's interesting is that,
doing such a small modification,
the problem of over penalization of
So the new formula called BM25+,
is empirically and
So to summarize all what we have
here are the major take away points.
First, in such a model,
Assuming that relevance of a document
basically proportional to the similarity
So naturally,
document must have been
And in this case, we will present them as
Where the dimensions are defined by words,
And we generally, need to use a lot of
We use some examples, which show
including Tf weighting and transformation.
And IDF weighting, and
These major heuristics are the most
to ensure such a general ranking function
And finally, BM25 and
to be the most effective formulas
Now I have to say that, I put BM25 in
in fact, the BM25 has been derived
So the reason why I've put it in
the ranking function actually has a nice
We can easily see,
it looks very much like a vector space
The second reason is because the original
And that form of IDF after
well as the standard IDF
So as effective retrieval function,
BM25 should probably use a heuristic
To make them even more look
There are some additional readings.
The first is, a paper about
It's an excellent example
analysis to suggest the need for
then further derive the length
The second, is the original paper
The third paper,
its extensions, particularly BM25 F.
And finally, in the last paper
BM25 to correct the over
[MUSIC]
[MUSIC]
This lecture is about,
In this lecture, we will continue
In particular,
we are going to look at, how we can
In the previous lecture,
These are the two basic measures for,
quantitatively measuring
But, as we talked about, ranking, before,
we framed that the text of retrieval
So, we also need to evaluate the,
How can we use precision-recall
Well, naturally, we have to look after the
Because in the end, the approximation
given by a ranked list, is determined
Right?
the list of results, the user would,
that point would determine the set.
And then,
that we have to consider,
Without knowing where
then we have to consider, all
So, let's look at these positions.
Look at this slide, and
what if the user stops at the,
What's the precision-recall at this point?
What do you think?
Well, it's easy to see, that this document
We have, got one document,
What about the recall?
Well, note that, we're assuming that,
this query in the collection,
What if the user stops
Top two.
Well, the precision is the same,
And, the record is two out of ten.
What if the user stops
Well, this is interesting,
additional relevant document,
But the precision is lower,
what's exactly the precision?
Well, it's two out of three, right?
And, recall is the same, two out of ten.
So, when would see another point,
Now, if you look down the list,
we have, seeing another relevant document.
In this case D5, at that point, the,
three out of ten, and,
So, you can see, if we keep doing this,
And then, we will have
because there are eight documents,
And, the recall is a four out of ten.
Now, when can we get,
Well, in this list, we don't have it,
We don't know, where it is?
But, as convenience, we often assume that,
at all the, the othe,
all the other levels of recall,
So, of course,
the actual position would be higher,
in order to, have an easy way to,
compute another measure called Average
Now, I should also say, now, here you see,
we make these assumptions that
But, this is okay, for
And, this is for the relative comparison,
or actual, actual number deviates
As long as the deviation,
is not biased toward any particular
We can still,
And, this is important point,
When you compare different algorithms,
the key's to avoid any
And, as long as, you can avoid that.
It's okay, for you to do transformation
you can preserve the order.
Okay, so, we'll just talk about,
we can get a lot of precision-recall
So, now, you can imagine,
And, this just shows on the,
And, on the y-axis, we show the precision.
So, the precision line was marked as .1,
Right?
this is, the different, levels of recall.
And,, the y-axis also has,
So, we plot the, these, precision-recall
as points on this picture.
Now, we can further, and
As you'll see,
we assumed all the other, precision
And, that's why, they are down here,
And this, the actual curve probably will
discussed, it, it doesn't matter that
because this would be,
Okay, so, now that we,
how can we compare ranked to back list?
All right, so, that means,
And here, we show, two cases.
Where system A is showing red,
All right, so, which one is better?
I hope you can see,
Why?
see same level of recall here,
the precision point by system A is better,
So, there's no question.
In here, you can imagine, what does the
Well, it has to have perfect,
it has to be this line.
That would be the ideal system.
In general, the higher the curve is,
The problem is that,
This actually happens often.
Like, the two curves cross each other.
Now, in this case, which one is better?
What do you think?
Now, this is a real problem,
Suppose, you build a search engine,
that's shown here in blue, or system B.
And, you have come up with a new idea.
And, you test it.
And, the results are shown in red,
Now, your question is, is your new
Or more, practically,
you're already using, your, in your search
So, should we use system,
This is going to be a real decision,
If you make the replacement, the search
whereas, if you don't do that,
So, what do you do?
Now, if you want to spend more time
And, it's actually very
As I said, it's a real decision that you
search engine, or if you're working, for
Now, if you have thought about this for
a moment, you might realize that,
Now, some users might like a system A,
So, what's the difference here?
Well, the difference is just that,
in the, low level of recall,
There's a higher precision.
But in high recall region,
Now, so, that also means,
cares about the high recall, or
You can imagine, if someone is just going
want to find out something
Well, which one is better?
What do you think?
In this case, clearly, system B is better,
because the user is unlikely
The user doesn't care about high recall.
On the other hand,
where a user is doing you are,
You want to find, whether your idea ha,
In that case, you emphasize high recall.
So, you want to see,
Therefore, you might, favor, system A.
So, that means, which one is better?
That actually depends on users,
So, this means, you may not necessarily
that would accurately
You have to look at the overall picture.
Yet, as I said, when you have
whether you replace ours with another,
then you may have to actually come up with
Or, when we compare many different
one number to compare, them with, so, that
So, for all these reasons, it is desirable
So, how do we do that?
And, that,
So, here again it's
And, one way to summarize
this whole curve,
Right?
There are other ways to measure that,
this particular way of matching
has been used, since a long time ago for
basically, in this way, and
Basically, we're going to take a, a look
And then, look out for the precision.
So, we know, you know,
And, this is another,
Now, this, we don't count to this one,
because the recall level is the same,
this number, and that's precision at
So, we have all these, you know, added up.
These are the precisions
corresponding to retrieving the first
then, the third, that follows, et cetera.
Now, we missed the many relevant
we just, assume,
And then, finally, we take the average.
So, we divide it by ten, and
which is the total number of relevant
Note that here,
Which is a number retrieved
Now, imagine, if I divide by four,
Now, think about this, for a moment.
It's a common mistake that people,
Right, so, if we, we divide this by four,
In fact, that you are favoring a system,
documents, as in that case,
So, this would be, not a good matching.
So, note that this denomina,
the total number of relevant documents.
And, this will basically ,compute
And, this is the standard method,
Note that, it actually combines
But first, you know, we have
we also consider recall, because if missed
All right, so,
And furthermore, you can see this
of a position of a relevant document.
Let's say, if I move this relevant
it would increase this means,
Whereas, if I move any relevant document,
document down, then it would decrease,
So, this is a very good,
because it's a very sensitive to
It can tell, small differences
And, that is what we want,
sometimes one algorithm only works
And, we want to see this difference.
In contrast, if we look at
If we look at this, this whole set, well,
what, what's the precision,
Well, it's easy to see,
So, that precision is very meaningful,
So, that's pretty useful, right?
So, it's a meaningful measure,
But, if we use this measure to
because it wouldn't be sensitive to where
If I move them around the precision
Right.
this is not a good measure for
In contrast, the average precision
It can tell the difference of, different,
a difference in ranked list in,
[MUSIC]
[SOUND]
This lecture is about query likelihood,
probabilistic retrieval model.
In this lecture,
we continue the discussion of
In particular, we're going to talk about
In the query light holder retrieval model,
How like their user who likes a document
So in this case,
particular document about
Now we assume,
a basis to impose a query to try and
So again, imagine use a process
Where we assume that
assembling words from the document.
So for example, a user might
from this document and
And then the user would pick
that would be the second query word.
Now this of course is an assumption
how a user would pose a query.
Whether a user actually followed this
this assumption has allowed us to formerly
And this allows us to also not rely on
to use empirical data to
And this is why we can use this
retrieval function that we can
So as you see the assumption
word is independent of the sample.
And also each word is basically
So now let's see how this works exactly.
Well, since we are completing
then the probability here is just
which is a sequence of words.
And we make the assumption that each
So as a result, the probability
of the probability of each query word.
Now how do we compute
Well, based on the assumption that a word
is picked from the document
Now we know the probability of each word
word in the document.
So for example, the probability of
Would be just the count
divided by the total number of words
So with these assumptions we now have
We can use this to rank our documents.
So does this model work?
Let's take a look.
Here are some example documents
Suppose now the query is
we see the formula here on the top.
So how do we score this document?
Well, it's very simple.
We just count how many times do
how many times do we have seen campaigns,
And we see here 44, and
So that's 2 over the length of
the length of document 4 for
And similarly, we can get probabilities
Now if you look at these numbers or
scoring all these documents,
Because if we assume d3 and
then looks like a nominal rank d4
And as we would expect,
a TF query state, and so
However, if we try a different
presidential campaign update
Well what problem?
Well think about the update.
Now none of these documents
So according to our assumption that a user
generate a query, then the probability of
Would be 0.
So that causes a problem,
to have zero probability
Now why it's fine to have zero probability
It's not okay to have 0 for d3 and
d4 because now we no longer
What's worse?
We can't even distinguish them from d2.
So that's obviously not desirable.
Now when a [INAUDIBLE] has such result,
we should think about what
So we have to examine what
as we derive this ranking function.
Now is you examine those assumptions
what has caused this problem?
So take a moment to think about it.
What do you think is the reason why update
So if you think about this from the moment
have made an assumption
be drawn from the document
So in order to fix this, we have to
a word not necessarily from the document.
So that's the improved model.
An improvement here is to say that,
well instead of drawing
let's imagine that the user would actually
And so I show a model here.
And we assume that this document is
model.
Now, this model doesn't necessarily assign
we can assume this model does not
Now if we're thinking this way then
different.
Now the user has this model in mind
Although the model has to be
So the user can again generate
Namely, pick a word for example,
Now the difference is that this time
even though update doesn't
to potentially generate
So that a query was updated
So this would fix our problem.
And it's also reasonable because when our
in a more general way, that is unique
So how do we compute
If we make this sum wide
The first one is compute this model, and
For example, I've shown two pulse models
And then given a query like a data mining
just compute the likelihood of this query.
And by making independence
probability as a product of
We do this for both documents, and
then rank them.
So that's the basic idea of this
So more generally this ranking function
Here we assume that the query has n words,
w1 through wn, and
The ranking function is the probability
given that the user is
And this is assume it will be product of
This is based on independent assumption.
Now we actually often score
using log of the query likelihood
Now we do this to avoid
having a lot of small probabilities,
And this could cause under flow and we
the value in our algorithm function.
We maintain the order of these documents
And so if we take longer than
the product would become a sum
So the sum of all the query
that is one of the probability of
And then we can further rewrite
So in the first sum here, in this sum,
we have it over all the query words and
And in this sum we have a sum
But we put a counter here
Essentially we are only considering
because if a word is not in the query,
So we're still considering
But we're using a different form as
all the words in the vocabulary.
And of course, a word might occur
That's why we have a count here.
And then this part is log of
given by the document language model.
So you can see in this retrieval function,
we actually know the count
So the only thing that we don't know
Therefore, we have converted
include the problem of estimating
So that we can compute the probability of
And different estimation methods would
This is just like a different way to
which leads to a different ranking
Here different ways to
a different ranking function for
[MUSIC]
[SOUND]
lecture is about the feedback in
In this lecture, we will continue the
In particular,
we're going to talk about the feedback
So we derive the query likelihood ranking
As a basic retrieval function,
But if we think about the feedback
use query likelihood to perform feedback,
a lot of times the feedback information is
But we assume the query has
from a language model in
It's kind of unnatural to sample
As a result, researchers proposed a way
and it's called Kullback-Leibler
And this model is actually going
retrieval function much
Yet this form of the language model
query likelihood, in the sense that it can
And in this case,
then feedback can be achieved through
This is very similar to Rocchio,
So let's see what is this
So on the top, what you see is a query
And then KL-divergence, or
retrieval model is basically to generalize
the frequency part here
So basically it's the difference given
by the probabilistic model here to
versus the count of query words there.
And this difference allows us to plug in
So this can be estimated
including using feedback information.
But this is called a KL-divergence,
this can be interpreted as matching
One is the query model,
One is the document
smooth them with a collection
And we are not going to talk
you'll find it in some references.
It's also called cross entropy because,
we ignore some terms in
we will end up having
And both are terms of information theory.
But anyway, for our purposes here,
you can just see the two
except that here we have a probability of
And here the sum is over all the words
also with the nonzero probability for
So it's kind of, again, a generalization
Now you can also easily see we can recover
by simply setting this query model to the
This is very easy to
into here you can eliminate this
And then you will get exactly like that.
So you can see the equivalence.
And that's also why this KL-divergence
of query likelihood, because we can cover
But it would also allow us
So this is how we can use the
The picture shows that we first
then we estimate a query language model,
This is often denoted by a D here.
But this basically means this is
because we compute a vector for the
the query, and
Only that these vectors are of special
And then we get the results and
Let's assume they are mostly
although we could also consider
So what we could do is, like in Rocchio,
model called the feedback
Again, this is going to be another vector
in Rocchio.
And then this model can be combined
a linear interpolation, and
just like, again, in Rocchio.
So here we can see the parameter alpha
If it's set to zero,
If it's set to one, we get full feedback
And this is generally not desirable,
So unless you are absolutely sure you
then the query terms are not important.
So of course, the main question here is,
This is the big question here, and
So here we will talk about
there are many approaches, of course.
This approach is based
I'm going to show you how it works.
This will use a generative mixture model.
So this picture shows that
the feedback model that
And the basis is the feedback documents.
Let's say we are observing
These are the clicked documents by users
or are simply top ranked documents
Now imagine how we can
these documents by using language model.
One approach is simply to assume
these documents are generated
As we did before, what we could do
here to here and
Now the question is whether this
Well, you can imagine the top
What do you think?
Well, those words would be common words.
As we always see in a language model,
the top ranked words are actually
So it's not very good for feedback,
words to our query when we interpolate
So this was not good, so
In particular, we are trying to
And we have seen actually one way
language model in the case of
the words that are related
We could do that and that would be
are going to talk about another approach
In this case, we're going to say well,
in these documents that should not
So now what we can do is to assume that,
those words are generated from
they will generate those words like the,
And if we use maximum likelihood estimate,
note that if all the words here
then this model is forced to assign
because it occurs so frequently here.
Note that in order to reduce its
another model, which is this one,
And in this case,
it's not appropriate to use the background
goal because this model would assign high
So in this approach, then,
we assume this machine that was generating
We have a source control up here.
Imagine we flip a coin here to
With probability of lambda,
we're going to use
And we're going to do that in
With probability of 1 minus lambda,
to use a known topic model, here,
And we're going to then
If we make this assumption and this whole
this a mixture model because there are two
And we actually don't know when
So again,
and we can still ask for words and it will
And of course, which word will show up
that distribution.
In addition,
because if you say lambda is very high and
always use the background distribution,
Then if you say, well, lambda is
So all of these
And then if you're thinking this way,
basically we can do exactly
We're going to use maximum likelihood
to estimate the parameters.
Basically we're going to
that we can best explain all the data.
The difference now is that we are not
But rather we are going to ask this whole
Because it has got some help
it doesn't have to assign high
As a result, it will then assign higher
are common here but
So those would be common here.
And if they're common, they would
according to a maximum
And if they are rare here,
much help from this background model.
As a result, this topic model
So the high probability words,
would be those that are common here but
So this is basically a little bit
But this would allow us to achieve the
are meaningless in the feedback.
So mathematically, what we have is
local likelihood,
And note that we also have another
we assume that the lambda denotes
So we are going to,
Let's say 50% of the words are noise or
And this can then be
If we assume this is fixed, then we only
just like in the simple
We have n parameters,
And then the likelihood
It's very similar to the global
except that inside the logarithm
And this sum is because we
And which one is used would depend on
But mathematically, this is the function
So this is just a function.
All the other values are known except for
So we can then choose this
this log likelihood,
the same idea as the maximum likelihood
We just have to solve this
We essentially would try all
that gives this whole thing
So it's a well-defined math problem.
Once we have done that, we obtain this
original query model to the feedback.
So here are some examples of
document collection.
And we do pseudo-feedback we just
we use this mixture model.
So the query is airport security.
What we do is we first retrieve ten
this is of course pseudo-feedback.
And then we're going to feed that
And these are the words
This is the probability of a word given
So in both cases you can see the highest
probability words include the very
So airport security, for example,
these query words still show up as high
because they occur frequently
But we also see beverage,
So these are relevant to this topic,
if combined with original query, can help
And also they can help us bring up
these other words, maybe, for example,
So this is how pseudo-feedback works.
It shows that this model really works and
What's also interesting is that if
you compare them,
when lambda is set to a small value,
And that means, well,
Remember, lambda confuses the probability
to generate the text.
If we don't rely much on background model,
we still have to use this topic model
Whereas if we set lambda
we will use the background model
Then there's no burden on
in the feedback documents
So as a result, the topic model
It contains all the relevant
So this can be added to the original
So to summarize,
in this lecture we have talked about
In general,
These examples can be assumed examples,
like assume the top ten documents
They could be based on user interactions,
like feedback based on clickthroughs or
We talked about the three major
pseudo feedback, and implicit feedback.
We talked about how to use Rocchio to
how to use query model estimation for
And we briefly talked about
There are many other methods.
For example,
the relevance model is a very effective
So you can read more about these
are listed at the end of this lecture.
So there are two additional readings here.
The first one is a book that
discussion of language models for
And the second one is a important research
paper that's about relevance
and it's a very effective way
[MUSIC]
[SOUND]
more of the Munster learning algorithms
they generally attempt to direct
Like a MAP or nDCG.
Note that the optimization object or
on the previous slide is not directly
By maximizing the prediction of one or
zero, we don't necessarily optimize
One can imagine that our
And let's say both are around 0.5.
So it's kind of in the middle of zero and
But the ranking can be wrong, so we might
So that won't be good from
even though function, it's not bad.
In contrast, we might have another
around the 0.9, it said.
And by the objective function,
But if we didn't get the order
that's actually a better result.
So these new, more advanced approaches
Of course, then the challenge is
be harder to solve.
And then, researchers have posed
and you can read more of the references at
Now, these learning ranked
So there accounts would be be applied
not just the retrieval problem.
So some people will go
computational advertising,
there are many others that you can
To summarize this lecture we
learning to combine much more
Actually the use of machine learning
in information retrieval has
So for example, the Rocchio feedback
was a machine learning approach
But the most recent use of machine
changes in the environment of
First, it's mostly freedom of
in the form of critical, such as
So the data can provide a lot of
machine learning methods can be
Secondly, it's also freedom by
and this is not only just
features available on the web that can
It's also because by combining them,
of ranking, so this is desired for
Modern search engines all use some
combine many features
this is a major feature of these
The topic of learning to rank is still
and so we can expect to see new results
perhaps.
Here are some additional readings
about how learning to rank at works and
[MUSIC]
[SOUND]
This lecture is about natural language
content analysis.
Natural language content analysis
So we're going to first talk about this.
And in particular,
natural language processing with
And this determines what algorithms can
We're going to take a look at the basic
And I'm going to explain these concepts
using a similar example
A dog is chasing a boy on the playground.
Now this is a very simple sentence.
When we read such a sentence
about it to get the meaning of it.
But when a computer has to
the computer has to go
First, the computer needs
how to segment the words in English.
And this is very easy,
And then the computer will need
syntactical categories.
So for example, dog is a noun,
And this is called a Lexical analysis.
In particular, tagging these words
is called a part-of-speech tagging.
After that the computer also needs to
these words.
So a and dog would form a noun phrase.
On the playground would be
And there is certain way for
them to create meaning.
Some other combinations
And this is called syntactical parsing, or
syntactical analysis,
The outcome is a parse tree
That tells us the structure
that we know how we can
But this is not semantics yet.
So in order to get the meaning we
these structures into some real world
So dog is a concept that we know,
So connecting these phrases
Now for a computer, would have to formally
So dog, d1 means d1 is a dog.
Boy, b1 means b1 refers to a boy etc.
And also represents the chasing
So, chasing is a predicate here with
three arguments, d1, b1, and p1.
Which is playground.
So this formal rendition of
Once we reach that level of understanding,
For example, if we assume there's a rule
the person can get scared, then we
This is the inferred meaning,
And finally, we might even further infer
what this sentence is requesting,
or why the person who say it in
And so, this has to do with
This is called speech act analysis or
Which first to the use of language.
So, in this case a person saying this
bring back the dog.
So this means when saying a sentence,
So the action here is to make a request.
Now, this slide clearly shows that
a sentence there are a lot of
Now, in general it's very hard for
especially if you would want
This is very difficult.
Now, the main reason why natural
it's because it's designed it will
As a result, for example,
Because we assume all of
there's no need to encode this knowledge.
That makes communication efficient.
We also keep a lot of ambiguities,
And this is again, because we assume we
So, there's no problem with
possibly different things
Yet for
because a computer does not have
So the computer will be confused indeed.
And this makes it hard for
Indeed, it makes it very hard for
every step in the slide
Ambiguity is a main killer.
Meaning that in every step
and the computer would have to
that decision can be very difficult
And in general,
we need common sense reasoning in order
And computers today don't yet have that.
That's why it's very hard for
computers to precisely understand
So here are some specific
Think about the world-level ambiguity.
A word like design can be a noun or
we've got ambiguous part of speech tag.
Root also has multiple meanings,
like in the square of, or
Syntactic ambiguity refers
of a sentence in terms structures.
So for example,
natural language processing can
So one is the ordinary meaning that we
will be getting as we're
So, it's processing of natural language.
But there's is also another
which is to say language
Now we don't generally have this problem,
the structure, the computer would have
Another classic example is a man
And this ambiguity lies in
This is called a prepositional
Meaning where to attach this
Should it modify the boy?
Or should it be modifying, saw, the verb.
Another problem is anaphora resolution.
In John persuaded Bill to buy a TV for
Does himself refer to John or Bill?
Presupposition is another difficulty.
He has quit smoking implies
we need to have such a knowledge in
Because of these problems, the state
techniques can not do anything perfectly.
Even for
we still can not solve the whole problem.
The accuracy that are listed here,
was just taken from some studies earlier.
And these studies obviously have to
the numbers here are not
take it out of the context of the data
But I show these numbers mainly to give
or how well we can do things like this.
It doesn't mean any data set
But, in general, we can do parsing speech
Parsing would be more difficult, but for
phrases correct, we can probably
But to get the complete parse tree
For semantic analysis, we can also do
particularly, extraction of entities and
For example, recognizing this is
this person and
We can also do word sense to some extent.
The occurrence of root in this sentence
Sentiment analysis is another aspect
That means we can tag the senses
it's talking about the product or
Inference, however, is very hard,
any big domain and if it's only
And that's a generally difficult
Speech act analysis is
we can only do this probably for
And with a lot of help from humans
the computers to learn from.
So the slide also shows that
computers are far from being able to
And that also explains why the text
Because we cannot rely on
computational methods to
Therefore, we have to use
A particular statistical machine learning
to try to get as much meaning
And, later you will see
many such algorithms
interesting model from text even though
Meaning of all the natural
[MUSIC]
[SOUND].
This lecture is about the syntagmatic
In this lecture we are going to continue
In particular,
the concept in the information series,
how it can be used to discover
Before we talked about the problem
that is the conditional entropy
It is not really comparable, so
strong synagmatic relations
So now we are going to introduce mutual
in the information series
normalize the conditional entropy to make
In particular, mutual information
matches the entropy reduction
More specifically the question we
of an entropy of X can
So mathematically it can be
the original entropy of X, and
And you might see,
as reduction of entropy of
Now normally the two conditional
the entropy of Y given X are not equal,
the reduction of entropy by knowing
So, this quantity is called a Mutual
And this function has some interesting
This is easy to understand because
not going to be lower than the possibility
In other words, the conditional entropy
Knowing some information can
will not hurt us in predicting x.
The signal property is that it
entropy is not symmetrical,
the third property is that It
only if the two random variables
That means knowing one of them does not
this last property can be verified by
it reaches 0 if and
[INAUDIBLE] Y is exactly the same
So that means knowing why it did not
a Y are completely independent.
Now when we fix X to rank different
would give the same order as
because in the function here,
So ranking based on mutual entropy is
the conditional entropy of X given Y, but
the mutual information allows us to
So, that is why mutual information is
So, let us examine the intuition
Syntagmatical Relation Mining.
Now, the question we ask forcing
whenever "eats" occurs,
So this question can be framed as
which words have high mutual
so computer the missing information
And if we do that, and it is basically
we will see that words that
will have a high point.
Whereas words that are not related
For this, I will give some example here.
The mutual information between "eats" and
which is the same as between "meats" and
symmetrical is expected to be higher than
the, because knowing the does not
It is similar, and
the as well.
And you also can easily
information between a word and
which is equal to
so, because in this case the reduction is
maximum because knowing one allows
So the conditional entropy is zero,
therefore the mutual information
It is going to be larger, then are equal
In other words picking any other word and
the computer picking between eats and
You will not get any information larger
So now let us look at how to
Now in order to do that, we often
use a different form of mutual
rewrite the mutual information
Where we essentially see
called a KL-divergence or divergence.
This is another term
It measures the divergence
Now, if you look at the formula,
different values of the two random
mainly we are doing a comparison
The numerator has the joint,
actual observed the joint distribution
The bottom part or the denominator can be
interpreted as the expected joint
if they were independent because when
they are joined distribution is equal to
So this comparison will tell us whether
If they are indeed independent then we
but if the numerator is different
the two variables are not independent and
The sum is simply to take into
of the values of these
In our case, each random variable
zero or one, so
If we look at this form of mutual
information matches the divergence
from the expected distribution
The larger this divergence is, the higher
So now let us further look at what
involved in this formula
And here, this is all the probabilities
you to verify that.
Basically, we have first to
corresponding to the presence or
So, for w1,
They should sum to one, because a word
In the segment, and similarly for
the second word, we also have two
absences of this word, and
And finally, we have a lot of
the scenarios of co-occurrences of
And they sum to one because the two
possible scenarios.
Either they both occur, so
in that case both variables will have
There are two scenarios.
In these two cases one of the random
the other will be zero and finally we have
This is when the two variables
So these are the probabilities involved
over here.
Once we know how to calculate
we can easily calculate
It is also interesting to know that
constraint among these probabilities,
So in the previous slide,
that you have seen that
words sum to one and
that says the two words have these
but we also have some additional
For example, this one means if we add up
the probabilities that we observe
the probabilities when the first word
We get exactly the probability
In other words, when the word is observed.
When the first word is observed, and
there are only two scenarios, depending on
So, this probability captures the first
actually is also observed, and
this captures the second scenario
So, we only see the first word, and
it is easy to see the other equations
Now these equations allow us to
other probabilities, and
So more specifically,
a word is present, like in this case,
if we know the probability of
then we can easily compute
It is very easy to use this
we take care of the computation of
absence of each word.
Now let's look at
Let us assume that we also have available
the probability that
Now it is easy to see that we can
probabilities based on these.
Specifically for
the probability that the first word
because we know these probabilities in
equation we can compute the probability
Word.
And then finally,
by using this equation because
this is also known, and
So this can be easier to calculate.
So now this can be calculated.
So this slide shows that we only
these three probabilities
naming the presence of each word and the
[MUSIC]
[SOUND]
lecture is about probabilistic and
In this lecture we're going to introduce
often called PLSA.
This is the most basic topic model,
Now this kind of models
mine multiple topics from text documents.
And PRSA is one of the most basic
So let's first examine this power
Here I show a sample article which is
And I show some simple topics.
For example government response,
Donation and the background.
You can see in the article we use
So we first for example see there's
this is followed by discussion of flooding
We also see background
So the overall of topic analysis here
the text, to segment the topics,
distribution and to figure out first,
How do we know there's a topic
There's a topic about a flood in the city.
So these are the tasks
If we had discovered these
as you see here,
Then you can do a lot of things,
of the topics,
So the formal definition of problem of
shown here.
And this is after a slide that you
So the input is a collection, the number
of course the text data.
And then the output is of two kinds.
One is the topic category,
Theta i's.
Each theta i is a word distribution.
And second, it's the topic coverage for
These are pi sub i j's.
And they tell us which document it covers.
Which topic to what extent.
So we hope to generate these as output.
Because there are many useful
So the idea of PLSA is
the two component mixture model
The only difference is that we
Otherwise, it is essentially the same.
So here I illustrate how we can generate
naturally in all cases
of Probabilistic modelling would want
So we would also ask the question,
what's the probability of observing
Now if you look at this picture and
compare this with the picture
you will see the only difference is
So, before we have just one topic,
But now we have more topics.
Specifically, we have k topics now.
All these are topics that we assume
So the consequence is that our switch for
Before it's just a two way switch.
We can think of it as flipping a coin.
But now we have multiple ways.
First we can flip a coin to decide
So it's the background lambda
1 minus lambda sub B gives
actually choosing a non-background topic.
After we have made this decision,
we have to make another decision to
So there are K way switch here.
And this is characterized by pi,
This is just the difference of designs.
Which is a little bit more complicated.
But once we decide which distribution to
just generate a word by using one of
So now lets look at the question
So what's the probability of observing
What do you think?
Now we've seen this
if you can recall, it's generally a sum.
Of all the different possibilities
So let's first look at how the word can
Well, the probability that the word is
is lambda multiplied by the probability
Model, right.
Two things must happen.
First, we have to have
and that's the probability of lambda,
Then second, we must have actually
and that's probability
Okay, so similarly,
we can figure out the probability of
Like the topic theta sub k.
Now notice that here's
And that's because of the choice
only happens if two things happen.
One is we decide not to
So, that's a probability
Second, we also have to actually choose
So that's probability of theta sub K,
And similarly, the probability of
The topic and the first topic
And so
in the end the probability of observing
And I have to stress again this is a very
really key to understanding all the topic
So make sure that you really
of w is indeed the sum of these terms.
So, next,
we would be interested in
All right, so to estimate the parameters.
But firstly,
let's put all these together to have the
The first line shows the probability of a
And this is an important
So let's take a closer look at this.
This actually commands all
So first of all we see lambda sub b here.
This represents a percentage
that we believe exist in the text data.
And this can be a known value
Second, we see the background
typically we also assume this is known.
We can use a large collection of text, or
use all the text that we have available
Now next in the next stop this formula.
[COUGH] Excuse me.
You see two interesting
those are the most important parameters.
That we are.
So one is pi's.
And these are the coverage
And the other is word distributions
So the next line,
in to calculate
This is, again, of the familiar
you have a count of
And then log of a probability.
Now it's a little bit more
Because now we have more components,
And then this line is just
And it's very similar, just accounting for
So what are the unknown parameters?
I already said that there are two kinds.
One is coverage,
Again, it's a useful exercise for
Exactly how many
How many unknown parameters are there?
Now, try and
think out that question will help you
And will also allow you to understand
when use PLSA to analyze text data?
And these are precisely
So after we have obtained
the next is to worry about
And we can do the usual think,
So again, it's a constrained optimization
Only that we have a collection of text and
And we still have two constraints,
One is the word distributions.
All the words must have probabilities
The other is the topic
a document will have to cover
the probability of covering each
So at this point though it's basically
you just need to figure out
There's a function with many variables.
and we need to just figure
variables to make the function
>> [MUSIC]
[SOUND]
This lecture is a overview of
In the previous lecture, we introduced
We explained that the main problem
is the design of ranking function
In this lecture,
we will give an overview of different
So the problem is the following.
We have a query that has
the document that's also
And we hope to define a function f
that can compute a score based
So the main challenge you hear is with
can rank all the relevant documents
Clearly, this means our function
the likelihood that a document
That also means we have to have
In particular, in order to
we have to have a computational
And we achieve this goal by
which gives us
Now, over many decades,
researchers have designed many
And they fall into different categories.
First, one family of the models
Basically, we assume that if
the query than another document is,
then we will say the first document
So in this case,
the similarity between the query and
One well known example in this
which we will cover more in
A second kind of models
In this family of models, we follow a very
queries and documents are all
And we assume there is a binary
to indicate whether a document
We then define the score of document with
this random variable R is equal to 1,
There are different cases
One is classic probabilistic model,
yet another is divergence
In a later lecture, we will talk more
A third kind of model are based
So here the idea is to associate
and we can then quantify
show that the query
Finally, there is also a family of models
that are using axiomatic thinking.
Here, an idea is to define
hope a good retrieval function to satisfy.
So in this case, the problem is
that can satisfy all
Interestingly, although these different
in the end, the retrieval function
And these functions tend to
So now let's take a look at the common
and to examine some of the common
First, these models are all
of using bag of words to represent text,
we explained this in the natural
Bag of words representation remains
the search engines.
So with this assumption,
like a presidential campaign news
would be based on scores computed
And that means the score would
such as presidential, campaign, and news.
Here, we can see there
each corresponding to how well the
Inside of these functions,
So for example, one factor that
is how many times does the word
This is called a term frequency, or TF.
We might also denote as
In general, if the word occurs
then the value of this
Another factor is,
And this is to use the document length for
In general, if a term occurs in a long
document many times,
if it occurred the same number
Because in a long document, any term
Finally, there is this factor
That is, we also want to look at how
collection, and we call this document
And in some other models,
to characterize this information.
So here, I show the probability of
So all these are trying to characterize
the collection.
In general, matching a rare term in
to the overall score than
So this captures some of the main ideas
the art original models.
So now, a natural question is,
Now it turns out that many
So here are a list of
that are generally regarded as
pivoted length normalization,
When optimized,
And this was discussed in detail in this
Among all these,
It's most likely that this has been used
and you will also often see this
And we'll talk more about this
So, to summarize, the main points made
of a good ranking function pre-requires a
we achieve this goal by designing
Second, many models are equally effective,
Researchers are still active and
trying to find a truly
Finally, the state of the art
to rely on the following ideas.
First, bag of words representation.
Second, TF and
Such information is used in
the overall contribution of matching
These are often combined in interesting
exactly they are combined to rank
There are two suggested additional
The first is a paper where you can
comparison of multiple
The second is a book with
review of different retrieval models.
[MUSIC]
[SOUND]
about how to use generative probabilistic
There are in general about two kinds
by using machine learning.
One is by generating probabilistic models.
The other is discriminative approaches.
In this lecture, we're going to
In the next lecture, we're going to
So the problem of text categorization
is actually a very similar
In that, we'll assume that each document
The main difference is that in
what are the predefined categories are,
In fact,
We want to find such clusters in the data.
But in the case of categorization,
So we kind of have
then based on these categories and
a document to one of these categories or
But because of the similarity
we can actually get the document
And we understand how we can
text categorization from
And so, this is a slide that we've talked
where we assume there are multiple topics
Each topic is one cluster.
So once we estimated such a model,
we faced a problem of deciding which
And this question boils down to decide
Now, suppose d has L words
Now, how can you compute
topic word distribution zeta i has
Well, in general, we use base
you can see this prior information here
cluster has a higher prior
that the document has
And so, we should favor such a cluster.
The other is a likelihood part,
And this has to do with whether
can explain the content
And we want to pick a topic
So more specifically,
then choose which topic
So more rigorously,
So we're going to choose
This posterior probability at the top
because this one,
That's our belief about
before we observe any document.
But this conditional probability here is
the posterior probability of the topic
And base wall allows us to update this
I have shown the details,
the prior here is related to
And this is related to how
explains the document here, and
So to find the topic that has the higher
posterior probability here it's
as we have seen also,
And we can then change the probability
the probability of each word, and
an assumption about independence
So this is just something that you
And we now can see clearly how we
based on the information
these categories and
So this idea can be directly
And this is precisely what
So here it's most really
that we're looking at
So we assume that if theta i
represents category i accurately,
characterizes the content of
Then, what we can do is precisely
Namely we're going to assign
that has the highest probability
In other words, we're going to maximize
And this is related to the prior and
the [INAUDIBLE] as you have
And so, naturally we can decompose
this [INAUDIBLE] into
Now, here, I change the notation so
product of all the words
even though the document
And the product is still accurately
of all the words in the document
When a word,
The count would be 0, so
So if actively we'll just have the product
So basically, with Naive Bayes Classifier,
we're going to score each category for
Now, you may notice that here it involves
And this can cause and the four problem.
So one way to solve the problem is
which it doesn't changes all
But will helps us preserve precision.
And so, this is often the function
score each category and
the category that has the highest
So this is called an Naive Bayes
understandable because we are applying
the posterior probability of the topic to
Now, it's also called a naive because
in the document is generated
assumption because in reality they're
Once you see some word,
For example,
Than that mixed category,
they see more clustering more likely to
But this assumption allows
And it's actually quite effective for
But you should know that
have to make this assumption.
We could for example, assume that
So that would make it a bigram analogy
And of course you can even use a mixture
like in each category.
So in nature, they will be all using
But the actual generating model for
And here, we just talk about very
So now the question is,
actually represents category i accurately?
Now in clustering,
what are the distributions for
But in our case,
this theta i represents indeed category i.
Well if you think about the question, and
you likely come up with the idea
Indeed in the textbook,
we typically assume that there
those are the documents that unknown
In other words, these are the documents
of course human experts must do that.
In here, you see that T1
that are known to have
And T2 represents the documents
generated from category 2, etc.
Now if you look at this picture,
here is really a simplified
It's no longer mixed modal, why?
Because we already know which distribution
There's no uncertainty here, there's
So the estimation problem of
But in general,
estimate these probabilities
And what other probability is that we have
Well there are two kinds.
So one is the prior,
this indicates how popular
how likely will it have observed
The other kind is
we want to know what words have high
So the idea then is to just
to estimate these two probabilities.
And in general, we can do this
That's just because these documents
from a specific category.
So once we know that,
it's in some sense irrelevant of what
So now this is a statistical
We have observed some
we want to guess
We want to take our best
And this is a problem that we have seen
Now, if you haven't thought
haven't seen life based classifier.
It would be very useful for
to think about how to solve this problem.
So let me state the problem again.
So let's just think about with category 1,
we know there is one word of distribution
And we generate each word in the document
observed a set of n sub 1
These documents have been all
Namely have been all generated
Now the question is,
estimate of the probability of
And what would be your guess of
Of course,
how likely are you to see
So think for a moment, how do you
all these documents that are known
to estimate all these parameters?
Now, if you spend some time
it would help you understand
So do spend some time to make sure that
do you best to solve the problem yourself.
Now if you have thought about and
First, what's the bases for estimating the
Well this has to do with whether you
form that category.
Intuitively, you have seen a lot
very few in medical science.
Then you guess is that the probability
your prior on the category
And what about the basis for estimating
Well the same, and you'll be just
frequently in the documents that are known
likely have a higher probability.
And that's just a maximum
Indeed, that's what we can do, so this
to answer the question,
Then we can simply normalize,
So here you see N sub i denotes
And we simply just normalize these
In other words, we make this
of training intercept in each category
Now what about the word distribution?
Well, we do the same.
Again this time we can do this for
So let's say,
So which word has a higher probability?
Well, we simply count the word occurrences
in the documents that are known
And then we put together all
And then we just normalize these
of all the words make all
So in this case, you're going to see this
the word in the collection of
that's denoted by c of w and T sub i.
Now, you may notice that we
estimate in the form of being
And this is often sufficient,
because we have some constraints
So the normalizer is
So in this case, it will be useful for
what are the constraints on these
So once you figure out
you will know how to
And so this is a good exercise to
There is another issue in
In fact the smoothing is a general problem
And this has to do with,
what would happen if you have
So smoothing is an important technique
In our case, the training data can be
we use maximum likely estimator we often
That means if an event is not observed
then the estimated
In this case, if we have not seen
let's say, category i.
Then our estimator would be zero for the
and this is generally not accurate.
So we have to do smoothing to make
The other reason for smoothing is that
and this is also generally true for
When the data set is small,
we tend to rely on some prior
So in this case our [INAUDIBLE] says that
So smoothing allows us to inject
order has a real zero probability.
There is also a third reason which
we explain that in a moment.
And that is to help achieve
And this is also called IDF weighting,
inverse document frequency weighting that
So how do we do smoothing?
Well in general we add pseudo
we'll make sure that no event has 0 count.
So one possible way of smoothing
is to simply add a small non active
Let's pretend that every category
documents represented by delta.
And in the denominator we also add
we want the probability to some to 1.
So in total we've added delta k times
Therefore in this sum,
delta as a total pseudocount
Now, it's interesting to think
obvious data is a smoothing
Meaning that the larger data is and
that means we'll more
And we might indeed ignore the actual
set to infinity.
Imagine what would happen if there
Well, we are going to say every category
And then there's no distinction to them so
What if delta is 0?
Well, we just go back to the original
data to estimate to estimate
Now we can do the same for
But in this case,
to use a nonuniform seudocount for
So here you'll see we'll add
that's mule multiplied
the word given by a background
Now that background model in
a logic collection of tests.
Or in this case we will use the whole
estimate this background language model.
But we don't have to use this one,
we can use larger test data that
Now if we use such a background
we'll find that some words will
So what are those words?
Well those are the common words
the background average model.
So the pseudocounts added for
Real words on the other hand
Now this addition of background
smoothing of these word distributions.
We're going to bring the probability of
because of the background model.
Now this helps make the difference
such words smaller across categories.
Because every category has some help
I get the, a,
Therefore, it's not always so
has documents that contain a lot
the estimate is more influenced
And the consequence is that
such words tend not to influence
as words that have small probabilities
Those words don't get some help
So the difference would be primary because
in the training documents
We also see another smoothing parameter
smoothing and just like a delta does for
And you can easily understand why we
represents the sum of all the pseudocounts
So view is also a non
it's [INAUDIBLE] set to control smoothing.
Now there are some interesting
First, let's think about when mu
Well in this case
to the background language model we'll
So we will bring every word distribution
that essentially remove the difference
Obviously, we don't want to do that.
The other special case is the thing
suppose, we actually set
And let's say,
So each one has the same probability,
going to be very similar to the one
It's because we're going to add
So in general,
in Naive Bayes categorization we
And then once we have these probabilities,
then we can compute the score for
For a document and
then choose the category where it was
Now, it's useful to
the Naive Bayes scoring
So to understand that, and also to
will actually achieve the effect of IDF
So suppose we have just two categories and
we're going to score based on
So this is the.
Lets say this is our scoring function for
two categories, right?
So, this is a score of a document for
And we're going to score based
So if the ratio is larger,
then it means it's more
So the larger the score is the more likely
the document is in category one.
So by using Bayes' rule,
we can write down this ratio as follows,
Now, we generally take logarithm of this
And this would then give us this
And here we see something
because this is our scoring function for
And if you look at this function,
The first part here is actually
And so this is a category bias.
It doesn't really depend on the document.
It just says which category is more
this category slightly, right?
So, the second part has a sum
So, these are the words that
in general we can consider all
So here we're going to
about which category is more likely,
So inside of the sum you can see
The first, is a count of the word.
And this count of the word serves as
And this is what we can
The second part is
here it's the weight on which word, right?
This weight tells us to
this word helps contribute in our decision
to put this document in category one.
Now remember,
the more likely it's in category one.
Now if you look at this ratio, basically,
on the ratio of the probability of the
Essentially we're comparing
distributions.
And if it's a higher according to theta 1,
then according to theta 2,
And therefore it means when
we will say that it's more
And the more we observe such a word,
the more likely the document
If, on the other hand,
theta 1 is smaller than the probability
then you can see that
Therefore, this is negative evidence for
That means the more we
the more likely the document
So this formula now makes a little sense,
So we're going to aggregate all
we take a sum of all the words.
We can call this the features
that we collected from the document
And then each feature has
does this feature support category one or
And this is estimated as the log of
And then finally we have
So that formula actually
be generalized to accommodate
that's why I have introduce
To introduce beta 0 to denote the Bayes
beta sub i to denote
Now we do this generalisation,
general we can represent
here of course in this case
But in general, we can put any features
categorization.
For example, document length or
font size or
And then our scoring function can be
the sum of the feature
So if each f sub i is a feature
by the corresponding weight,
And this is the aggregate of all evidence
features.
And of course there are parameters here.
So what are the parameters?
Well, these are the betas.
These betas are weights.
And with a proper setting of the weights,
to work well to classify documents,
We can clearly see naive Bayes
this general classifier.
Actually, this general form is very close
regression, and this is actually one
discriminative approaches
And we're going to talk more
here I want you to note that
a close connection between
And this slide shows how naive Bayes
a logistic regression.
And you can also see that in
that tend to use more
we can accommodate more
[MUSIC]
[NOISE] This lecture is about the ordinal
logistic regression for
So, this is our problem set up for a
Or more specifically a rating prediction.
We have an opinionated text document d as
a rating in the range of 1 through k so
it's a discrete rating, and
We have k categories here.
Now we could use a regular text for
categorization technique
But such a solution would not consider the
Intuitively, the features that can
or rather rating 2 from 1,
those that can distinguish k from k-1.
For example, positive words
When we train categorization
problem by treating these categories as
So what's the solution?
Well in general we can order to classify
And here we're going to
called ordinal logistic regression.
Now, let's first think about how
a binary sentiment.
A categorization problem.
So suppose we just wanted to distinguish
that is just a two category
So the predictors are represented as X and
And there are M features all together.
The feature value is a real number.
And this can be representation
And why it has two values,
1 means X is positive,
And then of course this is a standard
We can apply logistical regression.
You may recall that in logistical
of probability that the Y is equal to one,
assumed to be a linear function
So this would allow us to also write
in this equation that you
So that's a logistical function and
you can see it relates
probability that y=1
And of course beta i's
just a direct application of logistical
What if we have multiple categories,
Well we have to use such a binary
to solve this multi
And the idea is we can introduce
In each case we asked
whether the rating is j or above,
So when Yj is equal to 1,
When it's 0,
So basically if we want to predict
we first have one classifier to
And that's our classifier one.
And then we're going to have another
At k-1 from the rest.
That's Classifier 2.
And in the end, we need a Classifier
So altogether we'll have k-1 classifiers.
Now if we do that of course then
and the logistical regression program
as you have just seen
Only that here we have more parameters.
Because for each classifier,
So now the logistical regression
which corresponds to a rating level.
And I have also used of
And this is to.
Make the notation more consistent,
than was what we can show in
So here we now have basically k minus one
Each has it's own set of parameters.
So now with this approach,
After we have trained these k-1
separately of course,
then invoke a classifier
So first let look at the classifier
So this classifier will tell
have a rating of K or about.
If probability according to this
larger than point five,
The rating is K.
Now, what if it's not as
Well, that means the rating's below K,
So now,
which tells us whether
It's at least K minus one.
And if the probability is
then we'll say, well, then it's k-1.
What if it says no?
Well, that means the rating
And so we're going to just keep
And here we hit the end when we need
So this would help us solve the problem.
Right?
So we can have a classifier that would
in the range of 1 through k.
Now unfortunately such a strategy is not
And specifically there are two
So these equations are the same as.
You have seen before.
Now the first problem is that there
There are many parameters.
Now, can you count how many
Now this may be a interesting exercise.
To do.
you might want to just pause the video and
How many parameters do I have for
And how many classifiers do we have?
Well you can see the, and so
n plus one parameters, and we have k
so the total number of parameters is
That's a lot.
A lot of parameters, so when
we would in general need a lot of data
to help us decide the optimal
So that's not ideal.
Now the second problems
these k minus 1 plus fives,
These problems are actually dependent.
In general, words that are positive
for any of these classifiers.
For all these classifiers.
So we should be able to take
Now the idea of ordinal logistical
The key idea is just
independent logistical
And that idea is to tie
And that means we are going to
These are the parameters that indicated
And we're going to assume these
all the K- 1 parameters.
And this just encodes our intuition that,
positive words in general would
So this is intuitively assumptions,
And we have this order
Now in fact, this would allow us
One is it's going to reduce
And the other is to allow us
Because all these parameters
So these training data, for
shared to help us set
So we have more data to help
So what's the consequence,
well the formula would look very similar
now the beta parameter has just one
It no longer has the other index that
So that means we tie them together.
And there's only one set of better
However, each classifier still
The R for parameter.
Except it's different.
And this is of course needed to predict
So R for sub j is different it
has a different R value.
But the rest of the parameters,
So now you can also ask the question,
Again, that's an interesting
So if you think about it for a moment, and
you will see now, the param,
Specifically we have M plus K minus one.
Because we have M, beta values, and
So let's just look basically,
that's basically the main idea of
So, now, let's see how we can use such
It turns out that with this, this idea of
We also end up by having
And more specifically now, the criteria
are at least 0.5 above,
whether the score of
equal to negative authors of j,
Now, the scoring function is just
all the features with
So, this means now we can simply make
the value of this scoring function,
Now you can see the general
when the score is in the particular
then we will assign the corresponding
So in this approach,
by using the features and
This score will then be
alpha values to see which
And then,
using the range, we can then decide which
Because, these ranges of alpha
levels of ratings, and that's from
Each is tied to some level of rating.
[MUSIC]
[MUSIC]
This lecture is about the implementation
In this lecture we will discuss
retrieval method to build a search engine.
The main challenge is to
to enable a query to be answered very
This is a typical text
We can see the documents are first
get tokenized units, for example, words.
And then, these words, or
a indexer that will create a index,
the search engine to use
And the query would be going
So the Tokenizer would be
so that the text can be
The same units would be
The query's representation would
which would use the index to quickly
the documents and then ranking them.
The results will be given to the user.
And then the user can look at the results
explicit judgements of both
which documents are bad.
Or implicit feedback such as so that
End user will just look at the results,
skip some, and
So these interacting signals can be used
accuracy by assuming that viewed documents
So a search engine system then
The first part is the indexer, and
responds to the users query, and
Now typically, the Indexer is
you can pre-process the correct data and
to build the inventory index,
And this data structure can then be used
to process a user's query dynamically and
The feedback mechanism can be done online
The implementation of the indexer and
and this is the main topic of this
The feedback mechanism,
it depends on which method is used.
So that is usually done in
Let's first talk about the tokenizer.
Tokernization is a normalized lexical
so that semantically similar words
Now, in the language like English,
this will map all the inflectional
So for example, computer, computation, and
computing can all be matched
This way all these different forms of
Now normally, this is a good idea,
to increase the coverage of documents
But it's also not always beneficial,
because sometimes the subtlest
computation might still suggest the
But in most cases,
When we tokenize the text in some other
face some special challenges in segmenting
Because it's not obvious
there's no space to separate them.
So here of course, we have to use some
Once we do tokenization, then we would
convert the documents and do some data
The basic idea is to precompute
So the most commonly used index
And this has been used
to support basic search algorithms.
Sometimes the other indices, for example,
document index might be needed in order
And these kind of techniques
that they vary a lot according
To understand why we want to use
you to think about how you would
So if you want to use more time to
So think about how you can
that you can quickly respond
Where if you have thought
you might realize that where
the list of documents that match
In this way, you can basically
So when you see a term you can simply just
that term and return the list to the user.
So that's the fastest way to
Now the idea of the invert index
We're going to do pre-constructed
us to quickly find all the documents
So let's take a look at this example.
We have three documents here,
and these are the documents that you
Suppose that we want to create
Then we want to maintain a dictionary, in
each term and we're going to store
For example, the number of
the total number of code or
which means we would kind of duplicate
And so, for example, news,
this term occur in all
so the count of documents is three.
And you might also realize we needed this
for computing some statistics to
Can you think of that?
So what weighting heuristic
Well, that's the idea, right,
So, IDF is the property of a term,
So, with the document that count here,
either at this time, or
At random time when we see a query.
Now in addition to these basic statistics,
we'll also store all the documents
and these entries are stored
So in this case it matched
we store information about
This is the document id,
The tf is one for news, in the second
So from this list, we can get all
we can also know the frequency
So, if the query has just one word,
we have easily look up to this
go quicker into the postings to fetch
So, let's take a look at another term.
This time, let's take a look
This would occur in only one document,
So the document frequency is 1 but
So the frequency count is two, and
some other reachable method where
assess the popularity of
Similarly we'll have a pointer
and in this case,
the term occurred in just one document and
The document id is 3 and
So this is the basic
It's actually pretty simple, right?
With this structure we can easily fetch
And this will be the basis for
Now sometimes we also want to store
So in many of these cases the term
So there's only one position for
But in this case, the term occurred
Now the position information is very
the matching of query terms is
let's say, five words or ten words.
Or, whether the matching of the two query
That this can all be checked quickly
So, why is inverted index good for
Well, we just talked about the possibility
And that's very easy.
What about the multiple term queries?
Well let's first look at the some
A Boolean query is basically
So I want the value in the document
So that's one conjunctive query.
Or I want the web documents
That's a disjunctive query.
But how can we answer such
Well if you think a bit about it,
it would be obvious because
the documents that match term A and also
And then just take the intersection
Or to take the union to
So this is all very easy to answer.
It's going to be very quick.
Now what about the multi-term
We talked about the vector space model for
we will do a match such query with
And the score is based on
So in this case it's not
the scoring can be actually
Basically it's similar to
Basically, it's like A or B.
We take the union of all the documents
then we would aggregate the term weights.
So this is a basic idea of using inverted
And we're going to talk about
But for now,
let's just look at the question
Basically why is more efficient than
This is the obvious approach.
You can just compute a score for each
And this is a straightforward method but
this is going to be very slow imagine
If you do this then it will take
So the question now is why would
Well it has to do is the word
So, here's some common phenomena
There are some languages independent
And these patterns are basically
A few words like the common
we occur very, very frequently in text.
So they account for
But most words would occur just rarely.
There are many words that occur just once,
let's say, in a document or
And there are many such.
It's also true that the most
they have to be rare in another.
That means although the general
was observed in many cases that
may vary from context to context.
So this phenomena is characterized
This law says that the rank of a word
multiplied by the frequency of
So formally if we use F(w)
r(w) to denote the rank of a word.
Then this is the formula.
It basically says the same thing,
Where C is basically a constant and
alpha, that might be adjusted to
So if I plot the word
then you can see this more easily.
The x axis is basically the word rank.
This is r(w) and
Now this curve shows that the product
Now if you look at these words, we can see
In the middle,
These words tend to occur
they are not like those
And they are also not very rare.
So they tend to be often used in
queries and they also tend
These intermediate frequency words.
But if you look at the left
these are the highest frequency words.
They are covered very frequently.
They are usually words,
Those words are very, very frequent and
discriminated, and they are generally
So they are often removed and
So you can use pretty much just the kind
infer what words might be stop words.
Those are basically
And they also occupy a lot of
You can imagine the posting entries for
And then therefore,
if you can remove such words you can save
We also show the tail part,
Those words don't occur very frequently,
Those words are actually very useful for
search also, if a user happens to
But because they're rare,
aren't necessarily
But retain them would allow us to
They generally have very high IDF.
So what kind of data structures should
Well, it has two parts, right.
If you recall, we have a dictionary and
The dictionary has modest size, although
large but compare it with
And we also need to have fast
because we're going to look up
So therefore, we'd prefer to keep such
If the collection is not very large,
if the collection is very large
If the vocabulary size is very large,
So, in general that's how it goes.
So the data structures
storing dictionary,
There are structures like hash table, or
b-tree if we can't store
And then try to build a structure that
For postings they are huge.
And in general, we don't have to have
We generally would just look up
frequencies for all the documents
So would read those entries sequentially.
And therefore because it's large and
they have to stay on disc and they would
term frequency or
Now because they are very large,
Now this is not only to save disc space,
one benefit of compression, it It's
But it's also to help improving speed.
Can you see why?
Well, we know that input and
In comparison with the time taken by CPU.
So, CPU is much faster but
so by compressing the inverter index,
the entries, that we have the readings,
would be smaller, and
the amount of tracking IO and
Of course, we have to then do more
uncompress the data in the memory.
But as I said CPU is fast.
So over all we can still save time.
So compression here is both
to speed up the loading of the index.
[MUSIC]
[SOUND]
So average precision is computer for
just one.
one query.
different queries and this is to
Depending on the queries you use you
Right, so
If you use more queries then,
take the average of the average
So how can we do that?
Well, you can naturally.
Think of just doing arithmetic mean as we
always tend to, to think in, in this way.
So, this would give us what's called
In this case,
we take arithmetic mean of all the average
But as I just mentioned in
We call that.
We talked about the different ways
And we conclude that the arithmetic
But here it's the same.
We can also think about the alternative
Don't just automatically assume that,
Let's just also take the arithmetic
these queries.
Let's think about what's
If you think about the different ways,
probably be able to think about
And we call this kind of average a gMAP.
This is another way.
So now, once you think about
Of doing the same thing.
The natural question to ask is,
So.
So, do you use MAP or gMAP?
Again, that's important question.
Imagine you are again
by comparing the ways your old
Now you tested multiple topics.
Now you've got the average precision for
Now you are thinking of looking
You have to take the average.
But which, which strategy would you use?
Now first, you should also think about the
Can you think of scenarios where using
That is they would give different
And that also means depending on
Average of these average positions.
You will get different conclusions.
This makes the question
Right?
Well again, if you look at
Different ways of aggregating
You'll realize in arithmetic mean,
So what does large value here mean?
It means the query is relatively easy.
You can have a high pres,
Whereas gMAP tends to be
And those are the queries that
The average precision is low.
So if you think about the,
those difficult queries,
On the other hand, if you just want to.
Have improved a lot.
Over all the kinds of queries or
easy and you want to make the perfect and
So again, the answer depends on
their pref, their preferences.
So the point that here is to think
the same problem, and then compare them,
And which one makes more sense.
Often, when one of them might
another might make more sense
So it's important to pick out under
As a special case of the mean average
the case where there was precisely
And this happens often, for example,
Where you know a target page, let's
You have one relevant document there,
That's call a "known item search".
In that case,
Or in another application,
maybe there's only one answer.
Are there.
So if you rank the answers,
then your goal is to rank that one
So in this case, you can easily
will basically boil down
That is, 1 over r where r is the rank
So if that document is ranked
then it's 1 for reciprocal rank.
If it's ranked at the,
Et cetera.
And then we can also take a, a average
reciprocal rank over a set of topics, and
that would give us something
It's a very popular measure.
For no item search or, you know,
an problem where you have
Now again here, you can see this
And this r is basically
a user would have to make in order
If it's ranked on the top it's low effort
But if it's ranked at 100
read presumably 100 documents
So, in this sense r is also a meaningful
take the reciprocal of r,
So my natural question here
I imagine if you were to design
of a random system,
You might have thought about
After all,
But, think about if you take a average
Again it would make a difference.
Right, for one single topic, using r or
using 1 over r wouldn't
It's the same.
Larger r with corresponds
But the difference would only show when,
So again, think about the average of Mean
What's the difference?
Do you see any difference?
And would, would this difference
In our conclusion.
And this, it turns out that,
if you think about it, if you want to
then pause the video.
Basically, the difference is,
Again it will be dominated
So what are those values?
Those are basically large values that
That means the relevant items
And the sum that's also the average
Where those relevant documents
in the lower portion of the ranked.
But from a users perspective we care
So by taking this transformation
Here we emphasize more on
You know, think about
it would make a big difference, in 1 over
where and when won't make much
But if you use this there will
let's say 1,000, right.
So this is not the desirable.
On the other hand, a 1 and
So this is yet another case where there
thing and then you need to figure
So to summarize,
Can characterize the overall
And we emphasized that the actual
on how many top ranked results
Some users will examine more.
Than others.
An average person uses a standard measure
It combines precision and recall and
it's sensitive to the rank
[MUSIC]
[SOUND]
lecture is about smoothing
In this lecture,
we're going to continue talking about
In particular,
language model in the query
So you have seen this slide
This is the ranking function
Here, we assume that the independence of
would look like the following where
And inside the sum there is a log
the document or document image model.
So the main task now is to estimate this
document language model as we
estimating this model would lead
So in this lecture, we're going to
So how do we estimate this language model?
Well the obvious choice would be
that we have seen before.
And that is we're going to normalize
And estimate the probability
This is a step function here.
Which means all of the words that have
the same frequency count will
This is another freedom to count,
Note that for words that have not
they will have 0 probability.
So we know this is just like the model
Where we assume that the use of
a formula to clear it.
And there's no chance of assembling any
we know that's not good.
So how do we improve this?
Well in order to assign
to words that have not been observed in
some probability mass from the words
So for example here, we have to take away
need some extra probability mass for
So all these probabilities must sum to 1.
So to make this transformation and to
by assigning non zero probabilities to
We have to do smoothing and
the estimate by considering
had been asking to write more words for
the document,
If you think about this factor
would be a more accurate than
Imagine you have seen an abstract
Let's say this document is abstract.
If we assume and see words in this
That would mean there's
a word outside the abstract
But imagine a user who is interested
The user might actually
that chapter to use as query.
So obviously,
author would have written
So smoothing of the language
to recover the model for
And then of course,
words that are not
So that's why smoothing is
So let's talk a little more about
The key question here is, what probability
And there are many different
One idea here, that's very useful for
word be proportional to its probability
That means if you don't observe
We're going to assume that its
by another reference language
It will tell us which unseen words
In the case of retrieval,
take the collection language model
That is to say, if you don't
we're going to assume that
would be proportional to the probability
So more formally,
we'll be estimating the probability
If the word is seen in
would be this counted the maximum
Otherwise, if the word is not seen in the
be proportional to the probability
And here the coefficient that offer is to
control the amount of probability
Obviously, all these
alpha sub d is constrained in some way.
So what if we plug in this
query likelihood ranking function?
This is what we will get.
In this formula, we have this
as a sum over all the query words and
those that we have written here as the sum
This is the sum of all
but not that we have a count
So in fact, we are just taking
This is now a common
because of its convenience
So this is as I said,
In our smoothing method,
are not observed in the method would have
Name it's four, this foru.
So we're going to do then,
One sum is over all the query words
That means that in this sum, all the words
have a non zero probability
Sorry, it's the non zero count
They all occur in the document.
And they also have to of course
So these are the query words
On the other hand, in this sum we
that are not all query was
So they occur in the query
they don't occur in the document.
In this case,
these words have this probability because
That here, these seen words
Now, we can go further by
as a difference of two other sums.
Basically, the first sum is
Now, we know that the original sum
This is over all the query words that
So here we pretend that they
So we take a sum over all the query words.
Obviously, this sum has extra
Because, here we're taking
There, it's not matched in the document.
So in order to make them equal, we will
And this is the sum over all the query
And this makes sense, because here
And then we subtract the query
That would give us the query that
And this is almost a reverse
And you might wonder why
Well, that's because if we do this,
then we have different forms
So now, you can see in this sum
the query was matching the document
Here we have another sum over the same set
But inside the sum, it's different.
But these two sums can clearly be merged.
So if we do that, we'll get another form
of the formula that looks like
And note that this is
Because here we combine
some of the query words matching in
And the other sum now is
And these two parts
because these are the probabilities
This formula is very interesting
the match the query terms.
And just like in the vector space model,
of terms that are in the intersection of
So it already looks a little bit
In fact, there's even more similarity
[MUSIC]
This lecture is about Web Search.
In this lecture,
of the most important applications of
So let's first look at some
opportunities in web search.
Now, many informational
had been developed
So when the web was born,
those algorithms to major application
So naturally, there have to be some
search algorithms to address new
So here are some general challenges.
First, this is a scalability challenge.
How to handle the size of the web and
ensure completeness of
How to serve many users quickly and
And so that's one major challenge and
before the web was born the scale
The second problem is that there's
there are often spams.
The third challenge is
The new pages are constantly create and
so it makes it harder to
So these are some of the challenges
deal with high quality web searching.
On the other hand there are also some
leverage to include the search results.
There are many additional heuristics,
using links that we can
Now everything that we talked about
are general algorithms.
They can be applied to any search
On the other hand, they also don't take
of pages or documents in the specific
Web pages are linked with each other,
the linking is something
So, because of these challenges and
that have been developed for
One is parallel indexing and searching and
this is to address
In particular, Google's imaging of
has been very helpful in that aspect.
Second, there are techniques
addressing the problem of spams,
We'll have to prevent those spam
And there are also techniques
And we're going to use a lot
that it's not easy to spam the search
And the third line of techniques is link
analysis and these are techniques that can
allow us to improve such results
And in general in web searching,
ranking not just for link analysis.
But also exploring all kinds
anchor text that describes
So, here's a picture showing
Basically, this is the web on the left and
we're going to help this user to get
And the first component is a Crawler that
component is Indexer that would take
The third component there is a Retriever
answer user's query by talking
And then the search results will be given
show those results, it allows
So, we're going to talk about
First of all, we're going to talk about
software robot that would do something
To build a toy crawler is relatively easy,
because you just need to start
And then fetch pages from the web and
figure out new links.
And then add them to the priority que and
But to be able to real crawler
there are some complicated issues
For example robustness,
what if there's a trap that generates
that might attract your crawler to
to fetch dynamic generated pages?
The results of this issue
you don't want to overload one particular
you have to respect the robot
You also need to handle different
PDF files,
And you have to also
sometimes those are CGI scripts and
etc, and sometimes you have
they also create challenges.
And you ideally should also recognize
to duplicate those pages.
And finally, you may be interested
Those are URLs that may not be linked
the URL to a shorter path, you might
So what are the Major Crawling Strategies?
In general,
Breadth-First is most common because
You would not keep probing a particular
Also parallel crawling is very
easy to parallelize.
And there is some variations
and one interesting variation
In this case, we're going to crawl just
For example,
And this is typically going to
then you can use the query to get some
And then you can start it with those
The one channel in crawling,
is you will find the new
people probably are creating
And this is very challenging if
linked to any old pages.
If they are, then you can probably find
so these are also some interesting
And finally, we might face the scenario
repeated crawling, right.
Let's say,
and you first crawl a lot
But then,
in the future you just need
In general, you don't have to
It's not necessary.
So in this case, your goal is to
by using minimum resources
So, this is actually a very
and this is a open research question,
standard algorithms established yet
But in general, you can imagine,
So the two major factors that
first will this page
And do I have to quote this page again?
If the page is a static page and
you probably don't have to re-crawl it
will changed frequently.
On the other hand, if it's a sports score
you may need to re-crawl it and
The other factor to consider is,
If it is, then it means that
then thus it's more important to
Compared with another page that has
a year, then even though that
It's probably not that necessary to
not as urgent as to maintain the freshness
So to summarize, web search is one of
retrieval and there are some new
efficiency, quality information.
There are also new opportunities
layout, etc.
A crawler is an essential component
in general, you can find two scenarios.
One is initial crawling and
of the web if you are doing
focused crawling if you want to just
And then, there is another scenario that's
incremental crawling.
In this case,
try to use minimum resource
[MUSIC]
[SOUND].
This lecture is about
In this lecture, we're going to talk
of web search and intelligent information
In order to further improve
it's important that to consider
So one particular trend could be to
customized search engines, and they
These vertical search engines can be
the current general search engines
users are a special group of users that
and then the search engine can be
And because of the customization,
So the search can be personalized,
because we have a better
Because of the restrictions with domain,
in handling the documents, because we can
For example, particular words may
So we can bypass the problem of ambiguity.
Another trend we can expect to see,
is the search engine will
It's like a lifetime learning or
very attractive because that means the
As more people are using it, the search
this is already happening,
because the search engines can learn
More users use it, and the quality
the popular queries that are typed in by
so this is sort of another
The third trend might be
bottles of information access.
So search, navigation, and
combined to form a full-fledged
And in the beginning of this course,
These are different modes of information
And similarly, in the pull mode, querying
And in fact we're doing that basically,
We are querying, sometimes browsing,
Sometimes we've got some
Although most of the cases the information
But in the future, you can imagine
multi-mode for information access, and
Another trend is that we might see systems
that try to go beyond the searches
After all, the reason why people want
to make a decision or perform a task.
For example consumers might search for
opinions about products in
choose a good product by, so
support the whole workflow of purchasing
In this era, after the common search
For example, you can sometimes look at the
you can just click on the button to go the
But it does not provide a,
For example, for researchers,
you might want to find the realm in
And then, there's no, not much support for
So, in general, I think,
So in the following few slides, I'll
specific ideas or thoughts that hopefully,
can help you in imagining new
Some of them might be already relevant
In general, we can think about any
information system, as we specified
And so
then we'll able to specify
And I call this
So basically the three questions you
what kind of data are you are managing and
Right there, this would help us
And there are many different ways
how you connect them,
So let me give you some examples.
On the top,
On the left side, you can see different
on the bottom,
Now imagine you can connect
So, for example, you can connect
the support search and
Well, that's web search, right?
What if we connect UIUC employees with
documents to support the search and
If you connect the scientist
to provide all kinds of service,
alert of new random documents or
or provide the task with support or
For example, we might be,
automatically generating
a research paper, and
Right?
we can imagine this would
If we connect the online shoppers
then we can help these people
So we can provide, for example data mining
to compare products, compare sentiment of
decision support to have them
Or we can connect customer service
and, and we can imagine a system
of these emails to find that the major
We can imagine a system we
by automatically generating
Maybe intelligently attach
if appropriate, if they detect that that's
then you might take this opportunity
Whereas if it's a complaint,
automatically generate some
tell the customer that he or she can
All of these are trying to help
So this shows that
It's just only restricted
So this picture shows the trend
it characterizes the, intelligent
You can see in the center, there's
to search a bag of words representation.
That means the current search engines
to users and mostly model
and sees the data through
So it's a very simple approximation of
But that's what the current system does.
It connects these three nodes
it only provides a basic search function
and it doesn't really understand that
Now, I showed some trends to push each
So think about the user node here, right?
So we can go beyond the keyword queries,
and then further model the user
the user's task environment,
Okay, so this is pushing for
And this is a major
in order to build intelligent
On the document side,
go beyond bag of words implementation
This means we'll recognize people's names,
And this is already feasible with
And Google is the reason
If you haven't heard of it,
And once we can get to that level without
it can enable the search engine
In the future we would like to have
knowledge representation where we
then the search engine would
So this calls for
perhaps this is more feasible for
It's easier to make progress
Now on the service side,
we see we need to go beyond the search of
So search is only one way to get access
systems and push and pull so different
But going beyond access,
we also need to help people digest the
and this step has to do with analysis
We have to find patterns or
real knowledge that can
actionable knowledge that can be used for
And furthermore the knowledge
improve productivity in finishing a task,
Right, so this is a trend.
And, and, and so basically,
in the future intelligent information
interactive task support.
Now I should also emphasize interactive
the combined intelligence of the users and
So we, we can get some help
And we don't have to assume the system
user, and the machine can collaborate in
then the combined intelligence
we can minimize the user's overall
So this is the big picture of future
and this hopefully can provide
how to make further innovations
[MUSIC]
[SOUND]
So here are some specific examples of what
we can't do today and
part of speech tagging is still
So in the example, he turned off the
the two offs actually have somewhat
categories and also its very difficult
Again, the example, a man saw a boy
be very difficult to parse
Precise deep semantic
For example, to define the meaning of own,
precisely is very difficult in
So the state of the off can
Robust and
general NLP tends to be shallow while
For this reason in this course,
general, shallow techniques for
mining text data and they are generally
So there are robust and
the in category of shallow analysis.
So such techniques have
applied to any text data in
But the downside is that, they don't
For that, we have to rely on
That typically would require
a lot of examples of analysis that would
machine learning techniques and learn from
So in practical applications, we generally
with the general statistical and
These can be applied to any text data.
And on top of that, we're going to use
to use supervised machine learning
especially for those important
to analyze text data more precisely.
But this course will cover
that generally,
So they're practically,
analysis techniques that require a lot of
So to summarize,
is the foundation for text mining.
So obviously, the better we
the better we can do text mining.
Computers today are far from being able
Deep NLP requires common sense
Thus, only working for
large scale text mining.
Shallow NLP based on statistical
is the main topic of this course and
they are generally applicable
They are in some sense also,
In practice,
we'll have humans for
[MUSIC]
[SOUND]
In general, we can use the empirical count
of events in the observed data
And a commonly used technique is
where we simply normalize
So if we do that, we can see, we can
For estimating the probability that
we simply normalize the count of
So let's first take
On the right side, you see a list of some,
These are segments.
And in some segments you see both words
both columns.
In some other cases only one will occur,
the other column has zero.
And in all, of course, in some other
so they are both zeros.
And for estimating these probabilities, we
So the three counts are first,
And that's the total number of
It's just as the ones in the column of W1.
We can count how many
The segment count is for word 2, and we
And these will give us the total
The third count is when both words occur.
So this time, we're going to count
And then, so this would give us
where we have seen both W1 and W2.
Once we have these counts,
which is the total number of segments, and
this will give us the probabilities that
Now, there is a small problem,
And in this case, we don't want a zero
a small sample and in general, we would
a [INAUDIBLE] to avoid any context.
So, to address this problem,
And that's basically to add some
and so that we don't get
Now, the best way to understand smoothing
data than we actually have, because we'll
I illustrated on the top,
And these pseudo-segments would
of these words so
Now, in particular we introduce
Each is weighted at one quarter.
And these represent the four different
So now each event,
at least one count or at least a non-zero
So, in the actual segments
it's okay if we haven't observed
So more specifically, you can see
ones in the two pseudo-segments,
We add them up, we get 0.5.
And similar to this,
pseudo-segment that indicates
And of course in the denominator we add
we add, in this case,
Each is weighed at one quarter so
So, that's why in the denominator
So, this basically concludes
four syntagmatic relation discoveries.
Now, so to summarize,
be discovered by measuring correlations
We've introduced the three
Entropy, which measures the uncertainty
Conditional entropy, which measures
And mutual information of X and Y,
due to knowing Y, or
They are the same.
So these three concepts are actually very
That's why we spent some time
But in particular,
discovering syntagmatic relations.
In particular,
discovering such a relation.
It allows us to have values
words that are comparable and
discover the strongest syntagmatic
Now, note that there is some relation
[INAUDIBLE] relation discovery.
So we already discussed the possibility
terms in the context to potentially
that have syntagmatic relations
But here, once we use mutual information
we can also represent the context with
So this would give us
the context of a word, like a cat.
And if we do the same for all the words,
compare the similarity between these
So this provides yet
paradigmatic relation discovery.
And so to summarize this whole part
We introduce two basic associations,
a syntagmatic relations.
These are fairly general, they apply
the units don't have to be words,
We introduced multiple statistical
mainly showing that pure
are variable for
And they can be combined to
These approaches can be applied
mostly because they are based
they can actually discover
We can also use different ways with
this would lead us to some interesting
For example, the context can be very
a sentence, or maybe paragraphs,
allows to discover different flavors
And similarly,
visual information to discover
We also have to define the segment, and
text window or a longer text article.
And this would give us different
These discovery associations can
in both information retrieval and
So here are some recommended readings,
The first is a book with
which is quite relevant to
The second is an article
statistical measures to
Those are phrases that
For example,
blue chip is not a chip that's blue.
And the paper has a discussion about some
The third one is a new paper on a unified
relations and a syntagmatical relations,
[SOUND]
[SOUND]
This lecture is about the
We're going to give
In the last lecture, we talked about
a retrieval model, which would give
In this lecture, we're going to
designing a ramping function called
And we're going to give a brief
Vector space model is a special case of
similarity based models
Which means we assume relevance
between the document and the query.
Now whether is this assumption
But in order to solve the search problem,
we have to convert the vague notion
definition that can be implemented
So in this process,
This is the first assumption
Basically, we assume that if a document
another document.
Then the first document will be assumed it
And this is the basis for
Again, it's questionable whether this is
As we will see later there
The basic idea of vectors for
base retrieval model is actually
Imagine a high dimensional space where
So here I issue a three dimensional
programming, library and presidential.
So each term here defines one dimension.
Now we can consider vectors in this,
And we're going to assume
the query will be placed
So for example, on document might
Now this means this document
presidential, but
What does this mean in terms
That just means we're going to look at
this vector.
We're going to ignore everything else.
Basically, what we see here is only
Of course,
For example, the orders of
that's because we assume that
So with this presentation
d1 simply suggests a [INAUDIBLE] library.
Now this is different from another
a different vector, d2 here.
Now in this case, the document that
it doesn't talk about presidential.
So what does this remind you?
Well you can probably guess the topic
the library is software lab library.
So this shows that by using
we can actually capture the differences
Now you can also imagine
For example,
that might be a presidential program.
And in fact we can place all
And they will be pointing
And similarly,
we're going to place our query also
And then we're going to measure the
every document vector.
So in this case for example,
we can easily see d2 seems to be
And therefore,
So this is basically the main
So to be more precise,
vector space model is a framework.
In this framework,
First, we represent a document and
So here a term can be any basic concept.
For example, a word or a phrase or
Those are just sequence of
Each term is assumed that will
Therefore n terms in our vocabulary,
A query vector would consist
corresponding to the weights
Each document vector is also similar.
It has a number of elements and
indicating the weight of
Here, you can see,
Therefore, they are N elements
each corresponding to the weight
So the relevance in this case
will be assumed to be the similarity
Therefore, our ranking function
between the query vector and
Now if I ask you to write a program
in a search engine.
You would realize that
We haven't said a lot of things in detail,
therefore it's impossible to actually
That's why I said, this is a framework.
And this has to be refined
suggest a particular ranking function
So what does this framework not say?
Well, it actually hasn't said many things
that would be required in order
First, it did not say how we should define
We clearly assume
Otherwise, there will be redundancy.
For example, if two synonyms or somehow
Then they would be defining
that would clearly cause redundancy here.
Or all the emphasizing of
because it would be as if
when you actually matched
Secondly, it did not say how we
the query in this space.
Basically that show you some examples
But where exactly should the vector for
So this is equivalent to how
How do you compute the lose
This is a very important question,
because term weight in the query vector
So depending on how you assign the weight,
you might prefer some terms
Similarly, the total word in
It indicates how well the term
If you got it wrong then you clearly
Finally, how to define the similarity
So these questions must be addressed
function that we can actually
So how do we solve these problems
is the main topic of the next lecture.
[MUSIC]
[SOUND]
This lecture is about the inverted index
construction.
In this lecture, we will continue
In particular, we're going to discuss
The construction of the inverted index
very small.
It's very easy to construct a dictionary
The problem is that when our data
then we have to use some
And unfortunately, in most retrieval
And they generally cannot be
And there are many approaches to
method is quite common and
First, you collect the local termID,
Basically you will locate the terms
And then once you collect those accounts
So that you will be able to local
these are called rounds.
And then you write them into
then you merge in step 3.
Do pairwise merging of these runs, until
generate a single inverted index.
So this is an illustration of this method.
On the left you see some documents and
on the right we have a term lexicon and
These lexicons are to map string-based
terms into integer representations or
map back from integers to
The reason why we want our interest
IDs is because integers
For example,
array, and they are also easy to compress.
So this is one reason why we tend
so that we don't have to
So how does this approach work?
Well, it's very simple.
We're going to scan these
then parse the documents and
And in this stage we generally sort
because we process each
So we'll first encounter all
Therefore the document IDs
And this will be followed by document IDs
only just because we process
At some point,
that would have to write
Before we do that we 're going to sort
We can sort them and then this time
Note that here,
So all the entries that share the same
In this case,
that match term 1 would
And we're going to write this into
And would that allows you to
makes a batch of documents.
And we're going to do that for
So we're going to write a lot of
And then the next stage is
We're going to merge them and
Eventually, we will get
where the entries are sorted
And on the top, we're going to see
the documents that match term ID 1.
So this is basically, how we can do
Even though the data cannot be
Now, we mention earlier that
it's desirable to compress them.
So let's now take a little bit
Well the idea of compression in general,
leverage skewed distributions of values.
And we generally have to use
instead of the fixed-length
a program manager like C++.
And so how can we leverage
to compress these values?
Well in general, we will use few
words at the cost of using longer
So in our case, let's think about how
Now, if you can picture what
you will see in post things,
Those are the frequencies of
Now, if you think about it, what kind
You probably will be able to guess
far more frequently than large numbers.
Why?
Well, think about the distribution of
and many words occur just rarely so
Therefore, we can use fewer bits for
highly frequent integers and
that's cost of using more bits for
This is a trade off of course.
If the values are distributed to uniform,
but because we tend to see many small
We can save on average even though
we have to use a lot of bits.
What about the document IDs
Well they are not distributed
So how can we deal with that?
Well it turns out that we can
that is to store the difference
And we can imagine if a term has
there will be longest of document IDs.
So when we take the gap, and we take the
those gaps will be small.
So again, see a lot of small numbers.
Whereas if a term occurred
then the gap would be large,
So this creates some skewed distribution,
that would allow us to
This is also possible because
uncompress these document IDs,
Because we stored the difference and
document ID we have to first
And then we can add the difference to
the current document ID.
Now this was possible because we only
those document IDs.
Once we look up the term, we look up all
then we sequentially process them.
So it's very natural,
And there are many different methods for
So binary code is a commonly used
We use basically fixed glance in coding.
Unary code, gamma code, and
there are many other possibilities.
So let's look at some
Binary coding is really
that's a property for
The unary coding is a variable
In this case, integer this 1 will be
encoded as x -1, 1 bit followed by 0.
So for example, 3 will be encoded as 2,
whereas 5 will be encoded as 4,
So now you can imagine how many bits do we
So how many bits do you have to
Well exactly, we have to use 100 bits.
So it's the same number of bits
So this is very inefficient if you
Imagine if you occasionally see a number
So this only works well if you
no large numbers, mostly very
Now, how do you decode this code?
Now since these are variable
you can't just count how many bits and
You can't say 8-bits or 32-bits,
They are variable length, so
In this case for unary, you can see
Now you can easily see 0 would
So you just count up how many 1s you
You have finished one number,
Now we just saw that unary
In rewarding small numbers, and
if you occasionally can see a very
So what about some other
Well gamma coding's one of them and
in this method we can use unary coding for
a transform form of that.
So it's 1 plus the floor of log of x.
So the magnitude of this value is
So that's why we can afford
And so first I have the unary code for
And this would be followed by
And this basically the same uniform code,
And we're going to use this coder to code
And this is basically precisely
So the unary code are basically
well add one there and here.
But the remaining part
code through actually code the difference
between the x and this 2 to the log of x.
And it's easy to show that for this
difference we only need to use up
to this many bits and
And this is easy to understand,
if the difference is too large, then we
So here are some examples for
The first two digits are the unary code.
So this isn't for the value 2,
10 encodes 2 in unary coding.
And so that means the floor of
log of x is 1,
In code 1 plus the flow of log of x,
since this is two then we know that
So that 3 is still larger than 2 to the 1.
So the difference is 1, and
So that's why we have 101 for 3.
Now similarly 5 is encoded as 110,
And in this case the unary code in code 3.
And so this is a unary code 110 and
And that means we're going to
the 2 to the 2 and that's 1.
And so we now have again 1 at the end.
But this time we're going to use 2 bits,
because with this level
We could have more numbers a 5, 6, 7 they
So in order to differentiate them,
we have to use 2 bits in
So you can imagine 6 would be 10 here
It's also true that the form of
odd number of bits, and
That's the end of the unary code.
And before that or on the left side
And on the right side of this 0,
So how can you decode such code?
Well you again first do unary coding.
Once you hit 0, you have got the unary
many bits you have to read further
So this is how you can
There is also a delta code that's
that you replace the unary
So that's even less
in terms of wording the small integers.
So that means, it's okay if you
It's okay with delta code.
It's also fine with the gamma code,
And they are all operating of course,
at different degrees of favoring short or
And that also means they would be
But none of them is perfect for
And which method works the best would
in your dataset.
For inverted index compression,
people have found that gamma
So how to uncompress inverted index?
I will just talk about this.
Firstly, you decode
And we just I think discussed the how we
What about the document IDs that
Well, we're going to do
supposed the encoded I list is x1,
We first decode x1 to obtain
Then we can decode x2,
which is actually the difference between
So we have to add the decoder
the value of the ID at
So this is where you can
converting document IDs to integers.
And that allows us to do
And we just repeat until we
Every time we use the document ID in
the document ID in the next position.
[MUSIC]
[MUSIC]
This lecture is about how to evaluate
multiple levels of judgements.
In this lecture, we will continue
We're going to look at how to
when we have multiple
So far we have talked about
that means a document is judged as
But earlier, we also talk about
So we often can distinguish
those are very useful documents,
They are okay, they are useful perhaps.
And further from now, we're adding
So imagine you can have ratings for
Then, you would have
For example, here I show example of three
very relevant, 2 for marginally relevant,
Now, how do we evaluate the search
Obvious that the map doesn't work, average
recall doesn't work,
So let's look at some top ranked
Imagine the user would be mostly
And we marked the rating levels,
for these documents as shown here,
And we call these gain.
And the reason why we call it
that we are infusing is called the NDCG
So this gain, basically,
information a user can obtain by
So looking at the first document,
Looking at the non-relevant document
Looking at the moderator or
document the user would get 2 points,
So, this gain to each of the measures is
perspective.
Of course, if we assume the user
we're looking at the cutoff at 10,
And what's that?
Well, that's simply the sum of these,
So if the user stops after the position 1,
If the user looks at another document,
If the user looks at the more documents,
Of course this is at the cost of
So cumulative gain gives
much total gain the user would have if
Now, in NDCG, we also have another letter
So, why do we want to do discounting?
Well, if you look at this cumulative gain,
which is it did not consider the rank
So for example, looking at this sum here,
and we only know there is 1
1 marginally relevant document,
We don't really care
Ideally, we want these two to be ranked
But how can we capture that intuition?
Well we have to say, well this is 3 here
And that means the contribution
positions has to be
And this is the idea of discounting,
So we're going to to say, well, the first
because the user can be assumed
But the second one,
because there's a small possibility
So we divide this gain by
So log of 2,
And when we go to the third position,
because the normalizer is log of 3,
So when we take such a sum that a lower
that much as a highly ranked document.
So that means if you, for example,
this position, and this one, and then
for example very relevant
Imagine if you put the 3 here,
So it's not as good as if
So this is the idea of discounting.
Okay, so now at this point that we have
measuring the utility of this ranked
So are we happy with this?
Well, we can use this to rank systems.
Now, we still need to do a little bit more
in order to make this measure
And this is the last step, and by the way,
so this is the total sum of DCG,
So the last step is called N,
And if we do that,
So how do we do that?
Well, the idea here is we're
the ideal DCG at the same cutoff.
What is the ideal DCG?
Well, this is the DCG of an ideal ranking.
So imagine if we have 9 documents in
And that means in total we
Then our ideal rank lister would have put
So all these would have to be 3 and
Because that's the best we could
But all these positions would be 3.
Right?
So this would our ideal ranked list.
And then we had computed the DCG for
So this would be given by this
And so this ideal DCG would then
So here.
And this idea of DCG would
So you can imagine now,
compare the actual DCG with the best DCG
Now why do we want to do this?
Well, by doing this we'll map the DCG
So the best value, or the highest value,
That's when your rank list is,
otherwise, in general,
Now, what if we don't do that?
Well, you can see, this transformation,
doesn't really affect the relative
just one topic, because this ideal
so the ranking of systems based on
if you rank them based
The difference however is
Because if we don't do normalization,
different topics will have
For a topic like this one,
the DCG can get really high,
there are only two very relevant documents
Then the highest DCG that
such a topic would not be very high.
So again, we face the problem of
When we take an average,
we don't want the average to be
Those are, again, easy queries.
So, by doing the normalization,
making all the queries contribute
So, this is a idea of NDCG, it's used for
measuring a rank list based on multiple
In a more general way this
that can be applied to any ranked task
And the scale of the judgements
binary not only more than binary they
0, 5 or
And the main idea of this measure,
is to measure the total utility
So you always choose a cutoff and
And it would discount the contribution
And then finally,
it would do normalization to ensure
comparability across queries.
[MUSIC]
[SOUND]
So I showed you how we rewrite the query
like holder which is a function into
of this slide after if we make
the language model based on
Now if you look at this rewriting,
The first benefit is it helps us better
In particular, we're going to show that
with the collection language model would
and length normalization.
The second benefit is that
the query like holder more efficiently.
In particular we see that
is a sum over the match
So this is much better than if we
After we smooth the document the damage
for all the words.
So this new form of the formula is
It's also interesting to note that
the last term here is actually
Since our goal is to
the same query we can ignore this term for
Because it's going to be the same for
Ignoring it wouldn't affect
Inside the sum, we
also see that each matched query
And this weight actually
is very interesting because it
First we can already see it has
just like in the vector space model.
When we take a thought product,
we see the word frequency in
And so naturally this part would
element from the documented vector.
And here indeed we can see it actually
encodes a weight that has similar
I'll let you examine it, can you see it?
Can you see which part is capturing TF?
And which part is
So if want you can pause
So have you noticed that this P sub
in the sense that if a word occurs
then the s made through probability
So this means this term is really
Now have you also noticed that
is actually achieving the factor of IDF?
Why, because this is the popularity
But it's in the denominator, so if the
then the weight is actually smaller.
And this means a popular term.
We actually have a smaller weight and this
Only that we now have
Remember IDF has a logarithm
But here we have something different.
But intuitively it
Interestingly, we also have something
Again, can you see which factor is related
What I just say is that this term
This collection probability,
this term here is actually related
In particular, F of sub d might
So it encodes how much probability
How much smoothing do we want to do?
Intuitively, if a document is long,
then we need to do less smoothing because
We probably have observed all the words
But if the document is short then r of
We need to do more smoothing.
It's likey there are words that have
So this term appears to paralyze
other sub D would tend to be longer
But note that alpha sub d
this may not actually be necessary
The effect is not so clear yet.
But as we will see later, when we
it turns out that they do
Just like in TF-IDF weighting and
document length normalization
So, that's a very interesting
we don't even have to think about
We just need to assume that if we smooth
then we would have a formula that
documents length violation.
What's also interesting that we have
And see we have not heuristically
In fact, you can think about why
You look at the assumptions that
it's because we have used a logarithm
And we turned the product into a sum
that's why we have this logarithm.
Note that if only want to heuristically
IDF weighting, we don't necessary
Imagine if we drop this logarithm,
But what's nice with problem risk modeling
the logarithm function here.
And that's basically a fixed form
really have to heuristically design,
the logarithm the model probably won't
So a nice property of problem risk
assumptions and the probability rules
And the formula would have
And if we heuristically design
end up having such a specific formula.
So to summarize, we talked about the need
Otherwise it would give zero probability
that's not good for
It's also necessary, in general,
the model represent
The general idea of smoothing in retrieval
to give us some clue about which unseen
That is, the probability of an unseen
to its probability in the collection.
With this assumption, we've shown that we
query likelihood that has
document length normalization.
We also see that, through some rewriting,
the scoring of such a ranking function
matched query terms,
But, the actual ranking
automatically by the probability rules and
And like in the vector space model
think about the form of the function.
However, we still need to address
smooth the document and the model.
How exactly we should
model based on the connection
the maximum micro is made of and
[MUSIC]
[SOUND]
This lecture is about the Web Indexing.
In this lecture, we will continue
we're going to talk about how
So once we crawl the web,
The next step is to use the indexer
In general, we can use the same
creating an index and that is what we
but there are there are new
For web scale indexing, and the two main
The index would be so large,
that it cannot actually fit into
So we have to store the data
Also, because the data is so
process the data in parallel, so
Now to address these challenges,
One is the Google File System that's
programmers manage files stored
The second is MapReduce.
This is a general software framework for
Hadoop is the most well known open
Now used in many applications.
So, this is the architecture
It uses a very simple centralized
management mechanism to manage
Files, so
look up a table to know where
The application client will then
that obtains specific locations of
And once the GFS file kind obtained
then the application client can talk
data actually sits directly, so
In the network.
So when this file system stores
with great fixed sizes of chunks, so
Many chunks.
Each chunk is 64 MB, so it's pretty big.
And that's appropriate for
These chunks are replicated
So this is something that the programmer
and it's all taken care
So from the application perspective,
the programmer would see this
And the programmer doesn't have to
can just invoke high level.
Operators to process the file.
And another feature is that the data
and chunk servers.
So it's efficient in this sense.
On top of the Google file system, Google
also proposed MapReduce as a general
Now, this is very useful to support
And so, this framework is,
Hiding a lot of low-level
As a result, the programmer can make
that can be run a large
So some of the low level details
the specific and network communications or
where the task are executed.
All these details are hidden
There is also a nice feature which
If one server is broken,
the server is down, and
Then the MapReduce mapper will know
So it automatically dispatches a task
And therefore, again the program
here's how MapReduce works.
The input data would be separated
Now what exactly is in the value
it's actually a fairly general framework
into different parts and each part
Each key value pair would be and
The program was right the map function,
And then the map function will
then generate a number of
Of course, the new key is usually
that's given to the map as input.
And these key value pairs
all the outputs of all the map
and then there will be for
And the result is that,
with the same key will be
So now we've got a pair of of a key and
So this would then be sent
Now, of course, each reduce function
so we will send these output values to
multiple reduce functions
A reduce function would then
a key in a set of values to produce
So these output values would
to form the final output.
And so, this is the general
Now the programmer only needs to write
Everything else is actually taken
So you can see the program really
And with such a framework, the input data
which is processing parallel first by map,
then being the process after
The much more reduced if I'm
the different keys and
So it achieves some,
it achieves the purpose of parallel
So let's take a look at a simple example.
And that's Word Counting.
The input is containing words,
and the output that we want to generate is
So it's the Word Count.
We know this kind of counting
assess the popularity of a word in
achieving a factor of IDF wading for
So how can we solve this problem?
Well, one natural thought is that,
done in parallel by simply counting
and then in the end we just
And that's precisely the idea of
We can parallelize on
So more specifically, we can assume
a key value pair that represents the line
So the first line, for
that is another word by word and
So this key value pair would
The Map Function then would just
And in this case,
Each world gets a count of one and
these are the output that you see here
So the map function is really
what the pseudocode looks
you see it simply needs to iterate
And then just collect the function
which means it would then send the word
The collector would then try to
different Map Functions, right?
So the function is very simple and
this function as a way to
Of course, the second line will be
which we will produce a single output.
Okay, now the output from the map
send it to a collector and the collector
So at this stage, you can see,
Each pair is a word and
So, once we see all these pairs.
Then we can sort them based on the key,
So we will collect all the counts
And similarly, we do that for other words.
Like Hadoop, Hello, etc.
So each word now is attached to
And these counts represent the occurrences
So now we have got a new pair of a key and
this pair will then be fed into reduce
would have to finish the job of counting
Now, it has all ready got all
all it needs to do is
So the reduce function here
You have a counter, and
That you'll see in this array.
And that,
And then finally, you output the P and
And that's precisely what we want as
So you can see,
To building an Invert index.
And if you think about it,
And we have already got a dictionary,
We have got the count.
But what's missing is
frequency counts of words
So we can modify this slightly to
here's one way to do that.
So in this case, we can assume the input
which denotes the document ID,
denoting the screen for that document,
And so, the map function would do
seen in the word campaign example.
It simply groups all the counts of
And it would then generate
Each key is a word, and
the value is the count of this word in
Now, you can easily see why we need to
in inverted index, we would like to
should keep track of it, and this can then
Now similarly another document D2
So in the end, again, there is a sorting
And then we will have just a key,
associated with all the documents
Or all the documents where java occurred.
And the counts, so
And this will be collected together.
And this will be, so
So now you can see the reduce function
an inverted index entry.
So it's just the word and all
the frequencies of the word
So all you need to do is
into a continuous chunk of data.
And this can be done
So basically the reduce function
Work.
And so, this is a pseudo-code for
[INAUDIBLE] that's construction.
Here we see two functions,
And a programmer would specify these two
And you can see basically they
In the case of map, it's going to count
the occurrences of a word
And it would output all the counts
So, this is the reduce function,
simply concatenates all the input
and then put them together as
So this is a very simple
it would allow us to construct an inverted
the data can be processed
And program doesn't have to
So this is how we can do parallel
So to summarize,
web scale indexing requires some
Standard traditional indexing techniques.
Mainly, we have to store
And this is usually done by using a filing
But this should be through a file system.
And secondly, it requires creating
large and takes long time to create
So if we can do it in parallel,
this is done by using
Note that both the GFS and
they can also support
[MUSIC]
[MUSIC]
This lecture is about
So far we have talked about a lot
We have talked about the problem
different methods for ranking,
how to evaluate a search engine, etc.
This is important because we know
the most important applications
And they are the most useful tools
data into a small set
Another reason why we spend so
is because many techniques used in search
Recommender Systems,
And so, overall, the two systems
And there are many techniques
So this is a slide that
when we talked about the two
Pull and the Push.
And we mentioned that recommender
users in the Push Mode, where the systems
the information to the user or
And this often works
stable information need
So a Recommender System is sometimes
it's because recommending useful
filtering out the the useless articles,
and so
And in all the cases the system
usually there's a dynamic source
that you have some knowledge
And then the system would make a decision
about whether this item is
then if it's interesting then the system
So the basic filtering question here is
Will U like item X?
And there are two ways to answer this
And one is look at what items U likes and
then we can see if X is
The other is to look at who likes X,
user looks like a one of those users,
And these strategies can be combined.
If we follow the first strategy and
look at item similarity in the case
then we're talking about a content-based
If we look at the second strategy, then,
we're user similarity and the technique
So, let's first look at
This is what the system would look like.
Inside the system, there will be
knowledge about the user's interests, and
It maintains this profile to keep
then there is a utility function
a nice plan utility
It helps the system decide
And then the accepted documents will
according to the classified.
There should be also an initialization
maybe from a user's specified keywords or
etc., and this would be to feed into
There is also typically a learning
users' feedback over time.
Now note that in this case typical
the system would have a lot more
If the user has taken a recommended item,
this a signal to indicate that
If the user discarded it,
And so such feedback can be a long term
And the system can collect a lot of
this then can then be used
Now what's the criteria for
How do we know this filtering
Now in this case we cannot use the ranking
because we can't afford waiting for
then rank the documents to
And so the system must make
to decide whether the item is
So in other words, we're trying
So in this case,
one common user strategy is to use
So here, I show linear utility function.
That's defined as for example three
you delivered, minus two multiplied by the
So in other words, we could kind of just
treat this as almost in a gambling game.
If you delete one good item,
you gain three dollars but if you deliver
And this utility function
how much money you are get by
And so it's clear that if you want
this strategy should be delivered
and minimize the delivery of bad articles.
That's obvious, right?
Now one interesting question here is
I just showed a three and
But one can ask the question,
So what do you think?
Do you think that's a reasonable choice?
What about the other choices?
So for example, we can have 10 and
What's the difference?
What do you think?
How would this utility function affect
Right, you can think of
(10, -1) + (1, -10), which one do
system to over do it and which one would
If you think about it you will see that
our good document you incur only a small
Intuitively, you would be
And you can try to deliver more in
And then we'll get a big reward.
So on the other hand,
you really don't get such a big prize
On the other hand, you will have
You can imagine that,
the system would be very reluctant
It has to be absolutely
So this utility function has to be
The three basic problems in content-based
first, it has to make
So it has to be a binary decision maker,
Given a text document and
it has to say yes or no, whether this
So that's a decision module, and
module as you have seen earlier and
And we have to initialize the system
text exclusion or
And the third model is
has to be able to learn from limited
counted them from the user about their
If we don't deliver document
be able to know whether
And we had accumulate a lot of documents
All these modules will have to be
So how can we deal with such a system?
And there are many different approaches.
Here we're going to talk about
a search engine for information filtering.
Again, here's why we've spent a lot of
Because it's actually not very hard
information filtering.
So here's the basic idea for
information filtering.
First, we can reuse a lot of
Right, so we know how to score
We're going to match the similarity
a document.
And then we can use a score threshold for
We do retrieval and then we kind of find
apply a threshold to see whether the
And if it's passing the threshold,
we're going to say it's relevant and
Another component that we have to add is,
we had used is the traditional feedback
And we know rock hill can be using for
And, but we have to develop a new
And we need to set it initially and
then we have to learn how to
So here's what the system
generalize the vector-space model for
So you can see the document vector could
already exists in a search engine
And the profile will be treated
the profile vector can be matched with
And then this score would be fed into a
no, and then the evaluation would be based
If it says yes and then the document
And then user could give some feedback.
The feedback information would be
to adjust the vector representation.
So the vector learning is essentially
feedback in the case of search.
The threshold of learning
that we need to talk
[MUSIC]
This lecture is about the
In this lecture, we are going
to discuss textual
and discuss how natural
allow us to represent text
Let's take a look at this
We can represent this sentence
First, we can always
represent such a sentence
This is true for
when we store them
When we store a natural
as a string of characters,
we have perhaps the most general
since we always use
this approach to
But unfortunately, using
help us to do semantic analysis,
which is often needed
for many applications
The reason is because we're
So as a string,
we're going to keep
and these ASCII symbols.
We can perhaps count what's
the most frequent character
or the correlation
but we can't really
Yet, this is the most
text because we can use
this to represent any
If we try to do
a little bit more natural
by doing word segmentation,
then we can obtain a
but in the form of a
So here we see that
words like a dog is chasing etc.
Now with this level
we certainly can do
and this is mainly because
of human communication
so they are very powerful.
By identifying words, we can for
example easily count what are
the most frequent words in
this document or in
These words can be used to form
topics when we combine
and some words are positive,
some words negative, so we can
also do sentiment analysis.
So representing text data
opens up a lot of interesting
However, this level of
representation is slightly
of characters because in
it's actually not
all the word boundaries
you see text as a sequence of
characters with
So you'll have to rely on
some special techniques
In such a language,
we might make mistakes
So the sequence of
not as robust as
But in English, it's very
easy to obtain this level
so we can do that all the time.
Now, if we go further
to do naturally
we can add a part of speech tags.
Now once we do that,
we can count, for example,
the most frequent
nouns are associated with
So this opens up
a little bit more
for further analysis.
Note that I use a plus sign
representing text as a sequence
we don't necessarily replace
the original word
Instead, we add this as
an additional way of
so that now the data is
of words and a sequence
This enriches the
and thus also enables
If we go further, then we'll
often to obtain
Now this of course,
further open up
of, for example,
the writing styles or
If we go further for
then we might be able to
and we also can recognize
and playground as a location.
We can further analyze
dog is chasing the boy and
Now this will add
relations through
At this level,
then we can do even more
For example, now we
the most frequent person that's
mentioning this whole collection
or whenever you
you also tend to see mentioning
So this is a very
and it's also related to
the knowledge graph that
of that Google is doing as
a more semantic way of
However, it's also less robust
even syntactical analysis
always easy to identify
all the entities with
and we might make mistakes,
and relations are
and we might make mistakes.
So this makes this level of
yet it's very useful.
Now if we move further
then we can have predicates
With inference rules, we can
infer interesting derived
so that's very useful.
But unfortunately,
representation is even less
robust and we can make
mistakes and we can't do
that all the time for
Finally, speech acts would
of repetition of the intent
So in this case,
it might be a request.
So knowing that would
even more interesting
this observer or the author
What's the intention
What's scenarios? What kind
So this is another level
of analysis that would
So this picture shows
we generally see
natural language processing
Unfortunately,
require more human effort,
and they are less accurate.
That means there are mistakes.
So if we add an texts that are at
the levels that are
representing deeper
then we have to
So that also means it's
such deep analysis with
for example, sequence of words.
On the right side,
you'll see the arrow points
As we go down,
we are representation
to knowledge representation
and need for solving
Now this is desirable because as
we can represent text at
we can easily extract
That's the purpose
So there is a trade-off
here between doing
might have errors but would give
us direct knowledge that
Doing shallow analysis, which
is more robust but
give us the necessary deeper
I should also say that
humans and are meant to
So as a result, in
text-mining humans play
they are always in the loop.
Meaning that we should optimize
the collaboration of
So in that sense,
it's okay that computers
to have compute accurately
and the patterns
from text data can be
and humans can
to do more accurate analysis
by providing features
learning programs to make
[SOUND]
lecture is about topic mining and
We're going to talk about its
In this lecture we're going to talk
As you see on this road map,
mining knowledge about language,
word associations such as paradigmatic and
Now, starting from this lecture, we're
knowledge, which is content mining, and
trying to discover knowledge about
And we call that topic mining and
In this lecture, we're going to talk about
So first of all,
So topic is something that we
it's actually not that
Roughly speaking, topic is the main
And you can think of this as a theme or
It can also have different granularities.
For example,
A topic of article,
the topic of all the research articles
so different grand narratives of topics
Indeed, there are many applications that
they're analyzed then.
Here are some examples.
For example, we might be interested
users are talking about today?
Are they talking about NBA sports, or
are they talking about some
Or we are interested in
For example, one might be interested in
topics in data mining, and how are they
Now this involves discovery of topics
also we want to discover topics in
And then we can make a comparison.
We might also be also interested in
some products like the iPhone 6,
And this involves discovering
iPhone 6 and
Or perhaps we're interested in knowing
presidential election?
And all these have to do with discovering
and we're going to talk about a lot
In general we can view a topic as
So from text data we expect to
then these topics generally provide
And it tells us something about the world.
About a product, about a person etc.
Now when we have some non-text data,
then we can have more context for
For example, we might know the time
locations where the text
or the authors of the text, or
All such meta data, or
context variables can be associated
then we can use these context variables
For example, looking at topics over time,
whether there's a trending topic, or
Soon you are looking at topics
We might know some insights about
So that's why mining
Now, let's look at the tasks
In general, it would involve first
k topics.
And then we also would like to know, which
to what extent.
So for example, in document one, we
Topic 2 and
And other topics,
Document two, on the other hand,
but it did not cover Topic 1 at all, and
it also covers Topic k to some extent,
So now you can see there
sub-tasks, the first is to discover k
What are these k topics?
Okay, major topics in the text they are.
The second task is to figure out
to what extent.
So more formally,
First, we have, as input,
Here we can denote the text
denote text article as d i.
And, we generally also need to have
But there may be techniques that can
But in the techniques that we will
techniques, we often need to
Now the output would then be the k
in order as theta sub
Also we want to generate the coverage of
this is denoted by pi sub i j.
And pi sub ij is the probability
covering topic theta sub j.
So obviously for each document, we have
what extent the document covers,
And we can assume that these
Because a document won't be able to cover
other topics outside of the topics
So now, the question is, how do we define
Now this problem has not
until we define what is exactly theta.
So in the next few lectures,
we're going to talk about
[MUSIC]
In this lecture we're going to talk
about how to instantiate
that we can get very
So this is to continue the discussion
which is one particular approach
And we're going to talk about how
the the vector space
instantiate the framework to derive
And we're going to cover the symbolist
So as we discussed in
the vector space model
And this didn't say.
As we discussed in the previous lecture,
It does not say many things.
So, for example,
here it shows that it did not say
It also did not say how we place
It did not say how we place a query
And, finally, it did not say how we
the query vector and the document vector.
So you can imagine,
we have to say specifically
What is exactly xi?
And what is exactly yi?
This will determine where
where we place a query vector.
And, of course,
we also need to say exactly what
So if we can provide a definition
the dimensions and these xi's or
queries and document, then we will be
query vectors in this well defined space.
And then,
then we'll have a well
So let's see how we can do that and
Actually, I would suggest you to
spend a couple minutes to think about.
Suppose you are asked
You have come up with the idea of vector
out how to compute these vectors exactly,
What would you do?
So, think for a couple of minutes,
So, let's think about some simplest ways
First, how do we define the dimension?
Well, the obvious choice is to use
each word in our vocabulary
And show that there are N
Therefore, there are N dimensions.
Each word defines one dimension.
And this is basically
Now let's look at how we
Again here, the simplest strategy is to
use a Bit Vector to represent
And that means each element, xi and
yi will be taking a value
When it's 1,
it means the corresponding word is
When it's 0,
So you can imagine if the user
then the query vector will only
The document vector,
But it will also have many zeros since
Many words don't really
Many words will only occasionally
A lot of words will be absent
So now we have placed the documents and
Let's look at how we
So, a commonly used similarity
The Dot Product of two
the sum of the products of the
So, here we see that it's
So, here.
And then, x2 multiplied by y2.
And then, finally, xn multiplied by yn.
And then, we take a sum here.
So that's a Dot Product.
Now, we can represent this in a more
So this is only one of the many different
So, now we see that we have
we have defined the vectors, and we have
So now we finally have the simplest
on the bit vector [INAUDIBLE] dot product
And the formula looks like this.
So this is our formula.
And that's actually a particular retrieval
Now we can finally implement this
and then rank the documents for query.
Now, at this point you should
to think about how we can
So, we have gone through the process
using a vector space model.
And then,
vectors in the vector space, and
So in the end, we've got a specific
Now, the next step is to think about
actually makes sense, right?
Can we expect this function
when we used it to rank documents for
So it's worth thinking about what is
So, in the end, we'll get a number.
But what does this number mean?
Is it meaningful?
So, spend a couple minutes
And, of course,
the general question here is do you
Would it actually work well?
So, again,
Is it actually meaningful?
Does it mean something?
This is related to how well
So, in order to assess
vector space model actually works well,
So, here I show some sample documents and
The query is news about
And we have five documents here.
They cover different terms in the query.
And if you look at these documents for
some documents are probably relevant, and
Now, if I asked you to rank these
This is basically our ideal ranking.
When humans can examine the documents,
Now, so think for a moment,
And perhaps by pausing the lecture.
So I think most of you would
better than others because they
They match news,
So, it looks like these documents
They should be ranked on top.
And the other three d2, d1, and
So we can also say d4 and
d1, d2 and d5 are non-relevant.
So now let's see if our simplest
or could do something closer.
So, let's first think about
to score documents.
All right.
Here I show two documents, d1 and d3.
And we have the query also here.
In the vector space model, of course we
these documents and the query.
Now, I showed the vocabulary here as well.
So these are the end dimensions
So what do you think is the vector for
Note that we're assuming
to indicate whether a term is absent or
So these are zero,1 bit vectors.
So what do you think is the query vector?
Well, the query has four words here.
So for these four words,
And for the rest, there will be zeros.
Now, what about the documents?
It's the same.
So d1 has two rows, news and about.
So, there are two 1's here,
Similarly, so now that we
have the two vectors,
And we're going to use Do Product.
So you can see when we use Dot Product,
we just multiply the corresponding
So these two will be formal product,
and these two will
and these two will generate yet
Now you can easily see if we do that,
these zeroes because whenever we have
So when we take a sum
then the zero entries will be gone.
As long as you have one zero,
So, in the fact, we're just
In this case, we have seen two,
So what does that mean?
Well, that means this number, or
is simply the count of how many unique
Because if a term is matched in the
If it's not, then there will
Similarly, if the document has a term but
there will be a zero in the query vector.
So those don't count.
So, as a result,
measures how many unique query
This is how we interpret this score.
Now, we can also take a look at d3.
In this case, you can see the result
distinctive query words news, presidential
Now in this case, this seems
And this simplest vector
So that looks pretty good.
However, if we examine this model in
So, here I'm going to show all
And you can easily verify they're
counting the number of unique query
Now note that this measure
It basically means if a document
then the document will be
And that seems to make sense.
The only problem is here we can note that
And they tied with a 3 as a score.
So, that's a problem because if you look
should be ranked above d3 because
d3 only mentions the presidential once,
In the case of d3,
But d4 is clearly above
Another problem is that d2 and
But if you look at the three words
it matched the news, about and campaign.
But in the case of d3, it matched news,
So intuitively this reads better
is more important than matching about,
even though about and
So intuitively,
But this model doesn't do that.
So that means this model
We have to solve these problems.
To summarize,
in this lecture we talked about how
We mainly need to do three things.
One is to define the dimension.
The second is to decide how to place
and to also place a query in
And third is to define
particularly the query vector and
We also talked about various simple way
Indeed, that's probably the simplest
In this case,
We use a zero, 1 bit vector to
In this case, we basically only care
We ignore the frequency.
And we use the Dot Product
And with such a instantiation,
we showed that the scoring
a document based on the number of distinct
We also showed that such a simple vector
we need to improve it.
And this is a topic that we're
[MUSIC]
[SOUND]
This lecture is about how to do faster
In this lecture, we're going to continue
In particular,
we're going to talk about how to support
So let's think about what a general
Now of course, the vector space
we can imagine many other retrieval
So the form of this
We see this scoring function
a query Q is defined as
that adjustment a function that
That I'll assume here at the end,
f sub d of d and f sub q of q.
These are adjustment factors
so they are at the level of a document and
So and then inside of this function,
we also see there's
So this is the main part
these as I just said of
the level of the whole document and
For example, document [INAUDIBLE] and
this aggregate punching would
Now inside this h function,
there are functions that
of the contribution of
So this g,
of a matched query term ti in document d.
And this h function would then
So for example,
but it can also be a product or it could
And then finally, this adjustment
the document level or query level
for example, document [INAUDIBLE].
So, this general form would cover
Let's look at how we can score documents
So, here's a general algorithm
First this query level and
document level factors can be
Of course, for the query we have to
document, for example,
And then, we maintain a score accumulator
An h is an aggregation function
So how do we do that?
For each period term we're going to
from the invert index.
This will give us all the documents
and that includes d1, f1 and so dn fn.
So each pair is a document ID and
Then for each entry d sub j and
of the term in this
We'll going to compute the function
weight of this term, so
we're computing the weight completion of
And then, we're going to update
this document and
accumulator that would
So this is basically a general
functions of this form by
Note that we don't have to
that didn't match any query term.
Well, this is why it's fast,
we only need to process the documents
In the end, then we're going to adjust
sub a and then we can sort.
So let's take a look
In this case, let's assume the scoring
it just takes the sum of t f, the role of
This simplification would help
It's very easy to extend the computation
the transformation of tf, or [INAUDIBLE]
So let's take a look at specific example,
and it show some entries of
Information occurred in four documents and
their frequencies are also there,
So let's see how the arrows works, so
and we fetch the first query then,
That's information, right?
And imagine we have all these
scores for these documents.
We can imagine there will be other but
then they will only be
So before we do any waiting of terms,
we don't even need a score of.
That comes actually we have these score
So lets fetch the interest from
information, that the first one.
So these four accumulators obviously
So, the first entry is d1 and 3,
3 is occurrences of
Since our scoring function assume that the
We just need to add a 3 to the score
the increase of score due to matching
And then, we go to the next entry,
then we add a 4 to the score
Of course, at this point, that we will
And so at this point, we allocated
and we add one, we allocate another
And then finally,
information occurred five
Okay, so this completes the processing of
information.
It processed all the contributions
four documents.
So now, our error will go to
So, we're going to fetch all
So, in this case,
there are three entries, and
The first is d2 and 3 and
that means security occur three
Well, we do exactly the same,
So, this time we're going to change the
allocated and
value which is a 4, so
D2 score is increased because the match
the security.
Go to the next entry, that's d4 and
d4 and again, we add 1 to d4 so
Finally, we process d5 and a 3.
Since we have not yet allocated a score
we're going to allocate 1 for d5,
So, those scores, of the last rule,
If our scoring function is just
Now, what if we, actually,
Well, we going to do the [INAUDIBLE]
So, to summarize this,
we first process the information
we processed all the entries
Then we process the security,
what should be the order of processing
It might make a difference especially
the score accumulators.
Let's say, we only want to keep
What do you think would be
Would you process a common term first or
The answers is we just go to who
A rare term would match a few documents,
be higher,
And then, it allows us to attach
So, it helps pruning
if we don't need so
So those are all heuristics for
Here you can also see how we can
So they can [INAUDIBLE] when we
each query time.
When we fetch the inverted index we
then we can compute IDF.
Or maybe perhaps the IDF value
when we indexed the documents.
At that time, we already computed
so all these can be done at this time.
So that would mean when we process
these words would be adjusted by the same
So this is the basic idea of using
it works well for all kinds of
And this generally,
most state of art retrieval functions.
So there are some tricks to
some general techniques
This is we just store some results of
when you see the same query,
Similarly, you can also slow the list
a popular term.
And if the query term is popular likely,
you will soon need to factor the inverted
So keeping it in the memory would help,
improving efficiency.
We can also keep only the most promising
doesn't want to examine so many documents.
We only need to return high
likely are ranked on the top.
For that purpose,
We don't have to store
At some point, we just keep
Another technique is to do parallel
really process in such a large
And you scale up to
the special techniques you
to distribute the storage
So here is a list of some text retrieval
You can find more information
And here, I listed your four here,
that can support a lot of applications and
You can use it to build a search
The downside is that it's not
the algorithms implemented they are also
Lemur or Indri is another
a nice support web
it has many advanced search algorithms and
Terrier is yet another toolkit
application capability and
So that's maybe in between Lemur or
maybe rather combining
so that's also useful tool kit.
MeTA is a toolkit that we will use for
the problem assignment and
a combination of both text retrieval
And so talking models are implement they
implemented in the toolkit as
So to summarize all the discussion
here are the major takeaway points.
Inverted index is the primary data
and that's the key to enable
And the basic idea is to preprocess
we want to do compression
So that we can save disk space and
processing of inverted index in general.
We talked about how to construct the
the memory.
And then we talk about faster search using
the invective index to accumulate a scores
And we exploit the Zipf's law to
that don't match any query term and
this algorithm can actually support
So these basic techniques
further scaling up using distributed file
Here are two additional readings you
you are interested in
The first one is a classical
o inverted index and
And how to,
any inputs of the space,
The second one is a newer textbook that
evaluating search engines.
[MUSIC]
[SOUND].
This lecture is about some practical
evaluation of text retrieval systems.
In this lecture, we will continue
We'll cover some practical
in actual evaluation of
So, in order to create
we have to create a set of queries.
A set of documents and
It turns out that each is
First, the documents and
They must represent the real queries and
And we also have to use many queries and
many documents in order to
For the matching of relevant
We also need to ensure that there exists a
If a query has only one, that's
It's not very informative to
using such a query because there's not
So ideally, there should be more
the queries also should represent
In terms of relevance judgments,
complete judgments of all
Yet, minimizing human and
fault, because we have to use human
It's very labor intensive.
And as a result, it's impossible to
all the queries, especially considering
So this is actually a major challenge,
For measures, it's also challenging,
accurately reflect
We have to consider carefully
And then design measures to measure that.
If your measure is not
then your conclusion would be misled.
So it's very important.
So we're going to talk about
One is the statistical significance test.
And this also is a reason why
And the question here is how sure can
doesn't simply result from
So here are some sample results of
System B into different experiments.
And you can see in the bottom,
So the mean, if you look at the mean
of positions are exactly the same
So you can see this is 0.20,
And again here it's also 0.20 and
Yet, if you look at these exact average
If you look at these numbers in detail,
you would feel that you can trust
In the another case, in the other case,
So, why don't you take a look at all these
So, if you look at the average,
we can easily, say that well,
So, after all it's 0.40 and
this is twice as much as 0.20,
But if you look at these two experiments,
You will see that, we've been more
in experiment one.
In this case.
Because these numbers seem to be
Whereas in Experiment 2, we're not sure
after System A is better and
But yet if we look at only average,
So, what do you think?
How reliable is our conclusion,
Now in this case, intuitively,
But how can we quantitate
And this is why we need to do
So, the idea of the statistical
assess the variants across
If there is a big variance,
that means the results could fluctuate
Then we should believe that,
the results might change if we
Right, so this is then not so
if you have c high variance
So let's look at these results
So, here we show two different
One is a sign test where
If System B is better than System A,
When System A is better we
Using this case, if you see this,
We actually have four cases
But three cases of System A is better,
this is almost like a random results,
So if you just take a random
if you use plus to denote the head and
that could easily be the results of just
So, the fact that the average is
We can't reliably conclude that.
And this can be quantitatively
And that basically means
the probability that this result is
In this case, probability is 1.0.
It means it surely is
Now in Willcoxan test,
and we would be not only
we'll be also looking at
But we can draw a similar conclusion,
where you say it's very
To illustrate this, let's think
And this is called a now distribution.
We assume that the mean is zero here.
Lets say we started with
no difference between the two systems.
But we assume that because of random
we might observe a difference.
So the actual difference might
on the right side here, right?
So, and this curve kind of shows
actually observe values that
Now, so if we look at this picture then,
if a difference is observed here, then
the chance is very high that this is
We can define a region of
random fluctuation and
And in this then the observed may
But if you observe a value in this
then the difference is unlikely
All right, so there's a very small
such a difference just because
So in that case, we can then conclude
So System B is indeed better.
So this is the idea of
The takeaway message here is that you
jumping into a conclusion.
As in this case,
There are many different ways of doing
So now, let's talk about the other
as we said earlier,
completely unless it's
So the question is,
in the collection,
And the solution here is Pooling.
And this is a strategy that has been used
So the idea of Pooling is the following.
We would first choose a diverse
These are Text Retrieval systems.
And we hope these methods can help us
So the goal is to pick out
We want to make judgements on relevant
useful documents from users perspectives.
So then we're going to have
The K can vary from systems.
But the point is to ask them to suggest
And then we simply combine
to form a pool of documents for
To judge, so imagine you have many
We take the top-K documents,
Now, of course, there are many
many systems might have retrieved
So there will be some duplicate documents.
And there are also unique documents
So the idea of having diverse
set of ranking methods is to
And can include as many possible
And then, the users would,
the judgments on this data set, this pool.
And the other unjudged the documents are
Now if the pool is large enough,
But if the pool is not very large,
And we might use other
there are indeed other
And such a strategy is generally okay for
comparing systems that
That means if you participate
then it's unlikely that it
because the problematic
However, this is problematic for
evaluating a new system that may
In this case, a new system might
nominated some read only documents
So those documents might be
That's unfair.
So to summarize the whole part of textual
Because the problem is the empirically
don't rely on users, there's no way to
If we have in the property
we might misguide our research or
And we might just draw wrong conclusions.
And we have seen this is
So make sure to get it right for
The main methodology is the Cranfield
And they are the main paradigm used in
not just a search engine variation.
Map and nDCG are the two main
know about and they are appropriate for
You will see them often
Precision at 10 documents is easier
So that's also often useful.
What's not covered is some other
Where the system would mix two,
And then would show
Of course, the users don't see
The users would judge those results or
click on those documents in
In this case then, the search engine
see if one method has contributed
If the user tends to click on one,
then it suggests that
So this is what leverages the real users
It's called A-B Test and
the modern search engines or
Another way to evaluate IR or
textual retrieval is user studies and
I've put some references here
to know more about that.
So, there are three
These are three mini books about
in covering a broad review of
And it covers some of the things
they also have a lot of others to offer.
[MUSIC]
[SOUND]
This lecture is about the specific
smoothing methods for language models
In this lecture, we will continue
information retrieval, particularly
And we're going to talk about specifically
such a retrieval function.
So this is a slide from a previous
likelihood ranking and smoothing
we add up having a retrieval function
So this is the retrieval function based on
You can see it's a sum of all
And inside its sum is the count
some weight for the term in the document.
We have t of i, the f weight here, and
So clearly if we want to implement this
we still need to figure
In particular, we're going to need to
of a word exactly and how do we set alpha.
So in order to answer this question,
smoothing methods, and
We're going to talk about
The first is simple linear
And this is also called
So the idea is actually very simple.
This picture shows how
language model by using
That gives us word counts normalized by
The idea of using this method
is to maximize the probability
As a result,
in the text, it's going to get
So the idea of smoothing, then,
where this word is not going to have
nonzero probability should
So we can note that network has
So in this approach what we do is we do
likelihood placement here and
is computed by the smoothing parameter
So this is a smoothing parameter.
The larger lambda is,
So by mixing them together,
we achieve the goal of assigning nonzero
So let's see how it works for
For example, if we compute
Now the maximum likelihood
that's going to be here.
But the collection probability is this.
So we'll just combine them
We can also see the word network,
now is getting a non-zero
And that's because the count is
But this part is nonzero, and
Now if you think about this and
sub d in this smoothing
Because that's remember the coefficient
of the word given by the collection
Okay, so
The second one is similar but
linear interpolation.
It's often called Dirichlet Prior,
So again here we face problem
an unseen word like network.
Again we will use the collection
we're going to combine them
The formula first can be seen as
likelihood estimate and
as in the J-M smoothing method.
Only that the coefficient now
but a dynamic coefficient in this form,
where mu is a parameter,
And you can see if we
the effect is that a long document would
Because a long document
therefore the coefficient
And so a long document would have
So this seems to make more sense
Of course,
that the two coefficients would sum to 1.
Now this is one way to
Basically, it means it's a dynamic
There is another way to understand
easier to remember, and
So it's easier to see how we can rewrite
Now in this form we can easily
the maximum likelihood estimate,
So normalize the count
So in this form we can see what we did is
So what does this mean?
Well, this is basically something related
the collection.
And we multiply that by the parameter mu.
And when we combine this
essentially we are adding
We pretend every word has
So the total count would be
the actual count of
As a result, in total we would
Why?
over all the words, then we'll see the
and that gives us just mu.
So this is the total number of
And so
So in this case, we can easily
add this as a pseudocount to this data.
Pretend we actually augment the data
defined by the collection language model.
As a result, we have more counts is that
the total counts for
And as a result, even if a word has zero
count here, then it would still have
So this is how this method works.
Let's also take a look at
So for text again we will
that we actually observe, but
And so the probability of
Naturally, the probability of
And so here you can also see
Can you see it?
If you want to think about it,
But you'll notice that this
So we can see, in this case,
alpha sub d does depend on the document,
because this length
whereas in the linear interpolation,
the J-M smoothing method,
[MUSIC]
[SOUND] This lecture is about
link analysis for web search.
In this lecture, we're going to talk
focusing on how to do link analysis and
The main topic of this lecture is to look
In the previous lecture we talked
Now that we have index, we want to see
The web.
Now standard IR models,
In fact,
improve, for supporting web search.
But they aren't sufficient.
And mainly for the following reasons.
First, on the web, we tend to have
example, people might search for
And this is different from
where people are primarily interested
So this kind of query is often
The purpose is to navigate into
So for such queries we might benefit
Secondly, documents have additional
are web format,
such as the layout, the title,
So this has provided opportunity to use
extra context information of
And finally,
That means we have to consider
the range in the algorithm.
This would give us a more robust way
any spammer to just manipulate the one
So as a result,
people have made a number of major
One line is to exploit
And that's the main topic of this lecture.
People have also proposed algorithms to
Feedback information the form of
in the category of feedback techniques and
In general in web search the ranking
algorithms to combine
Many of them are based on
as BM25 that we talked about [INAUDIBLE]
to provide additional features
but link information
they provide additional scoring signals.
So let's look at links in
So this is a snapshot of some
So we can see there are many links that
And in this case, you can also
a description of a link that's pointing
Now, this description text
Now if you think about this text,
because it provides some extra
So for example, if someone wants
the person might say the biggest
then the link to Amazon, right?
So, the description here after is very
the query box when they are looking for
And that's why it's very useful for
Suppose someone types in
biggest online bookstore.
All right the query would match
And then this actually
matching the page that's being
a entry page.
So if you match anchor text that
actually that provides good evidence for
So anchor text is very useful.
If you look at the bottom part of this
patterns of some links and these links
So for example,
on the right side you'll see this
Now that means many other pages
This shows that this page is quite useful.
On the left side you can see this
many other pages.
So this is a director page
actually see a lot of other pages.
So we can call the first
the second case half page, but this means
One is to provide extra text for matching.
The other is to provide some
to characterize how likely a page is
So people then of course and proposed
Google's PageRank which was the main
is a good example and
popularity, basically to score authority.
So the intuitions here are links
Now think about one page
this is very similar to one
So, of course then,
then we can assume this page
So that's a very good intuition.
Now PageRank is essentially to take
implement with the principal approach.
Intuitively, it is essentially doing
It just improves the simple
One it will consider indirect citations.
So that means you don't just look
You also look at what are those
If those pages themselves have a lot
In some sense,
But if those pages that
being pointed to by other pages they
then well, you don't get that much.
So that's the idea of
All right, so
you can also understand this idea by
If you're cited by let's say ten papers,
are just workshop papers or some papers
So although you've got ten in-links,
are cited by ten papers that themselves
And so in this case where we would
page does that.
The other idea is it's
Assume that basically every page is having
Essentially you are trying to
links that will link all
that you actually get the pseudo
The reason why they want to do that.
Is this will allow them
elegantly with linear algebra technique.
So, I think maybe the best
the PageRank is to think
probability of random surfer
[MUSIC]
[SOUND]
There are some interesting challenges
in threshold for
So here I show the historical data that
so you can see the scores and
So the first one has a score of 36.5 and
The second one is not relevant and
Of course, we have a lot of documents for
because we have never
So as you can see here,
we only see the judgements of
So this is not a random sample,
It's kind of biased, so that creates
Secondly, there are in general very little
so it's also challenging for
typically they require more training data.
And in the extreme case at
labeled data as well.
The system there has to make a decision,
so that's a very difficult
Finally, there is also this issue of
Now, this means we also want
space a little bit and
interested in documents that
So in other words, we're going to
by testing whether the user might be
currently are not matching
So how do we do that?
Well, we could lower the threshold
deliver some near misses to the user
to see how the user would
And this is a tradeoff, because on
on the other hand,
because then you will over
So exploitation means you would
Let's say you know the user is
you don't want to deviate that much, but
if you don't deviate at all then you don't
You might miss opportunity to learn
So this is a dilemma.
And that's also a difficulty
Now, how do we solve these problems?
In general, I think one can use the
And this strategy is basically to optimize
just as you have seen
Right, so you can just compute
each candidate score threshold.
Pretend that, what if I cut at this point.
What if I cut at the different scoring
What's utility?
Since these are training data,
and we know that relevant status,
relevant status based on
So then we can just choose the threshold
on the training data.
But this of course, doesn't account for
And there is also the difficulty of
So, in general, we can only get the upper
because the threshold might
So, it's possible that this could
interesting to the user.
So how do we solve this problem?
Well, we generally, and
as I said we can low with this
So here's on particular approach
So the idea is falling.
So here I show a ranked list of all the
far, and
And on the y axis we show the utility,
how you specify the coefficients
we can then imagine, that depending on the
Suppose I cut at this position and
For example,
The optimal point,
when it will achieve the maximum utility
And there is also zero utility threshold.
You can see at this cutoff
What does that mean?
That means if I lower the threshold
The utility would be lower but
So it's not as high as
But it gives us as a safe point
as I have explained, it's desirable
So it's desirable to lower the threshold
So that means, in general, we want to set
Let's say we can use the alpha to control
the deviation from
So you can see the formula of the
of the zero utility threshold and
Now, the question is,
And when should we deviate more
Well, this can depend on multiple factors,
encourage this threshold
up to the zero point, and
we're not going to necessarily reach
Rather, we're going to use other
this specifically is as follows.
So there will be a beta parameter to
threshold and this can be based on can
to the training data let's say, and so
But what's more interesting
Here, and you can see in this formula,
gamma is controlling the inference
of the number of examples
So you can see in this formula as N which
becomes bigger, then it would
In other words, when these very
And that just means if we have seen few
examples we're not sure whether we
So we need to explore but as we have
many that have we feel that we
So this gives us a beta gamma for
The more examples we have seen
So the threshold would be closer
that's the basic idea of this approach.
This approach actually has been working
particularly effective.
And also can work on arbitrary utility
And explicitly addresses
it kind of uses the zero utility
exploration-exploitation tradeoff.
We're not never going to explore
So if you take the analogy of gambling,
you don't want to risk on losing money.
So it's a safe spend, really
And the problem is of course,
the zero utility lower boundary is also
course, more advance in machine learning
solving this problems and
So to summarize, there are two
filtering systems, one is content based,
and the other is collaborative filtering
We've covered content-based
In the next lecture, we will talk
In content-based filtering system,
several problems relative to
And such a system can actually be
by adding a threshold mechanism and
allow the system to learn from
[MUSIC]
[SOUND].
So, as we explained the different text
representation tends to
In particular,
more deeper analysis results
And that would open up a more
opportunities and
So, this table summarizes
So the first column shows
The second visualizes the generality
Meaning whether we can do this
all the text data or only some of them.
And the third column shows
And the final column shows some
can be achieved through this
So let's take a look at them.
So as a stream text can only be processed
It's very robust, it's general.
And there was still some interesting
at this level.
For example, compression of text.
Doesn't necessarily need to
Although knowing word boundaries
Word base repetition is a very
It's quite general and
relatively robust, indicating they
Such as word relation analysis,
And there are many applications that can
For example, thesaurus discovery has
And topic and
And there are, for example, people
might be interesting in knowing the major
And this can be the case
And scientists want to know what are the
Or customer service people might want to
customers by mining their e-mail messages.
And business intelligence
understanding consumers' opinions about
products to figure out what are the
And, in general, there are many
applications that can be enabled by
Now, moving down, we'll see we can
By adding syntactical structures,
syntactical graph analysis.
We can use graph mining algorithms
And some applications are related
For example,
stylistic analysis generally requires
We can also generate
And those are features that might help us
categories by looking at the structures
It can be more accurate.
For example,
different categories corresponding
You want to figure out which of
this article, then you generally need
When we add entities and relations,
then we can enable other techniques
answers, or information network and
And this analysis enable
For example,
discovery of all the knowledge and
You can also use this level representation
to integrate everything about
Finally, when we add logical predicates,
that would enable large inference,
And this can be very useful for
integrating analysis of
For example,
extracted the information from text,
A good of example of application in this
is a knowledge assistant for biologists.
And this program that can help a biologist
literature about a research problem such
And the computer can make inferences
about some of the hypothesis that
For example,
then the intelligent program can read the
doing compiling and
And then using a logic system to
to researchers questioning about what
So in order to support
we need to go as far as
Now, this course is covering techniques
And these techniques are general and
robust and that's more widely
In fact, in virtually all the text mining
representation and then techniques that
But obviously all these other
should be combined in order to support
So to summarize,
Text representation determines what
And there are multiple ways to
syntactic structures, entity-relation
And these different
be combined in real applications
For example, even if we cannot
of syntactic structures, we can state
And if we can recognize some entities,
So in general we want to
And when different levels
we can enable a richer analysis,
This course however focuses
Such techniques have also several
robust, so they are applicable
That's a big advantage over
more fragile natural language
Secondly, it does not require
sometimes, it does not
So that's, again, an important benefit,
because that means that you can apply
Third, these techniques are actually
effective form in implications.
Although not all of course
Now they are very effective
are invented by humans as basically
So they are actually quite sufficient for
So that makes this kind of word-based
And finally, such a word-based
by such a representation can be combined
So they're not competing with each other.
[MUSIC]
[MUSIC]
This lecture is about topic mining and
We're going to talk about
This is a slide that you have
where we define the task of
We also raised the question, how do
So in this lecture, we're going to
that's our initial idea.
Our idea here is defining
A term can be a word or a phrase.
And in general,
So our first thought is just
For example, we might have terms
as you see here.
Now if we define a topic in this way,
we can then analyze the coverage
Here for example,
we might want to discover to what
And we found that 30% of the content
And 12% is about the travel, etc.
We might also discover document
So the coverage is zero, etc.
So now, of course,
topic mining and analysis,
One is to discover the topics.
And the second is to analyze coverage.
So let's first think
topics if we represent
So that means we need to mine k
Now there are, of course,
And we're going to talk about
which is also likely effective.
So first of all,
we're going to parse the text data in
Here candidate terms can be words or
Let's say the simplest solution is
These words then become candidate topics.
Then we're going to design a scoring
is as a topic.
So how can we design such a function?
Well there are many things
For example, we can use pure statistics
Intuitively, we would like to
meaning terms that can represent
So that would mean we want
However, if we simply use the frequency
then the highest scored terms
functional terms like the, etc.
Those terms occur very frequently English.
So we also want to avoid having
we want to penalize such words.
But in general, we would like to favor
not so frequent.
So a particular approach could be based
And TF stands for term frequency.
IDF stands for inverse document frequency.
We talked about some of these
ideas in the lectures about
So these are statistical methods,
meaning that the function is
So the scoring function
It can be applied to any language,
But when we apply such a approach
we might also be able to leverage
For example, in news we might favor
We might want to favor title
use the title to describe
If we're dealing with tweets,
which are invented to denote topics.
So naturally, hashtags can be good
Anyway, after we have this design
the k topical terms by simply picking
Now, of course,
we might encounter situation where the
They're semantically similar, or
So that's not desirable.
So we also want to have coverage over
So we would like to remove redundancy.
And one way to do that is
which is sometimes called a maximal
Basically, the idea is to go down
function and gradually take terms
The first term, of course, will be picked.
When we pick the next term, we're
been picked and try to avoid
So while we are considering
we are also considering
with respect to the terms
And with some thresholding,
the redundancy removal and
Okay, so
And those can be regarded as the topics
Next, let's think about how we're going
So looking at this picture,
these topics.
And now suppose you are give a document.
How should we pick out coverage
Well, one approach can be to simply
So for example, sports might have occurred
travel occurred twice, etc.
And then we can just normalize these
probability for each topic.
So in general, the formula would
all the terms that represent the topics.
And then simply normalize them so
topic in the document would add to one.
This forms a distribution of the topics
of different topics in the document.
Now, as always,
solving problem, we have to ask
Or is this the best way
So now let's examine this approach.
In general,
by using actual data sets and
Well, in this case let's take
And we have a text document that's
So in terms of the content,
But if we simply count these
we will find that the word sports
even though the content
So the count of sports is zero.
That means the coverage of sports
Now of course,
the document and
And that's okay.
But sports certainly is not okay because
So this estimate has problem.
What's worse, the term travel
So when we estimate the coverage
we have got a non-zero count.
So its estimated coverage
So this obviously is also not desirable.
So this simple example illustrates
First, when we count what
we also need to consider related words.
We can't simply just count
In this case, it did not occur at all.
But there are many related words
So we need to count
The second problem is that a word
So here it probably means
we can imagine it might also
So in that case, the star might actually
So we need to deal with that as well.
Finally, a main restriction of this
term to describe the topic, so it cannot
For example, a very specialized
describe by using just a word or
We need to use more words.
So this example illustrates
this approach of treating a term as topic.
First, it lacks expressive power.
Meaning that it can only represent
it cannot represent the complicated topics
Second, it's incomplete
meaning that the topic itself
It does not suggest what other
Even if we're talking about sports,
So it does not allow us to easily
conversion to coverage of this topic.
Finally, there is this problem
A topical term or
For example,
So in the next lecture,
about how to solve
[MUSIC]
[SOUND]
So let's plug in these model masses
into the ranking function to
This is a general smoothing.
So a general ranking function for
you have seen this before.
And now we have a very specific smoothing
So now let's see what what's a value for
And what's the value for p sub c here?
Right, so we may need to decide this
in order to figure out the exact
And we also need to figure
So let's see.
Well this ratio is basically this,
here, this is the probability
and this is the probability
in other words basically 11
this, so it's easy to see that.
This can be then rewritten as this.
Very simple.
So we can plug this into here.
And then here, what's the value for alpha?
What do you think?
So it would be just lambda, right?
And what would happen if we plug in
What can we say about this?
Does it depend on the document?
No, so it can be ignored.
Right?
So we'll end up having this
And in this case you can easy to see,
this a precisely a vector space
a sum over all the matched query terms,
What do you think is a element
Well it's this, right.
So that's our document left element.
And let's further examine what's
Well one plus this.
So it's going to be nonnegative,
it's going to be at least 1, right?
And these, this is a parameter,
And let's look at this.
Now this is a TF.
Now we see very clearly
And the larger the count is,
We also see IDF weighting,
And we see docking the lan's
So all these heuristics
What's interesting that
weighting function automatically
Whereas in the vector space model,
we had to go through those heuristic
And in this case note that
And when you see whether this
All right so what do you think
This is a math of document.
Total number of words,
given by the collection, right?
So this actually can be interpreted
If we're going to draw, a word,
And, we're going to draw as many as
If you do that,
would be precisely given
So, this ratio basically,
The actual count of the word in the
product if the word is in fact following
And if this counter is larger than
this ratio would be larger than one.
So that's actually a very
It's very natural and intuitive,
And this is one advantage of using
where we have made explicit assumptions.
And, we know precisely why
And, why we have these probabilities here.
And, we also have a formula that
does TF-IDF weighting and
Let's look at the,
It's very similar to
In this case,
that's different from
But the format looks very similar.
The form of the function
So we still have linear operation here.
And when we compute this ratio,
one will find that is that
And what's interesting here is that we
We're comparing the actual count.
Which is the expected account of the world
the collection world probability.
So note that it's interesting we don't
lighter in the JMs model.
All right so this of course
So you might wonder, so
Interestingly the docking lens
this would be plugged into this part.
As a result what we get is
this is again a sum over
And we're against the queer,
And you can interpret this as
but this is no longer
Because we have this part,
right?
So that just means if
we have to take a sum over
then do some adjustment of
But it's still, it's still clear
modulation because this lens
a longer document will
And we can also see it has tf here and
Only that this time the form of the
in JMs one.
But intuitively it still implements TFIDF
the form of the function is dictated
assumptions that we have made.
Now there are also
And that is, there's no guarantee
of the formula will actually work well.
So if we look about at this geo function,
rendition for example it's unclear whether
Unfortunately we can see here there
So we do have also the,
So we do have the sublinear
we do not intentionally do that.
That means there's no guarantee that
Suppose we don't have logarithm,
As we discussed before, perhaps
So that's an example of the gap
the relevance that we have to model,
which is really a subject
So it doesn't mean we cannot fix this.
For example, imagine if we did
So we can take a risk and
or we can even add double logarithm.
But then, it would mean that the function
So the consequence of
longer as predictable as
So, that's also why, for example,
still, open channel how to use
better model than the PM25.
In particular how do we use query
that would work consistently
Currently we still cannot do that.
Still interesting open question.
So to summarize this part, we've talked
Jelinek-Mercer which is doing the fixed
Dirichlet Prior this is what add a pseudo
interpolation in that the coefficient
In most cases we can see, by using these
reach a retrieval function where
So they are less heuristic.
Explaining the results also show
Also are very effective and they are
So this is a major advantage
where we don't have to do
Yet in the end that we naturally
doc length normalization.
Each of these functions also has
In this case of course we still need
There are also methods that can be
So overall,
we follow very different strategies
Yet, in the end, we end up uh,with
look very similar to
With some advantages in having
And then, the form dictated
Now, this also concludes our discussion of
And let's recall what
in order to derive the functions
Well we basically have made four
The first assumption is that the relevance
And the second assumption with med is, are
that allows us to decompose
into a product of probabilities
And then,
if a word is not seen,
its probability proportional to
That's a smoothing with
And finally, we made one of these
So we either used JM smoothing or
If we make these four assumptions
to take the form of the retrieval
Fortunately the function has a nice
weighting and document machine and
So in that sense,
these functions are less heuristic
And there are many extensions of this,
you can find the discussion of them in
[MUSIC]
[MUSIC]
So let's take a look at this in detail.
So in this random surfing
random surfer would choose
So this is a small graph here.
That's of course, over simplification
But let's say there are four
And let's assume that a random surfer or
And then the random
just randomly jumping to any page or
follow a link and
So if the random surfer is at d1,
then there is some probability that
Now there are two outlinks here,
the other is pointing to d4.
So the random surfer could pick any
But it also assumes that the random so
So the random surfing which decide
simply randomly jump
So if it does that, it would be able
though there's no link you actually,
So this is to assume that
Imagine a random surfer is
then we can ask the question how
would actually reach a particular
That's the average probability of
this probability is precisely
So the page rank score of
probability that the surfer
Now intuitively, this would basically
Because if a page has a lot of inlinks,
then it would have a higher
Because there will be more
follow a link to come to this page.
And this is why the random surfing model
actually captures the ID
Note that it also considers
Because if the page is that point then
That would mean the random surfer would
therefore, it increase
So this is just a nice way to capture
So mathematically, how can we compute this
we need to take a look at how this
So first of all let's take a look
And this is just metrics with
the random surfer would go
So each rule stands for a starting page.
For example, rule one would
to any of the other four pages from d1.
And here we see there are only
So this is because if you look at
There is no link from d1 or d2.
So we've got 0s for the first 2
columns and 0.5 for d3 and d4.
In general, the M in this matrix,
M sub ij is the probability
And obviously for each rule,
because the surfer would have to go to
So this is a transition metric.
Now how can we compute the probability
Well if you look at the surf
we can compute the probability
So here on the left hand side,
visiting page dj at time plus 1,
On the right hand side, you can see
of at page di at time t.
So you can see the subscript
that indicates that's the probability that
So the equation basically,
possibilities of reaching
What are these two possibilities?
Well one is through random surfing and
one is through following a link,
So the first part captures the probability
that the random surfer would reach
And you can see the random
with probability 1 minus
And so
But the main party is realist
that the surfer could have been at time t.
There are n pages so
Inside the sum is a product
One is the probability that the surfer
was at di at time t, that's p sub t of di.
The other is the transition
And so in order to reach this dj page,
the surfer must first be at di at time t.
And then also, would also have to
So the probability is the probability
the probability of going from that
The second part is a similar sum, the only
probability is a uniform
1 over n and
of reaching this page
So the form is exactly the same and
see on why PageRank is essentially assumed
If you think about this 1 over n as
that has all the elements being
Then you can see very clearly
because they are of the same form.
We can imagine there's a different
that uniform metrics where
And in this sense PageRank uses
ensuring that there's no zero entry
Now of course this is the time dependent
Now we can imagine, if we'll compute
the average of probabilities probably
without considering the time index.
So let's drop the time index and
Now this would give us any equations,
each page we have such equation.
And if you look at the what
there are also precisely n variables.
So this basically means,
n equations with n variables and
So basically, now the problem boils
And here, I also show
It's the vector p here equals a matrix or
the transpose of the matrix here and
Now, if you still remember some knowledge
and then you will realize, this is
When multiply the metrics by this vector,
this can be solved by
So because the equations here
on the back are basically
So you'll see the relation between the
And this iterative approach or
we simply start with s
And then we repeatedly
multiplying the metrics
I also show a concrete example here.
So you can see this now.
If we assume alpha is 0.2,
then with the example that
we have the original
That includes the graph, the actual links
metrics, uniform transition metrics
And we can combine them together with
metric that would be like this.
So essentially,
we can imagine now the web looks like
They're all virtual links
The page we're on now would just
then just computed the updating of this
p vector by using this
Now if you rewrite this
terms of individual equations,
And this is basically,
this particular pages and page score.
So you can also see if you want to compute
You basically multiply
and we'll take the third
And that will give us the value for
So this is how we updated the vector
these guys for this.
And then we just revise
set of scores and
So we just repeatedly apply this and
And when the matrix is like this,
it can be guaranteed to converge.
And at that point the we will just have
We typically go to sets of
So interestingly,
also interpreted as propagating
Or if you look at this formula and
can you imagine,
essentially propagating
I hope you will see that indeed,
we can imagine we have values
So we can have values here and
And then we're going to use these
And if you look at the equation here
to combine the scores of the pages that
So we'll look at all the pages
then combine this score and propagate the
To look at the scores that we present
surfer would be visiting the other
And then just do
the probability of reaching this page, d1.
So there are two interpretations here.
One is just the matrix multiplication.
We repeat the multiplying
The other is to just think
these scores repeatedly on the web.
So in practice, the combination of
Because the matrices is fast and there
So that you avoid actually
all those elements.
Sometimes you may also normalize the
different form of the equation, but
The results of this potential
In that case, if a page does not have
these pages would not sum to 1.
Basically, the probability of reaching the
1, mainly because we have lost
One would assume there's some probability
the links, but
And one possible solution is simply to use
and that could easily fix this.
Basically, that's to say alpha would
In that case,
randomly jump to another page
There are many extensions of PageRank, one
Note that PageRank doesn't merely
So we can make PageRank specific however.
So for example,
we can simply assume
The surfer is not randomly
Instead, he's going to jump to only those
For example, if the query is not sports
doing random jumping, it's going
By doing this, then we can buy
And then if you know the current
then you can use this specialized
That would be better than if you
PageRank is also a channel that can be
network analysis particularly for
You can imagine if you compute
social network, where a link
a relation, you would get some
[MUSIC]
This lecture is about
In this lecture we're going to continue
In particular, we're going to look at
You have seen this slide before when
answer the basic question,
In the previous lecture,
we looked at the item similarity,
In this lecture, we're going to
This is a different strategy,
So first, what is collaborative filtering?
It is to make filtering decisions for
individual user based on
And that is to say we will
preferences from that
So the general idea is the following.
Given a user u, we're going to first
And then we're going to
based on the preferences of
Now, the user similarity here can
the preferences on a common set of items.
Now here you can see the exact
We're going to look at the only the
So this means this
It can be applied to any items,
So this approach would work well
First, users with the same interest
Second, the users with similar preferences
So for example, if the interest of
then we can infer the user
So those who are interested in
probably all favor SIGIR papers.
That's an assumption that we make.
And if this assumption is true,
then it would help collaborative
We can also assume that if we see
then we can infer their interest
So in these simple examples,
in many cases such assumption
So another assumption we have to make
number of user preferences
So for example, if you see a lot
those indicate their
And if you have a lot of such data,
filtering can be very effective.
If not, there will be a problem, and
That means you don't have many
the system could not fully take advantage
So let's look at the filtering
So this picture shows that we are,
in general, considering a lot of users and
we're showing m users here, so U1 through.
And we're also considering
Let's say n objects in
And then we will assume that
objects and the user could for
For example, those items could be movies,
then the users would give
So what you see here is that we have
some combinations.
So some users have watched some movies,
they obviously won't be able
some users may actually
So this is in general a small symmetrics.
So many items and
And what's interesting here is we
of an element in this matrix
And that's after the essential question
we assume there's an unknown
That would map a pair of user and
And we have observed the sum
And we want to infer the value
other pairs that don't have
So this is very similar to other
know the values of the function
And we hope to predict the values of
this is a function approximation.
And how can we pick out the function
So this is the setup.
Now there are many approaches
In fact,
reason that there are special
major conference devoted to the problem.
[MUSIC]
[SOUND] This lecture is
about the word association
mining and analysis.
In this lecture,
associations of words from text.
Now this is an example of knowledge
we can mine from text data.
Here's the outline.
We're going to first talk about
then explain why discovering such
we're going to talk about some general
In general there are two word
One is called a paradigmatic relation.
The other is syntagmatic relation.
A and B have paradigmatic relation
if they can be substituted for each other.
That means the two words that
would be in the same semantic class,
And we can in general
without affecting
That means we would still
For example, cat and dog, these two
because they are in
And in general,
the sentence would still be a valid
Similarly Monday and
The second kind of relation is
In this case, the two words that have this
So A and B have syntagmatic relation if
a sentence, that means these two
So for example, cat and sit are related
Similarly, car and
they can be combined with
However, in general, we can not
car with drive in the sentence
meaning that if we do that, the sentence
So this is different from
And these two relations are in fact so
generalized to capture basic relations
And definitely they can be
relations of any items in a language.
So, A and B don't have to be words and
And they can even be more complex
If you think about the general
then we can think about the units
Then we think of paradigmatic
are applied to units that tend to occur
or in a sequence of data
So they occur in similar locations
Syntagmatical relation on
co-occurrent elements that tend
So these two are complimentary and
And we're interested in discovering
Discovering such worded
First, such relations can be directly
tasks, and this is because this is part
So if you know these two words
and then you can help a lot of tasks.
And grammar learning can be also
Because if we can learn
then we form classes of words,
And if we learn syntagmatic relations,
the rules for putting together a larger
So we learn the structure and
Word relations can be also very useful for
many applications in text retrieval and
For example, in search and
associations to modify a query,
introduce additional related words into
It's often called a query expansion.
Or you can use related words to
to explore the information space.
Another application is to
automatically construct the top
We can have words as nodes and
A user could navigate from
find information in the information space.
Finally, such word associations can also
For example, we might be interested
negative opinions about the iPhone 6.
In order to do that, we can look at what
a feature word like battery in
Such a syntagmatical
show the detailed opinions
So, how can we discover such
Now, here are some intuitions
Now let's first look at
Here we essentially can take
So here you see some simple
You can see they generally
and that after all is the definition
On the right side you can kind
the context of cat and
I've taken away cat and
that you can see just the context.
Now, of course we can have different
For example, we can look at
part of this context.
So we can call this left context.
What words occur before we see cat or dog?
So, you can see in this case, clearly
You generally say his cat or my cat and
So that makes them similar
Similarly, if you look at the words
which we can call right context,
Of course, it's an extreme case,
And in general,
that can't follow cat and dog.
You can also even look
And that might include all
in sentences around this word.
And even in the general context, you also
So this was just a suggestion
relation by looking at
So, for example,
How similar are context of cat and
In contrast how similar are context
Now, intuitively,
the context of dog would
the context of cat and
That means, in the first case
between the context of cat and
the similarity between context of cat and
because they all not having a paradigmatic
relationship and imagine what words
It would be very different from
So this is the basic idea of what
What about the syntagmatic relation?
Well, here we're going to explore
again based on the definition
Here you see the same sample of text.
But here we're interested in knowing
with the verb eats and
And if you look at the right
you see,
I've taken away the word to its left and
also the word to its
And then we ask the question, what words
And what words tend to
Now thinking about this question
relations because syntagmatic relations
So the important question to ask for
whenever eats occurs,
So the question here has
are some other words that tend
Meaning that whenever you see eats
And if you don't see eats, probably,
So this intuition can help
Now again, consider example.
How helpful is occurrence of eats for
Right.
in a sentence would generally help us
And if we see eats occur in the sentence,
that should increase the chance
In contrast,
how helpful is the occurrence of eats for
Because eats and
knowing whether eats occurred
really help us predict the weather,
So this is in contrast to
This also helps explain that intuition
behind the methods of what
Mainly we need to capture the correlation
So to summarize the general ideas for
discovering word associations
For paradigmatic relation,
And then compute its context similarity.
We're going to assume the words
to have paradigmatic relation.
For syntagmatic relation, we will count
in a context, which can be a sentence,
And we're going to compare
their co-occurrences with
We're going to assume words
relatively low individual occurrences
because they attempt to occur together and
Note that the paradigmatic relation and
are actually closely related
related words tend to have syntagmatic
They tend to be associated
that suggests that we can also do join
So these general ideas can be
And the course won't cover all of them,
we will cover at least some of
discovering these relations.
[MUSIC]
[SOUND] This lecture is a continued
discussion of probabilistic topic models.
In this lecture, we're going to continue
We're going to talk about
are interested in just mining
So in this simple setup,
one document and
So this is the simplest
The input now no longer has k,
know there is only one topic and the
In the output,
we assumed that the document
So the main goal is just to discover
this single topic, as shown here.
As always, when we think about using a
we start with thinking about what
from what perspective we're going to
And then we're going to
the generating of the data,
Where our perspective just means we want
the data, so that the model will
discovering the knowledge that we want.
And then we'll be thinking
write down the microfunction to
a data point will be
And the likelihood function will have
And then we argue our interest in
by maximizing the likelihood which will
These estimator parameters
of the mining hours,
parameters as the knowledge
So let's look at these steps for
Later we'll look at this procedure for
So our data, in this case is, just
Each word here is denoted by x sub i.
Our model is a Unigram language model.
A word distribution that we hope to
So we will have as many parameters as many
And for convenience we're
denote the probability of word w sub i.
And obviously these theta
Now what does a likelihood
Well, this is just the probability
that given such a model.
Because we assume the independence in
the document will be just a product
And since some word might
So we can also rewrite this
So in this line, we have rewritten
over all the unique words in
Now this is different
Well, the product is over different
Now when we do this transformation,
introduce a counter function here.
This denotes the count of
similarly this is the count
because these words might
You can also see if a word did
It will have a zero count, therefore
So this is a very useful form of
writing down the likelihood function
So I want you to pay attention to this,
It's just to change the product over all
So in the end, of course, we'll use
function and it would look like this.
Next, we're going to find
of these words that would maximize
So now lets take a look at the maximum
This line is copied from
It's just our likelihood function.
So our goal is to maximize
We will find it often easy to
maximize the local likelihood
And this is purely for
the logarithm transformation our function
And we also have constraints
The sum makes it easier to take
finding the optimal
So please take a look at this sum again,
And this is a form of
see later also,
So it's a sum over all
And inside the sum there is
And this is macroed by
So let's see how we can
Now at this point the problem is purely a
to just the find the optimal solution
The objective function is
the constraint is that all these
So, one way to solve the problem is
Now this command is beyond
since Lagrange multiplier is a very
to just give a brief introduction to this,
So in this approach we will
And this function will combine
with another term that
we introduce Lagrange multiplier here,
lambda, so it's an additional parameter.
Now, the idea of this approach is just to
in some sense,
Now we are just interested in
As you may recall from calculus,
would be achieved when
This is a necessary condition.
It's not sufficient, though.
So if we do that you will
with respect to theta i
And this part comes from the derivative
this lambda is simply taken from here.
And when we set it to zero we can
easily see theta sub i is
Since we know all the theta
we can plug this into this constraint,
And this will allow us to solve for
And this is just a net
And this further allows us to then
eventually, to find the optimal
And if you look at this formula it turns
because this is just the normalized
which is also a sum of all
So, after all this mess, after all,
we have just obtained something
this will be just our
maximize the data by
mass as possible to all
And you might also notice that this is
raised estimator.
In general, the estimator would be to
the counts have to be done in a particular
So this is basically an analytical
In general though, when the likelihood
going to be able to solve the optimization
Instead we have to use some
we're going to see such cases later, also.
So if you imagine what would we
likelihood estimator to estimate one
Let's imagine this document
Now, what you might see is
On the top, you will see the high
common words,
And this will be followed by
characterize the topic well like text,
And then in the end,
words that are not really
they might be extraneously
As a topic representation,
That because the high probability
they are not really
So my question is how can we
Now this is the topic of the next module.
We're going to talk about how to use
these common words.
[MUSIC]
[SOUND]
we talked about PageRank as
Now, we also looked at some other examples
So there is another algorithm called HITS,
that going to compute the scores for
The intuitions are pages that are widely
whereas pages that cite many
I think that the most interesting
is it's going to use
to kind of help improve the scoring for
And so here's the idea,
it was assumed that good
That means if you are cited by many
that inquiry says, you're an authority.
And similarly, good hubs are those
So if you pointed to a lot
then your hubs score would be increased.
So then you will have literally reinforced
some good hubs.
And so you have pointed to some good
whereas those authority
improved because they
And this is algorithms is also general it
network analysis.
So just briefly, here's how it works.
We first also construct a matrix, but this
matrix and
So if there's a link there's a 1,
Again, it's the same graph.
And then we're going to
as the sum of the authority scores of
So whether you are hub,
really depends on whether you are pointing
That's what it says in the first equation.
In the second equation,
as a sum of the hub scores of all
So whether you are good authority
pages that are pointing
So you can see this forms
Now, these three questions can be
So what we get here is then the hub
of the adjacency matrix and
and this is basically the first equation.
And similarly, the second equation
vector is equal to the product of
Now, these are just different ways
But what's interesting is that
you can also plug in the authority
So if you do that, you have actually
and you get the equations
The hubs score vector is
by a transpose multiplied
Similarly, we can do a transformation
just the authorities also.
So although we frame the problem
we can actually eliminate one of them to
Now, the difference between this and page
a multiplication of the adjacency
So this is different from page rank.
But mathematically, then we will
So in HITS,
Let's say, 1 for all these values, and
then we would iteratively apply
And this is equivalent to multiply
So the arrows of these is exactly
But here because the adjacency
So what we have to do is after each
this would allow us to
Otherwise they would grow larger and
And if we do that, and
That was the computer, the hubs scores,
And these scores can then be used in
So to summarize in this lecture, we have
In particular,
increase the text
And we also talk about the PageRank and
page anchor as two major
Both can generate scores for web pages
Note that PageRank and
So they have many applications in
[MUSIC]
[SOUND]
here we're going to talk
And that would be based on
then predicting the rating of and
object by an active user using the ratings
This is called a memory based approach
storing all the user information and
when we are considering a particular
retrieve the rating users or
And then try to use this
to predict the preference of this user.
So here is the general idea and
x sub i j denotes the rating
and n sub i is average rating
So this n i is needed because
we would like to normalize
So how do you do normalization?
Well, we're going to just subtract
Now, this is to normalize these ratings so
that the ratings from different
Because some users might be more generous,
ratings but some others might be
cannot be directly compared with each
So we need to do this normalization.
Another prediction of
by another user or
can be based on the average
So the user u sub a is the user that we
And we now are interested in
So we're interested in knowing how
How do we know that?
Where the idea here is to look at
have liked this object.
So mathematically this is to say
this user on this app object,
combination of the normalized
and in fact here,
But not all users contribute
and this is conjured by the weights.
So this weight controls the inference
of the user on the prediction.
And of course,
the similarity between ua and
The more similar they are,
user ui can make in predicting
So, the formula is extremely simple.
You can see,
And inside the sum we have their ratings,
their normalized ratings
The ratings need to be normalized in
And then these ratings
So you can imagine w of a and i is just
Now what's k here?
Well k is simply a normalizer.
It's just one over the sum of all
So this means, basically, if you consider
we have coefficients of weight that
And it's just a normalization strategy so
in the same range as these ratings
Right?
So this is basically the main idea
collaborative filtering.
Once we make this prediction,
back through the rating that
the user would actually make,
and this is to further
average rating of this user u
This would recover a meaningful rating for
So if this user is generous, then
and when we add that the rating will be
Now when you recommend an item to a user
because you are interested in
that's more meaningful.
But when they evaluate these
they typically assume that
these objects to be unknown and
then you compare the predicted
So, you do have access
But, then you pretend that you don't know,
then you compare your systems
In that case, obviously, the systems
the actual ratings of the user and
Okay so this is the memory based approach.
Now, of course,
if you want to write
you still face the problem of
Once you know the w function, then
So, indeed, there are many different ways
w, and specific approaches generally
So here are some possibilities and
you can imagine there
One popular approach is we use
This would be a sum over
And the formula is a standard
coefficient formula as shown here.
So this basically measures
to all give higher ratings to similar
Another measure is the cosine measure,
vectors as vectors in the vector space.
And then,
compute the cosine of
And this measure has been using the vector
So as you can imagine there are just
In all these cases, note that the user's
on items and we did not actually use
It didn't matter these items are,
they can be books, they can be products,
they can be text documents which
so this allows such approach to be
Now in some newer approaches of course,
we would like to use more
Clearly, we know more about the user,
So in the actual filtering system,
we could also combine that
We could use more context information,
that people are just starting, and
But, this memory based approach has
and it's easy to implement in
a starting point to see if the strategy
So, there are some obvious ways
mainly we would like to improve
And there are some practical
So for example,
What do you do with them?
Well, you can set them to default values
And that would be a simple solution.
But there are advanced approaches that
missing values, and then use predictive
So in fact that the memory based apology
So you get you have iterative approach
prediction and
then you can use the predictive values to
So this is a heuristic
And the strategy obviously would affect
just like any other heuristics would
Another idea which is actually very
have seen in text search is called
Now here the idea is to look at where
If the item is a popular item that
seen [INAUDIBLE] to people interested
interesting but if it's a rare item,
But these two users deal with this
And, that says more
It's kind of to emphasize
on items that are not
[MUSIC]
[SOUND]
lecture is about
In this lecture we are going to talk about
association called
By definition,
related if they share a similar context.
Namely, they occur in
So naturally our idea of discovering such
of each word and then try to compute
So here is an example of
Here I have taken the word
you can see we are seeing some remaining
Now, we can do the same thing for
So in general we would like to capture
the similarity of the context of cat and
So now the question is how can we
then define the similarity function.
So first, we note that the context
So, they can be regarded as
document, but there are also different
For example, we can look at the word
We can call this context Left1 context.
All right, so in this case you
big, a, the, et cetera.
These are the words that can
So we say my cat, his cat,
Similarly, we can also collect the words
We can call this context Right1, and
here we see words like eats,
Or, more generally,
we can look at all the words in
Here, let's say we can take a window
We call this context Window8.
Now, of course, you can see all
so we'll have a bag of words in
Now, such a word based representation
an interesting way to define the
Because if you look at just
then we'll see words that share
and we kind of ignored the other words
So that gives us one perspective to
if we only use the Right1 context,
we will capture this narrative
Using both the Left1 and
the similarity with even
So in general, context may contain
my, that you see here, or
Tuesday, or
And this flexibility also allows us
different ways.
Sometimes this is useful,
as we might want to capture
That would give us loosely
Whereas if you use only the words
to the right of the word, then you
much related by their syntactical
So the general idea of discovering
is to compute the similarity
So here, for example,
dog based on the similarity
In general, we can combine all
And so the similarity function is,
a combination of similarities
And of course, we can also assign
similarities to allow us to focus
And this would be naturally
here the main idea for discovering
to computer the similarity
So next let's see how we exactly
Now to answer this question,
representation as vectors
Now those of you who have been
textual retrieval techniques would
been used frequently for
But here we also find it convenient
paradigmatic relation discovery.
So the idea of this
word in our vocabulary as defining one
So we have N words in
then we have N dimensions,
And on the bottom, you can see a frequency
and here we see where eats
ate occurred 3 times, et cetera.
So this vector can then be placed
So in general,
context of cat as one vector,
dog, might give us a different context,
And then we can measure
So by viewing context in
we convert the problem of
into the problem of computing
So the two questions that we
how to compute each vector, and
And the other question is how
Now in general, there are many approaches
most of them are developed for
And they have been shown to work well for
matching a query vector and
But we can adapt many of
of context documents for our purpose here.
So let's first look at
where we try to match
the expected overlap of words,
So the idea here is to represent
where each word has a weight
that a randomly picked word from
So in other words,
account of word wi in the context, and
this can be interpreted as
actually pick this word from d1
Now, of course these xi's would sum to one
and this means the vector is
actually probability of
So, the vector d2 can be also
this would give us then two probability
So, that addresses the problem
next let's see how we can define
Well, here, we simply define
vectors, and
of the corresponding
Now, it's interesting to see
actually has a nice interpretation,
Dot product, in fact that gives
randomly picked words from
That means if we try to pick a word
word from another context, we can then
If the two contexts are very similar,
see the two words picked from
If they are very different,
identical words being picked from
So this intuitively makes sense, right,
Now you might want to also take
see why this can be interpreted
two randomly picked words are identical.
So if you just stare at the formula
then you will see basically in each
we will see an overlap on
And where xi gives us a probability that
and yi gives us the probability
And when we pick the same
then we have an identical pick, right so.
That's one possible approach, EOWC,
Now as always, we would like to assess
Now of course, ultimately we have to
see if it gives us really
Really give us paradigmatical relations,
analytically we can also analyze
So first, as I said,
formula will give a higher score if there
So that's exactly what we want.
But if you analyze
then you also see there might
and specifically there
First, it might favor matching
over matching more distinct terms.
And that is because in the dot product,
element is shared by both contexts and
it might indeed make the score
where the two vectors actually have
But each term has a relatively low
Of course, this might be
But in our case, we should intuitively
more different terms in the context,
in saying that the two words
If you only rely on one term and
that's a little bit questionable,
Now the second problem is that it
So if you match a word like the and
it will be the same as
intuitively we know
surprising because the occurs everywhere.
So matching the is not as such
a word like eats,
So this is another
In the next chapter we are going to talk
[MUSIC]
[SOUND]
to summarize our discussion of
the filtering task for
in some other sense,
So it's easy because
In this case the system takes initiative
The user doesn't really make any effort,
any recommendation is better than nothing.
All right.
items or useless documents.
If you can recommend
users generally will appreciate it,
However, filtering is actually much harder
make a binary decision and you can't
then you're going to see whether
You have to make a decision
Think about news filtering.
As soon as you see the news enough
interesting to the user.
If you wait for a few days, well, even if
the most relevant news, the utility is
Another reason why it's hard
if you think of this
Collaborative filtering, for
example, is purely based on
So if you don't have many ratings there's
And yeah I just mentioned
This is actually a very serious,
But of course there are strategies that
and there are different strategies that
You can use, for example, more user
instead of using the preferences
items give me additional information
And we also talk about two strategies for
One is content-based where
is collaborative filtering where
And they obviously can be
You can imagine they generally
So that would give us a hybrid
And we also could recall that we talked
about push versus pull as two strategies
And recommender system easy to
search engines are serving
Obviously the two should be combined,
The two have a system
with multiple mode information access.
So in the future we could anticipate such
And either,
there are a lot of new algorithms
In particular those new algorithms tend
Now the context here could be
could also be the context of the user.
Items.
The items are not the isolated.
They're connected in many ways.
The users might form
so there's a rich context there
really solve the problem well and
research area where also machine
Here are some additional readings in
the handbook called
has a collection of a lot
can give you an overview
approaches through recommender systems.
[MUSIC]
In this lecture, we continue
discussing Paradigmatical
Earlier we introduced
Expected Overlap of
In this method, we
a word vector that represents
the probability of a
We measure the similarity
which can be interpreted as
randomly picked words from
the two contexts are identical.
We also discussed
The first is that
one frequent term very well over
matching more distinct terms.
It put too much emphasis on
The second is that it
Even a common word like
equally as content
So now we are
going to talk about how
More specifically, we're
some retrieval heuristics
These heuristics can effectively
as these problems also
when we match a query that
So to address the first problem,
we can use a sublinear
That is, we don't have to use
the raw frequency count of
a term to represent the context.
We can transform
that wouldn't emphasize so
To address the
we can put more weight
That is we can reward
This heuristic is called the IDF
term weighting in text retrieval.
IDF stands for
So now, we're going to talk about
the two heuristics
First let's talk about
That is to convert
a word in the document
that reflects our belief
about how important
So that will be
That's shown in the y-axis.
Now, in general, there are
Let's first look at
In this case, we're
any non-zero counts
one and the zero count
So with this mapping
all the frequencies will be
mapped to only two
The mapping function is shown
Now, this is naive
because it's not
However, this actually
emphasizing matching all
So it does not allow
a frequency of word to
Now, the approach
earlier in the expected
is a linear transformation.
We basically, take
So we use the raw count
That created the problem
that we just talked about namely;
it emphasize too much on just
Matching one frequent term
So we can have a lot
of other interesting
in between the two extremes,
and they generally form
So for example,
logarithm of the raw count,
and this will give us curve
that you are seeing here.
In this case, you can see
The high counts are
so the curve is a sublinear
the weight of
This is what we want,
terms from dominating
Now, there is also
another interesting
a BM25 transformation which
has been shown to be very
In this transformation, we have
a form that looks like this.
So it's k plus one multiplied
where k is a parameter,
x is the count,
the raw count of a word.
Now, the transformation
that it can actually go from
one extreme to the other
k. It also interesting
k plus one in this case.
So this puts
on high frequency terms,
because their weight would
As we vary k, if we can
So when k is set to zero,
we roughly have the 0,1 vector.
Whereas when we set k
it will behave more like
So this transformation
far the most effective
text retrieval and it also makes
sense for our problem setup.
So we just talked about how
overemphasizing a frequency term
Now let's look at
and that is how we can
Matching "the" is not surprising,
because "the" occurs everywhere.
But matching "eats"
So how can we address
Now in this case, we can
That's commonly
IDF stands for
Document frequency
of the total number of
documents that contain
So here we show that the IDF
a logarithm function
of documents that match a
So K is the number of
document frequency and M
here is the total number of
The IDF function is giving
meaning that it
The maximum value is
That's when the word occurred
So that's a very rare term,
the rare is term in
The lowest value you can
its maximum which would be M.
So that would be
close to zero in fact.
So this of course measure
is used in search where we
In our case, what would
Well, we can also
we can collect all the words
That is to say,
a word that's popular in
would also have a low IDF.
Because depending on the dataset,
we can construct the context
But in the end if a term is
very frequent in
then it will still be frequent
in the collective
So how can we add
improve our similarity function?
Well, here's one way
many other ways
But this is a reasonable way,
where we can adapt
for paradigmatical
In this case, we define the
elements representing
So in this
we take sum over all
normalize the weight of
of the weights of all the words.
This is to again ensure all the
xi's will sum to
So this would be very similar
in that this vector is
actually something similar
all the xi's will sum to one.
Now, the weight of BM25 for
If you compare this with
have a normalized count
So we only have this one
the total counts of words in
and that's what we had before.
But now with the BM25
we introduced something else.
First, of course,
this count is just to
achieve the sub-linear
But we also see we introduced
and this parameter is
although zero is also possible.
But this controls
and also controls to what extent
it simulates the
So this is one parameter,
but we also see there is
b, and this would be
This is a parameter to
In this case,
the normalization formula has
a average document lens here.
This is computed up
of the lenses of all the
In this case, all the lenses of
all the context of documents
So this average documents
will be a constant for
So it actually is only
affecting the effect
b, here because
But I kept it here because
for in retrieval where it would
give us a stabilized
But for our purpose,
this will be a constant so
the lens normalization
Now, with this definition then,
we have a new way to define
and we can compute
The difference is that
the high-frequency terms will now
have a somewhat lower weights.
This would help us control
the inference of
Now, the idea can be added
That means we'll
for matching each term.
So you may recall
all the possible words
overlap between the two contexts.
The x_i and the y_i
of picking the word
Therefore, it
we'll see a match on this word.
Now, IDF would give us
the importance of
A common word will be worth
So we emphasize more on
So with this modification,
then the new function will
likely address
Now, interestingly
this approach to discover
In general, when we re-brand
a context with a term vector,
we would likely see
some terms have high weights
and other terms have low weights.
Depending on how we assign
we might be able to
discover the words that
are strongly associated with
the candidate word
So let's take a look at
the term vector in
We have each x_i
defined as the normalized
Now, this weight alone only
reflects how frequent the word
But we can't just say
any frequent term in
the context that would
the candidate word because
many common words like 'the' will
occur frequently in
But if we apply IDF
we can then re-weight
That means the words that are
common like 'the'
So now the highest
those common terms because
Instead, those terms would
be the terms that are
but not frequent
So those are clearly the words
the context of the candidate
So for this reason,
the highly weighted terms in
can also be assumed to
be candidates for
Now, of course, this is
our approach for discovering
In the next lecture, we're
how to discover
But it clearly shows the relation
between discovering
Indeed they can be discovered in
a joint manner by leveraging
So to summarize,
paradigmatic relations is to
collect the context of
a candidate word to
This is typically represented
Then compute the similarity of
the corresponding
of two candidate words.
Then we can take
and treat them as having
These are the words that
There are many different ways to
implement this general idea.
We just talked about
More specifically, we
text retrieval models to help us
design effective
compute the
More specifically, we have used
the BM25 and IDF weighting
to discover
These approaches also represent
the state of the art in
Finally, syntagmatic relations
as a by-product when we discover
[NOISE]
This lecture is a summary of this course.
This map shows the major topics
And here are some key
First, we talked about natural
Here the main take-away messages
a foundation for text retrieval, but
the battle of wars is generally the main
And it's often sufficient before
obviously for
a deeper natural language
We then talked about the high
text access and
In pull we talked about
Now in general in future search engines,
to provide a math involved
And now we'll talk about a number of
We talked about the search problem.
And we framed that as a ranking problem.
And we talked about a number
We start with the overview
the probabilistic model and then we talked
We also later talked about
that's probabilistic model.
And here, many take-away message is that
look similar, and
Most important ones are TF-IDF weighting,
And the TF is often transformed through
And then we talked about how to
the main techniques that we talked about,
that we can prepare the system
And we talked about how to do a faster
And we then talked about how to
mainly introduced to
This was a very important
applied to many tasks.
We talked about the major
So, the most important measures for
are MAP, mean average precision,
accumulative gain and also precision and
And we then talked about
And we talked about the Rocchio
the mixture model and
Feedback is a very important
the opportunity of learning from
We then talked about Web search.
And here we talked about how
to solve the scalability issue in that
Then we talked about how to use linking
We talked about page rank and
hits as the major hours is to
We then talked about
This is the use of machine learning
improvement scoring.
Not only that the effectiveness can be
we can also improve the robustness of the.
The ranking function so that it's
It just some features to promote the page.
And finally we talked about
About the some major reactions
in the future in improving the count
And then finally we talked about
these are systems to
And we'll talk about the two approaches,
one is collaborative filtering and
Now, an obvious missing piece
in this picture is the user,
so user interface is also an important
Even though the current search interface
done a lot of studies of user interfaces
And this is the topic to that,
you can learn more by reading this book.
It's an excellent book about all kinds
If you want to know more about
you can also read some additional
In this short course we only
topics in text retrievals and
And these resources provide additional
they give a more thorough treatment of
And a main source is
that you can see a lot of short
or long tutorials.
They tend to provide a lot of
And there a lot of series that
One is information concepts,
One is human langauge technology.
And yet another is artificial
There are also some major journals and
tend to have a lot of research papers
And finally, for more information
tool kits, etc you can check out his URL.
So, if you have not taken the text
specialization series then naturally
As this picture shows,
we generally need two kinds of techniques.
One is text retrieval,
And these techniques will help us
relevant text data, which are actually
Now human plays important role in mining
written for humans to consume.
So involving humans in the process
in this course we have covered
access to the most relevant data.
These techniques are always so
to help provide prominence and
patterns that the user will
So, in general, the user would have
better understand the patterns.
So the text mining cause, or rather,
will be dealing with what to do once
So this is a second step in this
the text data into actionable knowledge.
And this has to do with helping users to
to find the patterns and
In text and such knowledge can
systems to help decision making or
So, if you have not taken that course,
that natural next step would
Thank you for taking this course.
I hope you had fun and
And I look forward to interacting
[MUSIC]
