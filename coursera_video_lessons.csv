course_id,week_nbr,video_id,content
cs-410,1,1,"Hello welcome to CS410 DSO Text Information Systems. This is an online course offered by University of Illinois at Urbana-Champaign. My name is ChengXiang Zhai. I also have a nickname, Cheng. I'm a professor of Computer Science at the University of Illinois at Urbana-Champaign. I'm the instructor of this course. This first lecture is just an introduction to the course. Let's first to start with some motivation. The problem we are trying to address in this course is how to harness big text data. Text data is all kinds of data in the form of natural languages such as English and Chinese. Now, this kind of data is everywhere and also growing very quickly. For example, you can find all kinds of web pages on the Internet and the number of pages is growing quickly. You can also find the blogs articles, news articles, or Emails and other kind of documents and enterprise environment. Of course, we will also have a lot of scientific literature in text form. And nowadays, social media has been growing quickly. So we now see, tweets and other social media data also in the form of text. All such text data encode a lot of useful knowledge about the world because it's in some sense the data reported by human census about the observe the world. So we can analyzed this kind of data to discover a lot of useful knowledge. Especially the knowledge about the human opinions or preferences. So this kind of data is very useful and we can use computational methods to turn such data into useful knowledge, which can then be further used in many applications. The main techniques for making this happen include the Text Retrieval and Text Mining. And these are the main techniques that we will cover in this course. Logically, in order to make use a lot of text data. We would first do text retrieval, and that's due to a large set of text data into a smaller but much more relevant set of data, that we actually need for a particular problem. And this step, is usually implemented by using text retrieval techniques that involve humans in the loop to find and locate the most relevant documents to a particular problem. Once we find the relevant documents, the next step is to do text mining, which is to further analyze the found of relevant documents to discover useful knowledge to extract the knowledge that can be directly used in application, especially in a application such as decision making. These two steps corresponding to Text Retrieval and Text Mining Techniques, that we will cover in this course. Based on this picture that I show you, this course is designed to leverage to corresponding books that I've offered on Coursera, which cover Text Retrieval and Text Mining respectively. The first book is called, Text Retrieval and Search Engines. The second book is called, the Text Mining and Analytics. These two books have comprehensive lecture videos that cover basic concepts and principles and methods for text retrieval and text mining respectively. So, this online course, will leverage all those lecture videos and also the online quizzes that are provided through the Coursera platform. But in addition to that, we also need to add additional components in order to explore applications because those books have covered a general techniques without a necessary discussing in depths how those techniques are using applications. So, to make the coverage more complete in CS410, we were adding two additional components as shown here. That is of course Products and Technology Review. Both are meant to give the students freedom to choose a particular way to apply some of the techniques that you have learned in those two books to solve a real world problem or to further learn about a particular topic. In the project, you will have opportunities to apply the knowledge that you have learned in those two books to solve a real world problem in an integrated manner. In technical review, you can leverage what you have learned to learn even more about either tool kit or a particular technology that can be used to extend your knowledge about how to solve a problem. The format of the course thus is a mixture of online videos plus some high engagement module which mostly consists of our interactions through forums as I will explain later. And also, we will have a lot of interactions to help you finish the course Project and Technology Review. There are a number of goals that we have kept in mind when designing this course and I want to share with you these goals because they would help you understand why the course has this particular format and why you are asked to do certain components of the tasks. The first goal is we want to emphasize both theory and practice. Theory is obviously very important because the basic concepts and general principles covering the theoretical part would be applicable to all applications. That means, they are not just used for today's applications but they can be used for solving future problems as well so, they are more general. Therefore, they have long lasting impact and utility value. And this is achieved by having you to watch all the lecture videos, and then to use quizzes and exams to make sure that you have mastered all the basic ideas, basic concepts and general principles and those general methods. Practical skills are also very important, and because of specific practical skills can be immediately useful for solving the problems today and maybe you will be able to resolve the problems that you encountered in your job. And this is achieved by having you to do Programming assignments, where you would be able to use existing tool kits, to look into some algorithms in depths and to use some of the algorithms to have some sense about how they work or how to improve them. And finally, of course it's very important to integrate the theory and practice. And this will be done by, having you to work on Course projects. The second goal is to Personalized Learning, because every one of you has a different in need and different in preference, and you will have a different in schedules as well, because many of you are full time workers, perhaps. And so, Personalized Learning is very important to ensure everyone to receive the best education. So to achieve this goal, we have intentionally designed the deadlines and the format of the course to be flexible so that, you can have a lot of freedom to do self-paced learning. For example, you can watch over the lecture videos at anytime that you want to. Watch it, watch that. And you can also finish all the quizzes pretty much anytime and you can do the assignment, program assigns particularly again in a flexible way. In addition to self paced learning, you would also have a choices of topics for project and the technology review. And you will be able to work on a topic that's most interesting to you, for both course project and technology review. Finally, we also have the goal of collaborative learning because this would maximize the efficiency of learning. Our common goal is to help everyone learn maximum amount of knowledge, while spending hopefully minimum amount of effort. So, this is a common goal that we should all work toward, and we should help each other achieve this goal. To promote collaborative learning and to facilitate collaborative learning, we will use forum-based interactions to enable all of us, including many of you from different time zones, to interact with each other effectively. Another way to support a collaborative learning is to have you to work in groups, to finish cost projects and technology reviews. So, both technology reviews and products can be done by working together with others in a group. So, overall the format is as follows: first, you will have to watch the lecture videos of those two MOOCs, and this is also shown here on this slide, and then you also need to take quizzes. And these quizzes are given in a weekly manner. But as I said, the deadlines are actually flexible. So, you should be able to finish quizzes as soon as you finish the corresponding lecture videos. These quizzes are two. There are also two kinds of quizzes. There are practice quizzes that you can use to help you understand some concepts. And there are also test quizzes. Those are of course to make sure that you have indeed mastered the materials. Then there are two exams. Those two exams will be given at the end of each MOOC corresponding. So, the first exam will be given at the end of the first MOOC on Test Retrieval, and the second one will be given at the end of the second MOOC on Test Mining. And then, during this period of watching these videos, you will be also working on programming assignments. Then at the end of the semester, we'll leave about two weeks time for you to work intensively on the course project, although you will be working on the course project throughout the semester. But most of the time will be in the end of the semester. Now, the course project will be down sequentially by following multiple steps. The first is for you to select a topic. You can select the topic from a list of topics that we provide or you can propose your topic, and then we'll ask you to submit a short proposal to be more specific about what you want to work on. And then, in the middle of the semester, you will be asked to provide a short progress report so that we can check your progress and we can provide help in a timely manner. And then, at the end of the semester, you will deliver two things: one is a software and its documentation. You upload that to a public website so that it's available to people. The second one is a tutorial presentation, a short presentation to explain how your software can be used by people. There is also a technology review component which is mandatory and this component will be correlated based on completion. Everyone is required to finish this component. Now, this component is designed to give you an opportunity to explore further any topic that you're interested in. This can be in that examination of a tool kit that you are interested in, perhaps a tool that you have used in the course project, or a comparison of multiple tools that can do similar things, and you might have looked into multiple tools, and you have some opportunity to compare them and to figure out which one is the best. Such a writing would be helpful for others to understand how to use a tool kit or which one to choose. Of course, those of you who are interested in the methods, you can also go in-depth to examine a certain type of methods and write a brief review of them, or looking to a cutting edge topic in your research and write a review of the recent papers about the topic. Now, we hope that you might be able to align the technology review with your course project so that the two will be kind of synergistic in that technology review will help you learn more about the topic related to your course project, while the course project will give you also a motivation for doing an in-depth study of the topic via technology review. Of course, we will try to help you finish all these tasks, and we'll do our best to help you. That means, the TAs and I will interact with you in various ways to help you, and one way is a synchronous question answering and discussing via forums. And the other way is to have synchronous weekly office hours and that will be down by using video teleconferencing. The grading will be done as follows: 25% of your grade will be based on the quizzes, all the quizzes for the two MOOCs; 30% will be based on the two exams, those two exams will be proctored exams; 25% will be based on the programming assignments, there will be multiple programming assignments throughout the semester. The remaining 20% will be based on your course project and that's for the distribution as follows: 5% is for topic selection, 5% is for proposal, and another 5% for progress report, 65% in most of the grade of the course project will be based on software deposit, your deliverable for the project. And finally, 20% is based on the tutorial presentation. Now, most of these components in the course project will be graded based on completion. Indeed, a lot of tasks you see in programming assignments are graded in the same way and that is because we believe we have designed the program assignments and course project in such a way that you will be able to learn a lot by simply going through these tasks. So, this is the effective way of learning by doing and that's why we choose to grade based on completion. That also means you have a lot of control over these grades, because you can ensure that you finish all these tasks and then you will get most of these grades. The only part that will be based on the quality of solution is the tutorial presentation. This is actually not just based on the quality of your tutorial presenting but rather based on whether you are software actually work as you propose. So, does it provide all the functions that you propose? Does it really work? And this will likely affect some of the grade for tutorial presentation, meaning that if a software for some reason doesn't really pass our test, then we would deduct the points from here. As I said, the technology review is not actually contributing to you grade, but it's a metric component and it will be graded based on completion. We also provide up to 5% of extra credit based on your participation in the forum discussions. And this is to encourage you to help each other and particularly by answering questions posed by others. We will be able to use log data from the forum to give you credit, extra credit, and these extra credit points will be actually added to the regular points. So that will allow you to actually increase your grade as will be shown here in more detail. And this is how we are going to determine your final letter grades. They are based on the grade points you have collected over the semester using this map shown here on the slide. Mostly it's five points for each bracket but not always. And as you can see, if you have earned 5% extra credit and when adding this extra credit, will likely help you move your grades up by one bracket, although it won't be more than one bracket. So, we feel that this fixed mapping would give you complete control over your grade, so you can monitor your grade over the semester and kind of assess your progress and also to adjust your schedule accordingly. So, this is a visualization of your workload. Horizontally, we show the timeline, from the first day of instruction to the last day of instruction. And then vertically, you can see there are mainly six tasks for you over the semester. First, you will spend most of your time on watching the lecture videos, and this is why we have a thick black line there that shows that most of your effort probably will be spent on watching lecture videos. And then, you will be taking 12 quizzes, and these will be spreading over most of the semester, corresponding to about 12 weeks when you are expected to watch those lecture videos. Now, we want to emphasize that you should spend most of your time to watch videos and make sure you understand the materials before you take quizzes, and this will ensure that you don't leave any holes. If you do it the other way and by using quizzes to guide you through the lecture videos, then you might leave some holes, and that might hurt your performance on exams. There will be two proctored exams that will be given in the middle of the semester and also later in the semester, at the end of the two box. So, this is your third task. The fourth task is programming assignments, and this will be, again, through the entire semester, actually not really all the weeks in the semester, but most of the weeks, because we don't want you to use up the last two weeks, which we reserve for you to work on course project. And the course project is the fifth component. And finally, you need to finish technology review. Now, although most of the work of the project is expected to be done at the end of the semester, you will actually start working on it from the very beginning. And, soon after the semester starts, we will ask you to think about the topics and you will discuss the topics and then form teams to submit the proposal. But of course, during the first 12 weeks, you will be most occupied by the lecture videos, quizzes, and your assignments for programming. So, that's why you will likely have more time to work on the project in the end. The technology review is kind of designed to extend your knowledge based on your project. But of course, you have the complete freedom to choose whatever topic that you want to review. So, you don't have to tie it to the project. And so, we imagine that you would start working on it a little bit after you have decided the project, the topic, so that you can decide whether you want to tie the technology review with your project. So, it's good to keep this picture in mind throughout the semester because you might have irregular schedule sometimes. For example, you might find that you are very busy in the middle of the semester, then this picture can help you adjust your schedule and the degree you probably want to then work more on some of the tasks at the beginning of the semester, so that you don't overwhelm yourself in the middle of the semester. And similarly, if you expect to be very busy the end of the semester, you might want to start working on the project much earlier. So, I hope this picture will be always in your mind, and you will be able to adjust your schedule accordingly and to particularly work on tasks proactively, in case you anticipate any busy time period. Some of you have already taken maybe both MOOCs. Now, if you are one of them, then I think naturally, you will have more time to work on other problems in this course. So, you should take advantage of this to proactively finish some of the tasks much more quickly. In particular, since you have already watched those videos and then you can simply review them and then you try to work on the quizzes so that you can finish all the quizzes much more quickly. You may be able to finish most of the programming assignments quickly too. This is not only going to be helpful for you in the sense that you will have a lot of time to work on course project, but also, it would allow you to help others by answering their questions on forums or helping them in other way, like teaming up with them to work on the project. However, I should also say that there are a few tasks that are, unfortunately, have very fixed time that you cannot really work on in advance. For example, the two exams will be scheduled on some particular dates that would have no flexibility for you to work on earlier. There may be some tasks in the programming assignments that have to be synchronized. For example, we might run competition of some tasks and there may be some synchronization that's needed, but we would like to minimize the dependency so that you could hopefully work on many of those tasks as early as you can. And of course, we encourage you to use more time to finish a more challenging course project or to finish a higher quality technology review. Now, we rely a lot on forum discussion, and this is because we are in different time zones and it's very hard to find a time that works well for everyone. But forum has important advantages of being able to accommodate everyone in the discussion. So, this will be the primary way of our interactions and engagement, and in particular, we will be using Piazza, which is a forum that we have used for many other courses, and it has proven to be a useful forum with a lot of useful functions. The second advantage of forum is it would also enable you to ask your questions as soon as you have a question. And therefore, we can hopefully accelerate question answering and so that you can have your question answered quickly on the forum without waiting until office hour. Finally, we hope that the forum discussions would help us identify difficult concepts in those lectures, so that we can focus on discussing these concepts or explaining these concepts that are hard to understand in our office hours. This would make better use of our office hours to help all of you. Because of these reasons, so the protocol of question answering will be as follows, and we want to emphasize that it's important to follow this protocol so that we can make effective use of our office hours, so that you can get your questions answered more quickly, so that you can all help each other in learning. So, first, as soon as you have a question or issue to discuss, post it immediately on the forum. And this has a number of advantages, and first is, it will give you the opportunity to have the question answered quickly because often, your question may be answered by your peers or may be answered by a teacher or by me. So, by posting the question immediately, you have a better chance of getting the question answered quickly, so that you won't have to wait. And secondly, your question might also help others because sometimes other students may have a similar question or may not realize that they have encountered this question, but you just articulated this question. So, this is helpful for your peers as well. And discussing of this question is often also a good way to learn. However, if your question is not answered in a timely manner on the forum, or addressed adequately from your perspective, then you should email the question to all of us including me and TAs. Please use a subject line that contains the keyword CS410DSO all together. Of course, your subject line, you can contain other keywords based on your question. And then, if you don't receive a reply from us by email in a timely manner, join an office-hour. Now, we would do our best to reply to your emails but then, depending on the number of emails, depending on our own schedule, we may not be able to always reply to your emails in a timely manner. Of course, we would do our best to respond quickly but if we can't, then you should come to one of our office hours. We would try to schedule our office hours in different time slots during the day. For example, we would have some time slot in the morning, or late morning, or early afternoon, and then another time slot in the evening. And this is so that, we can hopefully accommodate different time zones, because the same slot of time may not be equally convenient with all of you. So, we'll do our best again, to diversify the time slots, both in terms of the time in a day, and also in terms of the days in a week. The format of office hours is as follows, and this again, is based on our need to make effective use and the efficient use of our limited office hours to help you in the best way. So, we will hold weekly office hours, as I said, and we'll publish those time slots. And the office hours will be given by using video-teleconference, and particularly using Zoom, and its used for system that has worked well. And you can join or leave an office hour at any time and that means you can join late, or you can leave early, and it's very flexible. So, don't feel that you always have to come at the beginning of the office hour. Feel free to stop by in the last ten minutes, if that's the best time for you. And again, by taking advantage of the forum discussion, hopefully, by the time of going to office hour, we will only need to deal with some of the relatively difficult questions. And the priority we will use is to give the highest priority to any issues that have already been posted on forums, but have not been resolved even after some email communications. Those are clearly the toughest issues because it has been posted on forum without a good answer, and then was emailed to us and still not satisfactory solved. So, we would have to give those issues the highest priority, that means if there are such issues being raised during office hour, those issues would be given the first priority. After that, we'll look at the any other unresolved issues on forums. That's why it's very important for you to post the issue on the forum first, and only after resolving those issues that have already been posted on forums would we take other questions or issues that have not yet been posted on forums. And of course, you should not hesitate to bring any questions that you have or any issues you want to discuss. It's just that, we want to have a policy to prioritize the issues that we want to handle, so that we can provide the maximum benefit to all of you by using our office hours efficiently. Finally, I want to just say something in general about how to get the most out of this course. Perhaps, the most important advice is to plan ahead based on your own schedule, because many of you are very busy. So, you want to kind of take a look at the picture that I showed you earlier about the tasks, and then consider your own schedule. Try to imagine which periods will be a relatively busy period for you and identify what tasks you're supposed to work on in that period, and try to finish those tasks earlier, so that you don't overlap with yourself during that busy period. And of course, at any time, please let us know how we can help, again, by using the forums as the first step. And another thing to mention is also to allocate a sufficient time for the preparation of two proctored exams because they will be given only once. That means, you only have one chance to take each exam, so you want to really prepare well for them. However, those exams are mostly to confirm that you have indeed mastered the materials. So, they will have similar questions to the quiz questions that you have already seen. And in fact, some questions may be exactly the same as the questions in the quizzes. And we do that because this would allow you to have some sense about what the questions might look like in the exams. So, they should not be much surprise if you have actually worked on all the questions in those quizzes and have made sure that you have understood the answers to those questions. Of course, if you can, try to complete the quizzes and program assignments ahead of time. This would be to your advantage because you can now raise your questions earlier that would allow you to have more time to get your questions answered. Therefore, by the time when you take the quiz, then you would have all the questions resolved. And it also would allow you to actively help others and to discuss any of the problems that you have encountered that would help you earn the extra credit also. The second advice is to post questions on forum immediately and whenever you have difficulty understanding any part of the course materials. And again, I want to emphasize the immediate action of posting any issue, any question on the forums. We would do our best to resolve those issues through the forums. And it's the effective way to also engage the peers to help each other. So, do not hesitate to post questions and we won't to penalize you for posting many questions. In fact, we'll reward that perhaps, because that's one way to contribute to the forum discussion. Finally, you should leverage collaborative learning, this is kind of related to you posting your questions on forums, and again, to actively participate in the forum discussion. You will actually learn a lot from reading other people's posts, even if you know the answers because they are often opinions expressed about those materials. So, we do hope that you have active discussion on forums to help each other and to help each other, particularly, to save time, to understand the difficult concepts, to do well in the quizzes and exams, and to finish assignments smoothly. And finally, we do encourage you to help each other understand materials too. So, we encourage you to answer others' questions, and the system will record those answers. We'll have a statistics about that, and then we'll use that to give extra credit. So, this was just the overall introduction to the course. For more information, you can visit the course website on Coursera. We hope you enjoy this course, and I look forward to working with you. Thank you."
cs-410,1,2,"[SOUND] >> This lecture is about Natural Language of Content Analysis. As you see from this picture, this is really the first stepto process any text data. Text data are in natural languages. So computers have to understandnatural languages to some extent, in order to make use of the data. So that's the topic of this lecture. We're going to cover three things. First, what is naturallanguage processing, which is the main technique for processingnatural language to obtain understanding. The second is the state ofthe art of NLP which stands for natural language processing. Finally we're going to cover the relationbetween natural language processing and text retrieval. First, what is NLP? Well the best way to explain itis to think about if you see a text in a foreign languagethat you can understand. Now what do you have to do inorder to understand that text? This is basically whatcomputers are facing. So looking at the simple sentence likea dog is chasing a boy on the playground. We don't have any problemsunderstanding this sentence. But imagine what the computer wouldhave to do in order to understand it. Well in general,it would have to do the following. First, it would have to know dogis a noun, chasing's a verb, etc. So this is called lexical analysis,or part-of-speech tagging, and we need to figure out the syntacticcategories of those words. So that's the first step. After that, we're going to figureout the structure of the sentence. So for example, here it shows that A and the dog would go togetherto form a noun phrase. And we won't have dog and is to go first. And there are some structuresthat are not just right. But this structure shows what we mightget if we look at the sentence and try to interpret the sentence. Some words would go together first, and then they will go togetherwith other words. So here we show we have noun phrasesas intermediate components, and then verbal phrases. Finally we have a sentence. And you get this structure. We need to do something calleda semantic analysis, or parsing. And we may have a parseraccompanying the program, and that would automaticallycreated this structure. At this point you would knowthe structure of this sentence, but still you don't knowthe meaning of the sentence. So we have to go furtherto semantic analysis. In our mind we usually can map such a sentence to what we alreadyknow in our knowledge base. For example, you might imaginea dog that looks like that. There's a boy andthere's some activity here. But for a computer would haveto use symbols to denote that. We'd use a symbol (d1) to denote a dog. And (b)1 can denote a boy andthen (p)1 can denote a playground. Now there is also a chasingactivity that's happening here so we have a relationship chasingthat connects all these symbols. So this is how a computer would obtainsome understanding of this sentence. Now from this representation we couldalso further infer some other things, and we might indeed naturally think ofsomething else when we read a text and this is called inference. So for example, if you believethat if someone's being chased and this person might be scared,but with this rule, you can see computers could alsoinfer that this boy maybe scared. So this is some extra knowledgethat you'd infer based on some understanding of the text. You can even go further to understandwhy the person say at this sentence. So this has to do as a use of language. This is called pragmatic analysis. In order to understand the speakactor of a sentence, right? We say something tobasically achieve some goal. There's some purpose there. And this has to do withthe use of language. In this case the person who said this sentence might be remindinganother person to bring back the dog. That could be one possible intent. To reach this level ofunderstanding would require all of these steps anda computer would have to go through all these steps in order to completelyunderstand this sentence. Yet we humans have no troublewith understanding that, we instantly would get everything. There is a reason for that. That's because we have a largeknowledge base in our brain and we can use common sense knowledgeto help interpret the sentence. Computers unfortunately are hardto obtain such understanding. They don't have such a knowledge base. They are still incapable of doingreasoning and uncertainties, so that makes natural languageprocessing difficult for computers. But the fundamental reason why naturallanguage processing is difficult for computers is simply because naturallanguage has not been designed for computers. Natural languages are designed forus to communicate. There are other languages designed forcomputers. For example, programming languages. Those are harder for us, right? So natural languages is designed tomake our communication efficient. As a result,we omit a lot of common sense knowledge because we assume everyoneknows about that. We also keep a lot of ambiguities becausewe assume the receiver or the hearer could know how to decipher an ambiguous wordbased on the knowledge or the context. There's no need to demand differentwords for different meanings. We could overload the same word withdifferent meanings without the problem. Because of these reasons this makes everystep in natural language of processing difficult for computers,ambiguity is the main difficulty. And common sense and reasoning isoften required, that's also hard. So let me give you someexamples of challenges here. Consider the word level ambiguity. The same word can havedifferent syntactic categories. For example design can be a noun ora verb. The word of root mayhave multiple meanings. So square root in math sense orthe root of a plant. You might be able to thinkabout it's meanings. There are also syntactical ambiguities. For example, the main topic of thislecture, natural language processing, can actually be interpreted in twoways in terms of the structure. Think for a moment andsee if you can figure that out. We usually think of this asprocessing of natural language, but you could also think of this as dosay, language processing is natural. So this is an exampleof synaptic ambiguity. What we have different isstructures that can be applied to the same sequence of words. Another common example of an ambiguoussentence is the following. A man saw a boy with a telescope. Now in this case the question is,who had a telescope. This is called a prepositionalphrase attachment ambiguity or PP attachment ambiguity. Now we generally don't have a problem withthese ambiguities because we have a lot of background knowledge to helpus disambiguate the ambiguity. Another example of difficultyis anaphora resolution. So think about the sentence Johnpersuaded Bill to buy a TV for himself. The question here is doeshimself refer to John or Bill? So again this is something thatyou have to use some background or the context to figure out. Finally, presuppositionis another problem. Consider the sentence,he has quit smoking. Now this obviously impliesthat he smoked before. So imagine a computer wants to understandall these subtle differences and meanings. It would have to use a lot ofknowledge to figure that out. It also would have to maintain a largeknowledge base of all the meanings of words and how they are connected to ourcommon sense knowledge of the world. So this is why it's very difficult. So as a result, we are steep not perfect, in fact far from perfect in understandingnatural language using computers. So this slide sort of gains a simplifiedview of state of the art technologies. We can do part of speechtagging pretty well, so I showed 97% accuracy here. Now this number is obviouslybased on a certain dataset, so don't take this literally. This just shows that wecan do it pretty well. But it's still not perfect. In terms of parsing,we can do partial parsing pretty well. That means we can get noun phrasestructures, or verb phrase structure, or some segment of the sentence, and this dude correct them interms of the structure. And in some evaluation results,we have seen above 90% accuracy in terms of partialparsing of sentences. Again, I have to say these numbersare relative to the dataset. In some other datasets,the numbers might be lower. Most of the existing work has beenevaluated using news dataset. And so a lot of these numbers are more orless biased toward news data. Think about social media data,the accuracy likely is lower. In terms of a semantical analysis, we are far from being able to doa complete understanding of a sentence. But we have some techniquesthat would allow us to do partial understanding of the sentence. So I could mention some of them. For example, we have techniques that canallow us to extract the entities and relations mentioned in text articles. For example,recognizing dimensions of people, locations, organizations, etc in text. So this is called entity extraction. We may be able to recognize the relations. For example,this person visited that place or this person met that person orthis company acquired another company. Such relations can be extracted by using the computer currentNatural Language Processing techniques. They're not perfect butthey can do well for some entities. Some entities are harder than others. We can also do word sensedisintegration to some extend. We have to figure out whether this word inthis sentence would have certain meaning in another context the computer couldfigure out, it has a different meaning. Again, it's not perfect, butyou can do something in that direction. We can also do sentiment analysis, meaning, to figure out whethera sentence is positive or negative. This is especially useful forreview analysis, for example. So these are examplesof semantic analysis. And they help us to obtain partialunderstanding of the sentences. It's not giving us a completeunderstanding, as I showed it before, for this sentence. But it would still help us gainunderstanding of the content. And these can be useful. In terms of inference,we are not there yet, probably because of the general difficultyof inference and uncertainties. This is a general challengein artificial intelligence. Now that's probably also becausewe don't have complete semantical representation fornatural [INAUDIBLE] text. So this is hard. Yet in some domains perhaps,in limited domains when you have a lot of restrictions on the word uses, you may beable to perform inference to some extent. But in general we can notreally do that reliably. Speech act analysis is alsofar from being done and we can only do that analysis forvery special cases. So this roughly gives you someidea about the state of the art. And then we also talk a littlebit about what we can't do, and so we can't even do 100%part of speech tagging. Now this looks like a simple task, but think about the example here,the two uses of off may have different syntactic categories if youtry to make a fine grained distinctions. It's not that easy to figureout such differences. It's also hard to dogeneral complete parsing. And again, the same sentencethat you saw before is example. This ambiguity can be very hard todisambiguate and you can imagine example where you have to use a lot of knowledgein the context of the sentence or from the background, in order to figureout who actually had the telescope. So although the sentence looks verysimple, it actually is pretty hard. And in cases when the sentence isvery long, imagine it has four or five prepositional phrases, and thereare even more possibilities to figure out. It's also harder to do precisedeep semantic analysis. So here's an example. In the sentence ""John owns a restaurant.""How do we define owns exactly? The word own,it is something that we can understand but it's very hard to precisely describethe meaning of own for computers. So as a result we have a robust anda general Natural Language Processing techniquesthat can process a lot of text data. In a shallow way,meaning we only do superficial analysis. For example, parts of speech tagging or apartial parsing or recognizing sentiment. And those are not deep understanding, because we're not really understandingthe exact meaning of the sentence. On the other hand of the deepunderstanding techniques tend not to scale up well, meaning that they wouldfill only some restricted text. And if you don't restrictthe text domain or the use of words, then thesetechniques tend not to work well. They may work well based on machinelearning techniques on the data that are similar to the training datathat the program has been trained on. But they generally wouldn't work well onthe data that are very different from the training data. So this pretty much summarizes the stateof the art of Natural Language Processing. Of course, within such a short amountof time we can't really give you a complete view of NLP,which is a big field. And I'd expect to see multiple courses onNatural Language Processing topic itself. But because of its relevance to the topicthat we talk about, it's useful for you to know the background in caseyou happen to be exposed to that. So what does that mean for Text Retrieval? Well, in Text Retrieval weare dealing with all kinds of text. It's very hard to restricttext to a certain domain. And we also are often dealingwith a lot of text data. So that means The NLP techniques mustbe general, robust, and efficient. And that just implies today we can onlyuse fairly shallow NLP techniques for text retrieval. In fact, most search engines today use somethingcalled a bag of words representation. Now, this is probably the simplestrepresentation you can possibly think of. That is to turn text datainto simply a bag of words. Meaning we'll keep individual words, butwe'll ignore all the orders of words. And we'll keep duplicatedoccurrences of words. So this is called a bagof words representation. When you represent text in this way,you ignore a lot of valid information. That just makes it harder to understandthe exact meaning of a sentence because we've lost the order. But yet this representation tendsto actually work pretty well for most search tasks. And this was partly because the searchtask is not all that difficult. If you see matching of some ofthe query words in a text document, chances are that that document is aboutthe topic, although there are exceptions. So in comparison of some other tasks, for example, machine translation would requireyou to understand the language accurately. Otherwise the translation would be wrong. So in comparison such tasksare all relatively easy. Such a representation is often sufficientand that's also the representation that the major search engines today,like a Google or Bing are using. Of course, I put in parentheses butnot all, of course there are many queries that are not answered well bythe current search engines, and they do require the replantation thatwould go beyond bag of words replantation. That would require more naturallanguage processing to be done. There was another reason why wehave not used the sophisticated NLP techniques in modern search engines. And that's because someretrieval techniques actually, naturally solved the problem of NLP. So one example is wordsense disintegration. Think about a word like Java. It could mean coffee orit could mean program language. If you look at the word anome,it would be ambiguous, but when the user uses the word in the query,usually there are other words. For example, I'm looking forusage of Java applet. When I have applet there,that implies Java means program language. And that contest can help usnaturally prefer documents which Java is referringto program languages. Because those documents wouldprobably match applet as well. If Java occurs in thatdocuments where it means coffee then you would never match applet orwith very small probability. So this is the case whensome retrieval techniques naturally achieve the goal of word. Another example is some technique called feedback which we will talk aboutlater in some of the lectures. This technique would allow us to addadditional words to the query and those additional words couldbe related to the query words. And these words can help matchingdocuments where the original query words have not occurred. So this achieves, to some extent,semantic matching of terms. So those techniques also helped us bypass some of the difficultiesin natural language processing. However, in the long run we still needa deeper natural language processing techniques in order to improve theaccuracy of the current search engines. And it's particularly needed forcomplex search tasks. Or for question and answering. Google has recently launched a knowledgegraph, and this is one step toward that goal, because knowledge graph wouldcontain entities and their relations. And this goes beyond the simplebag of words replantation. And such technique should help usimprove the search engine utility significantly, although this is the opentopic for research and exploration. In sum, in this lecture wetalked about what is NLP and we've talked about the stateof that techniques. What we can do, what we cannot do. And finally, we also explain whythe bag of words replantation remains the dominant replantationused in modern search engines, even though deeper NLP would be needed forfuture search engines. If you want to know more, you can takea look at some additional readings. I only cited one here andthat's a good starting point. Thanks. [MUSIC]"
cs-410,1,3,"[SOUND] In this lecture,we're going to talk about the text access. In the previous lecture, we talked aboutthe natural language content, analysis. We explained that the state of the arenatural language processing techniques are still not good enough to processa lot of unrestricted text data in a robust manner. As a result, bag of words remains very popular inapplications like a search engine. In this lecture, we're going to talkabout some high-level strategies to help users get access to the text data. This is also important step to convertraw big text data into small random data. That are actually neededin a specific application. So the main question we'll address here,is how can a text information system, help usersget access to the relevant text data? We're going to cover two complimentarystrategies, push versus pull. And then we're going to talk abouttwo ways to implement the pull mode, querying versus browsing. So first push versus pull. These are two different ways connectthe users with the right information at the right time. The difference is whichtakes the initiative, which party takes the initiative. In the pull mode, the users take the initiative tostart the information access process. And in this case, a user typically woulduse a search engine to fulfill the goal. For example,the user may type in the query and then browse the results tofind the relevant information. So this is usually appropriate for satisfying a user's adhoc information need. An ad hoc information need isa temporary information need. For example, you want to buy a product so you suddenly have a need to readreviews about related product. But after you have cracked information,you have purchased in your product. You generally no longerneed such information, so it's a temporary information need. In such a case, it's very hard fora system to predict your need, and it's more proper forthe users to take the initiative, and that's why search engines are very useful. Today because many people have manyinformation needs all the time. So as we're speaking Google is probablyprocessing many queries from this. And those are all, or mostly adequate. Information needs. So this is a pull mode. In contrast in the push mode inthe system would take the initiative to push the information to the user orto recommend information to the user. So in this case this is usuallysupported by a recommender system. Now this would be appropriate if. The user has a stable information. For example you may have a researchinterest in some topic and that interest tends to stay for a while. So, it's rather stable. Your hobby is another example of. A stable information need is such a casethe system can interact with you and can learn your interest, andthen to monitor the information stream. If the system hasn't seen anyrelevant items to your interest, the system could then take the initiativeto recommend the information to you. So, for example, a news filter or news recommended system couldmonitor the news stream and identify interesting news to you andsimply push the news articles to you. This mode of information access may bealso a property that when this system has good knowledge about the users needand this happens in the search context. So for example, when you search forinformation on the web a search engine might infer you might bealso interested in something related. Formation. And they would recommend the informationto you, so that just reminds you, for example, of an advertisementplaced on the search page. So this is about the two high levelstrategies or two modes of text access. Now let's look at the pullmode in more detail. In the pull mode, we can furtherdistinguish it two ways to help users. Querying versus browsing. In querying,a user would just enter a query. Typical the keyword query, and the search engine system wouldreturn relevant documents to use. And this works well when the user knowswhat exactly are the keywords to be used. So if you know exactlywhat you are looking for, you tend to know the right keywords. And then query works very well,and we do that all of the time. But we also know that sometimesit doesn't work so well. When you don't know the rightkeywords to use in the query, or you want to browse informationin some topic area. You use because browsingwould be more useful. So in this case, in the case of browsing,the users would simply navigate it, into the relevant informationby following the paths supported by the structures of documents. So the system would maintainsome kind of structures and then the user could followthese structures to navigate. So this really works well when the userwants to explore the information space or the user doesn't know whatare the keywords to using the query. Or simply because the user finds itinconvenient to type in a query. So even if a user knows what query totype in if the user is using a cellphone to search for information. It's still harder to enter the query. In such a case, again,browsing tends to be more convenient. The relationship between browsing andquerying is best understood by making and imagine you're site seeing. Imagine if you're touring a city. Now if you know the exactaddress of attraction. Taking a taxi there isperhaps the fastest way. You can go directly to the site. But if you don't know the exact address,you may need to walk around. Or you can take a taxi to a nearbyplace and then walk around. It turns out that we do exactlythe same in the information studies. If you know exactly what youare looking for, then you can use the right keywords in your queryto find the information you're after. That's usually the fastest way to do,find information. But what if you don't knowthe exact keywords to use? Well, you clearly probably won't so well. You will not related pages. And then, you need to also walkaround in the information space, meaning by following the links orby browsing. You can then finally getinto the relevant page. If you want to learn about again. You will likely do a lot of browsing so just like you are looking around insome area and you want to see some interesting attractionsrelated in the same. [INAUDIBLE]. So this analogy also tells us thattoday we have very good support for query, but we don't really havegood support for browsing. And this is because in orderto browse effectively, we need a map to guide us,just like you need a map to. Of Chicago, through the city of Chicago, you need atopical map to tour the information space. So how to construct such a topicalmap is in fact a very interesting research question that might bring us more interesting browsing experienceon the web or in applications. So, to summarize this lecture, we've talked about the two high levelstrategies for text access; push and pull. Push tends to be supported bythe Recommender System, and Pull tends to be supportedby the Search Engine. Of course, in the sophisticated[INAUDIBLE] information system, we should combine the two. In the pull mode, we can further this[INAUDIBLE] Querying and Browsing. Again we generally want to combinethe two ways to help you assist, so that you can supportthe both querying nad browsing. If you want to know more aboutthe relationship between pull and push, you can read this article. This give excellent discussion of therelationship between machine filtering and information retrieval. Here informational filtering is similarto information recommendation or the push mode of information access. [MUSIC]"
cs-410,1,4,"[MUSIC] This lecture is aboutthe text retrieval problem. This picture shows our overall plan forlectures. In the last lecture, we talked aboutthe high level strategies for text access. We talked about push versus pull. Such engines are the main tools forsupporting the pull mode. Starting from this lecture, we're going to talk about the howsearch engines work in detail. So first it's aboutthe text retrieval problem. We're going to talk aboutthe three things in this lecture. First, we define Text Retrieval. Second we're going to make a comparisonbetween Text Retrieval and the related task Database Retrieval. Finally, we're going to talk aboutthe Document Selection versus Document Ranking as two strategies forresponding to a user's query. So what is Text Retrieval? It should be a task that's familiar for the most of us because we're usingweb search engines all the time. So text retrieval is basically a task where the system would respond toa user's query With relevant documents. Basically, it's for supporting a query as one way to implement the pollmode of information access. So the situation is the following. You have a collection oftext retrieval documents. These documents could be allthe webpages on the web, or all the literature articlesin the digital library. Or maybe all the textfiles in your computer. A user will typically give a query tothe system to express information need. And then, the system would returnrelevant documents to users. Relevant documents refer to thosedocuments that are useful to the user who typed in the query. All this task is a phone callthat information retrieval. But literally information retrieval wouldbroadly include the retrieval of other non-textual information as well,for example audio, video, etc. It's worth noting thatText Retrieval is at the core of information retrieval inthe sense that other medias such as video can be retrieved byexploiting the companion text data. So for example,current the image search engines actually match a user's query wasthe companion text data of the image. This problem is alsocalled search problem. And the technology is often calledthe search technology industry. If you ever take a course in databases it will be useful to pausethe lecture at this point and think about the differences betweentext retrieval and database retrieval. Now these two tasksare similar in many ways. But, there are some important differences. So, spend a moment to think aboutthe differences between the two. Think about the data, and the informationmanaged by a search engine versus those that are managedby a database system. Think about the different betweenthe queries that you typically specify for database system versus queries thatare typed in by users in a search engine. And then finally think about the answers. What's the difference between the two? Okay, so if we think about the informationor data managed by the two systems, we will see that in text retrieval. The data is unstructured, it's free text. But in databases, they are structured datawhere there is a clear defined schema to tell you this column is the namesof people and that column is ages, etc. The unstructured text is not obvious what are the names of peoplementioned in the text. Because of this difference, we also seethat text information tends to be more ambiguous and we talk about that in theprocessing chapter, whereas in databases. But they don't tend to havewhere to find the semantics. The results importantdifference in the queries, and this is partly due to the differencein the information or data. So test queries tend to be ambiguous. Whereas in their research,the queries are typically well-defined. Think about a SQL query that would clearlyspecify what records to be returned. So it has very well-defined semantics. Keyword queries or electronic queries tend to be incomplete,also in that it doesn't really specify what documentsshould be retrieved. Whereas complete specification forwhat should be returned. And because of these differences,the answers would be also different. Being the case of text retrieval, we'relooking for it rather than the documents. In the database search,we are retrieving records or match records with the sequelquery more precisely. Now in the case of text retrieval,what should be the right answers to the query is not very well specified,as we just discussed. So it's unclear what should bethe right answers to a query. And this has very important consequences,and that is, textual retrieval isan empirically defined problem. So this is a problem becauseif it's empirically defined, then we can not mathematically prove onemethod is better than another method. That also means we must relyon empirical evaluation involving users to knowwhich method works better. And that's why we have. You need more than one lecturesto cover the issue of evaluation. Because this is very important topic forSir Jennings. Without knowing how to evaluate heroismproperly, there's no way to tell whether we have got the better orwhether one system is better than another. So now let's look atthe problem in a formal way. So, this slide shows a formal formulationof the text retrieval problem. First, we have our vocabulary set, whichis just a set of words in a language. Now here,we are considering only one language, but in reality, on the web,there might be multiple natural languages. We have texts that are inall kinds of languages. But here for simplicity, we justassume that is one kind of language. As the techniques used for retrievingdata from multiple languages Are more or less similar to the techniques used forretrieving documents in one end, which although there is important difference,the principle methods are very similar. Next, we have the query,which is a sequence of words. And so here, you can see the query is defined asa sequence of words. Each q sub i is a word in the vocabulary. A document is defined in the same way,so it's also a sequence of words. And here,d sub ij is also a word in the vocabulary. Now typically, the documentsare much longer than queries. But there are also cases wherethe documents may be very short. So you can think about whatmight be a example of that case. I hope you can think of Twitter search. Tweets are very short. But in general,documents are longer than the queries. Now, then we havea collection of documents, and this collection can be very large. So think about the web. It could be very large. And then the goal of text retrievalis you'll find the set of relevant in the documents, which we denote by R'(q),because it depends on the query. And this in general, a subset of allthe documents in the collection. Unfortunately, this set of relevantdocuments is generally unknown, and user-dependent in the sense that,for the same query typed in by different users, they expectthe relevant documents may be different. The query given to us bythe user is only a hint on which document should be in this set. And indeed, the user is generallyunable to specify what exactly should be in this set, especially in the caseof web search, where the connection's so large, the user doesn't have completeknowledge about the whole production. So the best search systemcan do is to compute an approximation of thisrelevant document set. So we denote it by R'(q). So formerly,we can see the task is to compute this R'(q) approximation ofthe relevant documents. So how can we do that? Now imagine if you are now askedto write a program to do this. What would you do? Now think for a moment. Right, so these are your input. The query, the documents. And then you are to computethe answers to this query, which is a set of documents thatwould be useful to the user. So, how would you solve the problem? Now in general,there are two strategies that we can use. The first strategy is we do a documentselection, and that is, we're going to have a binary classificationfunction, or binary classifier. That's a function thatwould take a document and query as input, and then give a zero or one as output to indicate whether thisdocument is relevant to the query or not. So in this case, you can see the document. The relevant document is set,is defined as follows. It basically, all the documents thathave a value of 1 by this function. So in this case, you can see the system must have decideif the document is relevant or not. Basically, it has to saywhether it's one or zero. And this is called absolute relevance. Basically, it needs to knowexactly whether it's going to be useful to the user. Alternatively, there's anotherstrategy called document ranking. Now in this case, the system is not going to make a callwhether a document is random or not. But rather the system is going touse a real value function, f here. That would simply give us a value that would indicate whichdocument is more likely relevant. So it's not going to make a call whetherthis document is relevant or not. But rather it would say whichdocument is more likely relevant. So this function then can beused to random documents, and then we're going to letthe user decide where to stop, when the user looks at the document. So we have a threshold thetahere to determine what documents should be inthis approximation set. And we're going to assumethat all the documents that are ranked above the thresholdare in this set, because in effect, these are the documents thatwe deliver to the user. And theta is a cutoffdetermined by the user. So here we've got some collaborationfrom the user in some sense, because we don't really make a cutoff. And the user kind of helpedthe system make a cutoff. So in this case,the system only needs to decide if one document is morelikely relevant than another. And that is, it only needs todetermine relative relevance, as opposed to absolute relevance. Now you can probably already sense that relative relevance would be easier todetermine than absolute relevance. Because in the first case, we have to say exactly whethera document is relevant or not. And it turns out that ranking is indeedgenerally preferred to document selection. So let's look at these twostrategies in more detail. So this picture shows how it works. So on the left side,we see these documents, and we use the pluses to indicatethe relevant documents. So we can see the true relevantdocuments here consists this set of true relevant documents, consistsof these process, these documents. And with the document selection function, we're going to basicallyclassify them into two groups, relevant documents, and non-relevant ones. Of course, the classified will notbe perfect so it will make mistakes. So here we can see, in the approximationof the relevant documents, we have got some number in the documents. And similarly, there is a relevant document that'smisclassified as non-relevant. In the case of document ranking,we can see the system seems like, simply ranks all the documents inthe descending order of the scores. And then, we're going to let the userstop wherever the user wants to stop. If the user wants toexamine more documents, then the user will scroll down somemore and then stop [INAUDIBLE]. But if the user only wants toread a few random documents, the user might stop at the top position. So in this case, the user stops at d4. So in fact, we have deliveredthese four documents to our user. So as I said ranking is generallypreferred, and one of the reasons is because the classifier in the case ofdocument selection is unlikely accurate. Why? Because the only clueis usually the query. But the query may not be accurate in thesense that it could be overly constrained. For example, you might expect relevantdocuments to talk about all these topics by using specific vocabulary. And as a result,you might match no relevant documents. Because in the collection, no others have discussed the topicusing these vocabularies, right? So in this case,we'll see there is this problem of no relevant documents to return inthe case of over-constrained query. On the other hand,if the query is under-constrained, for example, if the query does not have sufficient descriptivewords to find the random documents. You may actually end up having ofover delivery, and this when you thought these words my be sufficientto help you find the right documents. But, it turns out theyare not sufficient and there are many distractions,documents using similar words. And so, this is a case of over delivery. Unfortunately, it's very hard to find theright position between these two extremes. Why? Because whether users looking forthe information in general the user does not have a good knowledge aboutthe information to be found. And in that case, the user does nothave a good knowledge about what vocabularies will be used inthose relevent documents. So it's very hard for a user to pre-specify the rightlevel of constraints. Even if the classifier is accurate,we also still want to rend these relevant documents, because theyare generally not equally relevant. Relevance is often a matter of degree. So we must prioritize these documents fora user to examine. And note that thisprioritization is very important because a user cannotdigest all the content the user generally would have tolook at each document sequentially. And therefore, it would make sense tousers with the most relevant documents. And that's what ranking is doing. So for these reasons,ranking is generally preferred. Now this preference also hasa theoretical justification and this is given by the probabilityranking principle. In the end of this lecture,there is reference for this. This principle says, returning a rankedlist of documents in descending order of probability that a documentis relevant to the query is the optimal strategy underthe following two assumptions. First, the utility ofa document (to a user) Is independent of the utilityof any other document. Second, a user would be assumed tobrowse the results sequentially. Now it's easy to understand why theseassumptions are needed in order to justify Site for the ranking strategy. Because if the documents are independent, then we can evaluate the utilityof each document that's separate. And this would allow the computerscore for each document independently. And then, we are going to rank thesedocuments based on the scrolls. The second assumption is to say that theuser would indeed follow the rank list. If the user is not going to followthe ranked list, is not going to examine the documents sequentially, then obviouslythe ordering would not be optimal. So under these two assumptions, we cantheoretically justify the ranking strategy is, in fact, the best that you could do. Now, I've put one question here. Do these two assumptions hold? I suggest you to pause the lecture,for a moment, to think about this. Now, can you think ofsome examples that would suggest these assumptionsaren't necessarily true. Now, if you think for a moment, you may realize none ofthe assumptions Is actually true. For example, in the case ofindependence assumption we might have documents that have similar orexactly the same content. If we look at each of them alone,each is relevant. But if the user has already seenone of them, we can assume it's generally not very useful for the user tosee another similar or duplicated one. So clearly the utilityon the document that is dependent on other documentsthat the user has seen. In some other cases you might seea scenario where one document that may not be useful to the user, but when threeparticular documents are put together. They provide answers tothe user's question. So this is a collective relevance andthat also suggests that the value of the document mightdepend on other documents. Sequential browsing generally would makesense if you have a ranked list there. But even if you have a rank list,there is evidence showing that users don't always just go strictlysequentially through the entire list. They sometimes will look at the bottom forexample, or skip some. And if you think about the morecomplicated interfaces that we could possibly use liketwo dimensional in the phase. Where you can put that additionalinformation on the screen then sequential browsing is a veryrestricted assumption. So the point here is that none of these assumptions isreally true but less than that. But probability ranking principleestablishes some solid foundation for ranking as a primary pattern forsearch engines. And this has actually been the basis for a lot of research work ininformation retrieval. And many hours have been designedbased on this assumption, despite that the assumptionsaren't necessarily true. And we can address this problemby doing post processing Of a ranked list, for example,to remove redundancy. So to summarize this lecture, the main points that you cantake away are the following. First, text retrieval isan empirically defined Problem. And that means which algorithm isbetter must be judged by the users. Second, document rankingis generally preferred. And this will help users prioritizeexamination of search results. And this is also to bypass the difficultyin determining absolute relevance Because we can get some help from usersin determining where to make the cut off, it's more flexible. So, this further suggests that the maintechnical challenge in designing a search engine is the designeffective ranking function. In other words, we need to definewhat is the value of this function F on the query and document pair. How we design such a function is the maintopic in the following lectures. There are two suggestedadditional readings. The first is the classical paper onthe probability ranking principle. The second one is a must-read for anyonedoing research on information retrieval. It's a classic IR book, which hasexcellent coverage of the main research and results in early days up tothe time when the book was written. Chapter six of this book hasan in-depth discussion of the Probability Ranking Principle andProbably for retrieval models in general. [MUSIC]"
cs-410,1,5,"[SOUND] This lecture is a overview oftext retrieval methods. In the previous lecture, we introducedthe problem of text retrieval. We explained that the main problem is the design of ranking functionto rank documents for a query. In this lecture, we will give an overview of differentways of designing this ranking function. So the problem is the following. We have a query that hasa sequence of words and the document that's alsoa sequence of words. And we hope to define a function f that can compute a score basedon the query and document. So the main challenge you hear is withdesign a good ranking function that can rank all the relevant documentson top of all the non-relevant ones. Clearly, this means our functionmust be able to measure the likelihood that a documentd is relevant to a query q. That also means we have to havesome way to define relevance. In particular, in order toimplement the program to do that, we have to have a computationaldefinition of relevance. And we achieve this goal bydesigning a retrieval model, which gives usa formalization of relevance. Now, over many decades, researchers have designed manydifferent kinds of retrieval models. And they fall into different categories. First, one family of the modelsare based on the similarity idea. Basically, we assume that ifa document is more similar to the query than another document is, then we will say the first documentis more relevant than the second one. So in this case,the ranking function is defined as the similarity between the query andthe document. One well known example in thiscase is vector space model, which we will cover more indetail later in the lecture. A second kind of modelsare called probabilistic models. In this family of models, we follow a verydifferent strategy, where we assume that queries and documents are allobservations from random variables. And we assume there is a binaryrandom variable called R here to indicate whether a documentis relevant to a query. We then define the score of document withrespect to a query as a probability that this random variable R is equal to 1,given a particular document query. There are different casesof such a general idea. One is classic probabilistic model,another is language model, yet another is divergencefrom randomness model. In a later lecture, we will talk moreabout one case, which is language model. A third kind of model are basedon probabilistic inference. So here the idea is to associateuncertainty to inference rules, and we can then quantifythe probability that we can show that the queryfollows from the document. Finally, there is also a family of models that are using axiomatic thinking. Here, an idea is to definea set of constraints that we hope a good retrieval function to satisfy. So in this case, the problem isto seek a good ranking function that can satisfy allthe desired constraints. Interestingly, although these differentmodels are based on different thinking, in the end, the retrieval functiontends to be very similar. And these functions tend toalso involve similar variables. So now let's take a look at the commonform of a state of the art retrieval model and to examine some of the commonideas used in all these models. First, these models are allbased on the assumption of using bag of words to represent text,and we explained this in the naturallanguage processing lecture. Bag of words representation remainsthe main representation used in all the search engines. So with this assumption,the score of a query, like a presidential campaign newswith respect to a document of d here, would be based on scores computedbased on each individual word. And that means the score woulddepend on the score of each word, such as presidential, campaign, and news. Here, we can see thereare three different components, each corresponding to how well thedocument matches each of the query words. Inside of these functions,we see a number of heuristics used. So for example, one factor thataffects the function d here is how many times does the wordpresidential occur in the document? This is called a term frequency, or TF. We might also denote asc of presidential and d. In general, if the word occursmore frequently in the document, then the value of thisfunction would be larger. Another factor is,how long is the document? And this is to use the document length forscoring. In general, if a term occurs in a long document many times,it's not as significant as if it occurred the same numberof times in a short document. Because in a long document, any termis expected to occur more frequently. Finally, there is this factorcalled document frequency. That is, we also want to look at howoften presidential occurs in the entire collection, and we call this documentfrequency, or df of presidential. And in some other models,we might also use a probability to characterize this information. So here, I show the probability ofpresidential in the collection. So all these are trying to characterizethe popularity of the term in the collection. In general, matching a rare term inthe collection is contributing more to the overall score thanmatching up common term. So this captures some of the main ideasused in pretty much older state of the art original models. So now, a natural question is,which model works the best? Now it turns out that manymodels work equally well. So here are a list ofthe four major models that are generally regarded asa state of the art original models, pivoted length normalization,BM25, query likelihood, PL2. When optimized,these models tend to perform similarly. And this was discussed in detail in thisreference at the end of this lecture. Among all these,BM25 is probably the most popular. It's most likely that this has been usedin virtually all the search engines, and you will also often see thismethod discussed in research papers. And we'll talk more about thismethod later in some other lectures. So, to summarize, the main points madein this lecture are first the design of a good ranking function pre-requires acomputational definition of relevance, and we achieve this goal by designingappropriate retrieval model. Second, many models are equally effective,but we don't have a single winner yet. Researchers are still active andworking on this problem, trying to find a trulyoptimal retrieval model. Finally, the state of the artranking functions tend to rely on the following ideas. First, bag of words representation. Second, TF anddocument frequency of words. Such information is used inthe weighting function to determine the overall contribution of matchinga word and document length. These are often combined in interestingways, and we'll discuss how exactly they are combined to rankdocuments in the lectures later. There are two suggested additionalreadings if you have time. The first is a paper where you canfind the detailed discussion and comparison of multiplestate of the art models. The second is a book witha chapter that gives a broad review of different retrieval models. [MUSIC]"
cs-410,1,6,"[SOUND] This lecture is about thevector space retrieval model. We're going to givean introduction to its basic idea. In the last lecture, we talked aboutthe different ways of designing a retrieval model, which would giveus a different arranging function. In this lecture, we're going totalk about a specific way of designing a ramping function calleda vector space retrieval model. And we're going to give a briefintroduction to the basic idea. Vector space model is a special case of similarity based modelsas we discussed before. Which means we assume relevanceis roughly similarity, between the document and the query. Now whether is this assumptionis true is actually a question. But in order to solve the search problem, we have to convert the vague notionof relevance into a more precise definition that can be implementedwith the program analogy. So in this process,we have to make a number of assumptions. This is the first assumptionthat we make here. Basically, we assume that if a documentis more similar to a query than another document. Then the first document will be assumed itwill be more relevant than the second one. And this is the basis forranking documents in this approach. Again, it's questionable whether this isreally the best definition for randoms. As we will see later thereare other ways to model randoms. The basic idea of vectors for base retrieval model is actuallyvery easy to understand. Imagine a high dimensional space whereeach dimension corresponds to a term. So here I issue a three dimensionalspace with three words, programming, library and presidential. So each term here defines one dimension. Now we can consider vectors in this,three dimensional space. And we're going to assumethat all our documents and the query will be placedin this vector space. So for example, on document mightbe represented by this vector, d1. Now this means this documentprobably covers library and presidential, butit doesn't really talk about programming. What does this mean in termsof representation of document? That just means we're going to look atour document from the perspective of this vector. We're going to ignore everything else. Basically, what we see here is onlythe vector root condition of the document. Of course,the document has all information. For example, the orders ofwords are [INAUDIBLE] model and that's because we assume thatthe [INAUDIBLE] of words will [INAUDIBLE]. So with this presentationyou can really see d1 simply suggests a [INAUDIBLE] library. Now this is different from anotherdocument which might be recommended as a different vector, d2 here. Now in this case, the document thatcovers programming and library, but it doesn't talk about presidential. So what does this remind you? Well you can probably guess the topicis likely about program language and the library is software lab library. So this shows that by usingthis vector space reproduction, we can actually capture the differencesbetween topics of documents. Now you can also imaginethere are other vectors. For example,d3 is pointing into that direction, that might be a presidential program. And in fact we can place allthe documents in this vector space. And they will be pointingto all kinds of directions. And similarly, we're going to place our query alsoin this space, as another vector. And then we're going to measure thesimilarity between the query vector and every document vector. So in this case for example, we can easily see d2 seems to bethe closest to this query vector. And therefore,d2 will be rendered above others. So this is basically the mainidea of the vector space model. So to be more precise, vector space model is a framework. In this framework,we make the following assumptions. First, we represent a document andquery by a term vector. So here a term can be any basic concept. For example, a word or a phrase oreven n gram of characters. Those are just sequence ofcharacters inside a word. Each term is assumed that willbe defined by one dimension. Therefore n terms in our vocabulary,we define N-dimensional space. A query vector would consistof a number of elements corresponding to the weightson different terms. Each document vector is also similar. It has a number of elements andeach value of each element is indicating the weight ofthe corresponding term. Here, you can see,we assume there are N dimensions. Therefore, they are N elements each corresponding to the weighton the particular term. So the relevance in this case will be assumed to be the similaritybetween the two vectors. Therefore, our ranking functionis also defined as the similarity between the query vector anddocument vector. Now if I ask you to write a programto implement this approach in a search engine. You would realize thatthis was far from clear. We haven't said a lot of things in detail, therefore it's impossible to actuallywrite the program to implement this. That's why I said, this is a framework. And this has to be refinedin order to actually suggest a particular ranking functionthat you can implement on a computer. So what does this framework not say? Well, it actually hasn't said many things that would be required in orderto implement this function. First, it did not say how we should defineor select the basic concepts exactly. We clearly assumethe concepts are orthogonal. Otherwise, there will be redundancy. For example, if two synonyms or somehowdistinguish it as two different concepts. Then they would be definingtwo different dimensions and that would clearly cause redundancy here. Or all the emphasizing ofmatching this concept, because it would be as ifyou match the two dimensions when you actually matchedone semantic concept. Secondly, it did not say how weexactly should place documents and the query in this space. Basically that show you some examplesof query and document vectors. But where exactly should the vector fora particular document point to? So this is equivalent to howto define the term weights? How do you compute the loseelement values in those vectors? This is a very important question, because term weight in the query vectorindicates the importance of term. So depending on how you assign the weight, you might prefer some termsto be matched over others. Similarly, the total word inthe document is also very meaningful. It indicates how well the termcharacterizes the document. If you got it wrong then you clearlydon't represent this document accurately. Finally, how to define the similaritymeasure is also not given. So these questions must be addressedbefore we can have a operational function that we can actuallyimplement using a program language. So how do we solve these problems is the main topic of the next lecture. [MUSIC]"
cs-410,1,7,"In this lecture we're going to talk about how to instantiatevector space model so that we can get veryspecific ranking function. So this is to continue the discussionof the vector space model, which is one particular approachto design a ranking function. And we're going to talk about howwe use the general framework of the the vector spacemodel as a guidance to instantiate the framework to derivea specific ranking function. And we're going to cover the symbolistinstantiation of the framework. So as we discussed inthe previous lecture, the vector space modelis really a framework. And this didn't say. As we discussed in the previous lecture,vector space model is really a framework. It does not say many things. So, for example, here it shows that it did not sayhow we should define the dimension. It also did not say how we placea document vector in this space. It did not say how we place a queryvector in this vector space. And, finally, it did not say how weshould measure the similarity between the query vector and the document vector. So you can imagine,in order to implement this model, we have to say specificallyhow we compute these vectors. What is exactly xi? And what is exactly yi? This will determine wherewe place a document vector, where we place a query vector. And, of course, we also need to say exactly whatshould be the similarity function. So if we can provide a definitionof the concepts that would define the dimensions and these xi's oryi's and namely weights of terms for queries and document, then we will beable to place document vectors and query vectors in this well defined space. And then,if we also specify similarity function, then we'll have a welldefined ranking function. So let's see how we can do that andthink about the instantiation. Actually, I would suggest you topause the lecture at this point, spend a couple minutes to think about. Suppose you are askedto implement this idea. You have come up with the idea of vectorspace model, but you still haven't figured out how to compute these vectors exactly,how to define the similarity function. What would you do? So, think for a couple of minutes,and then proceed. So, let's think about some simplest waysof instantiating this vector space model. First, how do we define the dimension? Well, the obvious choice is to use each word in our vocabularyto define the dimension. And show that there are Nwords in our vocabulary. Therefore, there are N dimensions. Each word defines one dimension. And this is basicallythe bag of words with Now let's look at how weplace vectors in this space. Again here, the simplest strategy is to use a Bit Vector to representboth the query and a document. And that means each element, xi and yi will be taking a valueof either zero or 1. When it's 1, it means the corresponding word ispresent in the document or in the query. When it's 0,it's going to mean that it's absent. So you can imagine if the usertypes in a few words in the query, then the query vector will onlyhave a few 1's, many, many zeros. The document vector,generally we have more 1's, of course. But it will also have many zeros sincethe vocabulary is generally very large. Many words don't reallyoccur in any document. Many words will only occasionallyoccur in a document. A lot of words will be absentin a particular document. So now we have placed the documents andthe query in the vector space. Let's look at how wemeasure the similarity. So, a commonly used similaritymeasure here is Dot Product. The Dot Product of twovectors is simply defined as the sum of the products of thecorresponding elements of the two vectors. So, here we see that it'sthe product of x1 and y1. So, here. And then, x2 multiplied by y2. And then, finally, xn multiplied by yn. And then, we take a sum here. So that's a Dot Product. Now, we can represent this in a moregeneral way using a sum here. So this is only one of the many differentways of measuring the similarity. So, now we see that we havedefined the dimensions, we have defined the vectors, and we havealso defined the similarity function. So now we finally have the simplestvector space model, which is based on the bit vector [INAUDIBLE] dot productsimilarity and bag of words [INAUDIBLE]. And the formula looks like this. So this is our formula. And that's actually a particular retrievalfunction, a ranking function right? Now we can finally implement thisfunction using a program language, and then rank the documents for query. Now, at this point you shouldagain pause the lecture to think about how we caninterpreted this score. So, we have gone through the processof modeling the retrieval problem using a vector space model. And then,we make assumptions about how we place vectors in the vector space, andhow do we define the similarity. So in the end, we've got a specificretrieval function shown here. Now, the next step is to think aboutwhether this retrieval function actually makes sense, right? Can we expect this functionto actually perform well when we used it to rank documents foruser's queries? So it's worth thinking about what isthis value that we are calculating. So, in the end, we'll get a number. But what does this number mean? Is it meaningful? So, spend a couple minutesto sort of think about that. And, of course, the general question here is do youbelieve this is a good ranking function? Would it actually work well? So, again,think about how to interpret this value. Is it actually meaningful? Does it mean something? This is related to how wellthe document matched the query. So, in order to assesswhether this simplest vector space model actually works well,let's look at the example. So, here I show some sample documents anda sample query. The query is news aboutthe presidential campaign. And we have five documents here. They cover different terms in the query. And if you look at these documents fora moment, you may realize that some documents are probably relevant, andsome others are probably not relevant. Now, if I asked you to rank thesedocuments, how would you rank them? This is basically our ideal ranking. When humans can examine the documents,and then try to rank them. Now, so think for a moment,and take a look at this slide. And perhaps by pausing the lecture. So I think most of you wouldagree that d4 and d3 are probably better than others because theyreally cover the query well. They match news,presidential and campaign. So, it looks like these documentsare probably better than the others. They should be ranked on top. And the other three d2, d1, andd5 are really not relevant. So we can also say d4 andd3 are relevant documents, and d1, d2 and d5 are non-relevant. So now let's see if our simplestvector space model could do the same, or could do something closer. So, let's first think abouthow we actually use this model to score documents. All right. Here I show two documents, d1 and d3. And we have the query also here. In the vector space model, of course wewant to first compute the vectors for these documents and the query. Now, I showed the vocabulary here as well. So these are the end dimensionsthat we'll be thinking about. So what do you think is the vector forthe query? Note that we're assumingthat we only use zero and 1 to indicate whether a term is absent orpresent in the query or in the document. So these are zero,1 bit vectors. So what do you think is the query vector? Well, the query has four words here. So for these four words,there will be a 1. And for the rest, there will be zeros. Now, what about the documents? It's the same. So d1 has two rows, news and about. So, there are two 1's here,and the rest are zeroes. Similarly, so now that we have the two vectors,let's compute the similarity. And we're going to use Do Product. So you can see when we use Dot Product, we just multiply the correspondingelements, right? So these two will be formal product, and these two willgenerate another product, and these two will generate yetanother product and so on, so forth. Now you can easily see if we do that,we actually don't have to care about these zeroes because whenever we havea zero the product will be zero. So when we take a sumover all these pairs, then the zero entries will be gone. As long as you have one zero,then the product would be zero. So, in the fact, we're justcounting how many pairs of 1 and 1. In this case, we have seen two,so the result will be 2. So what does that mean? Well, that means this number, orthe value of this scoring function, is simply the count of how many uniquequery terms are matched in the document. Because if a term is matched in thedocument, then there will be two one's. If it's not, then there willbe zero on the document side. Similarly, if the document has a term butthe term is not in the query, there will be a zero in the query vector. So those don't count. So, as a result,this scoring function basically measures how many unique queryterms are matched in a document. This is how we interpret this score. Now, we can also take a look at d3. In this case, you can see the resultis 3 because d3 matched to the three distinctive query words news, presidentialcampaign, whereas d1 only matched the two. Now in this case, this seemsreasonable to rank d3 on top of d1. And this simplest vectorspace model indeed does that. So that looks pretty good. However, if we examine this model indetail, we likely will find some problems. So, here I'm going to show allthe scores for these five documents. And you can easily verify they'recorrect because we're basically counting the number of unique queryterms matched in each document. Now note that this measureactually makes sense, right? It basically means if a documentmatches more unique query terms, then the document will beassumed to be more relevant. And that seems to make sense. The only problem is here we can note thatthere are three documents, d2, d3 and d4. And they tied with a 3 as a score. So, that's a problem because if you lookat them carefully, it seems that the d4 should be ranked above d3 because d3 only mentions the presidential once,but d4 mentioned it multiple times. In the case of d3,presidential could be an dimension. But d4 is clearly abovethe presidential campaign. Another problem is that d2 andd3 also have the same score. But if you look at the three wordsthat are matched, in the case of d2, it matched the news, about and campaign. But in the case of d3, it matched news,presidential and campaign. So intuitively this reads betterbecause matching presidential is more important than matching about, even though about andthe presidential are both in the query. So intuitively,we would like d3 to be ranked above d2. But this model doesn't do that. So that means this modelis still not good enough. We have to solve these problems. To summarize, in this lecture we talked about howto instantiate a vector space model. We mainly need to do three things. One is to define the dimension. The second is to decide how to placedocuments as vectors in the vector space, and to also place a query inthe vector space as a vector. And third is to definethe similarity between two vectors, particularly the query vector andthe document vector. We also talked about various simple wayto instantiate the vector space model. Indeed, that's probably the simplestvector space model that we can derive. In this case,we use each word to define the dimension. We use a zero, 1 bit vector torepresent a document or a query. In this case, we basically only careabout word presence or absence. We ignore the frequency. And we use the Dot Productas the similarity function. And with such a instantiation, we showed that the scoringfunction is basically to score a document based on the number of distinctquery words matched in the document. We also showed that such a simple vectorspace model still doesn't work well, and we need to improve it. And this is a topic that we'regoing to cover in the next lecture. [MUSIC]"
cs-410,2,1,"[SOUND] In this lecture, we are going to talk about howto improve the instantiation of the vector space model. This is a continued discussionof the vector space model. We're going to focus on how to improvethe instantiation of this model. In the previous lecture, you have seen that with simpleinstantiations of the vector space model, we can come up with a simple scoringfunction that would give us basically an account of how many unique queryterms are matched in the document. We also have seen that this functionhas a problem, as shown on this slide. In particular,if you look at these three documents, they will all get the same score becausethey match the three unique query words. But intuitively we would liked4 to be ranked above d3, and d2 is really not relevant. So the problem here is that this functioncouldn't capture the following heuristics. First, we would like to givemore credit to d4 because it matched presidential more times than d3. Second, intuitively, matching presidentialshould be more important than matching about, because about is a verycommon word that occurs everywhere. It doesn't really carry that much content. So in this lecture, let's see how we can improve the modelto solve these two problems. It's worth thinking at this pointabout why do we have these problems? If we look back at assumptions we havemade while instantiating the vector space model,we'll realize that the problem is really coming fromsome of the assumptions. In particular, it has to do with how weplaced the vectors in the vector space. So then naturally,in order to fix these problems, we have to revisit those assumptions. Perhaps we will have to use different waysto instantiate the vector space model. In particular, we have to placethe vectors in a different way. So let's see how we can improve this. One natural thought is in order toconsider multiple times of a term in the document, we should consider the term frequencyinstead of just the absence or presence. In order to consider the differencebetween a document where a query term occurred multiple times and onewhere the query term occurred just once, we have to consider the term frequency,the count of a term in the document. In the simplest model, we only modeledthe presence and absence of a term. We ignored the actual number of timesthat a term occurs in a document. So let's add this back. So we're going to thenrepresent a document by a vector with term frequency as element. So that is to say, now the elementsof both the query vector and the document vector will not be 0 or1s, but instead they will be the counts ofa word in the query or the document. So this would bring in additionalinformation about the document, so this can be seen as more accuraterepresentation of our documents. So now let's see what the formulawould look like if we change this representation. So as you'll see on this slide,we still use dot product. And so the formula looksvery similar in the form. In fact, it looks identical. But inside the sum, of course,x i and y i are now different. They are now the counts of word i in the query and in the document. Now at this point I also suggest youto pause the lecture for a moment and just to think about how we can interpretthe score of this new function. It's doing something very similarto what the simplest VSM is doing. But because of the change of the vector, now the new score hasa different interpretation. Can you see the difference? And it has to do with the considerationof multiple occurrences of the same term in a document. More importantly, we would like to knowwhether this would fix the problems of the simplest vector space model. So let's look at this example again. So suppose we change the vectorrepresentation to term frequency vectors. Now let's look at thesethree documents again. The query vector is the samebecause all these words occurred exactly once in the query. So the vector is still a 01 vector. And in fact, d2 is also essentiallyrepresenting the same way because none of these wordshas been repeated many times. As a result,the score is also the same, still 3. The same is true for d3,and we still have a 3. But d4 would be different, becausenow presidential occurred twice here. So the ending for presidential in thedocument vector would be 2 instead of 1. As a result, now the score ford4 is higher. It's a 4 now. So this means by using term frequency, we can now rank d4 above d2 andd3, as we hoped to. So this solved the problem with d4. But we can also see that d2 andd3 are still filtering the same way. They still have identical scores,so it did not fix the problem here. So how can we fix this problem? Intuitively, we would liketo give more credit for matching presidential than matching about. But how can we solvethe problem in a general way? Is there any way to determinewhich word should be treated more importantly andwhich word can be basically ignored? About is such a word which does notreally carry that much content. We can essentially ignore that. We sometimes call sucha word a stock word. Those are generally very frequent andthey occur everywhere. Matching it doesn't really mean anything. But computationally howcan we capture that? So again, I encourage you tothink a little bit about this. Can you came up with any statisticalapproaches to somehow distinguish presidential from about? Now if you think about it for a moment, you'll realize that one difference isthat a word like above occurs everywhere. So if you count the occurrence ofthe word in the whole collection, then we will see that about has muchhigher frequency than presidential, which tends to occuronly in some documents. So this idea suggeststhat we could somehow use the global statistics of terms or some other informationto trying to down-weight the element of about ina vector representation of d2. At the same time,we hope to somehow increase the weight of presidentialin the vector of d3. If we can do that, then we canexpect that d2 will get the overall score to be less than 3 whiled3 will get the score above 3. Then we would be able torank d3 on top of d2. So how can we do this systematically? Again, we can rely onsome statistical count. And in this case, the particular ideais called inverse document frequency. Now we have seen documentfrequency as one signal used in the modern retrieval functions. We discussed this in a previous lecture. So here is the specific way of using it. Document frequency is the count ofdocuments that contain a particular term. Here we say inverse document frequencybecause we actually want to reward a word that doesn't occur in many documents. And so the way to incorporate thisinto our vector representation is then to modify the frequencycount by multiplying it by the IDF of the corresponding word,as shown here. If we can do that,then we can penalize common words, which generally have a lower IDF, and reward rare words,which will have a higher IDF. So more specifically, the IDF can be defined asthe logarithm of M+1 divided by k, where M is the total number of documentsin the collection, k is the DF or document frequency, the total numberof documents containing the word W. Now if you plot thisfunction by varying k, then you would see the curvewould look like this. In general, you can see itwould give a higher value for a low DF word, a rare word. You can also see the maximum valueof this function is log of M+1. It would be interesting for you to thinkabout what's the minimum value for this function. This could be an interesting exercise. Now the specific functionmay not be as important as the heuristic to simplypenalize popular terms. But it turns out that this particularfunction form has also worked very well. Now whether there's a betterform of function here is the open research question. But it's also clear that ifwe use a linear penalization, like what's shown here with this line, then it may not be asreasonable as the standard IDF. In particular, you can seethe difference in the standard IDF, and we somehow havea turning point of here. After this point, we're going to say theseterms are essentially not very useful. They can be essentially ignored. And this makes sense whenthe term occurs so frequently and let's say a term occurs in morethan 50% of the documents, then the term is unlikely very importantand it's basically a common term. It's not very importantto match this word. So with the standard IDF you cansee it's basically assumed that they all have low weights. There's no difference. But if you look atthe linear penalization, at this point that thereis still some difference. So intuitively we'd want tofocus more on the discrimination of low DF words ratherthan these common words. Well, of course,which one works better still has to be validated by using the empiricallycorrelated dataset. And we have to use users tojudge which results are better. So now let's see howthis can solve problem 2. So now let's look atthe two documents again. Now without the IDF weighting before,we just have term frequency vectors. But with IDF weighting wenow can adjust the TF weight by multiplying with the IDF value. For example,here we can see is adjustment and in particular for about there's adjustmentby using the IDF value of about, which is smaller than the IDFvalue of presidential. So if you look at these,the IDF will distinguish these two words. As a result, adjustment here would belarger, would make this weight larger. So if we score with these new vectors,then what would happen is that, of course,they share the same weights for news and campaign, but the matching ofabout will discriminate them. So now as a result of IDF weighting,we will have d3 to be ranked above d2 because it matched a rare word,whereas d2 matched a common word. So this shows that the IDFweighting can solve problem 2. So how effective is this model ingeneral when we used TF-IDF weighting? Well, let's look at all thesedocuments that we have seen before. These are the new scoresof the new documents. But how effective is this new weightingmethod and new scoring function point? So now let's see overall how effectiveis this new ranking function with TF-IDF weighting. Here we show all the five documentsthat we have seen before, and these are their scores. Now we can see the scores for the first four documents hereseem to be quite reasonable. They are as we expected. However, we also see a newproblem because now d5 here, which did not have a very high scorewith our simplest vector space model, now actually has a very high score. In fact, it has the highest score here. So this creates a new problem. This is actually a common phenomenonin designing retrieval functions. Basically, when you tryto fix one problem, you tend to introduce other problems. And that's why it's very tricky howto design effective ranking function. And what's the best ranking functionis their open research question. Researchers are still working on that. But in the next few lectures we're goingto also talk about some additional ideas to further improve this model andtry to fix this problem. So to summarize this lecture, we've talkedabout how to improve the vector space model, andwe've got to improve the instantiation of the vector space modelbased on TD-IDF weighting. So the improvement is mostly onthe placement of the vector where we give high weight to a term thatoccurred many times in a document but infrequently in the whole collection. And we have seen that thisimproved model indeed looks better than the simplestvector space model. But it also still has some problems. In the next lecture we're going to look athow to address these additional problems. [MUSIC]"
cs-410,2,2,"[MUSIC] In this lecture, we continuethe discussion of vector space model. In particular, we're going totalk about the TF transformation. In the previous lecture, we have derived a TF idea of weightingformula using the vector space model. And we have assumed that this modelactually works pretty well for these examples as shown on this slide,except for d5, which has received a very high score. Indeed, it has received the highestscore among all these documents. But this document is intuitive andnon-relevant, so this is not desirable. In this lecture,we're going to talk about, how we're going to use TFtransformation to solve this problem. Before we discuss the details,let's take a look at the formula for this simple TF-IDFweighting ranking function. And see why this document hasreceived such a high score. So this is the formula, andif you look at the formula carefully, then you will see it involves a sumover all the matched query terms. And inside the sum, each matchedquery term has a particular weight. And this weight is TF-IDF weighting. So it has an idea of component,where we see two variables. One is the total number of documentsin the collection, and that is M. The other is the document of frequency. This is the number ofdocuments that are contained. This word w. The other variables involved in the formula includethe count of the query term. W in the query, andthe count of the word in the document. If you look at this document again,now it's not hard to realize that the reason why it hasn'treceived a high score is because it has a very high count of campaign. So the count of campaign in this documentis a 4, which is much higher than the other documents, and has contributedto the high score of this document. So in treating the amountto lower the score for this document, we need to somehowrestrict the contribution of the matching of thisterm in the document. And if you think about the matchingof terms in the document carefully, you actually would realize, we probably shouldn't rewardmultiple occurrences so generously. And by that I mean,the first occurrence of a term says a lot aboutthe matching of this term, because it goes from zerocount to a count of one. And that increase means a lot. Once we see a word in the document, it's very likely that the documentis talking about this word. If we see a extra occurrence ontop of the first occurrence, that is to go from one to two,then we also can say that, well the second occurrence kind of confirmed that it'snot a accidental managing of the word. Now we are more sure that thisdocument is talking about this word. But imagine we have seen, let's say,50 times of the word in the document. Now, adding one extra occurrence is notgoing to test more about the evidence, because we're already sure thatthis document is about this word. So if you're thinking this way, it seemsthat we should restrict the contribution of a high count of a term, andthat is the idea of TF Transformation. So this transformation function isgoing to turn the real count of word into a term frequency weight forthe word in the document. So here I show in x axis that we'll count,and y axis I show the term frequency weight. So in the previous breaking functions, we actually have imprison rateuse some kind of transformation. So for example,in the 0/1 bit vector recantation, we actually use such a transformationfunction, as shown here. Basically if the count is 0,then it has 0 weight, otherwise it would have a weight of 1. It's flat. Now, what about usingterm count as TF weight? Well, that's a linear function, so it hasjust exactly the same weight as the count. Now we have just seen thatthis is not desirable. So what we want is something like this. So for example,with an algorithm function, we can't have a sublineartransformation that looks like this. And this will control the influenceof really high weight, because it's going to lower its inference. Yet, it will retainthe inference of small counts. Or we might want to even bend the curvemore by applying logarithm twice. Now people have tried all these methods. And they are indeed working better thanthe linear form of the transformation. But so far, what works the best seemsto be this special transformation, called a BM25 transformation. BM stands for best matching. Now in this transformation,you can see there's a parameter k here. And this k controls the upperbound of this function. It's easy to see thisfunction has a upper bound, because if you look at the x divided byx + k, where k is a non-active number, then the numerator will never be ableto exceed the denominator, right? So it's upper bounded by k+1. Now, this is also difference betweenthis transformation function and a logarithm transformation. Which it doesn't have upper bound. Furthermore, one interesting propertyof this function is that, as we vary k, we can actually simulate differenttransformation functions. Including the two extremesthat are shown here. That is, the 0/1 bit transformation andthe linear transformation. So for example, if we set k to 0,now you can see the function value will be 1. So we precisely recoverthe 0/1 bit transformation. If you set k to very largenumber on the other hand, it's going to look more likethe linear transformation function. So in this sense,this transformation is very flexible. It allows us to controlthe shape of the transformation. It also has a nice propertyof the upper bound. And this upper bound is useful to controlthe inference of a particular term. And so that we can prevent a spammerfrom just increasing the count of one term to spam all queriesthat might match this term. In other words, this upper boundmight also ensure that all terms would be counted when we aggregatethe weights to compute the score. As I said, this transformationfunction has worked well so far. So to summarize this lecture,the main point is that we need to do Sublinear TF Transformation,and this is needed to capture the intuition of diminishingreturn from higher term counts. It's also to avoid the dominance byone single term over all others. This BM25 transformation that wetalked about is very interesting. It's so far one of the best-performingTF Transformation formulas. It has upper bound, and soit's also robust and effective. Now if we're plugging this function intoour TF-IDF weighting vector space model. Then we'd end up havingthe following ranking function, which has a BM25 TF component. Now, this is alreadyvery close to a state of the odd ranking function called BM25. And we'll discuss how we can furtherimprove this formula in the next lecture. [MUSIC]"
cs-410,2,3,"[SOUND] This lecture is about Document Length Normalizationin the Vector Space Model. In this lecture, we will continuethe discussion of the vector space model. In particular, we're going to discuss theissue of document length normalization. So far in the lectures about the vectorspace model, we have used the various signals from the document to assessthe matching of the document with a query. In particular,we have considered the tone frequency. The count of a tone in a document. We have also considered it'sglobal statistics such as, IDF, Inverse Document Frequency. But we have not considereddocument lengths. So here I show two example documents,d4 is much shorter with only 100 words. D6 on the other hand, has a 5000 words. If you look at the matchingof these query words, we see that in d6, there are morematchings of the query words. But one might reason that,d6 may have matched these query words in a scattered manner. So maybe the topic of d6, is notreally about the topic of the query. So, the discussion of the campaignat the beginning of the document, may have nothing to do with the managingof presidential at the end. In general,if you think about the long documents, they would have a higher chance formatching any query. In fact, if you generate a long documentrandomly by assembling words from a distribution of words, then eventuallyyou probably will match an inquiry. So in this sense, we should penalize ondocuments because they just naturally have better chance matching to any query, andthis is idea of document normalization. We also need to be careful in avoidingto over penalize long documents. On the one hand,we want to penalize the long document. But on the other hand,we also don't want to over-penalize them. Now, the reasoning is becausea document that may be long because of different reasons. In one case, the document may belong because it uses more words. So for example, think about the vortexarticle on the research paper. It would use more words thanthe corresponding abstract. So, this is a case where we probablyshould penalize the matching of long documents such as a full paper. When we compare the matchingof words in such a long document with matching ofthe words in the shop abstract. Then long papers in general, have a higher chance of matching clearerwords, therefore, we should penalize them. However, there is another casewhen the document is long, and that is when the documentsimply has more content. Now consider anothercase of long document, where we simply concatenate a lotof abstracts of different papers. In such a case, obviously, we don't wantto over-penalize such a long document. Indeed, we probably don't want to penalizesuch a document because it's long. So that's why, we need to be careful aboutusing the right degree of penalization. A method of that has been working well,based on recent results, is called a pivoted length normalization. And in this case, the idea is to use the average documentlength as a pivot, as a reference point. That means we'll assume that forthe average length documents, the score is about right sothe normalizer would be 1. But if the document is longerthan the average document length, then there will be some penalization. Whereas if it's a shorter,then there is even some reward. So this is illustrated atusing this slide, on the axis, x-axis you can see the length of document. On the y-axis, we show the normalizer. In this case, the Pivoted LengthNormalization formula for the normalizer, is seeing to be interpolation of 1 and the normalize the document in lengthcontrolled by a parameter B here. So you can see here,when we first divide the length of the document by the average documents,this not only gives us some sense about how this document iscompared with average documents, but also gives us a benefit of notworrying about the unit of length. We can measure the length by words orby characters. Anyway, this normalizerhas interesting property. First we see that, if we set the parameterb to 0 then the value would be 1. So, there's no lens normalization at all. So, b, in this sense,controls the lens normalization. Whereas, if we set b to a nonzero value,then the normalizer would look like this. All right, sothe value would be higher for documents that are longer thanthe average document lens. Whereas, the value ofthe normalizer would be shorter, would be smaller for shorter documents. So in this sense,we see there is a penalization for long documents, andthere's a reward for short documents. The degree of penalizationis controlled by b, because if we set b to a larger value,then the normalizer would look like this. There's even more penalization forlong documents and more reward for the short documents. By adjusting b, which varies from 0 to 1, we can control the degreeof length normalization. So, if we plug in this lengthnormalization fact that into the vector space model, ranking functionsis that we have already examined them. Then we will end up havingthe following formulas. And these are in fact the state ofthe vector space model formulas. Let's take a look at each of them. The first one is called a pivoted lengthnormalization vector space model, and a reference in [INAUDIBLE]duration of this model. And here we see that, it's basicallya TFI model that we have discussed, the idea of component shouldbe very familiar to you. There is also a query termfrequency component here. And then, in the middle, there isthe normalizer tf and in this case, we see we use the double logarithmas we discussed before and this is to achievea sublinear transformation. But we also put a documentthe length normalizer in the bottom. Right, so this would causepenalization for long document, because the larger the denominator is,then the smaller the is. And this is of course controlledby the parameter b here. And you can see again, if b is set to 0then there is no length normalization. Okay, so this is one of the two mosteffective at these base model formulas. The next one called a BM25 or Okapi, is also similar in that italso has a IDF component here, and query IDF component here. But in the middle,the normal issue's a little bit different. As we explained,there is our copy tf transformation here, and that does sublineartransformation with the upper bound. In this case we have put the lengthnormalization factor here. We're adjusting k butit achieves a similar factor, because we put a normalizerin the denominator. Therefore, again, if a document is longerthen the term weight will be smaller. So you can see after we have gone throughall the n answers that we talked about, and we have in the end reachedthe basically the state of god functions. So, So far, we have talked about mainly how to place the documentvector in the vector space. And, this has played an important rolein determining the effectiveness of the simple function. But there are also other dimensions,where we did not really examine details. For example, can we furtherimprove the instantiation of the dimension of the Vector Space Model? Now, we've just assumed that the bagof words representation should issue dimension as a word but obviously,we can see there are many other choices. For example, a stemmed word, thoseare the words that haven't transformed into the same root form, so that computation and computing were allbecome the same and they can be match. We get those stop word removal. This is to remove some very common wordsthat don't carry any content like the off. We get use of phrasesto define dimensions. We can even use later inthe semantical analysis, it will find some clusters of words that represent thea late in the concept as one by an engine. We can also use smaller unit,like a character end grams those are sequences of andthe characters for dimensions. However, in practice, people have foundthat the bag-of-words representation with phrases is still the most effectiveone and it's also efficient. So, this is still so far the mostpopular dimension instantiation method. And it's used in all major search engines. I should also mention, that sometimeswe need to do language specific and domain specific tokenization. And this is actually very important, as wemight have variations of terms that might prevent us from matching them with eachother, even when they mean the same thing. In some languages like Chinese,there is also the challenge in segmenting text to obtain word band rates becauseit's just a sequence of characters. A word might correspond to onecharacter or two characters or even three characters. So, it's easier in English when wehave a space to separate the words. In some other languages, we may needto do some Americanize processing to figure a way out of whatare the boundaries for words. There is also the possibility toimprove the similarity of the function. And sofar we have used as a top product, but one can imagine there are other measures. For example, we can measure the cosineof the angle between two vectors. Or we can use Euclidean distance measure. And these are all possible, but dot product seems still the best andone reason is because it's very general. In fact that it's sufficiently general, if you consider the possibilitiesof doing waiting in different ways. So, for example, cosine measure can be thought of as thethought product of two normalized factors. That means, we first normalize each factorand then we take the thought product. That would be criticalto the cosine measure. I just mentioned that the BM25, seems tobe one of the most effective formulas. But there has been also furtherdevelopments in improving BM25. Although, none of these words havechanged the BM25 fundamental. So in one line work,people have divide the BM25 F. Here, F stands for field, and this isuse BM25 for documents with structures. So for example, you might considera title field, the abstract, or body of the research article. Or even anchor text on the web page,those are the text fields that describe links to other pages andthese can all be combined with a proper way of different fields to helpimprove scoring for different documents. When we use BM25 for such a document and the obvious choice is to apply BM25 foreach field and then combine the scores. Basically, the idea of BM25F isto first combine the frequency counts of terms in all the fields,and then apply BM25. Now, this has advantage of avoiding overcounting the first occurrence of the term. Remember in the sublineartransformation of TF, the first occurrence is very important andit contributes a large weight. And if we do that for all the fields, then the same term might have gaineda lot of advantage in every field. But when we combine theseword frequencies together, we just do the transformation one time. At that time, then the extra occurrences will not becounted as fresh first recurrences. And this method has been working very wellfor scoring structure with documents. The other line of extensionis called a BM25+. In this line,risk is to have to address the problem of over penalization oflong documents by BM25. So to address this problem,the fix is actually quite simple. We can simply add a small constantto the TF normalization formula. But what's interesting is that,we can analytically prove that by doing such a small modification,we will fix the problem of over penalization oflaw documents by the original BM25. So the new formula called BM25+, is empirically andanalytically shown to be better than BM25. So to summarize all what we havesaid about vector space model, here are the major take away points. First, in such a model,we use the similarity of relevance. Assuming that relevance of a documentwith respect to a query, is basically proportional to the similaritybetween the query and the document. So naturally,that implies that the query and document must have beenrepresented in the same way. And in this case, we will present them asvectors in high-dimensional vector space. Where the dimensions are defined by words,or concepts, or terms, in general. And we generally, need to use a lot ofheuristics to design the ranking function. We use some examples, which showthe needs for several heuristics, including Tf weighting and transformation. And IDF weighting, anddocument length normalization. These major heuristics are the mostimportant of heuristics, to ensure such a general ranking functionto work well for all kinds of test. And finally, BM25 andpivoted normalization seem to be the most effective formulasout of the vector space model. Now I have to say that, I put BM25 inthe category of vector space model, but in fact, the BM25 has been derivedusing probabilistic model. So the reason why I've put it inthe vector space model is first, the ranking function actually has a niceinterpretation in the vector space model. We can easily see, it looks very much like a vector spacemodel, with a special waiting function. The second reason is because the originalBM25, has somewhat different form of IDF. And that form of IDF afterthe [INAUDIBLE] doesn't work so well as the standard IDFthat you have seen here. So as effective retrieval function, BM25 should probably use a heuristicmodification of the IDF. To make them even more looklike a vector space model There are some additional readings. The first is, a paper aboutthe pivoted length normalization. It's an excellent exampleof using empirical data analysis to suggest the need forlength normalization and then further derive the lengthnormalization formula. The second, is the original paperwhere the BM25 was proposed. The third paper,has a thorough discussion of BM25 and its extensions, particularly BM25 F. And finally, in the last paperhas a discussion of improving BM25 to correct the overpenalization of long documents. [MUSIC]"
cs-410,2,4,"[MUSIC] This lecture is about the implementationof text retrieval systems. In this lecture we will discusshow we can implement a text retrieval method to build a search engine. The main challenge is tomanage a lot of text data and to enable a query to be answered veryquickly and to respond to many queries. This is a typical textretrieval system architecture. We can see the documents are firstprocessed by a tokenizer to get tokenized units, for example, words. And then, these words, ortokens, will be processed by a indexer that will create a index,which is a data structure for the search engine to useto quickly answer a query. And the query would be goingthrough a similar processing step. So the Tokenizer would beapprised of the query as well, so that the text can beprocessed in the same way. The same units would bematched with each other. The query's representation wouldthen be given to the Scorer, which would use the index to quicklyanswer user's query by scoring the documents and then ranking them. The results will be given to the user. And then the user can look at the resultsand provided us some feedback that can be explicit judgements of bothwhich documents are good, which documents are bad. Or implicit feedback such as so thatuser didn't have to do anything extra. End user will just look at the results,and skip some, andclick on some result to view. So these interacting signals can be usedby the system to improve the ranking accuracy by assuming that viewed documentsare better than the skipped ones. So a search engine system thencan be divided into three parts. The first part is the indexer, andthe second part is a Scorer that responds to the users query, andthe third part is a Feedback mechanism. Now typically, the Indexer isdone in the offline manner, so you can pre-process the correct data and to build the inventory index,which we will introduce in moment. And this data structure can then be usedby the online module which is a scorer to process a user's query dynamically andquickly generate search results. The feedback mechanism can be done onlineor offline, depending on the method. The implementation of the indexer andthe scorer is very standard, and this is the main topic of thislecture and the next few lectures. The feedback mechanism,on the other hand, has variations, it depends on which method is used. So that is usually done inalgorithms specific way. Let's first talk about the tokenizer. Tokernization is a normalized lexicalunits in through the same form, so that semantically similar wordscan be matched with each other. Now, in the language like English,stemming is often used and this will map all the inflectionalforms of words into the same root form. So for example, computer, computation, and computing can all be matchedto the root form compute. This way all these different forms ofcomputing can be matched with each other. Now normally, this is a good idea, to increase the coverage of documentsthat are matched up with this query. But it's also not always beneficial, because sometimes the subtlestdifference between computer and computation might still suggest thedifference in the coverage of the content. But in most cases,stemming seems to be beneficial. When we tokenize the text in some otherlanguages, for example Chinese, we might face some special challenges in segmentingthe text to find the word boundaries. Because it's not obviouswhere the boundary is as there's no space to separate them. So here of course, we have to use somelanguage specific processing techniques. Once we do tokenization, then we wouldindex the text documents and than it'll convert the documents and do some datastructure that can enable faster search. The basic idea is to precomputeas much as we can basically. So the most commonly used indexis call an Inverted index. And this has been usedin many search engines to support basic search algorithms. Sometimes the other indices, for example, document index might be needed in orderto support feedback, like I said. And these kind of techniquesare not really standard in that they vary a lot accordingto the feedback methods. To understand why we want to useinverted index it will be useful for you to think about how you wouldrespond to a single term query quickly. So if you want to use more time tothink about that, pause the video. So think about how you canpre process the text data so that you can quickly respondto a query with just one word. Where if you have thoughtabout that question, you might realize that wherethe best is to simply create the list of documents that matchevery term in the vocabulary. In this way, you can basicallypre-construct the answers. So when you see a term you can simply justto fetch the random list of documents for that term and return the list to the user. So that's the fastest way torespond to a single term here. Now the idea of the invert indexis actually, basically, like that. We're going to do pre-constructedsearch an index, that will allows us to quickly find all the documentsthat match a particular term. So let's take a look at this example. We have three documents here, and these are the documents that youhave seen in some previous lectures. Suppose that we want to createan inverted index for these documents. Then we want to maintain a dictionary, inthe dictionary we will have one entry for each term and we're going to storesome basic statistics about the term. For example, the number ofdocuments that match the term, or the total number of code orfrequency of the term, which means we would kind of duplicatethe occurrences of the term. And so, for example, news, this term occur in allthe three documents, so the count of documents is three. And you might also realize we needed thiscount of documents, or document frequency, for computing some statistics tobe used in the vector space model. Can you think of that? So what weighting heuristicwould need this count. Well, that's the idea, right,inverse document frequency. So, IDF is the property of a term,and we can compute it right here. So, with the document that count here,it's easy to compute the idea of, either at this time, orwith the old index, or. At random time when we see a query. Now in addition to these basic statistics, we'll also store all the documentsthat matched the news, and these entries are storedin the file called Postings. So in this case it matchedthree documents and we store information aboutthese three documents here. This is the document id,document 1 and the frequency is 1. The tf is one for news, in the seconddocument it's also 1, et cetera. So from this list, we can get allthe documents that match the term news and we can also know the frequencyof news in these documents. So, if the query has just one word,news, and we have easily look up to thistable to find the entry and go quicker into the postings to fetchall the documents that matching yours. So, let's take a look at another term. This time, let's take a lookat the word presidential. This would occur in only one document,document 3. So the document frequency is 1 butit occurred twice in this document. So the frequency count is two, andthe frequency count is used for some other reachable method wherewe might use the frequency to assess the popularity ofa term in the collection. Similarly we'll have a pointerto the postings here, and in this case,there is only one entry here because the term occurred in just one document andthat's here. The document id is 3 andit occurred twice. So this is the basicidea of inverted index. It's actually pretty simple, right? With this structure we can easily fetchall the documents that match a term. And this will be the basis forscoring documents for a query. Now sometimes we also want to storethe positions of these terms. So in many of these cases the termoccurred just once in the document. So there's only one position forexample in this case. But in this case, the term occurredtwice so there's two positions. Now the position information is veryuseful for the checking whether the matching of query terms isactually within a small window of, let's say, five words or ten words. Or, whether the matching of the two queryterms is, in fact, a phrase of two words. That this can all be checked quicklyby using the position from each. So, why is inverted index good forfast search? Well, we just talked about the possibilityof using the two answer single-term query. And that's very easy. What about the multiple term queries? Well let's first look at the somespecial cases of the Boolean query. A Boolean query is basicallya Boolean expression like this. So I want the value in the documentto match both term A and term B. So that's one conjunctive query. Or I want the web documentsto match term A or term B. That's a disjunctive query. But how can we answer sucha query by using inverted index? Well if you think a bit about it, it would be obvious becausewe have simply fetch all the documents that match term A and alsofetch all the documents that match term B. And then just take the intersectionto answer a query like A and B. Or to take the union toanswer the query A or B. So this is all very easy to answer. It's going to be very quick. Now what about the multi-termkeyword query? We talked about the vector space model forexample and we will do a match such query withdocument and generate the score. And the score is based onaggregated term weights. So in this case it's notthe Boolean query but the scoring can be actuallydone in similar way. Basically it's similar todisjunctive Boolean query. Basically, it's like A or B. We take the union of all the documentsthat match at least one query term and then we would aggregate the term weights. So this is a basic idea of using invertedindex for scoring documents in general. And we're going to talk aboutthis in more detail later. But for now, let's just look at the questionwhy is in both index, a good idea? Basically why is more efficient thansequentially just scanning documents. This is the obvious approach. You can just compute a score for eachdocument and then you can then sort them. And this is a straightforward method but this is going to be very slow imaginethe wealth, there's a lot of documents. If you do this then it will takea long time to answer your query. So the question now is why wouldthe invert index be much faster? Well it has to do is the worddistribution in text. So, here's some common phenomenaof word distribution in the text. There are some languages independentof patterns that seem to be stable. And these patterns are basicallycharacterized by the following pattern. A few words like the commonwords like the, a, or we occur very, very frequently in text. So they account fora large percent of occurrences of words. But most words would occur just rarely. There are many words that occur just once, let's say, in a document oronce in the collection. And there are many such. It's also true that the mostfrequent the words in one corpus they have to be rare in another. That means although the generalphenomenon is applicable, was observed in many cases thatexact words that are common may vary from context to context. So this phenomena is characterizedby what's called a Zipf's Law. This law says that the rank of a word multiplied by the frequency ofthe word is roughly constant. So formally if we use F(w)to denote the frequency, r(w) to denote the rank of a word. Then this is the formula. It basically says the same thing,just mathematical term. Where C is basically a constant andso, and there is also a parameter, alpha, that might be adjusted tobetter fit any empirical observations. So if I plot the wordfrequencies in sorted order, then you can see this more easily. The x axis is basically the word rank. This is r(w) andthe y axis is word frequency or F(w). Now this curve shows that the productof the two is roughly the constant. Now if you look at these words, we can seeThey can be separated into three groups. In the middle,it's the intermediary frequency words. These words tend to occurquite in a few documents, but they are not like thosemost frequent words. And they are also not very rare. So they tend to be often used in queries and they also tendto have high TF-IDF weights. These intermediate frequency words. But if you look at the leftpart of the curve, these are the highest frequency words. They are covered very frequently. They are usually words,like the, we, of Etc. Those words are very, very frequent andthey are in fact the two frequent to be discriminated, and they are generallynot very useful for retrieval. So they are often removed andthis is called the stop words removal. So you can use pretty much just the kindof words in the collection to kind of infer what words might be stop words. Those are basicallythe highest frequency words. And they also occupy a lot ofspace in the inverted index. You can imagine the posting entries forsuch a word would be very long. And then therefore, if you can remove such words you can savea lot of space in the inverted index. We also show the tail part,which has a lot of rare words. Those words don't occur very frequently,and there are many such words. Those words are actually very useful for search also, if a user happens tobe interested in such a topic. But because they're rare,it's often true that users aren't necessarilyinterested in those words. But retain them would allow us tomatch such a document accurately. They generally have very high IDF. So what kind of data structures shouldwe use to store inverted index? Well, it has two parts, right. If you recall, we have a dictionary andwe also have postings. The dictionary has modest size, althoughfor the web it's still going to be very large but compare it withpostings it's more distinct. And we also need to have fastrandom access to the entries because we're going to look upon the query term very quickly. So therefore, we'd prefer to keep sucha dictionary in memory if it's possible. If the collection is not very large,this is feasible, but if the collection is very largethen it's in general not possible. If the vocabulary size is very large,obviously we can't do that. So, in general that's how it goes. So the data structuresthat we often use for storing dictionary,it would be direct access. There are structures like hash table, or b-tree if we can't storeeverything in memory or use disk. And then try to build a structure thatwould allow it to quickly look up entries. For postings they are huge. And in general, we don't have to havedirect access to a specific entry. We generally would just look upa sequence of document IDs and frequencies for all the documentsthat matches the query term. So would read those entries sequentially. And therefore because it's large andwe generally have store postings on disc, they have to stay on disc and they wouldcontain information such as document IDs, term frequency orterm positions, etcetera. Now because they are very large,compression is often desirable. Now this is not only to save disc space,and this is of course one benefit of compression, it It'snot going to occupy that much space. But it's also to help improving speed. Can you see why? Well, we know that input andoutput would cost a lot of time. In comparison with the time taken by CPU. So, CPU is much faster butIO takes time and so by compressing the inverter index,opposing files will become smaller, and the entries, that we have the readings,and memory to process a query term, would be smaller, andthen, so we can reduce the amount of tracking IO andthat can save a lot of time. Of course, we have to then do moreprocessing of the data when we uncompress the data in the memory. But as I said CPU is fast. So over all we can still save time. So compression here is bothto save disc space and to speed up the loading of the index. [MUSIC]"
cs-410,2,5,"[SOUND] This lecture is about the inverted index construction. In this lecture, we will continuethe discussion of system implementation. In particular, we're going to discusshow to construct the inverted index. The construction of the inverted indexis actually very easy if the dataset is very small. It's very easy to construct a dictionaryand then store the postings in a file. The problem is that when our datais not able to fit to the memory then we have to use somespecial method to deal with it. And unfortunately, in most retrievalapplications the dataset will be large. And they generally cannot beloaded into memory at once. And there are many approaches tosolve that problem, and sorting-based method is quite common andworks in four steps as shown here. First, you collect the local termID,documentID and frequency tuples. Basically you will locate the termsin a small set of documents. And then once you collect those accountsyou can sort those count based on terms. So that you will be able to locala partial inverted index and these are called rounds. And then you write them intoa temporary file on the disk and then you merge in step 3. Do pairwise merging of these runs, untilyou eventually merge all the runs and generate a single inverted index. So this is an illustration of this method. On the left you see some documents and on the right we have a term lexicon anda document ID lexicon. These lexicons are to map string-basedrepresentations of document IDs or terms into integer representations or map back from integers tothe stream representation. The reason why we want our interestusing integers to present these IDs is because integersare often easier to handle. For example,integers can be used as index for array, and they are also easy to compress. So this is one reason why we tendto map these strings into integers, so that we don't have tocarry these strings around. So how does this approach work? Well, it's very simple. We're going to scan thesedocuments sequentially and then parse the documents andcount the frequencies of terms. And in this stage we generally sortthe frequencies by document IDs, because we process eachdocument sequentially. So we'll first encounter allthe terms in the first document. Therefore the document IDsare all ones in this case. And this will be followed by document IDstwo and they are natural results in this only just because we processthe data in a sequential order. At some point,we will run out of memory and that would have to writethem into the disc. Before we do that we 're going to sortthem, just use whatever memory we have. We can sort them and then this timewe're going to sort based on term IDs. Note that here,we're using the term IDs as a key to sort. So all the entries that share the sameterm would be grouped together. In this case,we can see all the IDs of documents that match term 1 wouldbe grouped together. And we're going to write this intothat this is a temporary file. And would that allows you touse the memory to process and makes a batch of documents. And we're going to do that forall the documents. So we're going to write a lot oftemporary files into the disc. And then the next stage iswe do merge sort basically. We're going to merge them andthen sort them. Eventually, we will geta single inverted index, where the entries are sortedbased on term IDs. And on the top, we're going to seethese are the older entries for the documents that match term ID 1. So this is basically, how we can dothe construction of inverted index. Even though the data cannot beall loaded into the manner. Now, we mention earlier thatbecause of hostings are very large, it's desirable to compress them. So let's now take a little bithow we compressed inverted index. Well the idea of compression in general,is for leverage skewed distributions of values. And we generally have to usevariable-length encoding, instead of the fixed-lengthencoding as we use by default in a program manager like C++. And so how can we leveragethe skewed distributions of values to compress these values? Well in general, we will use fewbits to encode those frequent words at the cost of using longerbit string code those rare values. So in our case, let's think about howwe can compress the TF, tone frequency. Now, if you can picture whatthe inverted index look like, and you will see in post things,there are a lot of tone frequencies. Those are the frequencies ofterms in all those documents. Now, if you think about it, what kindof values are most frequent there? You probably will be able to guessthat small numbers tend to occur far more frequently than large numbers. Why? Well, think about the distribution ofwords and this is to do the sip of slopes, and many words occur just rarely sowe see a lot of small numbers. Therefore, we can use fewer bits forthe small, but highly frequent integers and that's cost of using more bits forlarger integers. This is a trade off of course. If the values are distributed to uniform,then this won't save us any space, but because we tend to see many smallvalues, they are very frequent. We can save on average even thoughsometimes when we see a large number we have to use a lot of bits. What about the document IDsthat we also saw in postings? Well they are not distributedin the skewed way. So how can we deal with that? Well it turns out that we canuse a trick called a d-gap and that is to store the differenceof these term IDs. And we can imagine if a term hasmatched that many documents then there will be longest of document IDs. So when we take the gap, and we take thedifference between adjacent document IDs, those gaps will be small. So again, see a lot of small numbers. Whereas if a term occurredin only a few documents, then the gap would be large,the large numbers would not be frequent. So this creates some skewed distribution, that would allow us tocompress these values. This is also possible becausein order to uncover or uncompress these document IDs,we have to sequentially process the data. Because we stored the difference andin order to recover the exact document ID we have to firstrecover the previous document ID. And then we can add the difference tothe previous document ID to restore the current document ID. Now this was possible because we onlyneeded to have sequential access to those document IDs. Once we look up the term, we look up allthe document IDs that match the term, then we sequentially process them. So it's very natural,that's why this trick actually works. And there are many different methods forencoding. So binary code is a commonly usedcode in just any program language. We use basically fixed glance in coding. Unary code, gamma code, anddelta code are all possibilities and there are many other possibilities. So let's look at someof them in more detail. Binary coding is reallyequal length coding, and that's a property forrandomly distributed values. The unary coding is a variablelength in coding method. In this case, integer this 1 will be encoded as x -1, 1 bit followed by 0. So for example, 3 will be encoded as 2,1s followed by 0, whereas 5 will be encoded as 4,1s, followed by 0, etc. So now you can imagine how many bits do wehave to use for a large number like 100? So how many bits do you have touse exactly for a number like 100? Well exactly, we have to use 100 bits. So it's the same number of bitsas the value of this number. So this is very inefficient if youwere likely to see some large numbers. Imagine if you occasionally see a numberlike 1,000, you have to use 1,000 bits. So this only works well if youare absolutely sure that there will be no large numbers, mostly veryoften you see very small numbers. Now, how do you decode this code? Now since these are variablelength encoding methods, you can't just count how many bits andthen just stop. You can't say 8-bits or 32-bits,then you will start another code. They are variable length, soyou will have to rely on some mechanism. In this case for unary, you can seeit's very easy to see the boundary. Now you can easily see 0 wouldsignal the end of encoding. So you just count up how many 1s youhave seen and at the end you hit 0. You have finished one number,you will start another number. Now we just saw that unarycoding is too aggressive. In rewarding small numbers, and if you occasionally can see a verybig number, it would be a disaster. So what about some otherless aggressive method? Well gamma coding's one of them and in this method we can use unary coding for a transform form of that. So it's 1 plus the floor of log of x. So the magnitude of this value ismuch lower than the original x. So that's why we can affordusing unary code for that. And so first I have the unary code forcoding this log of x. And this would be followed bya uniform code or binary code. And this basically the same uniform code,and binary code are the same. And we're going to use this coder to codethe remaining part of the value of x. And this is basically preciselyx-1 to the floor of log of x So the unary code are basicallycalled the flow of log of x, well add one there and here. But the remaining partwe'll be using uniform code through actually code the difference between the x and this 2 to the log of x. And it's easy to show that for this difference we only need to use up to this many bits andthe floor of log of x bits. And this is easy to understand, if the difference is too large, then wewould have a higher floor of log of x. So here are some examples forexample, 3 is is encoded as 101. The first two digits are the unary code. So this isn't for the value 2, 10 encodes 2 in unary coding. And so that means the floor of log of x is 1,because we won't actually use unary codes. In code 1 plus the flow of log of x, since this is two then we know thatthe flow of log of x is actually 1. So that 3 is still larger than 2 to the 1. So the difference is 1, andthe 1 is encoded here at the end. So that's why we have 101 for 3. Now similarly 5 is encoded as 110,followed by 01. And in this case the unary code in code 3. And so this is a unary code 110 andso the flow of log of x is 2. And that means we're going tocompute a difference between 5 and the 2 to the 2 and that's 1. And so we now have again 1 at the end. But this time we're going to use 2 bits, because with this levelof flow of log of x. We could have more numbers a 5, 6, 7 theywould all share the same prefix here, 110. So in order to differentiate them, we have to use 2 bits inthe end to differentiate them. So you can imagine 6 would be 10 herein the end instead of 01 after 10. It's also true that the form ofa gamma code is always the first odd number of bits, andin the center there is a 0. That's the end of the unary code. And before that or on the left sideof this 0, there will be all 1s. And on the right side of this 0,it's binary coding or uniform coding. So how can you decode such code? Well you again first do unary coding. Once you hit 0, you have got the unarycode and this also tell you how many bits you have to read furtherto decode the uniform code. So this is how you candecode a gamma code. There is also a delta code that'sbasically the same as a gamma code except that you replace the unaryprefix with the gamma code. So that's even lessconservative than gamma code in terms of wording the small integers. So that means, it's okay if youoccasionally see a large number. It's okay with delta code. It's also fine with the gamma code,it's really a big loss for unary code. And they are all operating of course, at different degrees of favoring short orfavoring small integers. And that also means they would beappropriate for a sorting distribution. But none of them is perfect forall distributions. And which method works the best wouldhave to depend on the actual distribution in your dataset. For inverted index compression, people have found that gammacoding seems to work well. So how to uncompress inverted index? I will just talk about this. Firstly, you decodethose encoded integers. And we just I think discussed the how wedecode unary coding and gamma coding. What about the document IDs thatmight be compressed using d-gap? Well, we're going to dosequential decoding so supposed the encoded I list is x1,x2, x3 etc. We first decode x1 to obtainthe first document ID, ID1. Then we can decode x2, which is actually the difference betweenthe second ID and the first one. So we have to add the decodervalue of x2 to ID1 to recover the value of the ID atthis secondary position. So this is where you cansee the advantages of converting document IDs to integers. And that allows us to dothis kind of compression. And we just repeat until wedecode all the documents. Every time we use the document ID inthe previous position to help to recover the document ID in the next position. [MUSIC]"
cs-410,2,6,"[SOUND] This lecture is about how to do fastersearch by using invert index. In this lecture, we're going to continuethe discussion of system implementation. In particular, we're going to talk about how to supporta faster search by using invert index. So let's think about what a generalscoring function might look like. Now of course, the vector spacemodel is a special case of this, but we can imagine many other retrievalfunctions of the same form. So the form of thisfunction is as follows. We see this scoring functionof a document D and a query Q is defined asfirst a function of fa that adjustment a function thatwould consider two factors. That I'll assume here at the end, f sub d of d and f sub q of q. These are adjustment factorsof a document and a query, so they are at the level of a document andthe query. So and then inside of this function, we also see there'sanother function called h. So this is the main partof the scoring function and these as I just said ofthe scoring factors at the level of the whole document andthe query. For example, document [INAUDIBLE] and this aggregate punching wouldthen combine all these. Now inside this h function, there are functions thatwould compute the weights of the contribution ofa matched query term ti. So this g,the function g gives us the weight of a matched query term ti in document d. And this h function would thenaggregate all these weights. So for example,take a sum of all the matched query terms, but it can also be a product or it couldbe another way of aggregating them. And then finally, this adjustmentthe functioning would then consider the document level or query levelfactors to further adjust this score, for example, document [INAUDIBLE]. So, this general form would covermany state of [INAUDIBLE] functions. Let's look at how we can score documentswith such a function using virtual index. So, here's a general algorithmthat works as follows. First this query level and document level factors can bepre-computed in the indexing time. Of course, for the query we have tocompute it at the query time but for document, for example,document [INAUDIBLE] can be pre-computed. And then, we maintain a score accumulatorfor each document d to computer h. An h is an aggregation functionover all the matching query terms. So how do we do that? For each period term we're going todo fetch the inverted list from the invert index. This will give us all the documentsthat match this query term and that includes d1, f1 and so dn fn. So each pair is a document ID andthe frequency of the term in the document. Then for each entry d sub j andf sub j are particular match of the term in thisparticular document d sub j. We'll going to compute the functiong that would give us something like weight of this term, so we're computing the weight completion ofmatching this query term in this document. And then, we're going to updatethe score accumulator for this document andthis would allow us to add this to our accumulator that wouldincrementally compute function h. So this is basically a generalway to allow pseudo computer or functions of this form byusing the inbound index. Note that we don't have toattach any of document and that didn't match any query term. Well, this is why it's fast, we only need to process the documentsthat matched at least one query term. In the end, then we're going to adjustthe score the computer this function f sub a and then we can sort. So let's take a lookat a specific example. In this case, let's assume the scoringfunction is a very simple one, it just takes the sum of t f, the role oft f, the count of a term in the document. This simplification would helpshield the algorithm clearly. It's very easy to extend the computationto include other weights like the transformation of tf, or [INAUDIBLE]or IDF [INAUDIBLE]. So let's take a look at specific example,where the queries information security and it show some entries ofinvert index on the right side. Information occurred in four documents and their frequencies are also there,security occurred in three documents. So let's see how the arrows works, sofirst we iterate overall query terms and we fetch the first query then,what is that? That's information, right? And imagine we have all thesescore accumulators who score the, scores for these documents. We can imagine there will be other but then they will only beallocated as needed. So before we do any waiting of terms, we don't even need a score of. That comes actually we have these scoreaccumulators eventually allocating. So lets fetch the interest fromthe entity [INAUDIBLE] for information, that the first one. So these four accumulators obviouslywould be initialize as zeros. So, the first entry is d1 and 3, 3 is occurrences ofinformation in this document. Since our scoring function assume that thescore is just a sum of these raw counts. We just need to add a 3 to the scoreaccumulator to account for the increase of score due to matchingthis term information, a document d1. And then, we go to the next entry,that's d2 and 4 and then we add a 4 to the scoreaccumulator of d2. Of course, at this point, that we willallocate the score accumulator as needed. And so at this point, we allocatedthe d1 and d2, and the next one is d3, and we add one, we allocate anotherscore [INAUDIBLE] d3 and add one to it. And then finally,the d4 gets a 5, because the term information occurred fivetimes in this document. Okay, so this completes the processing ofall the entries in the invert index for information. It processed all the contributionsof matching information in this four documents. So now, our error will go tothe next that's security. So, we're going to fetch allthe inverted index entries for security. So, in this case, there are three entries, andwe're going to go through each of them. The first is d2 and 3 and that means security occur threehumps in d2 and what do we do? Well, we do exactly the same,as what we did for information. So, this time we're going to change thescore [INAUDIBLE] d2 since it's already allocated andwhat we do is we'll add 3 to the existing value which is a 4, sowe now get a 7 for d2. D2 score is increased because the matchthat falls the information and the security. Go to the next entry, that's d4 and1, so we would the score for d4 and again, we add 1 to d4 sod4 goes from 5 to 6. Finally, we process d5 and a 3. Since we have not yet allocated a scoreaccumulated for d5, at this point, we're going to allocate 1 for d5,and we're going to add a 3 to it. So, those scores, of the last rule,are the final scores for these documents. If our scoring function is justa simple some of TF values. Now, what if we, actually,would like to do form addition? Well, we going to do the [INAUDIBLE]at this point, for each document. So, to summarize this,all right, so you can see, we first process the informationdetermine query term information and we processed all the entriesin what index for this term. Then we process the security,all right, its worst think about what should be the order of processinghere when we can see the query terms? It might make a difference especiallyif we don't want to keep all the score accumulators. Let's say, we only want to keepthe most promising score accumulators. What do you think would bea good order to go through? Would you process a common term first orwould you process a rare term first? The answers is we just go to whoshould process the rare term first. A rare term would match a few documents,and then the score contribution would be higher,because the ideal value would be higher. And then, it allows us to attachthe most diplomacy documents first. So, it helps pruningsome non-promising ones, if we don't need somany documents to be returned to the user. So those are all heuristics forfurther improving the accuracy. Here you can also see how we canincorporate the idea of waiting, right? So they can [INAUDIBLE] when weincorporate [INAUDIBLE] when we process each query time. When we fetch the inverted index wecan fetch the document frequency and then we can compute IDF. Or maybe perhaps the IDF valuehas already been precomputed when we indexed the documents. At that time, we already computedthe IDF value that we can just fetch it, so all these can be done at this time. So that would mean when we processall the entries for information, these words would be adjusted by the sameIDF, which is IDF for information. So this is the basic idea of usinginverted index for fast research and it works well for all kinds offormulas that are of the general form. And this generally,the general form covers actually most state of art retrieval functions. So there are some tricks tofurther improve the efficiency, some general techniquesto encode the caching. This is we just store some results ofpopular queries, so that next time when you see the same query,you simply return the stored results. Similarly, you can also slow the listof inverted index in the memory for a popular term. And if the query term is popular likely, you will soon need to factor the invertedindex for the same term again. So keeping it in the memory would help,and these are general techniques for improving efficiency. We can also keep only the most promisingaccumulators because a user generally doesn't want to examine so many documents. We only need to return highqualities subset of documents that likely are ranked on the top. For that purpose,we can then prune the accumulators. We don't have to storeall the accumulators. At some point, we just keepthe highest value accumulators. Another technique is to do parallelprocessing and that's needed for really process in such a largedata set like the web data set. And you scale up tothe Web-scale really to have the special techniques youdo parallel processing and to distribute the storageof files on machines. So here is a list of some text retrievaltoolkits, it's not a complete list. You can find more informationat this URL on the bottom. And here, I listed your four here,Lucene's one of the most popular toolkits that can support a lot of applications andit has very nice support for applications. You can use it to build a searchengine application very quickly. The downside is that it's notthat easy to extend it, and the algorithms implemented they are alsonot the most advanced algorithms. Lemur or Indri is anothertoolkit that does not have such a nice support webapplication as Lucene but it has many advanced search algorithms andit's also easy to extend. Terrier is yet another toolkitthat also has good support for application capability andsome advanced algorithms. So that's maybe in between Lemur orLucene, or maybe rather combiningthe strands of both, so that's also useful tool kit. MeTA is a toolkit that we will use for the problem assignment andthis is a new toolkit that has a combination of both text retrievalalgorithms and text mining algorithms. And so talking models are implement theyare a number of text analysis algorithms implemented in the toolkit aswell as basic search algorithms. So to summarize all the discussionabout the System Implementation, here are the major takeaway points. Inverted index is the primary datastructure for supporting a search engine and that's the key to enablefaster response to a user's query. And the basic idea is to preprocessthe data as much as we can, and we want to do compressionwhen appropriate. So that we can save disk space andwe can speed up IO and processing of inverted index in general. We talked about how to construct theinvert index when the data can't fit into the memory. And then we talk about faster search usingthat index basically, what's we exploit the invective index to accumulate a scoresfor documents [INAUDIBLE] algorithm. And we exploit the Zipf's law toavoid the touching many documents that don't match any query term and this algorithm can actually supporta wide range of ranking algorithms. So these basic techniqueshave great potential for further scaling up using distributed filesystem, parallel processing, and caching. Here are two additional readings youcan take a look, if you have time and you are interested inlearning more about this. The first one is a classicaltextbook on the efficiency o inverted index andthe compression techniques. And how to,in general feel that the efficient any inputs of the space,overhead and speed. The second one is a newer textbook thathas a nice discussion of implementing and evaluating search engines. [MUSIC]"
cs-410,3,1,"[MUSIC] This lecture is about Evaluation ofText Retrieval Systems In the previous lectures, we have talked aboutthe a number of Text Retrieval Methods, different kinds of ranking functions. But how do we know whichone works the best? In order to answer this question, we have to compare them and that means wehave to evaluate these retrieval methods. So this is the main topic of this lecture. First, lets think about whydo we have to do evaluation? I already give one reason. That is, we have to use evaluationto figure out which retrieval method works better. Now this is very important foradvancing our knowledge. Otherwise, we wouldn't know whether a newidea works better than an old idea. In the beginning of this course, we talkedabout the problem of text retrieval. We compare it with data base retrieval. There we mentioned that text retrievalis an empirically defined problem. So evaluation must rely on users. Which system works better,would have to be judged by our users. So, this becomes a verychallenging problem because how can we get usersinvolved in the evaluation? How can we do a fair comparisonof different method? So just go back to the reasons forevaluation. I listed two reasons here. The second reason, is basically what Ijust said, but there is also another reason which is to assess the actualutility of a Text Regional system. Imagine you're building yourown such annual applications, it would be interesting knowing how wellyour search engine works for your users. So in this case, matches must reflect the utility tothe actual users in real occasion. And typically, this has to bedone by using user starters and using the real search engine. In the second case, or the second reason, the measures actually all need to collatedwith the utility to actually use this. Thus, they don't have to accuratelyreflect the exact utility to users. So the measure only needs to be goodenough to tell which method works better. And this is usually donethrough a test collection. And this is the main idea that we'llbe talking about in this course. This has been very important forcomparing different algorithms and for improving searchengine system in general. So let's talk about what to measure. There are many aspects of searchingthat we can measure, we can evaluate. And here,I listed the three major aspects. One, is effectiveness or accuracy. How accurate are the search results? In this case, we're measuring a system'scapability of ranking relevant documents on top of non relevant ones. The second, is efficiency. How quickly can you get the results? How much computing resourcesare needed to answer a query? In this case, we need to measure the spaceand time overhead of the system. The third aspect is usability. Basically the question is,how useful is a system for new user tasks. Here, obviously, interfaces and many other things also important andwould typically have to do user studies. Now in this course, we're going totalk mostly about effectiveness and accuracy measures. Because the efficiency and usability dimensions are notreally unique to search engines. And so they are needed forwithout any other software systems. And there is also good coverageof such and other causes. But how to evaluate searchengine's quality or accuracy is something unique to text retrieval andwe're going to talk a lot about this. The main idea that people have proposedbefore using a test set to evaluate the text retrieval algorithm is calledthe Cranfield Evaluation Methodology. This one actually was developeda long time ago, developed in 1960s. It's a methodology forlaboratory test of system components. Its sampling methodology that hasbeen very useful, not just for search engine evaluation. But also for evaluating virtuallyall kinds of empirical tasks, and for example in natural language processingor in other fields where the problem is empirical to find, we typicallywould need to use such a methodology. And today with the big data challenging with the use of machinelearning everywhere. This methodology has been very popular,but it was first developed for a search engine application in the 1960s. So the basic idea of this approach isto build a reusable test collection and define measures. Once such a test collection is built,it can be used again and again to test different algorithms. And we're going to define measuresthat allow you to quantify performance of a system and algorithm. So how exactly will this work? Well we can do have a sample collection ofdocuments and this is adjusted to simulate the real document collectionin the search application. We're going to also have a sampleset of queries, or topics. This is a little simulatorthat uses queries. Then, we'll have to havethose relevance judgments. These are judgments of which documentsshould be returned for which queries. Ideally, they have to be made byusers who formulated the queries. Because those are the people that knowexactly what documents would be used for. And finally, we have to have matches for quantify how well our system's resultmatches the ideal ranked list. That would be constructed baseon user's relevance judgements. So this methodology is very useful forstarting retrieval algorithms, because the test can be reused many times. And it will also provide a faircomparison for all the methods. We have the same criteria or same dataset to be used tocompare different algorithms. This allows us to comparea new algorithm with an old algorithm that was divided manyyears ago, by using the same standard. So this is the illustration of this works,so as I said, we need our queries that are showing here. We have Q1, Q2 etc. We also need the documents andthat's called the document caching and on the right side you will seewe need relevance judgments. These are basically the binary judgmentsof documents with respect to a query. So for example,D1 is judged as being relevant to Q1, D2 is judged as being relevant as well,and D3 is judged as not relevant. And the Q1 etc. These will be created by users. Once we have these, andwe basically have a test collection. And then if you have two systems,you want to compare them, then you can just run eachsystem on these queries and the documents andeach system would then return results. Let's say if the queries Q1 andthen we would have the results here. Here I show R sub A asthe results from system A. So this is, remember we talked about task of computing approximationof the relevant document set. R sub A is system A's approximation here. And R sub B is system B'sapproximation of relevant documents. Now, let's take a look at these results. So which is better? Now imagine if a user,which one would you like? Now let's take a look at the both results. And there are some differences and there are some documents thatare returned by both systems. But if you look at the results,you will feel that maybe A is better in the sense that we don'thave many number element documents. And among the three documents returned,the two of them are relevant. So that's good, it's precise. On the other hand one councilsay maybe B is better, because we've got all ofthem in the documents. We've got three instead of two. So which one is better andhow do we quantify this? Well, obviously this questionhighly depends on a user's task. It depends on users as well. You might even imagine forsome users may be system A is better. If the user is not interested ingetting all the random documents. Right, in this case the user doesn'thave to read a million users will see most of the relevant documents. On the other hand,one can also imagine the user might need to have as many randomdocuments as possible. For example, if you're doing a literaturesurvey you might be in the sigma category, and you might find thatsystem B is better. So in the case, we will have to alsodefine measures that will quantify them. And we might need it to define multiplemeasures because users have different perspectives of looking at the results. [MUSIC]"
cs-410,3,2,"[SOUND]This lecture is about the basic measures forevaluation of text retrieval systems. In this lecture,we're going to discuss how we design basic measures to quantitativelycompare two retrieval systems. This is a slide that you have seenearlier in the lecture where we talked about the Granvilleevaluation methodology. We can have a test faction that consistsof queries, documents, and [INAUDIBLE]. We can then run two systems on thesedata sets to contradict the evaluator. Their performance. And we raise the question,about which set of results is better. Is system A better or is system B better? So let's now talk about how toaccurately quantify their performance. Suppose we have a total of 10 relevantdocuments in the collection for this query. Now, the relevant judgments show onthe right in [INAUDIBLE] obviously. And we have only seen 3 [INAUDIBLE] there,[INAUDIBLE] documents there. But, we can imagine there are other Randomdocuments in judging for this query. So now, intuitively,we thought that system A is better because itdid not have much noise. And in particular we have seenthat among the three results, two of them are relevant but in system B, we have five results andonly three of them are relevant. So intuitively it looks likesystem A is more accurate. And this infusion can be capturedby a matching holder position, where we simply compute to what extentall the retrieval results are relevant. If you have 100% position, that would mean that allthe retrieval documents are relevant. So in this case system A hasa position of two out of three System B has somesweet hold of 5 and this shows that systemA is better frequency. But we also talked about System Bmight be prefered by some other units would like to retrieve as manyrandom documents as possible. So in that case we'll have to comparethe number of relevant documents that they retrieve andthere's another method called recall. This method uses the completenessof coverage of random documents In your retrieval result. So we just assume that there are tenrelevant documents in the collection. And here we've got two of them,in system A. So the recall is 2 out of 10. Whereas System B has called a 3,so it's a 3 out of 10. Now we can see by recallsystem B is better. And these two measures turn out tobe the very basic of measures for evaluating search engine. And they are very important becausethey are also widely used in many other test evaluation problems. For example, if you look atthe applications of machine learning, you tend to see precision recall numbersbeing reported and for all kinds of tasks. Okay so, now let's define thesetwo measures more precisely. And these measures are to evaluate a setof retrieved documents, so that means we are considering that approximationof the set of relevant documents. We can distinguish 4 cases dependingon the situation of the documents. A document can be retrieved ornot retrieved, right? Because we are talkingabout a set of results. A document can be also relevant or not relevant depending on whether the userthinks this is a useful document. So we can now have counts of documents in. Each of the four categories againhave a represent the number of documents that have been retrieved andrelevant. B for documents that are not retrieved butrather etc. No with this table thenwe can define precision. As the ratio of the relevant retrieved documents A to the totalof relevant retrieved documents. So, this is just A dividedby The sum of a and c. The sum of this column. Singularly recall is defined bydividing a by the sum of a and b. So that's again to divide a by. The sum of the row instead of the column. All right, so we can see precision andrecall is all focused on looking at the a, that's the number ofretrieved relevant documents. But we're going to usedifferent denominators. Okay, so what would be an ideal result. Well, you can easily see beingthe ideal case would have precision and recall oil to be 1.0. That means We have got 1% ofall the Relevant documents in our results, and all of the resultsthat we returned all Relevant. At least there's no singleNot Relevant document returned. In reality, however, high recall tendsto be associated with low precision. And you can imagine why that's the case. As you go down the to try to get asmany random documents as possible, you tend to encounter a lot of documents,so the precision has to go down. Note that this set can alsobe defined by a cut off. In the rest of this, that's why althoughthese two measures are defined for retrieve the documents, they are actuallyvery useful for evaluating a rank list. They are the fundamental measures intask retrieval and many other tasks. We often are interested in The precisionat ten documents for web search. This means we look at how many documents among the top ten resultsare actually relevant. Now, this is a very meaningful measure, because it tells us how many relevantdocuments a user can expect to see On the first page of where theytypically show ten results. So precision and recallare the basic matches and we need to use them to further evaluate a searchengine, but they are the Building blocks. We just said that there tends to bea trailoff between precision and recall, so naturally it would beinteresting to combine them. And here's one method that's often used,called F-measure And it's a [INAUDIBLE] mean of precision andrecall as defined on this slide. So, you can see at first, compute the. Inverse of R and P here,and then it would interpret the 2 by using coefficientsdepending on parameter beta. And after some transformation you caneasily see it would be of this form. And in any case it just becomesan agent of precision and recall, and beta is a parameter,that's often set to 1. It can control the emphasison precision or recall always set beta to 1 We end up having a specialcase of F-Measure, often called F1. This is a popular measure that's oftenused as a combined precision and recall. And the formula looks very simple. It's just this, here. Now it's easy to see that ifyou have a Larger precision, or larger recall than fmeasure would be high. But, what's interesting is thatthe trade off between precision and recall is capturedan interesting way in f1. So, in order to understand that, we can first look at the naturalWhy not just the combining and using the symbol arithmeticallyas efficient here? That would be likely the most natural wayof combining them So what do you think? If you want to think more,you can pause the video. So why is this not as good as F1? Or what's the problem with this? Now, if you think aboutthe arithmetic mean, you can see this isthe sum of multiple terms. In this case,it's the sum of precision and recall. In the case of a sum, the total valuetends to be dominated by the large values. that means if you have a very high P orvery high R then you really don't care about whether the other valueis low so the whole sum would be high. Now this is not desirable because onecan easily have a perfect recall. We have perfect recall easily. Can we imagine how? It's probably very easy toimagine that we simply retrieve all the documents in the collection andthen we have a perfect recall. And this will give us 0.5 as the average. But such results are clearly notvery useful for the users even though the average using thisformula would be relevantly high. In contrast you can see F 1 wouldreward a case where precision and recall are roughly That seminar, so it would a case where you hadextremely high value for one of them. So this means f one encodesa different trade off between that. Now this example showsactually a very important. Methodology here. But when you try to solve a problem youmight naturally think of one solution, let's say in this it'sthis error mechanism. But it's important not tosettle on this source. It's important to think whether youhave other ways to combine that. And once you think about the multiplevariance It's important to analyze their difference, and then think aboutwhich one makes more sense. In this case, if you think more carefully, you will think that F1probably makes more sense. Than the simple. Although in other cases theremay be different results. But in this case the seems not reasonable. But if you don't pay attentionto these subtle differences you might just take a easy way tocombine them and then go ahead with it. And here later, you will find that,the measure doesn't seem to work well. All right. So this methodology is actually veryimportant in general, in solving problems. Try to think about the best solution. Try to understand the problem very well,and then know why you needed this measure, and whyyou need to combine precision and recall. And then use that to guide you infinding a good way to solve the problem. To summarize, we talked aboutprecision which addresses the question are there retrievableresults all relevant? We also talk about the Recall. Which addresses the question, have all ofthe relevant documents been retrieved. These two, are the two,basic matches in text and retrieval in. They are used formany other tasks, as well. We talk about F measure as a way tocombine Precision Precision and recall. We also talked about the tradeoffbetween precision and recall. And this turns out to dependon the user's search tasks and we'll discuss this pointmore in a later lecture. [MUSIC]"
cs-410,3,3,"[MUSIC] This lecture is about,how we can evaluate a ranked list? In this lecture, we will continuethe discussion of evaluation. In particular, we are going to look at, how we canevaluate a ranked list of results. In the previous lecture,we talked about, precision-recall. These are the two basic measures for, quantitatively measuringthe performance of a search result. But, as we talked about, ranking, before, we framed that the text of retrievalproblem, as a ranking problem. So, we also need to evaluate the,the quality of a ranked list. How can we use precision-recallto evaluate, a ranked list? Well, naturally, we have to look after theprecision-recall at different, cut-offs. Because in the end, the approximationof relevant documents, set, given by a ranked list, is determinedby where the user stops browsing. Right?If we assume the user, securely browses, the list of results, the user would,stop at some point, and that point would determine the set. And then,that's the most important, cut-off, that we have to consider,when we compute the precision-recall. Without knowing whereexactly user would stop, then we have to consider, allthe positions where the user could stop. So, let's look at these positions. Look at this slide, andthen, let's look at the, what if the user stops at the,the first document? What's the precision-recall at this point? What do you think? Well, it's easy to see, that this documentis So, the precision is one out of one. We have, got one document,and that's relevent. What about the recall? Well, note that, we're assuming that,there are ten relevant documents, for this query in the collection,so, it's one out of ten. What if the user stopsat the second position? Top two. Well, the precision is the same,100%, two out of two. And, the record is two out of ten. What if the user stopsat the third position? Well, this is interesting,because in this case, we have not got any, additional relevant document,so, the record does not change. But the precision is lower,because we've got number [INAUDIBLE] so, what's exactly the precision? Well, it's two out of three, right? And, recall is the same, two out of ten. So, when would see another point,where the recall would be different? Now, if you look down the list,well, it won't happen until, we have, seeing another relevant document. In this case D5, at that point, the,the recall is increased through three out of ten, and,the precision is three out of five. So, you can see, if we keep doing this,we can also get to D8. And then, we will havea precision of four out of eight, because there are eight documents,and four of them are relevant. And, the recall is a four out of ten. Now, when can we get,a recall of five out of ten? Well, in this list, we don't have it,so, we have to go down on the list. We don't know, where it is? But, as convenience, we often assume that,the precision is zero, at all the, the othe,the precision are zero at all the other levels of recall,that are beyond the search results. So, of course,this is a pessimistic assumption, the actual position would be higher,but we make, make this assumption, in order to, have an easy way to, compute another measure called AveragePrecision, that we will discuss later. Now, I should also say, now, here you see, we make these assumptions thatare clearly not, accurate. But, this is okay, forthe purpose of comparing to, text methods. And, this is for the relative comparison,so, it's okay, if the actual measure, or actual, actual number deviatesa little bit, from the true number. As long as the deviation, is not biased toward any particularretrieval method, we are okay. We can still,accurately tell which method works better. And, this is important point,to keep in mind. When you compare different algorithms, the key's to avoid anybias toward each method. And, as long as, you can avoid that. It's okay, for you to do transformationof these measures anyway, so, you can preserve the order. Okay, so, we'll just talk about, we can get a lot of precision-recallnumbers at different positions. So, now, you can imagine,we can plot a curve. And, this just shows on the,x-axis, we show the recalls. And, on the y-axis, we show the precision. So, the precision line was marked as .1,.2, .3, and, 1.0. Right?So, this is, the different, levels of recall. And,, the y-axis also has,different amounts, that's for precision. So, we plot the, these, precision-recallnumbers, that we have got, as points on this picture. Now, we can further, andlink these points to form a curve. As you'll see, we assumed all the other, precisionas the high-level recalls, be zero. And, that's why, they are down here,so, they are all zero. And this, the actual curve probably willbe something like this, but, as we just discussed, it, it doesn't matter thatmuch, for comparing two methods. because this would be,underestimated, for all the method. Okay, so, now that we,have this precision-recall curve, how can we compare ranked to back list? All right, so, that means,we have to compare two PR curves. And here, we show, two cases. Where system A is showing red,system B is showing blue, there's crosses. All right, so, which one is better? I hope you can see,where system A is clearly better. Why?Because, for the same level of recall, see same level of recall here,and you can see, the precision point by system A is better,system B. So, there's no question. In here, you can imagine, what does thecode look like, for ideal search system? Well, it has to have perfect,precision at all the recall points, so, it has to be this line. That would be the ideal system. In general, the higher the curve is,the better, right? The problem is that,we might see a case like this. This actually happens often. Like, the two curves cross each other. Now, in this case, which one is better? What do you think? Now, this is a real problem,that you actually, might have face. Suppose, you build a search engine,and you have a old algorithm, that's shown here in blue, or system B. And, you have come up with a new idea. And, you test it. And, the results are shown in red,curve A. Now, your question is, is your newmethod better than the old method? Or more, practically,do you have to replace the algorithm that you're already using, your, in your searchengine, with another, new algorithm? So, should we use system,method A, to replace method B? This is going to be a real decision,that you to have to make. If you make the replacement, the searchengine would behave like system A here, whereas, if you don't do that,it will be like a system B. So, what do you do? Now, if you want to spend more timeto think about this, pause the video. And, it's actually veryuseful to think about that. As I said, it's a real decision that youhave to make, if you are building your own search engine, or if you're working, fora company that, cares about the search. Now, if you have thought about this for a moment, you might realize that,well, in this case, it's hard to say. Now, some users might like a system A,some users might like, like system B. So, what's the difference here? Well, the difference is just that,you know, in the, low level of recall,in this region, system B is better. There's a higher precision. But in high recall region,system A is better. Now, so, that also means,it depends on whether the user cares about the high recall, orlow recall, but high precision. You can imagine, if someone is just goingto check out, what's happening today, and want to find out somethingrelevant in the news. Well, which one is better? What do you think? In this case, clearly, system B is better, because the user is unlikelyexamining a lot of results. The user doesn't care about high recall. On the other hand,if you think about a case, where a user is doing you are,starting a problem. You want to find, whether your idea ha,has been started before. In that case, you emphasize high recall. So, you want to see,as many relevant documents as possible. Therefore, you might, favor, system A. So, that means, which one is better? That actually depends on users,and more precisely, users task. So, this means, you may not necessarilybe able to come up with one number, that would accuratelydepict the performance. You have to look at the overall picture. Yet, as I said, when you havea practical decision to make, whether you replace ours with another, then you may have to actually come up witha single number, to quantify each, method. Or, when we compare many differentmethods in research, ideally, we have one number to compare, them with, so, thatwe can easily make a lot of comparisons. So, for all these reasons, it is desirableto have one, single number to match it up. So, how do we do that? And, that,needs a number to summarize the range. So, here again it'sthe precision-recall curve, right? And, one way to summarizethis whole ranked, list, for this whole curve,is look at the area underneath the curve. Right?So, this is one way to measure that. There are other ways to measure that,but, it just turns out that,, this particular way of matchingit has been very, popular, and has been used, since a long time ago fortext And, this is, basically, in this way, andit's called the average precision. Basically, we're going to take a, a lookat the, every different, recall point. And then, look out for the precision. So, we know, you know,this is one precision. And, this is another,with, different recall. Now, this, we don't count to this one, because the recall level is the same,and we're going to, look at the, this number, and that's precision ata different recall level et cetera. So, we have all these, you know, added up. These are the precisionsat the different points, corresponding to retrieving the firstrelevant document, the second, and then, the third, that follows, et cetera. Now, we missed the many relevantdocuments, so, in all of those cases, we just, assume,that they have zero precisions. And then, finally, we take the average. So, we divide it by ten, and which is the total number of relevantdocuments in the collection. Note that here,we're not dividing this sum by four. Which is a number retrievedrelevant documents. Now, imagine, if I divide by four,what would happen? Now, think about this, for a moment. It's a common mistake that people,sometimes, overlook. Right, so, if we, we divide this by four,it's actually not very good. In fact, that you are favoring a system,that would retrieve very few random documents, as in that case,the denominator would be very small. So, this would be, not a good matching. So, note that this denomina,denominator is ten, the total number of relevant documents. And, this will basically ,computethe area, and the needs occur. And, this is the standard method,used for evaluating a ranked list. Note that, it actually combinesrecall and, precision. But first, you know, we haveprecision numbers here, but secondly, we also consider recall, because if missedmany, there would be many zeros here. All right, so,it combines precision and recall. And furthermore, you can see thismeasure is sensitive to a small change of a position of a relevant document. Let's say, if I move this relevantdocument up a little bit, now, it would increase this means,this average precision. Whereas, if I move any relevant document,down, let's say, I move this relevant document down, then it would decrease,uh,the average precision. So, this is a very good, because it's a very sensitive tothe ranking of every relevant document. It can tell, small differencesbetween two ranked lists. And, that is what we want, sometimes one algorithm only worksslightly better than another. And, we want to see this difference. In contrast, if we look atthe precision at the ten documents. If we look at this, this whole set, well, what, what's the precision,what do you think? Well, it's easy to see,that's a four out of ten, right? So, that precision is very meaningful,because it tells us, what user would see? So, that's pretty useful, right? So, it's a meaningful measure,from a users perspective. But, if we use this measure tocompare systems, it wouldn't be good, because it wouldn't be sensitive to wherethese four relevant documents are ranked. If I move them around the precisionat ten, still, the same. Right.So, this is not a good measure forcomparing different algorithms. In contrast, the average precisionis a much better measure. It can tell the difference of, different, a difference in ranked list in,subtle ways. [MUSIC]"
cs-410,3,4,"[SOUND] So average precision is computer for just one. one query.But we generally experiment with many different queries and this is toavoid the variance across queries. Depending on the queries you use youmight make different conclusions. Right, soit's better then using more queries. If you use more queries then,you will also have to take the average of the averageprecision over all these queries. So how can we do that? Well, you can naturally. Think of just doing arithmetic mean as we always tend to, to think in, in this way. So, this would give us what's calleda ""Mean Average Position"", or MAP. In this case, we take arithmetic mean of all the averageprecisions over several queries or topics. But as I just mentioned inanother lecture, is this good? We call that. We talked about the different waysof combining precision and recall. And we conclude that the arithmeticmean is not as good as the MAP measure. But here it's the same. We can also think about the alternativeways of aggregating the numbers. Don't just automatically assume that,though. Let's just also take the arithmeticmean of the average position over these queries. Let's think about what'sthe best way of aggregating them. If you think about the different ways,naturally you will, probably be able to think aboutanother way, which is geometric mean. And we call this kind of average a gMAP. This is another way. So now, once you think aboutthe two different ways. Of doing the same thing. The natural question to ask is,which one is better? So. So, do you use MAP or gMAP? Again, that's important question. Imagine you are againtesting a new algorithm in, by comparing the ways your oldalgorithms made the search engine. Now you tested multiple topics. Now you've got the average precision forthese topics. Now you are thinking of lookingat the overall performance. You have to take the average. But which, which strategy would you use? Now first, you should also think about thequestion, well did it make a difference? Can you think of scenarios where usingone of them would make a difference? That is they would give differentrankings of those methods. And that also means depending onthe way you average or detect the. Average of these average positions. You will get different conclusions. This makes the questionbecoming even more important. Right?So, which one would you use? Well again, if you look atthe difference between these. Different ways of aggregatingthe average position. You'll realize in arithmetic mean,the sum is dominating by large values. So what does large value here mean? It means the query is relatively easy. You can have a high pres,average position. Whereas gMAP tends to beaffected more by low values. And those are the queries thatdon't have good performance. The average precision is low. So if you think about the,improving the search engine for those difficult queries,then gMAP would be preferred, right? On the other hand, if you just want to. Have improved a lot. Over all the kinds of queries orparticular popular queries that might be easy and you want to make the perfect andmaybe MAP would be then preferred. So again, the answer depends onyour users, your users tasks and their pref, their preferences. So the point that here is to thinkabout the multiple ways to solve the same problem, and then compare them,and think carefully about the differences. And which one makes more sense. Often, when one of them mightmake sense in one situation and another might make more sensein a different situation. So it's important to pick out underwhat situations one is preferred. As a special case of the mean averageposition, we can also think about the case where there was preciselyone rank in the document. And this happens often, for example,in what's called a known item search. Where you know a target page, let'ssay you have to find Amazon, homepage. You have one relevant document there,and you hope to find it. That's call a ""known item search"". In that case,there's precisely one relevant document. Or in another application,like a question and answering, maybe there's only one answer. Are there. So if you rank the answers, then your goal is to rank that oneparticular answer on top, right? So in this case, you can easilyverify the average position, will basically boil downto reciprocal rank. That is, 1 over r where r is the rankposition of that single relevant document. So if that document is rankedon the very top or is 1, and then it's 1 for reciprocal rank. If it's ranked at the,the second, then it's 1 over 2. Et cetera. And then we can also take a, a averageof all these average precision or reciprocal rank over a set of topics, and that would give us somethingcalled a mean reciprocal rank. It's a very popular measure. For no item search or, you know, an problem where you havejust one relevant item. Now again here, you can see thisr actually is meaningful here. And this r is basicallyindicating how much effort a user would have to make in orderto find that relevant document. If it's ranked on the top it's low effortthat you have to make, or little effort. But if it's ranked at 100then you actually have to, read presumably 100 documentsin order to find it. So, in this sense r is also a meaningfulmeasure and the reciprocal rank will take the reciprocal of r,instead of using r directly. So my natural question hereis why not simply using r? I imagine if you were to designa ratio to, measure the performance of a random system,when there is only one relevant item. You might have thought aboutusing r directly as the measure. After all,that measures the user's effort, right? But, think about if you take a averageof this over a large number of topics. Again it would make a difference. Right, for one single topic, using r or using 1 over r wouldn'tmake any difference. It's the same. Larger r with correspondsto a small 1 over r, right? But the difference would only show when,show up when you have many topics. So again, think about the average of MeanReciprocal Rank versus average of just r. What's the difference? Do you see any difference? And would, would this differencechange the oath of systems. In our conclusion. And this, it turns out that,there is actually a big difference, and if you think about it, if you want tothink about it and then, yourself, then pause the video. Basically, the difference is,if you take some of our directory, then. Again it will be dominatedby large values of r. So what are those values? Those are basically large values thatindicate that lower ranked results. That means the relevant itemsrank very low down on the list. And the sum that's also the averagethat would then be dominated by. Where those relevant documentsare ranked in, in ,in, in the lower portion of the ranked. But from a users perspective we caremore about the highly ranked documents. So by taking this transformationby using reciprocal rank. Here we emphasize more onthe difference on the top. You know, think aboutthe difference between 1 and the 2, it would make a big difference, in 1 overr, but think about the 100, and 1, and where and when won't make muchdifference if you use this. But if you use this there willbe a big difference in 100 and let's say 1,000, right. So this is not the desirable. On the other hand, a 1 and2 won't make much difference. So this is yet another case where theremay be multiple choices of doing the same thing and then you need to figureout which one makes more sense. So to summarize,we showed that the precision-recall curve. Can characterize the overallaccuracy of a ranked list. And we emphasized that the actualutility of a ranked list depends on how many top ranked resultsa user would actually examine. Some users will examine more. Than others. An average person uses a standard measurefor comparing two ranking methods. It combines precision and recall and it's sensitive to the rankof every random document. [MUSIC]"
cs-410,3,5,"[MUSIC] This lecture is about how to evaluatethe text retrieval system when we have multiple levels of judgements. In this lecture, we will continuethe discussion of evaluation. We're going to look at how toevaluate a text retrieval system, when we have multiplelevels of judgements. So far we have talked aboutthe binary judgements, that means a document is judged asbeing relevant or not relevant. But earlier, we also talk aboutthe relevance as a medal of degrees. So we often can distinguishvery high relevant documents, those are very useful documents,from moderately relevant documents. They are okay, they are useful perhaps. And further from now, we're addingthe documents, those are not useful. So imagine you can have ratings forthese pages. Then, you would havemultiple levels of ratings. For example, here I show example of threelevels, 3 for relevant, sorry 3 for very relevant, 2 for marginally relevant,and 1 for non-relevant. Now, how do we evaluate the searchengine system using these judgements? Obvious that the map doesn't work, averageof precision doesn't work, precision, and recall doesn't work,because they rely on binary judgements. So let's look at some top rankedresults when using these judgements. Imagine the user would be mostlycare about the top ten results here. And we marked the rating levels,or relevance levels, for these documents as shown here,3, 2, 1, 1, 3, etcetera. And we call these gain. And the reason why we call itthe gain is because the measure that we are infusing is called the NDCGnormalized or accumulated gain. So this gain, basically,can measure how much a gain of random information a user can obtain bylooking at each document, right? So looking at the first document,the user can gain 3 points. Looking at the non-relevant documentuser would only gain 1 point. Looking at the moderator ormarginally relevant, document the user would get 2 points,etcetera. So, this gain to each of the measures isa utility of the document from a user's perspective. Of course, if we assume the userstops at the 10 documents and we're looking at the cutoff at 10,we can look at the total gain of the user. And what's that? Well, that's simply the sum of these,and we call it the Cumulative Gain. So if the user stops after the position 1,that's just a 3. If the user looks at another document,that's a 3+2. If the user looks at the more documents,then the cumulative gain is more. Of course this is at the cost ofspending more time to examine the list. So cumulative gain givesus some idea about how much total gain the user would have ifthe user examines all these documents. Now, in NDCG, we also have another letterhere, D, discounted cumulative gain. So, why do we want to do discounting? Well, if you look at this cumulative gain,there is one deficiency, which is it did not consider the rankposition of these documents. So for example, looking at this sum here, and we only know there is 1highly relevant document, 1 marginally relevant document,2 non-relevant documents. We don't really carewhere they are ranked. Ideally, we want these two to be rankedon the top which is the case here. But how can we capture that intuition? Well we have to say, well this is 3 hereis not as good as this 3 on the top. And that means the contributionof the gain from different positions has to beweighted by their position. And this is the idea of discounting,basically. So we're going to to say, well, the firstone does not need to be discounted because the user can be assumedthat will always see this document. But the second one,this one will be discounted a little bit because there's a small possibilitythat the user wouldn't notice it. So we divide this gain bya weight based on the position. So log of 2,2 is the rank position of this document. And when we go to the third position,we discounted even more, because the normalizer is log of 3,and so on and so forth. So when we take such a sum that a lowerranked document would not contribute that much as a highly ranked document. So that means if you, for example,switch the position of this, let's say this position, and this one, and thenyou would get more discount if you put, for example very relevantdocument here as opposed to here. Imagine if you put the 3 here,then it would have to be discounted. So it's not as good as ifyou we would put the 3 here. So this is the idea of discounting. Okay, so now at this point that we havegot a discounted cumulative gain for measuring the utility of this rankedlist with multiple levels of judgements. So are we happy with this? Well, we can use this to rank systems. Now, we still need to do a little bit more in order to make this measurecomparable across different topics. And this is the last step, and by the way,here we just show the DCG at 10, so this is the total sum of DCG,all these 10 documents. So the last step is called N,normalization. And if we do that,then we'll get the normalized DCG. So how do we do that? Well, the idea here is we'regoing to normalize DCG by the ideal DCG at the same cutoff. What is the ideal DCG? Well, this is the DCG of an ideal ranking. So imagine if we have 9 documents inthe whole collection rated 3 here. And that means in total wehave 9 documents rated 3. Then our ideal rank lister would have putall these 9 documents on the very top. So all these would have to be 3 andthen this would be followed by a 2 here. Because that's the best we coulddo after we have run out of the 3. But all these positions would be 3. Right? So this would our ideal ranked list. And then we had computed the DCG forthis ideal rank list. So this would be given by thisformula that you see here. And so this ideal DCG would thenbe used as the normalizer DCG. So here. And this idea of DCG wouldbe used as a normalizer. So you can imagine now,normalization essentially is to compare the actual DCG with the best DCGyou can possibly get for this topic. Now why do we want to do this? Well, by doing this we'll map the DCGvalues into a range of 0 through 1. So the best value, or the highest value,for every query would be 1. That's when your rank list is,in fact, the ideal list but otherwise, in general,you will be lower than one. Now, what if we don't do that? Well, you can see, this transformation,or this normalization, doesn't really affect the relativecomparison of systems for just one topic, because this idealDCG is the same for all the systems, so the ranking of systems based ononly DCG would be exactly the same as if you rank them basedon the normalized DCG. The difference however iswhen we have multiple topics. Because if we don't do normalization, different topics will havedifferent scales of DCG. For a topic like this one,we have 9 highly relevant documents, the DCG can get really high,but imagine in another case, there are only two very relevant documentsin total in the whole collection. Then the highest DCG thatany system could achieve for such a topic would not be very high. So again, we face the problem ofdifferent scales of DCG values. When we take an average, we don't want the average to bedominated by those high values. Those are, again, easy queries. So, by doing the normalization,we can have avoided the problem, making all the queries contributeto equal to the average. So, this is a idea of NDCG, it's used for measuring a rank list based on multiplelevel of relevance judgements. In a more general way thisis basically a measure that can be applied to any ranked taskwith multiple level of judgements. And the scale of the judgementscan be multiple, can be more than binary not only more than binary theycan be much multiple levels like 1, 0, 5 oreven more depending on your application. And the main idea of this measure,just to summarize, is to measure the total utilityof the top k documents. So you always choose a cutoff andthen you measure the total utility. And it would discount the contributionfrom a lowly ranked document. And then finally, it would do normalization to ensure comparability across queries. [MUSIC]"
cs-410,3,6,"[SOUND]. This lecture is about some practicalissues that you would have to address in evaluation of text retrieval systems. In this lecture, we will continuethe discussion of evaluation. We'll cover some practicalissues that you have to solve in actual evaluation oftext retrieval systems. So, in order to createthe test collection, we have to create a set of queries. A set of documents anda set of relevance judgments. It turns out that each isactually challenging to create. First, the documents andqueries must be representative. They must represent the real queries andreal documents that the users handle. And we also have to use many queries and many documents in order toavoid a bias of conclusions. For the matching of relevantdocuments with the queries. We also need to ensure that there exists alot of relevant documents for each query. If a query has only one, that'sa relevant option we can actually then. It's not very informative tocompare different methods using such a query because there's notthat much room for us to see difference. So ideally, there should be morerelevant documents in the clatch but yet the queries also should representthe real queries that we care about. In terms of relevance judgments,the challenge is to ensure complete judgments of allthe documents for all the queries. Yet, minimizing human and fault, because we have to use humanlabor to label these documents. It's very labor intensive. And as a result, it's impossible toactually label all the documents for all the queries, especially consideringa giant data set like the web. So this is actually a major challenge,it's a very difficult challenge. For measures, it's also challenging,because we want measures that would accurately reflectthe perceived utility of users. We have to consider carefullywhat the users care about. And then design measures to measure that. If your measure is notmeasuring the right thing, then your conclusion would be misled. So it's very important. So we're going to talk abouta couple of issues here. One is the statistical significance test. And this also is a reason whywe have to use a lot of queries. And the question here is how sure canyou be that observe the difference doesn't simply result fromthe particular queries you choose. So here are some sample results ofaverage position for System A and System B into different experiments. And you can see in the bottom,we have mean average of position. So the mean, if you look at the meanaverage of position, the mean average of positions are exactly the samein both experiments, right? So you can see this is 0.20,this is 0.40 for System B. And again here it's also 0.20 and0.40, so they are identical. Yet, if you look at these exact averagepositions for different queries. If you look at these numbers in detail,you would realize that in one case, you would feel that you can trustthe conclusion here given by the average. In the another case, in the other case,you will feel that, well, I'm not sure. So, why don't you take a look at all thesenumbers for a moment, pause the media. So, if you look at the average,the mean average of position, we can easily, say that well,System B is better, right? So, after all it's 0.40 and this is twice as much as 0.20,so that's a better performance. But if you look at these two experiments,look at the detailed results. You will see that, we've been moreconfident to say that, in the case one, in experiment one. In this case. Because these numbers seem to beconsistently better for System B. Whereas in Experiment 2, we're not surebecause looking at some results like this, after System A is better andthis is another case System A is better. But yet if we look at only average,System B is better. So, what do you think? How reliable is our conclusion,if we only look at the average? Now in this case, intuitively,we feel Experiment 1 is more reliable. But how can we quantitatethe answer to this question? And this is why we need to dostatistical significance test. So, the idea of the statisticalsignificance test is basically to assess the variants acrossthese different queries. If there is a big variance, that means the results could fluctuatea lot according to different queries. Then we should believe that,unless you have used a lot of queries, the results might change if weuse another set of queries. Right, so this is then not so if you have c high variancethen it's not very reliable. So let's look at these resultsagain in the second case. So, here we show two differentways to compare them. One is a sign test wherewe just look at the sign. If System B is better than System A,we have a plus sign. When System A is better wehave a minus sign, etc. Using this case, if you see this,well, there are seven cases. We actually have four caseswhere System B is better. But three cases of System A is better,intuitively, this is almost like a random results,right? So if you just take a randomsample of you flip seven coins and if you use plus to denote the head andminus to denote the tail and that could easily be the results of justrandomly flipping these seven coins. So, the fact that the average islarger doesn't tell us anything. We can't reliably conclude that. And this can be quantitativelymeasured by a p value. And that basically means the probability that this result isin fact from a random fluctuation. In this case, probability is 1.0. It means it surely isa random fluctuation. Now in Willcoxan test,it's a non-parametric test, and we would be not onlylooking at the signs, we'll be also looking atthe magnitude of the difference. But we can draw a similar conclusion, where you say it's verylikely to be from random. To illustrate this, let's thinkabout that such a distribution. And this is called a now distribution. We assume that the mean is zero here. Lets say we started withassumption that there's no difference between the two systems. But we assume that because of randomfluctuations depending on the queries, we might observe a difference. So the actual difference mightbe on the left side here or on the right side here, right? So, and this curve kind of showsthe probability that we will actually observe values thatare deviating from zero here. Now, so if we look at this picture then,we see that if a difference is observed here, then the chance is very high that this isin fact a random observation, right? We can define a region oflikely observation because of random fluctuation andthis is that 95% of all the outcomes. And in this then the observed maystill be from random fluctuation. But if you observe a value in thisregion or a difference on this side, then the difference is unlikelyfrom random fluctuation. All right, so there's a very smallprobability that you are observe such a difference just becauseof random fluctuation. So in that case, we can then concludethe difference must be real. So System B is indeed better. So this is the idea ofStatical Significance Test. The takeaway message here is that youhave to use many queries to avoid jumping into a conclusion. As in this case,to say System B is better. There are many different ways of doingthis Statistical Significance Test. So now, let's talk about the otherproblem of making judgments and, as we said earlier,it's very hard to judge all the documents completely unless it'sa very small data set. So the question is,if we can afford judging all the documents in the collection,which is subset should we judge? And the solution here is Pooling. And this is a strategy that has been usedin many cases to solve this problem. So the idea of Pooling is the following. We would first choose a diverseset of ranking methods. These are Text Retrieval systems. And we hope these methods can help usnominate like the relevant documents. So the goal is to pick outthe relevant documents. We want to make judgements on relevantdocuments because those are the most useful documents from users perspectives. So then we're going to haveeach to return top-K documents. The K can vary from systems. But the point is to ask them to suggestthe most likely relevant documents. And then we simply combineall these top-K sets to form a pool of documents forhuman assessors. To judge, so imagine you have manysystems each were ten k documents. We take the top-K documents,and we form a union. Now, of course, there are manydocuments that are duplicated because many systems might have retrievedthe same random documents. So there will be some duplicate documents. And there are also unique documentsthat are only returned by one system. So the idea of having diverse set of ranking methods is toensure the pool is broad. And can include as many possiblerelevant documents as possible. And then, the users would,human assessors would make complete the judgments on this data set, this pool. And the other unjudged the documents areusually just assumed to be non relevant. Now if the pool is large enough,this assumption is okay. But if the pool is not very large,this actually has to be reconsidered. And we might use otherstrategies to deal with them and there are indeed othermethods to handle such cases. And such a strategy is generally okay for comparing systems thatcontribute to the pool. That means if you participatein contributing to the pool, then it's unlikely that itwould penalize your system because the problematicdocuments have all been judged. However, this is problematic for evaluating a new system that mayhave not contributed to the pool. In this case, a new system mightbe penalized because it might have nominated some read only documentsthat have not been judged. So those documents might beassumed to be non relevant. That's unfair. So to summarize the whole part of textualevaluation, it's extremely important. Because the problem is the empiricallydefined problem, if we don't rely on users, there's no way totell whether one method works better. If we have in the propertyexperiment design, we might misguide our research orapplications. And we might just draw wrong conclusions. And we have seen this isin some of our discussions. So make sure to get it right foryour research or application. The main methodology is the Cranfieldevaluation methodology. And they are the main paradigm used inall kinds of empirical evaluation tasks, not just a search engine variation. Map and nDCG are the two mainmeasures that you should definitely know about and they are appropriate forcomparing ranking algorithms. You will see them oftenin research papers. Precision at 10 documents is easierto interpret from user's perspective. So that's also often useful. What's not covered is some otherevaluation strategy like A-B Test. Where the system would mix two,the results of two methods, randomly. And then would showthe mixed results to users. Of course, the users don't seewhich result, from which method. The users would judge those results or click on those documents ina search engine application. In this case then, the search enginecan check or click the documents and see if one method has contributedmore through the click the documents. If the user tends to click on one,the results from one method, then it suggests thatmessage may be better. So this is what leverages the real usersof a search engine to do evaluation. It's called A-B Test andit's a strategy that is often used by the modern search engines orcommercial search engines. Another way to evaluate IR or textual retrieval is user studies andwe haven't covered that. I've put some references herethat you can look at if you want to know more about that. So, there are threeadditional readings here. These are three mini books aboutevaluation and they are all excellent in covering a broad review ofInformation Retrieval Evaluation. And it covers some of the thingsthat we discussed, but they also have a lot of others to offer. [MUSIC]"
cs-410,4,1,"[SOUND]This lecture is aboutthe Probabilistic Retrieval Model. In this lecture, we're going to continue the discussionof the Text Retrieval Methods. We're going to look at another kind ofvery different way to design ranking functions than the Vector Space Modelthat we discussed before. In probabilistic models,we define the ranking function, based on the probability that thisdocument is relevant to this query. In other words, we introducea binary random variable here. This is the variable R here. And we also assume that the query and the documents are all observationsfrom random variables. Note that in the vector-based models,we assume they are vectors, but here we assume they are the dataobserved from random variables. And so, the problem of retrieval becomesto estimate the probability of relevance. In this category of models,there are different variants. The classic probabilistic model hasled to the BM25 retrieval function, which we discussed in inthe vectors-based model because its a form is actuallysimilar to a backwards space model. In this lecture,we will discuss another sub class in this P class called a languagemodeling approaches to retrieval. In particular, we're going to discussthe query likelihood retrieval model, which is one of the most effectivemodels in probabilistic models. There was also another line calledthe divergence from randomness model which has led to the PL2 function, it's also one of the most effectivestate of the art retrieval functions. In query likelihood, our assumptionis that this probability of relevance can be approximated by the probabilityof query given a document and relevance. So intuitively, this probability justcaptures the following probability. And that is if a user likes document d,how likely would the user enter query q ,inorder to retrieve document d? So we assume that the user likes d,because we have a relevance value here. And then we ask the question about howlikely we'll see this particular query from this user? So this is the basic idea. Now, to understand this idea,let's take a look at the general idea or the basic idea ofProbabilistic Retrieval Models. So here, I listed some imaginedrelevance status values or relevance judgments of queries anddocuments. For example, in this line, it shows that q1 is a querythat the user typed in. And d1 is a documentthat the user has seen. And 1 means the user thinksd1 is relevant to q1. So this R here can be also approximatedby the click-through data that a search engine can collect by watching how youinteracted with the search results. So in this case, let's saythe user clicked on this document. So there's a 1 here. Similarly, the user clicked on d2 also,so there is a 1 here. In other words,d2 is assumed to be relevant to q1. On the other hand,d3 is non-relevant, there's a 0 here. And d4 is non-relevant and then d5 isagain, relevant, and so on and so forth. And this part, maybe,data collected from a different user. So this user typed in q1 and then foundthat the d1 is actually not useful, so d1 is actually non-relevant. In contrast, here we see it's relevant. Or this could be the same query typedin by the same user at different times. But d2 is also relevant, etc. And then here,we can see more data about other queries. Now, we can imagine wehave a lot of such data. Now we can ask the question, how can we then estimatethe probability of relevance? So how can we compute thisprobability of relevance? Well, intuitively that just means if we look at all the entrieswhere we see this particular d and this particular q, how likely we'llsee a one on this other column. So basically that just means thatwe can just collect the counts. We can first count how manytimes we have seen q and d as a pair in this table andthen count how many times we actually have also seen1 in the third column. And then, we just compute the ratio. So let's take a look atsome specific examples. Suppose we are trying to compute thisprobability for d1, d2 and d3 for q1. What is the estimated probability? Now, think about that. You can pause the video if needed. Try to take a look at the table. And try to give yourestimate of the probability. Have you seen that,if we are interested in q1 and d1, we'll be looking at these two pairs? And in both cases, well, actually, in one of the cases, the userhas said this is 1, this is relevant. So R = 1 in only one of the two cases. In the other case, it's 0. So that's one out of two. What about the d1 and the d2? Well, they are here, d1 and d2, d1 and d2, in both cases, in this case, R = 1. So it's a two out of two andso on and so forth. So you can see with this approach, we can actually score these documents forthe query, right? We now have a score for d1,d2 and d3 for this query. And we can simply rank thembased on these probabilities and so that's the basic ideaprobabilistic retrieval model. And you can see it makes a lot of sense,in this case, it's going to rank d2 aboveall the other documents. Because in all the cases,when you have c and q1 and d2, R = 1. The user clicked on this document. So this also should show thatwith a lot of click-through data, a search engine can learn a lot fromthe data to improve their search engine. This is a simple examplethat shows that with even with small amount of entries here we canalready estimate some probabilities. These probabilities would give ussome sense about which document might be more relevant or more usefulto a user for typing this query. Now, of course, the problems that wedon't observe all the queries and all the documents andall the relevance values, right? There would be a lot of unseen documents,in general, we have only collected the data from thedocuments that we have shown to the users. And there are even more unseen queriesbecause you cannot predict what queries will be typed in by users. So obviously,this approach won't work if we apply it to unseen queries or unseen documents. Nevertheless, this shows the basic ideaof probabilistic retrieval model and it makes sense intuitively. So what do we do in such a case whenwe have a lot of unseen documents and unseen queries? Well, the solutions that we haveto approximate in some way. So in this particular case calleda query likelihood retrieval model, we just approximate this byanother conditional probability. p(q given d, R=1). So in the condition part, we assume thatthe user likes the document because we have seen that the userclicked on this document. And this part shows thatwe're interested in how likely the user wouldactually enter this query. How likely we will see thisquery in the same row. So note that here, we have madean interesting assumption here. Basically, we're going to do, assume thatwhether the user types in this query has something to do with whetheruser likes the document. In other words,we actually make the following assumption. And that is a user formulates a querybased on an imaginary relevant document. Where if you just look at thisas conditional probability, it's not obvious weare making this assumption. So what I really meant is thatto use this new conditional probability to help us score,then this new conditional probability will have to somehowbe able to estimate this conditional probability withoutrelying on this big table. Otherwise we would be havingsimilar problems as before, and by making this assumption,we have some way to bypass this big table, and try to just model how the userformulates the query, okay? So this is how you cansimplify the general model so that we can derive a specificrelevant function later. So let's look at how this model work forour example. And basically, what we are going to do in this caseis to ask the following question. Which of these documents is mostlikely the imaginary relevant document in the user's mind whenthe user formulates this query? So we ask this question and we quantifythe probability and this probability is a conditional probability of observingthis query if a particular document is in fact the imaginary relevantdocument in the user's mind. Here you can see we've computed allthese query likelihood probabilities. The likelihood of queriesgiven each document. Once we have these values, we can then rank these documentsbased on these values. So to summarize, the general ideaof modern relevance in the proper risk model is to assume the we introducea binary random variable R, here. And then, let the scoring function be definedbased on this conditional probability. We also talked about approximatingthis by using the query likelihood. And in this case we have a rankingfunction that's basically based on the probability ofa query given the document. And this probability should be interpretedas the probability that a user who likes document d, would pose query q. Now, the question of course is, how dowe compute this conditional probability? At this in general has to do with howyou compute the probability of text, because q is a text. And this has to do with a modelcalled a Language Model. And these kind of modelsare proposed to model text. So more specifically, we will bevery interested in the following conditional probabilityas is shown in this here. If the user liked this document,how likely the user would pose this query. And in the next lecture we're going to do, giving introduction to languagemodels that we can see how we can model text that was a probablerisk model, in general. [MUSIC]"
cs-410,4,2,"[SOUND] This lecture is aboutthe statistical language model. In this lecture, we're going to give an introductionto statistical language model. This has to do with how do you modeltext data with probabilistic models. So it's related to how we modelquery based on a document. We're going to talk aboutwhat is a language model. And then we're going to talk about thesimplest language model called the unigram language model, which also happens to bethe most useful model for text retrieval. And finally, what this classwill use is a language model. What is a language model? Well, it's just a probabilitydistribution over word sequences. So here, I'll show one. This model gives the sequence Todayis Wednesday a probability of 0.001. It give Today Wednesday is a very, very small probabilitybecause it's non-grammatical. You can see the probabilitiesgiven to these sentences or sequences of words can varya lot depending on the model. Therefore, it's clearly context dependent. In ordinary conversation, probably Today is Wednesday is mostpopular among these sentences. Imagine in the context ofdiscussing apply the math, maybe the eigenvalue is positive,would have a higher probability. This means it can be used torepresent the topic of a text. The model can also be regardedas a probabilistic mechanism for generating text. And this is why it's also oftencalled a generating model. So what does that mean? We can imagine this is a mechanism that's visualised here as a stochastic systemthat can generate sequences of words. So, we can ask for a sequence,and it's to send for a sequence from the device if you want,and it might generate, for example, Today is Wednesday, but it couldhave generated any other sequences. So for example,there are many possibilities, right? So in this sense,we can view our data as basically a sample observed fromsuch a generating model. So, why is such a model useful? Well, it's mainly because it can quantifythe uncertainties in natural language. Where do uncertainties come from? Well, one source is simplythe ambiguity in natural language that we discussed earlier in the lecture. Another source is because we don'thave complete understanding, we lack all the knowledgeto understand the language. In that case,there will be uncertainties as well. So let me show some examples of questionsthat we can answer with a language model that would have interestingapplications in different ways. Given that we see John and feels,how likely will we see happy as opposed to habit as the nextword in a sequence of words? Now, obviously, this would be very usefulfor speech recognition because happy and habit would have similar acoustic sound,acoustic signals. But, if we look at the language model, we know that John feels happy would befar more likely than John feels habit. Another example, given that weobserve baseball three times and game once in a news article,how likely is it about sports? This obviously is related to textcategorization and information retrieval. Also, given that a user isinterested in sports news, how likely would the useruse baseball in a query? Now, this is clearly relatedto the query likelihood that we discussed in the previous lecture. So now,let's look at the simplest language model, called a unigram language model. In such a case, we assume that we generate a text bygenerating each word independently. So this means the probability ofa sequence of words would be then the product ofthe probability of each word. Now normally,they're not independent, right? So if you have single word in likea language, that would make it far more likely to observe model than ifyou haven't seen the language. So this assumption is notnecessarily true, but we make this assumptionto simplify the model. So now the model has precisely Nparameters, where N is vocabulary size. We have one probability for each word, andall these probabilities must sum to 1. So strictly speaking,we actually have N-1 parameters. As I said,text can then be assumed to be assembled, drawn from this word distribution. So for example,now we can ask the device or the model to stochastically generatethe words for us, instead of sequences. So instead of giving a whole sequence, like Today is Wednesday,it now gives us just one word. And we can get all kinds of words. And we can assemble thesewords in a sequence. So that will still allow youto compute the probability of Today is Wednesday as the productof the three probabilities. As you can see, even though we have notasked the model to generate the sequences, it actually allows us to computethe probability for all the sequences, but this model now only needs Nparameters to characterize. That means if we specifyall the probabilities for all the words, then the model'sbehavior is completely specified. Whereas if we don't make this assumption,we would have to specify probabilities for all kinds of combinationsof words in sequences. So by making this assumption, it makes itmuch easier to estimate these parameters. So let's see a specific example here. Here I show two unigram languagemodels with some probabilities. And these are high probabilitywords that are shown on top. The first one clearly suggestsa topic of text mining, because the high probabilitywas all related to this topic. The second one is more related to health. Now we can ask the question, how likely were observe a particulartext from each of these two models? Now suppose we samplewords to form a document. Let's say we take the first distribution,would you like to sample words? What words do you think would begenerated while making a text or maybe mining maybe another word? Even food, which has a very small probability,might still be able to show up. But in general, high probabilitywords will likely show up more often. So we can imagine what general textof that looks like in text mining. In fact, with small probability, you might be able to actually generatethe actual text mining paper. Now, it will actually be meaningful,although the probability will be very, very small. In an extreme case, you mightimagine we might be able to generate a text mining paper that would beaccepted by a major conference. And in that case,the probability would be even smaller. But it's a non-zero probability, if we assume none of the wordshave non-zero probability. Similarly from the second topic, we can imagine we can generatea food nutrition paper. That doesn't mean we cannot generate thispaper from text mining distribution. We can, but the probability would be very,very small, maybe smaller than even generating a paper that can be acceptedby a major conference on text mining. So the point is thatthe keeping distribution, we can talk about the probability ofobserving a certain kind of text. Some texts will have higherprobabilities than others. Now let's look at the problemin a different way. Suppose we now have availablea particular document. In this case, many of the abstract orthe text mining table, and we see these word counts here. The total number of words is 100. Now the question you ask hereis an estimation question. We can ask the question which model, which one of these distribution hasbeen used to generate this text, assuming that the text has been generatedby assembling words from the distribution. So what would be your guess? What we have to decide are whatprobabilities text mining, etc., would have. Suppose the view for a second, andtry to think about your best guess. If you're like a lot of people,you would have guessed that well, my best guess is text has a probabilityof 10 out of 100 because I've seen text 10 times, andthere are in total 100 words. So we simply normalize these counts. And that's in fact the word justified, and your intuition is consistentwith mathematical derivation. And this is called the maximumlikelihood estimator. In this estimator,we assume that the parameter settings of those that would give our observethe data the maximum probability. That means if we change theseprobabilities, then the probability of observing the particular textdata would be somewhat smaller. So you can see,this has a very simple formula. Basically, we just need to look atthe count of a word in a document, and then divide it by the total number ofwords in the document or document lens. Normalize the frequency. A consequence of this is, of course, we're going to assignzero probabilities to unseen words. If we have an observed word, there will be no incentive to assign anon-zero probability using this approach. Why? Because that would take away probabilitymass for these observed words. And that obviously wouldn't maximize the probability of thisparticular observed text data. But one has still question whetherthis is our best estimate. Well, the answer depends on what kindof model you want to find, right? This estimator gives a best modelbased on this particular data. But if you are interested in a modelthat can explain the content of the full paper for this abstract, then youmight have a second thought, right? So for thing,there should be other words in the body of that article, sothey should not have zero probabilities, even though they're notobserved in the abstract. So we're going to cover thisa little bit more later in this class in the querylikelihood model. So let's take a look at some possibleuses of these language models. One use is simply to useit to represent the topics. So here I show some generalEnglish background texts. We can use this text toestimate a language model, and the model might look like this. Right, so on the top, we have thoseall common words, the, a, is, we, etc., and then we'll see somecommon words like these, and then some very,very rare words in the bottom. This is a background language model. It represents the frequency ofwords in English in general. This is the background model. Now let's look at another text,maybe this time, we'll look at the computerscience research papers. So we have a collection ofcomputer science research papers, we do as mentioned again, we can justuse the maximum likelihood estimator, where we simply normalize the frequencies. Now in this case, we'll getthe distribution that looks like this. On the top, it looks similar becausethese words occur everywhere, they are very common. But as we go down,we'll see words that are more related to computer science,computer software, text, etc. And so although here, we might also seethese words, for example, computer, but we can imagine the probability here ismuch smaller than the probability here. And we will see many other words here thatwould be more common in general English. So you can see this distributioncharacterizes a topic of the corresponding text. We can look at even the smaller text. So in this case,let's look at the text mining paper. Now if we do the same,we have another distribution, again the can be expectedto occur in the top. The sooner we see text, mining,association, clustering, these words have relativelyhigh probabilities. In contrast, in this distribution, thetext has a relatively small probability. So this means, again,based on different text data, we can have a different model,and the model captures the topic. So we call this documentthe language model, and we call this collection language model. And later, you will see how they'reused in the retrieval function. But now,let's look at another use of this model. Can we statistically find what wordsare semantically related to computer? Now how do we find such words? Well, our first thought is that let's takea look at the text that match computer. So we can take a look at all the documentsthat contain the word computer. Let's build a language model. We can see what words we see there. Well, not surprisingly, we see thesecommon words on top as we always do. So in this case, this language model givesus the conditional probability of seeing the word in the context of computer. And these common words willnaturally have high probabilities. But we also see the computer itself and software will have relativelyhigh probabilities. But if we just use this model, we cannot just say all these wordsare semantically related to computer. So ultimately, what we'd like toget rid of is these common words. How can we do that? It turns out that it's possibleto use language model to do that. But I suggest you think about that. So how can we know whatwords are very common, so that we want to kindof get rid of them? What model will tell us that? Well, maybe you can think about that. So the background language modelprecisely tells us this information. It tells us what wasour common in general. So if we use this background model, we would know that these wordsare common words in general. So it's not surprising to observethem in the context of computer. Whereas computer has a verysmall probability in general, so it's very surprising that we have seencomputer with this probability, and the same is true for software. So then we can use these twomodels to somehow figure out the words that are related to computer. For example, we can simply take the ratioof these group probabilities and normalize the topic of language modelby the probability of the word in the background language model. So if we do that, we take the ratio,we'll see that then on the top, computer is ranked, andthen followed by software, program, all these wordsrelated to computer. Because they occur very frequently in thecontext of computer, but not frequently in the whole collection, whereas these commonwords will not have a high probability. In fact,they have a ratio about 1 down there because they are not reallyrelated to computer. By taking the sample of textthat contains the computer, we don't really see more occurrencesof that than in general. So this shows that even withthese simple language models, we can do some limitedanalysis of semantics. So in this lecture,we talked about language model, which is basically a probabilitydistribution over text. We talked about the simplest languagemodel called unigram language model, which is also just a word distribution. We talked about the twouses of a language model. One is we represent the topic in adocument, in a collection, or in general. The other is we discoverword associations. In the next lecture, we're going to talkabout how language model can be used to design a retrieval function. Here are two additional readings. The first is a textbook on statisticalnatural language processing. The second is an article thathas a survey of statistical language models with a lot ofpointers to research work. [MUSIC]"
cs-410,4,3,"[SOUND] This lecture is about query likelihood, probabilistic retrieval model. In this lecture, we continue the discussion ofprobabilistic retrieval model. In particular, we're going to talk aboutthe query light holder retrieval function. In the query light holder retrieval model,our idea is model. How like their user who likes a documentwith pose a particular query? So in this case,you can imagine if a user likes this particular document abouta presidential campaign news. Now we assume,the user would use this a document as a basis to impose a query to try andretrieve this document. So again, imagine use a processthat works as follows. Where we assume thatthe query is generated by assembling words from the document. So for example, a user mightpick a word like presidential, from this document andthen use this as a query word. And then the user would pickanother word like campaign, and that would be the second query word. Now this of course is an assumptionthat we have made about how a user would pose a query. Whether a user actually followed thisprocess may be a different question, but this assumption has allowed us to formerlycharacterize this conditional probability. And this allows us to also not rely onthe big table that I showed you earlier to use empirical data toestimate this probability. And this is why we can use thisidea then to further derive retrieval function that we canimplement with the program language. So as you see the assumptionthat we made here is each query word is independent of the sample. And also each word is basicallyobtained from the document. So now let's see how this works exactly. Well, since we are completinga query likelihood then the probability here is justthe probability of this particular query, which is a sequence of words. And we make the assumption that eachword is generated independently. So as a result, the probabilityof the query is just a product of the probability of each query word. Now how do we computethe probability of each query word? Well, based on the assumption that a word is picked from the documentthat the user has in mind. Now we know the probability of each wordis just the relative frequency of each word in the document. So for example, the probability ofpresidential given the document. Would be just the countof presidential document divided by the total number of wordsin the document or document s. So with these assumptions we now haveactually a simple formula for retrieval. We can use this to rank our documents. So does this model work? Let's take a look. Here are some example documentsthat you have seen before. Suppose now the query ispresidential campaign and we see the formula here on the top. So how do we score this document? Well, it's very simple. We just count how many times dowe have seen presidential or how many times do we have seen campaigns,etc. And we see here 44, andwe've seen presidential twice. So that's 2 over the length ofdocument 4 multiplied by 1 over the length of document 4 forthe probability of campaign. And similarly, we can get probabilitiesfor the other two documents. Now if you look at these numbers orthese formulas for scoring all these documents,it seems to make sense. Because if we assume d3 andd4 have about the same length, then looks like a nominal rank d4above d3 and which is above d2. And as we would expect,looks like it did captures a TF query state, and sothis seems to work well. However, if we try a differentquery like this one, presidential campaign updatethen we might see a problem. Well what problem? Well think about the update. Now none of these documentshas mentioned update. So according to our assumption that a userwould pick a word from a document to generate a query, then the probability ofobtaining the word update would be what? Would be 0. So that causes a problem,because it would cause all these documents to have zero probabilityof generating this query. Now why it's fine to have zero probabilityfor d2, which is non-relevant? It's not okay to have 0 for d3 and d4 because now we no longercan distinguish them. What's worse? We can't even distinguish them from d2. So that's obviously not desirable. Now when a [INAUDIBLE] has such result, we should think about whathas caused this problem? So we have to examine whatassumptions have been made, as we derive this ranking function. Now is you examine those assumptionscarefully you will realize, what has caused this problem? So take a moment to think about it. What do you think is the reason why updatehas zero probability and how do we fix it? So if you think about this from the momentyou realize that that's because we have made an assumptionthat every query word must be drawn from the documentin the user's mind. So in order to fix this, we have toassume that the user could have drawn a word not necessarily from the document. So that's the improved model. An improvement here is to say that, well instead of drawinga word from the document, let's imagine that the user would actuallydraw a word from a document model. And so I show a model here. And we assume that this document isgenerated using this unigram language model. Now, this model doesn't necessarily assignzero probability for update in fact, we can assume this model does notassign zero probability for any word. Now if we're thinking this way thenthe generation process is a little bit different. Now the user has this model in mindinstead of this particular document. Although the model has to beestimated based on the document. So the user can again generatethe query using a singular process. Namely, pick a word for example,presidential and another word campaign. Now the difference is that this timewe can also pick a word like update, even though update doesn'toccur in the document to potentially generatea query word like update. So that a query was updated1 times 0 probabilities. So this would fix our problem. And it's also reasonable because when ourthinking of what the user is looking for in a more general way, that is uniquelanguage model instead of fixed document. So how do we computethis query likelihood? If we make this sum wideinvolved two steps. The first one is compute this model, andwe call it document language model here. For example, I've shown two pulse modelshere, it's major based on two documents. And then given a query like a data miningalgorithms the thinking is that we'll just compute the likelihood of this query. And by making independenceassumptions we could then have this probability as a product ofthe probability of each query word. We do this for both documents, andthen we can score these two documents and then rank them. So that's the basic idea of thisquery likelihood retrieval function. So more generally this ranking functionwould look like in the following. Here we assume that the query has n words, w1 through wn, andthen the scoring function. The ranking function is the probabilitythat we observe this query, given that the user isthinking of this document. And this is assume it will be product ofprobabilities of all individual words. This is based on independent assumption. Now we actually often scorethe document before this query by using log of the query likelihoodas shown on the second line. Now we do this to avoid having a lot of small probabilities,mean multiply together. And this could cause under flow and wemight loose the precision by transforming the value in our algorithm function. We maintain the order of these documentsyet we can avoid the under flow problem. And so if we take longer thantransformation of course, the product would become a sumas you on the second line here. So the sum of all the querywords inside of the sum that is one of the probability ofthis word given by the document. And then we can further rewritethe sum to a different form. So in the first sum here, in this sum, we have it over all the query words andquery word. And in this sum we have a sumof all the possible words. But we put a counter hereof each word in the query. Essentially we are only consideringthe words in the query, because if a word is not in the query,the count will be 0. So we're still consideringonly these n words. But we're using a different form asif we were going to take a sample of all the words in the vocabulary. And of course, a word might occurmultiple times in the query. That's why we have a count here. And then this part is log ofthe probability of the word, given by the document language model. So you can see in this retrieval function, we actually know the countof the word in the query. So the only thing that we don't knowis this document language model. Therefore, we have convertedthe retrieval problem include the problem of estimatingthis document language model. So that we can compute the probability ofeach query word given by this document. And different estimation methods wouldlead to different ranking functions. This is just like a different way toplace document in the vector space which leads to a different rankingfunction in the vector space model. Here different ways toestimate will lead to a different ranking function forquery likelihood. [MUSIC]"
cs-410,4,4,"[SOUND]This lecture is about smoothingof language models. In this lecture, we're going to continue talking aboutthe probabilistic retrieval model. In particular,we're going to talk about the smoothing of language model in the querylikelihood retrieval method. So you have seen this slidefrom a previous lecture. This is the ranking functionbased on the query likelihood. Here, we assume that the independence ofgenerating each query word And the formula would look like the following wherewe take a sum of all the query words. And inside the sum there is a logof probability of a word given by the document or document image model. So the main task now is to estimate this document language model as wesaid before different methods for estimating this model would leadto different retrieval functions. So in this lecture, we're going tobe looking to this in more detail. So how do we estimate this language model? Well the obvious choice would bethe maximum likelihood estimate that we have seen before. And that is we're going to normalizethe word frequencies in the document. And estimate the probabilityit would look like this. This is a step function here. Which means all of the words that have the same frequency count willhave identical problem with it. This is another freedom to count,that has different probability. Note that for words that have notoccurred in the document here they will have 0 probability. So we know this is just like the modelthat we assume earlier in the lecture. Where we assume that the use ofthe simple word from the document to a formula to clear it. And there's no chance of assembling anyword that's not in the document and we know that's not good. So how do we improve this? Well in order to assigna none 0 probability to words that have not been observed inthe document, we would have to take away some probability mass from the wordsthat are observed in the document. So for example here, we have to take awaysome probability of the mass because we need some extra probability mass forthe words otherwise they won't sum to 1. So all these probabilities must sum to 1. So to make this transformation and toimprove the maximum likelihood estimated by assigning non zero probabilities towords that are not observed in the data. We have to do smoothing andsmoothing has to do with improving the estimate by consideringthe possibility that if the author had been asking to write more words for the document,the author might have written other words. If you think about this factorthen the a smoothed language model would be a more accurate thanthe representation of the actual topic. Imagine you have seen an abstractof a research article. Let's say this document is abstract. If we assume and see words in thisabstract that we have a probability of 0. That would mean there'sno chance of sampling a word outside the abstractof the formulated query. But imagine a user who is interestedin the topic of this subject. The user might actuallychoose a word that's not in that chapter to use as query. So obviously,if we has asked this author to write more author would have writtena full text of the article. So smoothing of the languagemodel is an attempt to try to recover the model forthe whole article. And then of course,we don't have knowledge about any words that are notobserved in the abstract. So that's why smoothing isactually a tricky problem. So let's talk a little more abouthow to smooth a language model. The key question here is, what probabilityshould be assigned to those unseen words? And there are many differentways of doing that. One idea here, that's very useful forretrieval is let the probability of unseen word be proportional to its probabilitygiven by a reference language model. That means if you don't observethe word in the dataset. We're going to assume that itsprobability is kind of governed by another reference languagemodel that we will construct. It will tell us which unseen wordswould have a higher probability. In the case of retrieval,a natural choice would be to take the collection language modelas the reference language model. That is to say, if you don'tobserve a word in the document, we're going to assume thatthe probability of this word would be proportional to the probabilityof word in the whole collection. So more formally, we'll be estimating the probabilityof a word key document as follows. If the word is seen inthe document then the probability would be this counted the maximumlikelihood estimate P sub c here. Otherwise, if the word is not seen in thedocument we're going to let probability be proportional to the probabilityof the word in the collection. And here the coefficient that offer is to control the amount of probabilitymass that we assign to unseen words. Obviously, all theseprobabilities must sum to 1, so alpha sub d is constrained in some way. So what if we plug in thissmoothing formula into our query likelihood ranking function? This is what we will get. In this formula, we have this as a sum over all the query words and those that we have written here as the sumof all the vocabulary, you see here. This is the sum of allthe words in the vocabulary, but not that we have a countof the word in the query. So in fact, we are just takinga sample of query words. This is now a commonway that we would use, because of its conveniencein some transformations. So this is as I said,this is sum of all the query words. In our smoothing method,we assume that the words that are not observed in the method would havea somewhat different form of probability. Name it's four, this foru. So we're going to do then,decompose the sum into two parts. One sum is over all the query wordsthat are matching the document. That means that in this sum, all the words have a non zero probabilityin the document. Sorry, it's the non zero countof the word in the document. They all occur in the document. And they also have to of coursehave a non zero count in the query. So these are the query wordsthat are matching the document. On the other hand, in this sum weare taking a sum of all the words that are not all query wasnot matching the document. So they occur in the querydue to this term, but they don't occur in the document. In this case, these words have this probability becauseof our assumption about the smoothing. That here, these seen wordshave a different probability. Now, we can go further byrewriting the second sum as a difference of two other sums. Basically, the first sum isthe sum of all the query words. Now, we know that the original sumis not over all the query words. This is over all the query words thatare not matched in the document. So here we pretend that theyare actually over all the query words. So we take a sum over all the query words. Obviously, this sum has extraterms that are not in this sum. Because, here we're takingsum over all the query words. There, it's not matched in the document. So in order to make them equal, we willhave to then subtract another sum here. And this is the sum over all the querywords that are matching in the document. And this makes sense, because herewe are considering all query words. And then we subtract the querythat was matched in the document. That would give us the query thatwas not matched in the document. And this is almost a reverseprocess of the first step here. And you might wonder whydo we want to do that. Well, that's because if we do this, then we have different formsof terms inside of these sums. So now, you can see in this sumwe have all the words matched, the query was matching the documentwith this kind of term. Here we have another sum over the same setof terms, matched query terms in document. But inside the sum, it's different. But these two sums can clearly be merged. So if we do that, we'll get another form of the formula that looks likebefore me at the bottom here. And note that this isa very interesting formula. Because here we combinethese two that all or some of the query words matching inthe document in the one sum here. And the other sum now isdecomposing into two parts. And these two partslook much simpler just, because these are the probabilitiesof unseen words. This formula is very interestingbecause you can see the sum is now over the match the query terms. And just like in the vector space model,we take a sum of terms that are in the intersection ofquery vector and the document vector. So it already looks a little bitlike the vector space model. In fact, there's even more similarityhere as we explain on this slide. [MUSIC]"
cs-410,4,5,"[SOUND] So I showed you how we rewrite the query like holder which is a function intoa form that looks like the formula of this slide after if we makethe assumption about the smoothing, the language model based onthe collection language model. Now if you look at this rewriting,it will actually give us two benefits. The first benefit is it helps us betterunderstand this ranking function. In particular, we're going to show thatfrom this formula we can see smoothing with the collection language model wouldgive us something like a TF-IDF weighting and length normalization. The second benefit is thatit also allows us to compute the query like holder more efficiently. In particular we see thatthe main part of the formula is a sum over the matchof the query terms. So this is much better than if wetake a sum over all the words. After we smooth the document the damagemodel we essentially have non zero problem for all the words. So this new form of the formula ismuch easier to score or to compute. It's also interesting to note that the last term here is actuallyindependent of the document. Since our goal is torank the documents for the same query we can ignore this term forranking. Because it's going to be the same forall the documents. Ignoring it wouldn't affectthe order of the documents. Inside the sum, we also see that each matched queryterm would contribute a weight. And this weight actually is very interesting because itlooks like a TF-IDF weighting. First we can already see it hasa frequency of the word in a query just like in the vector space model. When we take a thought product, we see the word frequency inthe query to show up in such a sum. And so naturally this part wouldcorrespond between the vector element from the documented vector. And here indeed we can see it actually encodes a weight that has similarin factor to TF-IDF weight. I'll let you examine it, can you see it? Can you see which part is capturing TF? And which part isa capturing IDF weighting? So if want you can pausethe video to think more about it. So have you noticed that this P subseen is related to the term frequency in the sense that if a word occursvery frequently in the document, then the s made through probabilityhere will tend to be larger. So this means this term is reallydoing something like a TF weight. Now have you also noticed thatthis term in the denominator is actually achieving the factor of IDF? Why, because this is the popularityof the term in a collection. But it's in the denominator, so if theprobability in the collection is larger then the weight is actually smaller. And this means a popular term. We actually have a smaller weight and thisis precisely what IDF weighting is doing. Only that we now havea different form of TF and IDF. Remember IDF has a logarithmof documented frequency. But here we have something different. But intuitively itachieves a similar effect. Interestingly, we also have somethingrelated to the length of libation. Again, can you see which factor is relatedto the document length in this formula? What I just say is that this termis related to IDF weighting. This collection probability,but it turns out that this term here is actually relatedto document length normalization. In particular, F of sub d mightbe related to document length. So it encodes how much probabilitymass we want to give to unseen worlds. How much smoothing do we want to do? Intuitively, if a document is long, then we need to do less smoothing becausewe can assume that data is large enough. We probably have observed all the wordsthat the author could have written. But if the document is short then r ofsub t could be expected to be large. We need to do more smoothing. It's likey there are words that havenot been written yet by the author. So this term appears to paralyzethe non document in that other sub D would tend to be longerthan or larger than for a long document. But note that alpha sub dalso occurs here and so this may not actually be necessaryparalyzing long documents. The effect is not so clear yet. But as we will see later, when weconsider some specific smoothing methods, it turns out that they doparalyze long documents. Just like in TF-IDF weighting and document length normalizationformula in the vector space model. So, that's a very interestingobservation because it means we don't even have to think aboutthe specific way of doing smoothing. We just need to assume that if we smoothwith this collection memory model, then we would have a formula thatlooks like TF-IDF weighting and documents length violation. What's also interesting that we havevery fixed form of the ranking function. And see we have not heuristicallyput a logarithm here. In fact, you can think about whywe would have a logarithm here. You look at the assumptions thatwe have made, it would be clear it's because we have used a logarithmof query like for scoring. And we turned the product into a sumof logarithm of probability, and that's why we have this logarithm. Note that if only want to heuristicallyimplement a TF weighting and IDF weighting, we don't necessaryhave to have a logarithm here. Imagine if we drop this logarithm,we would still have TF and IDF weighting. But what's nice with problem risk modelingis that we are automatically given the logarithm function here. And that's basically a fixed formof the formula that we did not really have to heuristically design,and in this case if you try to drop the logarithm the model probably won'twork as well as if you keep the logarithm. So a nice property of problem riskmodeling is that by following some assumptions and the probability ruleswe'll get a formula automatically. And the formula would havea particular form like in this case. And if we heuristically designthe formula we may not necessarily end up having such a specific formula. So to summarize, we talked about the needfor smoothing the document imaging model. Otherwise it would give zero probabilityfor unseen words in the document, and that's not good forstoring a query with such an unseen word. It's also necessary, in general,to improve the accuracy of estimating the model representthe topic of this document. The general idea of smoothing in retrievalis to use the connecting memory model to, to give us some clue about which unseenwords should have a higher probability. That is, the probability of an unseenword is assumed to be proportional to its probability in the collection. With this assumption, we've shown that wecan derive a general ranking formula for query likelihood that haseffect of TF-IDF weighting and document length normalization. We also see that, through some rewriting, the scoring of such a ranking functionis primarily based on sum of weights on matched query terms,just like in the vector space model. But, the actual rankingfunction is given us automatically by the probability rules andassumptions that we have made. And like in the vector space modelwhere we have to heuristically think about the form of the function. However, we still need to addressthe question how exactly we should smooth the document and the model. How exactly we shoulduse the reference and model based on the connectionto adjust the probability of the maximum micro is made of andthis is the topic of the next batch. [MUSIC]"
cs-410,4,6,"[SOUND] This lecture is about the specific smoothing methods for language modelsused in probabilistic retrieval model. In this lecture, we will continuethe discussion of language models for information retrieval, particularlythe query likelihood retrieval method. And we're going to talk about specificallythe smoothing methods used for such a retrieval function. So this is a slide from a previouslecture where we show that with a query likelihood ranking and smoothingwith the collection language model, we add up having a retrieval functionthat looks like the following. So this is the retrieval function based onthese assumptions that we have discussed. You can see it's a sum of allthe matching query terms, here. And inside its sum is the countof the term in the query and some weight for the term in the document. We have t of i, the f weight here, andthen we have another constant here in n. So clearly if we want to implement thisfunction using programming language, we still need to figureout a few variables. In particular, we're going to need toknow how to estimate the probability of a word exactly and how do we set alpha. So in order to answer this question,we have to think about very specific smoothing methods, andthat is main topic of this lecture. We're going to talk abouttwo smoothing methods. The first is simple linearinterpolation with a fixed coefficient. And this is also calleda Jelinek-Mercer smoothing. So the idea is actually very simple. This picture shows howwe estimate a document language model by usingmaximum likelihood estimate. That gives us word counts normalized bythe total number of words in the text. The idea of using this method is to maximize the probabilityof the observed text. As a result,if a word like network is not observed in the text, it's going to get0 probability, as shown here. So the idea of smoothing, then,is to rely on collection language model where this word is not going to havea zero probability to help us decide what nonzero probability shouldbe assigned to such a word. So we can note that network hasa nonzero probability here. So in this approach what we do is we doa linear interpolation between the maximum likelihood placement here andthe collection language model, and this is computed by the smoothing parameterlambda, which is between 0 and 1. So this is a smoothing parameter. The larger lambda is,the more smoothing we will have. So by mixing them together, we achieve the goal of assigning nonzeroprobabilities to a word like network. So let's see how it works forsome of the words here. For example, if we computethe smooth probability for text. Now the maximum likelihoodestimated gives us 10 over 100, and that's going to be here. But the collection probability is this. So we'll just combine themtogether with this simple formula. We can also see the word network,which used to have a zero probability, now is getting a non-zeroprobability of this value. And that's because the count isgoing to be zero for network here. But this part is nonzero, andthat's basically how this method works. Now if you think about this andyou can easily see now the alpha sub d in this smoothingmethod is basically lambda. Because that's remember the coefficientin front of the probability of the word given by the collectionlanguage model here. Okay, sothis is the first smoothing method. The second one is similar butit has a tie-in into the coefficient for linear interpolation. It's often called Dirichlet Prior,or Bayesian, Smoothing. So again here we face problemof zero probability for an unseen word like network. Again we will use the collectionlanguage model, but in this case, we're going to combine themin somewhat different ways. The formula first can be seen asa interpolation of the maximum likelihood estimate andthe collection language model as before, as in the J-M smoothing method. Only that the coefficient nowis not lambda, a fixed number, but a dynamic coefficient in this form, where mu is a parameter,it's a non-negative value. And you can see if weset mu to a constant, the effect is that a long document wouldactually get a smaller coefficient here. Because a long documentwill have longer lengths, therefore the coefficientis actually smaller. And so a long document would haveless smoothing, as we would expect. So this seems to make more sensethan a fixed coefficient smoothing. Of course,this part would be of this form so that the two coefficients would sum to 1. Now this is one way tounderstand this smoothing. Basically, it means it's a dynamiccoefficient interpolation. There is another way to understandthis formula which is even easier to remember, andthat's on this side. So it's easier to see how we can rewritethe smoothing method in this form. Now in this form we can easilysee what change we have made to the maximum likelihood estimate,which would be this part. So normalize the countby the document length. So in this form we can see what we did iswe add this to the count of every word. So what does this mean? Well, this is basically something relatedto the probability of the word in the collection. And we multiply that by the parameter mu. And when we combine thiswith the count here, essentially we are addingpseudocounts to the observed text. We pretend every word hasgot this many pseudocount. So the total count would bethe sum of these pseudocounts and the actual count ofthe word in the document. As a result, in total we wouldhave added this many pseudocounts. Why?Because if you take somewhat this one over all the words, then we'll see theprobability of the words would sum to 1, and that gives us just mu. So this is the total number ofpseudocounts that we added. And sothese probabilities would still sum to 1. So in this case, we can easilysee the method is essentially to add this as a pseudocount to this data. Pretend we actually augment the databy including some pseudo data defined by the collection language model. As a result, we have more counts is that the total counts fora word would be like this. And as a result, even if a word has zerocount here, let's say if we have zero count here, then it would still havenonzero count because of this part. So this is how this method works. Let's also take a look atsome specific example here. So for text again we willhave 10 as the original count that we actually observe, butwe also add some pseudocount. And so the probability oftext would be of this form. Naturally, the probability ofnetwork would be just this part. And so here you can also seewhat's alpha sub d here. Can you see it? If you want to think about it,you can pause the video. But you'll notice that thispart is basically alpha sub d. So we can see, in this case, alpha sub d does depend on the document, because this lengthdepends on the document, whereas in the linear interpolation, the J-M smoothing method,this is a constant. [MUSIC]"
cs-410,4,7,"[SOUND] So let's plug in these model masses into the ranking function tosee what we will get, okay? This is a general smoothing. So a general ranking function forsmoothing with subtraction and you have seen this before. And now we have a very specific smoothingmethod, the JM smoothing method. So now let's see what what's a value foroffice of D here. And what's the value for p sub c here? Right, so we may need to decide this in order to figure out the exactform of the ranking function. And we also need to figureout of course alpha. So let's see. Well this ratio is basically this,right, so, here, this is the probabilityof c board on the top, and this is the probabilityof unseen war or, in other words basically 11times basically the alpha here, this, so it's easy to see that. This can be then rewritten as this. Very simple. So we can plug this into here. And then here, what's the value for alpha? What do you think? So it would be just lambda, right? And what would happen if we plug inthis value here, if this is lambda. What can we say about this? Does it depend on the document? No, so it can be ignored. Right? So we'll end up having thisranking function shown here. And in this case you can easy to see, this a precisely a vector spacemodel because this part is a sum over all the matched query terms,this is an element of the query map. What do you think is a elementof the document up there? Well it's this, right. So that's our document left element. And let's further examine what'sinside of this logarithm. Well one plus this. So it's going to be nonnegative,this log of this, it's going to be at least 1, right? And these, this is a parameter,so lambda is parameter. And let's look at this. Now this is a TF. Now we see very clearlythis TF weighting here. And the larger the count is,the higher the weighting will be. We also see IDF weighting,which is given by this. And we see docking the lan'srelationship here. So all these heuristicsare captured in this formula. What's interesting thatwe kind of have got this weighting function automaticallyby making various assumptions. Whereas in the vector space model, we had to go through those heuristicdesign in order to get this. And in this case note thatthere's a specific form. And when you see whether thisform actually makes sense. All right so what do you thinkis the denominator here, hm? This is a math of document. Total number of words,multiplied by the probability of the word given by the collection, right? So this actually can be interpretedas expected account over word. If we're going to draw, a word,from the connection that we model. And, we're going to draw as many asthe number of words in the document. If you do that,the expected account of a word, w, would be precisely givenby this denominator. So, this ratio basically,is comparing the actual count, here. The actual count of the word in thedocument with expected count given by this product if the word is in fact followingthe distribution in the clutch this. And if this counter is larger thanthe expected counter in this part, this ratio would be larger than one. So that's actually a veryinteresting interpretation, right? It's very natural and intuitive,it makes a lot of sense. And this is one advantage of usingthis kind of probabilistic reasoning where we have made explicit assumptions. And, we know precisely whywe have a logarithm here. And, why we have these probabilities here. And, we also have a formula thatintuitively makes a lot of sense and does TF-IDF weighting anddocumenting and some others. Let's look at the,the Dirichlet Prior Smoothing. It's very similar tothe case of JM smoothing. In this case,the smoothing parameter is mu and that's different fromlambda that we saw before. But the format looks very similar. The form of the functionlooks very similar. So we still have linear operation here. And when we compute this ratio, one will find that is thatthe ratio is equal to this. And what's interesting here is that weare doing another comparison here now. We're comparing the actual count. Which is the expected account of the worldif we sampled meal worlds according to the collection world probability. So note that it's interesting we don'teven see docking the lens here and lighter in the JMs model. All right so this of courseshould be plugged into this part. So you might wonder, sowhere is docking lens. Interestingly the docking lensis here in alpha sub d so this would be plugged into this part. As a result what we get isthe following function here and this is again a sum overall the match query words. And we're against the queer,the query, time frequency here. And you can interpret this asthe element of a document vector, but this is no longera single dot product, right? Because we have this part,I know that n is the name of the query, right? So that just means ifwe score this function, we have to take a sum overall the query words, and then do some adjustment ofthe score based on the document. But it's still, it's still clearthat it does documents lens modulation because this lensis in the denominator so a longer document willhave a lower weight here. And we can also see it has tf here andnow idf. Only that this time the form of theformula is different from the previous one in JMs one. But intuitively it still implements TFIDFwaiting and document lens rendition again, the form of the function is dictatedby the probabilistic reasoning and assumptions that we have made. Now there are alsodisadvantages of this approach. And that is, there's no guaranteethat there's such a form of the formula will actually work well. So if we look about at this geo function,all those TF-IDF waiting and document lens rendition for example it's unclear whetherwe have sub-linear transformation. Unfortunately we can see here thereis a logarithm function here. So we do have also the,so it's here right? So we do have the sublineartransformation, but we do not intentionally do that. That means there's no guarantee thatwe will end up in this, in this way. Suppose we don't have logarithm,then there's no sub-linear transformation. As we discussed before, perhapsthe formula is not going to work so well. So that's an example of the gapbetween a formal model like this and the relevance that we have to model, which is really a subjectmotion that is tied to users. So it doesn't mean we cannot fix this. For example, imagine if we didnot have this logarithm, right? So we can take a risk andwe're going to add one, or we can even add double logarithm. But then, it would mean that the functionis no longer a proper risk model. So the consequence ofthe modification is no longer as predictable aswhat we have been doing now. So, that's also why, for example,PM45 remains very competitive and still, open channel how to usepublic risk models as they arrive, better model than the PM25. In particular how do we use querylike how to derive a model and that would work consistentlybetter than DM 25. Currently we still cannot do that. Still interesting open question. So to summarize this part, we've talkedabout the two smoothing methods. Jelinek-Mercer which is doing the fixedcoefficient linear interpolation. Dirichlet Prior this is what add a pseudocounts to every word and is doing adaptive interpolation in that the coefficientwould be larger for shorter documents. In most cases we can see, by using thesesmoothing methods, we will be able to reach a retrieval function wherethe assumptions are clearly articulate. So they are less heuristic. Explaining the results also showthat these, retrieval functions. Also are very effective and they arecomparable to BM 25 or pm lens adultation. So this is a major advantageof probably smaller where we don't have to doa lot of heuristic design. Yet in the end that we naturallyimplemented TF-IDF weighting and doc length normalization. Each of these functions also hasprecise ones smoothing parameter. In this case of course we still needto set this smoothing parameter. There are also methods that can beused to estimate these parameters. So overall,this shows by using a probabilistic model, we follow very different strategiesthen the vector space model. Yet, in the end, we end up uh,withsome retrievable functions that look very similar tothe vector space model. With some advantages in havingassumptions clearly stated. And then, the form dictatedby a probabilistic model. Now, this also concludes our discussion ofthe query likelihood probabilistic model. And let's recall whatassumptions we have made in order to derive the functionsthat we have seen in this lecture. Well we basically have made fourassumptions that I listed here. The first assumption is that the relevancecan be modeled by the query likelihood. And the second assumption with med is, arequery words are generated independently that allows us to decomposethe probability of the whole query into a product of probabilitiesof old words in the query. And then,the third assumption that we have made is, if a word is not seen,the document or in the late, its probability proportional toits probability in the collection. That's a smoothing witha collection ama model. And finally, we made one of thesetwo assumptions about the smoothing. So we either used JM smoothing orDirichlet prior smoothing. If we make these four assumptionsthen we have no choice but to take the form of the retrievalfunction that we have seen earlier. Fortunately the function has a niceproperty in that it implements TF-IDF weighting and document machine andthese functions also work very well. So in that sense, these functions are less heuristiccompared with the vector space model. And there are many extensions of this,this basic model and you can find the discussion of them inthe reference at the end of this lecture. [MUSIC]"
cs-410,5,1,"[SOUND]This lecture is about the feedbackin text retrieval. So in this lecture, we will continue withthe discussion of text retrieval methods. In particular, we're going to talkabout the feedback in text retrieval. This is a diagram that showsthe retrieval process. We can see the user would type in a query. And then, the query would besent to a retrieval engine or search engine, andthe engine would return results. These results would be issued to the user. Now, after the user hasseen these results, the user can actually make judgements. So for example, the user says,well, this is good and this document is not very useful andthis is good again, etc. Now, this is called a relevance judgmentor relevance feedback because we've got some feedback information fromthe user based on the judgements. And this can be very useful to the system, knowing what exactly isinteresting to the user. So the feedback module wouldthen take this as input and also use the document collectionto try to improve ranking. Typically it would involveupdating the query so the system can now render the resultsmore accurately for the user. So this is called relevance feedback. The feedback is based on relevancejudgements made by the users. Now, these judgements are reliable but the users generally don't want to makeextra effort unless they have to. So the down side is that it involvessome extra effort by the user. There's another form of feedbackcalled pseudo relevance feedback, or blind feedback,also called automatic feedback. In this case, we can see oncethe user has gotten [INAUDIBLE] or in fact we don't have to invoke users. So you can see there'sno user involved here. And we simply assume that the toprank documents to be relevant. Let's say we have assumedtop 10 as relevant. And then, we will then use thisassume the documents to learn and to improve the query. Now, you might wonder, how could this help if we simplyassume the top rank of documents? Well, you can imagine these toprank of documents are actually similar to relevant documentseven if they are not relevant. They look like relevant documents. So it's possible to learn some relatedterms to the query from this set. In fact, you may recall that wetalked about using language model to analyze what association, to learnrelated words to the word of computer. And there, what we did is wefirst use computer to retrieve all the documents that contain computer. So imagine now the queryhere is a computer. And then, the result will be thosedocuments that contain computer. And what we can do then isto take the top n results. They can match computer very well. And we're going to countthe terms in this set. And then, we're going to then usethe background language model to choose the terms that are frequent in this setbut not frequent in the whole collection. So if we make a contrast betweenthese two what we can find is that related to termsto the word computer. As we have seen before. And these related words can then be addedto the original query to expand the query. And this would help us bring the documentsthat don't necessarily match computer but match other words like program andsoftware. So this is very effective forimproving the search result. But of course, pseudo-relevancyvalues are completely unreliable. We have to arbitrarily set a cut off. So there's also something inbetween called implicit feedback. In this case,what we do is we do involve users, but we don't have to askusers to make judgments. Instead, we're going to observe how theuser interacts with the search results. So in this case we'll lookat the clickthroughs. So the user clicked on this one. And the user viewed this one. And the user skipped this one. And the user viewed this one again. Now, this also is a clue about whetherthe document is useful to the user. And we can even assume that we'regoing to use only the snippet here in this document,the text that's actually seen by the user instead of the actualdocument of this entry. The link they are saying web searchmay be broken but it doesn't matter. If the user tries to fetch thisdocument because of the displayed text we can assume these displayedtext is probably relevant is interesting to you sowe can learn from such information. And this is called interesting feedback. And we can, again,use the information to update the query. This is a very importanttechnique used in modern. Now, think about the Google and Bing and they can collect a lot of useractivities while they are serving us. So they would observe what documentswe click on, what documents we skip. And this information is very valuable. And they can use this toimprove the search engine. So to summarize, we talked aboutthe three kinds of feedback here. Relevant feedback where the usermakes explicit judgements. It takes some user effort, butthe judgment information is reliable. We talk about the pseudo feedback wherewe seem to assume top brand marking will be relevant. We don't have to involve the usertherefore we could do that, actually before we returnthe results to the user. And the third is implicit feedbackwhere we use clickthroughs. Where we involve the users, but the user doesn't have to makeit explicitly their fault. Make judgement. [MUSIC]"
cs-410,5,2,"[SOUND]This lecture is about the feedbackin the vector space model. In this lecture, we continue talkingabout the feedback in text retrieval. Particularly, we're going to talk aboutfeedback in the vector space model. As we have discussed before,in the case of feedback the task of text retrieval system is removed fromexamples in improved retrieval accuracy. We will have positive examples. Those are the documents thatassume would be relevant or be charged with being relevant. All the documents thatare viewed by users. We also have negative examples. Those are documents knownto be non-relevant. They can also be the documentsthat are skipped by users. The general method inthe vector space model for feedback is to modify our query vector. We want to place the query vector ina better position to make it accurate. And what does that mean exactly? Well, if we think about the query vectorthat would mean we would have to do something to the vector elements. And in general,that would mean we might add new terms. Or we might just weight of old terms orassign weights to new terms. As a result, in general,the query will have more terms. We often call this query expansion. The most effective method inthe vector space model for feedback is called the Rocchio Feedback, which wasactually proposed several decades ago. So the idea is quite simple. We illustrate this idea byusing a two dimensional display of all the documents in the collection andalso the query vector. So now we can see the queryvector is here in the center, and these are all the documents. So when we use the query back there anduse the same narrative function to find the most similar documents,we are basically doing a circle here and that these documents would bebasically the top-ranked documents. And these process are relevant documents,and these are relevant documents,for example, it's relevant, etc. And then these minuses are negativedocuments, like these. So our goal here is trying to movethis query back to some position, to improve the retrieval accuracy. By looking at this diagram,what do you think? Where should we move the query vector so that we can improvethe retrieval accuracy? Intuitively, where do youwant to move query vector? If you want to think more,you can pause the video. If you think about this picture, you canrealize that in order to work well in this case you want the query vector to be asclose to the positive vectors as possible. That means ideally, you want to placethe query vectors somewhere here. Or we want to move the queryvector closer to this point. Now so what exactly is this point? Well, if you want these relevantdocuments to rank on the top, you want this to be in the center ofall these relevant documents, right? Because then if you drawa circle around this one, you'll get all these relevant documents. So that means we can move the queryvector towards the centroid of all the relevant document vectors. And this is basically the idea of Rocchio. Of course, you can considerthe centroid of negative documents and we want to move away fromthe negative documents. Now your match that we're talking aboutmoving vector closer to some other vec and away from other vectors. It just means that we have this formula. Here you can see this isoriginal query vector and this average basically is the centroidvector of relevant documents. When we take the average of these vectors, then were computingthe centroid of these vectors. Similarly, this is the average ofnon-relevant document like this. So it's essentially ofnon-relevant documents. And we have these three parameters here,alpha, beta, and gamma. They are controllingthe amount of movement. When we add these two vectors together, we're moving the query vectorcloser to the centroid. This is when we add them together. When we subtracted this part, we kind of move the queryvector away from that centroid. So this is the main ideaof Rocchio feedback. And after we have done this, we will get a new query vector whichcan be used to score documents. This new query vector,will then reflect the move of this original query vector toward thisrelevant centroid vector and away from the non-relevant value. Okay, so let's take a look at the example. This is the example thatwe've seen earlier. Only that I deemed that displayof the actual documents. I only showed the vectorrepresentation of these documents. We have five documents here and we have to read in the documents here, right. And they're displayed in red. And these are the term vectors. Now I have just assumed some of weights. A lot of terms,we have zero weights of course. Now these are negative arguments. There are two here. There is another one here. Now in this Rocchio method, we firstcompute the centroid of each category. And so let's see,look at the centroid vector of the positive documents, we simply just,so it's very easy to see. We just add this with this onethe corresponding element. And then that's down here andtake the average. And then we're going to addthe corresponding elements and then just take the average. And so we do this for all this. In the end, what we have is this one. This is the average vector of these two,so it's a centroid of these two. Let's also look at the centroidof the negative documents. This is basically the same. We're going to take the averageof the three elements. And these are the correspondingelements in the three vectors, and so on and so forth. So in the end, we have this one. Now in the Rocchio feedbackmethod we're going to combine all these with the originalquery vector which is this. So now let's see how wecombine them together. Well, that's basically this. So we have a parameter alphacontrolling the original query times weight that's one. And now we have beta to controlthe inference of the positive centroid of the weight, that's 1.5. That comes from here. All right, so this goes here. And we also have this negativeweight here gamma here. And this way, it has come from,of course, the negative centroid here. And we do exactly the same forother terms, each is for one term. And this is our new vector. And we're going to use this new queryvector, this one to rank the documents. You can imagine what would happen, right? Because of the movement that this onewould matches these red documents much better because we movedthis vector closer to them. And it's going to penalize these blackdocuments, these non relevent documents. So this is precisely whatwe wanted from feedback. Now of course if we apply this method inpractice we will see one potential problem and that is the original query hasonly four terms that are now zero. But after we do query explaining and merging, we'll have many timesthat would have non zero weights. So the calculation willhave to involve more terms. In practice,we often truncate this matter and only retain the termswith highest weights. So let's talk about how weuse this method in practice. I just mentioned that they'reoften truncated vector. Consider only a small number ofwords that have highest weights in the centroid vector. This is for efficiency concern. I also said here that negative examples,or non-relevant examples tend not to be very useful, especiallycompared with positive examples. Now you can think about why. One reason is because negative documentstend to distract the query in all directions. So, when you take the average, it doesn't really tell you whereexactly it should be moving to. Whereas positive documentstend to be clustered together. And they will point you toa consistent direction. So that also means that sometimes we don'thave to use those negative examples. But note that in some cases, in difficultqueries where most results are negative, negative feedback after is very useful. Another thing is to avoid over-fitting. That means we have to keep relativelyhigh weight on the original query terms. Why? Because the sample that we see infeedback Is a relatively small sample. We don't want to overlytrust the small sample. And the original query termsare still very important. Those terms are heightened by the user and the user has decided that thoseterms are most important. So in order to preventthe us from over-fitting or drifting, prevent topic drifting due tothe bias toward the feed backing symbols. We generally would have to keep a prettyhigh weight on the original terms so it was safe to do that. And this is especially true forpseudo relevance feedback. Now, this method can be used for both relevance feedback andpseudo-relevance feedback. In the case of pseudo-feedback, the primeand the beta should be set to a smaller value because the relevant examplesare assumed not to be relevant. They're not as reliable asthe relevance feedback. In the case of relevance feedback,we obviously could use a larger value. So those parameters,they have to be set empirically. And the Rocchio Method isusually robust and effective. It's still a very popular method forfeedback. [MUSIC]"
cs-410,5,3,"[SOUND]This lecture is about the feedback inthe language modeling approach. In this lecture, we will continue thediscussion of feedback in text retrieval. In particular, we're going to talk about the feedbackin language modeling approaches. So we derive the query likelihood rankingfunction by making various assumptions. As a basic retrieval function,all those formulas worked well. But if we think about the feedbackinformation, it's a little bit awkward to use query likelihood to perform feedback,because a lot of times the feedback information isadditional information about the query. But we assume the query hasgenerated it by assembling words from a language model inthe query likelihood method. It's kind of unnatural to samplewords that form feedback documents. As a result, researchers proposed a wayto generalize query likelihood function, and it's called Kullback-Leiblerdivergence retrieval model. And this model is actually goingto make the query likelihood retrieval function muchcloser to vector space model. Yet this form of the language modelcan be regarded as a generalization of query likelihood, in the sense that it cancover query likelihood as a special case. And in this case, then feedback can be achieved throughsimply query model estimation or updating. This is very similar to Rocchio,which updates the query vector. So let's see what is thisKL-divergence retrieval model. So on the top, what you see is a querylikelihood retrieval function, this one. And then KL-divergence, oralso called cross entropy, retrieval model is basically to generalize the frequency part hereinto a language model. So basically it's the difference given by the probabilistic model here tocharacterize what the user is looking for, versus the count of query words there. And this difference allows us to plug invarious different ways to estimate this. So this can be estimatedin many different ways, including using feedback information. But this is called a KL-divergence,because this can be interpreted as matchingthe KL-divergence of two distributions. One is the query model,denoted by this distribution. One is the documentlanguage model here and smooth them with a collectionlanguage model, of course. And we are not going to talkabout the detail of that, and you'll find it in some references. It's also called cross entropy because,in fact, we ignore some terms inthe KL-divergence function and we will end up havingactually cross entropy. And both are terms of information theory. But anyway, for our purposes here, you can just see the twoformulas look almost identical, except that here we have a probability ofa word given by a query language model. And here the sum is over all the wordsthat are in the document and also with the nonzero probability forthe query model. So it's kind of, again, a generalizationof sum over all the matching query words. Now you can also easily see we can recoverthe query likelihood retrieval function by simply setting this query model to therelative frequency of a word in the query. This is very easy tosee once you plug this into here you can eliminate thisquery length as a constant. And then you will get exactly like that. So you can see the equivalence. And that's also why this KL-divergencemodel can be regarded as a generalization of query likelihood, because we can coverquery likelihood as a special case. But it would also allow usto do much more than that. So this is how we can use theKL-divergence model to then do feedback. The picture shows that we firstestimate a document language model, then we estimate a query language model,and we compute the KL-divergence. This is often denoted by a D here. But this basically means this isexactly like the vector space model, because we compute a vector for thedocument, then compute another vector for the query, andthen we compute the distance. Only that these vectors are of specialforms, they are probability distributions. And then we get the results andwe can find some feedback documents. Let's assume they are mostlypositive documents, although we could also considerboth kinds of documents. So what we could do is, like in Rocchio,we're going to compute another language model called the feedbacklanguage model here. Again, this is going to be another vectorjust like the computing centroid of vector in Rocchio. And then this model can be combinedwith the original query model using a linear interpolation, andthis would then give us an update model, just like, again, in Rocchio. So here we can see the parameter alphacan control the amount of feedback. If it's set to zero,then essentially there is no feedback. If it's set to one, we get full feedbackand we ignore the original query. And this is generally not desirable,right? So unless you are absolutely sure youhave seen a lot of relevant documents, then the query terms are not important. So of course, the main question here is,how do you compute this theta F? This is the big question here, andonce you can do that, the rest is easy. So here we will talk aboutone of the approaches, and there are many approaches, of course. This approach is basedon generative model, and I'm going to show you how it works. This will use a generative mixture model. So this picture shows thatwe have this model here, the feedback model thatwe want to estimate. And the basis is the feedback documents. Let's say we are observingthe positive documents. These are the clicked documents by usersor random documents judged by users, or are simply top ranked documentsthat we assume to be relevant. Now imagine how we cancompute a centroid for these documents by using language model. One approach is simply to assume these documents are generatedfrom this language model. As we did before, what we could dois just normalize the word frequency here to here andthen we will get this word distribution. Now the question is whether thisdistribution is good for feedback. Well, you can imagine the topranked word would be what? What do you think? Well, those words would be common words. As we always see in a language model, the top ranked words are actuallycommon words like the, a, etc. So it's not very good for feedback,because we would be adding a lot of such words to our query when we interpolatethis with the original query model. So this was not good, sowe need to do something. In particular, we are trying toget rid of those common words. And we have seen actually one wayto do that by using background language model in the case oflearning the associations of words, the words that are relatedto the word computer. We could do that and that would beanother way to do this, but here we are going to talk about another approachwhich is a more principled approach. In this case, we're going to say well,you said that there are common words here in these documents that should notbelong to this topic model, right? So now what we can do is to assume that,well, those words are generated frombackground language model, so they will generate those words like the,for example. And if we use maximum likelihood estimate, note that if all the words heremust be generated from this model, then this model is forced to assignhigh probabilities to a word like the, because it occurs so frequently here. Note that in order to reduce itsprobability in this model, we have to have another model, which is this one,to help explain the word the here. And in this case, it's not appropriate to use the backgroundlanguage model to achieve this goal because this model would assign highprobabilities to these common words. So in this approach, then, we assume this machine that was generatingthese words would work as follows. We have a source control up here. Imagine we flip a coin here todecide what distribution to use. With probability of lambda,the coin shows up as head and we're going to usethe background language model. And we're going to do that insample word from that model. With probability of 1 minus lambda,we're going to decide to use a known topic model, here,that we would like to estimate. And we're going to thengenerate a word here. If we make this assumption and this wholething will be just one model, and we call this a mixture model because there are twodistributions that are mixed together. And we actually don't know wheneach distribution is used. So again,think of this whole thing as one model, and we can still ask for words and it willstill give us a word in a random manner. And of course, which word will show upwill depend on both this distribution and that distribution. In addition,it would also depend on this lambda, because if you say lambda is very high andit's going to always use the background distribution,you will get different words. Then if you say, well, lambda isvery small, we're going to use this. So all of theseare parameters in this model. And then if you're thinking this way, basically we can do exactlythe same as what we did before. We're going to use maximum likelihoodestimator to adjust this model, to estimate the parameters. Basically we're going toadjust this parameter so that we can best explain all the data. The difference now is that we are notasking this model a known to explain this. But rather we are going to ask this wholemodel, mixture model, to explain the data. Because it has got some helpfrom the background model, it doesn't have to assign highprobabilities to words like the. As a result, it will then assign higherprobabilities to other words that are common here butnot having high probability here. So those would be common here. And if they're common, they wouldhave to have high probabilities, according to a maximumlikelihood estimate method. And if they are rare here,then you don't get much help from this background model. As a result, this topic modelmust assign high probabilities. So the high probability words,according to the topic model, would be those that are common here butrare in the background. So this is basically a little bitlike an idea of weighting here. But this would allow us to achieve theeffect of removing these topic words that are meaningless in the feedback. So mathematically, what we have isto compute the likelihood, again, local likelihood,of the feedback documents. And note that we also have anotherparameter, lambda here, but we assume that the lambda denotesthe noise in the feedback document. So we are going to,let's say set this to a parameter. Let's say 50% of the words are noise or90% are noise. And this can then beassumed it will be fixed. If we assume this is fixed, then we onlyhave these probabilities as parameters, just like in the simpleunigram language model. We have n parameters,n is the number of words. And then the likelihoodfunction would look like this. It's very similar to the globallikelihood function we see before, except that inside the logarithmthere's a sum here. And this sum is because weconsider two distributions. And which one is used would depend onlambda, and that's why we have this form. But mathematically, this is the functionwith theta as unknown variables. So this is just a function. All the other values are known except forthis guy. So we can then choose thisprobability distribution to maximize this log likelihood, the same idea as the maximum likelihoodestimate as a mathematical problem. We just have to solve thisoptimization problem. We essentially would try allthe theta values until we find one that gives this whole thingthe maximum probability. So it's a well-defined math problem. Once we have done that, we obtain thistheta F that can then be interpolated with original query model to the feedback. So here are some examples ofthe feedback model learned from a web document collection. And we do pseudo-feedback we justuse the top ten documents and we use this mixture model. So the query is airport security. What we do is we first retrieve tendocuments from the web database and this is of course pseudo-feedback. And then we're going to feed thatmixture model to this ten document set. And these are the wordslearned using this approach. This is the probability of a word givenby the feedback model in both cases. So in both cases you can see the highest probability words include the veryrelevant words to the query. So airport security, for example, these query words still show up as highprobabilities in each case naturally, because they occur frequentlyin the top ranked documents. But we also see beverage,alcohol, bomb, terrorist, etc. So these are relevant to this topic,and they, if combined with original query, can helpus much more accurately on documents. And also they can help us bring updocuments that only mention some of these other words, maybe, for example,just airport and then bomb, for example. So this is how pseudo-feedback works. It shows that this model really works andpicks up some related words to the query. What's also interesting is that ifyou look at the two tables here and you compare them,then you'll see, in this case, when lambda is set to a small value,then we'll see some common words here. And that means, well,we don't use the background model often. Remember, lambda confuses the probabilityof using background model to generate the text. If we don't rely much on background model, we still have to use this topic modelto account for the common words. Whereas if we set lambdato a very high value, we will use the background modelvery often to explain these words. Then there's no burden onexpanding those common words in the feedback documentsby the topic model. So as a result, the topic modelhere is very discriminative. It contains all the relevantwords without common words. So this can be added to the originalquery to achieve feedback. So to summarize, in this lecture we have talked aboutthe feedback in language model approach. In general,feedback is to learn from examples. These examples can be assumed examples,can be pseudo-examples, like assume the top ten documentsthat are assumed to be relevant. They could be based on user interactions, like feedback based on clickthroughs orimplicit feedback. We talked about the three majorfeedback scenarios, relevance feedback, pseudo feedback, and implicit feedback. We talked about how to use Rocchio todo feedback in vector space model and how to use query model estimation forfeedback in language model. And we briefly talked aboutthe mixture model and the basic idea. There are many other methods. For example, the relevance model is a very effectivemodel for estimating query model. So you can read more about thesemethods in the references that are listed at the end of this lecture. So there are two additional readings here. The first one is a book thathas a systematic review and discussion of language models forinformation retrieval. And the second one is a important research paper that's about relevancebased language models, and it's a very effective wayof computing query model. [MUSIC]"
cs-410,5,4,"This lecture is about Web Search. In this lecture,we're going to talk about one of the most important applications oftext retrieval, web search engines. So let's first look at somegeneral challenges and opportunities in web search. Now, many informationalretrieval algorithms had been developedbefore the web was born. So when the web was born,it created the best opportunity to apply those algorithms to major applicationproblem that everyone would care about. So naturally, there have to be somefurther extensions of the classical search algorithms to address newchallenges encountered in web search. So here are some general challenges. First, this is a scalability challenge. How to handle the size of the web and ensure completeness ofcoverage of all information. How to serve many users quickly andby answering all their queries. And so that's one major challenge and before the web was born the scalesearch was relatively small. The second problem is that there'sno quality information and there are often spams. The third challenge isDynamics of the Web. The new pages are constantly create andsome pages may be updated very quickly, so it makes it harder tokeep it indexed fresh. So these are some of the challengesthat we have to solve in order to deal with high quality web searching. On the other hand there are also someinteresting opportunities that we can leverage to include the search results. There are many additional heuristics,for example, using links that we canleverage to improve scoring. Now everything that we talked aboutsuch as the vector space model are general algorithms. They can be applied to any searchapplications, so that's the advantage. On the other hand, they also don't takeadvantage of special characteristics of pages or documents in the specificapplications, such as web search. Web pages are linked with each other,so obviously, the linking is somethingthat we can also leverage. So, because of these challenges andopportunities and there are new techniques that have been developed forweb search or due to need for web search. One is parallel indexing and searching and this is to addressthe issue of scalability. In particular, Google's imaging ofmap reduce is very influential and has been very helpful in that aspect. Second, there are techniquesthat are developing for addressing the problem of spams,so spam detection. We'll have to prevent those spampages from being ranked high. And there are also techniquesto achieve robust ranking. And we're going to use a lotof signals to rank pages, so that it's not easy to spam the searchengine with a particular trick. And the third line of techniques is link analysis and these are techniques that can allow us to improve such resultsby leveraging extra information. And in general in web searching,we're going to use multiple features for ranking not just for link analysis. But also exploring all kindsof crawls like the layout or anchor text that describesa link to another page. So, here's a picture showingthe basic search engine technologies. Basically, this is the web on the left andthen user on the right side and we're going to help this user to getthe access for the web information. And the first component is a Crawler thatwould crawl pages and then the second component is Indexer that would takethese pages create the inverted index. The third component there is a Retrieverand that would use inverted index to answer user's query by talkingto the user's browser. And then the search results will be givento the user and when the browser would show those results, it allowsthe user to interact with the web. So, we're going to talk abouteach of these components. First of all, we're going to talk aboutthe crawler, also called a spider or software robot that would do somethinglike crawling pages on the web. To build a toy crawler is relatively easy, because you just need to startwith a set of seed pages. And then fetch pages from the web andparse these pages and figure out new links. And then add them to the priority que andthen just explore those additional links. But to be able to real crawleractually is tricky and there are some complicated issuesthat we have to deal with. For example robustness,what if the server doesn't respond, what if there's a trap that generatesdynamically generated webpages that might attract your crawler tokeep crawling on the same side and to fetch dynamic generated pages? The results of this issueof crawling courtesy and you don't want to overload one particularserver with many crawling requests and you have to respect the robotexclusion protocol. You also need to handle differenttypes of files, there are images, PDF files,all kinds of formats on the web. And you have to alsoconsider URL extension, so sometimes those are CGI scripts andthere are internal references, etc, and sometimes you haveJavaScripts on the page and they also create challenges. And you ideally should also recognizeredundant pages because you don't have to duplicate those pages. And finally, you may be interestedin the discover hidden URLs. Those are URLs that may not be linkedto any page, but if you truncate the URL to a shorter path, you mightbe able to get some additional pages. So what are the Major Crawling Strategies? In general, Breadth-First is most common becauseit naturally balances the sever load. You would not keep probing a particularserver with many requests. Also parallel crawling is verynatural because this task is very easy to parallelize. And there is some variationsof the crawling task, and one interesting variationis called a focused crawling. In this case, we're going to crawl justsome pages about a particular topic. For example,all pages about automobiles, all right. And this is typically going tostart with a query, and then you can use the query to get someresults from a major search engine. And then you can start it with thoseresults and then gradually crawl more. The one channel in crawling, is you will find the newchannels that people created and people probably are creatingnew pages all the time. And this is very challenging ifthe new pages have not been actually linked to any old pages. If they are, then you can probably findthem by re-crawling the old pages, so these are also some interestingchallenges that have to be solved. And finally, we might face the scenarioof incremental crawling or repeated crawling, right. Let's say,if you want to build a web search engine, and you first crawl a lotof data from the web. But then,once you have cracked all the data, in the future you just needto crawl the updated pages. In general, you don't have tore-crawl everything, right? It's not necessary. So in this case, your goal is tominimize the resource overhead by using minimum resourcesto just the update pages. So, this is actually a veryinteresting research question here, and this is a open research question,in that there aren't many standard algorithms established yetfor doing this task. But in general, you can imagine,you can learn, from the past experience. So the two major factors thatyou have to consider are, first will this pagebe updated frequently? And do I have to quote this page again? If the page is a static page andthat hasn't being changed for months, you probably don't have to re-crawl iteveryday because it's unlikely that it will changed frequently. On the other hand, if it's a sports scorepage that gets updated very frequently and you may need to re-crawl it andmaybe even multiple times on the same day. The other factor to consider is,is this page frequently accessed by users? If it is, then it means thatit is a high utility page and then thus it's more important toensure such a page to refresh. Compared with another page that hasnever been fetched by any users for a year, then even though thatpage has been changed a lot then. It's probably not that necessary tocrawl that page or at least it's not as urgent as to maintain the freshnessof frequently accessed page by users. So to summarize, web search is one ofthe most important applications of text retrieval and there are some newchallenges particularly scalability, efficiency, quality information. There are also new opportunitiesparticularly rich link information and layout, etc. A crawler is an essential componentof web search applications and in general, you can find two scenarios. One is initial crawling andhere we want to have complete crawling of the web if you are doinga general search engine or focused crawling if you want to justtarget as a certain type of pages. And then, there is another scenario that'sincremental updating of the crawl data or incremental crawling. In this case,you need to optimize the resource, try to use minimum resourceto get the [INAUDIBLE] [MUSIC]"
cs-410,5,5,"[SOUND] This lecture is about the Web Indexing. In this lecture, we will continuetalking about the Web Search and we're going to talk about howto create a Web Scale Index. So once we crawl the web,we've got a lot of web pages. The next step is to use the indexerto create the inverted index. In general, we can use the sameinformation retrieval techniques for creating an index and that is what wetalked about in previous lectures, but there are there are newchallenges that we have to solve. For web scale indexing, and the two mainchallenges are scalability and efficiency. The index would be so large, that it cannot actually fit intoany single machine or single disk. So we have to store the dataon virtual machines. Also, because the data is solarge, it's beneficial to process the data in parallel, sothat we can produce index quickly. Now to address these challenges,Google has made a number of innovations. One is the Google File System that'sa general File system, that can help programmers manage files storedon a cluster of machines. The second is MapReduce. This is a general software framework forsupporting parallel computation. Hadoop is the most well known opensource implementation of MapReduce. Now used in many applications. So, this is the architectureof the google file system. It uses a very simple centralized management mechanism to manageall the specific locations of. Files, soit maintains the file namespace and look up a table to know whereexactly each file is stored. The application client will thentalk to this GFS master, and that obtains specific locations ofthe files they want to process. And once the GFS file kind obtainedthe specific location about the files, then the application client can talkto the specific servers whether data actually sits directly, soyou can avoid involving other node. In the network. So when this file system storesthe files on machines, the system also with great fixed sizes of chunks, sothe data files are separated into. Many chunks. Each chunk is 64 MB, so it's pretty big. And that's appropriate forlarge data processing. These chunks are replicatedto ensure reliability. So this is something that the programmerdoesn't have to worry about, and it's all taken careof by this file system. So from the application perspective, the programmer would see thisas if it's a normal file. And the programmer doesn't have toknow where exactly it is stored and can just invoke high level. Operators to process the file. And another feature is that the datatransfer is directly between application and chunk servers. So it's efficient in this sense. On top of the Google file system, Google also proposed MapReduce as a generalframework for parallel programming. Now, this is very useful to supporta task like building inverted index. And so, this framework is, Hiding a lot of low-levelfeatures from the program. As a result, the programmer can makea minimum effort to create an application that can be run a largecluster in parallel. So some of the low level detailsare hidden in the framework including the specific and network communications orload balancing or where the task are executed. All these details are hiddenfrom the programmer. There is also a nice feature whichis the built in fault tolerance. If one server is broken, the server is down, andthen some tasks may not be finished. Then the MapReduce mapper will knowthat the task has not been done. So it automatically dispatches a taskon other servers that can do the job. And therefore, again the programdoesn't have to worry about that So here's how MapReduce works. The input data would be separatedinto a number of key value pairs. Now what exactly is in the valuewould depend on the data and it's actually a fairly general frameworkto allow you to just partition the data into different parts and each partcan be then processed in parallel. Each key value pair would be andsend it to a map function. The program was right the map function,of course. And then the map function willprocess this Key Value pair and then generate a number ofother Key Value pairs. Of course, the new key is usuallydifferent from the old key that's given to the map as input. And these key value pairsare the output of the map function and all the outputs of all the mapfunctions would be then collected, and then there will be forthe sorting based on the key. And the result is that,all the values that are associated with the same key will bethen grouped together. So now we've got a pair of of a key andseparate values attached to this key. So this would then be sentto a reduce function. Now, of course, each reduce functionwill handle a different key, so we will send these output values to multiple reduce functionseach handling a unique key. A reduce function would thenprocess the input, which is a key in a set of values to produceanother set of key values as the output. So these output values wouldbe then corrected together to form the final output. And so, this is the generalframework of MapReduce. Now the programmer only needs to writethe Map function and the Reduce function. Everything else is actually takencare of by the MapReduce framework. So you can see the program reallyonly needs to do minimum work. And with such a framework, the input datacan be partitioned into multiple parts, which is processing parallel first by map,and then being the process afterwe reach the reduced stage. The much more reduced if I'm[INAUDIBLE] can also further process the different keys andtheir associated values in parallel. So it achieves some, it achieves the purpose of parallelprocessing of a large data set. So let's take a look at a simple example. And that's Word Counting. The input is containing words, and the output that we want to generate isthe number of occurrences of each word. So it's the Word Count. We know this kind of countingwould be useful to, for example, assess the popularity of a word ina large collection and this is useful for achieving a factor of IDF wading forsearch. So how can we solve this problem? Well, one natural thought is that,well this task can be done in parallel by simply countingdifferent parts of the file in parallel, and then in the end we justcombine all the counts. And that's precisely the idea ofwhat we can do with MapReduce. We can parallelize onlines in this input file. So more specifically, we can assumethe input to each map function is a key value pair that represents the linenumber and the string on that line. So the first line, forexample, has a key of one and that is another word by word andjust the four words on that line. So this key value pair wouldbe sent to a Map Function. The Map Function then would justcount the words in this line. And in this case,of course there are only four words. Each world gets a count of one and these are the output that you see hereon this slide from this map function. So the map function is reallyvery simple if you look at what the pseudocode lookslike on the right side, you see it simply needs to iterateall the words and this line. And then just collect the function which means it would then send the wordand the count to the collector. The collector would then try tosort all these key value pairs from different Map Functions, right? So the function is very simple andthe programmer specifies this function as a way toprocess each part of the data. Of course, the second line will behandled by a different Map Function which we will produce a single output. Okay, now the output from the mapfunctions will be then and send it to a collector and the collectorwould do the internal grouping or sorting. So at this stage, you can see,we have collected a match for pairs. Each pair is a word andits count in a line. So, once we see all these pairs. Then we can sort them based on the key,which is the word. So we will collect all the countsof a word, like bye here, together. And similarly, we do that for other words. Like Hadoop, Hello, etc. So each word now is attached toa number of values, a number of counts. And these counts represent the occurrencesto solve this word in different lights. So now we have got a new pair of a key anda set of values, and this pair will then be fed into reducefunction, so the reduce function now would have to finish the job of countingthe total occurrences of this word. Now, it has all ready got allthese puzzle accounts, so all it needs to do issimply to add them up. So the reduce function hereis very simple, as well. You have a counter, andthen iterate all the other words. That you'll see in this array. And that,you just accumulate accounts, right? And then finally, you output the P andthe proto account. And that's precisely what we want asthe output of this whole program. So you can see,this is all ready very similar to. To building an Invert index. And if you think about it,the output here is index. And we have already got a dictionary,basically. We have got the count. But what's missing isthe document the specific frequency counts of wordsin those documents. So we can modify this slightly toactually be able to index in parallel, so here's one way to do that. So in this case, we can assume the inputfrom Map Function is a pair of a key which denotes the document ID,and the value denoting the screen for that document,so it's all the words in that document. And so, the map function would dosomething very similar to what we have seen in the word campaign example. It simply groups all the counts ofthis word in this document together. And it would then generatea set of key value pairs. Each key is a word, and the value is the count of this word inthis document plus the document ID. Now, you can easily see why we need toadd document ID here, because later in inverted index, we would like tokeep this formation, so the Map Function should keep track of it, and this can thenbe sent to the reduce function later. Now similarly another document D2can be processed in the same way. So in the end, again, there is a sortingmechanism that would group them together. And then we will have just a key,like a java, associated with all the documentsthat match this key. Or all the documents where java occurred. And the counts, sothe counts of java in those documents. And this will be collected together. And this will be, sofed into the reduce function. So now you can see the reduce functionhas already got input that looks like an inverted index entry. So it's just the word and allthe documents that contain the word and the frequencies of the wordin those documents. So all you need to do issimply to concatenate them into a continuous chunk of data. And this can be donewritten to a file system. So basically the reduce functionis going to do very minimal. Work. And so, this is a pseudo-code for [INAUDIBLE] that's construction. Here we see two functions,procedure Map and procedure Reduce. And a programmer would specify these twofunctions to program on top of map reduce. And you can see basically theyare doing what I just described. In the case of map, it's going to count the occurrences of a wordusing the AssociativeArray. And it would output all the countstogether with the document ID here. So, this is the reduce function,on the other hand, simply concatenates all the inputthat it has been given, and then put them together asone single entry for this key. So this is a very simpleMapReduce function, yet it would allow us to construct an invertedindex at very large scale, and the data can be processedby different machines. And program doesn't have totake care of the details. So this is how we can do parallelindex construction for web search. So to summarize, web scale indexing requires somenew techniques that go beyond the. Standard traditional indexing techniques. Mainly, we have to storeindex on multiple machines. And this is usually done by using a filingsystem, like a Google file system. But this should be through a file system. And secondly, it requires creatingan index an parallel, because it's so large and takes long time to createan index for all the documents. So if we can do it in parallel,it will be much faster and this is done by usingthe MapReduce framework. Note that both the GFS andMapReduce frameworks are very general, so they can also supportmany other applications. [MUSIC]"
cs-410,5,6,"[SOUND] This lecture is about link analysis for web search. In this lecture, we're going to talkabout the web search and particularly, focusing on how to do link analysis anduse the results to improve search. The main topic of this lecture is to lookat the ranking algorithms for Web Search. In the previous lecture we talkedabout how to create index. Now that we have index, we want to seehow we can improve ranking of Pages. The web. Now standard IR models,can be also applied here. In fact,they are important building blocks, for, improve, for supporting web search. But they aren't sufficient. And mainly for the following reasons. First, on the web, we tend to havevery different information needs, for example, people might search fora webpage, or an entry page. And this is different fromthe traditional library search, where people are primarily interestedin collecting literature Information. So this kind of query is oftencalled a navigational queries. The purpose is to navigate intoa particular type of the page. So for such queries we might benefitfrom using link information. Secondly, documents have additionalinformation and on the web pages, are web format,there are a lot of other clues, such as the layout, the title,or link information again. So this has provided opportunity to use extra context information ofthe document to improve the scoring. And finally,information quality varies a lot. That means we have to considermany factors to improve the range in the algorithm. This would give us a more robust wayto rank pages, making it harder for any spammer to just manipulate the onesignal to improve the ranking of a page. So as a result, people have made a number of majorextensions to the ranking algorithms. One line is to exploitlinks to improve scoring. And that's the main topic of this lecture. People have also proposed algorithms toexploit the loudest, they are implicit. Feedback information the form ofclick throughs and that's of course in the category of feedback techniques andmachine all is often used there. In general in web search the rankingalgorithms are based on machine learning algorithms to combineall kinds of features. Many of them are based onthe standard of virtual models such as BM25 that we talked about [INAUDIBLE]to score different parts of documents or to provide additional featuresbased on content matching, but link informationis also very useful so they provide additional scoring signals. So let's look at links inmore detail on the web. So this is a snapshot of somepart of the web, let's say. So we can see there are many links thatlink the different pages together. And in this case, you can alsolook at the center here, there is a description of a link that's pointingto the document on the right side. Now, this description textis called anchor text. Now if you think about this text,it's actually quite useful because it provides some extradescription of that page be points with. So for example, if someone wantsto bookmark Amazon.com front page the person might say the biggestonline bookstore and then the link to Amazon, right? So, the description here after is verysimilar to what the user would type in the query box when they are looking foror such a page. And that's why it's very useful formanaging pages. Suppose someone types inthe query like online bookstore or biggest online bookstore. All right the query would matchthis anchor text in the page here. And then this actuallyprovides evidence for matching the page that's beingpointed to that is the Amazon. a entry page. So if you match anchor text thatdescribes an anchor to a page, actually that provides good evidence forthe elements of the page being pointed to. So anchor text is very useful. If you look at the bottom part of thispicture you can also see there are some patterns of some links and these linksmight indicate the utility of a document. So for example, on the right side you'll see thispage has received the many inlinks. Now that means many other pagesare pointing to this page. This shows that this page is quite useful. On the left side you can see thisis another page that points to many other pages. So this is a director pagethat would allow you to actually see a lot of other pages. So we can call the firstcase authority page and the second case half page, but this meansthe link information can help intuit. One is to provide extra text for matching. The other is to provide someadditional scores for the webpage to characterize how likely a page isa hub, how likely a page is a authority. So people then of course and proposedideas to leverage this link information. Google's PageRank which was the maintechnique that they used in early days is a good example andthat is an algorithm to capture page and popularity, basically to score authority. So the intuitions here are linksare just like citations in literature. Now think about one pagepointing you to another page, this is very similar to onepaper citing another paper. So, of course then,if a page is cited often, then we can assume this pageto be more useful in general. So that's a very good intuition. Now PageRank is essentially to takeadvantage of this Intuition to implement with the principal approach. Intuitively, it is essentially doingcitation counting or in link counting. It just improves the simpleidea in two ways. One it will consider indirect citations. So that means you don't just lookat how many in links you have. You also look at what are thosepages that are pointing to you. If those pages themselves have a lotof in-links, that means a lot. In some sense,you will get some credit from that. But if those pages thatare pointing to you are not being pointed to by other pages theythemselves don't have many in-links, then well, you don't get that much. So that's the idea ofgetting indirect citation. All right, so you can also understand this idea bylooking at again the research papers. If you're cited by let's say ten papers,and those ten papers are just workshop papers or some papersthat are not very influential, right? So although you've got ten in-links,and that's not as good as if you are cited by ten papers that themselveshave attracted a lot of other citations. And so in this case where we wouldlike to consider indirect links and page does that. The other idea is it'sgood to pseudo citations. Assume that basically every page is havinga number zero pseudo citation count. Essentially you are trying toimagine there are many virtual links that will link allthe pages together so that you actually get the pseudocitations from everyone. The reason why they want to do that. Is this will allow themto solve the problem elegantly with linear algebra technique. So, I think maybe the bestway to understand the PageRank is to thinkof this as through computer probability of random surfervisiting every webpage. [MUSIC]"
cs-410,5,7,"[MUSIC] So let's take a look at this in detail. So in this random surfingmodel at any page would assume random surfer would choosethe next page to visit. So this is a small graph here. That's of course, over simplificationof the complicated web. But let's say there are fourdocuments here, d1, d2, d3 and d4. And let's assume that a random surfer orrandom walker can be any of these pages. And then the randomsurfer could decide to, just randomly jumping to any page or follow a link andthen visit the next page. So if the random surfer is at d1, then there is some probability thatrandom surfer will follow the links. Now there are two outlinks here,one is pointing to d3, the other is pointing to d4. So the random surfer could pick anyof these two to reach d3 and d4. But it also assumes that the random sofar might get bore sometimes. So the random surfing which decideto ignore the actual links and simply randomly jumpinto any page in the web. So if it does that, it would be ableto reach any of the other pages even though there's no link you actually,you want from that page. So this is to assume thatrandom surfing model. Imagine a random surfer isreally doing surfing like this, then we can ask the question howlikely on average the surfer would actually reach a particularpage like a d1, a d2, or a d3. That's the average probability ofvisiting a particular page and this probability is preciselywhat a page ranker computes. So the page rank score ofthe document is the average probability that the surfervisits a particular page. Now intuitively, this would basicallycapture the inlink account, why? Because if a page has a lot of inlinks, then it would have a higherchance of being visited. Because there will be moreopportunities of having the server to follow a link to come to this page. And this is why the random surfing model actually captures the IDof counting the inlinks. Note that it also considersthe interacting links, why? Because if the page is that point thenyou have themselves a lot of inlinks. That would mean the random surfer wouldvery likely reach one of them and therefore, it increasethe chance of visiting you. So this is just a nice way to captureboth indirect and a direct links. So mathematically, how can we compute thisproblem in a day in order to see that, we need to take a look at how thisproblem there is a computing. So first of all let's take a lookat the transition metrics here. And this is just metrics withvalues indicating how likely the random surfer would gofrom one page to another. So each rule stands for a starting page. For example, rule one wouldindicate the probability of going to any of the other four pages from d1. And here we see there are only2 non 0 entries which is 1/2. So this is because if you look atthe graph d1 is pointing to d3 and d4. There is no link from d1 or d2. So we've got 0s for the first 2 columns and 0.5 for d3 and d4. In general, the M in this matrix, M sub ij is the probabilityof going from di to dj. And obviously for each rule,the values should sum to 1, because the surfer would have to go toprecisely one of these other pages. So this is a transition metric. Now how can we compute the probabilityof a surfer visiting a page? Well if you look at the surfmodel then basically, we can compute the probabilityof reaching a page as follows. So here on the left hand side,you see it's the probability visiting page dj at time plus 1,so it's the next time point. On the right hand side, you can seethe equation involves the probability of at page di at time t. So you can see the subscriptin that t here, and that indicates that's the probability thatthe server was at a document at time t. So the equation basically,captures the two possibilities of reachingat dj at the time t plus 1. What are these two possibilities? Well one is through random surfing and one is through following a link,as we just explained. So the first part captures the probability that the random surfer would reachthis page by following a link. And you can see the randomsurfer chooses this strategy with probability 1 minusalpha as we assume. And sothere is a factor of 1 minus alpha here. But the main party is realistsum over all the possible pages that the surfer could have been at time t. There are n pages soit's a sum over all possible n pages. Inside the sum is a productof two probabilities. One is the probability that the surfer was at di at time t, that's p sub t of di. The other is the transitionprobability from di to dj. And so in order to reach this dj page, the surfer must first be at di at time t. And then also, would also have tofollow the link to go from di to dj. So the probability is the probabilityof being at di at time t multiplied by the probability of going from thatpage to the target page, dj here. The second part is a similar sum, the onlydifference is that now the transition probability is a uniformtransition probability. 1 over n andthis part of captures is the probability of reaching this pagethrough random jumping. So the form is exactly the same andthis also allows us to see on why PageRank is essentially assumeda smoothing of the transition matrix. If you think about this 1 over n ascoming from another transition matrix that has all the elements being1 over n in uniform matrix. Then you can see very clearlyessentially we can merge the two parts, because they are of the same form. We can imagine there's a differentmetrics that's combination of this m and that uniform metrics whereevery m is 1 over n. And in this sense PageRank usesthis idea of smoothing and ensuring that there's no zero entryin such as transition matrix. Now of course this is the time dependentthe calculation of the probabilities. Now we can imagine, if we'll computethe average of the probabilities, the average of probabilities probablywith the sets of file this equation without considering the time index. So let's drop the time index andjust assume that they will be equal. Now this would give us any equations,because for each page we have such equation. And if you look at the whatvariables we have in these equations there are also precisely n variables. So this basically means,we now have a system of n equations with n variables andthese are linear equations. So basically, now the problem boilsdown to solve this system of equations. And here, I also showthe equations in the metric form. It's the vector p here equals a matrix or the transpose of the matrix here andmultiplied by the vector again. Now, if you still remember some knowledgethat you've learned from linear algebra and then you will realize, this isprecisely the equation for eigenvector. When multiply the metrics by this vector,you get the same value as this matter and this can be solved byusing iterative algorithm. So because the equations here on the back are basicallytaken from the previous slide. So you'll see the relation between thepage that ran sports on different pages. And this iterative approach orpower approach, we simply start with srandomly initialized vector p. And then we repeatedlyjust update this p by multiplying the metricshere by this p factor. I also show a concrete example here. So you can see this now. If we assume alpha is 0.2, then with the example thatwe show here on the slide, we have the originaltransition matrix is here. That includes the graph, the actual linksand we have this smoothing transition metrics, uniform transition metricsrepresenting random jumping. And we can combine them together witha liner interpolation to form another metric that would be like this. So essentially, we can imagine now the web looks likethis and can be captured like that. They're all virtual linksbetween all the pages now. The page we're on now would justinitialize the p vector first and then just computed the updating of this p vector by using thismetrics multiplication. Now if you rewrite thismetric multiplication in terms of individual equations,you'll see this. And this is basically,the updating formula for this particular pages and page score. So you can also see if you want to computethe value of this updated score for d1. You basically multiplythis rule by this column, and we'll take the thirdproduct of the two. And that will give us the value forthis value. So this is how we updated the vectorwe started with an initial values for these guys for this. And then we just revisethe scores which generate a new set of scores andthe updating formula is this one. So we just repeatedly apply this andhere it converges. And when the matrix is like this,where there's no 0 values and it can be guaranteed to converge. And at that point the we will just havethe PageRank scores for all the pages. We typically go to sets ofinitial values just to 1 over n. So interestingly,this updating formula can be also interpreted as propagatingscores on the graph, can you see why? Or if you look at this formula andthen compare that with this graph and can you imagine,how we might be able to interpret this as essentially propagatingscores over the graph. I hope you will see that indeed, we can imagine we have valuesinitialized on each of these pages. So we can have values here andsay, that's a 1 over 4 for each. And then we're going to use thesemetrics to update this the scores. And if you look at the equation herethis one, basically we're going to combine the scores of the pages thatpossibly would lead to reaching this page. So we'll look at all the pagesthat are pointing to this page and then combine this score and propagate thesum of the scores to this document, d1. To look at the scores that we presentthe probability that the random surfer would be visiting the otherpages before it reached d1. And then just dothe propagation to simulate the probability of reaching this page, d1. So there are two interpretations here. One is just the matrix multiplication. We repeat the multiplyingthat by these metrics. The other is to just thinkof it as a propagating these scores repeatedly on the web. So in practice, the combination ofPageRank score is actually efficient. Because the matrices is fast and thereare some, ways we transform the equation. So that you avoid actuallyliterally computing the values for all those elements. Sometimes you may also normalize theequation and that will give you a somewhat different form of the equation, butthen the ranking of pages will not change. The results of this potentialproblem of zero-outlink problem. In that case, if a page does not haveany outlink then the probability of these pages would not sum to 1. Basically, the probability of reaching thenext page from this page would not sum to 1, mainly because we have lostsome probability to mass. One would assume there's some probabilitythat the surfer would try to follow the links, butthen there is no link to follow. And one possible solution is simply to usea page that is specific damping factor, and that could easily fix this. Basically, that's to say alpha wouldbe 1.0 for a page with no outlink. In that case,the surfer would just have to randomly jump to another pageinstead of trying to follow a link. There are many extensions of PageRank, oneextension is to topic-specific PageRank. Note that PageRank doesn't merelyuse the query information. So we can make PageRank specific however. So for example,at the top of a specific page you rank, we can simply assumewhen the surfer is bored. The surfer is not randomlyjumping to any page on the web. Instead, he's going to jump to only thosepages that are relevant to our query. For example, if the query is not sportsthen we can assume that when it's doing random jumping, it's goingto randomly jump to a sports page. By doing this, then we can buya PageRank through topic and sports. And then if you know the currenttheory is about sports, and then you can use this specializedPageRank score to rank documents. That would be better than if youuse the generic PageRank score. PageRank is also a channel that can beused in many other applications for network analysis particularly forexample, social networks. You can imagine if you computethe PageRank scores for social network, where a linkmight indicate a friendship or a relation, you would get somemeaningful scores for people [MUSIC]"
cs-410,5,8,"[SOUND]So we talked about PageRank asa way to capture the assault. Now, we also looked at some other exampleswhere a hub might be interesting. So there is another algorithm called HITS,and that going to compute the scores forauthorities and hubs. The intuitions are pages that are widelycited are good authorities and whereas pages that cite manyother pages are good hubs. I think that the most interestingidea of this algorithm HITS, is it's going to usea reinforcement mechanism to kind of help improve the scoring forhubs and the authorities. And so here's the idea, it was assumed that goodauthorities are cited by good hubs. That means if you are cited by manypages with good hub scores then that inquiry says, you're an authority. And similarly, good hubs are thosethat point at good authorities. So if you pointed to a lotof good authority pages, then your hubs score would be increased. So then you will have literally reinforcedeach other, because you have pointed so some good hubs. And so you have pointed to some goodauthorities to get a good hubs score, whereas those authorityscores would be also improved because theyare pointing to by a good hub. And this is algorithms is also general itcan have many applications in graph and network analysis. So just briefly, here's how it works. We first also construct a matrix, but thistime we're going to construct an adjacent matrix andwe're not going to normalize the values. So if there's a link there's a 1,if there's no link that's 0. Again, it's the same graph. And then we're going todefine the hubs score of page as the sum of the authority scores ofall the pages that it appoints to. So whether you are hub, really depends on whether you are pointingto a lot of good authority pages. That's what it says in the first equation. In the second equation,we define the authorities of a page as a sum of the hub scores of allthose pages that appoint to you. So whether you are good authoritywould depend on whether those pages that are pointingto you are good hubs. So you can see this formsiterative reinforcement mechanism. Now, these three questions can bealso written in the metrics format. So what we get here is then the hubvector is equal to the product of the adjacency matrix andthe authority vector, and this is basically the first equation. And similarly, the second equationcan be returned as the authority vector is equal to the product ofa transpose multiplied by the hub vector. Now, these are just different waysof expressing these equations. But what's interesting is thatif you look at the matrix form, you can also plug in the authorityequation into the first one. So if you do that, you have actuallyeliminated the authority vector completely and you get the equationsof only hubs scores. The hubs score vector isequal to a multiplied by a transpose multipliedby the hub score again. Similarly, we can do a transformationto have equation for just the authorities also. So although we frame the problemas computing hubs and authorities, we can actually eliminate one of them toobtain equation just for one of them. Now, the difference between this and pagerandom is that now the matrix is actually a multiplication of the adjacencymatrix and it's transpose. So this is different from page rank. But mathematically, then we willbe computing the same problem. So in HITS,we typically would initialize the values. Let's say, 1 for all these values, and then we would iteratively applythese equations, essentially. And this is equivalent to multiplythat by the metrics a and a transpose. So the arrows of these is exactlythe same in the PageRank. But here because the adjacencymatrix is not normalized. So what we have to do is after eachiteration we're going to normalize, and this would allow us tocontrol the growth of value. Otherwise they would grow larger andlarger. And if we do that, andthat will basically get HITS. That was the computer, the hubs scores,and authority scores for all the pages. And these scores can then be used inbranching just like the PageRank scores. So to summarize in this lecture, we haveseen that link information's very useful. In particular,the anchor text is very useful to increase the textrepresentation of a page. And we also talk about the PageRank and page anchor as two majorlink analysis algorithms. Both can generate scores for web pagesthat can be used in the ranking function. Note that PageRank andthe HITS are also very general algorithms. So they have many applications inanalyzing other graphs or networks. [MUSIC]"
cs-410,6,1,"[MUSIC] This lecture is aboutthe Learning to Rank. In this lecture, we are going tocontinue talking about web search. In particular we're going to talkabout the using machine learning to combine different featuresto improve the ranking function. So the question that we address inthis lecture is how we can combine many features to generate a single rankingfunction to optimize search results? In the previous lectures we have talkedabout a number of ways to rank documents. We have talked about some retrievalmodels like a BM25 or Query Light Code. They can generate a based this course formatching documents with a query. And we also talked about the linkbased approaches like page rank that can give additional scoresto help us improve ranking. Now the question now is,how can we combine all these features and potentially many otherfeatures to do ranking? And this will be very useful forranking webpages, not only just to improve accuracy, but also to improvethe robustness of the ranking function. So that it's not easy fora spammer to just perturb a one or a few features to promote a page. So the general idea of learningto rank is to use machine learning to combine thisfeatures to optimize the weights on different features to generatethe optimal ranking function. So we will assume that the givena query document pair Q and D, we can define a number of features. And these features can vary fromcontent based features such as a score of the document withrespect to the query according to a retrieval function such as BM25 orQuery Light Hold of punitive commands from a machine orPL2 etcetera. It can also be a link based score like orpage rank score like. It can be also application of retrievalmodels to the ink text of the page. Those are the types of descriptionsof links that point to this page. So, these can all the clues whetherthis document is relevant, or not. We can even include a featuresuch as whether the URL has a tilde because this might beindicator of home page or entry page. So all these features can then be combinedtogether to generate a ranking function. The question is, of course. How can we combine them? In this approach,we simply hypothesize that the probability that this document isn't relevant to thisquery is a function of all these features. So we can hypothesize this that the probability of relevanceis related to these features through a particular form ofthe function that has some parameters. These parameters can control the influence of differentfeatures of the final relevance. Now this is of course just an assumption. Whether this assumption reallymakes sense is a big question and that's they have to empiricallyevaluate the function. But by hypothesizing thatthe relevance is related to these features in the particular way, we canthen combine these features to generate the potential more powerful rankingfunction, a more robust ranking function. Naturally the next question is howdo we estimate those parameters? How do we know which featuresshould have a higher weight, and which features will have lower weight? So this is the task of training orlearning, so in this approach what we willdo is use some training data. Those are the data that havebeen charted by users so that we already knowthe relevant judgments. We already know which documents shouldbe ranked high for which queries. And this information can be basedon real judgments by users or this can also be approximated by justusing click through information, where we can assume the clicked documentsare better than the skipped documents clicked documents are relevant andthe skipped documents are non-relevant. So in general with the fitsuch hypothesize ranking function to the training data meaning that we will try to optimize it'sretrieval accuracy on the training data. And we can adjust these parameters to see how we can optimize the performance ofthe functioning on the training data in terms of some measures such as MAP orNDCG. So the training date wouldlook like a table of tuples. Each tuple has three elements, the query,the document, and the judgement. So it looks very much like ourrelevance judgement that we talked about in the evaluationof retrieval systems. [MUSIC]"
cs-410,6,2,"[MUSIC] So now let's take a look at the specificmethod that's based on regression. Now, this is one of the manydifferent methods, and in fact, it's one of the simplest methods. And I choose this to explainthe idea because it's simple. So in this approach, we simply assumethat the relevance of document with respect to a query is related to a linearcombination of all the features. Here I used Xi to denote the feature. So Xi of Q and D is a feature. And we can have as manyfeatures as we would like. And we assume that these featurescan be combined in a linear manner. And each feature is controlledby a parameter here, and this beta i is a parameter. That's a weighting parameter. A larger value would mean the featurewould have a higher weight, and it would contribute moreto the scoring function. This specific form of the functionactually also involves a transformation ofthe probability of relevance. So this is the probability of relevance. And we know that the probability ofrelevance is within the range from 0 to 1. And we could have just assumed thatthe scoring function is related to this linear combination. So we can do a linear regression. But then, the value of this linearcombination could easily go beyond 1. So this transformationhere would map the 0 to 1 range to the wholerange of real values, you can verify it by yourself. So this allows us then to connectto the probability of variance which is between 0 and 1 to a linearcombination of arbitrary features. And if we rewrite this into a probabilityfunction, we would get the next one. So on this equation, now we'llhave the probability of relevance. And on the right hand side,we'll have this form. Now, this form is clearly nonnegative, and it still involves a linearcombination of features. And it's also clear that if this value is, this is actually negative of the linearcombination in the equation above. If this value here is large, then it would mean this value is small. And therefore,this whole probability would be large. And that's we expect, that basically,it would mean if this combination gives us a high value, thenthe document's more likely irrelevant. So this is our hypothesis. Again, this is not necessarily the besthypothesis, but this is a simple way to connect these features withthe probability of relevance. So now we have this combination function. The next task is toestimate the parameters so that the function cache will be applied. But without knowing the beta values,it's harder to apply this function. So let's see how canestimate our beta values. All right,let's take a look at a simple example. In this example, we have three features. One is the BM25 score of the document andthe query. One is the PageRank score of the document,which might or might not depend on the query. We might have a topic-sensitive PageRank,that would depend on the query. Otherwise, the general PageRankdoesn't really depend on the query. And then we have BM25 score onthe anchor test of the document. Now, these are then the feature values fora particular document query pair. And in this case, the document is D1 andthe judgment says that it's relevant. Here's another training instance andit's these feature values, but in this case, it's not relevant. This is an oversimplified case wherewe just have two instances, but it's sufficient to illustrate the point. So what we can do is we usethe maximum likelihood estimator to actually estimate the parameters. Basically, we're going topredict the relevance status of the document basedon the feature values. That is, given that we observedthese feature values here. Can we predict the relevance here? Now, of course, the prediction would beusing this function that you see here. And we hypothesize that the probabilityof relevance is related to features in this way. So we are going to see, for what values ofbeta we can predict the relevance well. What do we mean by predictingthe relevance well? Well, we just mean, in the first case, for D1 this expression right hereshould give high values. In fact, we'll hope thisto gave a value close to 1. Why?Because this is a relevant document. On the other hand,in the second case, for D2, we hope this value will be small, right. Why? Because it's a non-relevant document. So now let's see how this canbe mathematically expressed. And this is similar to expressingthe probability of document, only that we are not talking aboutthe probability of words, but talking about the probabilityof relevance, 1 or 0. So what's the probabilityof this document being relevant if it has these feature values? Well, this is just this expression. We just need to plug in the Xi's. So that's what we will get. It's exactly like what we have seen above, only that we replaced theseXi's with now specific values. So for example, this 0.7 goes to here and this 0.11 goes to here. And these are different feature values, and we combine them inthis particular way. The beta values are still unknown. But this gives us the probabilitythat this document is relevant, if we assume such a model. Okay? And we want to maximize this probability,since this is a relevant document. What do we do for the second document? Well, we want to compute the probabilitythat the prediction is non-relevant. So this would mean we have tocompute 1 minus this expression, since this expression is actuallythe probability of relevance. So to compute the non-relevancefrom relevance, we just do 1 minusthe probability of relevance. Okay? So this whole expression thenjust is our probability of predicting these two relevance values. One is 1 here, one is 0. And this whole equationis our probability of observing a 1 here and observing a 0 here. Of course, this probabilitydepends on the beta values. So then our goal is to adjustthe beta values to make this whole thing reach its maximum,make it as large as possible. So that means we're going to compute this. The beta is just the parametervalues that would maximize this whole likelihood expression. And what it means is,if you look at the function, is, we're going to choose betas tomake this as large as possible and make this also as large as possible,which is equivalent to say, make this part as small as possible. And this is precisely what we want. So once we do the training,now we will know the beta values. So then this functionwould be well-defined. Once beta values are known, both this andthis would be completely specified. So for any new query and new document, we can simply compute the features forthat pair. And then we just use this formulato generate the ranking score. And this scoring function can be used torank documents for a particular query. So that's the basic ideaof learning to rank. [MUSIC]"
cs-410,6,3,"[SOUND]There are many more of the Munster learning algorithmsthan the regression based approaches and they generally attempt to directthe optimizer retrieval method. Like a MAP or nDCG. Note that the optimization object orfunction that we have seen on the previous slide is not directlyrelated to the retrieval measure. By maximizing the prediction of one or zero, we don't necessarily optimizethe ranking of those documents. One can imagine that ourprediction may not be too bad. And let's say both are around 0.5. So it's kind of in the middle of zero andone for the two documents. But the ranking can be wrong, so we mighthave a larger value for E2 and then E1. So that won't be good fromretrieval perspective, even though function, it's not bad. In contrast, we might have anothercase where we predicted the values, or around the 0.9, it said. And by the objective function,the error would be larger. But if we didn't get the orderof the two documents correct, that's actually a better result. So these new, more advanced approacheswill try to correct that problem. Of course, then the challenge isthat the optimization problem will be harder to solve. And then, researchers have posedmany solutions to the problem, and you can read more of the references atthe end, know more about these approaches. Now, these learning rankedapproaches after the general. So there accounts would be be appliedwith many other ranking problems, not just the retrieval problem. So some people will gowith recommender systems, computational advertising,or summarization and there are many others that you canprobably encounter in your applications.. To summarize this lecture wehave talked about using machine learning to combine much morefeatures including ranking results. Actually the use of machine learning in information retrieval hasstarted since many decades ago. So for example, the Rocchio feedbackapproach that we talked about earlier was a machine learning approachprior to relevance feedback. But the most recent use of machinelearning has been driven by some changes in the environment ofapplications of retrieval systems. First, it's mostly freedom ofavailability of a lot of training data in the form of critical, such asthey are more available than before. So the data can provide a lot ofuseful knowledge about relevance and machine learning methods can beapplied into a leverage list. Secondly, it's also freedom bythe need for combining many features, and this is not only justbecause there are more features available on the web that canbe naturally used for improved scoring. It's also because by combining them,we can improve the robustness of ranking, so this is desired forcombating spams. Modern search engines all use somekind of machine learning techniques to combine many featuresto optimize ranking and this is a major feature of thesecommercial engines such a Google or Bing. The topic of learning to rank is stillactive research topic in the community, and so we can expect to see new resultsin development in the next few years, perhaps. Here are some additional readingsthat can give you more information about how learning to rank at works andalso some advanced methods. [MUSIC]"
cs-410,6,4,"[SOUND]. This lecture is aboutthe future of web search. In this lecture, we're going to talkabout some possible future trends of web search and intelligent informationretrieval systems in general. In order to further improvethe accuracy of a search engine, it's important that to considerspecial cases of information need. So one particular trend could be tohave more and more specialized than customized search engines, and theycan be called vertical search engines. These vertical search engines can beexpected to be more effective than the current general search enginesbecause they could assume that users are a special group of users thatmight have a common information need, and then the search engine can becustomized with this ser, so, such users. And because of the customization,it's also possible to do personalization. So the search can be personalized, because we have a betterunderstanding of the users. Because of the restrictions with domain,we also have some advantages in handling the documents, because we canhave better understanding of documents. For example, particular words maynot be ambiguous in such a domain. So we can bypass the problem of ambiguity. Another trend we can expect to see, is the search engine willbe able to learn over time. It's like a lifetime learning orlifelong learning, and this is, of course, very attractive because that means thesearch engine will self-improve itself. As more people are using it, the searchengine will become better and better, and this is already happening, because the search engines can learnfrom the [INAUDIBLE] of feedback. More users use it, and the qualityof the search engine allows for the popular queries that are typed in bymany users allow it to become better, so this is sort of anotherfeature that we will see. The third trend might beto the integration of bottles of information access. So search, navigation, andrecommendation or filtering might be combined to form a full-fledgedinformation management system. And in the beginning of this course,we talked about push versus pull. These are different modes of informationaccess, but these modes can be combined. And similarly, in the pull mode, queryingand the browsing could also be combined. And in fact we're doing that basically,today, is the [INAUDIBLE] search endings. We are querying, sometimes browsing,clicking on links. Sometimes we've got someinformation recommended. Although most of the cases the informationrecommended is because of advertising. But in the future, you can imagineseamlessly integrate the system with multi-mode for information access, andthat would be convenient for people. Another trend is that we might see systems that try to go beyond the searchesto support the user tasks. After all, the reason why people wantto search is to solve a problem or to make a decision or perform a task. For example consumers might search for opinions about products inorder to purchase a product, choose a good product by, soin this case it would be beneficial to support the whole workflow of purchasinga product, or choosing a product. In this era, after the common searchengines already provide a good support. For example, you can sometimes look at thereviews, and then if you want to buy it, you can just click on the button to go theshopping site and directly get it done. But it does not provide a,a good task support for many other tasks. For example, for researchers, you might want to find the realm inthe literature or site of the literature. And then, there's no, not much support forfinishing a task such as writing a paper. So, in general, I think,there are many opportunities in the wait. So in the following few slides, I'llbe talking a little bit more about some specific ideas or thoughts that hopefully, can help you in imagining newapplication possibilities. Some of them might be already relevantto what you are currently working on. In general, we can think about anyintelligent system, especially intelligent information system, as we specifiedby these these three nodes. And soif we connect these three into a triangle, then we'll able to specifyan information system. And I call thisData-User-Service Triangle. So basically the three questions youask would be who are you serving and what kind of data are you are managing andwhat kind of service you provide. Right there, this would help usbasically specify in your system. And there are many different waysto connect them depending on how you connect them,you will have a different kind of systems. So let me give you some examples. On the top,you can see different kinds of users. On the left side, you can see differenttypes of data or information, and on the bottom,you can see different service functions. Now imagine you can connectall these in different ways. So, for example, you can connecteveryone with web pages, and the support search andbrowsing, what do you get? Well, that's web search, right? What if we connect UIUC employees withorganization documents or enterprise documents to support the search andbrowsing, but that's enterprise search. If you connect the scientistwith literature information to provide all kinds of service,including search, browsing, or alert of new random documents ormining analyzing research trends, or provide the task with support ordecision support. For example, we might be,might be able to provide a support for automatically generatingrelated work section for a research paper, andthis would be closer to task support. Right?So then we can imagine this wouldbe a literature assistant. If we connect the online shopperswith blog articles or product reviews then we can help these peopleto improve shopping experience. So we can provide, for example data miningcapabilities to analyze the reviews, to compare products, compare sentiment ofproducts and to provide task support or decision support to have themchoose what product to buy. Or we can connect customer servicepeople with emails from the customers, and, and we can imagine a systemthat can provide a analysis of these emails to find that the majorcomplaints of the customers. We can imagine a system wecould provide task support by automatically generatinga response to a customer email. Maybe intelligently attachalso a promotion message if appropriate, if they detect that that'sa positive message, not a complaint, and then you might take this opportunityto attach some promotion information. Whereas if it's a complaint,then you might be able to automatically generate somegeneric response first and tell the customer that he or she canexpect a detailed response later, etc. All of these are trying to helppeople to improve the productivity. So this shows thatthe opportunities are really a lot. It's just only restrictedby our imagination. So this picture shows the trendof the technology, and also, it characterizes the, intelligentinformation system in three angles. You can see in the center, there'sa triangle that connects keyword queries to search a bag of words representation. That means the current search enginesbasically provides search support to users and mostly modelusers based on keyword queries and sees the data throughbag of words representation. So it's a very simple approximation ofthe actual information in the documents. But that's what the current system does. It connects these three nodesin such a simple way, or it only provides a basic search functionand doesn't really understand the user, and it doesn't really understand thatmuch information in the documents. Now, I showed some trends to push eachnode toward a more advanced function. So think about the user node here, right? So we can go beyond the keyword queries,look at the user search history, and then further model the usercompletely to understand the, the user's task environment,task need context or other information. Okay, so this is pushing forpersonalization and complete user model. And this is a majordirection in research in, in order to build intelligentinformation systems. On the document side,we can also see, we can go beyond bag of words implementationto have entity relation representation. This means we'll recognize people's names,their relations, locations, etc. And this is already feasible withtoday's natural processing technique. And Google is the reasonthe initiative on the knowledge graph. If you haven't heard of it,it is a good step toward this direction. And once we can get to that level withoutinitiating robust manner at larger scale, it can enable the search engineto provide a much better service. In the future we would like to have knowledge representation where wecan add perhaps inference rules, and then the search engine wouldbecome more intelligent. So this calls forlarge-scale semantic analysis, and perhaps this is more feasible forvertical search engines. It's easier to make progressin the particular domain. Now on the service side, we see we need to go beyond the search ofsupport information access in general. So search is only one way to get accessto information as well recommender systems and push and pull so differentways to get access to random information. But going beyond access, we also need to help people digest theinformation once the information is found, and this step has to do with analysisof information or data mining. We have to find patterns orconvert the text information into real knowledge that canbe used in application or actionable knowledge that can be used fordecision making. And furthermore the knowledgewill be used to help a user to improve productivity in finishing a task,for example, a decision-making task. Right, so this is a trend. And, and, and so basically,in this dimension, we anticipate in the future intelligent informationsystems will provide intelligent and interactive task support. Now I should also emphasize interactivehere, because it's important to optimize the combined intelligence of the users andthe system. So we, we can get some helpfrom users in some natural way. And we don't have to assume the systemhas to do everything when the human, user, and the machine can collaborate inan intelligent way, an efficient way, then the combined intelligencewill be high and in general, we can minimize the user's overalleffort in solving problem. So this is the big picture of futureintelligent information systems, and this hopefully can provideus with some insights about how to make further innovationson top of what we handled today. [MUSIC]"
cs-410,6,5,"[MUSIC] This lecture is aboutthe Recommender Systems. So far we have talked about a lotof aspects of search engines. We have talked about the problemof search and ranking problem, different methods for ranking,implementation of search engine and how to evaluate a search engine, etc. This is important because we knowthat web search engines are by far the most important applicationsof text retrieval. And they are the most useful toolsto help people convert big raw text data into a small setof relevant documents. Another reason why we spend somany lectures on search engines, is because many techniques used in searchengines are actually also very useful for Recommender Systems,which is the topic of this lecture. And so, overall, the two systemsare actually well connected. And there are many techniquesthat are shared by them. So this is a slide thatyou have seen before, when we talked about the twodifferent modes of text access. Pull and the Push. And we mentioned that recommendersystems are the main systems to serve users in the Push Mode, where the systemswill take the initiative to recommend the information to the user orpushes information to the user. And this often workswell when the user has stable information needin the system has a good. So a Recommender System is sometimescalled a filtering system and it's because recommending usefulitems to people is like discarding or filtering out the the useless articles, and soin this sense they are kind of similar. And in all the cases the systemmust make a binary decision and usually there's a dynamic sourceof information items, and that you have some knowledgeabout the users' interest. And then the system would make a decision about whether this item isinteresting to the user, and then if it's interesting then the systemwould recommend the article to the user. So the basic filtering question here isreally will this user like this item? Will U like item X? And there are two ways to answer thisquestion, if you think about it. And one is look at what items U likes and then we can see if X isactually like those items. The other is to look at who likes X,and we can see if this user looks like a one of those users,or like most of those users. And these strategies can be combined. If we follow the first strategy and look at item similarity in the caseof recommending text objects, then we're talking about a content-basedfiltering or content-based recommendation. If we look at the second strategy, then,it's to compare users and in this case we're user similarity and the techniqueis often called collaborative filtering. So, let's first look atthe content-based filtering system. This is what the system would look like. Inside the system, there will bea Binary Classifier that would have some knowledge about the user's interests, andthis is called a User Interest Profile. It maintains this profile to keeptrack of all users interests, and then there is a utility functionto guide the user to make decision a nice plan utilityfunction in the moment. It helps the system decidewhere to set the threshold. And then the accepted documents willbe those that have passed the threshold according to the classified. There should be also an initializationmodule that would take a user's input, maybe from a user's specified keywords orchosen category, etc., and this would be to feed intothe system with the initiator's profile. There is also typically a learningmodule that would learn from users' feedback over time. Now note that in this case typicalusers information is stable so the system would have a lot moreopportunities to observe the users. If the user has taken a recommended item,has viewed that, and this a signal to indicate thatthe recommended item may be relevant. If the user discarded it,no, it's not relevant. And so such feedback can be a long termfeedback, and can last for a long time. And the system can collect a lot ofinformation about the user's interest and this then can then be usedto improve the classify. Now what's the criteria forevaluating such a system? How do we know this filteringsystem actually performs well? Now in this case we cannot use the rankingevaluation measures like a map because we can't afford waiting fora lot of documents and then rank the documents tomake a decision for the users. And so the system must makea decision in real time in general to decide whether the item isabove the threshold or not. So in other words, we're tryingto decide on absolute relevance. So in this case, one common user strategy is to usea utility function to evaluate the system. So here, I show linear utility function. That's defined as for example threemultiplied the number of good items that you delivered, minus two multiplied by thenumber of bad items that you delivered. So in other words, we could kind of just treat this as almost in a gambling game. If you delete one good item,let's say you win three dollars, you gain three dollars but if you delivera bad one you will lose two dollars. And this utility functionbasically kind of measures how much money you are get bydoing this kind of game, right? And so it's clear that if you wantto maximize this utility function, this strategy should be deliveredas many good articles as possible, and minimize the delivery of bad articles. That's obvious, right? Now one interesting question here ishow should we set these coefficients? I just showed a three andnegative two as possible coefficients. But one can ask the question,are they reasonable? So what do you think? Do you think that's a reasonable choice? What about the other choices? So for example, we can have 10 andminus 1, or 1, minus 10. What's the difference? What do you think? How would this utility function affectthe systems' threshold of this issue. Right, you can think ofthese two extreme cases. (10, -1) + (1, -10), which one doyou think would encourage this system to over do it and which one wouldencourage this system to be conservative? If you think about it you will see thatwhen we get a bigger award for delivering our good document you incur only a smallpenalty for delivering a bad one. Intuitively, you would beencouraged to deliver more. And you can try to deliver more inhope of getting a good one delivered. And then we'll get a big reward. So on the other hand,if you choose (1,-10), you really don't get such a big prizeif you deliver a good document. On the other hand, you will havea big loss if you deliver a bad one. You can imagine that, the system would be very reluctantto deliver a lot of documents. It has to be absolutelysure that it's not. So this utility function has to bedesigned based on a specific application. The three basic problems in content-basedfiltering are the following, first, it has to makea filtering decision. So it has to be a binary decision maker,a binary classifier. Given a text document anda profile description of the user, it has to say yes or no, whether thisdocument should be deleted or not. So that's a decision module, andit should be an initialization module as you have seen earlier andthis will get the system started. And we have to initialize the systembased on only very limited text exclusion orvery few examples from the user. And the third model isa learning model which you have, has to be able to learn from limitedrelevance judgements, because we counted them from the user about theirpreferences on the deliver documents. If we don't deliver documentto the user we'll never be able to know whetherthe user likes it or not. And we had accumulate a lot of documentseven then from entire history. All these modules will have to beoptimized to maximize the utility. So how can we deal with such a system? And there are many different approaches. Here we're going to talk abouthow to extend a retrieval system, a search engine for information filtering. Again, here's why we've spent a lot oftime talking about the search engines. Because it's actually not very hardto extend the search engine for information filtering. So here's the basic idea forextending a retrieval system for information filtering. First, we can reuse a lot ofretrieval techniques to do scoring. Right, so we know how to scoredocuments against queries, etc. We're going to match the similaritybetween profile text description and a document. And then we can use a score threshold forthe filtering decision. We do retrieval and then we kind of findthe scores of documents and then we'll apply a threshold to see whether thedocument is passing the threshold or not. And if it's passing the threshold, we're going to say it's relevant andwe're going to deliver it to the user. Another component that we have to add is,of course, to learn from the history, and we had used is the traditional feedbacktechniques to learn to improve scoring. And we know rock hill can be using forscoring improvement. And, but we have to develop a newapproaches to learn how to accept this. And we need to set it initially and then we have to learn how toupdate the threshold over time. So here's what the systemmight look like if we just generalize the vector-space model forfiltering problems, right? So you can see the document vector couldbe fed into a scoring module which already exists in a search enginethat implements a vector-space model. And the profile will be treatedas a query essentially, and then the profile vector can be matched withthe document vector to generate the score. And then this score would be fed into athresholding module that would say yes or no, and then the evaluation would be basedon the utility for the filtering results. If it says yes and then the documentwould be sent to the user. And then user could give some feedback. The feedback information would beused to both adjust the threshold and to adjust the vector representation. So the vector learning is essentiallythe same as query modification or feedback in the case of search. The threshold of learningis a new component and that we need to talka little bit more about. [MUSIC]"
cs-410,6,6,"[SOUND] There are some interesting challenges in threshold forthe learning the filtering problem. So here I show the historical data thatyou can collect in the filtering system, so you can see the scores andthe status of relevance. So the first one has a score of 36.5 andit's relevant. The second one is not relevant andit's separate. Of course, we have a lot of documents forwhich we don't know the status, because we have neverdelivered them to the user. So as you can see here, we only see the judgements ofdocuments delivered to the user. So this is not a random sample,so it's a sensitive data. It's kind of biased, so that createssome difficultly for learning. Secondly, there are in general very littlelabeled data and very few relevant data, so it's also challenging formachine learning approaches, typically they require more training data. And in the extreme case atthe beginning we don't even have any labeled data as well. The system there has to make a decision, so that's a very difficultproblem at the beginning. Finally, there is also this issue ofexploration versus exploitation tradeoff. Now, this means we also wantto explore the document space a little bit andto see if the user might be interested in documents thatwe have in data labeled. So in other words, we're going toexplore the space of user interests by testing whether the user might beinterested in some other documents that currently are not matchingthe user's interests so well. So how do we do that? Well, we could lower the thresholda little bit until we just deliver some near misses to the userto see what the user would respond, to see how the user wouldrespond to this extra document. And this is a tradeoff, because onthe one hand, you want to explore, but on the other hand,you don't want to really explore too much, because then you will overdeliver non-relevant Information. So exploitation means you wouldexploit what you learn about the user. Let's say you know the user isinterested in this particular topic, so you don't want to deviate that much, but if you don't deviate at all then you don'texploit so that's also are not good. You might miss opportunity to learnanother interest of the user. So this is a dilemma. And that's also a difficultyproblem to solve. Now, how do we solve these problems? In general, I think one can use theempirical utility optimization strategy. And this strategy is basically to optimizethe threshold based on historical data, just as you have seenon the previous slide. Right, so you can just computethe utility on the training data for each candidate score threshold. Pretend that, what if I cut at this point. What if I cut at the different scoringthreshold point, what would happen? What's utility? Since these are training data,we can kind of compute the utility, and we know that relevant status,or we assume that we know relevant status based onapproximation of click-throughs. So then we can just choose the thresholdthat gives the maximum utility on the training data. But this of course, doesn't account forexploration that we just talked about. And there is also the difficulty ofbiased training sample, as we mentioned. So, in general, we can only get the upperbound for the true optimal threshold, because the threshold mightbe actually lower than this. So, it's possible that this coulddiscarded item might be actually interesting to the user. So how do we solve this problem? Well, we generally, and as I said we can low with thisthreshold to explore a little bit. So here's on particular approachcalled beta-gamma threshold learning. So the idea is falling. So here I show a ranked list of all thetraining documents that we have seen so far, andthey are ranked by their positions. And on the y axis we show the utility,of course, this function depends on how you specify the coefficientsin the utility function, but we can then imagine, that depending on thecutoff position, we will have a utility. Suppose I cut at this position andthat would be a utility. For example,identify some cutting cutoff point. The optimal point,theta optimal, is the point when it will achieve the maximum utilityif we had chosen this as threshold. And there is also zero utility threshold. You can see at this cutoffthe utility is zero. What does that mean? That means if I lower the thresholda little bit, now I reach this threshold. The utility would be lower butit's still non-active at least, right? So it's not as high asthe optimal utility. But it gives us as a safe pointto explore the threshold, as I have explained, it's desirableto explore the interest of space. So it's desirable to lower the thresholdbased on your training there. So that means, in general, we want to setthe threshold somewhere in this range. Let's say we can use the alpha to control the deviation fromthe optimal utility point. So you can see the formula of thethreshold would be just the interpolation of the zero utility threshold andthe optimal utility threshold. Now, the question is,how should we set alpha? And when should we deviate morefrom the optimal utility point? Well, this can depend on multiple factors,and the one way to solve the problem is to encourage this thresholdmechanism to explore up to the zero point, andthat's a safe point, but we're not going to necessarily reachall the way to the zero point. Rather, we're going to use otherparameters to further define alpha and this specifically is as follows. So there will be a beta parameter tocontrol the deviation from the optimal threshold and this can be based on canbe accounting for the over-fitting to the training data let's say, and sothis can be just an adjustment factor. But what's more interestingis this gamma parameter. Here, and you can see in this formula, gamma is controlling the inference of the number of examplesin training that are set. So you can see in this formula as N whichdenotes the number of training examples becomes bigger, then it wouldactually encourage less exploration. In other words, when these verysmall it would try to explore more. And that just means if we have seen few examples we're not sure whether wehave exhausted the space of interest. So we need to explore but as we haveseen many examples from the user many that have we feel that weprobably don't have to explore more. So this gives us a beta gamma forexploration, right. The more examples we have seenthe less exploration we need to do. So the threshold would be closerto the optimal threshold so that's the basic idea of this approach. This approach actually has been workingwell in some evaluation studies, particularly effective. And also can work on arbitrary utilitywith the appropriate lower bound. And explicitly addressesthe exploration-exploitation tradeoff and it kind of uses the zero utilitythreshold point as a safeguard for exploration-exploitation tradeoff. We're not never going to explorefurther than the zero utility point. So if you take the analogy of gambling,and you don't want to risk on losing money. So it's a safe spend, reallyconservative strategy for exploration. And the problem is of course,this approach is purely heuristic and the zero utility lower boundary is alsooften too conservative, and there are, of course, more advance in machine learningapproaches that have been proposed for solving this problems andthis is their active research area. So to summarize, there are twostrategies for recommended systems or filtering systems, one is content based,which is looking at the item similarity, and the other is collaborative filteringthat was looking at the user similarity. We've covered content-basedfiltering approach. In the next lecture, we will talkabout the collaborative filtering. In content-based filtering system,we generally have to solve several problems relative tofiltering decision and learning, etc. And such a system can actually bebuilt based on a search engine system by adding a threshold mechanism andadding adaptive learning algorithm to allow the system to learn fromlong term feedback from the user. [MUSIC]"
cs-410,6,7,"This lecture is aboutcollaborative filtering. In this lecture we're going to continuethe discussion of recommended systems. In particular, we're going to look atthe approach of collaborative filtering. You have seen this slide before whenwe talked about the two strategies to answer the basic question,will user U like item X? In the previous lecture, we looked at the item similarity,that's content-based filtering. In this lecture, we're going tolook at the user similarity. This is a different strategy,called a collaborative filtering. So first, what is collaborative filtering? It is to make filtering decisions for individual user based onthe judgements of other uses. And that is to say we willinfer individual's interest or preferences from thatof other similar users. So the general idea is the following. Given a user u, we're going to firstfind the similar users, U1 through. And then we're going topredict the use preferences based on the preferences ofthese similar users, U1 through. Now, the user similarity here canbe judged based their similarity, the preferences on a common set of items. Now here you can see the exactcontent of item doesn't really matter. We're going to look at the only therelation between the users and the items. So this means thisapproach is very general. It can be applied to any items,not just the text of objects. So this approach would work wellunder the following assumptions. First, users with the same interestwill have similar preferences. Second, the users with similar preferencesprobably share the same interest. So for example, if the interest ofthe user is in information retrieval, then we can infer the userprobably favor SIGIR papers. So those who are interested ininformation retrieval researching, probably all favor SIGIR papers. That's an assumption that we make. And if this assumption is true, then it would help collaborativefiltering to work well. We can also assume that if we seepeople favor See SIGIR papers, then we can infer their interestis probably information retrieval. So in these simple examples,it seems to make sense, and in many cases such assumptionactually does make sense. So another assumption we have to makeis that there are sufficiently large number of user preferencesavailable to us. So for example, if you see a lotof ratings of users for movies and those indicate theirpreferences on movies. And if you have a lot of such data,then cluttered and filtering can be very effective. If not, there will be a problem, andthat's often called a cold start problem. That means you don't have manypreferences available, so the system could not fully take advantageof collaborative filtering yet. So let's look at the filteringproblem in a more formal way. So this picture shows that we are, in general, considering a lot of users and we're showing m users here, so U1 through. And we're also consideringa number of objects. Let's say n objects inorder to O1 through On. And then we will assume thatthe users will be able to judge those objects and the user could forexample give ratings to those items. For example, those items could be movies,could be products and then the users would giveratings 1 through 5 and see. So what you see here is that we haveshown some ratings available for some combinations. So some users have watched some movies,they have rated those movies, they obviously won't be ableto watch all the movies and some users may actuallyonly watch a few movies. So this is in general a small symmetrics. So many items andmany entries have unknown values. And what's interesting here is wecould potentially infer the value of an element in this matrixbased on other values. And that's after the essential questionin collaborative filtering, and that is, we assume there's an unknownfunction here, f. That would map a pair of user andobject to a rating. And we have observed the sumvalues of this function. And we want to infer the valueof this function for other pairs that don't havethat as available here. So this is very similar to othermachinery problems where we'd know the values of the functionon some training data set. And we hope to predict the values ofthis function on some test data so this is a function approximation. And how can we pick out the functionbased on the observed ratings. So this is the setup. Now there are many approachesto solving this problem. In fact,this is a very active research area or reason that there are specialconferences dedicated to the problem, major conference devoted to the problem. [MUSIC]"
cs-410,6,8,"[SOUND]And here we're going to talkabout basic strategy. And that would be based onsimilarity of users and then predicting the rating of and object by an active user using the ratingsof similar users to this active user. This is called a memory based approachbecause it's a little bit similar to storing all the user information and when we are considering a particularuser we going to try to retrieve the rating users orthe similar users to this user case. And then try to use thisinformation about those users to predict the preference of this user. So here is the general idea andwe use some notations here, so x sub i j denotes the ratingof object o j by user u i and n sub i is average ratingof object by this user. So this n i is needed because we would like to normalizethe ratings of objects by this user. So how do you do normalization? Well, we're going to just subtractthe average rating from all the ratings. Now, this is to normalize these ratings so that the ratings from differentusers would be comparable. Because some users might be more generous,and they generally give more high ratings but some others might bemore critical so their ratings cannot be directly compared with eachother or aggregate them together. So we need to do this normalization. Another prediction ofthe rating on the item by another user oractive user, u sub a here can be based on the averageratings of similar users. So the user u sub a is the user that weare interested in recommending items to. And we now are interested inrecommending this o sub j. So we're interested in knowing howlikely this user will like this object. How do we know that? Where the idea here is to look atwhether similar users to this user have liked this object. So mathematically this is to saywell the predicted the rating of this user on this app object,user a on object o j is basically combination of the normalizedratings of different users, and in fact here,we're taking a sum over all the users. But not all users contributeequally to the average, and this is conjured by the weights. So this weight controls the inference of the user on the prediction. And of course,naturally this weight should be related to the similarity between ua andthis particular user, ui. The more similar they are,then the more contribution user ui can make in predictingthe preference of ua. So, the formula is extremely simple. You can see,it's a sum of all the possible users. And inside the sum we have their ratings,well, their normalized ratingsas I just explained. The ratings need to be normalized inorder to be comparable with each other. And then these ratingsare weighted by their similarity. So you can imagine w of a and i is justa similarity of user a and user i. Now what's k here? Well k is simply a normalizer. It's just one over the sum of allthe weights, over all the users. So this means, basically, if you considerthe weight here together with k, and we have coefficients of weight thatwill sum to one for all the users. And it's just a normalization strategy sothat you get this predictor rating in the same range as these ratingsthat we used to make the prediction. Right? So this is basically the main ideaof memory-based approaches for collaborative filtering. Once we make this prediction,we also would like to map back through the rating that the user would actually make, and this is to furtheradd the mean rating or average rating of this user usub a to the predicted value. This would recover a meaningful rating forthis user. So if this user is generous, thenthe average it would be is somewhat high, and when we add that the rating will beadjusted to our relatively high rate. Now when you recommend an item to a userthis actually doesn't really matter, because you are interested inbasically the normalized reading, that's more meaningful. But when they evaluate theserather than filter approaches, they typically assume thatactual ratings of the user on these objects to be unknown andthen you do the prediction and then you compare the predictedratings with their actual ratings. So, you do have accessto the actual ratings. But, then you pretend that you don't know,and then you compare your systemspredictions with the actual ratings. In that case, obviously, the systemsprediction would be adjusted to match the actual ratings of the user andthis is what's happening here basically. Okay so this is the memory based approach. Now, of course,if you look at the formula, if you want to writethe program to implement it, you still face the problem ofdetermining what is this w function? Once you know the w function, thenthe formula is very easy to implement. So, indeed, there are many different waysto compute this function or this weight, w, and specific approaches generallydiffer in how this is computed. So here are some possibilities and you can imagine thereare many other possibilities. One popular approach is we usethe Pearson correlation coefficient. This would be a sum overcommonly rated items. And the formula is a standardappears in correlation coefficient formula as shown here. So this basically measureswhether the two users tended to all give higher ratings to similaritems or lower ratings to similar items. Another measure is the cosine measure,and this is going to treat the rating vectors as vectors in the vector space. And then,we're going to measure the angle and compute the cosine ofthe angle of the two vectors. And this measure has been using the vectorspace model for retrieval, as well. So as you can imagine there are justas many different ways of doing that. In all these cases, note that the user'ssimilarity is based on their preferences on items and we did not actually useany content information of these items. It didn't matter these items are,they can be movies, they can be books, they can be products, they can be text documents whichhas been cabled the content and so this allows such approach to beapplied to a wide range of problems. Now in some newer approaches of course, we would like to use moreinformation about the user. Clearly, we know more about the user,not just these preferences on these items. So in the actual filtering system,is in collaborative filtering, we could also combine thatwith content based filtering. We could use more context information,and those are all interesting approaches that people are just starting, andthere are new approaches proposed. But, this memory based approach hasbeen shown to work reasonably well, and it's easy to implement inpractical applications this could be a starting point to see if the strategyworks well for your application. So, there are some obvious waysto also improve this approach and mainly we would like to improvethe user similarity measure. And there are some practicalissues we deal with here as well. So for example,there will be a lot of missing values. What do you do with them? Well, you can set them to default valuesor the average ratings of the user. And that would be a simple solution. But there are advanced approaches thatcan actually try to predict those missing values, and then use predictivevalues to improve the similarity. So in fact that the memory based apologycan predict those missing values, right? So you get you have iterative approachwhere you first use some preliminary prediction and then you can use the predictive values tofurther improve the similarity function. So this is a heuristicway to solve the problem. And the strategy obviously would affectthe performance of claritative filtering just like any other heuristics wouldimprove these similarity functions. Another idea which is actually verysimilar to the idea of IDF that we have seen in text search is calleda Inverse User Frequency or IUF. Now here the idea is to look at wherethe two users share similar ratings. If the item is a popular item thathas been viewed by many people and seen [INAUDIBLE] to people interestedin this item may not be so interesting but if it's a rare item,it has not been viewed by many users. But these two users deal with thisitem and they give similar ratings. And, that says moreabout their similarity. It's kind of to emphasizemore on similarity on items that are notviewed by many users. [MUSIC]"
cs-410,6,9,"[SOUND]So to summarize our discussion ofrecommender systems, in some sense, the filtering task forrecommender task is easy, and in some other sense,the task is actually difficult. So it's easy becausethe user's expectation is low. In this case the system takes initiativeto push information to the user. The user doesn't really make any effort,so any recommendation is better than nothing. All right.So, unless you recommend the noise items or useless documents. If you can recommendsome useful information users generally will appreciate it,so that's, in that sense that's easy. However, filtering is actually much hardertask than retrieval because you have to make a binary decision and you can'tafford waiting for a lot of items and then you're going to see whetherone item is better than others. You have to make a decisionwhen you see this item. Think about news filtering. As soon as you see the news enoughto decide whether the news would be interesting to the user. If you wait for a few days, well, even ifyou can make accurate recommendation of the most relevant news, the utility isgoing to be significantly decreased. Another reason why it's hardis because of data sparseness if you think of thisas a learning problem. Collaborative filtering, for example, is purely based onlearning from the past ratings. So if you don't have many ratings there'sreally not that much you can do, right? And yeah I just mentionedthis cold start problem. This is actually a very serious,serious problem. But of course there are strategies thathave been proposed for the soft problem, and there are different strategies thatyou can use to alleviate the problem. You can use, for example, more userinformation to asses their similarity, instead of using the preferencesof these users on these items give me additional informationavailable about the user, etc. And we also talk about two strategies forfiltering task. One is content-based wherewe look at items there is collaborative filtering wherewe look at Use a similarity. And they obviously can becombined in a practical system. You can imagine they generallywould have to be combined. So that would give us a hybridstrategy for filtering. And we also could recall that we talked about push versus pull as two strategiesfor getting access to the text data. And recommender system easy tohelp users in the push mode, and search engines are servingusers in the pull mode. Obviously the two should be combined,and they can be combined. The two have a systemthat can support user with multiple mode information access. So in the future we could anticipate sucha system to be more useful the user. And either,this is an active research area so there are a lot of new algorithmsbeing proposed all the time. In particular those new algorithms tendto use a lot of context information. Now the context here could bethe context of the user and could also be the context of the user. Items. The items are not the isolated. They're connected in many ways. The users might formsocial network as well, so there's a rich context therethat we can leverage in order to really solve the problem well andthen that's active research area where also machinelearning algorithms have been applied. Here are some additional readings in the handbook calledRecommender Systems and has a collection of a lotof good articles that can give you an overviewof a number of specific approaches through recommender systems. [MUSIC]"
cs-410,6,10,"[NOISE] This lecture is a summary of this course. This map shows the major topicswe have covered in this course. And here are some keyhigh-level take-away messages. First, we talked about naturallanguage content analysis. Here the main take-away messagesis natural language processing is a foundation for text retrieval, butcurrently the NLP isn't robust enough so the battle of wars is generally the mainmethod used in modern search engines. And it's often sufficient beforemost of the search tasks, but obviously formore complex search tasks then we need a deeper natural languageprocessing techniques. We then talked about the highlevel strategies for text access andwe talked about push versus pull. In pull we talked aboutquerying versus browsing. Now in general in future search engines,we should integrate all these techniques to provide a math involvedinformation access. And now we'll talk about a number ofissues related to search engines. We talked about the search problem. And we framed that as a ranking problem. And we talked about a numberof retrieval methods. We start with the overviewof vector space model and the probabilistic model and then we talkedabout the vector space model in depth. We also later talked aboutthe language modeling approach, and that's probabilistic model. And here, many take-away message is thatthe modeling retrieval function tend to look similar, andthey generally use various heuristics. Most important ones are TF-IDF weighting,document length normalization. And the TF is often transformed througha sub media transformation function. And then we talked about how toimplement a retrieval system, and here, the main techniques that we talked about,how to construct an inverted index so that we can prepare the systemto answer a query quickly. And we talked about how to do a fastersearch by using the inverted index. And we then talked about how toevaluate the text retrieval system, mainly introduced tothe Cranfield Evaluation Methodology. This was a very importantevaluation methodology that can be applied to many tasks. We talked about the majorevaluation measures. So, the most important measures fora search engine are MAP, mean average precision,and nDCG Summarize the discount or accumulative gain and also precision andrecall are the two basic measures. And we then talked aboutfeedback techniques. And we talked about the Rocchioin the Vector Space Model and the mixture model andthe language modeling approach. Feedback is a very importanttechnique especially considering the opportunity of learning froma lot of pixels on the Web. We then talked about Web search. And here we talked about howto use parallel in that scene to solve the scalability issue in thatscene we're going to use the net reduce. Then we talked about how to use linkingpermission model app to improve search. We talked about page rank and hits as the major hours is toanalyzing links on the Web. We then talked aboutlearning through rank. This is the use of machine learningto combine multiple features for improvement scoring. Not only that the effectiveness can beimproved in using this approach, but we can also improve the robustness of the. The ranking function so that it'snot easy to expand a search engine. It just some features to promote the page. And finally we talked aboutthe future of Web search. About the some major reactionsthat we might to see in the future in improving the countof regeneration of such engines. And then finally we talked aboutthe recommended systems and, these are systems toincrement the push mode. And we'll talk about the two approaches,one is content-based, one is collaborative filtering andthey can be combined together. Now, an obvious missing piece in this picture is the user, so user interface is also an importantcomponent in any search engine. Even though the current search interfaceis relatively simple they actually have done a lot of studies of user interfaceswhere we do visualization for example. And this is the topic to that, you can learn more by reading this book. It's an excellent book about all kindsof studies of search using the face. If you want to know more aboutthe topics that we talked about, you can also read some additionalreadings that are listed here. In this short course we onlymanage to cover some basic topics in text retrievals andsearch engines. And these resources provide additionalinformation about more advanced topics and they give a more thorough treatment ofsome of the topics that we talked about. And a main source isthe Synthesis Digital Library that you can see a lot of shortto textbook or textbooks, or long tutorials. They tend to provide a lot ofinformation to explain a topic. And there a lot of series thatare related to this cause. One is information concepts,retrieval, and services. One is human langauge technology. And yet another is artificialintelligence and machine learning. There are also some major journals andconferences listed here that tend to have a lot of research paperswe need to and topic of this course. And finally, for more informationabout resources Including readings, tool kits, etc you can check out his URL. So, if you have not taken the textmining course in this data mining specialization series then naturallythe next step is to take that course. As this picture shows,to mine big text data, we generally need two kinds of techniques. One is text retrieval,which is covered in this course. And these techniques will help usconvert raw big text data into small relevant text data, which are actuallyneeded in the specific application. Now human plays important role in miningany text data because text data is written for humans to consume. So involving humans in the processof data mining is very important and in this course we have coveredthe various strategies to help users get access to the most relevant data. These techniques are always soessential in any text mining system to help provide prominence andto help users interpret the inner patterns that the user willdefine through text data mining. So, in general, the user would haveto go back to the original data to better understand the patterns. So the text mining cause, or rather,text mining and analytics course will be dealing with what to do oncethe user has a following information. So this is a second step in thispicture where we would convert the text data into actionable knowledge. And this has to do with helping users tofurther digest the found information or to find the patterns andto reveal knowledge. In text and such knowledge canthen be used in application systems to help decision making orto help a user finish a task. So, if you have not taken that course,the natural step and that natural next step wouldbe to take that course. Thank you for taking this course. I hope you had fun andfound this course to be useful to you. And I look forward to interactingwith you at a future opportunity. [MUSIC]"
cs-410,7,1,"[SOUND]In this lecture we give an overviewof Text Mining and Analytics. First, let's define the term text mining,and the term text analytics. The title of this course iscalled Text Mining and Analytics. But the two terms text mining, and textanalytics are actually roughly the same. So we are not really going toreally distinguish them, and we're going to use them interchangeably. But the reason that we have chosen to use both terms in the title is becausethere is also some subtle difference, if you look at the two phrases literally. Mining emphasizes more on the process. So it gives us a error ratemedical view of the problem. Analytics, on the other handemphasizes more on the result, or having a problem in mind. We are going to look at textdata to help us solve a problem. But again as I said, we can treatthese two terms roughly the same. And I think in the literatureyou probably will find the same. So we're not going to reallydistinguish that in the course. Both text mining andtext analytics mean that we want to turn text data into high qualityinformation, or actionable knowledge. So in both cases, we have the problem of dealing witha lot of text data and we hope to. Turn these text data into something moreuseful to us than the raw text data. And here we distinguishtwo different results. One is high-quality information,the other is actionable knowledge. Sometimes the boundary betweenthe two is not so clear. But I also want to say a little bit about these two different angles ofthe result of text field mining. In the case of high quality information,we refer to more concise information about the topic. Which might be much easier forhumans to digest than the raw text data. For example, you might facea lot of reviews of a product. A more concise form of informationwould be a very concise summary of the major opinions aboutthe features of the product. Positive about,let's say battery life of a laptop. Now this kind of results are very usefulto help people digest the text data. And so this is to minimize a human effortin consuming text data in some sense. The other kind of outputis actually more knowledge. Here we emphasize the utilityof the information or knowledge we discover from text data. It's actionable knowledge for somedecision problem, or some actions to take. For example, we might be able to determinewhich product is more appealing to us, or a better choice fora shocking decision. Now, such an outcome could becalled actionable knowledge, because a consumer can take the knowledgeand make a decision, and act on it. So, in this case text mining suppliesknowledge for optimal decision making. But again, the two are not soclearly distinguished, so we don't necessarily haveto make a distinction. Text mining is alsorelated to text retrieval, which is a essential componentin many text mining systems. Now, text retrieval refers tofinding relevant information from a large amount of text data. So I've taught another separate MOOCon text retrieval and search engines. Where we discussed various techniques fortext retrieval. If you have taken that MOOC,and you will find some overlap. And it will be useful To knowthe background of text retrieval of understanding some ofthe topics in text mining. But, if you have not taken that MOOC, it's also fine because in this MOOCon text mining and analytics, we're going to repeat some of the key conceptsthat are relevant for text mining. But they're at the high level and they also explain the relation betweentext retrieval and text mining. Text retrieval is very useful fortext mining in two ways. First, text retrieval can bea preprocessor for text mining. Meaning that it can helpus turn big text data into a relatively small amountof most relevant text data. Which is often what's needed forsolving a particular problem. And in this sense, text retrievalalso helps minimize human effort. Text retrieval is also needed forknowledge provenance. And this roughly correspondsto the interpretation of text mining as turning text datainto actionable knowledge. Once we find the patterns in text data, or actionable knowledge, we generallywould have to verify the knowledge. By looking at the original text data. So the users would have to have some textretrieval support, go back to the original text data to interpret the pattern orto better understand an analogy or to verify whether a patternis really reliable. So this is a high level introductionto the concept of text mining, and the relationship betweentext mining and retrieval. Next, let's talk about textdata as a special kind of data. Now it's interesting toview text data as data generated by humans as subjective sensors. So, this slide shows an analogybetween text data and non-text data. And between humans assubjective sensors and physical sensors,such as a network sensor or a thermometer. So in general a sensor wouldmonitor the real world in some way. It would sense some signalfrom the real world, and then would report the signal as data,in various forms. For example, a thermometer would watchthe temperature of real world and then we report the temperaturebeing a particular format. Similarly, a geo sensor would sensethe location and then report. The location specification, for example, in the form of longitudevalue and latitude value. A network sends overthe monitor network traffic, or activities in the network andare reported. Some digital format of data. Similarly we can think ofhumans as subjective sensors. That will observe the real world andfrom some perspective. And then humans will express what theyhave observed in the form of text data. So, in this sense, human is actuallya subjective sensor that would also sense what's happening in the world and then express what's observed in the formof data, in this case, text data. Now, looking at the text data inthis way has an advantage of being able to integrate alltypes of data together. And that's indeed needed inmost data mining problems. So here we are looking atthe general problem of data mining. And in general we would Bedealing with a lot of data about our world thatare related to a problem. And in general it will be dealing withboth non-text data and text data. And of course the non-text dataare usually produced by physical senses. And those non-text data canbe also of different formats. Numerical data, categorical,or relational data, or multi-media data like video or speech. So, these non text data are oftenvery important in some problems. But text data is also very important, mostly because they containa lot of symmetrical content. And they often containknowledge about the users, especially preferences andopinions of users. So, but by treating text data asthe data observed from human sensors, we can treat all this datatogether in the same framework. So the data mining problem isbasically to turn such data, turn all the data in your actionableknowledge to that we can take advantage of it to change the realworld of course for better. So this means the data mining problem is basically taking a lot of data as inputand giving actionable knowledge as output. Inside of the data mining module,you can also see we have a number of differentkind of mining algorithms. And this is because, fordifferent kinds of data, we generally need different algorithms formining the data. For example, video data might require computervision to understand video content. And that would facilitatethe more effective mining. And we also have a lot of generalalgorithms that are applicable to all kinds of data and those algorithms,of course, are very useful. Although, for a particular kind of data, we generally want to alsodevelop a special algorithm. So this course will coverspecialized algorithms that are particularly useful formining text data. [MUSIC]"
cs-410,7,2,"[SOUND]So, looking at the text mining problem moreclosely, we see that the problem is similar to general data mining, exceptthat we'll be focusing more on text data. And we're going to have text miningalgorithms to help us to turn text data into actionable knowledge thatwe can use in real world, especially for decision making, or for completing whatever tasks thatrequire text data to support. Because, in general,in many real world problems of data mining we also tend to have other kindsof data that are non-textual. So a more general picture would beto include non-text data as well. And for this reason we might beconcerned with joint mining of text and non-text data. And so in this course we'regoing to focus more on text mining, but we're also going to also touch how doto joint analysis of both text data and non-text data. With this problem definition wecan now look at the landscape of the topics in text mining and analytics. Now this slide shows the process ofgenerating text data in more detail. More specifically, a human sensor or human observer would look atthe word from some perspective. Different people would be looking atthe world from different angles and they'll pay attention to different things. The same person at different times mightalso pay attention to different aspects of the observed world. And so the humans are able to perceivethe world from some perspective. And that human, the sensor,would then form a view of the world. And that can be called the Observed World. Of course, this would be different fromthe Real World because of the perspective that the person has takencan often be biased also. Now the Observed World can berepresented as, for example, entity-relation graphs orin a more general way, using knowledge representation language. But in general, this is basically whata person has in mind about the world. And we don't really know whatexactly it looks like, of course. But then the human wouldexpress what the person has observed using a natural language,such as English. And the result is text data. Of course a person could have useda different language to express what he or she has observed. In that case we might have text data ofmixed languages or different languages. The main goal of text miningIs actually to revert this process of generating text data. We hope to be able to uncoversome aspect in this process. Specifically, we can think about mining,for example, knowledge about the language. And that means by looking at text datain English, we may be able to discover something about English, some usageof English, some patterns of English. So this is one type of mining problems,where the result is some knowledge about language whichmay be useful in various ways. If you look at the picture, we can also then mine knowledgeabout the observed world. And so this has much to do withmining the content of text data. We're going to look at what the textdata are about, and then try to get the essence of it orextracting high quality information about a particular aspect ofthe world that we're interested in. For example, everything that has beensaid about a particular person or a particular entity. And this can be regarded as mining content to describe the observed world inthe user's mind or the person's mind. If you look further,then you can also imagine we can mine knowledge about this observer,himself or herself. So this has also to do withusing text data to infer some properties of this person. And these properties couldinclude the mood of the person or sentiment of the person. And note that we distinguishthe observed word from the person because text data can't describe what theperson has observed in an objective way. But the description can be alsosubjected with sentiment and so, in general, you can imagine the textdata would contain some factual descriptions of the world plussome subjective comments. So that's why it's also possible to do text mining to mineknowledge about the observer. Finally, if you look at the pictureto the left side of this picture, then you can see we can certainly alsosay something about the real world. Right? So indeed we can do text mining toinfer other real world variables. And this is often calleda predictive analytics. And we want to predict the valueof certain interesting variable. So, this picture basically covered multiple types of knowledge thatwe can mine from text in general. When we infer otherreal world variables we could also use some of the results from mining text data as intermediateresults to help the prediction. For example, after we mine the content of text data wemight generate some summary of content. And that summary could be then used to help us predict the variablesof the real world. Now of course this is still generatedfrom the original text data, but I want to emphasize here thatoften the processing of text data to generate some features that can helpwith the prediction is very important. And that's why here we show the results of some other mining tasks, includingmining the content of text data and mining knowledge about the observer,can all be very helpful for prediction. In fact, when we have non-text data,we could also use the non-text data to help prediction, andof course it depends on the problem. In general, non-text data can be veryimportant for such prediction tasks. For example,if you want to predict stock prices or changes of stock prices based ondiscussion in the news articles or in social media, then this is an example of using text data to predictsome other real world variables. But in this case, obviously, the historical stock price data wouldbe very important for this prediction. And so that's an example ofnon-text data that would be very useful for the prediction. And we're going to combine both kindsof data to make the prediction. Now non-text data can be also used foranalyzing text by supplying context. When we look at the text data alone, we'll be mostly looking at the contentand/or opinions expressed in the text. But text data generally alsohas context associated. For example, the time and the locationthat associated are with the text data. And these are useful context information. And the context can provide interestingangles for analyzing text data. For example, we might partition textdata into different time periods because of the availability of the time. Now we can analyze text data in eachtime period and then make a comparison. Similarly we can partition textdata based on locations or any meta data that's associated toform interesting comparisons in areas. So, in this sense,non-text data can actually provide interesting angles orperspectives for text data analysis. And it can help us make context-sensitive analysis of content orthe language usage or the opinions about the observer orthe authors of text data. We could analyze the sentimentin different contexts. So this is a fairly general landscape ofthe topics in text mining and analytics. In this course we're going toselectively cover some of those topics. We actually hope to covermost of these general topics. First we're going to covernatural language processing very briefly because this has to dowith understanding text data and this determines how we can representtext data for text mining. Second, we're going to talk about how tomine word associations from text data. And word associations is a form of use forlexical knowledge about a language. Third, we're going to talk abouttopic mining and analysis. And this is only one way toanalyze content of text, but it's a very useful waysof analyzing content. It's also one of the most usefultechniques in text mining. Then we're going to talk aboutopinion mining and sentiment analysis. So this can be regarded as one exampleof mining knowledge about the observer. And finally we're going tocover text-based prediction problems where we try to predict somereal world variable based on text data. So this slide also serves asa road map for this course. And we're going to usethis as an outline for the topics that we'll coverin the rest of this course. [MUSIC]"
cs-410,7,3,"[SOUND] This lecture is about natural language content analysis. Natural language content analysisis the foundation of text mining. So we're going to first talk about this. And in particular, natural language processing witha factor how we can present text data. And this determines what algorithms canbe used to analyze and mine text data. We're going to take a look at the basicconcepts in natural language first. And I'm going to explain these concepts using a similar examplethat you've all seen here. A dog is chasing a boy on the playground. Now this is a very simple sentence. When we read such a sentencewe don't have to think about it to get the meaning of it. But when a computer has tounderstand the sentence, the computer has to gothrough several steps. First, the computer needsto know what are the words, how to segment the words in English. And this is very easy,we can just look at the space. And then the computer will needthe know the categories of these words, syntactical categories. So for example, dog is a noun,chasing's a verb, boy is another noun etc. And this is called a Lexical analysis. In particular, tagging these wordswith these syntactic categories is called a part-of-speech tagging. After that the computer also needs tofigure out the relationship between these words. So a and dog would form a noun phrase. On the playground would bea prepositional phrase, etc. And there is certain way forthem to be connected together in order for them to create meaning. Some other combinationsmay not make sense. And this is called syntactical parsing, or syntactical analysis,parsing of a natural language sentence. The outcome is a parse treethat you are seeing here. That tells us the structureof the sentence, so that we know how we caninterpret this sentence. But this is not semantics yet. So in order to get the meaning wewould have to map these phrases and these structures into some real worldantithesis that we have in our mind. So dog is a concept that we know,and boy is a concept that we know. So connecting these phrasesthat we know is understanding. Now for a computer, would have to formallyrepresent these entities by using symbols. So dog, d1 means d1 is a dog. Boy, b1 means b1 refers to a boy etc. And also represents the chasingaction as a predicate. So, chasing is a predicate here with three arguments, d1, b1, and p1. Which is playground. So this formal rendition ofthe semantics of this sentence. Once we reach that level of understanding,we might also make inferences. For example, if we assume there's a rulethat says if someone's being chased then the person can get scared, then wecan infer this boy might be scared. This is the inferred meaning,based on additional knowledge. And finally, we might even further infer what this sentence is requesting, or why the person who say it ina sentence, is saying the sentence. And so, this has to do withpurpose of saying the sentence. This is called speech act analysis orpragmatic analysis. Which first to the use of language. So, in this case a person saying thismay be reminding another person to bring back the dog. So this means when saying a sentence,the person actually takes an action. So the action here is to make a request. Now, this slide clearly shows thatin order to really understand a sentence there are a lot ofthings that a computer has to do. Now, in general it's very hard fora computer will do everything, especially if you would wantit to do everything correctly. This is very difficult. Now, the main reason why naturallanguage processing is very difficult, it's because it's designed it willmake human communications efficient. As a result, for example,with only a lot of common sense knowledge. Because we assume all ofus have this knowledge, there's no need to encode this knowledge. That makes communication efficient. We also keep a lot of ambiguities,like, ambiguities of words. And this is again, because we assume wehave the ability to disambiguate the word. So, there's no problem withhaving the same word to mean possibly different thingsin different context. Yet fora computer this would be very difficult because a computer does not havethe common sense knowledge that we do. So the computer will be confused indeed. And this makes it hard fornatural language processing. Indeed, it makes it very hard for every step in the slidethat I showed you earlier. Ambiguity is a main killer. Meaning that in every stepthere are multiple choices, and the computer would have todecide whats the right choice and that decision can be very difficultas you will see also in a moment. And in general, we need common sense reasoning in orderto fully understand the natural language. And computers today don't yet have that. That's why it's very hard for computers to precisely understandthe natural language at this point. So here are some specificexamples of challenges. Think about the world-level ambiguity. A word like design can be a noun ora verb, so we've got ambiguous part of speech tag. Root also has multiple meanings,it can be of mathematical sense, like in the square of, orcan be root of a plant. Syntactic ambiguity refersto different interpretations of a sentence in terms structures. So for example, natural language processing canactually be interpreted in two ways. So one is the ordinary meaning that we will be getting as we'retalking about this topic. So, it's processing of natural language. But there's is also anotherpossible interpretation which is to say languageprocessing is natural. Now we don't generally have this problem,but imagine for the computer to determine the structure, the computer would haveto make a choice between the two. Another classic example is a mansaw a boy with a telescope. And this ambiguity lies inthe question who had the telescope? This is called a prepositionalphrase attachment ambiguity. Meaning where to attach thisprepositional phrase with the telescope. Should it modify the boy? Or should it be modifying, saw, the verb. Another problem is anaphora resolution. In John persuaded Bill to buy a TV forhimself. Does himself refer to John or Bill? Presupposition is another difficulty. He has quit smoking impliesthat he smoked before, and we need to have such a knowledge inorder to understand the languages. Because of these problems, the stateof the art natural language processing techniques can not do anything perfectly. Even forthe simplest part of speech tagging, we still can not solve the whole problem. The accuracy that are listed here,which is about 97%, was just taken from some studies earlier. And these studies obviously have tobe using particular data sets so the numbers here are notreally meaningful if you take it out of the context of the dataset that are used for evaluation. But I show these numbers mainly to giveyou some sense about the accuracy, or how well we can do things like this. It doesn't mean any data setaccuracy would be precisely 97%. But, in general, we can do parsing speechtagging fairly well although not perfect. Parsing would be more difficult, but forpartial parsing, meaning to get some phrases correct, we can probablyachieve 90% or better accuracy. But to get the complete parse treecorrectly is still very, very difficult. For semantic analysis, we can also dosome aspects of semantic analysis, particularly, extraction of entities andrelations. For example, recognizing this isthe person, that's a location, and this person andthat person met in some place etc. We can also do word sense to some extent. The occurrence of root in this sentencerefers to the mathematical sense etc. Sentiment analysis is another aspectof semantic analysis that we can do. That means we can tag the sensesas generally positive when it's talking about the product ortalking about the person. Inference, however, is very hard,and we generally cannot do that for any big domain and if it's onlyfeasible for a very limited domain. And that's a generally difficultproblem in artificial intelligence. Speech act analysis isalso very difficult and we can only do this probably forvery specialized cases. And with a lot of help from humansto annotate enough data for the computers to learn from. So the slide also shows that computers are far from being able tounderstand natural language precisely. And that also explains why the textmining problem is difficult. Because we cannot rely onmechanical approaches or computational methods tounderstand the language precisely. Therefore, we have to usewhatever we have today. A particular statistical machine learningmethod of statistical analysis methods to try to get as much meaningout from the text as possible. And, later you will seethat there are actually many such algorithmsthat can indeed extract interesting model from text even thoughwe cannot really fully understand it. Meaning of all the naturallanguage sentences precisely. [MUSIC]"
cs-410,7,4,"[SOUND] So here are some specific examples of what we can't do today and part of speech tagging is stillnot easy to do 100% correctly. So in the example, he turned off thehighway verses he turned off the fan and the two offs actually have somewhata differentness in their active categories and also its very difficultto get a complete the parsing correct. Again, the example, a man saw a boywith a telescope can actually be very difficult to parsedepending on the context. Precise deep semanticanalysis is also very hard. For example, to define the meaning of own, precisely is very difficult inthe sentence, like John owns a restaurant. So the state of the off canbe summarized as follows. Robust and general NLP tends to be shallow whilea deep understanding does not scale up. For this reason in this course,the techniques that we cover are in general, shallow techniques foranalyzing text data and mining text data and they are generallybased on statistical analysis. So there are robust andgeneral and they are in the in category of shallow analysis. So such techniques havethe advantage of being able to be applied to any text data inany natural about any topic. But the downside is that, they don'tgive use a deeper understanding of text. For that, we have to rely ondeeper natural language analysis. That typically would requirea human effort to annotate a lot of examples of analysis that wouldlike to do and then computers can use machine learning techniques and learn fromthese training examples to do the task. So in practical applications, we generallycombine the two kinds of techniques with the general statistical andmethods as a backbone as the basis. These can be applied to any text data. And on top of that, we're going to usehumans to, and you take more data and to use supervised machine learningto do some tasks as well as we can, especially for those importanttasks to bring humans into the loop to analyze text data more precisely. But this course will coverthe general statistical approaches that generally,don't require much human effort. So they're practically,more useful that some of the deeper analysis techniques that require a lot ofhuman effort to annotate the text today. So to summarize,the main points we take are first NLP is the foundation for text mining. So obviously, the better wecan understand the text data, the better we can do text mining. Computers today are far from being ableto understand the natural language. Deep NLP requires common senseknowledge and inferences. Thus, only working forvery limited domains not feasible for large scale text mining. Shallow NLP based on statisticalmethods can be done in large scale and is the main topic of this course and they are generally applicableto a lot of applications. They are in some sense also,more useful techniques. In practice,we use statistical NLP as the basis and we'll have humans forhelp as needed in various ways. [MUSIC]"
cs-410,7,5,"This lecture is about thetextual representation. In this lecture, we are going to discuss textualrepresentation, and discuss how naturallanguage processing can allow us to represent textin many different ways. Let's take a look at thisexample sentence again. We can represent this sentencein many different ways. First, we can always represent such a sentenceas a string of characters. This is true forall the languages when we store themin the computer. When we store a naturallanguage sentence as a string of characters, we have perhaps the most generalway of representing text since we always use this approach torepresent any text data. But unfortunately, usingsuch a representation will not help us to do semantic analysis, which is often needed for many applicationsof text mining. The reason is because we'renot even recognizing words. So as a string, we're going to keepall the spaces and these ASCII symbols. We can perhaps count what's the most frequent characterin English text, or the correlationbetween those characters, but we can't reallyanalyze semantics. Yet, this is the mostgeneral way of representing text because we can use this to represent anynatural language text. If we try to do a little bit more naturallanguage processing by doing word segmentation, then we can obtain arepresentation of the same text, but in the form of asequence of words. So here we see thatwe can identify words like a dog is chasing etc. Now with this levelof representation, we certainly can doa lot of things, and this is mainly becausewords are the basic units of human communicationin natural language, so they are very powerful. By identifying words, we can for example easily count what are the most frequent words in this document or inthe whole collection etc. These words can be used to form topics when we combinerelated words together, and some words are positive, some words negative, so we can also do sentiment analysis. So representing text dataas a sequence of words opens up a lot of interestinganalysis possibilities. However, this level of representation is slightlyless general than string of characters because insome languages such as Chinese, it's actually notthat easy to identify all the word boundariesbecause in such a language, you see text as a sequence of characters withno space in between. So you'll have to rely on some special techniquesto identify words. In such a language,of course then, we might make mistakesin segmenting words. So the sequence ofwords representation is not as robust asstring of characters. But in English, it's very easy to obtain this levelof representation, so we can do that all the time. Now, if we go further to do naturallylanguage processing, we can add a part of speech tags. Now once we do that, we can count, for example, the most frequentnouns or what kind of nouns are associated withwhat kind of verbs etc. So this opens up a little bit moreinteresting opportunities for further analysis. Note that I use a plus signhere because by representing text as a sequenceof part of speech tags, we don't necessarily replace the original wordsequence written. Instead, we add this as an additional way ofrepresenting text data, so that now the data isrepresented as both a sequence of words and a sequenceof part of speech tags. This enriches therepresentation of text data, and thus also enablesmore interesting analysis. If we go further, then we'llbe pausing the sentence often to obtaina syntactic structure. Now this of course, further open upa more interesting analysis of, for example, the writing styles orcorrecting grammar mistakes. If we go further forsemantic analysis, then we might be able torecognize dog as an animal, and we also can recognizea boy as a person, and playground as a location. We can further analyzetheir relations, for example, dog is chasing the boy andthe boy is on the playground. Now this will addmore entities and relations throughentity relation recreation. At this level, then we can do even moreinteresting things. For example, now wecan count easily the most frequent person that's mentioning this whole collectionof news articles, or whenever youmention this person, you also tend to see mentioningof another person etc. So this is a veryuseful representation, and it's also related to the knowledge graph thatsome of you may have heard of that Google is doing as a more semantic way ofrepresenting text data. However, it's also less robustthan sequence of words or even syntactical analysisbecause it's not always easy to identify all the entities withthe right types, and we might make mistakes, and relations areeven harder to find, and we might make mistakes. So this makes this level ofrepresentation less robust, yet it's very useful. Now if we move furtherto logical condition, then we can have predicatesand even inference rules. With inference rules, we can infer interesting derivedfacts from the text, so that's very useful. But unfortunately,at this level of representation is even less robust and we can make mistakes and we can't do that all the time forall kinds of sentences. Finally, speech acts wouldadd a yet another level of repetition of the intentof saying this sentence. So in this case, it might be a request. So knowing that wouldallow us to analyze even more interestingthings about this observer or the authorof this sentence. What's the intentionof saying that? What's scenarios? What kindof actions would be made? So this is another level of analysis that wouldbe very interesting. So this picture showsthat if we move down, we generally seemore sophisticated natural language processingtechniques to be used. Unfortunately,such techniques would require more human effort, and they are less accurate. That means there are mistakes. So if we add an texts that are at the levels that are representing deeperanalysis of language, then we have totolerate the errors. So that also means it'sstill necessary to combine such deep analysis withshallow analysis based on, for example, sequence of words. On the right side, you'll see the arrow pointsdown to indicate that. As we go down, we are representationof text is closer to knowledge representationin our mind, and need for solvinga lot of problems. Now this is desirable because as we can represent text atthe level of knowledge, we can easily extractthe knowledge. That's the purposeof text-mining. So there is a trade-off here between doinga deeper analysis that might have errors but would give us direct knowledge thatcan be extracted from text. Doing shallow analysis, which is more robust butwouldn't actually give us the necessary deeperrepresentation of knowledge. I should also say thattext data are generated by humans and are meant tobe consumed by humans. So as a result, intext data analysis, text-mining humans playa very important role, they are always in the loop. Meaning that we should optimize the collaboration ofhumans and computers. So in that sense, it's okay that computersmay not be able to have compute accuratelyrepresentation of text data, and the patternsthat are extracted from text data can beinterpreted by humans, and humans canguide the computers to do more accurate analysisby annotating more data, by providing featuresto guide a machine learning programs to makethem work more effectively."
cs-410,7,6,"[SOUND]. So, as we explained the different text representation tends toenable different analysis. In particular,we can gradually add more and more deeper analysis resultsto represent text data. And that would open up a moreinteresting representation opportunities andalso analysis capacities. So, this table summarizeswhat we have just seen. So the first column showsthe text representation. The second visualizes the generalityof such a representation. Meaning whether we can do thiskind of representation accurately for all the text data or only some of them. And the third column showsthe enabled analysis techniques. And the final column shows someexamples of application that can be achieved through thislevel of representation. So let's take a look at them. So as a stream text can only be processedby stream processing algorithms. It's very robust, it's general. And there was still some interestingapplications that can be down at this level. For example, compression of text. Doesn't necessarily need toknow the word boundaries. Although knowing word boundariesmight actually also help. Word base repetition is a veryimportant level of representation. It's quite general and relatively robust, indicating theywere a lot of analysis techniques. Such as word relation analysis,topic analysis and sentiment analysis. And there are many applications that canbe enabled by this kind of analysis. For example, thesaurus discovery hasto do with discovering related words. And topic andopinion related applications are abounded. And there are, for example, people might be interesting in knowing the majortopics covered in the collection of texts. And this can be the casein research literature. And scientists want to know what are themost important research topics today. Or customer service people might want toknow all our major complaints from their customers by mining their e-mail messages. And business intelligencepeople might be interested in understanding consumers' opinions abouttheir products and the competitors' products to figure out what are thewinning features of their products. And, in general, there are many applications that can be enabled bythe representation at this level. Now, moving down, we'll see we cangradually add additional representations. By adding syntactical structures,we can enable, of course, syntactical graph analysis. We can use graph mining algorithmsto analyze syntactic graphs. And some applications are relatedto this kind of representation. For example, stylistic analysis generally requiressyntactical structure representation. We can also generatethe structure based features. And those are features that might help usclassify the text objects into different categories by looking at the structuressometimes in the classification. It can be more accurate. For example,if you want to classify articles into different categories correspondingto different authors. You want to figure out which ofthe k authors has actually written this article, then you generally needto look at the syntactic structures. When we add entities and relations, then we can enable other techniquessuch as knowledge graph and answers, or information network andanswers in general. And this analysis enableapplications about entities. For example, discovery of all the knowledge andopinions about real world entities. You can also use this level representation to integrate everything aboutanything from scaled resources. Finally, when we add logical predicates, that would enable large inference,of course. And this can be very useful for integrating analysis ofscattered knowledge. For example,we can also add ontology on top of the, extracted the information from text,to make inferences. A good of example of application in thisenabled by this level of representation, is a knowledge assistant for biologists. And this program that can help a biologistmanage all the relevant knowledge from literature about a research problem suchas understanding functions of genes. And the computer can make inferences about some of the hypothesis thatthe biologist might be interesting. For example,whether a gene has a certain function, and then the intelligent program can read theliterature to extract the relevant facts, doing compiling andinformation extracting. And then using a logic system toactually track that's the answers to researchers questioning about whatgenes are related to what functions. So in order to supportthis level of application we need to go as far aslogical representation. Now, this course is covering techniquesmainly based on word based representation. And these techniques are general and robust and that's more widelyused in various applications. In fact, in virtually all the text miningapplications you need this level of representation and then techniques thatsupport analysis of text in this level. But obviously all these otherlevels can be combined and should be combined in order to supportthe sophisticated applications. So to summarize,here are the major takeaway points. Text representation determines whatkind of mining algorithms can be applied. And there are multiple ways torepresent the text, strings, words, syntactic structures, entity-relationgraphs, knowledge predicates, etc. And these differentrepresentations should in general be combined in real applicationsto the extent we can. For example, even if we cannotdo accurate representations of syntactic structures, we can statethat partial structures strictly. And if we can recognize some entities,that would be great. So in general we want todo as much as we can. And when different levelsare combined together, we can enable a richer analysis,more powerful analysis. This course however focuseson word-based representation. Such techniques have also severaladvantage, first of they are general and robust, so they are applicableto any natural language. That's a big advantage overother approaches that rely on more fragile natural languageprocessing techniques. Secondly, it does not requiremuch manual effort, or sometimes, it does notrequire any manual effort. So that's, again, an important benefit, because that means that you can applyit directly to any application. Third, these techniques are actuallysurprisingly powerful and effective form in implications. Although not all of courseas I just explained. Now they are very effectivepartly because the words are invented by humans as basicallyunits for communications. So they are actually quite sufficient forrepresenting all kinds of semantics. So that makes this kind of word-basedrepresentation all so powerful. And finally, such a word-basedrepresentation and the techniques enable by such a representation can be combinedwith many other sophisticated approaches. So they're not competing with each other. [MUSIC]"
cs-410,7,7,"[SOUND] This lecture is about the word association mining and analysis. In this lecture,we're going to talk about how to mine associations of words from text. Now this is an example of knowledgeabout the natural language that we can mine from text data. Here's the outline. We're going to first talk aboutwhat is word association and then explain why discovering suchrelations is useful and finally we're going to talk about some generalideas about how to mine word associations. In general there are two wordrelations and these are quite basic. One is called a paradigmatic relation. The other is syntagmatic relation. A and B have paradigmatic relation if they can be substituted for each other. That means the two words thathave paradigmatic relation would be in the same semantic class,or syntactic class. And we can in generalreplace one by the other without affectingthe understanding of the sentence. That means we would stillhave a valid sentence. For example, cat and dog, these twowords have a paradigmatic relation because they are inthe same class of animal. And in general,if you replace cat with dog in a sentence, the sentence would still be a validsentence that you can make sense of. Similarly Monday andTuesday have paradigmatical relation. The second kind of relation iscalled syntagmatical relation. In this case, the two words that have thisrelation, can be combined with each other. So A and B have syntagmatic relation ifthey can be combined with each other in a sentence, that means these twowords are semantically related. So for example, cat and sit are relatedbecause a cat can sit somewhere. Similarly, car anddrive are related semantically and they can be combined witheach other to convey meaning. However, in general, we can notreplace cat with sit in a sentence or car with drive in the sentenceto still get a valid sentence, meaning that if we do that, the sentencewill become somewhat meaningless. So this is different fromparadigmatic relation. And these two relations are in fact sofundamental that they can be generalized to capture basic relationsbetween units in arbitrary sequences. And definitely they can begeneralized to describe relations of any items in a language. So, A and B don't have to be words andthey can be phrases, for example. And they can even be more complexphrases than just a non-phrase. If you think about the generalproblem of the sequence mining then we can think about the unitsbeing and the sequence data. Then we think of paradigmaticrelation as relations that are applied to units that tend to occurin a singular locations in a sentence, or in a sequence of dataelements in general. So they occur in similar locationsrelative to the neighbors in the sequence. Syntagmatical relation onthe other hand is related to co-occurrent elements that tendto show up in the same sequence. So these two are complimentary andare basic relations of words. And we're interested in discoveringthem automatically from text data. Discovering such wordedrelations has many applications. First, such relations can be directlyuseful for improving accuracy of many NLP tasks, and this is because this is partof our knowledge about a language. So if you know these two wordsare synonyms, for example, and then you can help a lot of tasks. And grammar learning can be alsodone by using such techniques. Because if we can learnparadigmatic relations, then we form classes of words,syntactic classes for example. And if we learn syntagmatic relations,then we would be able to know the rules for putting together a largerexpression based on component expressions. So we learn the structure andwhat can go with what else. Word relations can be also very useful for many applications in text retrieval andmining. For example, in search andtext retrieval, we can use word associations to modify a query,and this can be used to introduce additional related words intoa query and make the query more effective. It's often called a query expansion. Or you can use related words tosuggest related queries to the user to explore the information space. Another application is touse word associations to automatically construct the topof the map for browsing. We can have words as nodes andassociations as edges. A user could navigate fromone word to another to find information in the information space. Finally, such word associations can alsobe used to compare and summarize opinions. For example, we might be interestedin understanding positive and negative opinions about the iPhone 6. In order to do that, we can look at whatwords are most strongly associated with a feature word like battery inpositive versus negative reviews. Such a syntagmaticalrelations would help us show the detailed opinionsabout the product. So, how can we discover suchassociations automatically? Now, here are some intuitionsabout how to do that. Now let's first look atthe paradigmatic relation. Here we essentially can takeadvantage of similar context. So here you see some simplesentences about cat and dog. You can see they generallyoccur in similar context, and that after all is the definitionof paradigmatic relation. On the right side you can kindof see I extracted expressly the context of cat anddog from this small sample of text data. I've taken away cat anddog from these sentences, so that you can see just the context. Now, of course we can have differentperspectives to look at the context. For example, we can look atwhat words occur in the left part of this context. So we can call this left context. What words occur before we see cat or dog? So, you can see in this case, clearlydog and cat have similar left context. You generally say his cat or my cat andyou say also, my dog and his dog. So that makes them similarin the left context. Similarly, if you look at the wordsthat occur after cat and dog, which we can call right context,they are also very similar in this case. Of course, it's an extreme case,where you only see eats. And in general,you'll see many other words, of course, that can't follow cat and dog. You can also even lookat the general context. And that might include allthe words in the sentence or in sentences around this word. And even in the general context, you alsosee similarity between the two words. So this was just a suggestionthat we can discover paradigmatic relation by looking atthe similarity of context of words. So, for example,if we think about the following questions. How similar are context of cat andcontext of dog? In contrast how similar are contextof cat and context of computer? Now, intuitively,we're to imagine the context of cat and the context of dog wouldbe more similar than the context of cat andcontext of the computer. That means, in the first casethe similarity value would be high, between the context of cat anddog, where as in the second, the similarity between context of cat andcomputer would be low because they all not having a paradigmatic relationship and imagine what wordsoccur after computer in general. It would be very different fromwhat words occur after cat. So this is the basic idea of whatthis covering, paradigmatic relation. What about the syntagmatic relation? Well, here we're going to explorethe correlated occurrences, again based on the definitionof syntagmatic relation. Here you see the same sample of text. But here we're interested in knowingwhat other words are correlated with the verb eats andwhat words can go with eats. And if you look at the rightside of this slide and you see,I've taken away the two words around eats. I've taken away the word to its left and also the word to itsright in each sentence. And then we ask the question, what wordstend to occur to the left of eats? And what words tend tooccur to the right of eats? Now thinking about this questionwould help us discover syntagmatic relations because syntagmatic relationsessentially captures such correlations. So the important question to ask forsyntagmatical relation is, whenever eats occurs,what other words also tend to occur? So the question here hasto do with whether there are some other words that tendto co-occur together with each. Meaning that whenever you see eatsyou tend to see the other words. And if you don't see eats, probably,you don't see other words often either. So this intuition can helpdiscover syntagmatic relations. Now again, consider example. How helpful is occurrence of eats forpredicting occurrence of meat? Right.All right, so knowing whether eats occurs in a sentence would generally help uspredict whether meat also occurs indeed. And if we see eats occur in the sentence,and that should increase the chancethat meat would also occur. In contrast,if you look at the question in the bottom, how helpful is the occurrence of eats forpredicting of occurrence of text? Because eats andtext are not really related, so knowing whether eats occurredin the sentence doesn't really help us predict the weather,text also occurs in the sentence. So this is in contrast tothe question about eats and meat. This also helps explain that intuition behind the methods of whatdiscovering syntagmatic relations. Mainly we need to capture the correlationbetween the occurrences of two words. So to summarize the general ideas for discovering word associationsare the following. For paradigmatic relation,we present each word by its context. And then compute its context similarity. We're going to assume the wordsthat have high context similarity to have paradigmatic relation. For syntagmatic relation, we will counthow many times two words occur together in a context, which can be a sentence,a paragraph, or a document even. And we're going to compare their co-occurrences withtheir individual occurrences. We're going to assume wordswith high co-occurrences but relatively low individual occurrencesto have syntagmatic relations because they attempt to occur together andthey don't usually occur alone. Note that the paradigmatic relation andthe syntagmatic relation are actually closely relatedin that paradigmatically related words tend to have syntagmaticrelation with the same word. They tend to be associatedwith the same word, and that suggests that we can also do jointhe discovery of the two relations. So these general ideas can beimplemented in many different ways. And the course won't cover all of them,but we will cover at least some ofthe methods that are effective for discovering these relations. [MUSIC]"
cs-410,7,8,"[SOUND]This lecture is aboutthe Paradigmatics Relation Discovery. In this lecture we are going to talk abouthow to discover a particular kind of word association calleda paradigmatical relation. By definition,two words are paradigmatically related if they share a similar context. Namely, they occur insimilar positions in text. So naturally our idea of discovering sucha relation is to look at the context of each word and then try to computethe similarity of those contexts. So here is an example ofcontext of a word, cat. Here I have taken the wordcat out of the context and you can see we are seeing some remainingwords in the sentences that contain cat. Now, we can do the same thing foranother word like dog. So in general we would like to capturesuch a context and then try to assess the similarity of the context of cat andthe context of a word like dog. So now the question is how can weformally represent the context and then define the similarity function. So first, we note that the contextactually contains a lot of words. So, they can be regarded asa pseudo document, a imagine document, but there are also differentways of looking at the context. For example, we can look at the wordthat occurs before the word cat. We can call this context Left1 context. All right, so in this case youwill see words like my, his, or big, a, the, et cetera. These are the words that canoccur to left of the word cat. So we say my cat, his cat,big cat, a cat, et cetera. Similarly, we can also collect the wordsthat occur right after the word cat. We can call this context Right1, and here we see words like eats,ate, is, has, et cetera. Or, more generally, we can look at all the words inthe window of text around the word cat. Here, let's say we can take a windowof 8 words around the word cat. We call this context Window8. Now, of course, you can see allthe words from left or from right, and so we'll have a bag of words ingeneral to represent the context. Now, such a word based representationwould actually give us an interesting way to define theperspective of measuring the similarity. Because if you look at justthe similarity of Left1, then we'll see words that sharejust the words in the left context, and we kind of ignored the other wordsthat are also in the general context. So that gives us one perspective tomeasure the similarity, and similarly, if we only use the Right1 context, we will capture this narrativefrom another perspective. Using both the Left1 andRight1 of course would allow us to capture the similarity with evenmore strict criteria. So in general, context may containadjacent words, like eats and my, that you see here, ornon-adjacent words, like Saturday, Tuesday, orsome other words in the context. And this flexibility also allows usto match the similarity in somewhat different ways. Sometimes this is useful, as we might want to capturesimilarity base on general content. That would give us looselyrelated paradigmatical relations. Whereas if you use only the wordsimmediately to the left and to the right of the word, then youlikely will capture words that are very much related by their syntacticalcategories and semantics. So the general idea of discoveringparadigmatical relations is to compute the similarityof context of two words. So here, for example,we can measure the similarity of cat and dog based on the similarityof their context. In general, we can combine allkinds of views of the context. And so the similarity function is,in general, a combination of similaritieson different context. And of course, we can also assignweights to these different similarities to allow us to focusmore on a particular kind of context. And this would be naturallyapplication specific, but again, here the main idea for discoveringpardigmatically related words is to computer the similarityof their context. So next let's see how we exactlycompute these similarity functions. Now to answer this question,it is useful to think of bag of words representation as vectorsin a vector space model. Now those of you who have beenfamiliar with information retrieval or textual retrieval techniques wouldrealize that vector space model has been used frequently formodeling documents and queries for search. But here we also find it convenientto model the context of a word for paradigmatic relation discovery. So the idea of thisapproach is to view each word in our vocabulary as defining onedimension in a high dimensional space. So we have N words intotal in the vocabulary, then we have N dimensions,as illustrated here. And on the bottom, you can see a frequencyvector representing a context, and here we see where eatsoccurred 5 times in this context, ate occurred 3 times, et cetera. So this vector can then be placedin this vector space model. So in general,we can represent a pseudo document or context of cat as one vector,d1, and another word, dog, might give us a different context,so d2. And then we can measurethe similarity of these two vectors. So by viewing context inthe vector space model, we convert the problem ofparadigmatical relation discovery into the problem of computingthe vectors and their similarity. So the two questions that wehave to address are first, how to compute each vector, andthat is how to compute xi or yi. And the other question is howdo you compute the similarity. Now in general, there are many approachesthat can be used to solve the problem, and most of them are developed forinformation retrieval. And they have been shown to work well for matching a query vector anda document vector. But we can adapt many ofthe ideas to compute a similarity of context documents for our purpose here. So let's first look atthe one plausible approach, where we try to matchthe similarity of context based on the expected overlap of words,and we call this EOWC. So the idea here is to representa context by a word vector where each word has a weightthat's equal to the probability that a randomly picked word fromthis document vector, is this word. So in other words,xi is defined as the normalized account of word wi in the context, and this can be interpreted asthe probability that you would actually pick this word from d1if you randomly picked a word. Now, of course these xi's would sum to onebecause they are normalized frequencies, and this means the vector is actually probability ofthe distribution over words. So, the vector d2 can be alsocomputed in the same way, and this would give us then two probabilitydistributions representing two contexts. So, that addresses the problemhow to compute the vectors, and next let's see how we can definesimilarity in this approach. Well, here, we simply definethe similarity as a dot product of two vectors, andthis is defined as a sum of the products of the correspondingelements of the two vectors. Now, it's interesting to seethat this similarity function actually has a nice interpretation,and that is this. Dot product, in fact that givesus the probability that two randomly picked words fromthe two contexts are identical. That means if we try to pick a wordfrom one context and try to pick another word from another context, we can thenask the question, are they identical? If the two contexts are very similar,then we should expect we frequently will see the two words picked fromthe two contexts are identical. If they are very different,then the chance of seeing identical words being picked fromthe two contexts would be small. So this intuitively makes sense, right,for measuring similarity of contexts. Now you might want to also takea look at the exact formulas and see why this can be interpretedas the probability that two randomly picked words are identical. So if you just stare at the formulato check what's inside this sum, then you will see basically in eachcase it gives us the probability that we will see an overlap ona particular word, wi. And where xi gives us a probability thatwe will pick this particular word from d1, and yi gives us the probabilityof picking this word from d2. And when we pick the sameword from the two contexts, then we have an identical pick, right so. That's one possible approach, EOWC,extracted overlap of words in context. Now as always, we would like to assesswhether this approach it would work well. Now of course, ultimately we have totest the approach with real data and see if it gives us reallysemantically related words. Really give us paradigmatical relations,but analytically we can also analyzethis formula a little bit. So first, as I said,it does make sense, right, because this formula will give a higher score if thereis more overlap between the two contexts. So that's exactly what we want. But if you analyzethe formula more carefully, then you also see there mightbe some potential problems, and specifically thereare two potential problems. First, it might favor matchingone frequent term very well, over matching more distinct terms. And that is because in the dot product,if one element has a high value and this element is shared by both contexts andit contributes a lot to the overall sum, it might indeed make the scorehigher than in another case, where the two vectors actually havea lot of overlap in different terms. But each term has a relatively lowfrequency, so this may not be desirable. Of course, this might bedesirable in some other cases. But in our case, we should intuitivelyprefer a case where we match more different terms in the context,so that we have more confidence in saying that the two wordsindeed occur in similar context. If you only rely on one term and that's a little bit questionable,it may not be robust. Now the second problem is that ittreats every word equally, right. So if you match a word like the and it will be the same asmatching a word like eats, but intuitively we knowmatching the isn't really surprising because the occurs everywhere. So matching the is not as suchstrong evidence as matching what a word like eats,which doesn't occur frequently. So this is anotherproblem of this approach. In the next chapter we are going to talkabout how to address these problems. [MUSIC]"
cs-410,7,9,"In this lecture, we continue discussing ParadigmaticalRelation Discovery. Earlier we introduceda method called Expected Overlap ofWords in Context. In this method, werepresent each context by a word vector that represents the probability of aword in the context. We measure the similarityby using the.product, which can be interpreted asthe probability that two randomly picked words from the two contexts are identical. We also discussedthe two problems of this method. The first is thatit favors matching one frequent term very well over matching more distinct terms. It put too much emphasis onmatching one term very well. The second is that ittreats every word equally. Even a common word likethe will contribute equally as contentword like eats. So now we are going to talk about howto solve these problems. More specifically, we'regoing to introduce some retrieval heuristicsused in text retrieval. These heuristics can effectivelysolve these problems, as these problems alsooccur in text retrieval when we match a query thatthough with a document vector. So to address the first problem, we can use a sublineartransformation of tone frequency. That is, we don't have to use the raw frequency count of a term to represent the context. We can transformit into some form that wouldn't emphasize somuch on the raw frequency. To address thesynchronous problem, we can put more weighton rare terms. That is we can rewardmatching a real-world. This heuristic is called the IDF term weighting in text retrieval. IDF stands forInverse Document Frequency. So now, we're going to talk about the two heuristicsin more detail. First let's talk aboutthe TF Transformation. That is to convertthe raw count of a word in the documentinto some weight that reflects our belief about how importantthis word in the document. So that will bedenoted by TF of w,d. That's shown in the y-axis. Now, in general, there aremany ways to map that. Let's first look atthe simple way of mapping. In this case, we'regoing to say, well, any non-zero countswill be mapped to one and the zero countwill be mapped to zero. So with this mapping all the frequencies will be mapped to only twovalues; zero or one. The mapping function is shownhere as a flat line here. Now, this is naive because it's notthe frequency of words. However, this actuallyhas the advantage of emphasizing matching allthe words in the context. So it does not allow a frequency of word todominate the matching. Now, the approachthat we have taken earlier in the expectedoverlap count approach, is a linear transformation. We basically, takey as the same as x. So we use the raw countas a representation. That created the problem that we just talked about namely; it emphasize too much on justmatching one frequent term. Matching one frequent termcan contribute a lot. So we can have a lot of other interestingtransformations in between the two extremes, and they generally forma sublinear transformation. So for example,one possibility is to take logarithm of the raw count, and this will give us curvethat looks like this, that you are seeing here. In this case, you can seethe high frequency counts. The high counts arepenalize a little bit, so the curve is a sublinearcurve and it brings down the weight ofthose really high counts. This is what we want,because it prevents that terms from dominatingthe scoring function. Now, there is also another interestingtransformation called a BM25 transformation which has been shown to be veryeffective for retrieval. In this transformation, we have a form that looks like this. So it's k plus one multipliedby x divided by x plus k, where k is a parameter, x is the count, the raw count of a word. Now, the transformationis very interesting in that it can actually go from one extreme to the otherextreme by varying k. It also interestingthat it has upper bound, k plus one in this case. So this putsa very strict constraint on high frequency terms, because their weight wouldnever exceed k plus one. As we vary k, if we cansimulate the two extremes. So when k is set to zero, we roughly have the 0,1 vector. Whereas when we set kto a very large value, it will behave more likethe linear transformation. So this transformationfunction is by far the most effectivetransformation function for text retrieval and it also makes sense for our problem setup. So we just talked about howto solve the problem of overemphasizing a frequency term Now let's look atthe second problem, and that is how we canpenalize popular terms. Matching ""the"" is not surprising, because ""the"" occurs everywhere. But matching ""eats""would count a lot. So how can we addressthat problem? Now in this case, we canuse the IDF weighting. That's commonlyused in retrieval. IDF stands forInverse Document Frequency. Document frequencymeans the count of the total number of documents that containa particular word. So here we show that the IDFmeasure is defined as a logarithm functionof the number of documents that match aterm or document frequency. So K is the number ofdocuments containing word or document frequency and M here is the total number ofdocuments in the collection. The IDF function is givinga higher value for a lower K, meaning that itrewards rare term. The maximum value islog of M plus one. That's when the word occurredjust once in a context. So that's a very rare term, the rare is term inthe whole collection. The lowest value you cansee here is when K reaches its maximum which would be M. So that would bea very low value, close to zero in fact. So this of course measure is used in search where wenaturally have a collection. In our case, what wouldbe our collection? Well, we can alsouse the context that we can collect all the wordsas our collection. That is to say, a word that's popular inthe collection in general, would also have a low IDF. Because depending on the dataset, we can construct the contextvectors in different ways. But in the end if a term is very frequent inthe original dataset, then it will still be frequent in the collectivecontext documents. So how can we addthese heuristics to improve our similarity function? Well, here's one wayand there are many other waysthat are possible. But this is a reasonable way, where we can adaptthe BM25 retrieval model for paradigmaticalrelation mining. In this case, we define thedocument vector as containing elements representingnormalized BM25 values. So in thisnormalization function, we take sum over allthe words and we normalize the weight ofeach word by the sum of the weights of all the words. This is to again ensure all the xi's will sum toone in this vector. So this would be very similarto what we had before, in that this vector is actually something similarto a word distribution, all the xi's will sum to one. Now, the weight of BM25 foreach word is defined here. If you compare this withour old definition where we just have a normalized countof this one, right? So we only have this oneand the document lens or the total counts of words inthat context to document, and that's what we had before. But now with the BM25transformation, we introduced something else. First, of course,this extra occurrence of this count is just to achieve the sub-linearnormalization. But we also see we introducedthe parameter, k, here, and this parameter isgenerally a non-active number, although zero is also possible. But this controlsthe upper bound, and also controls to what extent it simulates thelinear transformation. So this is one parameter, but we also see there isanother parameter here, b, and this would bewithin zero and one. This is a parameter tocontrol lens normalization. In this case, the normalization formula has a average document lens here. This is computed upby taking the average of the lenses of all thedocuments in the collection. In this case, all the lenses of all the context of documentsthat we're considering. So this average documents will be a constant forany given collection. So it actually is only affecting the effectof the parameter, b, here becausethis is a constant. But I kept it here becauseit's a constant that's used for in retrieval where it would give us a stabilizedinterpretation of parameter, b. But for our purpose, this will be a constant soit would only be affecting the lens normalizationtogether with parameter, b. Now, with this definition then, we have a new way to defineour document of vectors, and we can computethe vector d2 in the same way. The difference is that the high-frequency terms will now have a somewhat lower weights. This would help us control the inference ofthese high-frequency terms. Now, the idea can be addedhere in the scoring function. That means we'llintroduce a weight for matching each term. So you may recallthis sum indicates all the possible wordsthat can be overlap between the two contexts. The x_i and the y_iare probabilities of picking the wordfrom both contexts. Therefore, itindicates how likely we'll see a match on this word. Now, IDF would give us the importance ofmatching this word. A common word will be worthless than a rare word. So we emphasize more onmatching rare words now. So with this modification, then the new function will likely addressthose two problems. Now, interestinglywe can also use this approach to discoversyntagmatic relations. In general, when we re-brand a context with a term vector, we would likely see some terms have high weights and other terms have low weights. Depending on how we assignweights to these terms, we might be able touse these weights to discover the words that are strongly associated with the candidate wordin the context. So let's take a look at the term vector inmore detail here. We have each x_i defined as the normalizedweight of BM25. Now, this weight alone only reflects how frequent the wordoccurs in the context. But we can't just say any frequent term in the context that wouldbe correlated with the candidate word because many common words like 'the' will occur frequently inall the context. But if we apply IDFweighting as you see here, we can then re-weightthese terms based on IDF. That means the words that are common like 'the'will get penalized. So now the highestweighted terms will not be those common terms becausethey have lower IDFs. Instead, those terms would be the terms that arefrequent in the context, but not frequentin the collection. So those are clearly the wordsthat tend to occur in the context of the candidateword, for example, cat. So for this reason, the highly weighted terms inthis idea of weighted vector can also be assumed to be candidates forsyntagmatic relations. Now, of course, this isonly a by-product of our approach for discoveringparadigmatic relations. In the next lecture, we'regoing to talk more about how to discoversyntagmatic relations. But it clearly shows the relation between discoveringthe two relations. Indeed they can be discovered in a joint manner by leveragingsuch associations. So to summarize,the main idea for discovering paradigmatic relations is to collect the context of a candidate word toform a pseudo document. This is typically representedas a bag of words. Then compute the similarity of the correspondingcontext documents of two candidate words. Then we can takethe highly similar word pairs, and treat them as havingparadigmatic relations. These are the words thatshare similar contexts. There are many different ways to implement this general idea. We just talked aboutsome of the approaches. More specifically, wetalked about using text retrieval models to help us design effectivesimilarity function to compute theparadigmatic relations. More specifically, we have used the BM25 and IDF weighting to discoverparadigmatic relation. These approaches also represent the state of the art intext retrieval techniques. Finally, syntagmatic relationscan also be discovered as a by-product when we discoverparadigmatic relations."
cs-410,8,1,"[SOUND]. This lecture is about the syntagmaticrelation discovery, and entropy. In this lecture, we're going to continuetalking about word association mining. In particular, we're going to talk abouthow to discover syntagmatic relations. And we're going to start withthe introduction of entropy, which is the basis for designing somemeasures for discovering such relations. By definition, syntagmatic relations hold between wordsthat have correlated co-occurrences. That means,when we see one word occurs in context, we tend to see the occurrenceof the other word. So, take a more specific example, here. We can ask the question, whenever eats occurs,what other words also tend to occur? Looking at the sentences on the left,we see some words that might occur together with eats, like cat,dog, or fish is right. But if I take them out andif you look at the right side where we only show eats and some other words,the question then is. Can you predict what other wordsoccur to the left or to the right? Right sothis would force us to think about what other words are associated with eats. If they are associated with eats,they tend to occur in the context of eats. More specifically ourprediction problem is to take any text segment which can be a sentence,a paragraph, or a document. And then ask I the question,is a particular word present or absent in this segment? Right here we ask about the word W. Is W present or absent in this segment? Now what's interesting is that some words are actually easierto predict than other words. If you take a look at the threewords shown here, meat, the, and unicorn, which one do youthink is easier to predict? Now if you think about it fora moment you might conclude that the is easier to predict becauseit tends to occur everywhere. So I can just say,well that would be in the sentence. Unicorn is also relatively easybecause unicorn is rare, is very rare. And I can bet that it doesn'toccur in this sentence. But meat is somewhere inbetween in terms of frequency. And it makes it harder to predict becauseit's possible that it occurs in a sentence or the segment, more accurately. But it may also not occur in the sentence,so now let's study thisproblem more formally. So the problem can be formally defined as predicting the value ofa binary random variable. Here we denote it by X sub w,w denotes a word, so this random variable is associatedwith precisely one word. When the value of the variable is 1,it means this word is present. When it's 0, it means the word is absent. And naturally, the probabilities for1 and 0 should sum to 1, because a word is either present orabsent in a segment. There's no other choice. So the intuition with this concept earliercan be formally stated as follows. The more random this random variable is,the more difficult the prediction will be. Now the question is how does onequantitatively measure the randomness of a random variable like X sub w? How in general, can we quantifythe randomness of a variable and that's why we need a measurecalled entropy and this measure introduced in informationtheory to measure the randomness of X. There is also some connectionwith information here but that is beyond the scope of this course. So forour purpose we just treat entropy function as a function definedon a random variable. In this case, it is a binary randomvariable, although the definition can be easily generalized fora random variable with multiple values. Now the function form looks like this, there's the sum of all the possiblevalues for this random variable. Inside the sum for each value wehave a product of the probability that the random variable equals thisvalue and log of this probability. And note that there is alsoa negative sign there. Now entropy in general is non-negative. And that can be mathematically proved. So if we expand this sum, we'll see thatthe equation looks like the second one. Where I explicitly pluggedin the two values, 0 and 1. And sometimes when we have 0 log of 0, we would generally define that as 0,because log of 0 is undefined. So this is the entropy function. And this function willgive a different value for different distributionsof this random variable. And it clearly depends on the probability that the random variabletaking value of 1 or 0. If we plot this function against the probability that the randomvariable is equal to 1. And then the function looks like this. At the two ends,that means when the probability of X equals 1 is very small or very large,then the entropy function has a low value. When it's 0.5 in the middlethen it reaches the maximum. Now if we plot the functionagainst the probability that X is taking a value of 0 and the function would show exactly the same curve here,and you can imagine why. And so that's because the two probabilities are symmetric,and completely symmetric. So an interesting question youcan think about in general is for what kind of X does entropyreach maximum or minimum. And we can in particular thinkabout some special cases. For example, in one case,we might have a random variable that always takes a value of 1. The probability is 1. Or there's a random variable that is equally likely taking a value of one orzero. So in this case the probabilitythat X equals 1 is 0.5. Now which one has a higher entropy? It's easier to look at the problemby thinking of a simple example using coin tossing. So when we think about randomexperiments like tossing a coin, it gives us a random variable,that can represent the result. It can be head or tail. So we can define a random variableX sub coin, so that it's 1 when the coin shows up as head,it's 0 when the coin shows up as tail. So now we can compute the entropyof this random variable. And this entropy indicates howdifficult it is to predict the outcome of a coin toss. So we can think about the two cases. One is a fair coin, it's completely fair. The coin shows up as head ortail equally likely. So the two probabilities would be a half. Right?So both are equal to one half. Another extreme case iscompletely biased coin, where the coin always shows up as heads. So it's a completely biased coin. Now let's think aboutthe entropies in the two cases. And if you plug in these values you cansee the entropies would be as follows. For a fair coin we see the entropyreaches its maximum, that's 1. For the completely biased coin,we see it's 0. And that intuitively makes a lot of sense. Because a fair coin ismost difficult to predict. Whereas a completely biasedcoin is very easy to predict. We can always say, well, it's a head. Because it is a head all the time. So they can be shown onthe curve as follows. So the fair coin corresponds to the middlepoint where it's very uncertain. The completely biased coincorresponds to the end point where we have a probabilityof 1.0 and the entropy is 0. So, now let's see how we can useentropy for word prediction. Let's think about our problem isto predict whether W is present or absent in this segment. Again, think about the three words,particularly think about their entropies. Now we can assume high entropywords are harder to predict. And so we now have a quantitative way totell us which word is harder to predict. Now if you look at the three words meat,the, unicorn, again, and we clearly would expect meat to havea higher entropy than the unicorn. In fact if you look at the entropy of the,it's close to zero. Because it occurs everywhere. So it's like a completely biased coin. Therefore the entropy is zero. [MUSIC]"
cs-410,8,2,"[SOUND] This lecture isabout the syntagmatic relation discovery andconditional entropy. In this lecture, we're going to continue the discussionof word association mining and analysis. We're going to talk about the conditionalentropy, which is useful for discovering syntagmatic relations. Earlier, we talked aboutusing entropy to capture how easy it is to predict the presence orabsence of a word. Now, we'll addressa different scenario where we assume that we know somethingabout the text segment. So now the question is, suppose we knowthat eats occurred in the segment. How would that help uspredict the presence or absence of water, like in meat? And in particular, we want toknow whether the presence of eats has helped us predictthe presence of meat. And if we frame this using entrophy, that would mean we are interestedin knowing whether knowing the presence of eats could reduceuncertainty about the meats. Or, reduce the entrophyof the random variable corresponding to the presence orabsence of meat. We can also ask as a question,what if we know of the absents of eats? Would that also help us predictthe presence or absence of meat? These questions can beaddressed by using another concept called a conditioning entropy. So to explain this concept, let's firstlook at the scenario we had before, when we know nothing about the segment. So we have these probabilities indicatingwhether a word like meat occurs, or it doesn't occur in the segment. And we have an entropy function thatlooks like what you see on the slide. Now suppose we know eats is present, so now we know the value of anotherrandom variable that denotes eats. Now, that would change allthese probabilities to conditional probabilities. Where we look at the presence orabsence of meat, given that we know eatsoccurred in the context. So as a result, if we replace these probabilitieswith their corresponding conditional probabilities in the entropy function,we'll get the conditional entropy. So this equation now here would be the conditional entropy. Conditional on the presence of eats. So, you can see this is essentiallythe same entropy function as you have seen before, except that allthe probabilities now have a condition. And this then tells usthe entropy of meat, after we have known eatsoccurring in the segment. And of course, we can also definethis conditional entropy for the scenario where we don't see eats. So if we know it did not occur inthe segment, then this entry condition of entropy would capture the instancesof meat in that condition. So now,putting different scenarios together, we have the completed definitionof conditional entropy as follows. Basically, we're going to consider bothscenarios of the value of eats zero, one, and this gives us a probabilitythat eats is equal to zero or one. Basically, whether eats is present orabsent. And this of course, is the conditional entropy ofmeat in that particular scenario. So if you expanded this entropy, then you have the following equation. Where you see the involvement ofthose conditional probabilities. Now in general, for any discreterandom variables x and y, we have the conditional entropy is no largerthan the entropy of the variable x. So basically, this is upper bound forthe conditional entropy. That means by knowing moreinformation about the segment, we want to be able toincrease uncertainty. We can only reduce uncertainty. And that intuitively makes sensebecause as we know more information, it should always helpus make the prediction. And cannot hurtthe prediction in any case. Now, what's interesting here is also tothink about what's the minimum possible value of this conditional entropy? Now, we know that the maximumvalue is the entropy of X. But what about the minimum,so what do you think? I hope you can reach the conclusion thatthe minimum possible value, would be zero. And it will be interesting to think aboutunder what situation will achieve this. So, let's see how we can use conditionalentropy to capture syntagmatic relation. Now of course,this conditional entropy gives us directly one way to measurethe association of two words. Because it tells us to what extent,we can predict the one word given that we know the presence orabsence of another word. Now before we look at the intuitionof conditional entropy in capturing syntagmatic relations, it's useful tothink of a very special case, listed here. That is, the conditional entropyof the word given itself. So here, we listed this conditionalentropy in the middle. So, it's here. So, what is the value of this? Now, this means we know wherethe meat occurs in the sentence. And we hope to predict whetherthe meat occurs in the sentence. And of course, this is 0 becausethere's no incident anymore. Once we know whether the wordoccurs in the segment, we'll already know the answerof the prediction. So this is zero. And that's also when this conditionalentropy reaches the minimum. So now, let's look at some other cases. So this is a case of knowing the andtrying to predict the meat. And this is a case of knowing eats andtrying to predict the meat. Which one do you think is smaller? No doubt smaller entropy means easier forprediction. Which one do you think is higher? Which one is not smaller? Well, if you at the uncertainty,then in the first case, the doesn't really tellus much about the meat. So knowing the occurrence of the doesn'treally help us reduce entropy that much. So it stays fairly close tothe original entropy of meat. Whereas in the case of eats,eats is related to meat. So knowing presence of eats orabsence of eats, would help us predict whether meat occurs. So it can help us reduce entropy of meat. So we should expect the sigma term, namelythis one, to have a smaller entropy. And that means there is a strongerassociation between meat and eats. So we now also know whenthis w is the same as this meat, then the conditional entropywould reach its minimum, which is 0. And for what kind of wordswould either reach its maximum? Well, that's when this stuffis not really related to meat. And like the for example,it would be very close to the maximum, which is the entropy of meat itself. So this suggests that when youuse conditional entropy for mining syntagmatic relations,the hours would look as follows. For each word W1, we're going toenumerate the overall other words W2. And then, we can computethe conditional entropy of W1 given W2. We thought all the candidate was inascending order of the conditional entropy because we're out of favor,a world that has a small entropy. Meaning that it helps us predictthe time of the word W1. And then, we're going to take the top ringof the candidate words as words that have potential syntagmatic relations with W1. Note that we need to usea threshold to find these words. The stresser can be the numberof top candidates take, or absolute value forthe conditional entropy. Now, this would allow us to mine the most strongly correlated words witha particular word, W1 here. But, this algorithm does nothelp us mine the strongest that K syntagmatical relationsfrom an entire collection. Because in order to do that, we have toensure that these conditional entropies are comparable across different words. In this case of discoveringthe mathematical relations for a targeted word like W1, we only needto compare the conditional entropies for W1, given different words. And in this case, they are comparable. All right. So, the conditional entropy of W1, givenW2, and the conditional entropy of W1, given W3 are comparable. They all measure how hardit is to predict the W1. But, if we think about the two pairs, where we share W2 in the same condition,and we try to predict the W1 and W3. Then, the conditional entropiesare actually not comparable. You can think of about this question. Why? So why are they not comfortable? Well, that was because theyhave a different outer bounds. Right?So those outer bounds are precisely the entropy of W1 and the entropy of W3. And they have different upper bounds. So we cannot reallycompare them in this way. So how do we address this problem? Well later, we'll discuss, we can usemutual information to solve this problem. [MUSIC]"
cs-410,8,3,"[SOUND]. This lecture is about the syntagmaticrelation discovery and mutual information. In this lecture we are going to continuediscussing syntagmatic relation discovery. In particular,we are going to talk about another the concept in the information series,we called it mutual information and how it can be used to discoversyntagmatic relations. Before we talked about the problemof conditional entropy and that is the conditional entropycomputed different pairs of words. It is not really comparable, sothat makes it harder with this cover, strong synagmatic relationsglobally from corpus. So now we are going to introduce mutualinformation, which is another concept in the information seriesthat allows us to, sometimes, normalize the conditional entropy to makeit more comparable across different pairs. In particular, mutual informationin order to find I(X:Y), matches the entropy reductionof X obtained from knowing Y. More specifically the question weare interested in here is how much of an entropy of X canwe obtain by knowing Y. So mathematically it can bedefined as the difference between the original entropy of X, andthe condition of Y of X given Y. And you might see,as you can see here it can also be defined as reduction of entropy ofY because of knowing X. Now normally the two conditionalinterface H of X given Y and the entropy of Y given X are not equal,but interestingly, the reduction of entropy by knowingone of them, is actually equal. So, this quantity is called a MutualInformation in order to buy I here. And this function has some interestingproperties, first it is also non-negative. This is easy to understand becausethe original entropy is always not going to be lower than the possibilityreduced conditional entropy. In other words, the conditional entropywill never exceed the original entropy. Knowing some information canalways help us potentially, but will not hurt us in predicting x. The signal property is that itis symmetric like additional entropy is not symmetrical,mutual information is, and the third property is that Itreaches its minimum, zero, if and only if the two random variablesare completely independent. That means knowing one of them does nottell us anything about the other and this last property can be verified bysimply looking at the equation above and it reaches 0 if andonly the conditional entropy of X [INAUDIBLE] Y is exactly the sameas original entropy of X. So that means knowing why it did nothelp at all and that is when X and a Y are completely independent. Now when we fix X to rank differentYs using conditional entropy would give the same order asranking based on mutual information because in the function here,H(X) is fixed because X is fixed. So ranking based on mutual entropy isexactly the same as ranking based on the conditional entropy of X given Y, but the mutual information allows us tocompare different pairs of x and y. So, that is why mutual information ismore general and in general, more useful. So, let us examine the intuitionof using mutual information for Syntagmatical Relation Mining. Now, the question we ask forcingthat relation mining is, whenever ""eats"" occurs,what other words also tend to occur? So this question can be framed asa mutual information question, that is, which words have high mutualinformation was eats, so computer the missing informationbetween eats and other words. And if we do that, and it is basicallya base on the same as conditional we will see that words thatare strongly associated with eats, will have a high point. Whereas words that are not relatedwill have lower mutual information. For this, I will give some example here. The mutual information between ""eats"" and""meats"", which is the same as between ""meats"" and""eats,"" because the information is symmetrical is expected to be higher thanthe mutual information between eats and the, because knowing the does notreally help us as a predictor. It is similar, andknowing eats does not help us predicting, the as well. And you also can easilysee that the mutual information between a word anditself is the largest, which is equal tothe entropy of this word and so, because in this case the reduction is maximum because knowing one allowsus to predict the other completely. So the conditional entropy is zero, therefore the mutual informationreaches its maximum. It is going to be larger, then are equalto the machine volume eats in other words. In other words picking any other word and the computer picking between eats andthat word. You will not get any information largerthe computation from eats and itself. So now let us look at how tocompute the mute information. Now in order to do that, we often use a different form of mutualinformation, and we can mathematically rewrite the mutual informationinto the form shown on this slide. Where we essentially seea formula that computes what is called a KL-divergence or divergence. This is another termin information theory. It measures the divergencebetween two distributions. Now, if you look at the formula,it is also sum over many combinations of different values of the two randomvariables but inside the sum, mainly we are doing a comparisonbetween two joint distributions. The numerator has the joint, actual observed the joint distributionof the two random variables. The bottom part or the denominator can be interpreted as the expected jointdistribution of the two random variables, if they were independent because whentwo random variables are independent, they are joined distribution is equal tothe product of the two probabilities. So this comparison will tell us whetherthe two variables are indeed independent. If they are indeed independent then wewould expect that the two are the same, but if the numerator is differentfrom the denominator, that would mean the two variables are not independent andthat helps measure the association. The sum is simply to take intoconsideration of all of the combinations of the values of thesetwo random variables. In our case, each random variablecan choose one of the two values, zero or one, sowe have four combinations here. If we look at this form of mutualinformation, it shows that the mutual information matches the divergenceof the actual joint distribution from the expected distributionunder the independence assumption. The larger this divergence is, the higherthe mutual information would be. So now let us further look at whatare exactly the probabilities, involved in this formulaof mutual information. And here, this is all the probabilitiesinvolve, and it is easy for you to verify that. Basically, we have first to[INAUDIBLE] probabilities corresponding to the presence orabsence of each word. So, for w1,we have two probabilities shown here. They should sum to one, because a wordcan either be present or absent. In the segment, and similarly for the second word, we also have twoprobabilities representing presence or absences of this word, andthere is some to y as well. And finally, we have a lot ofjoined probabilities that represent the scenarios of co-occurrences ofthe two words, and they are shown here. And they sum to one because the twowords can only have these four possible scenarios. Either they both occur, so in that case both variables will havea value of one, or one of them occurs. There are two scenarios. In these two cases one of the randomvariables will be equal to one and the other will be zero and finally we havethe scenario when none of them occurs. This is when the two variablestaking a value of zero. So these are the probabilities involvedin the calculation of mutual information, over here. Once we know how to calculatethese probabilities, we can easily calculatethe mutual information. It is also interesting to know thatthere are actually some relations or constraint among these probabilities,and we already saw two of them, right? So in the previous slide, that you have seen thatthe marginal probabilities of these words sum to one andwe also have seen this constraint, that says the two words have thesefour scenarios of co-occurrency, but we also have some additionalconstraints listed in the bottom. For example, this one means if we add up the probabilities that we observethe two words occur together and the probabilities when the first wordoccurs and the second word does not occur. We get exactly the probabilitythat the first word is observed. In other words, when the word is observed. When the first word is observed, and there are only two scenarios, depending onwhether the second word is also observed. So, this probability captures the firstscenario when the second word actually is also observed, and this captures the second scenariowhen the second word is not observed. So, we only see the first word, and it is easy to see the other equationsalso follow the same reasoning. Now these equations allow us tocompute some probabilities based on other probabilities, andthis can simplify the computation. So more specifically,if we know the probability that a word is present, like in this case,so if we know this, and if we know the probability ofthe presence of the second word, then we can easily computethe absence probability, right? It is very easy to use thisequation to do that, and so we take care of the computation ofthese probabilities of presence and absence of each word. Now let's look atthe [INAUDIBLE] distribution. Let us assume that we also have available the probability thatthey occurred together. Now it is easy to see that we canactually compute all the rest of these probabilities based on these. Specifically forexample using this equation we can compute the probability that the first wordoccurred and the second word did not, because we know these probabilities inthe boxes, and similarly using this equation we can compute the probabilitythat we observe only the second word. Word. And then finally,this probability can be calculated by using this equation becausenow this is known, and this is also known, andthis is already known, right. So this can be easier to calculate. So now this can be calculated. So this slide shows that we onlyneed to know how to compute these three probabilitiesthat are shown in the boxes, naming the presence of each word and theco-occurence of both words, in a segment. [MUSIC]"
cs-410,8,4,"[SOUND] In general, we can use the empirical count of events in the observed datato estimate the probabilities. And a commonly used technique iscalled a maximum likelihood estimate, where we simply normalizethe observe accounts. So if we do that, we can see, we cancompute these probabilities as follows. For estimating the probability thatwe see a water current in a segment, we simply normalize the count ofsegments that contain this word. So let's first takea look at the data here. On the right side, you see a list of some,hypothesizes the data. These are segments. And in some segments you see both wordsoccur, they are indicated as ones for both columns. In some other cases only one will occur,so only that column has one and the other column has zero. And in all, of course, in some othercases none of the words occur, so they are both zeros. And for estimating these probabilities, wesimply need to collect the three counts. So the three counts are first,the count of W1. And that's the total number ofsegments that contain word W1. It's just as the ones in the column of W1. We can count how manyones we have seen there. The segment count is for word 2, and wejust count the ones in the second column. And these will give us the totalnumber of segments that contain W2. The third count is when both words occur. So this time, we're going to countthe sentence where both columns have ones. And then, so this would give usthe total number of segments where we have seen both W1 and W2. Once we have these counts,we can just normalize these counts by N, which is the total number of segments, and this will give us the probabilities thatwe need to compute original information. Now, there is a small problem,when we have zero counts sometimes. And in this case, we don't want a zeroprobability because our data may be a small sample and in general, we wouldbelieve that it's potentially possible for a [INAUDIBLE] to avoid any context. So, to address this problem,we can use a technique called smoothing. And that's basically to add somesmall constant to these counts, and so that we don't getthe zero probability in any case. Now, the best way to understand smoothingis imagine that we actually observed more data than we actually have, because we'llpretend we observed some pseudo-segments. I illustrated on the top,on the right side on the slide. And these pseudo-segments wouldcontribute additional counts of these words sothat no event will have zero probability. Now, in particular we introducethe four pseudo-segments. Each is weighted at one quarter. And these represent the four differentcombinations of occurrences of this word. So now each event,each combination will have at least one count or at least a non-zerocount from this pseudo-segment. So, in the actual segmentsthat we'll observe, it's okay if we haven't observedall of the combinations. So more specifically, you can seethe 0.5 here after it comes from the two ones in the two pseudo-segments,because each is weighted at one quarter. We add them up, we get 0.5. And similar to this,0.05 comes from one single pseudo-segment that indicatesthe two words occur together. And of course in the denominator we addthe total number of pseudo-segments that we add, in this case,we added a four pseudo-segments. Each is weighed at one quarter sothe total of the sum is, after the one. So, that's why in the denominatoryou'll see a one there. So, this basically concludesthe discussion of how to compute a these four syntagmatic relation discoveries. Now, so to summarize,syntagmatic relation can generally be discovered by measuring correlationsbetween occurrences of two words. We've introduced the threeconcepts from information theory. Entropy, which measures the uncertaintyof a random variable X. Conditional entropy, which measuresthe entropy of X given we know Y. And mutual information of X and Y,which matches the entropy reduction of X due to knowing Y, orentropy reduction of Y due to knowing X. They are the same. So these three concepts are actually veryuseful for other applications as well. That's why we spent some timeto explain this in detail. But in particular,they are also very useful for discovering syntagmatic relations. In particular,mutual information is a principal way for discovering such a relation. It allows us to have valuescomputed on different pairs of words that are comparable andso we can rank these pairs and discover the strongest syntagmaticfrom a collection of documents. Now, note that there is some relationbetween syntagmatic relation discovery and [INAUDIBLE] relation discovery. So we already discussed the possibilityof using BM25 to achieve waiting for terms in the context to potentiallyalso suggest the candidates that have syntagmatic relationswith the candidate word. But here, once we use mutual informationto discover syntagmatic relations, we can also represent the context withthis mutual information as weights. So this would give usanother way to represent the context of a word, like a cat. And if we do the same for all the words,then we can cluster these words or compare the similarity between thesewords based on their context similarity. So this provides yetanother way to do term weighting for paradigmatic relation discovery. And so to summarize this whole partabout word association mining. We introduce two basic associations,called a paradigmatic and a syntagmatic relations. These are fairly general, they applyto any items in any language, so the units don't have to be words,they can be phrases or entities. We introduced multiple statisticalapproaches for discovering them, mainly showing that purestatistical approaches are visible, are variable fordiscovering both kind of relations. And they can be combined toperform joint analysis, as well. These approaches can be appliedto any text with no human effort, mostly because they are basedon counting of words, yet they can actually discoverinteresting relations of words. We can also use different ways withdefining context and segment, and this would lead us to some interestingvariations of applications. For example, the context can be verynarrow like a few words, around a word, or a sentence, or maybe paragraphs,as using differing contexts would allows to discover different flavorsof paradigmatical relations. And similarly,counting co-occurrences using let's say, visual information to discoversyntagmatical relations. We also have to define the segment, andthe segment can be defined as a narrow text window or a longer text article. And this would give us differentkinds of associations. These discovery associations cansupport many other applications, in both information retrieval andtext and data mining. So here are some recommended readings,if you want to know more about the topic. The first is a book witha chapter on collocations, which is quite relevant tothe topic of these lectures. The second is an articleabout using various statistical measures todiscover lexical atoms. Those are phrases thatare non-compositional. For example,hot dog is not really a dog that's hot, blue chip is not a chip that's blue. And the paper has a discussion about sometechniques for discovering such phrases. The third one is a new paper on a unifiedway to discover both paradigmatical relations and a syntagmatical relations,using random works on word graphs. [SOUND]"
cs-410,8,5,"[SOUND]>> This lecture is about topic mining andanalysis. We're going to talk about itsmotivation and task definition. In this lecture we're going to talkabout different kind of mining task. As you see on this road map,we have just covered mining knowledge about language,namely discovery of word associations such as paradigmatic andrelations and syntagmatic relations. Now, starting from this lecture, we'regoing to talk about mining another kind of knowledge, which is content mining, and trying to discover knowledge aboutthe main topics in the text. And we call that topic mining andanalysis. In this lecture, we're going to talk aboutits motivation and the task definition. So first of all,let's look at the concept of topic. So topic is something that weall understand, I think, but it's actually not thateasy to formally define. Roughly speaking, topic is the mainidea discussed in text data. And you can think of this as a theme orsubject of a discussion or conversation. It can also have different granularities. For example,we can talk about the topic of a sentence. A topic of article,aa topic of paragraph or the topic of all the research articlesin the research library, right, so different grand narratives of topicsobviously have different applications. Indeed, there are many applications thatrequire discovery of topics in text, and they're analyzed then. Here are some examples. For example, we might be interestedin knowing about what are Twitter users are talking about today? Are they talking about NBA sports, or are they talking about someinternational events, etc.? Or we are interested inknowing about research topics. For example, one might be interested inknowing what are the current research topics in data mining, and how are theydifferent from those five years ago? Now this involves discovery of topicsin data mining literatures and also we want to discover topics intoday's literature and those in the past. And then we can make a comparison. We might also be also interested inknowing what do people like about some products like the iPhone 6,and what do they dislike? And this involves discoveringtopics in positive opinions about iPhone 6 andalso negative reviews about it. Or perhaps we're interested in knowingwhat are the major topics debated in 2012 presidential election? And all these have to do with discoveringtopics in text and analyzing them, and we're going to talk about a lotof techniques for doing this. In general we can view a topic assome knowledge about the world. So from text data we expect todiscover a number of topics, and then these topics generally providea description about the world. And it tells us something about the world. About a product, about a person etc. Now when we have some non-text data, then we can have more context foranalyzing the topics. For example, we might know the timeassociated with the text data, or locations where the textdata were produced, or the authors of the text, orthe sources of the text, etc. All such meta data, or context variables can be associatedwith the topics that we discover, and then we can use these context variableshelp us analyze patterns of topics. For example, looking at topics over time,we would be able to discover whether there's a trending topic, orsome topics might be fading away. Soon you are looking at topicsin different locations. We might know some insights aboutpeople's opinions in different locations. So that's why miningtopics is very important. Now, let's look at the tasksof topic mining and analysis. In general, it would involve firstdiscovering a lot of topics, in this case, k topics. And then we also would like to know, whichtopics are covered in which documents, to what extent. So for example, in document one, wemight see that Topic 1 is covered a lot, Topic 2 andTopic k are covered with a small portion. And other topics,perhaps, are not covered. Document two, on the other hand,covered Topic 2 very well, but it did not cover Topic 1 at all, and it also covers Topic k to some extent,etc., right? So now you can see thereare generally two different tasks, or sub-tasks, the first is to discover ktopics from a collection of text laid out. What are these k topics? Okay, major topics in the text they are. The second task is to figure outwhich documents cover which topics to what extent. So more formally,we can define the problem as follows. First, we have, as input,a collection of N text documents. Here we can denote the textcollection as C, and denote text article as d i. And, we generally also need to haveas input the number of topics, k. But there may be techniques that canautomatically suggest a number of topics. But in the techniques that we willdiscuss, which are also the most useful techniques, we often need tospecify a number of topics. Now the output would then be the ktopics that we would like to discover, in order as theta subone through theta sub k. Also we want to generate the coverage oftopics in each document of d sub i And this is denoted by pi sub i j. And pi sub ij is the probabilityof document d sub i covering topic theta sub j. So obviously for each document, we havea set of such values to indicate to what extent the document covers,each topic. And we can assume that theseprobabilities sum to one. Because a document won't be able to cover other topics outside of the topicsthat we discussed, that we discovered. So now, the question is, how do we definetheta sub i, how do we define the topic? Now this problem has notbeen completely defined until we define what is exactly theta. So in the next few lectures, we're going to talk aboutdifferent ways to define theta. [MUSIC]"
cs-410,8,6,"[MUSIC] This lecture is about topic mining andanalysis. We're going to talk aboutusing a term as topic. This is a slide that you haveseen in a earlier lecture where we define the task oftopic mining and analysis. We also raised the question, how dowe exactly define the topic of theta? So in this lecture, we're going tooffer one way to define it, and that's our initial idea. Our idea here is defininga topic simply as a term. A term can be a word or a phrase. And in general,we can use these terms to describe topics. So our first thought is justto define a topic as one term. For example, we might have termslike sports, travel, or science, as you see here. Now if we define a topic in this way, we can then analyze the coverageof such topics in each document. Here for example, we might want to discover to whatextent document one covers sports. And we found that 30% of the contentof document one is about sports. And 12% is about the travel, etc. We might also discover documenttwo does not cover sports at all. So the coverage is zero, etc. So now, of course,as we discussed in the task definition for topic mining and analysis,we have two tasks. One is to discover the topics. And the second is to analyze coverage. So let's first thinkabout how we can discover topics if we representeach topic by a term. So that means we need to mine ktopical terms from a collection. Now there are, of course,many different ways of doing that. And we're going to talk abouta natural way of doing that, which is also likely effective. So first of all, we're going to parse the text data inthe collection to obtain candidate terms. Here candidate terms can be words orphrases. Let's say the simplest solution isto just take each word as a term. These words then become candidate topics. Then we're going to design a scoringfunction to match how good each term is as a topic. So how can we design such a function? Well there are many thingsthat we can consider. For example, we can use pure statisticsto design such a scoring function. Intuitively, we would like tofavor representative terms, meaning terms that can representa lot of content in the collection. So that would mean we wantto favor a frequent term. However, if we simply use the frequencyto design the scoring function, then the highest scored termswould be general terms or functional terms like the, etc. Those terms occur very frequently English. So we also want to avoid havingsuch words on the top so we want to penalize such words. But in general, we would like to favorterms that are fairly frequent but not so frequent. So a particular approach could be basedon TF-IDF weighting from retrieval. And TF stands for term frequency. IDF stands for inverse document frequency. We talked about some of these ideas in the lectures aboutthe discovery of word associations. So these are statistical methods, meaning that the function isdefined mostly based on statistics. So the scoring functionwould be very general. It can be applied to any language,any text. But when we apply such a approachto a particular problem, we might also be able to leveragesome domain-specific heuristics. For example, in news we might favortitle words actually general. We might want to favor titlewords because the authors tend to use the title to describethe topic of an article. If we're dealing with tweets,we could also favor hashtags, which are invented to denote topics. So naturally, hashtags can be goodcandidates for representing topics. Anyway, after we have this designscoring function, then we can discover the k topical terms by simply pickingk terms with the highest scores. Now, of course, we might encounter situation where thehighest scored terms are all very similar. They're semantically similar, orclosely related, or even synonyms. So that's not desirable. So we also want to have coverage overall the content in the collection. So we would like to remove redundancy. And one way to do that isto do a greedy algorithm, which is sometimes called a maximalmarginal relevance ranking. Basically, the idea is to go downthe list based on our scoring function and gradually take termsto collect the k topical terms. The first term, of course, will be picked. When we pick the next term, we'regoing to look at what terms have already been picked and try to avoidpicking a term that's too similar. So while we are consideringthe ranking of a term in the list, we are also consideringthe redundancy of the candidate term with respect to the termsthat we already picked. And with some thresholding,then we can get a balance of the redundancy removal andalso high score of a term. Okay, soafter this that will get k topical terms. And those can be regarded as the topicsthat we discovered from the connection. Next, let's think about how we're goingto compute the topic coverage pi sub ij. So looking at this picture,we have sports, travel and science and these topics. And now suppose you are give a document. How should we pick out coverageof each topic in the document? Well, one approach can be to simplycount occurrences of these terms. So for example, sports might have occurredfour times in this this document and travel occurred twice, etc. And then we can just normalize thesecounts as our estimate of the coverage probability for each topic. So in general, the formula wouldbe to collect the counts of all the terms that represent the topics. And then simply normalize them sothat the coverage of each topic in the document would add to one. This forms a distribution of the topicsfor the document to characterize coverage of different topics in the document. Now, as always,when we think about idea for solving problem, we have to askthe question, how good is this one? Or is this the best wayof solving problem? So now let's examine this approach. In general,we have to do some empirical evaluation by using actual data sets andto see how well it works. Well, in this case let's takea look at a simple example here. And we have a text document that'sabout a NBA basketball game. So in terms of the content,it's about sports. But if we simply count thesewords that represent our topics, we will find that the word sportsactually did not occur in the article, even though the contentis about the sports. So the count of sports is zero. That means the coverage of sportswould be estimated as zero. Now of course,the term science also did not occur in the document andit's estimate is also zero. And that's okay. But sports certainly is not okay becausewe know the content is about sports. So this estimate has problem. What's worse, the term travelactually occurred in the document. So when we estimate the coverageof the topic travel, we have got a non-zero count. So its estimated coveragewill be non-zero. So this obviously is also not desirable. So this simple example illustratessome problems of this approach. First, when we count whatwords belong to to the topic, we also need to consider related words. We can't simply just countthe topic word sports. In this case, it did not occur at all. But there are many related wordslike basketball, game, etc. So we need to countthe related words also. The second problem is that a wordlike star can be actually ambiguous. So here it probably meansa basketball star, but we can imagine it might alsomean a star on the sky. So in that case, the star might actuallysuggest, perhaps, a topic of science. So we need to deal with that as well. Finally, a main restriction of thisapproach is that we have only one term to describe the topic, so it cannotreally describe complicated topics. For example, a very specializedtopic in sports would be harder to describe by using just a word orone phrase. We need to use more words. So this example illustratessome general problems with this approach of treating a term as topic. First, it lacks expressive power. Meaning that it can only representthe simple general topics, but it cannot represent the complicated topicsthat might require more words to describe. Second, it's incompletein vocabulary coverage, meaning that the topic itselfis only represented as one term. It does not suggest what otherterms are related to the topic. Even if we're talking about sports,there are many terms that are related. So it does not allow us to easilycount related terms to order, conversion to coverage of this topic. Finally, there is this problemof word sense disintegration. A topical term orrelated term can be ambiguous. For example,basketball star versus star in the sky. So in the next lecture,we're going to talk about how to solvethe problem with of a topic. [MUSIC]"
cs-410,8,7,"This lecture is about Probabilistic TopicModels for topic mining and analysis. In this lecture, we're going to continue talkingabout the topic mining and analysis. We're going to introduceprobabilistic topic models. So this is a slide thatyou have seen earlier, where we discussed the problemswith using a term as a topic. So, to solve these problemsintuitively we need to use more words to describe the topic. And this will address the problemof lack of expressive power. When we have more words that wecan use to describe the topic, that we can describe complicated topics. To address the second problem weneed to introduce weights on words. This is what allows you to distinguishsubtle differences in topics, and to introduce semanticallyrelated words in a fuzzy manner. Finally, to solve the problem ofword ambiguity, we need to split ambiguous word, sothat we can disambiguate its topic. It turns out that all these can be doneby using a probabilistic topic model. And that's why we're going to spend a lotof lectures to talk about this topic. So the basic idea here is that, improve the replantation oftopic as one distribution. So what you see now isthe older replantation. Where we replanted each topic, it was justone word, or one term, or one phrase. But now we're going to use a worddistribution to describe the topic. So here you see that for sports. We're going to usethe word distribution over theoretical speaking allthe words in our vocabulary. So for example, the highprobability words here are sports, game, basketball,football, play, star, etc. These are sports related terms. And of course it would also givea non-zero probability to some other word like Trouble which might berelated to sports in general, not so much related to topic. In general we can imagine a nonzero probability for all the words. And some words that are not read andwould have very, very small probabilities. And these probabilities will sum to one. So that it forms a distributionof all the words. Now intuitively, this distributionrepresents a topic in that if we assemble words from the distribution, we tendedto see words that are ready to dispose. You can also see, as a very special case,if the probability of the mass is concentrated in entirely onjust one word, it's sports. And this basically degeneratesto the symbol foundation of a topic was just one word. But as a distribution,this topic of representation can, in general,involve many words to describe a topic and can model several differencesin semantics of a topic. Similarly we can model Travel and Sciencewith their respective distributions. In the distribution for Travel we see topwords like attraction, trip, flight etc. Whereas in Science we see scientist,spaceship, telescope, or genomics, and, you know,science related terms. Now that doesn't mean sports related terms will necessarily have zeroprobabilities for science. In general we can imagine all of thesewords we have now zero probabilities. It's just that for a particulartopic in some words we have very, very small probabilities. Now you can also see there are somewords that are shared by these topics. When I say shared it just means evenwith some probability threshold, you can still see one wordoccurring much more topics. In this case I mark them in black. So you can see travel, for example,occurred in all the three topics here, but with different probabilities. It has the highest probability forthe Travel topic, 0.05. But with much smaller probabilities forSports and Science, which makes sense. And similarly, you can see a Staralso occurred in Sports and Science with reasonablyhigh probabilities. Because they might be actuallyrelated to the two topics. So with this replantation it addresses thethree problems that I mentioned earlier. First, it now uses multiplewords to describe a topic. So it allows us to describea fairly complicated topics. Second, it assigns weights to terms. So now we can model severaldifferences of semantics. And you can bring in relatedwords together to model a topic. Third, because we have probabilities forthe same word in different topics, we can disintegrate the sense of word. In the text to decodeit's underlying topic, to address all these three problems withthis new way of representing a topic. So now of course our problem definitionhas been refined just slightly. The slight is very similar to whatyou've seen before except we have added refinement for what our topic is. Now each topic is word distribution,and for each word distribution we know that all the probabilities should sum toone with all the words in the vocabulary. So you see a constraint here. And we still have another constrainton the topic coverage, namely pis. So all the Pi sub ij's must sum to one forthe same document. So how do we solve this problem? Well, let's look at this problemas a computation problem. So we clearly specify it's input and output andillustrate it here on this side. Input of course is our text data. C is our collection but we also generallyassume we know the number of topics, k. Or we hypothesize a number andthen try to bind k topics, even though we don't know the exacttopics that exist in the collection. And V is the vocabulary that hasa set of words that determines what units would be treated asthe basic units for analysis. In most cases we'll use wordsas the basis for analysis. And that means each word is a unique. Now the output would consist of as firsta set of topics represented by theta I's. Each theta I is a word distribution. And we also want to know the coverageof topics in each document. So that's. That the same pi ijsthat we have seen before. So given a set of text data we wouldlike compute all these distributions and all these coverages as youhave seen on this slide. Now of course there may be manydifferent ways of solving this problem. In theory, you can write the [INAUDIBLE]program to solve this problem, but here we're going to introduce a general way of solving thisproblem called a generative model. And this is, in fact,a very general idea and it's a principle way of using statisticalmodeling to solve text mining problems. And here I dimmed the picturethat you have seen before in order to show the generation process. So the idea of this approach is actuallyto first design a model for our data. So we design a probabilistic modelto model how the data are generated. Of course,this is based on our assumption. The actual data aren'tnecessarily generating this way. So that gave us a probabilitydistribution of the data that you are seeing on this slide. Given a particular model andparameters that are denoted by lambda. So this template of actually consists of all the parameters thatwe're interested in. And these parameters in generalwill control the behavior of the probability risk model. Meaning that if you set theseparameters with different values and it will give some data pointshigher probabilities than others. Now in this case of course,for our text mining problem or more precisely topic mining problemwe have the following plans. First of all we have theta i's whichis a word distribution snd then we have a set of pis for each document. And since we have n documents, so we haven sets of pis, and each set the pi up. The pi values will sum to one. So this is to say that wefirst would pretend we already have these word distributions andthe coverage numbers. And then we can see how we can generatedata by using such distributions. So how do we model the data in this way? And we assume that the dataare actual symbols drawn from such a model thatdepends on these parameters. Now one interesting question here is to think about how manyparameters are there in total? Now obviously we can already seen multiplied by K parameters. For pi's. We also see k theta i's. But each theta i is actually a setof probability values, right? It's a distribution of words. So I leave this as an exercise for you to figure out exactly howmany parameters there are here. Now once we set up the model thenwe can fit the model to our data. Meaning that we canestimate the parameters or infer the parameters based on the data. In other words we would like toadjust these parameter values. Until we give our data setthe maximum probability. I just said,depending on the parameter values, some data points will have higherprobabilities than others. What we're interested in, here, is what parameter values will giveour data set the highest probability? So I also illustrate the problemwith a picture that you see here. On the X axis I just illustrate lambda,the parameters, as a one dimensional variable. It's oversimplification, obviously,but it suffices to show the idea. And the Y axis shows the probabilityof the data, observe. This probability obviously dependson this setting of lambda. So that's why it varies as youchange the value of lambda. What we're interested hereis to find the lambda star. That would maximize the probabilityof the observed data. So this would be, then,our estimate of the parameters. And these parameters, note that are precisely what wehoped to discover from text data. So we'd treat these parametersas actually the outcome or the output of the data mining algorithm. So this is the general idea of using a generative model for text mining. First, we design a model withsome parameter values to fit the data as well as we can. After we have fit the data,we will recover some parameter value. We will use the specificparameter value And those would be the outputof the algorithm. And we'll treat those as actuallythe discovered knowledge from text data. By varying the model of course wecan discover different knowledge. So to summarize, we introduceda new way of representing topic, namely representing as word distributionand this has the advantage of using multiple words to describe a complicatedtopic.It also allow us to assign weights on words so we have more thanseveral variations of semantics. We talked about the task of topic mining,and answers. When we define a topic as distribution. So the importer is a clashing of textarticles and a number of topics and a vocabulary set andthe output is a set of topics. Each is a word distribution and also the coverage of allthe topics in each document. And these are formally representedby theta i's and pi i's. And we have two constraints here forthese parameters. The first is the constraintson the worded distributions. In each worded distributionthe probability of all the words must sum to 1,all the words in the vocabulary. The second constraint is onthe topic coverage in each document. A document is not allowed to recovera topic outside of the set of topics that we are discovering. So, the coverage of each of these ktopics would sum to one for a document. We also introduce a general idea of usinga generative model for text mining. And the idea here is, first we're designa model to model the generation of data. We simply assume that theyare generative in this way. And inside the model we embed someparameters that we're interested in denoted by lambda. And then we can infer the mostlikely parameter values lambda star, given a particular data set. And we can then take the lambda star asknowledge discovered from the text for our problem. And we can adjustthe design of the model and the parameters to discover variouskinds of knowledge from text. As you will see laterin the other lectures. [MUSIC]"
cs-410,8,8,"[SOUND]>> This lecture is about the Overviewof Statistical Language Models, which cover propermodels as special cases. In this lecture we're going to give a overview of Statical Language Models. These models are general models that cover probabilistic topic modelsas a special cases. So first off,what is a Statistical Language Model? A Statistical Language Model isbasically a probability distribution over word sequences. So, for example,we might have a distribution that gives, today is Wednesday a probability of .001. It might give today Wednesday is, which is a non-grammatical sentence, a very,very small probability as shown here. And similarly another sentence, the eigenvalue is positive mightget the probability of .00001. So as you can see such a distributionclearly is Context Dependent. It depends on the Context of Discussion. Some Word Sequences might have higherprobabilities than others but the same Sequence of Words might have differentprobability in different context. And so this suggests that such adistribution can actually categorize topic such a model can also be regardedas Probabilistic Mechanism for generating text. And that just means we can view textdata as data observed from such a model. For this reason,we call such a model as Generating Model. So, now given a model we can thenassemble sequences of words. So, for example, based on the distributionthat I have shown here on this slide, when matter it say assemblea sequence like today is Wednesday because it has a relativehigh probability. We might often get such a sequence. We might also get the itemvalue as positive sometimes with a smaller probability andvery, very occasionally we might get today is Wednesday becauseit's probability is so small. So in general, in order to categorize sucha distribution we must specify probability values forall these different sequences of words. Obviously, it's impossibleto specify that because it's impossible to enumerate all ofthe possible sequences of words. So in practice, we will have tosimplify the model in some way. So, the simplest language model iscalled the Unigram Language Model. In such a case, it was simply a the text is generated by generatingeach word independently. But in general, the words maynot be generated independently. But after we make this assumption, we cansignificantly simplify the language more. Basically, now the probability ofa sequence of words, w1 through wn, will be just the product ofthe probability of each word. So for such a model, we have as many parameters asthe number of words in our vocabulary. So here we assume we have n words,so we have n probabilities. One for each word. And then some to 1. So, now we assume thatour text is a sample drawn according to this word distribution. That just means,we're going to draw a word each time and then eventually we'll get a text. So for example, now again, we can try to assemble wordsaccording to a distribution. We might get Wednesday often ortoday often. And some other words like eigenvaluemight have a small probability, etcetera. But with this, we actually canalso compute the probability of every sequence, even though our modelonly specify the probabilities of words. And this is because of the independence. So specifically, we can computethe probability of today is Wednesday. Because it's just a productof the probability of today, the probability of is, andprobability of Wednesday. For example,I show some fake numbers here and when you multiply these numbers together you getthe probability that today's Wednesday. So as you can see, with N probabilities,one for each word, we actually can characterize the probability situationover all kinds of sequences of words. And so, this is a very simple model. Ignore the word order. So it may not be, in fact, in someproblems, such as for speech recognition, where you may care aboutthe order of words. But it turns out to bequite sufficient for many tasks that involve topic analysis. And that's also whatwe're interested in here. So when we have a model, we generally havetwo problems that we can think about. One is, given a model, how likely are weto observe a certain kind of data points? That is,we are interested in the Sampling Process. The other is the Estimation Process. And that, is to think ofthe parameters of a model given, some observe the data and we'regoing to talk about that in a moment. Let's first talk about the sampling. So, here I show two examples of WaterDistributions or Unigram Language Models. The first one has higher probabilities for words like a text mining association,it's separate. Now this signals a topic about text miningbecause when we assemble words from such a distribution, we tend to see wordsthat often occur in text mining contest. So in this case,if we ask the question about what is the probability ofgenerating a particular document. Then, we likely will see text thatlooks like a text mining paper. Of course, the text that wegenerate by drawing words. This distribution is unlikely coherent. Although, the probabilityof generating attacks mine [INAUDIBLE] publishingin the top conference is non-zero assuming that no word hasa zero probability in the distribution. And that just means,we can essentially generate all kinds of text documents including verymeaningful text documents. Now, the second distribution show, on the bottom, has different thanwhat was high probabilities. So food [INAUDIBLE] healthy [INAUDIBLE],etcetera. So this clearly indicatesa different topic. In this case it's probably about health. So if we sample a wordfrom such a distribution, then the probability of observing a textmining paper would be very, very small. On the other hand, the probability ofobserving a text that looks like a food nutrition paper would be high,relatively higher. So that just means, given a particulardistribution, different than the text. Now let's look atthe estimation problem now. In this case, we're going to assumethat we have observed the data. I will know exactly whatthe text data looks like. In this case,let's assume we have a text mining paper. In fact, it's abstract of the paper,so the total number of words is 100. And I've shown some countsof individual words here. Now, if we ask the question,what is the most likely Language Model that has beenused to generate this text data? Assuming that the text is observedfrom some Language Model, what's our best guessof this Language Model? Okay, so the problem now is just toestimate the probabilities of these words. As I've shown here. So what do you think? What would be your guess? Would you guess text hasa very small probability, or a relatively large probability? What about query? Well, your guess probablywould be dependent on how many times we have observedthis word in the text data, right? And if you think about it for a moment. And if you are like many others,you would have guessed that, well, text has a probability of 10out of 100 because I've observed the text 10 times in the textthat has a total of 100 words. And similarly, mining has 5 out of 100. And query has a relatively smallprobability, just observed for once. So it's 1 out of 100. Right, so that, intuitively,is a reasonable guess. But the question is, is this our bestguess or best estimate of the parameters? Of course,in order to answer this question, we have to define what do we mean by best,in this case, it turns out that ourguesses are indeed the best. In some sense and this is calledMaximum Likelihood Estimate. And it's the best thing that, it will givethe observer data our maximum probability. Meaning that, if you changethe estimate somehow, even slightly, then the probability of the observedtext data will be somewhat smaller. And this is calleda Maximum Likelihood Estimate. [MUSIC]"
cs-410,8,10,"[SOUND] This lecture is a continued discussion of probabilistic topic models. In this lecture, we're going to continuediscussing probabilistic models. We're going to talk abouta very simple case where we are interested in just miningone topic from one document. So in this simple setup,we are interested in analyzing one document andtrying to discover just one topic. So this is the simplestcase of topic model. The input now no longer has k,which is the number of topics because we know there is only one topic and thecollection has only one document, also. In the output,we also no longer have coverage because we assumed that the documentcovers this topic 100%. So the main goal is just to discoverthe world of probabilities for this single topic, as shown here. As always, when we think about using agenerating model to solve such a problem, we start with thinking about whatkind of data we are going to model or from what perspective we're going tomodel the data or data representation. And then we're going todesign a specific model for the generating of the data,from our perspective. Where our perspective just means we wantto take a particular angle of looking at the data, so that the model willhave the right parameters for discovering the knowledge that we want. And then we'll be thinkingabout the microfunction or write down the microfunction tocapture more formally how likely a data point will beobtained from this model. And the likelihood function will havesome parameters in the function. And then we argue our interest inestimating those parameters for example, by maximizing the likelihood which willlead to maximum likelihood estimated. These estimator parameterswill then become the output of the mining hours,which means we'll take the estimating parameters as the knowledgethat we discover from the text. So let's look at these steps forthis very simple case. Later we'll look at this procedure forsome more complicated cases. So our data, in this case is, justa document which is a sequence of words. Each word here is denoted by x sub i. Our model is a Unigram language model. A word distribution that we hope todenote a topic and that's our goal. So we will have as many parameters as manywords in our vocabulary, in this case M. And for convenience we'regoing to use theta sub i to denote the probability of word w sub i. And obviously these thetasub i's will sum to 1. Now what does a likelihoodfunction look like? Well, this is just the probabilityof generating this whole document, that given such a model. Because we assume the independence ingenerating each word so the probability of the document will be just a productof the probability of each word. And since some word mighthave repeated occurrences. So we can also rewrite thisproduct in a different form. So in this line, we have rewrittenthe formula into a product over all the unique words inthe vocabulary, w sub 1 through w sub M. Now this is differentfrom the previous line. Well, the product is over differentpositions of words in the document. Now when we do this transformation,we then would need to introduce a counter function here. This denotes the count ofword one in document and similarly this is the countof words of n in the document because these words mighthave repeated occurrences. You can also see if a word didnot occur in the document. It will have a zero count, thereforethat corresponding term will disappear. So this is a very useful form of writing down the likelihood functionthat we will often use later. So I want you to pay attention to this,just get familiar with this notation. It's just to change the product over allthe different words in the vocabulary. So in the end, of course, we'll usetheta sub i to express this likelihood function and it would look like this. Next, we're going to findthe theta values or probabilities of these words that would maximizethis likelihood function. So now lets take a look at the maximumlikelihood estimate problem more closely. This line is copied fromthe previous slide. It's just our likelihood function. So our goal is to maximizethis likelihood function. We will find it often easy to maximize the local likelihoodinstead of the original likelihood. And this is purely formathematical convenience because after the logarithm transformation our functionwill becomes a sum instead of product. And we also have constraintsover these these probabilities. The sum makes it easier to takederivative, which is often needed for finding the optimalsolution of this function. So please take a look at this sum again,here. And this is a form ofa function that you will often see later also,the more general topic models. So it's a sum over allthe words in the vocabulary. And inside the sum there isa count of a word in the document. And this is macroed bythe logarithm of a probability. So let's see how we cansolve this problem. Now at this point the problem is purely amathematical problem because we are going to just the find the optimal solutionof a constrained maximization problem. The objective function isthe likelihood function and the constraint is that all theseprobabilities must sum to one. So, one way to solve the problem isto use Lagrange multiplier approace. Now this command is beyondthe scope of this course but since Lagrange multiplier is a veryuseful approach, I also would like to just give a brief introduction to this,for those of you who are interested. So in this approach we willconstruct a Lagrange function, here. And this function will combineour objective function with another term thatencodes our constraint and we introduce Lagrange multiplier here, lambda, so it's an additional parameter. Now, the idea of this approach is just toturn the constraint optimization into, in some sense,an unconstrained optimizing problem. Now we are just interested inoptimizing this Lagrange function. As you may recall from calculus,an optimal point would be achieved whenthe derivative is set to zero. This is a necessary condition. It's not sufficient, though. So if we do that you willsee the partial derivative, with respect to theta ihere ,is equal to this. And this part comes from the derivativeof the logarithm function and this lambda is simply taken from here. And when we set it to zero we can easily see theta sub i isrelated to lambda in this way. Since we know all the thetai's must a sum to one we can plug this into this constraint,here. And this will allow us to solve forlambda. And this is just a netsum of all the counts. And this further allows us to thensolve the optimization problem, eventually, to find the optimalsetting for theta sub i. And if you look at this formula it turnsout that it's actually very intuitive because this is just the normalizedcount of these words by the document ns, which is also a sum of allthe counts of words in the document. So, after all this mess, after all, we have just obtained somethingthat's very intuitive and this will be just ourintuition where we want to maximize the data byassigning as much probability mass as possible to allthe observed the words here. And you might also notice that this isthe general result of maximum likelihood raised estimator. In general, the estimator would be tonormalize counts and it's just sometimes the counts have to be done in a particularway, as you will also see later. So this is basically an analyticalsolution to our optimization problem. In general though, when the likelihoodfunction is very complicated, we're not going to be able to solve the optimizationproblem by having a closed form formula. Instead we have to use somenumerical algorithms and we're going to see such cases later, also. So if you imagine what would weget if we use such a maximum likelihood estimator to estimate onetopic for a single document d here? Let's imagine this documentis a text mining paper. Now, what you might see issomething that looks like this. On the top, you will see the highprobability words tend to be those very common words,often functional words in English. And this will be followed bysome content words that really characterize the topic well like text,mining, etc. And then in the end,you also see there is more probability of words that are not reallyrelated to the topic but they might be extraneouslymentioned in the document. As a topic representation,you will see this is not ideal, right? That because the high probabilitywords are functional words, they are not reallycharacterizing the topic. So my question is how can weget rid of such common words? Now this is the topic of the next module. We're going to talk about how to useprobabilistic models to somehow get rid of these common words. [MUSIC]"
cs-410,9,1,"[MUSIC] This lecture is about the mixtureof unigram language models. In this lecture we will continuediscussing probabilistic topic models. In particular, what we introducea mixture of unigram language models. This is a slide thatyou have seen earlier. Where we talked about how toget rid of the background words that we have on top of forone document. So if you want to solve the problem, it would be useful to think aboutwhy we end up having this problem. Well, this obviously because thesewords are very frequent in our data and we are using a maximumlikelihood to estimate. Then the estimate obviously wouldhave to assign high probability for these words in order tomaximize the likelihood. So, in order to get rid of them thatwould mean we'd have to do something differently here. In particular we'll haveto say this distribution doesn't have to explain allthe words in the tax data. What were going to say is that, these common words should not beexplained by this distribution. So one natural way to solve the problem isto think about using another distribution to account for just these common words. This way, the two distributions can bemixed together to generate the text data. And we'll let the other model whichwe'll call background topic model to generate the common words. This way our target topic thetahere will be only generating the common handle words that arecharacterised the content of the document. So, how does this work? Well, it is just a smallmodification of the previous setup where we have just one distribution. Since we now have two distributions, we have to decide which distributionto use when we generate the word. Each word will still be a samplefrom one of the two distributions. Text data is stillgenerating the same way. Namely, look at the generatingof the one word at each time and eventually we generate a lot of words. When we generate the word, however, we're going to first decidewhich of the two distributions to use. And this is controlled by anotherprobability, the probability of theta sub d andthe probability of theta sub B here. So this is a probability of enactingthe topic word of distribution. This is the probability ofenacting the background word of distribution denoted by theta sub B. On this case I just give examplewhere we can set both to 0.5. So you're going to basically flip a coin,a fair coin, to decide what you want to use. But in general these probabilitiesdon't have to be equal. So you might bias toward usingone topic more than the other. So now the process of generating a wordwould be to first we flip a coin. Based on these probabilities choosingeach model and if let's say the coin shows up as head, which means we're goingto use the topic two word distribution. Then we're going to use this worddistribution to generate a word. Otherwise we might begoing slow this path. And we're going to use the backgroundword distribution to generate a word. So in such a case,we have a model that has some uncertainty associated with the useof a word distribution. But we can still think of this asa model for generating text data. And such a model iscalled a mixture model. So now let's see. In this case, what's the probabilityof observing a word w? Now here I showed some words. like ""the"" and ""text"". So as in all cases, once we setup a model we are interestedin computing the likelihood function. The basic question is, so what's the probability ofobserving a specific word here? Now we know that the word can be observedfrom each of the two distributions, so we have to consider two cases. Therefore it's a sum over these two cases. The first case is to use the topic forthe distribution to generate the word. And in such a case thenthe probably would be theta sub d, which is the probabilityof choosing the model multiplied by the probability of actuallyobserving the word from that model. Both events must happenin order to observe. We first must have choosingthe topic theta sub d and then, we also have to actually have sampledthe word the from the distribution. And similarly,the second part accounts for a different way of generallythe word from the background. Now obviously the probability oftext the same is all similar, right? So we also can see the twoways of generating the text. And in each case, it's a product of theprobability of choosing a particular word is multiplied by the probability ofobserving the word from that distribution. Now whether you will see,this is actually a general form. So might want to make sure that you havereally understood this expression here. And you should convince yourself thatthis is indeed the probability of obsolete text. So to summarize what we observed here. The probability of a word froma mixture model is a general sum of different ways of generating the word. In each case, it's a product of the probabilityof selecting that component model. Multiplied by the probability ofactually observing the data point from that component of the model. And this is something quite general andyou will see this occurring often later. So the basic idea of a mixturemodel is just to retrieve thesetwo distributionstogether as one model. So I used a box to bring allthese components together. So if you view thiswhole box as one model, it's just like any other generative model. It would just give usthe probability of a word. But the way that determines thisprobability is quite the different from when we have just one distribution. And this is basically a morecomplicated mixture model. So the more complicated is morethan just one distribution. And it's called a mixture model. So as I just said we can treatthis as a generative model. And it's often useful to think ofjust as a likelihood function. The illustration thatyou have seen before, which is dimmer now, is justthe illustration of this generated model. So mathematically,this model is nothing but to just define the followinggenerative model. Where the probability of a word isassumed to be a sum over two cases of generating the word. And the form you are seeing nowis a more general form that what you have seen inthe calculation earlier. Well I just use the symbolw to denote any water but you can still see this isbasically first a sum. Right? And this sum is due to the fact that thewater can be generated in much more ways, two ways in this case. And inside a sum,each term is a product of two terms. And the two terms are firstthe probability of selecting a component like of D Second, the probability of actually observingthe word from this component of the model. So this is a very general descriptionof all the mixture models. I just want to make surethat you understand this because this is really the basis forunderstanding all kinds of on top models. So now once we setup model. We can write down that likefunctioning as we see here. The next question is,how can we estimate the parameter, or what to do with the parameters. Given the data. Well, in general, we can use some of the text datato estimate the model parameters. And this estimation would allow us to discover the interestingknowledge about the text. So you, in this case, what do we discover? Well, these are presentedby our parameters and we will have two kinds of parameters. One is the two worded distributions,that result in topics, and the other is the coverageof each topic in each. The coverage of each topic. And this is determined byprobability of C less of D and probability of theta, so this is to one. Now, what's interesting isalso to think about special cases like when we send one ofthem to want what would happen? Well with the other, with the zero right? And if you look atthe likelihood function, it will then degenerate to the specialcase of just one distribution. Okay so you can easily verify that byassuming one of these two is 1.0 and the other is Zero. So in this sense,the mixture model is more general than the previous model where wehave just one distribution. It can cover that as a special case. So to summarize, we talked about themixture of two Unigram Language Models and the data we're consideringhere is just One document. And the model is a mixturemodel with two components, two unigram LM models,specifically theta sub d, which is intended to denote the topic ofdocument d, and theta sub B, which is representing a background topic thatwe can set to attract the common words because common words would beassigned a high probability in this model. So the parameters canbe collectively called Lambda which I show here you can again think about the question about how manyparameters are we talking about exactly. This is usually a good exercise to dobecause it allows you to see the model in depth and to have a complete understandingof what's going on this model. And we have mixing weights,of course, also. So what does a likelihoodfunction look like? Well, it looks very similarto what we had before. So for the document, first it's a product over all the words inthe document exactly the same as before. The only difference is that inside herenow it's a sum instead of just one. So you might have recalled beforewe just had this one there. But now we have this sumbecause of the mixture model. And because of the mixture model wealso have to introduce a probability of choosing that particularcomponent of distribution. And sothis is just another way of writing, and by using a product over all the uniquewords in our vocabulary instead of having that product over allthe positions in the document. And this form where we look atthe different and unique words is a commutative that formed for computingthe maximum likelihood estimate later. And the maximum likelihood estimator is,as usual, just to find the parameters that wouldmaximize the likelihood function. And the constraints hereare of course two kinds. One is what are probabilities in each [INAUDIBLE] must sum to 1 the other is the choice of each[INAUDIBLE] must sum to 1. [MUSIC]"
cs-410,9,2,"This lecture is aboutthe mixture model estimation. In this lecture, we'regoing to continue discussing probabilistictopic models. In particular, we're goingto talk about the how to estimate the parametersof a mixture model. So let's first lookat our motivation for using a mixture model, and we hope to effect out the background words fromthe topic word distribution. So the idea is to assume that the text data actuallycontain two kinds of words. One kind is fromthe background here, so the ""is"", ""we"" etc. The other kind is from our topic word distributionthat we're interested in. So in order to solvethis problem of factoring out background words, we can set up our mixturemodel as follows. We are going to assume that we already know the parameters of all the values for all the parameters inthe mixture model except for the word distribution ofTheta sub d which is our target. So this is a case ofcustomizing probably some model so that we embedded the unknown variablesthat we are interested in, but we're going tosimplify other things. We're going to assumewe have knowledge about others and this is a powerful way of customizing a modelfor a particular need. Now you can imagine, wecould have assumed that we also don't know thebackground word distribution, but in this case,our goal is to affect out precisely those high probabilityin the background words. So we assume the backgroundmodel is already fixed. The problem here is, how can we adjust the Theta subd in order to maximize the probability ofthe observed document here and we assume all theother parameters are known? Now, although wedesigned the modal heuristically to try to factor out thesebackground words, it's unclear whether if we use maximumlikelihood estimator, we will actually end up havinga word distribution where the common words like ""the"" will be indeed having smallerprobabilities than before. So now, in this case, it turns out thatthe answer is yes. When we set up the probabilisticmodeling this way, when we use maximumlikelihood estimator, we will end up havinga word distribution where the common wordswould be factored out by the use ofthe background distribution. So to understand why this is so, it's useful to examinethe behavior of a mixture model. So we're going to lookat a very simple case. In order to understand some interesting behaviorsof a mixture model, the observed patternshere actually are generalizable to mixturemodel in general, but it's much easier tounderstand this behavior when we use a very simple caselike what we're seeing here. So specifically in this case, let's assume thatthe probability of choosing each of the two modelsis exactly the same. So we're going to flip a fair coin to decidewhich model to use. Furthermore, we are goingto assume there are precisely to words,""the"" and ""text."" Obviously, this isa very naive oversimplification of the actual text, but again, it is useful to examine the behaviorin such a special case. So we further assume that, the background model givesprobability of 0.9 to the word ""the"" and ""text"" 0.1. Now, let's also assume thatour data is extremely simple. The document has just two words""text"" and then ""the."" So now, let's write down the likelihood functionin such a case. First, what's the probability of ""text"" and what's theprobability of ""the""? I hope by this point, you will be ableto write it down. So the probability of ""text"" is basically a sum oftwo cases where each case corresponds to each of the water distribution and it accounts for the two waysof generating text. Inside each case, we have the probability of choosingthe model which is 0.5 multiplied by the probability of observing ""text""from that model. Similarly, ""the"" wouldhave a probability of the same form just as it was different exactlyprobabilities. So naturally,our likelihood function is just the product of the two. So it's very easy to see that, once you understandwhat's the probability of each word and whichis also why it's so important to understand what's exactly the probability of observing each word fromsuch a mixture model. Now, the interestingquestion now is, how can we then optimizethis likelihood? Well, you will notice that, there are only two variables. They are preciselythe two probabilities of the two words""text"" and ""the"" given by Theta sub d. This isbecause we have assumed that, all the otherparameters are known. So now, the question isa very simple algebra question. So we have a simple expression with two variables and we hope to choose the values of these two variables tomaximize this function. It's exercises that we have seen some simplealgebra problems, and note that the twoprobabilities must sum to one. So there's some constraint. If there wereno constraint of course, we will set both probabilities to their maximum value whichwould be one to maximize this, but we can't do that because ""text"" and""the"" must sum to one. We can't give those aprobability of one. So now the question is, how should we allocatethe probability in the mass between the two words?What do you think? Now, it will be useful to look at this formula formoment and to see intuitively whatwe do in order to set these probabilities to maximize the valueof this function. If we look into this further, then we'll seesome interesting behavior of the two componentmodels in that, they will becollaborating to maximize the probability ofthe observed data which is dictated by the maximumlikelihood estimator, but they're alsocompeting in some way. In particular, theywould be competing on the words and theywill tend to bet high probabilities ondifferent words to avoid this competition in some sense or to gain advantagein this competition. So again, looking at thisobjective function and we have a constraint onthe two probabilities, now if you look atthe formula intuitively, you might feel thatyou want to set the probability of ""text"" to be somewhat larger than ""the"". This intuition canbe well-supported by mathematical fact which is, when the sum oftwo variables is a constant then the product of them which is maximumthen they are equal, and this is a fact thatwe know from algebra. Now, if we plug that in, we will would meanthat we have to make the two probabilities equal. When we make them equaland then if we consider the constraint that we caneasily solve this problem, and the solution is theprobability of ""text"" would be 0.9 and probabilityof ""the"" is 0.1. As you can see indeed, the probability of textis not much larger than probability of ""the"" and this is not the case when wehave just one distribution. This is clearly because of the use of thebackground model which assign a very high probability to ""the"" lowprobability to ""text"". If you look at the equation, you will see obviously some interaction ofthe two distributions here. In particular, you will see in order to make them equal and then the probability assignedby Theta sub d must be higher for a word that has a smaller probabilitygiven by the background. This is obvious fromexamining this equation because ""the"" background part is weak for ""text"" it's a small. So in order tocompensate for that, we must make the probabilityof ""text"" that's given by Theta sub d somewhat larger so that the two sidescan be balanced. So this is in fact a very general behaviorof this mixture model. That is, if one distribution assigns a high probabilityto one word than another, then the other distribution would tend to do the opposite. Basically, it would discourage other distributions to do the same and this is tobalance them out so that, we can account for all words. This also means that, by using a backgroundmodel that is fixed to assign high probabilitiesto background words, we can indeed encourage the unknown topicword distribution to assign smaller probabilitiesfor such common words. Instead, put more probabilitymass on the content words that cannot be explained well by the backgroundmodel meaning that, they have a verysmall probability from the backgroundmodel like ""text"" here."
cs-410,9,4,"This lecture is about the expectation-maximizationalgorithm, also called the EM algorithm. In this lecture, we'regoing to continue the discussion ofprobabilistic topic models. In particular, we're going tointroduce the EM algorithm, which is a family ofuseful algorithms for computing the maximum likelihood estimateof mixture models. So this is now familiar scenario ofusing a two component, the mixture model, to try to factor outthe background words from one topic wordof distribution here. So we're interested incomputing this estimate, and we're going to try to adjust these probability values to maximize the probabilityof the observed document. Note that we assume that all the other parameters are known. So the only thing unknown is the word probabilitiesgiven by theta sub. In this lecture, we'regoing to look into how to compute this maximumlikelihood estimate. Now, let's start with the idea of separating the words inthe text data into two groups. One group would be explainedby the background model. The other group wouldbe explained by the unknown topicword distribution. After all, this isthe basic idea of mixture model. But suppose we actually know which word is fromwhich distribution? So that would mean, for example, these words the, is, and we are known to be from this backgroundword distribution. On the other hand, theother words text, mining, clustering etc are known to be from the topic word distribution. If you can see the color, then these are shown in blue. These blue words are then assumed that to be fromthe topic word distribution. If we already know howto separate these words, then the problem of estimating the word distributionwould be extremely simple. If you think aboutthis for a moment, you'll realize that, well, we can simply takeall these words that are known to be from this word distribution theta sub dand normalize them. So indeed this problem would be very easy to solve if we had known which words are from which a distribution precisely, and this is in fact making this model nolonger a mixture model because we can already observe which distribution has been used to generatewhich part of the data. So we actually go back to the single worddistribution problem. In this case let's call these words that areknown to be from theta d, a pseudo document of d prime, and now all we need todo is just normalize these words countsfor each word w_i. That's fairly straightforward. It's just dictated by themaximum likelihood estimator. Now, this idea howeverdoesn't work because we in practice don't really know which word is fromwhich distribution, but this gives usthe idea of perhaps we can guess which word isfrom which it is written. Specifically givenall the parameters, can we infer the distributiona word is from. So let's assume that we actually know tentative probabilities for these words in theta sub d. So now all the parameters are known for this mixture model, and now let's considera word like a ""text"". So the question is, do youthink ""text"" is more likely having been generated from theta sub d or fromtheta sub of b? So in other words,we want to infer which distribution has beenused to generate this text. Now, this inference process is a typical Bayesian inferencesituation where we have some prior aboutthese two distributions. So can you see whatis our prior here? Well, the prior here is the probability ofeach distribution. So the prior is given bythese two probabilities. In this case, the prior is saying that each modelis equally likely, but we can imagine perhaps adifferent prior is possible. So this is called a priorbecause this is our guess of which distribution hasbeen used to generate a word before we evenoff reserve the word. So that's why wecall it the prior. So if we don't observe the word, we don't know what wordhas been observed. Our best guess is to saywell, they're equally likely. All right. So it'sjust flipping a coin. Now in Bayesian inference wetypically learn with update our belief after we haveobserved the evidence. So what is the evidence here? Well, the evidencehere is the word text. Now that we know we'reinterested in the word text. So text that can beregarded as evidence, and if we use Bayes rule to combine theprior and the data likelihood, what we will end upwith is to combine the prior with the likelihoodthat you see here, which is basicallythe probability of the word text fromeach distribution. We see that in both casesthe text is possible. Note that even in the backgroundit is still possible, it just has a verysmall probability. So intuitively what wouldbe your guess in this case. Now if you're like many others, you are guess textis probably from theta sub d. It's more likelyfrom theta sub d. Why? You will probably see thatit's because text that has a much higher probabilityhere by the theta sub d, then by the background model which has a verysmall probability. By this we're going to say, well, text is more likely from theta sub d. So you see our guess of whichdistribution has been used to generatethe text would depend on how high the probability of the text is ineach word distribution. We can do, tend to guess the distribution that gives us a word a higher probability, and this is likely tomaximize the likelihood. So we're going to choose a word that hasa higher likelihood. So in other words,we're going to compare these two probabilities of the word given byeach distributions. But our guess must alsobe affected by the prior. So we also need tocompare these two priors. Why? Because imagine if weadjust these probabilities, we're going to saythe probability of choosing a background model isalmost 100 percent. Now, if you have that kindof strong prior, then that wouldaffect your guess. You might think,well, wait a moment, maybe text could have beenfrom the background as well. Although the probabilityis very small here, the prior is very high. So in the end, we haveto combine the two, and the base formula provides us a solid and principled way of making this kind ofguess to quantify that. So more specifically, let's think aboutthe probability that this word has been generated in fact from from theta sub d. Well, in order for textsto be generated from theta sub d two thingsmust happen. First, the theta sub dmust have been selected, so we have the selectionprobability here. Secondly, we also have to actually have observed textfrom the distribution. So when we multiplythe two together, we get the probabilitythat text has in fact been generated fromtheta sub d. Similarly, for the background model, the probability of generating text is another productof a similar form. Now, we also introduced the latent variablez here to denote whether the word is fromthe background or the topic. When z is zero, it means it's from the topictheta sub d. When it's one, it means it's fromthe background theta sub b. So now we have the probability that textis generated from each. Then we can simply normalize them to have an estimateof the probability that the word text is from theta sub d orfrom theta sub b. Then equivalently, theprobability that z is equal to zero given thatthe observed evidence is text. So this is applicationof Bayes rule. But this step is verycrucial for understanding the EM algorithm becauseif we can do this, then we would be able to first initialize the parameter valuessomewhat randomly, and then we're going to takea guess of these z values. Which distributing has beenused to generate which word, and the initializedthe parameter values would allow us to have a complete specification ofthe mixture model which further allows us toapply Bayes rule to infer which distribution is morelikely to generate each word. This predictionessentially helped us to separate the words fromthe two distributions. Although we can'tseparate them for sure, but we can separate themprobabilistically as shown here."
cs-410,9,5,"[SOUND]So this is indeed a general idea ofthe Expectation-Maximization, or EM, Algorithm. So in all the EM algorithms weintroduce a hidden variable to help us solve the problem more easily. In our case the hidden variableis a binary variable for each occurrence of a word. And this binary variable wouldindicate whether the word has been generated from 0 sub d or 0 sub p. And here we show some possiblevalues of these variables. For example, for the it's from background,the z value is one. And text on the other hand. Is from the topic then it's zero forz, etc. Now, of course, we don't observe these zvalues, we just imagine they're all such. Values of z attaching to other words. And that's why we callthese hidden variables. Now, the idea that wetalked about before for predicting the word distribution thathas been used when we generate the word is it a predictor,the value of this hidden variable? And, so, the EM algorithm then,would work as follows. First, we'll initialize allthe parameters with random values. In our case,the parameters are mainly the probability. of a word, given by theta sub d. So this is an initial addition stage. These initialized values would allowus to use base roll to take a guess of these z values, sowe'd guess these values. We can't say for sure whethertextt is from background or not. But we can have our guess. This is given by this formula. It's called an E-step. And so the algorithm would then try touse the E-step to guess these z values. After that, it would then invokeanother that's called M-step. In this step we simply take advantageof the inferred z values and then just group words that are inthe same distribution like these from that ground including this as well. We can then normalize the countto estimate the probabilities or to revise our estimate of the parameters. So let me also illustratethat we can group the words that are believed to havecome from zero sub d, and that's text, mining algorithm,for example, and clustering. And we group them together to help us re-estimate the parametersthat we're interested in. So these will help usestimate these parameters. Note that before we just setthese parameter values randomly. But with this guess, we will havesomewhat improved estimate of this. Of course, we don't know exactlywhether it's zero or one. So we're not going to reallydo the split in a hard way. But rather we're going todo a softer split. And this is what happened here. So we're going to adjust the count bythe probability that would believe this word has been generatedby using the theta sub d. And you can see this,where does this come from? Well, this has come from here, right? From the E-step. So the EM Algorithm woulditeratively improve uur initial estimate of parameters by usingE-step first and then M-step. The E-step is to augment the datawith additional information, like z. And the M-step is to take advantage of the additional informationto separate the data. To split the data accounts andthen collect the right data accounts to re-estimate our parameter. And then once we have a new generation ofparameter, we're going to repeat this. We are going the E-step again. To improve our estimateof the hidden variables. And then that would lead to anothergeneration of re-estimated parameters. For the word distributionthat we are interested in. Okay, so, as I said,the bridge between the two is really the variable z, hidden variable,which indicates how likely this water is from the top waterdistribution, theta sub p. So, this slide has a lot of content andyou may need to. Pause the reader to digest it. But this basically capturesthe essence of EM Algorithm. Start with initial values thatare often random themself. And then we invoke E-step followedby M-step to get an improved setting of parameters. And then we repeated this, sothis a Hill-Climbing algorithm that would gradually improvethe estimate of parameters. As I will explain laterthere is some guarantee for reaching a local maximum ofthe log-likelihood function. So lets take a look at the computation fora specific case, so these formulas are the EM. Formulas that you see before, andyou can also see there are superscripts, here, like here, n,to indicate the generation of parameters. Like here for example we have n plus one. That means we have improved. From here to here we have an improvement. So in this setting we have assumed the twonumerals have equal probabilities and the background model is null. So what are the relevanceof the statistics? Well these are the word counts. So assume we have just four words,and their counts are like this. And this is our background model thatassigns high probabilities to common words like the. And in the first iteration,you can picture what will happen. Well first we initialize all the values. So here, this probability that we'reinterested in is normalized into a uniform distribution of all the words. And then the E-step would give us a guessof the distribution that has been used. That will generate each word. We can see we have differentprobabilities for different words. Why? Well, that's because these words havedifferent probabilities in the background. So even though the twodistributions are equally likely. And then our initial audition say uniformdistribution because of the difference in the background of the distribution,we have different guess the probability. So these words are believed tobe more likely from the topic. These on the other hand are less likely. Probably from background. So once we have these z values, we know in the M-step these probabilitieswill be used to adjust the counts. So four must be multiplied by this 0.33 in order to get the allocatedaccounts toward the topic. And this is done by this multiplication. Note that if our guess says thisis 100% If this is one point zero, then we just get the full countof this word for this topic. In general it's not goingto be one point zero. So we're just going to get some percentageof this counts toward this topic. Then we simply normalize these counts to have a new generationof parameters estimate. So you can see, compare this withthe older one, which is here. So compare this with this one andwe'll see the probability is different. Not only that, we also see some words that are believed to have come fromthe topic will have a higher probability. Like this one, text. And of course, this new generation ofparameters would allow us to further adjust the inferred latent variable orhidden variable values. So we have a new generation of values, because of the E-step based onthe new generation of parameters. And these new inferred valuesof Zs will give us then another generation of the estimateof probabilities of the word. And so on and so forth so this is whatwould actually happen when we compute these probabilitiesusing the EM Algorithm. As you can see in the last rowwhere we show the log-likelihood, and the likelihood is increasingas we do the iteration. And note that these log-likelihood isnegative because the probability is between 0 and 1 when you take a logarithm,it becomes a negative value. Now what's also interesting is,you'll note the last column. And these are the inverted word split. And these are the probabilitiesthat a word is believed to have come from one distribution, in thiscase the topical distribution, all right. And you might wonder whetherthis would be also useful. Because our main goal is toestimate these word distributions. So this is our primary goal. We hope to have a more discriminativeorder of distribution. But the last column is also bi-product. This also can actually be very useful. You can think about that. We want to use, is to for example is to estimate to what extent thisdocument has covered background words. And this, when we add this up or take the average we will kind of know towhat extent it has covered background versus content was that are notexplained well by the background. [MUSIC]"
cs-410,9,6,"So, I just showed you that empiricallythe likelihood will converge, but theoretically it can alsobe proved that EM algorithm will converge to a local maximum. So here's just an illustration of whathappened and a detailed explanation. This required more knowledge about that, some of that inequalities,that we haven't really covered yet. So here what you see is on the Xdimension, we have a c0 value. This is a parameter that we have. On the y axis we seethe likelihood function. So this curve is the originallikelihood function, and this is the one thatwe hope to maximize. And we hope to find a c0 valueat this point to maximize this. But in the case of Mitsumoto we cannot easily find an analytic solution to the problem. So, we have to resolvethe numerical errors, and the EM algorithm is such an algorithm. It's a Hill-Climb algorithm. That would mean you startwith some random guess. Let's say you start from here,that's your starting point. And then you try to improvethis by moving this to another point where you canhave a higher likelihood. So that's the ideal hill climbing. And in the EM algorithm, the way weachieve this is to do two things. First, we'll fix a lowerbound of likelihood function. So this is the lower bound. See here. And once we fit the lower bound,we can then maximize the lower bound. And of course, the reason why this works, is because the lower boundis much easier to optimize. So we know our current guess is here. And by maximizing the lower bound,we'll move this point to the top. To here. Right? And we can then map to the originallikelihood function, we find this point. Because it's a lower bound, we areguaranteed to improve this guess, right? Because we improve our lower bound andthen the original likelihood curve which is above this lower boundwill definitely be improved as well. So we already know it'simproving the lower bound. So we definitely improve thisoriginal likelihood function, which is above this lower bound. So, in our example, the current guess is parameter valuegiven by the current generation. And then the next guess isthe re-estimated parameter values. From this illustration youcan see the next guess is always better than the current guess. Unless it has reached the maximum,where it will be stuck there. So the two would be equal. So, the E-step is basically to compute this lower bound. We don't directly just computethis likelihood function but we compute the length ofthe variable values and these are basically a partof this lower bound. This helps determine the lower bound. The M-step on the other hand isto maximize the lower bound. It allows us to moveparameters to a new point. And that's why EM algorithm is guaranteedto converge to a local maximum. Now, as you can imagine,when we have many local maxima, we also have to repeat the EMalgorithm multiple times. In order to figure out which oneis the actual global maximum. And this actually in general is adifficult problem in numeral optimization. So here forexample had we started from here, then we gradually justclimb up to this top. So, that's not optimal, andwe'd like to climb up all the way to here, so the only way to climb up to this gearis to start from somewhere here or here. So, in the EM algorithm, we generallywould have to start from different points or have some other way to determinea good initial starting point. To summarize in this lecture weintroduced the EM algorithm. This is a general algorithm for computingmaximum maximum likelihood estimate of all kinds of models, sonot just for our simple model. And it's a hill-climbing algorithm, so itcan only converge to a local maximum and it will depend on initial points. The general idea is that we will havetwo steps to improve the estimate of. In the E-step we roughly [INAUDIBLE]how many there are by predicting values of useful hidden variables that wewould use to simplify the estimation. In our case, this is the distributionthat has been used to generate the word. In the M-step then we would exploitsuch augmented data which would make it easier to estimate the distribution,to improve the estimate of parameters. Here improve is guaranteed interms of the likelihood function. Note that it's not necessary that wewill have a stable convergence of parameter value even though the likelihoodfunction is ensured to increase. There are some properties that have tobe satisfied in order for the parameters also to convert into some stable value. Now here data augmentationis done probabilistically. That means, we're not going to just say exactlywhat's the value of a hidden variable. But we're going to have a probabilitydistribution over the possible values of these hidden variables. So this causes a split of countsof events probabilistically. And in our case we'll split the wordcounts between the two distributions. [MUSIC]"
cs-410,9,7,"[SOUND]This lecture is about probabilistic andlatent Semantic Analysis or PLSA. In this lecture we're going to introduceprobabilistic latent semantic analysis, often called PLSA. This is the most basic topic model,also one of the most useful topic models. Now this kind of modelscan in general be used to mine multiple topics from text documents. And PRSA is one of the most basictopic models for doing this. So let's first examine this powerin the e-mail for more detail. Here I show a sample article which isa blog article about Hurricane Katrina. And I show some simple topics. For example government response,flood of the city of New Orleans. Donation and the background. You can see in the article we usewords from all these distributions. So we first for example see there'sa criticism of government response and this is followed by discussion of floodingof the city and donation et cetera. We also see backgroundwords mixed with them. So the overall of topic analysis hereis to try to decode these topics behind the text, to segment the topics,to figure out which words are from which distribution and to figure out first,what are these topics? How do we know there's a topicabout government response. There's a topic about a flood in the city. So these are the tasksat the top of the model. If we had discovered thesetopics can color these words, as you see here,to separate the different topics. Then you can do a lot of things,such as summarization, or segmentation, of the topics,clustering of the sentences etc. So the formal definition of problem ofmining multiple topics from text is shown here. And this is after a slide that youhave seen in an earlier lecture. So the input is a collection, the numberof topics, and a vocabulary set, and of course the text data. And then the output is of two kinds. One is the topic category,characterization. Theta i's. Each theta i is a word distribution. And second, it's the topic coverage foreach document. These are pi sub i j's. And they tell us which document it covers. Which topic to what extent. So we hope to generate these as output. Because there are many usefulapplications if we can do that. So the idea of PLSA isactually very similar to the two component mixture modelthat we have already introduced. The only difference is that weare going to have more than two topics. Otherwise, it is essentially the same. So here I illustrate how we can generatethe text that has multiple topics and naturally in all cases of Probabilistic modelling would wantto figure out the likelihood function. So we would also ask the question, what's the probability of observinga word from such a mixture model? Now if you look at this picture and compare this with the picturethat we have seen earlier, you will see the only difference isthat we have added more topics here. So, before we have just one topic,besides the background topic. But now we have more topics. Specifically, we have k topics now. All these are topics that we assumethat exist in the text data. So the consequence is that our switch forchoosing a topic is now a multiway switch. Before it's just a two way switch. We can think of it as flipping a coin. But now we have multiple ways. First we can flip a coin to decidewhether we're talk about the background. So it's the background lambdasub B versus non-background. 1 minus lambda sub B givesus the probability of actually choosing a non-background topic. After we have made this decision, we have to make another decision tochoose one of these K distributions. So there are K way switch here. And this is characterized by pi,and this sum to one. This is just the difference of designs. Which is a little bit more complicated. But once we decide which distribution touse the rest is the same we are going to just generate a word by using one ofthese distributions as shown here. So now lets look at the questionabout the likelihood. So what's the probability of observinga word from such a distribution? What do you think? Now we've seen thisproblem many times now and if you can recall, it's generally a sum. Of all the different possibilitiesof generating a word. So let's first look at how the word canbe generated from the background mode. Well, the probability that the word isgenerated from the background model is lambda multiplied by the probabilityof the word from the background mode. Model, right. Two things must happen. First, we have to havechosen the background model, and that's the probability of lambda,of sub b. Then second, we must have actuallyobtained the word w from the background, and that's probabilityof w given theta sub b. Okay, so similarly, we can figure out the probability ofobserving the word from another topic. Like the topic theta sub k. Now notice that here'sthe product of three terms. And that's because of the choiceof topic theta sub k, only happens if two things happen. One is we decide not totalk about background. So, that's a probabilityof 1 minus lambda sub B. Second, we also have to actually choosetheta sub K among these K topics. So that's probability of theta sub K,or pi. And similarly, the probability ofgenerating a word from the second. The topic and the first topicare like what you are seeing here. And so in the end the probability of observingthe word is just a sum of all these cases. And I have to stress again this is a veryimportant formula to know because this is really key to understanding all the topicmodels and indeed a lot of mixture models. So make sure that you reallyunderstand the probability of w is indeed the sum of these terms. So, next,once we have the likelihood function, we would be interested inknowing the parameters. All right, so to estimate the parameters. But firstly, let's put all these together to have thecomplete likelihood of function for PLSA. The first line shows the probability of aword as illustrated on the previous slide. And this is an importantformula as I said. So let's take a closer look at this. This actually commands allthe important parameters. So first of all we see lambda sub b here. This represents a percentageof background words that we believe exist in the text data. And this can be a known valuethat we set empirically. Second, we see the backgroundlanguage model, and typically we also assume this is known. We can use a large collection of text, or use all the text that we have availableto estimate the world of distribution. Now next in the next stop this formula. [COUGH] Excuse me. You see two interestingkind of parameters, those are the most important parameters. That we are. So one is pi's. And these are the coverageof a topic in the document. And the other is word distributionsthat characterize all the topics. So the next line,then is simply to plug this in to calculatethe probability of document. This is, again, of the familiarform where you have a sum and you have a count ofa word in the document. And then log of a probability. Now it's a little bit morecomplicated than the two component. Because now we have more components,so the sum involves more terms. And then this line is justthe likelihood for the whole collection. And it's very similar, just accounting formore documents in the collection. So what are the unknown parameters? I already said that there are two kinds. One is coverage,one is word distributions. Again, it's a useful exercise foryou to think about. Exactly how manyparameters there are here. How many unknown parameters are there? Now, try and think out that question will help youunderstand the model in more detail. And will also allow you to understandwhat would be the output that we generate when use PLSA to analyze text data? And these are preciselythe unknown parameters. So after we have obtainedthe likelihood function shown here, the next is to worry aboutthe parameter estimation. And we can do the usual think,maximum likelihood estimator. So again, it's a constrained optimizationproblem, like what we have seen before. Only that we have a collection of text andwe have more parameters to estimate. And we still have two constraints,two kinds of constraints. One is the word distributions. All the words must have probabilitiesthat's sum to one for one distribution. The other is the topiccoverage distribution and a document will have to coverprecisely these k topics so the probability of covering eachtopic that would have to sum to 1. So at this point though it's basicallya well defined applied math problem, you just need to figure outthe solutions to optimization problem. There's a function with many variables. and we need to just figureout the patterns of these variables to make the functionreach its maximum. >> [MUSIC]"
cs-410,9,10,"[SOUND] Sonow let's talk about the exchanging of PLSA to of LDA and to motivate that, we need to talk about somedeficiencies of PLSA. First, it's not really a generative modelbecause we can compute the probability of a new document. You can see why, and that's because thepis are needed to generate the document, but the pis are tied to the documentthat we have in the training data. So we can't compute the pis forfuture document. And there's some heuristic workaround,though. Secondly, it has many parameters, and I'veasked you to compute how many parameters exactly there are in PLSA, andyou will see there are many parameters. That means that model is very complex. And this also means that thereare many local maxima and it's prone to overfitting. And that means it's very hard toalso find a good local maximum. And that we are representingglobal maximum. And in terms of explaining future data,we might find that it will overfit the training databecause of the complexity of the model. The model is so flexible to fit preciselywhat the training data looks like. And then it doesn't allow us to generalizethe model for using other data. This however is not a necessary problemfor text mining because here we're often only interested in hittingthe training documents that we have. We are not always interested in modernfuture data, but in other cases, or if we would care about the generality,we would worry about this overfitting. So LDA is proposing to improve that,and basically to make PLSA a generative model by imposinga Dirichlet prior on the model parameters. Dirichlet is just a special distributionthat we can use to specify product. So in this sense, LDA is justa Bayesian version of PLSA, and the parameters are nowmuch more regularized. You will see there are manyfew parameters and you can achieve the same goal as PLSA fortext mining. It means it can compute the top coverageand topic word distributions as in PLSA. However, there's no. Why are the parameters forPLSA here are much fewer, there are fewer parameters andin order to compute a topic coverage and word distributions,we again face a problem of influence of these variables becausethey are not parameters of the model. So the influence part againface the local maximum problem. So essentially they are doing somethingvery similar, but theoretically, LDA is a more elegant way of lookingat the top and bottom problem. So let's see how we cangeneralize the PLSA to LDA or a standard PLSA to have LDA. Now a full treatment of LDA isbeyond the scope of this course and we just don't have time to go indepth on that talking about that. But here, I just want to give youa brief idea about what's extending and what it enables, all right. So this is the picture of LDA. Now, I remove the backgroundof model just for simplicity. Now, in this model, all theseparameters are free to change and we do not impose any prior. So these word distributions are nowrepresented as theta vectors. So these are word distributions, so here. And the other set of parameters are pis. And we would present it as a vector also. And this is more convenientto introduce LDA. And we have one vector for each document. And in this case, in theta,we have one vector for each topic. Now, the difference between LDA and PLSA is that in LDA, we're not goingto allow them to free the chain. Instead, we're going to force them tobe drawn from another distribution. So more specifically, they will be drawn from two Dirichletdistributions respectively, but the Dirichlet distribution isa distribution over vectors. So it gives us a probability offour particular choice of a vector. Take, for example, pis, right. So this Dirichlet distribution tellsus which vectors of pi is more likely. And this distribution in itself iscontrolled by another vector of parameters of alphas. Depending on the alphas, we cancharacterize the distribution in different ways but with full certain choices ofpis to be more likely than others. For example, you might favor the choice of a relativelyuniform distribution of all the topics. Or you might favor generatinga skewed coverage of topics, and this is controlled by alpha. And similarly here, the topic orword distributions are drawn from another Dirichletdistribution with beta parameters. And note that here,alpha has k parameters, corresponding to our inference onthe k values of pis for our document. Whereas here, beta has n values corresponding tocontrolling the m words in our vocabulary. Now once we impose this price, thenthe generation process will be different. And we start with joined pis from the Dirichlet distribution andthis pi will tell us these probabilities. And then, we're going to use the pito further choose which topic to use, and this is of coursevery similar to the PLSA model. And similar here, we're not goingto have these distributions free. Instead, we're going to draw onefrom the Dirichlet distribution. And then from this,then we're going to further sample a word. And the rest is very similar to the. The likelihood function nowis more complicated for LDA. But there's a close connection between thelikelihood function of LDA and the PLSA. So I'm going to illustratethe difference here. So in the top, you see PLSA likelihood functionthat you have already seen before. It's copied from previous slide. Only that I dropped the background forsimplicity. So in the LDA formulas yousee very similar things. You see the first equationis essentially the same. And this is the probability of generatinga word from multiple word distributions. And this formula is a sum of allthe possibilities of generating a word. Inside a sum is a product ofthe probability of choosing a topic multiplied by the probability ofobserving the word from that topic. So this is a very important formula,as I've stressed multiple times. And this is actually the coreassumption in all the topic models. And you might see other topic modelsthat are extensions of LDA or PLSA. And they all rely on this. So it's very important to understand this. And this gives us a probability ofgetting a word from a mixture model. Now, next in the probability ofa document, we see there is a PLSA component in the LDA formula, but the LDAformula will add a sum integral here. And that's to account forthe fact that the pis are not fixed. So they are drawn from the originaldistribution, and that's shown here. That's why we have to take an integral,to consider all the possible pis that we could possibly draw fromthis Dirichlet distribution. And similarly in the likelihood forthe whole collection, we also see further components added,another integral here. Right? So basically in the area we're justadding this integrals to account for the uncertainties and we added of coursethe Dirichlet distributions to cover the choice of this parameters,pis, and theta. So this is a likelihood function for LDA. Now, next to this, let's talk about theparameter as estimation and inferences. Now the parameters can be now estimatedusing exactly the same approach maximum likelihood estimate for LDA. Now you might think about how manyparameters are there in LDA versus PLSA. You'll see there're a fewer parametersin LDA because in this case the only parameters are alphas and the betas. So we can use the maximum likelihoodestimator to compute that. Of course, it's more complicated becausethe form of likelihood function is more complicated. But what's also importantis notice that now these parameters that we are interestedin name and topics, and the coverage are nolonger parameters in LDA. In this case we have touse basic inference or posterior inference to compute them basedon the parameters of alpha and the beta. Unfortunately, thiscomputation is intractable. So we generally have to resortto approximate inference. And there are many methods available forthat and I'm sure you will see them when you use different tool kitsfor LDA, or when you read papers about these different extensions of LDA. Now here we, of course, can't givein-depth instruction to that, but just know that they are computed based in inference by usingthe parameters alphas and betas. But our math [INAUDIBLE],actually, in the end, in some of our math list,it's very similar to PLSA. And, especially when we usealgorithm called class assembly, then the algorithm looks verysimilar to the Algorithm. So in the end,they are doing something very similar. So to summarize our discussionof properties of topic models, these models providea general principle or way of mining and analyzing topicsin text with many applications. The best basic task setup isto take test data as input and we're going to output the k topics. Each topic is characterizedby word distribution. And we're going to also output proportionsof these topics covered in each document. And PLSA is the basic topic model, andin fact the most basic of the topic model. And this is often adequate formost applications. That's why we spend a lot oftime to explain PLSA in detail. Now LDA improves overPLSA by imposing priors. This has led to theoreticallymore appealing models. However, in practice, LDA andPLSA tend to give similar performance, so in practice PLSA and LDA would workequally well for most of the tasks. Now here are some suggested readings ifyou want to know more about the topic. First is a nice review ofprobabilistic topic models. The second has a discussion about howto automatically label a topic model. Now I've shown you some distributions andthey intuitively suggest a topic. But what exactly is a topic? Can we use phrases to label the topic? To make it the more easy to understand and this paper is about the techniques fordoing that. The third one is empirical comparisonof LDA and the PLSA for various tasks. The conclusion is that theytend to perform similarly. [MUSIC]"
cs-410,10,6,"[MUSIC] This lecture is aboutevaluation of text clustering. So far we have talked about multipleways of doing text clustering but how do we know whichmethod works the best? So this has to do with evaluation. Now to talk about evaluation one must go back to the clustering bias thatwe introduced at the beginning. Because two objects can be similardepending on how you look at them, we must clearly specifythe perspective of similarity. Without that, the problem ofclustering is not well defined. So this perspective is alsovery important for evaluation. If you look at this slide, and you can see we have two differentways to cluster these shapes, and if you ask a question, which one isthe best, or which one is better? You actually see, there's no way to answerthis question without knowing whether we'd like to cluster based on shapes,or cluster based on sizes. And that's precisely whythe perspective on clustering bias is crucial for evaluation. In general,we can evaluate text clusters in two ways, one is direct evaluation, andthe other indirect evaluation. So in direct evaluation, we want to answer the following questions,how close are the system-generated clusters to the ideal clustersthat are generated by humans? So the closeness here can be assessed from multiple perspectives andthat will help us characterize the quality of cluster result in multiple angles,and this is sometimes desirable. Now we also want to quantifythe closeness because this would allow us to easily compare different measuresbased on their performance figures. And finally, you can see, in this case,we essentially inject the clustering bias by using humans, basically humanswould bring in the the need or desire to clustering bias. Now, how do we do that exactly? Well, the general procedurewould look like this. Given a test set which consistsof a lot of text objects, we can have humans to createthe ideal clustering result, that is, we're going to ask humans to partitionthe objects to create the gold standard. And they will use their judgments basedon the need of a particular application to generate what they think are the bestclustering results, and this would be then used to compare with the system generatedclusters from the same test set. And ideally, we want the system resultsto be the same as the human generated results, but in general,they are not going to be the same. So we would like to then quantify thesimilarity between the system-generated clusters and the gold standard clusters. And this similarity can also be measurefrom multiple perspectives and this will give us various meshes to quantitativelyevaluate a cluster, a clustering result. And some of the commonly used measuresinclude the purity, which measures whether a cluster has a similar object fromthe same cluster, in the gold standard. And normalized mutual informationis a commonly used measure which basically measuresbased on the identity of cluster of object in the system generally. How well can you predict the clusterof the object in the gold standard or vice versa? And mutual information captures, thecorrelation between these cluster labels and normalized mutual information is often used for quantifying the similarity for this evaluation purpose,F measure is another possible measure. Now again a thorough discussionof this evaluation and these evaluation issues would bebeyond the scope of this course. I've suggested some reading inthe end that you can take a look at to know more about that. So here I just want todiscuss some high level ideas that would allow you to think about howto do evaluation in your applications. The second way to evaluate textclusters is to do indirect evaluation. So in this case the question to answer is,how useful are the clustering results for the intended applications? Now this of course is applicationspecific question, so usefulness is going to dependon specific applications. In this case, the clustering bias isimposed by the independent application as well, so what counts as a best cluster resultwould be dependent on the application. Now procedure wise we also would createa test set with text objects for the intended application to quantifythe performance of the system. In this case,what we care about is the contribution of clustering to some application so we oftenhave a baseline system to compare with. This could be the current system fordoing something, and then you hope to adda clustering to improve it, or the baseline system could be usinga different clustering method. And then what you are tryingto experiment with, and you hope to have betteridea of word clustering. So in any case you have a baseline systemwork with, and then you add a clustering algorithm to the baseline systemto produce a clustering system. And then we have to compare theperformance of your clustering system and the baseline system in termsof the performance measure for that particular application. So in this case we call it indirectevaluation of clusters because there's no explicit assessment ofthe quality of clusters, but rather it's to assess the contributionof clusters to a particular application. So, to summarize text clustering, it's a very useful unsupervisedgeneral text mining technique, and it's particularly useful for obtainingan overall picture of the text content. And this is often neededto explore text data, and this is often the first step whenyou deal with a lot of text data. The second application orsecond kind of applications is through discover interesting clusteringstructures in text data and these structures can be very meaningful. There are many approaches that canbe used to form text clustering and we discussed model based approaches andsome narrative based approaches. In general, strong clusters tend toshow up no matter what method is used. Also the effectiveness of a methodhighly depends on whether the desired clustering bias is captured appropriately,and this can be done either through using the right generating model, the modeldesign appropriate for the clustering, or the right similarity functionexpressly define the bias. Deciding the optimal number of customersis a very difficult problem for order cluster methods, and that'sbecause it's unsupervised algorithm, and there's no training there how to guideus to select the best number of clusters. Now sometimes you may see some methodsthat can automatically determine the number of clusters, butin general that has some implied application of clustering bias there andthat's just not specified. Without clearly defining a clusteringbias, it's just impossible to say the optimal number of cluster is what,so this important to keep in mind. And I should also say sometimes wecan also use application to determine the number of clusters, for example,if you're clustering search results, then obviously you don't wantto generate the 100 clusters, so the number can be dictatedby the interface design. In other situations, we might beable to use the fitness to data to assess whether we've got a good numberof clusters to explain our data well. And to do that,you can vary the number of clusters and watch how well you can fit the data. In general when you add a more componentsto a mixed model you should fit the data better because you, you don't, you can always set the probabilityof using the new component as zero. So you can't in general fit the dataworse than before, but the question is as you add more components would you beable to significantly improve the fitness of the data and that can be used todetermine the right number of clusters. And finally evaluationof clustering results, this kind can be done both directly andindirectly, and we often would like to do both in order to get a good senseabout how well our method works. So here's some suggested reading andthis is particularly useful to better understand how the matchesare calculated and clustering in general [MUSIC]"
cs-410,10,7,"[SOUND] This lecture is about text categorization. In this lecture, we're going totalk about text categorization. This is a very important technique fortext data mining and analytics. It is relevant to discoveryof various different kinds of knowledge as shown here. First, it's related to topic mining andanalysis. And, that's because it has to do with analyzing text to data basedon some predefined topics. Secondly, it's also related toopinion mining and sentiment analysis, which has to do with discovery knowledgeabout the observer, the human sensor. Because we can categorize the authors,for example, based on the content of the articlesthat they have written, right? We can, in general,categorize the observer based on the content that they produce. Finally, it's also relatedto text-based prediction. Because, we can often use textcategorization techniques to predict some variables in the real world thatare only remotely related to text data. And so, this is a very importanttechnique for text to data mining. This is the overall plan forcovering the topic. First, we're going to talk aboutwhat is text categorization and why we're interested indoing that in this lecture? And now, we're going to talk abouthow to do text categorization for how to evaluatethe categorization results. So, the problem of textcategorization is defined as follows. We're given a set of predefined categoriespossibly forming a hierarchy or so. And often,also a set of training examples or training set of labeled textobjects which means the text objects have already beenenabled with known categories. And then, the task is to classifyany text object into one or more of these predefined categories. So, the picture on thisslide shows what happens. When we do text categorization, we have a lot of text objects to beprocessed by a categorization system and the system will, in general,assign categories through these documents. As shown on the right andthe categorization results, and we often assume the availabilityof training examples and these are the documents thatare tag with known categories. And these examples are very important for helping the system to learnpatterns in different categories. And, this would further helpthe system then know how to recognize the categories of new textobjects that it has not seen. So, here are some specificexamples of text categorization. And in fact, there are many examples,here are just a few. So first, text objects can vary,so we can categorize a document, or a passage, or a sentence,or collections of text. As in the case of clustering, the unitsto be analyzed can vary a lot, so this creates a lot of possibilities. Secondly, categories can also vary. Allocate in general,there's two major kinds of categories. One is internal categories. These are categories thatcategorize content of text object. For example, topic categories orsentiment categories and they generally have to do withthe content of the text objects throughout the categorizationof the content. The other kind is external categoriesthat can characterize an entity associated with the text object. For example, authors are entities associatedwith the content that they produce. And so, we can use their content indetermining which author has written, which part, for example, andthat's called author attribution. Or, we can have anyother mininal categories associate with text dataas long as there is minimal connection between the entity andtext data. For example, we might collect a lotof reviews about a restaurant or a lot of reviews about a product,and then, this text data can help us inferproperties of a product or a restaurant. In that case, we can treat thisas a categorization problem. We can categorize restaurants or categorize products based ontheir corresponding reviews. So, this is an example forexternal category. Here are some specificexamples of the applications. News categorization is verycommon as being started a lot. News agencies would liketo assign predefined categories to categorizenews generated everyday. And, these virtual articlecategorizations are not important aspect. For example, in the biomedical domain,there's MeSH annotations. MeSH stands for Medical Subject Heading,and this is ontology of terms, characterize content ofliterature articles in detail. Another example of application is spamemail detection or filtering, right? So, we often have a spam filter to help us distinguish spamsfrom legitimate emails and this is clearly a binaryclassification problem. Sentiment categorization ofproduct reviews or tweets is yet another kind of applications where wecan categorize, comparing to positive or negative or positive andnegative or neutral. So, you can have send them to categories,assign the two text content. Another application is automaticemail routing or sorting, so, you might want to automatically sort youremails into different folders and that's one application of text categorizationwhere each folder is a category. The results are another important kindof applications of routing emails to the right person to handle,so, in helpdesk, email messaging is generally routedto a particular person to handle. Different people tend to handledifferent kinds of requests. And in many cases, a person would manuallyassign the messages to the right people. But, if you can imagine,you can't be able to automatically text categorization systemto help routing request. And, this is a class file, the incomingrequest in the one of the categories where each category actually correspondsto a person to handle the request. And finally, author attribution, as I justmentioned, is yet another application, and it's another example of using textto actually infer properties of some other entities. And, there are also many variantsof the problem formulation. And so, first, we have the simplest case,which is a binary categorization, where there are only two categories. And, there are many examples like that,information retrieval or search engine. Applications with one distinguishingrelevant documents from non-relevant documents for a particular query. Spam filtering just distinguishing spamsfrom non-spams, so, also two categories. Sometimes, classifications ofopinions can be in two categories, positive and a negative. A more general case would be K-categorycategorization and there are also many applications like that,there could be more than two categories. So, topic categorization is oftensuch an example where you can have multiple topics. Email routing would be another examplewhen you may have multiple folders or if you route the email tothe right person to handle it, then there are multiplepeople to classify. So, in all these cases, there are morethan two kinds of categories. Another variation is to havehierarchical categorization where categories form a hierarchy. Again, topical hierarchy is very common. Yet another variation isjoint categorization. That's when you have multiplecategorization tasks that are related and then you hope to kind ofjoin the categorization. Further leverage the dependency ofthese tasks to improve accuracy for each individual task. Among all these binary categorizationsis most fundamental and part of it also is because it's simple andprobably it's because it can actually be used to performall the other categorization tasks. For example, a K-categorycategorization task can be actually performed by using binary categorization. Basically, we can look ateach category separately and then the binary categorization problemis whether object is in this category or not, meaning in other categories. And, the hierarchical categorizationcan also be done by progressively doing flat categorization at each level. So, we have, first, we categorizeall the objects into, let's say, a small number of high-level categories, and inside each category, we have furthercategorized to sub-categories, etc. So, why is text categorization important? Well, I already showed that you,several applications but, in general, there are several reasons. One is text categorization helps enrichtext representation and that's to achieve more understanding of text data that'sall it was useful for text analysis. So, now with categorization text canbe represented in multiple levels. The keyword conditions that's oftenused for a lot text processing tasks. But we can now also add categories andthey provide two levels of transition. Semantic categories assigned can alsobe directly or indirectly useful for application. So, for example, semantic categoriescould be already very useful or other attribution mightbe directly useful. Another example is when semanticcategories can facilitate aggregation of text content and this is another caseof applications of text categorization. For example, if we want to knowthe overall opinions about a product, we could first categorize the opinionsin each individual view as positive or negative and then, thatwould allow us to easy to aggregate all the sentiment, and it would tell us about the 70% of the views are positive and30% are negative, etc. So, without doing categorization, it will be much harder to aggregatesuch opinions to provide a concise way of coding text in some sensebased on all of the vocabulary. And, sometimes you may see in someapplications, text with categorizations called a text coded,encoded with some control of vocabulary. The second kind of reasons is to use text categorization to inferproperties of entities, and text categories allowsus to infer the properties of such entities thatare associate with text data. So, this means we canuse text categorization to discover knowledge about the world. In general, as long as we can associatethe entity with text of data, we can always the text of data to helpcategorize the corresponding entities. So, it's used for single information network that willconnect the other entities with text data. The obvious entities that can bedirectly connected are authors. But, you can also imagine the author'saffiliations or the author's age and other things can be actuallyconnected to text data indirectly. Once we have made the connection, then wecan make a prediction about those values. So, this is a general way to allowus to use text mining through, so the text categorization to discoverknowledge about the world. Very useful, especially in big textdata analytics where we are often just using text data as extra setsof data extracted from humans to infer certain decision factorsoften together with non-textual data. Specifically with text, for example, we can also think of examples ofinferring properties of entities. For example, discovery ofnon-native speakers of a language. And, this can be done by categorizingthe content of speakers. Another example is to predict the partyaffiliation of a politician based on the political speech. And, this is again an exampleof using text data to infer some knowledge about the real world. In nature,the problems are all the same, and that's as we defined andit's a text categorization problem. [MUSIC]"
cs-410,11,4,"[SOUND] This lecture isa continued discussion of evaluation of text categorization. Earlier we have introduced measures thatcan be used with computer provision and recall. For each category and each documentnow in this lecture we're going to further examine how to combine theperformance of the different categories of different documents how to aggregate them,how do we take average? You see on the title here I indicatedit's called a macro average and this is in contrast to micro averagethat we'll talk more about later. So, again, for each category we're goingto compute the precision require an f1 so for example category c1 we haveprecision p1, recall r1 and F value f1. And similarly we can do that for category2 and and all the other categories. Now once we compute that andwe can aggregate them, so for example we can aggregateall the precision values. For all the categories, forcomputing overall precision. And this is often very useful to summarizewhat we have seen in the whole data set. And aggregation can bedone many different ways. Again as I said, in a case when youneed to aggregate different values, it's always good to think about what'sthe best way of doing the aggregation. For example, we can consider arithmeticmean, which is very commonly used, or you can use geometric mean,which would have different behavior. Depending on the way you aggregate,you might have got different conclusions. in terms of which method works better,so it's important to consider these differences and choosing the right one ora more suitable one for your task. So the difference fore examplebetween arithmetically and geometrically is that the arithmeticallywould be dominated by high values whereas geometrically wouldbe more affected by low values. Base and so whether you are wantto emphasis low values or high values would be a questionrelate with all you And similar we can do that forrecal and F score. So that's how we can generate the overallprecision, recall and F score. Now we can do the same for aggregationof other all the document All right. So it's exactly the same situation foreach document on our computer. Precision, recall, and F. And then after we have completedthe computation for all these documents, we're going to aggregate them to generatethe overall precision, overall recall, and overall F score. These are, again, examiningthe results from different angles. Which one's more useful willdepend on your application. In general, it's beneficial to look atthe results from all these perspectives. And especially if you compare differentmethods in different dimensions, it might reveal which methodIs better in which measure or in what situations andthis provides insightful. Understanding the strands of a method ora weakness and this provides further insight forimproving them. So as I mentioned,there is also micro-average in contrast to the macro averagethat we talked about earlier. In this case, what we do is youpool together all the decisions, and then compute the precision and recall. So we can compute the overallprecision and recall by just counting how many cases are in true positive,how many cases in false positive, etc, it's computing the valuesin the contingency table, and then we can compute the precision andrecall just once. In contrast, in macro-averaging, we'regoing to do that for each category first. And then aggregate over these categoriesor we do that for each document and then aggregate all the documents buthere we pooled them together. Now this would be very similar tothe classification accuracy that we used earlier, and one problem here of course to treat allthe instances, all the decisions equally. And this may not be desirable. But it may be a property forsome applications, especially if we associate the, forexample, the cost for each combination. Then we can actually compute for example,weighted classification accuracy. Where you associate the different cost orutility for each specific decision, so there could be variations of thesemethods that would be more useful. But in general macro average tends tobe more information than micro average, just because it might reflect the need forunderstanding performance on each category or performance on eachdocument which are needed in applications. But macro averaging and micro averaging,they are both very common, and you might see both reported inresearch papers on Categorization. Also sometimes categorizationresults might actually be evaluated from ranking prospective. And this is because categorizationresults are sometimes or often indeed passed it to a human forvarious purposes. For example, it might be passedto humans for further editing. For example, news articles can be temptedto be categorized by using a system and then human editors wouldthen correct them. And all the email messages might bethroughout to the right person for handling in the help desk. And in such a case the categorizationswill help prioritizing the task forparticular customer service person. So, in this case the resultshave to be prioritized and if the system can't give a scoreto the categorization decision for confidence then we can use the scoresto rank these decisions and then evaluate the results as a rank list,just as in a search engine. Evaluation where you rankthe documents in responsible query. So for example a discovery ofspam emails can be evaluated based on ranking emails forthe spam category. And this is useful if you want peopleto to verify whether this is really spam, right? The person would then takethe rank To check one by one and then verify whether this is indeed a spam. So to reflect the utility forhumans in such a task, it's better to evaluate Ranking Chris and thisis basically similar to a search again. And in such a case oftenthe problem can be better formulated as a ranking probleminstead of a categorization problem. So for example, ranking documents ina search engine can also be framed as a binary categorization problem,distinguish the relevant documents that are useful to users from those thatare not useful, but typically we frame this as a ranking problem,and we evaluate it as a rank list. That's because people tendto examine the results so ranking evaluation more reflectsutility from user's perspective. So to summarize categorization evaluation, first evaluation is always veryimportant for all these tasks. So get it right. If you don't get it right,you might get misleading results. And you might be misled to believeone method is better than the other, which is in fact not true. So it's very important to get it right. Measures must also reflectthe intended use of the results for a particular application. For example, in spam filtering and news categorization the resultsare used in maybe different ways. So then we would need toconsider the difference and design measures appropriately. We generally need to consider how will theresults be further processed by the user and think from a user's perspective. What quality is important? What aspect of quality is important? Sometimes there are trade offs betweenmultiple aspects like precision and recall and so we need to know for thisapplication is high recall more important, or high precision is more important. Ideally we associate the different costwith each different decision arrow. And this of course has to be designedin an application specific way. Some commonly used measures for relativecomparison methods are the following. Classification accuracy, it's verycommonly used for especially balance. [INAUDIBLE] preceding [INAUDIBLE]Scores are common and report characterizing performances,given angles and give us some [INAUDIBLE] like a [INAUDIBLE] Perdocument basis [INAUDIBLE] And then take a average of all of them, differentways micro versus macro [INAUDIBLE]. In general, you want to look at theresults from multiple perspectives and for particular applications some perspectiveswould be more important than others but diagnoses andanalysis of categorization methods. It's generally useful to look atas many perspectives as possible to see subtle differences between methodsor tow see where a method might be weak from which you can obtain sight forimproving a method. Finally sometimes rankingmay be more appropriate so be careful sometimes categorization hasgot may be better frame as a ranking tasks and there're machine running methods foroptimizing ranking measures as well. So here are two suggested readings. One is some chapters of this book whereyou can find more discussion about evaluation measures. The second is a paper aboutcomparison of different approaches to text categorization and it also has an excellent discussion ofhow to evaluate textual categorization. [MUSIC]"
cs-410,12,4,"[SOUND]This lecture is aboutthe contextual text mining. Contextual text miningis related to multiple kinds of knowledge that we mine fromtext data, as I'm showing here. It's related to topic mining because youcan make topics associated with context, like time or location. And similarly, we can make opinionmining more contextualized, making opinions connected to context. It's related to text based predictionbecause it allows us to combine non-text data with text data to derivesophisticated predictors for the prediction problem. So more specifically, why are weinterested in contextual text mining? Well, that's first because textoften has rich context information. And this can include direct context suchas meta-data, and also indirect context. So, the direct context can growthe meta-data such as time, location, authors, andsource of the text data. And they're almost always available to us. Indirect context refers to additionaldata related to the meta-data. So for example, from office,we can further obtain additional context such as social network ofthe author, or the author's age. Such information is not in generaldirectly related to the text, yet through the process, we can connect them. There could be other textdata from the same source, as this one through the other text canbe connected with this text as well. So in general, any related datacan be regarded as context. So there could be removed orrated for context. And so what's the use? What is text context used for? Well, context can be used to partitiontext data in many interesting ways. It can almost allow us to partitiontext data in other ways as we need. And this is very importantbecause this allows us to do interesting comparative analyses. It also in general,provides meaning to the discovered topics, if we associate the text with context. So here's illustration of how context can be regarded as interestingways of partitioning of text data. So here I just showed some researchpapers published in different years. On different venues, different conference names here listed onthe bottom like the SIGIR or ACL, etc. Now such text data can be partitioned in many interesting waysbecause we have context. So the context here just includes time andthe conference venues. But perhaps we can includesome other variables as well. But let's see how we can partitionthis interesting of ways. First, we can treat eachpaper as a separate unit. So in this case, a paper ID and the,each paper has its own context. It's independent. But we can also treat all the paperswithin 1998 as one group and this is only possible becauseof the availability of time. And we can partition data in this way. This would allow us to compare topics forexample, in different years. Similarly, we can partitionthe data based on the menus. We can get all the SIGIR papers andcompare those papers with the rest. Or compare SIGIR papers with KDD papers,with ACL papers. We can also partition the data to obtainthe papers written by authors in the U.S., and that of course,uses additional context of the authors. And this would allow us to thencompare such a subset with another set of papers writtenby also seen in other countries. Or we can obtain a set ofpapers about text mining, and this can be compared withpapers about another topic. And note that thesepartitionings can be also intersected with each other to generateeven more complicated partitions. And so in general, this enablesdiscovery of knowledge associated with different context as needed. And in particular,we can compare different contexts. And this often gives usa lot of useful knowledge. For example, comparing topics over time,we can see trends of topics. Comparing topics in differentcontexts can also reveal differences about the two contexts. So there are many interesting questionsthat require contextual text mining. Here I list some very specific ones. For example, what topics havebeen getting increasing attention recently in data mining research? Now to answer this question, obviously we need to analyzetext in the context of time. So time is context in this case. Is there any difference in the responsesof people in different regions to the event, to any event? So this is a very broadan answer to this question. In this case of course,location is the context. What are the common researchinterests of two researchers? In this case, authors can be the context. Is there any difference in the researchtopics published by authors in the USA and those outside? Now in this case,the context would include the authors and their affiliation and location. So this goes beyond justthe author himself or herself. We need to look at the additionalinformation connected to the author. Is there any difference in the opinionsof all the topics expressed on one social network and another? In this case, the social network ofauthors and the topic can be a context. Other topics in news data thatare correlated with sudden changes in stock prices. In this case, we can use a time seriessuch as stock prices as context. What issues mattered in the 2012presidential campaign, or presidential election? Now in this case,time serves again as context. So, as you can see,the list can go on and on. Basically, contextual text miningcan have many applications. [MUSIC]"
cs-410,12,8,"This lecture is a summaryof this whole course. First, let's revisit the topicsthat we covered in this course. In the beginning, we talked aboutthe natural language processing and how it can enrich text representation. We then talked about how to mineknowledge about the language, natural language used to express the, what's observing the world in text anddata. In particular, we talked abouthow to mine word associations. We then talked about howto analyze topics in text. How to discover topics and analyze them. This can be regarded asknowledge about observed world, and then we talked about how to mineknowledge about the observer and particularly talk about the, how tomine opinions and do sentiment analysis. And finally, we will talk aboutthe text-based prediction, which has to do with predicting values of other realworld variables based on text data. And in discussing this, we will alsodiscuss the role of non-text data, which can contribute additionalpredictors for the prediction problem, and also can provide context foranalyzing text data, and in particular we talked about howto use context to analyze topics. So here are the key high-leveltake away messages from this cost. I going to go over these major topics and point out what are the key take-awaymessages that you should remember. First the NLP and text representation. You should realize that NLPis always very important for any text replication because itenriches text representation. The more NLP the better textrepresentation we can have. And this further enables moreaccurate knowledge discovery, to discover deeper knowledge,buried in text. However, the current estate of artof natural energy processing is, still not robust enough. So, as an result,the robust text mining technologies today, tend to be based on world [INAUDIBLE]. And tend to rely a loton statistical analysis, as we've discussed in this course. And you may recall we've mostlyused word based representations. And we've relied a lot onstatistical techniques, statistical learningtechniques particularly. In word-association mining andanalysis the important points first, we are introduced the two concepts fortwo basic and complementary relations of words,paradigmatic and syntagmatic relations. These are actually very generalrelations between elements sequences. If you take it as meaningelements that occur in similar context in the sequence and elementsthat tend to co-occur with each other. And these relations might be alsomeaningful for other sequences of data. We also talked a lot abouttest the similarity then we discuss how to discoverparadynamic similarities compare the context of words discoverwords that share similar context. At that point level, we talked about representing textdata with a vector space model. And we talked about some retrievaltechniques such as BM25 for measuring similarity of text andfor assigning weights to terms, tf-idf weighting, et cetera. And this part is well-connectedto text retrieval. There are other techniques thatcan be relevant here also. The next point is aboutco-occurrence analysis of text, and we introduce some informationtheory concepts such as entropy, conditional entropy,and mutual information. These are not only very useful for measuring the co-occurrences of words,they are also very useful for analyzing other kind of data, andthey are useful for, for example, for feature selection in textcategorization as well. So this is another important concept,good to know. And then we talked aboutthe topic mining and analysis, and that's where we introduce inthe probabilistic topic model. We spent a lot of time toexplain the basic topic model, PLSA in detail and this is, those are thebasics for understanding LDA which is. Theoretically, a more opinion model, but we did not have enough time to reallygo in depth in introducing LDA. But in practice,PLSA seems as effective as LDA and it's simpler to implement andit's also more efficient. In this part of Wilson videos is somegeneral concepts that would be useful to know, one is generative model,and this is a general method for modeling text data andmodeling other kinds of data as well. And we talked about the maximum lifeerase data, the EM algorithm for solving the problem ofcomputing maximum estimator. So, these are all general techniquesthat tend to be very useful in other scenarios as well. Then we talked about the textclustering and the text categorization. Those are two important building blocksin any text mining application systems. In text with clustering we talkedabout how we can solve the problem by using a slightly different mixture modulethan the probabilistic topic model. and we then also prefer toview the similarity based approaches to test for cuss word. In categorization we also talkabout the two kinds of approaches. One is generative classifiesthat rely on to base word to infer the condition of orprobability of a category given text data, in deeper we'll introduce you shoulduse [INAUDIBLE] base in detail. This is the practical use for technique,for a lot of text, capitalization tasks. We also introduce the somediscriminative classifiers, particularly logistical regression,can nearest labor and SBN. They also very important, they are verypopular, they are very useful for text capitalization as well. In both parts, we'll also discusshow to evaluate the results. Evaluation is quite important because ifthe matches that you use don't really reflect the volatility of the method thenit would give you misleading results so its very important toget the variation right. And we talked about variation ofcategorization in detail was a lot of specific measures. Then we talked about the sentimentanalysis and the paradigm and that's where we introducedsentiment classification problem. And although it's a specialcase of text recalculation, but we talked about how to extend orimprove the text recalculation method by using more sophisticated features thatwould be needed for sentiment analysis. We did a review of some common use forcomplex features for text analysis, and then we also talked about how tocapture the order of these categories, in sentiment classification, andin particular we introduced ordinal logistical regression then we also talkedabout Latent Aspect Rating Analysis. This is an unsupervised way of usinga generative model to understand and review data in more detail. In particular, it allows us tounderstand the composed ratings of a reviewer on differentaspects of a topic. So given text reviewswith overall ratings, the method allows even furtherratings on different aspects. And it also allows us to infer, the viewers laying theirweights on these aspects or which aspects are more important toa viewer can be revealed as well. And this enables a lot ofinteresting applications. Finally, in the discussion of prediction,we mainly talk about the joint mining of text and non text data, as theyare both very important for prediction. We particularly talked about how text datacan help non-text data and vice versa. In the case of using non-textdata to help text data analysis, we talked aboutthe contextual text mining. We introduced the contextual PLSA as ageneralizing or generalized model of PLSA to allows us to incorporate the contextof variables, such as time and location. And this is a general way to allow usto reveal a lot of interesting topic of patterns in text data. We also introduced the net PLSA,in this case we used social network or network in general of textdata to help analyze puppets. And finally we talk about howcan be used as context to mine potentially causalTopics in text layer. Now, in the other way of using text to help interpret patternsdiscovered from LAM text data, we did not really discuss anything indetail but just provide a reference but I should stress that that's after a veryimportant direction to know about, if you want to build a practicaltext mining systems, because understanding andinterpreting patterns is quite important. So this is a summary of the keytake away messages, and I hope these will be veryuseful to you for building any text mining applications or to you forthe starting of these algorithms. And this should provide a good basis foryou to read from your research papers, to know more about more of allowance for other organisms orto invent new hours in yourself. So to know more about this topic, I would suggest you to lookinto other areas in more depth. And during this short periodof time of this course, we could only touch the basic concepts,basic principles, of text mining and we emphasize the coverageof practical algorithms. And this is after the costof covering algorithms and in many cases we omit the discussionof a lot of algorithms. So to learn more about the subjectyou should definitely learn more about the natural language processbecause this is foundation for all text based applications. The more NLP you can do, the betterthe additional text that you can get, and then the deeper knowledgeyou can discover. So this is very important. The second area you should look intois the Statistical Machine Learning. And these techniques are nowthe backbone techniques for not just text analysis applications butalso for NLP. A lot of NLP techniques are nowadaysactually based on supervised machinery. So, they are very importantbecause they are a key to also understanding someadvancing NLP techniques and naturally they will provide more tools fordoing text analysis in general. Now, a particularly interesting area, called deep learning has attracteda lot of attention recently. It has also shown promisein many application areas, especially in speech and vision, andhas been applied to text data as well. So, for example, recently there haswork on using deep learning to do segment analysis toachieve better accuracy. So that's one example of [INAUDIBLE]techniques that we weren't able to cover, but that's also very important. And the other area that has emergedin status learning is the water and baring technique, where they canlearn better recognition of words. And then these better recognitions willallow you confuse similarity of words. As you can see, this provides directly a way to discoverthe paradigmatic relations of words. And results that people have got,so far, are very impressive. That's another promising techniquethat we did not have time to touch, but, of course,whether these new techniques would lead to practical useful techniquesthat work much better than the current technologies is still an openquestion that has to be examined. And no serious evaluationhas been done yet. In, for example, examiningthe practical value of word embedding, other than word similarity andbasic evaluation. But nevertheless,these are advanced techniques that surely will make impactin text mining in the future. So its very important toknow more about these. Statistical learning is also the key topredictive modeling which is very crucial for many big data applications and we didnot talk about that predictive modeling component but this is mostly aboutthe regression or categorization techniques and this is another reasonwhy statistical learning is important. We also suggest that you learn more aboutdata mining, and that's simply because general data mining algorithms can alwaysbe applied to text data, which can be regarded as as specialcase of general data. So there are many applicationsof data mining techniques. In particular for example, patterndiscovery would be very useful to generate the interesting features for test analysisand the reason that an information network that mining techniques can also be usedto analyze text information at work. So these are all good to know. In order to develop effectivetext analysis techniques. And finally, we also recommend you tolearn more about the text retrieval, information retrieval, of search engines. This is especially important if youare interested in building practical text application systems. And a search ending wouldbe an essential system component in any text-based applications. And that's because texts dataare created for humans to consume. So humans are at the best positionto understand text data and it's important to have human in the loopin big text data applications, so it can in particular help textmining systems in two ways. One is through effectively reducethe data size from a large collection to a small collection with the mostrelevant text data that only matter for the particular interpretation. So the other is to provide a way toannotate it, to explain parents, and this has to do withknowledge providence. Once we discover some knowledge,we have to figure out whether or not the discovery is really reliable. So we need to go back tothe original text to verify that. And that is why the searchengine is very important. Moreover, some techniquesof information retrieval, for example BM25, vector space andare also very useful for text data mining. We only mention some of them,but if you know more about text retrieval you'll see that thereare many techniques that are used for it. Another technique that it's used foris indexing technique that enables quick response of search engine to a user'squery, and such techniques can be very useful for building efficienttext mining systems as well. So, finally, I want to remindyou of this big picture for harnessing big text data that I showedyou at your beginning of the semester. So in general, to deal witha big text application system, we need two kinds text,text retrieval and text mining. And text retrieval, as I explained,is to help convert big text data into a small amount of most relevant data fora particular problem, and can also help providing knowledge provenance,help interpreting patterns later. Text mining has to do with furtheranalyzing the relevant data to discover the actionable knowledge that can bedirectly useful for decision making or many other tasks. So this course covers text mining. And there's a companion coursecalled Text Retrieval and Search Engines that covers text retrieval. If you haven't taken that course,it would be useful for you to take it, especially if you are interestedin building a text caching system. And taking both courses will give youa complete set of practical skills for building such a system. So in [INAUDIBLE]I just would like to thank you for taking this course. I hope you have learned useful knowledgeand skills in test mining and [INAUDIBLE]. As you see from our discussionsthere are a lot of opportunities for this kind of techniques andthere are also a lot of open channels. So I hope you can use what you havelearned to build a lot of use for applications will benefit society andto also join the research community to discover newtechniques for text mining and benefits. Thank you. [MUSIC]"
cs-416-dv,1,1,"Welcome to Data Visualization. My name is John Hart, andI'm a Professor of Graphics and Visualization in the Computer ScienceDepartment at the University of Illinois at Urbana-Champaign. And I've been working indata visualization for over three decades on a variety of topicsranging from mathematical visualization, to visualization of large databases,to visualizations of graphs and other relationships andnetworks to many other things. So in Week One, we'll be focusingon database visualization, on connecting a visualization systemto the data in a large database in a data warehouse andsome very large database system. And the database semantics,the query structures, and other nuances of working with a databaseto get the data out of the database into the visualization system sothat you can visualize it. In Week Two, we'll focus on connectingthat visualization system to the user. We'll start by looking at human perceptionand how we understand graphics, how we read a chart, how we extractinformation from a data visualization. We'll also look at how togenerate the graphics of a chart. In Week Three, we'll look at the semanticsof a chart itself so we can make sure we're speaking in the correct chartlanguage to communicate visually. We'll focus less on quantitative data,and more on relationships, the relationships found in networks orgraphs. Ways of displaying data that doesn'thave numbers associated with it. And in Week Four,we'll create dashboards of charts. And so we'll look at how to connect onechart to another chart interactively so that we can find out information by connecting pieces of data acrossmultiple charts in a dashboard."
cs-416-dv,1,2,"The point of datavisualization is to be able to display datato answer a question. The first step of that process is finding the appropriate datato answer the question, and then once we'vefound that data, we can then figureout how to display it to help the observersee the answer. So we'll spend the first weekfocusing on how to access data and process it intoa form we can visualize. We'll start by accessing a large dataset thatholds many answers to a lot of questions about the well-being ofeveryone in the world. We'll be usingthe World Development Indicators Datasetfrom the World Bank. You can download the datasethere from wdi.worldbank.org, which gets expanded to this. This dataset has loadsof world economics data, and can help us answer questions about each country's development. The data is available in a text format as commaseparated values, which can be downloaded here. The dataset is quitelarge and text is not a very spaceefficient storage format. So it's compressed as a zip file. That dataset will unzip toover 250 megabytes with over eight million facts about 263 differentcountries of the world, measured over half a century."
cs-416-dv,1,3,"Now that we have a data set, the next step is to prepare the data for visual processing. This involves data cleaning to make sure the data isinterpreted properly. We'll also look atthe organization of the data using the semanticsof a data warehouse. So we'll treat thedata as a table of facts or measures acrossseveral dimensions. Now that we have downloaded the World DevelopmentIndicators data set, we can use it to find the answers to any questionswe might have. For example, we mightwant to know what is the current population ofthe world and its countries, and how does the populationchange over time. First, we need to connectTableau to our data, in this case, the WDI data file. This data is organized as a comma-separatedvalue text file. Each line is a record, and each field isseparated by a comma. Quotes are used in casethe field contents include a comma characterso that the comma wouldn't be misinterpretedas a field separator. This file has three weirdcharacters at its beginning, specifically hex EF BB BF, which indicate that this is a UTF-8 compliant textfile for Unicode. The first line consistsof the field names. We can indicate to Tableauthat the first line holds the field names byselecting this menu item. The rest of the linesare the records. Many records havenull entries in each field, indicated by empty quotes. These are missing values anddifferent than zero entries. The lines in this filehave a trailing comma. This causes Tableau to expect the last field thatdoesn't exist, so we can simply hide that field. This data set is quite large: 422,136 records andabout 10 million data items. This is fine for Tableau Desktop, but too large for Tableau Public. So we can reduce this data set by selecting the itemswe're interested in. This data set hasa separate record for each item, for each country, identifiedby the indicator code, and the indicator name. The World Development Indicatorsare organized in a very common structurefor data warehouses, and that structureis a list of facts, a list of measures that are quantitativeacross dimensions. We can see what these look like, these indicators listed as facts in the structureof this database. Tableau Public can'thandle a lot of data, so let's reduce the amountof data we have for a moment so we can takea look at these indicators. If we select years 1960 to 2009, we can hide that, and then, we'll also hideyears 2011 to 2018. That gives us one yearand an amount of data that Tableau Publiccan handle easily. Then, if we go to our worksheet, we can take a look atthe indicator names by just dragging them into a row. There's a lot of indicator names, and Tableau won't be ableto display all of them easily on our screenfor visualization, but they can be displayedso that we can take a look at what theseindicator names look like. Now, this is going todefault to a sorted order, and you can change theorder that they appear. They can be in data sourceorder or sorted order, alphabetic, manual order. But this is the order thatthey're listed in a database, and it happens tobe sorted order. That order doesn't make toomuch sense because you've got different indicatorsnext to each other based on theiralphabetic ordering. There's also an indicator code, and so each indicator namehas a corresponding code. These indicator names can change from database to database, whereas these codes are specified so thatthey don't change, they mean the same thingfrom database to database. So for tutorial purposes, for illustration to betterunderstand what we're doing, we'll often do things like, we'll filter accordingto the indicator name, and then, we can just selecta specific indicator. But if that indicator's namechanges in a later database, then this view won't work. So it's always better tofilter based on the codes. That way, if the name changes, then the code will stay the same, and your views willremain the same, and they'll be more robust across changes in the data set. So for each of these indicators, we have a value measured in 2010. Some values aren't measuredin 2010, but some are. This is the sum which doesn't really tellus much of anything. A better indicator of what's been measured is to do a count, and that's the numberof records that have a non null value. So you can see there aresome things like the number of tractors wasn't measured in 2010, and we don't haveany records that are non-null in 2010 for that, but we have many recordsfor other measurements. So we can see each of these indicators hasa certain number of facts associated with it in the data set I recordedin the warehouse. Since we have so much data, we'll go ahead and filter the data based on the item name. Here is Indicator Name. We can drag IndicatorName to Filters, select from a list, and then selectthe specific item code we're interested in, in this case, total population. Can see a bunch ofdifferent population variables measured in this data set, and we're interested inthe total population. Here it is. We can then tryto answer the question, what is the currentworld population? A Tableau worksheetis set up like a spreadsheet withrows and columns, but designed to outputthe rows and columns as visuals instead of just a spreadsheet arrayof numbers and text. In this data set, the values for each indicator arestored for each year. The most recent year withpopulation data is 2017. If we drag 2017 over to the rows, we can see a bar chartwith one bar indicating the sum ofall population entries. This sum showsalmost 80 billion entries. If we right-click andselect View Data, we can see why this is the case. We select Full Data. We can see that we haveall the countries here, but we also have, for example, the World, and this is a sum for indicatorsfor the entire world. In fact, there areseveral regions in here as well, and all of those are gettingadded up in the sum. So to get the world population, we can filter bythe country name, and just select the World. Now, we see thatthe world population, as of 2017, was about sevenand a half billion people. So before we create a chartto visualize our data, it's important to first ensure the veracity of the data to make sure the chart accurately represents the factsof the data set."
cs-416-dv,1,4,"Once we have a clean data set, we will want to reshape thedata into a fact table. Our file of facts of the worldis going to consist of a long list of singlevalue measurements, and these measurementswill be taken across multiple dimensions. For example, the year, the country, and the thingthat we are measuring, such as the populationof the country or the income of one ofits average citizens. In order to reshape thedata in this list of facts, we will need to utilizea technique called pivoting. So now we can askanother question: How has the populationgrown since 1960? The structure of thisdata set makes it difficult to comparemultiple years, because each year isin a different field. However, Tableau hasspecial fields called Measure Names and Measure Valuesthat make this easier. If we drag Measure Valuesup here to the rows, we get a new shelf. This shelf shows what values are incorporated intothe Measure Values, and we get a stackedbar chart here adding up all theworld populations over the different years. We can then drag the MeasureNames into the columns, and we get a disaggregation across these years,across these fields. This is only goingfrom 1916 to 1979, so we can go here and select of all the fields thatwe're interested in, drag those over toMeasure Values, and now we get the growth of the world populationfrom 1960 all the way to 2017 measuredas a bar chart, with each bar showinga single year's measurements. So Tableau allows the user to treat multiple fieldsas a dimension, but this is oftenawkward and cumbersome, and there a better way. If we go back to the data source, we can select all of the year fields holdingindicator values. So we can select from1960 all the way to 2018, and then select Pivot. A pivot is a transposeoperation on the table in a databaseexchanging field data, horizontal field data in the rows with vertical recorddata in the columns. In this case, a single record with multiple fields one for each year of measured data is replaced by multiple records, with each record holding just holding a singleyear's measurements. This creates new fields, a field calledthe Pivot Field Names, and that holds the fields, in this case thecorresponding year, and then the Pivot Field Values, the values stored in that year'smeasurements values. We can renamethese fields to have them be more appropriatefor our data set. So in this case, we can rename the Pivot FieldNames to simply the year, and then PivotField Values we can rename that just to value. Now the field valueholds a measure of the records particular indicator measured at the record'sparticular year. We can then move back to the worksheet and useTableau in the way it was intended to create 2D visual plots froma selected table of data. Since this is a large data set, we will have to selecta particular indicator to filter it down into something that Tableau Public can handle. Tableau Desktop can handlea whole data set but Tableau Public handlessmaller amounts of data. So we set the filterto limit our data to just the totalpopulation data setting the indicator name justto population, total. We can also do this withthe indicator code, but we'll do it withthe indicator name just to make it easy to see. So now we have population and we no longerhave a long list of years, we now have yearin our dimensions. So we can select year and have that goacross the columns, and then have a value,in this case, the population ofthe world in our rows, and we get that samebar chart measuring the world population as itgoes from here to here. By pivoting, wereshaped our data set. Pivoting converts a cross tabof measurements where each year measurementwas stored in a separate field ofa single record. We convert that intoa fact table of data, where the year becomesanother dimension and that single recordbecomes multiple records, one new record for each year field inthe original record. Reshaping the data intothat fact table with year as a dimension makes it easier to query to visualize trends overtime in the measurements."
cs-416-dv,1,5,"Databases are often organizedinto multiple data tables, often with relations thatconnect records from one table to correspondingrecords in another. These relations can often provide further information importantto visualizing the data. Here we're lookingat the population of various countries. We're looking at thepopulation data from the year 2017 recorded under the total populationindicator name. So we're plotting the countryname in the rows, and then we have these bar charts indicating the sum ofthat value for the population. As we scroll down, we see that we notonly have countries, we also have aggregationsof country data. So these aggregationof country data are listed in the dataset alongwith these countries, but they don't necessarily give us what we'relooking for if we're looking for just a comparison of the populationsof each country. So we can differentiate between actual countries inthese aggregations of countries by going back to the data source andloading another table. That table is WDICountry. So when we open that table, it's automaticallylinked to WDIData. We can see what thatlink looks like here. That link is throughthe country code field. There's a country code field in the WDIData file and also a country code fieldand WDICountry, and so Tableau is automaticallycreating a relation between thesetwo tables by matching entries in these two fields. We can better understandhow to create a relation to join two tables togetherwith a simpler example. Here I've got a simplifiedexample with two tables, a population tableand a currency table. So the population table has three countries: Aruba, Burundi, and the Central African Republic, and then it liststheir populations as of 2017. I've got a second table here, and that's a table of currencies. Here, we also have country codes inthree different countries, Aruba, Burundi, butnow we have Germany. We no longer havethe Central African Republic, but we have Germany, and then we can listtheir currencies. So now if I bring over the populations and I createa relation with currencies, that relation is created by the shared country code field, because both tables have the same country code field and corresponding entriesin that field. But we have our choicebetween an inner join, a left join, a right join, and a full outer join. The inner join onlyresults in two records. That's because the innerjoin only results in records that have correspondingentries in both tables. The left join looks for corresponding entries inthe first table population, and then it willreproduce any entries in currency withthe corresponding country code. In this case, theCentral African Republic is in the population table, but not in the currency table, and so the currency entries are null in this resulting recordof the left join. In the right join, wehave the exact opposite. Germany is in the currency table, but there'sno corresponding entry in the population table, and so that just results in no. You'll notice thatthe Central African Republic does not appear inthe right join. In the full outer join, we have any recordsfrom either table, but we have nullentries if there's missing informationfrom both tables. So what kind ofjoin do we want to use to connectthe country code in the WDIData table to the country code inthe WDICountry table? The answer is a left join. Because the WDICountry table provides further dataabout each country. But we only care aboutthe information for the country we'rereferencing in the WDIData. If the WDICountry tableincludes a record for a country that wasnot included in WDIData, then we would want toignore that country. However, if the WDIDatatable referred to a country that wasn't listedin the WDICountry table, then we wouldn't want toignore that data just because we didn't have the extra information on that country. So now we can joinour data with our country information so thatthe country code that's used for the WDIDatacan also refer to the country code that's a fieldin the WDICountry table. So if we go to our worksheet, we can see both theWDICountry fields are available and the WDIDatafields are available, and we've pivoted our year and value as we described before. So I can list the country names, and here we haveall the country names, but you'll see that some of these countries are aggregations. So if we go to WDICountry, I can look at the region fieldof the WDICountry table. So if we bring that here, each of our country nameshas a corresponding region, and you'll notice thatthings like Arab world, which is an aggregationof the Arab countries, doesn't have a region. So if we flip these around, Tableau willautomatically cluster these so that you can see all of the country namesthat have a null region, and then actual countries here have a corresponding region. So you can filter out all of these aggregations by setting a filter that excludes null, and so now that'sset as a filter. So now what we're left with are the actual countriesof the world. Now if we don't want tolook at the regions, we can delete regions since we've already setthat as a filter. So we can removeregion and we just get an alphabetical listof our countries, and then we can lookat their populations. If we want to lookat their populations over a given year, we can set the year in thefilter to be for example 2017, and then the indicator name we'll want to lookat the populations, so set the indicator nameto population, total. Then we can plot thepopulation as of 2017 for each of these countries and dragging its valueinto the columns, and you can see the variancein the populations. You might want to be able to sort on the populations so we can find out what are the mostpopulated countries and the leastpopulated countries, and you can see that Chinaand India are outliers with a lot of people and thenthere's this long tail. So by joining multiple tables, we're able to use the region information fromthe WDICountry table to help us filter and group countries and their measurements fromthe WDIData table."
cs-416-dv,1,6,"Another important toolto preparing data for visualizationis aggregation. We can't display all the data at once because it istoo overwhelming. So we use aggregation asa way to combine lots of measurements into a simpler form that summarizes the data. Aggregations areprojections of measurements across one or more of thedimensions of a data set. So the way we're going toreduce the dimensions of the data set is going to be largely with data aggregation, and there's a large number of database visualization tools based on dataaggregation as a way of simplifying a data set andreducing the dimensions of variation by providingsummaries of the data. So for example, here's some data. We're plotting some quantitativevalue vertically as we change some otherquantitative value horizontally. We got an independent variable here and the dependentvariable here. So this is atwo-dimensional plot. We could always project thisonto a one-dimensional axis, and we can alsosummarize this data. These would be values. These would be measureschanging across one axis into a single value by representing it by the mean value orthe maximum value, the minimum value or thesum of these values. So we have variousoperators: sum, mean, median, minimum, maximum. There is otherstatistical operators like standard deviationor variance or other characteristicsyou can use that will simplify your data andbasically remove one dimension, in this case, removingthe change in value over one dimension to a singlevalue over no dimension. We can also convertcategorical data into quantitative datausing a count function. For example, if wehave these blue, red, and green data pointsas three categories here, they're being plotted over two-dimensional areathat's going to represent continuousquantitative data horizontally and vertically. We can then remove those axes and just have a single axisrepresenting the count, the number of blue items, the number of red items, and the number of green items, and that reducesthe dimensionality of this data set from varyingover two dimensions with a third category into just a category overessentially one dimension. Finally, there's binning. Binning is adiscretization method that converts quantitativecontinuous data into quantitative discrete data or ordinal or nominal datadepending on how you use it. For example, we may haveour continuous function. These are values that are varying over a single dimension, and instead of having these values representedcontinuously over a continuous dimension, we can discretize this dimension. Then we havea continuous variable, quantitative variablethat's being sampled on four regions and represented by the mean value overthose four bin areas. So we're doing a projectionoperator over, in this case, over four areas insteadof over the single area, but again that does reduce the dimensionalityof the data set. Finally, we can use thisto create a histogram, and the histogram isbinning, in this case, in the vertical direction instead of the horizontal direction. So instead of havingbins over the dimension, you're having bins over the value that you're plottingover that dimension. In this case, we've separatedthe values into an a, b, c, d, and e range. Then we're adding upall the values in the a range, all the values in the b range, all the values in the c range, and all the values in the d range and just plotting those sums. There's no values in the e range, so there would bezero for the e range. When you choose these buckets, the size of these bins gives you other characteristics ofthe data that can reduce the dimensionality ofthe data or at least reduce a continuous variable into discrete categories ofa continuous variable. We can demonstratethese aggregations using our population data. Here we have world populations by country listed by country code and the correspondingcountry name, and we're looking ata sum of the value. In this case, the value is total populationin the year 2017. Each of these entriesis just a single entry. So we're seeing an aggregation, the sum, but there's only one element ineach of these sums. So we could use attribute. Attribute's a helpful functionbecause if any of these populations correspondedto two records or more, we would see an asterisk, and that attribute aggregation helps ensure thatwe're only seeing one element in the database for each of those populations. We can switch back to sum. We can also pull in the region of the countries fromour country's database. This will organize ourcountries by region and null region corresponds to these predefinedcollections of countries, so we'll exclude that. That creates a new filter thatexcludes that null region. Now we have a countrieslocated by region. Now if I delete country codeand country name, this sum operator now adds up the total population in 2017of each of these regions, and if I switch tothe attribute function, we will get asterisksbecause there's multiple values going in here. So in fact we reallywant an aggregate sum. We can also look, for example, what's the average population of a country in each region? This gives us the averagepopulation of each country in the region. Aggregations are usedto simplify data, they project the dataacross one of the dimensions reducingthat dimension from the data. Tableau includes some controls over which dimensionsthat projection occurs on. So in this example, we've got the countries of the world and their populations. Alongside, I've also got their regionalaverage population, and that regionalaverage population was computed usinga calculated field. This calculated field shows that we're computingthe average of the value, which is filtered to bejust the population values, but we've fixed the region, the year, and the indicator name. So it's only goingto be populations, it's only going to befor the current year, and it's only goingto be for the region. But I haven'tmentioned the country, so Tableau will average the population ofeach country for each region, for each year, foreach indicator name, in this case, population. That's split here alongside the values ofthe actual populations. So you could usethese in formulas to have some deriveddata subcalculation, for example, how each population compares to its regionalaverage population. Similarly, we may want to have a 10-year average ofa population of a region. Now the populationof each region is the sum of the populationsof the countries. So we need a sum. So if I just look at the sum of the values of the populations, I get the sum of the populations of each country overthose 10 years, which gives me10 times the population of the region becauseI'm adding up not only the populationof each country but the population of each countryeach year for 10 years. So these values are too large. If I compute the average value, that gives methe average population each year but also the averageof each countries. So this gives mea 10-year average of each country's population. What I really wantis something that combines the sum and the average. In this case, I'm computing the total sum of every country in each region and thentaking that total sum of all the countries, populations in each region and averagingthem over 10 years, and I get the proper10 year average of a region's population. In order to do this, I need an intermediate calculationcalled regional total. Regional total is a formula that includes the yearand its aggregation. So I'm summing the values, and this will sum across all the values in the country but because I'm includinghere in this aggregation, it's only including the countries populationsfor the year. That way when I take that sum of all the countries populations within that region for each year, the average gives me the average population of that region for eachof the 10 years. As we can see, when the numberof dimensions gets large, these projectionscan get complicated, for example, findingthe average over several years of the sum of the population ofall the countries in a region. Tableau's level ofdetail operators, like include, exclude, and fixed, are designed to allow different aggregationsacross different dimensions."
cs-416-dv,1,7,"In 2013, a couple of economistsChristoph Lakner and Branko Milanovic published a dataset that was very controversial and raised a lot of attentionabout world economics and the world income distribution. rThis data set Resulted inthis elephant chart and this elephant chart this thiselephant shape chart shows the increase in income from 1988 to 2008. Of the world income earners,where those world income earners are separatedin the percentile groups. So here we have the bottom 5%, the poorest5% of the population of the world. Here we have the top 1% populationof the world in terms of income, and then you have the range ofthe world's population based on the percent of incomethey receive each year. And what this chartshows from 1988 to 2008, is at the bottom 5% of worldearners made less than 2008 than in 1988 asthe income groups grow from the bottom 10% allthe way up to about 75%. Those owners made more in2008 than in 1988, and the top 1%, the top 5%,made more in 2080 and 1988. But the owners from aboutthe 80th percentile to the 95th, 99th percentile made less. And this has been characterizedas the world middle class. And so what this actually represents,you can find out by creating your own elephant chart andlooking at the data more closely. So in order to createyour own elephant chart, you'll need to access this data andreformulate it so you can see the growth per percentilein order to create this elephant chart. I've used Tableau toreformulate this data, but you can use any tool you like to do this. You could do this in Microsoft Excel,or a Google sheet, or Python orany other tool that you're familiar with. So here, I've loaded the data set,it's organized with several fields some dimensions some measures andwe have been year here. I've shown the data using been year 1998which is in the middle of the data set. The min years are every fiveyears from 1988 to 2008 so you can see what these field's mean bylooking at the metadata of the data set. So there is a document here which is the World Panel Income Distributiondescription. And so, if we go to that pdf file, we can see a table that hasa description of each field. The most important field is RR income,this is the average per person income in uniform dollarscalled 2005 PPP, a uniform monetary unit. And it's the per capita income forevery person in a particular decile group. Bottom 10%, next 10%, all the way upto the top 10%, for that country. And so this is the raw data thatyou're given for each bin year, 1988, 1993, 1998, 2003, 2008. And then we want to look at the totalpopulation of the country, but more specifically, we want to look at the population ofthe decile group of that country. And so the population will be10% of the total population, and so this population of the decile, means you've got this much people,each making this much income. And we can assemble those fieldsto find out how many people are making how much money inthe bottom 5% the next 5% and so on all the way to the top 5% ofearners world wide in a given bean year. The last field I want to talk about hereis my sample, and we'll set my sample equal to 1, meaning that we are lookingat separate income distributions for large countries like China,India and Indonesia. So using those fields, we can setmy sample equal to 1 as a filter. And we can set the bin year to 1998right in the middle of the distribution, and then I can display these fields. So for each country,we have 10 decill groups using the group variable from 1 to 10. And that means group one of Albaniameans you have this population, this should be multiplied by 1 million. And so, for example,the population of the United States in 1998 was about 275 million people. And so 10% of that would be 27 anda half million people, and so this is a decimal representinghow many millions of people. It doesn't really matter whatpopulation is in terms of scale, as long as it's population relative toall the other measurements of population. And then we have RR income,and here we have 687, that means the bottom 10%of earners in Albania made an average of 687,2005 PPP of effectively dollars. All the way up to 4126 forthe top 10% was their base income. These 2005 PPP valuesare effectively US dollars in 2005, so these would be the equivalentof 2005 US dollars. And so the top 10% of the Albanians, still only made an averageof about $4000 each year. And so, we've grouped these valuesby country and then by decile, indicated the population, andsorted by increasing income. And that's automatically the sameordering by the income decile. So what we can do now isthe same organization, except we're gong to add a copyof this income to the left to remove the sorting andorganization by country and group. And this allows us to sort by RR income atthe highest level of the data hierarchy. And so now we can see foreach decile who the lowest earners and the highest earners were in 1988. So for example, Angola the bottom10% of earners earn $45 a year, and the highest earnerswere in the United States, the top 10% earned an average of $56,000 a year in 1998. But we don't want to sort thisby how much each each decile is earning relative to each other. We want to sort this by world population,and so what we don't want the bottom 5% ofearners based on how much they're making. We want the bottom 5% of people, of the total population basedon what they're earning. So in order to do that, we need toconstruct a running sum of the population, and sohere we're using a table computation. And this table calculationis computing a running total of the sum of the population,based on this sorting. So as we sort based on annual incomeper decile per country, we're keeping a runninng sum of the total popullationthat earns that much per year or less, and that's what we'll useto construct our percentiles. And so we divide these upin the quintiles, and so what we want to look at is the first 5%,the second 5% and so on. So to do that,we use this quintile variable, and this quintile variableis a calculated field, it's another table calculation, andwe can see what that calculation is here. It's basically a running sumof the sum of the population divided by the total sumof the population, and then quantized into 20values going from 0 to 19. The very top will end up being 20,and so we clamp this to 19 here, but this basically gives usthe percentile of where each population group isquantized in 5% quintiles. And sothen we can output this data per quintile, since these are quintiles,these are population quintiles. So the bottom 5% will represent the samenumber of people as the second 5%, all the way up to the top5% earning population. But we can then compute the averageof their annual income per quintile. We can store that as a cross tab oras a record and then you can do that forevery quintile for the years 1988 andthe years 2008 for bean years. And then you can look at the differencesof those as a percent of growth of 2008's income versus 1988's income,and plot your own elephant curve. In order to plot your own elephant chart, you'll need to create these incomequintiles for 1988 and 2008. For each of these two years, yourquintile data will just be 20 numbers, the average income foreach of the 20 quintiles for each year. Then your elephant chart is justthe percentage of each quintile's average income increase ordecrease from 1988 to 2008. Once you've created this chart youcan use it to find out for example, what income levels representedthe quintiles that decreased from 1988 to 2008, or which ones increased. You can also plot the elephant charts for different collections of countries to seeif any of the effects vary by region."
cs-416-dv,1,8,"We've been using Tableaufor data visualization, by dragging fieldsto create charts. Now, will look intothe details of how Tableau convertsthese field dragging operations, into the database queriesthat generate the data to createthe resulting chart. By understandinghow Tableau works, you'll not only be able touse it more effectively, you will also know how toformulate database queries, to get a desired chart without needing Tableauto do it for you. Tableau is based on an earliersystem called Polaris, which was an experimental system published at Infovis in 2002, and it won the Best Paper Award. Pat Hanrahan and I spokeat a conference in 2005 together and hadsome time to talk about this, and he explained thatPolaris and Tableau. We're less about improving the visual presentation of data. The real impact of this work, was was more about improving the user interface into the data. Polaris and Tableaudon't introduce any new visualization techniques, they're justgenerating bar charts, and line graphs, and standardvisualization chart types. The real revolutionof Polaris and Tableau is the easieruser interface. The more effective userinterface to connecting the user to the data through a visual mediumthrough visual charts. It was built ona very standard approach to accessing databasescalled pivot tables, where users would selectrows and columns, and by switchingrows and columns, the user would be changing the presentation of thedata in an ordinary table. They generalize those tables,those pivot tables, into a variety of plots, bar charts, linecharts, and so on. So the elements of Polarishave continued in Tableau, are a connection toa database, a database schema. This database schema isbasically the fields separated into measuresand dimensions, and there are stillaccess shelves. You have basically,a two-dimensional table. You have rows and columns, and we don't try to doanything in higher dimensions. Two dimensions is a niceintuitive space to work in even though what we're looking at in this visualizationis four-dimensional, it's still presented assomething two-dimensional. You also have various shelves, various locations to drag fields dimensions in order to change the data that'sbeing presented. These shelves can helpgroup and sort the data, and they can control which fields are beingplotted horizontally, vertically, and which fieldsare being encoded as marks. Those marks caninclude color, size, shape, and many othersin Tableau. In order to do this, Tableau and Polaris before it, was making a direct connection between a visual presentationof the data designed by the user andthe ultimate query that the user is generatingto access that data. That approach is called VizQL, which is an integralpart of Tableau, but came about through Polaris. It's based on this frameworkthat works directly from chart designedto database query. Before Polaris, andfor pivot tables, and for generaldata visualization of spreadsheets andother databases, there would bea two-step process, where the user wouldgenerate a query and assemble all the datafor the visualization, and then in a second step take that data and create a visualization fromthat query result. This QL is designedto go directly from the specification of a chart designed to answersome questions about the data, that the user conceives of some chart that willanswer that question, and then startsdesigning that chart by dragging fields, measures, and dimensions around andnear the appropriate places, then that chartgenerates the query automatically to be able to populate the data andthese databases are large. We're talking about big data and an interactiveuser interface. So to design thingsfor interactivity, for an effective user interface, this QL needs to be fast. So this Q0 was implementedas a declarative language. A declarative language is a programming language whereyou describe the result, and then the compiler goes and constructs the commands inorder to produce that result as opposed toan imperative language where you're telling the computer step-by-step howto create the result. A declarative language, another declarative languagemight be HTML, where you'redescribing a web page almost as if you arewriting a program, but that programisn't going from top to bottom necessarily in order. It's basically describingthe element of a page and that specification is then being converted into another set of instructionsby a compiler, by an HTML renderer that will then generatethe instructions. VizQL similarly, isdescribing a result, a chart of some sort, a visualization of the data,and then it's goingthrough that specification or the visualization of the data and generatingthe resulting commands, in this case, SQL in order to generate the data neededto produce that chart. So here's an example, chart. This is generated and Tableau. It plots four countries Aruba, Burundi, the CentralAfrican Republic, and Germany, and its plotting as a bar chart it'spopulation as of 2017. We can also exportthis data as a cross tab, and this gives you a picture of the internal data that's beenassembled for this chart. In this case, wehave country name, and then its population. So an intermediaterepresentation is this cross tab thatTableau holds, that ordinarily wouldwithout using Tableau, we might have togenerate a query that generated that cross tab data. We may then have togenerate the visualization, whereas Tableau and Polarisand other systems like that, allow you to construct the visualization directly and automatically generate the query. You can also describe this sheet. So if you describe the sheet, you'll find out that 2017 is an attribute of value attributefor each country name, and that the indicator nameis population total. So that's the actual value that's being storedin the field 2017, because we're filteringonly the records that have indicator nameof population total. The country code is also filtered for justthose four countries. So that gives youanother representation of the query that's going onwritten out in plain English. If you go into Tableau's logs, you'll see the actual querythat's generating the data for this visualization. So this QL inside Tableau, is taking thatspecification by dragging those fields country name2017, country code, and indicator name,into those shelves, in the process of draggingthose fields into those shelves that automaticallygenerating a bar chart, tableau is generatingthis SQL code in order to do the query to populate that chart withthe appropriate data. So it's a select commandfor an SQL query. That is selectingcountry name as a field, and then it's selecting 2017 in two fieldsas temporary fields. In this case, it's bindingan aggregation a minimum of the 2017 field and amaximum of 2017 field, that's because we'replotting an attribute. We're expecting justone data item for each entry in this bar chart and my queryingthe minimum and maximum, we can compare those tomake sure we're only getting one data item, and may be repeated, but it's the same data itemif it's repeated. We're querying our WDIData,separated value database, and also another temporarydatabase that the compiler might produce tooptimize the query. We have a WHERE clause that'simplementing the filtering. We're filtering 'Country Code'to these four countries, and we're filteringthe 'Indicator Name' to population total. So Tableau converts charts specifications intodatabase queries. As each field isdragged into place, that field becomespart of the query, which part ofthe query depends on which chart elementthe field is dragged to."
cs-416-dv,1,9,"Tableau creates charts by dragging fields intochart elements, but the charts it createsdepends on what kind of fields are dragged tothe chart elements, and also how many of the fields are draggedinto each chart element. So Tableau is designedto be able to allow the user to dragfields, dimensions, and measures into the rows and columns of a table and also in the filters and the marks of a table in order to generatea desired visualization, in order to expressthe kind of question that they want the data to answer throughthis visual metaphor. In order to do that, Tableau needs toknow how to generate the query that produces this data and also how to lay out the data in order toanswer the question. In order to do that, Tableau and Polaris before it, were built onthis grammar of graphics, which is a book writtenby a Statistician, Leland Wilkinson on how to take data and based onthe type of that data, figure out the appropriatechart to communicate the data. So a grammar tells youhow to speak properly. The Grammar of Graphicstells you how to find the correct chart to properlycommunicate your data. The fields in database maybe quantitative or ordinal. Quantitative fields are numbers. Ordinals are non-numeric. They may have an orderor they may be just simple categories thatare generally unordered. But even an unordered categorycan still have an order, either alphabetical oreven in database order. So we'll just call them ordinal. Quantitative tendsto be continuous, not always, and ordinaltends to be discrete. These fields whether they'requantitative or ordinal can influence which graphic is used in order to communicatethe data as a chart. If the axis is quantitative, then it's a plotting axis. If the axis corresponds toa field that's an ordinal, then it's a table axis. So if we have two ordinal axes,we have a table. For example, here, is a tablewith two ordinal axes, a country name: Aruba, Burundi, Central AfricanRepublic, or Germany, and a region: Europe,Latin America, Caribbean, or Africa,Sub-Saharan Africa. In this case, wehave one entry per column correspondingto the region. But this is an exampleof a table that you get from two ordinal axes. You can also havetwo quantitative axes. For example, if we plot life expectancy versustotal population, both of them quantitative, four are four countrieswe get a scatter plot. Because that's theappropriate plot we would want to use fortwo quantitative axes. Then, finally, we could have a quantitative axisand an ordinal axis. In this case,the quantitative axis is the populationmeasured in 2017, across the ordinals ofthe four country names. In this case sortedalphabetically, and we get a bar chart. That's the proper wayto communicate these populations forthese four countries. So we can describe howwe went to formulate an appropriate tableto communicate the data based onthe type of data, and use that todescribe the query that will access that dataand produce the answer. To do that, we usean Algebra, a table algebra. So this table algebrashows us how to combine more informationinto each axis. Instead of just one itemin each axis. To understand this tablealgebra, we'll use variables. A or B correspondsto an ordinal field, and a P or a Q will correspondto a quantitative field. So if we want to concatenate two ordinal fieldsinto a single axis, we would say A plus B. That means all ofthe elements of field A, plus all of the elements offield B get concatenated into all of theelements of field A followed by all ofthe elements of field B. You could similarly concatenate a quantitative fieldto an ordinal field. In that case, you wouldhave all of the elements of the first field plus a referenceto the second field. In each record, you'reonly going to have a single value ofa quantitative field, so we just referred to that quantitative fieldby the field name. That corresponding set of values that you get from the records thatyou're looking at. So in that case, wewould get all of the ordinal values and then a reference tothe quantitative value. Finally, you can concatenatetwo quantitative values. In this case, you wouldhave a reference to the to the firstquantitative value and a reference tothe second quantitative value. We just simply havea reference to both quantitative values and that's available at everyrecord we're interested in. So here's an example in Tableau. Tableau doesn't support easily the concatenationof ordinal values, but it does supportthe concatenation of quantitative values. In this case, we havetotal population concatenated with total life expectancyin the columns. So for each of our rows, each of our countries, we have the data forthe total population and life expectancyconcatenated together so that they can bedisplayed side-by-side. Similarly, we can have a multiplication operationin our table Algebra. That's the cross-product. So if you have fieldA and field B, both ordinal fields, and youtake their cross-product, that means you take everyelement of field A crossed with every element of field B and you create every combinationof those A1, B1, A1B2, A1B3 and so on, and then you go A2B1, A2B2 and so on, withevery combination. So here, in the rowsof this display, we've draggedthe fields country name and year into the rows shelf. We're getting a cross productof those two fields. So for every country namewe've got every year. So each of the country namesis listed here, and with each of those countrynames we have each year. We can also createthe cross-product between an ordinal field anda quantitative field. So now we have every elementin the ordinal field crossed with a referenceto the quantitative value. Now every ordinal field has that quantitative valueattached to it. So we can see in this example,for our countries, in our rows, we have yearfollowed by life expectancy. So now we have ineach of these rows, we have for each year,the life expectancy. All available in each row. The cross-product isactually implemented as an operation calledNesting in Tableau. Nesting is likea cross product, A cross B, except we're removingcombinations that don't exist in the database. For example, if wewant B nested in A, we would ordinarily treatthis as a cross product, and every combination ofA and B would appear. Except we're only interestedin records of our database, where that record haselement AI and element BJ. So for example, if we haveregion and country name, then the three regions areEurope, the Caribbean, and Africa, but every country doesn't appear in those regions. So we have the country namenested in each region. So the only record that hasa combination of Europe with a country name in ourparticular filtered set of records is Germanyis in Europe. The only record wehave in the region of the Caribbean that hasa country name as Aruba, in our filtered set of records. But in Africa, we have Burundi and theCentral African Republic. So for each of these regions, we're only seeingthe elements that exist in the databasein those same records. This is the only way that Tableau computes cross-products isthis nesting operation. It only looks forrecords that have the two items and it does not create every possiblecombination of AI and BJ. So Tableau implements nesting. It does not actuallyimplement the cross product. So The Grammar ofGraphics defines the rules for visuallycommunicating data as charts, so you can speak properlyin the language of data. These rules, indicate what kindof chart to use, say, between a table, a bar chart, or a scatter plot, as well as the rules for combining multiple fields to make compound hierarchicalaxes that help organize the data so that theycan be better understood."
cs-416-dv,1,10,"The table algebra cangenerate compound hierarchical axis to bettergroup the data in a chart. The Polaris paper gives us the details of how Tableaucreates a series of successive queries to isolate the specific data needed for each level ofthese chart hierarchies. So now that we understandtable algebras, and we also understand this SQL, how Tableau takes a specification of a chart that would beused to answer a question, and then convertsthat into a query in order to find theinformation to fill that chart, we can look at the steps ofthis SQL in constructing that specific query basedon the chart specification. Step one, is to select the records that passthe current filter. This is a simpleSQL commands select of all the fields wherefilters would apply. So for example, we'rejust looking at four countries in our database. We have Aruba, Burundi, the Central AfricanRepublic, and Germany. So in order to do this, we've set a filter onthese four country codes. There's an initial stepthat Tableau performs that implements this filter and just restricts the records tothe ones passing that filter. The next step is to partitionthe records into panes. These are panes controlledby the rows and columns. These axis, the row axisand the column axis, are controlled bytable algebra on the fields that you have dragged into the rows or columns. So in this case, we're displaying life expectancyfor each country, for each four decades. So we've dragged countryname into the rows, and year into the rows, and this sets up a nestingof country name and year. In this case, each ofthe countries has all four of these years indicated, there's measurements forall four of those years, so this ends up beinga cross-product. This nest ends up beinga full cross-product. A pane in Tableau andPolaris in the layout, a pane is the intersection of the row and column axis elements. So here we have a single columninto two nests of rows. So each one of thesebecomes panes. So a row in eachone of these panes, we have Aruba 1980, Aruba 1990, and so on, the table algebra that goes into creating the cross-product, the nest of yearwithin country name, that ends up getting implemented as a selection criterion. So we have row one, is the row where thecountry is Aruba, and the year is 1980. So in order for this to happen, all of these countries and all of these years need to beenumerated, and sorted, and ordered, and grouped in the resulting queryfrom step one. Then in step two, we go and look atall the combinations existing in the database, and assign them row numbers, and that creates panes inthis window to interface. So now, we can gothrough each of the i's, and for each one of these panes, we can select all of the records that would be needed in order to plot the chart elementin each one of these panes. So for each one of those panes, we select all of the fields, such that the pane selectionpredicate is true. Then in step three, once we're inside each pane, we go and aggregate allof the records needed. So for example, in this case, we have a single columnof life expectancy, and our rows are regionand country name. So for each one of these panes, we are going to compute the average life expectancy over all of the yearsin the database. So we are going to selectthe appropriate dimensions, region and country name, and the appropriate aggregates, in this case life expectancyand the average of the life expectancy havingan aggregate filter. So for each one of these panes, such that the dimensions in this case are equal toEurope and Germany, and then ordered byany sorting function that might be necessary, in this case we're justcomputing the average. So a sorting functionisn't needed. Then we can aggregate all of the life expectancy valuesin order to report an average lifeexpectancy here for Germans for each yearon our database. This table algebra that we use creates a natural hierarchy. So for example, herewe have region, and within each region we have the short name of each country, and within eachshort name, we have years. So this hierarchy meansthat these panes that we're partitioning end up beinghierarchical as well, and we will iterate step two. We'll compute step oneonce for the region, and then again for theshort name within each region, and then again forthe year within each region, and ultimately, we'll come up with something that we can aggregatein order to display in this case a simple aggregation of just picking therecord because it's an attribute for each one ofthese bar chart elements. So for example, forSub-Saharan Africa, that becomes a single panein this entire display. Then the CentralAfrican Republic, becomes a single paneinside that African pane, and then 1980 becomesa pane inside the Central AfricanRepublic pane. Then we can aggregateall of the records, in this case just one record, for 1980, and thatbecomes a query. So this hierarchyhelps set up a method for visualizingmany dimensions in just a two-dimensionaltable interface. So Tableau and Polaris and all of these othervisualization tools, are really designed todisplay two-dimensions. You've got a row and column, a horizontal and a vertical. But you can then embedhierarchies of dimensions. You can have hierarchical axiswithin the table, and those hierarchies of axis setup cross-productsor nest things that allow you to displaymultiple dimensions inside each horizontalor vertical dimension. So for example, we can setup a table of scatter plots. We have a O by O table, a table of two ordinal axis. In this case we've gotregion as one ordinal axis, and year as the otherordinal axis from 2000 to 2004, those five years. Then within each ofthose table entries, we're placing a scatter plot. In this case, it'sa scatter plot of infant mortality versuslife expectancy. Then we have an aggregateinside each of these panes. So for example, wehave a pane here, that's Africa in 2000. So this pane is one row, one is true andcolumn one is true. So it's going to find all of the records such thatthe year is 2000, and Africa is the region. Then it's going to computethe average infant mortality and life expectancyquantitative variables, and plot them in the scatterplot at that table entry. So we have a table here offive panes, by six panes, 30 panes total, whereeach pane is a scatter plot. That gives us a worldswithin worlds visualization, and we're actually visualizing four dimensions ina two-dimensional table layout. Those four dimensionsbeing region, year, infant mortality,and life expectancy. Now that we understandthese steps, and the order thatthis SQL within Tableau is constructingthe query, we can understand howto use Tableau in certain ways so that itbehaves the way we expect. So there's a filtershelf in Tableau. So in the filters, I can say the indicator nameis CO2 emissions. Then I can list all thecountries in the rows, and for each country, I can list its value ofits emissions for the year 2014, and this will give me the valueof the CO2 emissions in kilotons for the year 2014, and there areno missing elements. So there may becountries in here. In fact, one of the country's, American Samoa, doesnot have a measurement, but it doesn't show upin this table because I filtered by indicatorname being CO2 emissions, and if a country doesn't have a measurement for that year, it simply won't pass this filter, and won't be involvedin the listing of country names that occursin steps two and three. In step two when we'reconstructing the pane, and in step three whenwe're aggregating the data. So that element issimply missing from the pool of records that we're using to create this table. If we want American Samoa to appear and to show thatit doesn't have an entry, then we can't filter it out here by requiring all ofour records that we're considering to have the CO2emissions indicator name. Instead, we have to create a new variable calledCO2 emissions, and CO2 emissions is a calculated fieldthat's just checking its indicator code to see if it's equal to the CO2emissions indicator code, and if it is, itreports the value, otherwise it reports null. By reporting null, that means that there'sa record that's used in the list of records that we'recreating our table from, that we're using tocreate our list of panes, and because that record is in the records that we're using to createa list of panes, it appears but without an entry. But it would otherwiseget filtered out if the tablewas created using this filtering mechanism.Finally, sorting. Sorting is an importantoperation, visualization. It helps understand an ordering, and to be able to inferinformation from that ordering. In the steps that we usedto create these records, as we are partitioning therecords hierarchically, then that can interferewith sorting. So for example, if I wanted to sort all of thesecountries, I can't, because the countries aregrouped by region names and I can only sort the countries withinthe region names. Similarly, if I wanted tosort according to population, total population from least population tomost population, I can't, becausethese axis, these panes, are setting up the aggregation, and I can only sortwithin the pane, I can't sort outside of the pane. So in order to do that, I need to swap regionand short name. So here I've gotregion on the outside, short name on the inside, and I'm getting all of the short nameswithin each region, and that's settingup panes here that I sort within regionaccording to that hierarchy, according to that nesting. If I reverse that and I display short name and then its region, I get one region per short name, and that regioncould be repeated, and it's no longer a nesting. Because it's no longer a nesting, I can sort according to the pane, and the pane is the entire tableinstead of the regions, and then I can sort and see from least populated to mostpopulated according to country, and still listthe appropriate region. Now you understand howTableau specifically this SQL uses a chartspecification described by dragging fields inthe chart elements, and turns that intoa database query to find the specific dataneeded to create that chart. By understanding the stepsof query construction, you'd becomea Tableau power-user, and can understand why Tableaugives you the results it does because you understand how it arrives at those results. You also don't needto depend on Tableau, and you can implementyour own queries directly to provide appropriate datafor plotting with any tool."
cs-416-dv,2,1,"[SOUND]So visualization is an interfacebetween the computer and the human. And to understand visualization,you really need to understand how a human perceives what's beingdisplayed by the computer. So what will we learn? We'll learn, we may understand computers,but we'll learn about how humans work, specifically, how humans perceive andprocess what's presented to them. How can we use that knowledge to create a more effective computerinterface for the human? So I like giving this talk to computerscience students, because computer science students are largely alreadyfamiliar with how a computer works. But they really don't haveany idea how a human works. And so we can use what's calledThe Model Human Processor to describe how a human works interms of how a computer works. So here's a computer, this might be a diagram of a computer that might bea computer in a cellphone, for example. You can have a CPU, some centralprocessing unit, and there'll be memory. You'll have cache memory and then RAM. The cache is sort of a short-termmemory that makes the RAM, the slower RAM run faster forthe CPU, because the cache is faster. You might have an output processorthat generates graphics or causes the phone to vibrate ordo other things. And there might be an input processor,some media processor that takes the video input from a cameraand the audio input from a microphone. And so we can use the same design tosort of understand how a human works. And this is calledThe Model Human Processor, and it was invented byStuart Card back in 1981 as a way of describing how humans workto people that work with computers. And so with the Model Human Processor,we don't have a CPU, we have our cognitive processor. And we don't have a media processoraccepting video input and audio input. We have eyes and ears that generatemedia in a visual image store and auditory image store that are thenprocessed by a perceptual processor before they're processed bythe cognitive process of the brain. And we don't have cache and RAM, we havea working memory and long-term memory. And we don't have a graphics processor andsomething that'll vibrate a phone. We have a motor processor that controlshow our hands work and for example, how our hands might draw a picture for us. And in each case,each of these processes, memories and processors have certain decay andcycle times. Storage sizes that we can discussin the same way that we discussed the storage size of random access memoryor the processing time of a processor. So first we might look at videoprocessing, visual processing. We have an eye. And the eye takes 230milliseconds to access, meaning we need something from the eye,we get it about 230 milliseconds later. And that can vary between up to 70 andto 700 milliseconds. So we can get things as fast as70 milliseconds from the eye. And then we have a visual image store,and the size is about 17. And we're going to talk,not necessarily in terms of bits, bytes, or words, we're going totalk about letters or chunks. And we can store about 17 thingsin our visual image stores, and it depends what those things are. And the visual imagestore is very very brief. It decays in about 200 milliseconds, and this 200 milliseconds is ourpersistence of the vision. It's the fact that somethingcan be flashed to our eye, and our eye remembers what it sees forabout two-tenths of a second. And that's enough time forthe perceptual processor to catch it. It's also enough time to docertain processes, like reading. And, so when we read, it may appear,when you read the text on this slide, that your eye is moving forward in a verycontinuous fashion because it's how we're processing the information. And it may seem like we're processingevery letter at a time, but that's not what's happening. What happens is we have these saccadesin our eyes jerking from one location to another. And we do those very quickly, andthen our eye fixates on a single portion. And we look at that portion andthe surrounding images on our retina. And we don't perceive our saccades. Our eye is jerking left andright, and we don't see that. Our perceptual system blanks thatout from our conscious selves. In terms of reading, we can readsmaller fonts and larger fonts about at the same rate, although larger fonts,we probably feel they're easier to read. We read them at the same rate, and we still tend to read printedpages faster than we read displays. This text on the right shows thatyou're not processing every letter, but you're processing letters in chunks. And you can still read this even thoughit doesn't itself really make sense. There's also audio processing. And I haven't drawn the ear yet, but you have an ear that sends signalsto an auditory image store. And again, about five letters ofinformation, five things of information. And when you hear something, you haveabout a minute and a half to process it. So if you're not paying attention tosomething, if somebody's talking to you and you're not paying attention,and they say what did I just say? If they said it in the last one and a halfseconds, then you got a pretty good chance of being able to recover itwithout even thinking about it. And so, the way that the ear works,there's an auditory canal in your ear that focuses sounds,that vibrates an eardrum. That basically vibrates some mechanicalstructures that then works its way around a cochlea. That's basically figuring out whatfrequency composes the sound, and stimulates the cilia, that then send signals to the restof your perceptual system. And the sound, there's pitch,loudness, and timbre. Pitch is the frequency wherein the cochlea it gets sensed. Loudness is the amplitude of the waveform, and then timbre isthe shape of the waveform. And that can differ based, for example, different instruments producedifferent timbres of sound. We can do stereo location of a source. We have two ears, and we can dosome stereo location of a source. But in general, it's not very accurate. The more important thing is wehave a cocktail party effect. That means that we can focus ona particular conversation and mentally filter out some ofthe other noise that's going on. And humans still do that much betterthan computers ever have been able to. And finally hand-eye coordination, andthis pathway's a little bit more involved. We have an eye, a visual image store that's helpingus process the imagery from the eye. Then a perceptual processor that'sactually doing the processing on the data in this visual image store. That gets sent to the brain thatdecides what to do with it, and then the motor processor, the hand. So we've got this hand-eye coordinationthat goes through all these paths. And you can start adding up the time,230 milliseconds for the eye. Another 100 milliseconds foryou to perceive what the eye is seeing. Another 70 seconds for you to think aboutthe eye, and another 70 seconds for your hand to respond toinstructions from your brain. And so you can use that to figure outhow long it takes for you to react or how long it for you to move the mouse. If I have the mouse in some location,and I want to move the mouse, say to this figure's head. I can figure out how long it'sgoing to take me to do that. And this is based on severalproperties of the human kinesthesis, the fact that we know where are limbs are. We know where our fingersare bringing things to. And also the fact thatlarger movements are faster. I can move the mouse quickly acrossthe screen and click as well. I can move the mouse quickly acrossthe screen, but I do it less accurately. And so we tend to, when we're finding atarget, like if I want to find the center of the 0 in that 70 milliseconds,I move quickly towards the 0. But then I have to do a seriesof finer and finer steps. So we do large lunges followedby finer and finer refinements. And so that gives us a logarithmicrate at which we find things. And so if I need to move things fartherthan the distance, D gets larger. But if I need to get to a smaller object,then the size matters as well. So I need to divide by the size. If I need to point to a larger thing,it should take me less time. So, it's one 1 over the size. So, the distance / the size givesme an indication I can point to larger things farther away as fast as Ican point to smaller things closer to me. And we take the logarithm, because the number of steps I needis based on this distance / size. I'm taking shorter and shorter steps. So this is a log base 2. And then we have 240 milliseconds ittakes for each one of those lunges. 240 milliseconds is 70milliseconds to move your hand. It takes you a 100 milliseconds to beable to see the result of your lunge, and then another 70 milliseconds to decidehow to correct it with your next lunge. So 70 milliseconds to move the hand,100 milliseconds to see the result. And then, another 70 milliseconds tofigure out what the next instruction to your hand ought to be. That's 240 milliseconds on average. And then you need about 600 milliseconds, six-tenths of a step, just to prepareyourself to be able to do this exercise. So this is a roughapproximation called Fitt's Law that tells you how quickly itshould take you to move the mouse to a certain region of the screen basedon where the mouse is to begin with. So what did we learn? We learned that the Model Human Processoris a way of describing the human to people that work with computers. And it helps us now as we figure outhow to model the human when we're presenting the human with variousdata visualization presentations. And that we deal with data in chunksnot necessarily bits, bytes, or words, but chunks, andthey're very abstract chunks. And we can use this model and all of itsterms in order to make predictions on performance and also to provide insightinto how we can improve performance. [MUSIC]"
cs-416-dv,2,2,"[SOUND]So as we discuss the human we'll discusshuman memory and ways of, that can help take data visualizations and help peopleremember them and learn from them. So, we're going to learn, what'sthe difference between, for example, short term memory andlong term memory and sensory memory. And how can we get things betweenshort term and long term memory? How can we remember things better? And essentially, how can we learn better? So we'll start again withthis model human processor. Except where we organize. And I want to focus on the memory of this. And that's the visual image store, theauditory image store, the working memory, the long term memory. And these are connected by the perceptualprocessors in the cognitive processes. And so I can reorganize this. We have the sensory image storesthat go to the perceptual processor, which places information in the workingmemory, which then goes to the cognitive processor, and places informationin the long-term memory. And this process of takinginformation from your senses and putting it into workingmemory is based on attention. When you're paying attention to something,like for example, the fact that I'm pointing the pointer andtalking to you right now, you're taking the elements in your very,very short term sensory stores and you're processing themand putting them in your working memory. Now, because this is classroommaterial you want to remember that. And so you need to take these detailsthat I'm describing to you, and take them from working memory andput them in long-term memory. And that's a process that'susually called rehearsal. There's a number of ways to do that butit's effectively called rehearsal. And you'll notice working memoryonly holds about 7 items. And those items are available forabout seven seconds. You can hold one item much longer, but for several items it's aboutseven seconds before they decay. And I've got several things on the screen. So you're processing each of these. Like, you know, the perceptual storeattention, and perceptual processor. And trying to rehearse those and put those in long term memory beforeyou try to memorize these other things. So, sensory memory is reallyshort term device memory. You know, on a cell phone, it would be theCCD array that's recording the image data. And it'd be the registersthat are used to store and organize that information beforebeing sent as an image to the CPU. Visually, we have iconic memory. This is persistence of vision. You look at somethingflashed on the screen and you can remember it eventhough it's not there anymore. That's your iconic memory,your visual sensory memory. And this only lasts about half a second. You have echoic memory for hearingthings that last for about a minute and a half, or sorry, one and a half seconds. And then you have haptic memory whichyou can remember touch as well. And then these constants can go up ordown, based on arousal. How much attention are you paying? Arousal means that you'repaying more attention. And that can increase boththe severity of the memory. And how long it lasts in your memory. And these things get placedinto working memory so your cognitive processor can process them. This is like human dynamic ram memorybecause it needs to be refreshed. It dies about every 200ms. So it needs to be refreshed. And it has about 70ms access time. But it's very small. It only holds about seven chunks,digits or words, seven things. And, again, recency effects are important. The last thing that goes in the memoryis usually the strongest, and things start to fade aftera certain amount of time. And so we can have fun with looking atworking memory, and figuring out how things are organized in our short-termworking memory that we use to process. So I can give you a sequence of digits. Here's a sequence of digits, 25439762608. And I'll hide those. Now, what were those numbers? If you're like me, I can remembermaybe two, three, something or other. I don't remember any of those digits, and that's because it overflowedmy working memory. I couldn't remember that many items forthat long. Let me take another set of the same numberof items, these are going to be different items, different numbers, butthe same number of digits. 456 295 1413, and you notice we'veorganized them in the chunks. 456 295 1413. Now I hide them andI can remember 456 293 1413. Oops and I got it wrong. 456 295 1413. 456 295 1413. Because I'm remembering chunksinstead of individual digits. And you can do the same thing, the samething is true for characters and words. Here's some random characters. H-E-C A-T-R A-N-U P-T-H E-T-R E-E-T. If I hide those, I don't know if you canremember those particular characters. But, those, I can remember them. Those charactersare H-E-C-A-T-R-A-N-U-P-T-R-E-T. And the reason I can remember themis that if I start with this T and then go back to the beginning,this spells out, the cat ran up the tree. And I can remember the cat ran upthe tree, that's only five words, where as I'm looking at eighteendifferent letters here, which would over flow my working memory. So items are processed betterin working memory when we chunk them together into higher level objects. Then once things are in our shortterm memory, our working memory, we need to get them in our long termmemory so we can remember them later. And long term memory is kind oflike the Human World Wide Web. It's a persistent memory. It's effectively infinite andlasts forever. And there's two types of long term memory. There's episodic memory andsemantic memory. Episodic memory is time varying memory. We remember sequences, events thingsthat are organized temporally. This is how we remember songs. You remember the lyrics. You may not be able to remember every wordto a song unless you sing the song to get to that word. And semantic memory ismemory that's facts, but they're organized associatively,there's links. So you get this stream of consciousnesswhere you remember one thing, and then you can remember something relatedto that and something related to that. And so there's various representations forthese kinds of memories, there's semantic nets whichare really just a graph. There's frames which are kindof a database with fields and entries that you could create relationsand create associations between them, and then there's scripts which are similar toframes that basically defined the roles, scenes, props and soon for various memories. So when we go throughthe process of remembering, there's a couple concepts to keep in mind. One is the total time hypothesis. And that says that you can remembersomething if you spend more time with it. The more time you spend on something, the better you are,the more likely you are to remember it. This could be memorizing by rote whereyou just keep repeating something but it's not necessarilybest to always repeat and learn something by repeating it over andover again. It's sometimes betterto remember things by working with them differently butstill spending time on them. There's the distributionof practice effect, which means you can't memorizea bunch of stuff all at once. It's better to memorize a few things ata time, and then go do something else and then come back and remember a fewthings at a time by distributing the memories over a wider time andnot all at once. Concrete things are easier toremember than abstract things, and so, we remember meanings. So if I read off the words faith,age, cold, tenet, quiet, logic, idea, value, past. Those are harder to remember. But if I read off,because they're abstract concepts. If I read off more concreteobjects like boat, tree, cat, child, rug, plate, gun, flame, head. They tend to painta picture in your mind and it becomes a little bit easier foryou to remember those. And so we tend to memorizethings based on structure, based on familiarity,mostly based on concreteness. So on the right I've got an example. If you want to memorize numbers,if you have the digits one through zero, effectively one through ten here,but one through zero. And you want to memorize a longsequence of numbers, for example, Pi. If you remember pi is 3.14159,if it's difficult to remember those digits,those arbitrary digits. You can make them more concreteby associating each digit with a concrete word that rhymes with it. So instead of three, one, four,you remember tree, bun, door. And the words tree, bun,door start to paint a picture. And if I go tree, bun, door, bun, hive, wine, I start to tella little bit of a story, and you can start using more of your episodicmemory to remember these abstract ideas. We also forget, even when thingsare put in our long-term memory. They will decay over time. And that decay is logarithmic. We forget most things earlier andthen we forget fewer things longer away. This leads to Jost's law. If you have two equally strong memories, than the older of the twomemories is the more durable. If you can remember something,if you don’t forget something quickly than your chances ofmemorizing it forever are much greater. And then there’s interference. One of the reasons we forget things is that we learn things thatconflict with that memory. For example there's proactive inhibition. Which means you can't teachan old dog new tricks. If you've learned something one way,then that one way which has been in your long-term memory for a long timeis going to be a strong memory. And it's going to be difficult to rewritethat memory with new information, because that old memory is somuch stronger. And you're more likely to forgetthings you've learned more recently. There's also retroactive interference. This is when you've just received somuch information. That you start to forget thingsbecause of the amount of information. This is, for example,why it's not good to cram for an exam. Because you're filling your brain with somuch information that the more recent information is overridingthe less recent information. And because the information is lessrecent, it's less strong in your memory. And finally, emotion. We remember the good old days, we forget the mundane day to day detailsthat were boring about the good old days. You remember things based on strongemotions and so if you can change your emotional state when you're studying thatmight help, but it might also exhaust you. But it does have a impact on how youremember and how you forget things. So what did we learn? We learned about memory andsensory memory, working memory, and long term memory. Sensory memory decays quickly butsupports fast sensory processing. Working memory decays less quickly, butallows us to do cognitive processing and the challenges to keepingthings in working memory are skills that allow us to clumpthings into bigger and bigger clumps. And then long term memoriespersist indefinitely. The challenge there is getting informationfrom working memory to long term memory. In a way that it will persist longer. And, in general, if you want to learn andremember better, it's best if you can vary the learningstyles over a longer period of time. [MUSIC]"
cs-416-dv,2,3,"[SOUND]. So when we present a datavisualization to a human observer, we're going to expect that observerto think about the information. So it's useful to understand how a humanobserver thinks about information. So what we'll learn is how does a humanobserver understand a visualization. And how does a human observerrespond to a visualization? What are they thinking about when theysee this data presented to them visually? So we use the Model Human Processor, but we're going to focus onthe Cognitive Processor, and the information provided to it through thevisual image store and visual perception. And how we reason about that information. And there's three kinds ofreasoning that we'll look at. Deductive Reasoning, Inductive Reasoning,and Abductive Reasoning. Deductive reasoning is basicallydrawing a conclusion based on the data. It's based on logic. If A then B. So if you see A, then you canconclude that B is going to exist. And if B doesn't exist,you might conclude that A doesn't exist, because if A happened,then B would have happened. So these are all valid logicalconclusions based on visual data. Sherlock Holmes used to say,when you have eliminated the impossible, whatever remains,however improbably must be the truth. And that's basically justthe process of elimination. Which again, is a deductive reasoningmethod for drawing a conclusion. And so for example, these reasoning examples I'm going touse this life expectancy data. This is average data forall countries in the world and their life expectancy has grown inthe 1960's from an average 54 years to an average of about 71 years in 2012. And so, based on this data I might deduce that I'm one of the members of one of the countries that wasused to create this data. I might expect that my lifeexpectancy has similarly increased over the past few decades and that would be a logical conclusion basedon the information from this graph. Also, you can see a correlation here anytime you have a graphthat's increasing this way. It's showing a correlation betweentime and between life expectancy. And so we're deducing, we are concluding that lifeexpectancy is increasing over time. And so people now days are living longerthan they were a few decades ago. And that's a reasonable deduction. We have to be careful that this isshowing correlation and not causation. And that we're not showing thatbecause people are growing older, time is passing necessarily. There are many other cases wheretwo items are correlated but one does not necessarily cause the other. Or you get the causationin the wrong direction. There's also inductive reasoning. Which basically says if somethingis true for x, then it's true for x +1, and you know that it's true for x. So, in that case, it's true for all x. If it's true for one specific case,it's true for all the cases. And these are generalizations. We can use this to extrapolate data. For example, life expectancy. I can look at this shape and I mightexpect that life expectancy is going to continue to grow at a rate of aboutten years every decade and a half. And so we eventually,we'll live to be 200 years old. And so some generalizations are helpful. Some generalizations aren't helpful. And you have to be careful withthose kinds of extrapolations, also, interpolation. I've got this data yearly, butI'm drawing this as a smooth line, which is an inductivereasoning prediction that says that this is varying smoothlyin between the data items. It's interpolated, and it's not varying widely betweenan individual year and the next year. So, inductive reasoning allowsus to infer missing data. It allows us to extrapolate data,either before or after the graph. It's less reliable than deductivereasoning, but still can be helpful. And finally, abductive reasoning. This is based on sort ofthe human need for meaning. To ask why. So, given this data, that life expectancy,on average, has been increasing over the past few decades, we mightstart to ask why is that the case? And we might create models. And so, in this case, the graph isincreasing almost like a straight line and so you can do regression. You can fit a line to this data,and create a model for this data. And you can use that modelto replace the data. And in our minds, we might not thinkof every single detail of this data. And we might think in generalthe data is growing at the rate of our life expectancy is growing at therate of ten years every decade and a half. And those models are good,but they create a problem. For example, cognitive dissonance. It's our ability to entertain simultaneouscontradictory opinions at the same time. So, it may be difficult forus to think of two things, two opposite models, at the same time. Even though we don't know which model iscorrect, and so we tend to take sides. And if our evidence disagrees with ourmodels, sometimes the model is so powerful in our minds that we tend to disregardthe evidence, and that can be dangerous. And so as we go about with visualization,we have sort of the baggage of our models. Our expectations of the waythe world works and our abductive reasoning is tryingto fit this data to those models. So, what did we learn? We learned that there'sthree kinds of reasoning. Deductive reasoning whichdraws a conclusion. Inductive reasoning which createsgeneralizations, interpolations, and extrapolations. And abductive reasoning, which tries to put meaning behind the dataand tries to create models for the data. Tries to change the waywe think about the world. And all of them are important and all ofthem are important to keep in mind as you present information for a visualization. So that they are applied correctly andnot incorrectly. [MUSIC]"
cs-416-dv,2,4,"[NOISE] Sowhen we communicate information to a human observer throughdata visualization, the first step of that process is for the human to perceivethe data through the retina. And so we'll discuss the retina. We're going to learn about the eye andhow does the eye sense light. And based on that knowledge of howthe eye physically senses the light, we can figure out how small the detailsof a visualization can be, and what colors should youuse in a visualization. So we're going to focus on the eye andthe mechanics of the eye. And this is a image of the eye,an illustration from Gray's Anatomy. And the eye consists ofa lens that focuses the light that it receives from an aperturethat is controlled by this iris, and it focuses thatlight onto this retina. And this retina contains the sensorsthat basically determine what light is hitting what part of this image plane. So we don't have an image plane. It's more of a hemisphere. And the important part hereis this fovea centralis. This is your center of vision. When you look at something, it's becausethat something you're looking at is getting focused onto the fovea centralis. This is where you have a lot of sensorsthat can tell a lot of detail about what you're looking at, then less sensorsas you get farther away from that. You also have an optic nervethat connects to your retina. And you don't have any sensors here to beable to see the light that's coming in, so there's a blind spot that we have. And we don't even realize we have theblind spot because our perceptual system is covering it up in a way that wedon't even realize is happening. So this leads to the notionof visual acuity. How sharp is your vision? What can you see? How small of a detail canyou present to a viewer? And we measure our visual acuitywhen we go to an eye doctor and we look at one of these eye charts. And it tells us our Snellen ratio, 20/X,which means you can distinguish at 20 feet what the average personcan distinguish at X feet. So if you can only see this letter E,then you have 20/200 vision, meaning you can see this E at 20 feetwhen the average person can see this E even as far away as 200 feet. And if you can see below this redline you have better than 20/20, better than average vision. And the reason you can distinguishthese letters is that their features project to a certaindistance apart on the retina. And if your lens is doing a goodjob of focusing that information, then the sensors on your retinacan resolve those details. If your lens is not doing such a good job,then the image that's getting presented isn't going to be as resolvedas well by your sensors. And so your retina basically isgoing to consist of rods and cones. Rods perceive the brightness andcones perceive color. There's three varieties of cones, [COUGH] cones that perceivemostly red colors, cones that perceive mostly green colors,and cones that perceive mostly blue colors. And then there is a variety of additionalprocessing that happens by the ganglions, these other nerve cells after the retina. But these rod and cones are usefulto look at for the moment. You have 80 million rods andonly 5 million cones, so your intensity vision is muchbetter than your color vision. Your rods are denser away from the fovea. Your fovea, your center of vision,is mostly cones, as it turns out. So astronomers have learned when they'relooking at very dim stars to look slightly away from the star, look slightly off tothe side of what they're studying, so that their rods can processthe information and not their cones because their rods aremore sensitive to the changes in light. And they're quite sensitive. They don't even turnon when it's daylight. And then you have cones. And they're, again,sensitive to various wavelengths of light. They're very dense in the fovea, anywhere from 100,000 to 325,000 conesper millimeter squared in the fovea. And they can distinguish about 150 hues. And combined between the different hues, you know a hue is a color in the rainbow,and the different shades and intensities and saturations,you end up with about 7 million different shades that you candistinguish with your eye. Depending on your lens, all lenses exhibitsome form of a chromatic aberration, and that means that refractiondepends on the wavelength of light. And so light at the redwavelengths is going to focus at a different location than light in thegreen wavelengths or the blue wavelengths. And this creates a dispersion thatcan create problems with focus. And this is why Blue Blocker sunglasses oramber sunglasses, which are nothing more than justamber-shaded filters on your sunglasses, they're going to aid your vision becausethey're filtering out blue light. And blue tends to focus off the retina,whereas red and green tend to focus closer to the retina. So the blue wavelengths are actuallyblurring the image and the red and green wavelengths are generallysharper on the retina. It'd be good to avoid pure blue text, or at least desaturate it with a littlebit of red or green colors, to aid in focusing on edges of texts orimportant figures. There's some interestingproperties of color. This isn't necessarily chromaticaberration, but we tend to perceive the red as being closer than the blue, andthis is part of our perceptual system. For color perception there'sa difference in brightness as well. Blue is generally a darkercolor than red or green. If you look at the pureluminance of a color, the luminance of that color will be30% of its red component plus 59% of its green component plus10% of its blue component. That means that green is sixtimes as bright as blue, and green is twice as bright as red, andred is three times as bright as blue. And if you have yellow, you can create yellow in the additivecolor system by red plus green. Yellow is very bright. It's 31% plus 59% givesyou 90% brightness. And you can figure out the luminanceof these other colors based on that. Also it's important to acknowledgethat a good number of people, in particular 10% of males,are colorblind. And if you're displayinginformation with color, you may want to add additionalcues to that information so that people that are colorblindcan also understand it. And also to pay attention to contrast,because if you're displaying something only withcolor, then people who are colorblind aren't going to be able to tellthe difference just on color contrast. And so you may want to changethe brightness of the color as well, and use this formula to makesure neighboring colors have different brightnesses inaddition to different hues. These red, green and blue colors getchanged into a different color space, basically a brightness shade and then a brightness minus blue and a red minus green channel. So you get a color space that goes fromblack to white, yellow to blue, and red to green. This is the black-white channel. This is the yellow-blue channel. And this is the red-green channel. That's what actually gets sent tothe brain after some processing of the retinal image. And so your cones androds will start to get exhausted. [COUGH] If you stare at the + inthe middle of the screen here, you'll start to see thatthe purple dot that's missing, instead of missing willstart to turn green, because it's turning intothe complement of the purple color. And if you keep staring at the +,eventually that green dot will eat up the rest of the dots andyou're left with a solid gray screen. And this is all due to basicallysome exhaustion of the rods and cones in your perceptual system. So what did we learn? We learned that the retina sensesbrightness with rods and color with cones. And that we have more cones inthe center of our vision but more rods in our peripheral vision. And we tend to focus better on warmercolors because of chromatic aberrations. We also tend bring them to the forefront. It's good to keep these things in mindwhen you're deciding how big to make text and what color to choose for variousproperties in your data visualization. [MUSIC]"
cs-416-dv,2,5,"[MUSIC] So as we present data visually toan observer to a human observer, we want to make sure that thatdata is perceived correctly. And that means that the datathat's sensed by the rods and cones in our retina, ends up beingprocessed further by our perceptual system before it even getsto our cognitive system. How can we make sure that the visualdata is going to be presented so that it's perceived properly andcorrectly? So in the model human processor, thisdata that's coming from the human retina goes through a visual image storewhere it goes through significant perceptual processing beforeyou even think about it. Before you even realizewhat you're looking at. And this perceptual processing starts right after the data's beensensed by the rods and cones. This is an image from Grey's Anatomy. And you can see that as soon as the datahas been sensed, we've got other cells. These nerves called gangliathat are processing the data. In this case a horizontal cell isactually comparing the results of what's being sensed by the rods andthe cones. And this helps us to detect patterns andaccentuate and exaggerate differencesin neighboring sensors. This helps us detect edges, andmotion, and tigers hiding in the bush. Because tigers hiding in the bush willcreate patterns in our peripheral vision, and those patterns can change over time. And these horizontal cellsthat are comparing and exaggerating those differencescall tell us to then run away. So for example,when we see a shade of gray, we see that in context ofthe surrounding shades of gray. And so here we have a gray color,surrounded by a lighter gray color. And here we have a gray colorsurrounded by a darker gray color. And this box looks likeit's darker than this box. So this is a darker shadeof gray than this gray. When in fact those two shades of gray areidentical, they're the same shade of gray. It's just when we surround them withthese boxes in different contexts, our perceptual system will accentuatethe difference and make this gray look darker and this gray look brighter whenin fact they're the same shade of gray. This also works with color. And so on the left block I, you can see that these twoblock Is are the same color. But here the color appears lighter, because these squaresare next to white squares. And here they appear darker becausethese squares are next to green squares. And you can also demonstratethis with mach bands. Here I've got a gradationfrom dark gray to light gray. And here I've got a plot of basicallythe number of photons coming from each of these bands, to our eye. And in fact, what our rods andcones are sensing for each one of these bands isproportional to this graph. What's actually being sent to our brainfrom our perceptual system though looks like this. And you can see that this flat gray rectangle next to thislighter gray rectangle. This darker rectangle gets darker here andthis lighter rectangle is lighter here. There's actually,you can see it get lighter here and that's not because I've changed any color. This is all the same shade of gray. It's that our visual system is making this shade of gray brighterbecause it's next to a darker color. And our visual system wants toaccentuate the existence of this edge when these two shadesare very close to each other. And this can cause problems too. In this case we've gotthe lateral inhibitors, which are these cells comparingneighboring rod and cone values. In our peripheral vision,if I look at any one of these junctions, I see a solid gray color. But the junction's neighboring cells startto flicker black and white because of this lateral inhibition of our perceptualsystem trying to accentuate those edges. This also works with orientations, our perception of orientation is alsobased on sort of this local comparison. And sothese lines look like they're diverging. It looks like these two parallel linesare getting farther apart as I move this direction because they are the sumof all of these intersections that are slightly askew from these cubes. But if I remove these cubes, you seethat these lines really are parallel, and the perception that those linesare not parallel is basically because of adding up all of those misperceptions oforientations due to the offset cubes. Similarly, I can take this diamondshape made from four straight lines. And if I superimpose on top of these fourstraight lines of this diamond shape a bunch of concentric circles,this diamond shape, these lines start to looklike they are bowed inside. And that's because the orientation ofthis line is based on our perception of the orientations relativeto these concentric circles. And we get this bowing effect. At a much higher level,it can also confuse our cognitive system. In this case,we have an eight by eight block of shapes, and that should give us64 units squared of area. If I take these very same shapes andreorganize them here, I'm now covering a 13 by 5 area which givesme 65 unit squares of area. And so somehow I've taken the sameshape with the same area and gotten an extra unit squareof area on the right. And the reason is the accuracy ofour perception of orientation. We have basically this triangle's anglegoing into this rhombus' angle here. And the slope of these trianglesis basically three-eighths and the slope of these quadrilaterals, thisedge of this quadrilateral, is two-fifths. And two-fifths is just notequal to three-eighths, but we just don't see the difference. We also see size in context. And so, here, I have two blue disks. This blue disk is sittingamid some small yellow disks. And this blue disk is sittingamidst some large yellow disks. And this blue disk lookslarge as a result. And this blue disk lookssmall as a result. But in fact they are the same size. And sowe see things in context because of these local comparisons that ourperceptual system does. So what did we learn? We learned that our perceptual systemperforms lateral inhibition, and other temporal processing andother processing at low levels and higher levels in order to see and accentuate differences in color,in size, in orientation. And it does soin the context of neighboring shapes. So this lateral inhibition helps us,for example, when we're trying to survive inthe jungle with tigers in the bush. But it can also interfere withour perception of visual data. And so when you present visual data,you need to be sensitive that these illusions can occur andto try to avoid them when you can. And to provide consistent context forvisual comparisons. [MUSIC]"
cs-416-dv,2,6,"[SOUND]When we present visual data to an observer, sometimesthe perceptual processing that we use to perceive our three-dimensional worldgets in the way of perceiving visual data. So we'll learn how we perceive a 3-Dworld from the two-dimensional image on our retina, and how that ability toperceive a 3-D world can interfere with the visual presentationof two-dimensional data. And what we can do toavoid that misperception. So again, we'll look at the visualpart of the model human processor. Basically, our eye is going to besensing data that is going to be processed by this perceptual processor. And this perceptual processorhas been developed to understand three dimensional scenesmore than two dimensional scenes. In particular, perspective,perspective is a broad word. It includes things like foreshortening;the fact that three dimensional objects are projected onto an image plane oronto our retinal hemisphere. And that projectioncreates foreshortening. So if I look at a unit cube, when I look a projection ofa unit cube on an image plane. Some of the edgesare shorter than the others. The edges in the depth directionare going to be shorter than the edges in the transverse directions. And so that's Foreshortening. The fact that this edge is shorterthan this edge in the projection. Even though in the three dimensionalscene they're the same length. And so lengths in a projection are not necessarily accurate comparedto the lengths in the real world. But perceptually we've grown to understand that these lengths couldbe the same length. And the differences are the result ofthe projection onto the image plane. And then there's linear perspective,and you can see that here. Here we have a projector which isbasically aligned from the object to a focal point through an image plane herethat's being used by this early artist in order to get to createa perspective drawing of this object, when we were first understandinghow perspective worked. Basically, Linear Perspective says thatobjects that are farther away appear smaller. And you can see that here. And then we have Size Constancy. We know that objects like this musicalinstrument, aren't going to change size. And soif an object appears to be changing size, it must be moving farther away from us orcloser to us. And that objects that appears smallerthan other objects must be farther away. That set a size constancyexpectation we have, and our perceptual system depends on that. So it can also malfunction. And so,here I've got a very simple line drawing. With two parallel lines A and B, and then I've got two lines thatare receding in the distance. They're kind of sloped against each other. And these two lines can setup the appearance of, for example, railroad tracks or some two lines that are converging to avanishing point in a perspective drawing. And if that's the case, then line A,if this was a perspective rendering of a scene, line A shouldbe farther away from us than line B. And if that's the case,line A should be much bigger. And in fact line A looks biggerin this image because of that. But actually line A andline B are the same size. But when we add these additional linesin here, when we add a perspective context to these lines, line A looksbigger because it's further away. Similarly I can have two equallength vertical lines here. And if I add a few other decorations,suddenly I've changed the length. And now this line looks shorter, and this line looks taller, even thoughI know that they're the same length. After adding these additional linesI've increased, I've decreased the size of this line and increasedthe size of this line perceptually. And the reason is, if I follow theseout further, I'm thinking of this line, In perspective as beingthe front corner of a cube and this line as being the back corner,for example, a room. If this is the floor andthe ceiling and these are two walls. This is the back corner of a room. And this is the closestcorner of an external cube. If that's the case then I wouldexpect this line to be farther away, because it's the back corner. And this line to be closer to mebecause it's the front corner. And in perspective if I have two line butthis line is closer to me and this line is further away, even thoughthese two lines are the same length this one looks bigger becauseit's farther away. Perspective can also confuse us just from foreshortening notfrom linear perspective. So here I've got an parallelogram and anytime you have parallel lines thataren't meeting at 90 degree angles. We expect them to bereceding into the distance. And, so, here,even though this is just a parallelogram, you want it to be a quadrilateralwith right angles. A rectangle that just happens tobe receding into the distance and that it's being foreshortened into aparallelogram because it's being projected onto our retina. If that was the case, then if thisis receding into the distance, then the fact that B is closer tothis point than it is to this point would make this linea shorter than line C. And just looking at the figure, it looks like line AB is shorter thanline BC, when in fact line AB and BC are the same length,because this is an isosceles triangle. Now this also works with texture. Here I just havean arrangement of ellipsoids. A two-dimensionalarrangement of ellipsoids. We tend to perceive it as, for example, the tops of oil barrelsreceding off into the distance, because I've got smaller features,and more features, in the distance. And so I've got a higherfrequency of smaller features. In the distance and a lower frequency offewer, larger, features in the foreground, and we tend to attribute that to aprojection of linear perspective as well. And we also have certainlighting assumptions. We're used to the sun being overhead,and so we're used to objects, seeing the front sides of objects, andthose objects are illuminated from above. And so we see this dot illuminatedfrom above as a convex sphere. And we see this dot illuminatedfrom below with the illumination on the bottom side as being a concaveindentation because we don't expect illumination to come from the bottom. We expect illumination to comefrom the top, and the only way for this to be illuminated from the toplike this is if it was an indentation instead of a bowl just asit is on the left side. And so, what are we learning? We're learning thatperception of an object. For example of the sizeof the object is based on our perception of the distanceof the object from us. And if there are cues in an ordinaryflat image that might trigger our perceptual- our perception ofdistance based on perspective, then that can confuse ourperception of size as well. We're going to want to compare sizes and positions of objects when we'representing data visually. And we have to ensure that we're notconfusing that perception of size and position based on the misperception ofdepth due to these extraneous cues. [MUSIC]"
cs-416-dv,2,7,"[SOUND] We're going to use two dimensional graphics a lot when we visualize data. We're used to using two dimensionalgraphics, for example, for plotting functions. And so we'll focus on two dimensionalgraphics now as we look at methods for using graphics for visualization. What we're going to learn isthe difference between vector graphics, which are used to specify two dimensionalgraphics, and raster graphics, which are used to displaytwo dimensional graphics. And then we'll look at the coordinatesystem, specifically that are used for each of them. Vector Graphics are the graphicsthat used for drawing. We're used to drawing. We take a pen, andwe'll put our pen down at one point. And then we'll move ourpen across the paper. And then lift our pen up atanother point and you get a nice, straight line between them,a nice continuous line. In Raster Graphics, these are the graphicsthat are used, for example, for our televisions and our phones,they are a rectilinear ray of pixels and these pixels are assigned colors andby assigning. Certain pixels, certain colors youcan represent those same shapes. So you will draw a shape using vectorgraphic to describe a point where you want to start a line anda point where you want to stop a line. You'll get either a straight line orsmooth curved line between them. Those will be converted toRaster Graphics for display, which will consist of the pixelsthat get illuminated along that path in order to display that path thatyou described with vector graphics. So this process is called Rasterization. And so we'll specify a primitiveas in a vector graphics format. We'll describe Vertices,points on the plane, and then we will connectthose points with strokes. In this case with straight lines,but they could be curved paths and then those strokes can enclose a region. And so we can fill that region andwe may assign a color for that region. And for strokes we may assign a color anda width of the stroke. Or have the strokes stylized,say with dashed lines, or so on. The process of Rasterizationstarts with these Primitives defined by these few data points. And converts them into an array of pixelsso that they can be displayed, and so this raster format representation ofthis triangle is as an array of pixels. And the dashed lines representthe original Primitive on the left, superimposed over the array ofpixels that are representing it. And in this case, we have pixels alongthe edges that are colored blue, and pixels in the filled inregion that are colored pink. And you'll notice, when we rasterizea shape, we can get Aliasing. And that's the fact that this nicestraight line here in our vector graphics representation,appears as a stair-cased line. You get these stair step artifacts whenyou Rasterize a smooth, straight line and because those stair steps tryto look like the original line, but look a little bit different,we call that an alias. And that problem is called Aliasing. When we draw primitives in two dimensions,when we want to draw shapes for two dimensional graphics, forexample for plotting functions. We're going to need the coordinatesystem in which to draw those shapes so we know where to place the vertices forexample of the triangle. And sowe need to define a coordinates system. And these coordinates I will call CanvasCoordinates they are the coordinates that we use to draw things is with. In this case, the Canvas Coordinateshave been defined to go minus one, minus one to one, one. And sothey set up the square region of a plane. The origin will be here in the middle. You can define your canvascoordinates to be anything you like. You want to define them tobe something convenient so you can draw your shapeswithout a lot of trouble. In this case I've drawn a plot ofa parabola, the parabola y = x squared. And so I've got a curved path starting atthis point here going to this point here, and I've defined my canvas coordinatesystem to be something convenient for that plot. In this case I've started itgoing from minus one-eighth, minus one-eighth to one-and-one-eighth,one-and-one-eighth. And that's so that, as I move one-eighth,one-eighth in, I'm at the point 0,0 in my coordinate system and I candraw my plot going from 0,0 to 1,1 here. And then I've got an additionaleighth of a unit surrounding the plot in order to add metadata like thattitle of the visualization and draw the axis and label the axis. So, that's convenient, but, as I'mresizing the display, I may want the fonts to be larger or smaller, and I'll needa bigger margin surrounding the plot. Or, I may want the plot to be larger andthe margin to be smaller. In two dimensional computer graphics, we can set up higherarchival coordinate systems. This just means you havea canvas in a canvas. In this case we have a yellow canvasthat is a coordinate system for the entire plot the entire visualization. And then we have an inner canvasthat is a coordinate system just for the plotted data. In this case, the parabola. And so I set up the outer coordinatesystem to go from 0,0 to 1, 1 in this region. And then I've set up an innercoordinate system to go from one-tenth, to nine-tenths, and one-tenth, one-tenth,to nine-tenths, nine-tenths here. And I've defined it's coordinatessystem to go from (0,0) to (1,1). So now I can plot insidethis coordinate system using coordinates that are convenient forplotting this parabola, and then I can plot in thisouter coordinate system. Using coordinates convenient for drawingthe decorations, the axis, and the title. And so we can define whatever coordinatesystem we want, wherever we want, in order to make it more convenientto draw two dimensional graphics. There's also Screen Coordinates. These are the coordinates thatare used for raster graphics for display of the information. And in this coordinate system,we have a grid going from (0,0) to whateverour screen resolution is. In this case, since we go from (0,0) wego to our horizontal resolution minus 1, vertical resolution minus. That's one. If our screen resolution was 100, 100,we'd be going from (0,0) to (99,99). And the pixels are locatedon this grid intersections. And so, you have a integer coordinate foreach pixel location. Which is useful when you're actuallydisplaying an image using these pixels. You want to be able to locateeach one of these pixels and so there is a canvas to screen transformationthat happens, so we're going to define our coordinate system going from some leftbottom point to some right top point. And then we're going to plotusing those coordinates, and those coordinates are going to beconverted to the corresponding pixel locations on our display screen. And those pixel locations are going to bedefined some place on the display screen, starting with (x,y) andgoing to the point (x+w-1). And then y + the height in pixels- one. So, this coordinate systemhappens automatically, and you can define thesecoordinates to be anything. And you can define these coordinates tobe any location on the screen, and so your 2D graphics that you'replotting on your canvas can be automatically resized andrepositioned anywhere on the screen just by controlling this canvasto screen transformation. You can also work directly inscreen coordinates by setting up a canvas to screen transformation that uses canvas coordinates thatmatch up with your screen coordinates. In this case, you're just setting yourleft edge and your bottom edge to x and y. And your right edge and your top edgeto x and y + the width minus- 1 and the height- 1. And in this case you can specify thecoordinates of your primitives and vector graphics using the same coordinates of thepixels that they will be translated to. I don't recommend doing this because whenyou are working in screen coordinates you are not going to know what youroutput screen display might be. It could be a cell phone it could be atelevision it could be a watch It could be a video wall, and all of thosewill have different resolutions. And you want to make sure thatyour two-dimensional graphics is properly displayed. It's not too small or too large,when it's displayed on different devices. So it's better to work in some canvascoordinates that's convenient for you. And let the canvas toscreen transformation, worry about converting it tothe corresponding pixels. So what have we learned? We've learned that vector graphicsis used to describe shapes and that raster graphics are what we use to displaythose shapes using a table of pixels. And that we can set up coordinates thatare convenient for us to plot in a canvas, and those coordinates are differentfrom the raster coordinates that we used to display the canvas. And we can set upcanvases within a canvas, which allows us to divide up the screenin ways that make it more convenient for us to Set up a two dimensionalvisualization display. [MUSIC]"
cs-416-dv,2,8,"[SOUND] So now we can learn how to draw two-dimensional graphics. What we're going to learn is how to setup a two-dimensional drawing canvas, and then how to draw things in it. And we're going to learn how do to thisusing the scalable vector graphics specification fortwo-dimensional graphics. The scalable vector graphicsspecification is a method for describing two-dimensional graphics fora variety of systems. One of the systems thatit works with is HTML. So you can embed two-dimensionalgraphics in a Web page by putting an SVG tag in your HTML code forthe Web page. And this is an example SVG tag. It has a width, a height, and a viewbox. The width and the height,describe the width and height usually in pixels of howthe graphics will appear in your Web page. The viewbox describes the coordinatesthat you'll be drawing with. And so these are your canvas coordinates. It'll start with an x and y-coordinates. And the canvas will extend tothe point x plus w, y plus h. So, it'll have a width andheight of w and h units for drawing. And so, the important thing toremember for SVG is at this origin. This x and y is the upper-left corner and not the lower-left cornerin my previous example. SVG particularly when it's being used fora Web page is being used to describe a document system, and we tend toread left to right, top to bottom. The coordinate system isset up appropriately. The other thing to remember isthat when we set up this viewbox, we're setting up ourcanvas coordinate system. These are the coordinates we'regoing to be drawing with. And when we describe the width andthe height of the SVG tag, that's describing the actualdisplay pixels of the result. And so we're automatically gettinga canvas to screen transformation between these two coordinate systems. So here's an example ofthat plot of a parabola, y = x squared,using SVG graphing coordinates. Here's our SVG tag,we'll set up some width and height for some number of pixels dependingon our output device. But we set up a viewbox that'sgoing to give us the coordinates, the canvas coordinates that we'regoing to use to draw the graph. And so, we'll set up coordinatesystems going from (0,0) to (1,1) for canvas coordinates to draw in. Then we'll draw the axis title andother decorations. Then we can set up anothercanvas with another SVG tag. And so one of the elements of the scalablevector graphics specification is another scalable vector graphics canvas. And in this case we're going toput the canvas at a point 10% in, since our original canvas goes from0 to 1, 10% would be 0.1 units. So we'd have a 0.0, 0.1, 0.1 here. That's the x, y-coordinate ofthe upper-left corner of our canvas, our blue canvas. And it's going to have a width of 80%. That'll be 80% of the originaldrawing coordinates. So it'll go from 0.1, 0.1 to 0.9, 0.9. And its width will be 0.8 units andits height will be 0.8 units. And so I can set this sub-canvas up andI can set its own coordinates systems. So this is the coordinatesthat's going to be displayed in in the outer coordinates system. And then these are the inner coordinates,the blue coordinates, that we're going to use to draw thingsin this blue coordinates system. So its canvas coordinatesextend from (0,0) to (1,1), (0,0) to 0 plus 1, 0 plus 1. So then we can plot the data in this case,since we're starting at the lower-left system with it plotted as (x,1-y),then we can use a coordinate system where we don't need to rescalethings inside the center coordinates. When we plot data here it goes througha coordinate transformation to figure out where the point is in the yellowcoordinate system and then it goes through another coordinate transformation to getout of the yellow canvas coordinates into the display coordinates definedby your width and height. So, there's several coordinate systemtransformations that are happening. But, we're setting up coordinatesystems to make it more convenient for us to draw, and letting the computertake care of all the transformations. As before we're going to draw primitivesby specifying vertices, and then we're going to connect those vertices witheither straight lines or smooth paths. Those are called strokes. And then we can fillin regions with fills. And we can specify various properties forfills and strokes and so on. And so we can go through the differentdrawing primitives available to us in the scalable vectorgraphics specification. A very simple one is a circle. So, if I've set my canvascoordinates go from (0,0) to (1,1), I can specify a circle by specifyingits center at the point cx, cy in canvas coordinates,in this case one-half, one-half, and I can set its radius to befour-tenths of the unit. Since five times of the unitwould get me to the edge, that leaves aboutone-tenth of a unit there. I can set a property ofthe stroke to be the color black. And I can set the property ofits fill to the color blue, and I get a black circle witha blue interior as a result. And that will be embedded,in this case, in my Web page. You can also specify a rectangle, and this is going to be anotheruseful data visualization tool. Again, we set up a canvas, a drawingcanvas, and then I say, rect, and I specify a point, in this case(0.3,0.2) gives me a single point. And then, I specify the width andthe height of the rectangle. And this point, I've specified will bethe upper-left point of the rectangle, and then I say that I want the lines,the strokes of the rectangle to be black. And I want it to be filled with the bluecolor, and I get this resulting rectangle. You can also specify paths. In this case, I've specified a pathby specifying a bunch of points, 0.2 0.1, 0.2 0.3, 0.4 0.3, through here. And then I've specifieda bunch of commands, and these commands are moved to the point0.2 0.1, so move without drawing a line. And then move while drawinga line from 0.2 0.1 to 0.2 0.3. And that way I can specifya line by starting here and ending here, and at 0.2 0.3. And I can specify the next line bystarting at the current position and moving to a new position, 0.4 0.3. So these Ms mean move from the previousposition to a new position without drawing a line. And these Ls mean move from a positionto a new position while drawing a line. This way I don't need to,if I'm drawing a continuous line from one point through several other points,I don't need to respecify these points. I don't need to say, draw a line from hereto here and then from here to here, and specify this point twice. I can specify this point once,and say just draw a point to here from the previous point, whichis the endpoint of the previous line. And so this will draw, what I wanted to bea block I but I'm missing this top part. And this is part ofthe path specification. So I'm drawing a path andd is the data for the path. If I want to get this missing line at thetop, I just need to add a Z at the bottom. And the Z just says close the path. That means draw one last linefrom the last point I specified to the first point I specified andthat will close the path. And then I can finally fill in thatclosed path by just specifying a fill specification. In this case, I didn't say stroke equals black becauseit may have been set up before that way. But that would give you a blackoutline of a blue region. So what did we learn? We learned how to usethe scalable vector graphics specification to make our own line art forWeb pages. We figured out how to set up an SVGdrawing canvas, which has its origin specification in the upper-left corner,instead of the lower-left corner. And then we figured out how todraw a circle, a rectangle, and any polygonal shape. [MUSIC]"
cs-416-dv,2,9,"[SOUND]So now we'll discuss three dimensionalgraphics for visualization. And in order to do threedimensional graphics, we have to simulate how lightgets from a light source. Off of an object into our eyes sothat we can perceive it, the same way that we wouldperceive a real world scene. So what will we learn? We'll learn about the real world waysthat light leaves a light source and reaches our eye. We'll learn the physics of why isthe sky blue, why is the sun yellow, why is grass green, and in general, how does three-dimensional graphicssimulate these physical processes? And so here's a typical scene of the realworld and how we would perceive that. We have a light source, an objectreflecting light and then a sensor, which is our eye. And light leaves,in this case, the sun, and this light is powerful across thespectrum, and so it's essentially white. But then that light goes to the sky. And the sky scatters the lightdifferently based on which region of the spectrum it's dealing with. So, light at the red wavelengthswill get scattered less and light at the blue wavelengthswill get scattered more. So that light gets scatteredto other locations in the sky, which makes the sky blue. And which makes the sunappear yellow because that light is making itall the way through. That yellow sunlight hits a tree orgrass or any other biome and certain wavelengths of thatlight are getting absorbed by chlorophyll molecules and turnedinto photosynthate which feeds the tree. And so the chlorophyll is absorbinglight mostly in the blue regions, the blue wavelengths and then the redwavelengths and reflecting light. Not absorbing the lightin the green wavelengths. So the light reflected offof a tree is mostly green. And so then it reaches, it hits us inthe eye, reaches our censor in our eye, so that we can perceive it. And there it gets sensedby essentially cones, the colors, the wavelengths of lightstimulate three different kinds of cones. A mostly red cone, a mostly green cone anda mostly blue cone. And these send three differentresponses to our perceptual system. And so, you may have a complex wavelengthof light indicating it's color, but at some point whenthe eye perceives that, that gets turned into three numbersinstead of a bunch of numbers. And so we get this tristimulustheory of light perception. And that's why we displaythings using red, green, blue colors in pixelson graphics displays, to directly stimulate the red cones,the green cones and the blue cones. So, we can simulate this process, andwe can look, for example, at a camera. And, a modern camera. In this case, a pinhole camerathat has a very small aperture is getting light reflectedoff of an object. In this case, this blue bunny rabbit. That goes through a focal point andit gets projected onto an image plane that would be sensed in moderncameras by a CCD array. We can take that image plane and we canjust bring that in front of the camera and we get the exact same image. It's not inverted. It's just placed in front. And you can think of thisas being a window and you're looking through a window outinto the world and seeing a blue bunny. You can imagine where that bluebunny would be there in the window that you see there and that'sthe exact same image inverted that you would get on the film plane, becauseof the geometry of this camera system. Well, we can remove the camera system and we can put an eye rightat that focal point. And this gives us this image plane. And so basically, when we're lookingout into a three-dimensional scene, what we're seeing is the same thing thatwould be projected onto a two-dimensional window to the world,through those same projectors. Just a certain fixed distance away. And that's the way we can simulatewith a two-dimensional image, what the eye would see ina three-dimensional world. And, in computer graphics, as we discussedbefore with two dimensional graphics, we're going to simulateshapes using primitives. In this case, vector primitives, except these are nowthree-dimensional vector primitives. And so, we have triangles. And the triangles are specifiedby three vertices. And those three verticeshave edges between them and a region we'll call a polygonal face. And you can create a mesh ofthese triangles to define a three dimensional object. In this case a bunny rabbit. And those triangles are thenprojected by what's called a three dimensional graphics pipeline ontoa two-dimensional image plain. And then the polygons on thattwo-dimensional image plane are then discretized in the pixels. This is a rasterization process. And in fact, the same rasterizationprocess we were discussing. In two-dimensional graphics whereregions and edges are discretized into pixels, instead of straight lines andcontinuous regions. And so the way 3-D graphics works is foreach one of these triangles, each one of these primitives we'regoing to figure out its' illumination by simulating the light coming froma light source reflecting off of it. And then we're going toproject that triangle and its illumination onto this image plane. And then we're going torasterize that triangle by filling its region in with pixels. And that's what gets displayedon the pixel display. And so we can speak of theseprocesses as vertex processing, This is also called transform andlighting. Where we're taking vertices of triangles,and projecting them onto an image plane. Rasterization, taking thosetwo dimensional triangles, and converting them into pixels. And then, pixel processing,which takes those pixel regions, and figures out what colorsshould be on their interior. We also will meet the viewthree-dimensional scenes. We need to figure out whereis the eye in the scene. And so we'll set up thingslike world coordinate systems. And this eye andthis teapot example, will be located at positions in three-dimensionalcoordinates in the same way that primitives were located in two-dimensionalcoordinates in the previous example. So we use this world coordinate system,and its origin is off someplace. And we can describe everythingrelative to this coordinate system. And we will specify a view. Basically, where are we andwhat are we looking at? Our eye point will be located at a certainset of coordinates and world coordinates. And we'll be looking at a pointalso specified as a set of coordinates in world coordinates,three coordinates, x, y, z. And then that view is going to betransformed into a new coordinate system that looks very similar to ourcanvas coordinate system, so that we have an x-axis and a y-axis. And in this case we're extendingfrom (-1,-1) to (1,1) and these coordinates are the coordinatesof the window we were looking through that forms the image plane. And another thing we want to do isto create a sense of perspective so that things farther away appear smallerand things closer to us, appear larger. In computer graphics, we do that byprocessing the primitives, themselves. We actually make the things thatare farther away smaller and the things that are closer to us larger. So this may look correct for a perspectivescene of a tea pot on a table, but if I look at it from the side view, what I've actually done is madethe front of the table larger. And the back of the table smaller, so thatwhen it gets projected on to my 2D image plane, I get correct proportions fora perspective rendering. And from this point,we've processed our primitives, and we would then rasterized them,and fill them in, in the same fashions that we did fortwo-dimensional graphics examples. [MUSIC]"
cs-416-dv,2,10,"[SOUND] When we use three dimensionalgraphics for data visualization, we're going to depend on photorealismto help us understand the data. We're going to use photorealistic shadingmethods with three-dimensional graphics, in order to make visualizationseasier to understand. Because we're going to usethese shading methods, these photorealism methods in order toadd depth to a two-dimensional image, so that we understand it's depictinga three-dimensional scene. So photorealism describes visual cues thattell our perceptual system that when it sees a two-dimensional image, it's seeinga depiction of a three-dimensional scene. Some of these cues are occlusion. Occlusion is shown here. The fact that this boy's handis in front of the frame. It starts to triggeryour perceptual system to think that this is a three-dimensionalscene of a boy climbing out of the frame instead of a two-dimensionalimage of a boy on a frame. Other things like perspective,the fact that this water's receding into the horizon, and details are gettingcloser together in the distance. Others like lighting, the light bouncing off the foreheadof this boy, or attenuation, that the water is becoming whiter asthe fog is attenuating the light. All of these add cues so that your perceptual system That's lookingat a two-dimensional image can understand that it's actually a three-dimensionalscene that's being depicted. One of the strongest cuesof these is occlusion. For example, this red orange rectangleis in front of this blue rectangle. Because you can see all ofthis red orange rectangle, you can't see all of this bluerectangle because it's being occluded. This sets up a depth ordering, we don'tknow much closer the red orange rectangle is to us from the blue rectangle,but we do know that it is closer. And occlusion is the strongest cue, and it's fun to play with it with opticalillusions because of that strength. Illumination helps us perceivethe orientation of surfaces and there's two kinds of illuminationsthat we tend to use. One is diffuse illumination, which isbrightest when a surface is facing a light source andwe usually see this from soft objects. Rough objects, things like cloth orpencil erasers or other surfaces that are scatteringlight in many directions. And then there's specular illumination andthat adds these white highlights and we usually see these on shiny objects,metallic objects. And these highlights are added inregions of the surface that are facing in a mirror direction, that would reflectthe light source towards the eye. And so we can combine these whenwe're doing data visualization. This is an example of particlesthat are leaving trails to show a simulation of the flow ofblood in the abdominal aorta. And we're using certain cues, for example, occlusion that some of thesetrails are in front of other trails. You can see this green trailis in front of the blue trail. We're using lighting because you cansee that these trails are cylindrical, they have a brighter side and a darkerside because of diffused lighting. Also we're seeing perspective,we see larger things here and smaller things, more details, in the back. And all of those add up togive you a better sense of the shape of this aorta andthe blood flow inside of it. There's some other cues thatare important, one is shadowing. Shadowing is an occlusion cue froma second viewpoint where that second viewpoint is at the light source. So here I've got twotwo-dimensional images. In both cases, we can think of these,due to perspective, as a flat blue plane witha red-orange ball on top of it. But we don't know if this red-orangeball is hovering over the middle of the plane or if it's laying onthe plane towards the back of it. If I add a shadow,you can see on the left case here, the red-orange ball is laying onthe blue plane towards its back. But, in this case, which is the exactsame image, I've just changed the shadow. We can see that this blue ball is hoveringover the middle of the blue square. And so in, for example,this visualization of a molecule, you can use these shadows to get a bettersense of depth due to the directions that these shadows are being cast froman understood light source direction. There's also perspective, in addition tobeing able to see more detail far away with perspective,perspective is based on size constancy. That objects are the same size,but farther ones appear smaller. You don't think that the objectis changing its size. You understand that the visual system is looking at a largerarea that's farther away. Because of the way it getsprojected onto our retina or to our image plane in the caseof computer graphics. In computer graphics, the way we do this is we actually makethings that are farther away smaller. And make things that are closer to uslarger by actually moving the vertices and then projecting onto a projection plane. And so, to get this perspective view where you've got the front of the tablelarger than the back of the table. What we've actually done isactually made the primitives of the table larger in the front andsmaller in the back. But computer graphics does this foryou automatically, so you don't have to actually do this. And so, these examples are demonstratedin this animation from the National Center for SupercomputingApplications on an F3 tornado. And you can see we're using perspective. The perspective allows us to seethe glyphs in the front of this visualization and also the patterns thatthese glyphs form as you go father away. And so this is a storm chaser view and you can see the occlusions of thesearrows, so you can get a sense. And the arrows are larger in the front andsmaller in the back so you can get a sense of the depth ofthis simulation of this tornado and the paths that theseparticles are tracing. And also we see shadows on the ground, shadows being cast fromone primitive to another. And also the lighting that theseprimitives are illuminated on one side better than they're illuminated,or more brightly than on another side to give you a sense that the shapeof these arrows are cylindrical. Also, as the storm chasermoves through the data set, things in the front are moving bythe viewer faster than things in the back. That's another 3D cuecalled motion parallax. And finally there's stereo, we can usestereo in the absence of other cues. In this case, I was just plotting pointsof a mathematical equation to create some shapes. And the way you visualize this is youhold your finger up in front of your two eyes and try to cross your eyes. So that your right eye islooking at the left image, and your left eye is lookingat the right image. Basically by focusing onthe finger until those two images in the background start to merge together. If you do that, then you get a stereogramand this two-dimensional set of points on this image plane should pop out intoa three-dimensional configuration. If you want to simulate this, then you can simulate this with twoseparate views of you from a left eye and a view from a right eye,looking in parallel at the data. Just make sure you don'trotate those views and you want to make surethe views are in parallel. So what did we learn? We learned that occlusion isthe strongest of all the depth cues and that shadowing is an occlusion froma a viewpoint at the light source. Illumination is useful for surfaceorientation that helps reveal details. And perspective gives us differentscales of visualization in addition to adding depth perception. And when none of thesecues are available to us, sometimes some ofthe other cues can be used. For example, stereo, we can usestereo in the absence of other cues. In this case, I was just plotting pointsof a mathematical equation to create some shapes. And the way you visualize this is you holdyour finger up in front of your two eyes and try to cross your eyes. So that your right eye islooking at the left image and your left eye is lookingat the right image. Basically by focusing onthe finger until those two images in the background start to merge together. If you do that, you get a stereogram, andthis two-dimensional set of points on this image plane should pop out intoa three-dimensional configuration. If you want to simulate this, then you cansimulate this with two separate views. A view from a left eye, and a view from a right eye,looking in parallel at the data. Just make sure you don'trotate those views. You want to make surethe views are in parallel. So, what did we learn? We learned that occlusion isthe strongest of all the depth cues and that shadowing is an occlusion froma viewpoint at the light source. Illumination is useful for surfaceorientation that helps reveal details. And perspective gives us different scalesof visualization in addition to adding depth perception. And when none of these cuesare available to us sometimes some of the other cues can be used,for example stereo. [MUSIC]"
cs-416-dv,2,11,"[MUSIC] So when we go to visualize data usingthree-dimensional computer graphics, we find that photorealism isn'talways the best choice, and sometimes we need alternative methods fordepicting three-dimensional scenes. If we can photograph orrender with computer graphics everything, then why do we need thingslike illustrations? And the answer is that illustrationscan more clearly depict a three-dimensional configurationthan a photograph can. And so then the question is how can we usethree-dimensional computer graphics to render an illustration? So a good example of thisform of non-photorealistic rendering is Gray's Anatomy. And here's an illustration ofthe human heart from Gray's Anatomy, and it's not a photograph. A photograph would be gory andcovered with blood and it would be hard to see allof these configurations. But with an illustration the artist hasmuch more control over the lighting and the texturing and the cut-aways, in order to more clearlydenote the structures of the heart. What we'd like to do is be able to dothe same thing with data visualization. So for example,if I have a simple model of a pig, I could render it usingphotorealistic illumination and inclusion andfills to get this shape here. But I could also use non-photorealistictechniques to get a more cartoony version of this pig that conveysmuch the same information. And you'll notice that this pig hasoutline strokes and it has fills. And so the outline strokesare created from contours. And for a three-dimensional scene,the way we get those contours is by tracing them to create thingslike silhouette curves, the border between the shape andthe background. And sometimes this boundary will coveritself and other contours of the shading. And so sometimes these contours willtrace behind other surfaces and so we still need to respect occlusion, so we have to trim away portions of thecontour that are hidden by other regions. But that leaves us with a setof outlines and a set of regions that we can then render using someof our vector graphics techniques instead of some of our three-dimensionalphotorealistic techniques. So when we fill in the surfaces we maynot want to use photorealistic lighting. This top line shows a bunch of spheresrendered with photorealistic lighting, and you can see that in each case we havea diffuse color illumination and then a white specular highlight. So we're getting diffuse illumination andspecular illumination. The light source is to the up andright of the object. And yet we don't see any informationin the dark side of the moon here, the dark side of the sphere. That's because this is shadowedbecause of photorealistic lighting. You're not going to see anyinformation in the shadows. You can still see the color of the objectbecause there's enough ambient light bouncing around to get the color, butyou don't see any orientation information. This very well could bea disk instead of the sphere. We know it's a sphere because ofthe illuminated side, and so we can create better illumination of the sphereusing non-photorealistic illumination. In this case replacing the diffuseillumination here with a non-photorealistic diffuse illuminationthat we get by taking the color map, in this case going from red to black, and then mixing it with a cool-to-warm, in this case blue-to-yellow, color map. And that creates a gradation thatgoes from a cool form of red, kind of a bluer shade of red, all the way toa warmer shade of red, kind of an orange. So we're going from a violet to an orange. And then we can apply that to the entiresphere based on the orientation of the surface and the light source, and thenwe get more information on the shadowed side of the object in addition tothe illuminated side of the object. And these shapes look even more sphericalthan they do in the physical case. So when we're displaying three-dimensionalshapes, in this case skeletal shapes, with photorealistic illumination you'renot seeing anything in the dark sides of these objects because of the natureof shadows and illumination. But when we apply non-photorealisticillumination we can see these contours, these divots in the bone that are revealedby this non-photorealistic shading, that are also hidden inphotorealistic shading. In both cases we've also addedthe contour to better indicate the outlines of these shapes. Here's another example. This is a photorealisticrendering of a part and this is a non-photorealisticrendering of a part. You're missing all ofthe shape information of the orientation of the surfaceon the shadowed side, and we gain that with ournon-photorealistic shading. And also the contours helpus see the configuration and the definite occlusion ofthe individual pieces. So what did we learn? We learned that photorealisticrendering is based on physics, whereas non-photorealistic rendering isbased on the psychology of perception. For non-photorealistic renderingwe're pulling out the contours and filling them in instead of necessarilyfocusing on the surface itself. And non-photorealistic rendering,in general, makes it easier to communicate shape andthe depiction of an object without being burdened with the physical realityof what the object would look like. [MUSIC]"
cs-416-dv,3,1,"[MUSIC] So we understand how a computer works. We understand how a human works. Now we need to understand the data. The different kinds of data and how data works its way througha data visualization system. And so there's a Data VisualizationFramework that helps us understand how data is processed forvisualization. First, data's available to usfrom a variety of sources, so there's a Data Collection operation. That basically combines the data froma bunch of different sources and processes it into one package forvisualization. Then there is a Mapping Layer,this mapper that takes the data, and the data's abstract and whatevercomputational representation it may be in, and it converts it into somegeometric representation, something concrete, that thena Graphics Layer can take and display. And that prepares it forvisualization, for communication across this visualchannel for human perception. So the Data Layer Is responsible forfinding the data, collecting the datafrom different sources. Making sure those sources, which mayhave the data in different formats, that the data can be combinedinto a uniform data store. And making sure that the data is relatedbetween different sources properly. And then there's some analysis andaggregation to run statistics on the data, make sure the data is collectedat the right frequency. And then that data, once it's in a singlepackage, is sent to the Mapping Layer, which assigns the geometry tocorresponding data channels, and can also perform other rathersophisticated data processing operations, things like contouring which take gridsamples and convert them into outlines of contours that can be useful forunderstanding what's going on in the data. And finally, there's a Graphics Layer and this converts the geometry intoa displayable image as we saw last week. It applies Decorations to the geometry. It assigns color. It shades the geometry. It's also responsible formanaging the interactions. The user input during the visualization. So that's the framework fordata visualization. As we look at the actual data involved andthat mapping operation, it's important to understand somefundamental features of data, the types of data that are out there. And so data can either be Discrete orContinuous. Discrete data has Discrete values,there's no between values. Whereas Continuous data has values betweenneighboring values that are possible. And then there's comparable data so thatthe data can be compared to each other or Unordered data wherethe values are not comparable. So we look at all the possibilities ofDiscrete and Continuous data of Ordered and Unordered, and you get the differentkinds of data that you'll be dealing with. For example, Ordinal data. Ordinal data is data that's ordered,but discreet. Things like shirt sizes. You can say a medium shirt is goingto be larger than a small shirt or a large shirt is going to besmaller than a medium shirt. But there may not be values ofa shirt size between medium and large. And these can be quantitativewhich are discrete and ordered. For example the counting numbers one,two, three. There's not one point five. So there's no value between 1 and2 in our counting numbers. So they're discrete, but yet2 is greater than 1 so they're ordered. And so those are examplesof discrete Ordered values. You can have Continuous Ordered values,and these are common real numbers,the real number line and so on. and values that we'd use, for example,for altitude or temperature or any continuous field of values. You can also have Unordered datain discrete and continuous forms. The discrete form of Unordered datacan be Nominal or Categorical. Things like shapesare examples of nominal data. They're not ordered. A square, circle and a triangle. You wouldn't say thatthe square is greater than or less than a circle,because there's no order between them. You could have categories. Different nationalities, orother other categories for data that may not necessarily becomparable and those are discreet. You can also have continuousvalues that are Unordered often Cyclic values are like this sodirections or cues, you wouldn't say that northis any greater or less than east. And you can having angle going from 0 to360 in degrees but after you get a 360 degrees, you go back to 0 degrees and soany one is not greater than any other. And then hues you could have red,green, yellow, blue and you may not say that red is anygreater than green and so these values are continuous there are colors betweenred and green, but you wouldn't think of red as being greater than orless than green, so they're unordered. And finally, when we're treatingdata as variables, in science, you may recall Independent Variables andDependent Variables. And when Independent Variables change, they can have an effecton Dependent Variables. That same notion happens in the datawe get from, for example, Databases. And so you'll have Key Value pairs, and Databases will store an index andthen a value associated with that index. And as the Key changesthe resulting value will change. So you have this Independent,Dependent relationship. In Data Warehouses the IndependentVariable is often a Dimension, something that's changingthat represents Dimension. And then the Dependent Variableis a measure that's happening as a result of that Dimension. So a dimension might be time, and then the measure might bethe temperature at that time of the day. And so as the time of the day changes, which is an Independent Variable,you would have a Dependent Variable, the Measure, being the temperatureat that time of the day changing. And you can store that in a Databaseas a bunch of times of the day and then the temperature at that time ofthe day as a bunch of Key Value pairs. And so the data is basically storedin these Independent, Dependent relationships in a variety of namesdepending on the context of the data. So the important thing to remember,the three fundamental kinds of data. There's Nominal data, there's Ordereddata, and there's Quantitative data. And we'll be using those distinctions whenwe determine how that data ends up being displayed in a visualization system. [MUSIC]"
cs-416-dv,3,2,"[MUSIC] So when we map data,we figure out how to convert the data into some graphics primitive thatwe can display, and that conversion is going to depend on how we perceivedifferent graphical objects and connect their characteristics tothe characteristics of the data. So, as we saw before, the data visualization frameworkhas this mapping layer. And this mapping layer takesdata in our data collection, and takes its abstract data values, and assigns them more concrete geometricvalues, and those geometric values are used by the graphics layerto display the data for visualization. So there's been some studies inthe 80s that basically uncovered how effective differentgeometric mappings are in the perception of quantitativevalues that they're mapped to. So, for example, we have this list here,and it's sorted by perceptual accuracy. So when we see data Indicated by mapping to position,we do a better job of understanding the quantities than we doif the data is mapped to color or density. So, if we have data varyingalong the horizontal axis here, we can map the data to position. And we can see that the rightbar is farther along. It's higher than the left bar. It's got a greater position. And, in fact, we can see howmuch greater the position is. We can also see it from length, andthe right bar is longer than the left bar. And these are very good methods fordisplaying quantitative data. And you can see that if youcombine the two of these, you basically getthe elements of a bar chart. Similarly, you can map data to angle orslope, and you can see that this right angleis greater than this left angle. That's a little bit less effective, but these are the elements that you would seein a pie chart, and pie charts are just a little less effective at displayingquantitative data than are bar charts. You can also map data to area. And so the circle on the right issmaller than the circle on the left. It represents a smaller area. It's just a little harder tofigure out how much smaller the circle on the right isthan the circle on the left. And you can also map to volume,and so the sphere on the right represents a smaller volume thanthe sphere on the left, but it's even more difficult to figure outhow much smaller the volume is here in the orange sphere on the right thanit is for the blue sphere on the left. And finally, color or density. In this case,we have two equal size spheres. This sphere on the right is lighter,and the sphere on the left is darker. It's just difficult to know how muchlighter the sphere on the right is, compared to the sphere on the left. So it's interesting to note that,as we move farther down the list and find less effective ways of mappingdata to geometric or spatial values, that we move from one-dimensionalvalues of position, length, angle, andslope to two-dimensional values, or three-dimensional values,of area or volume. And when we get to color or density, we're no longer talkingabout spatial values at all. These are non-spatial values. And so further studies have investigatedthis at a finer level of detail and expanded that list. Again, we have position andlength being the most effective representations of data forspatially, or geometrically. But, for example, angle andslope have been divided so that angle is slightly better than slopefor discerning quantitative values. And, finally, density has been expandedto density, saturation, and hue, so we've basically got brightness,the intensity of a color, how saturated it is,how brilliant the color is, and then which color you actuallypick in the color wheel. So these are all representing, and theseare consistent with the previous study, representing quantitative values thatyou may want to display geometrically or spatially. We can also display other values. For example, ordinal values,values, again, that you'll want to mapto a visual display. In ordinal's case,instead of quantitative, we don't know how much greaterone value is to another. We just know that one value is bigger thananother or greater than another value. For example, shirt sizes. We know that medium shirt islarger than a small shirt. We just don't know how much larger. So for ordinal data,position still wins out, so position is still at the top of thelist for indicating ordinal data values. But then we have density, saturation, andhue second, third, and fourth in the list. So whereas the brightness of a color,or how intense the color is, or which color you pick, it's difficultto make actual quantitative comparisons. It is actually quite easy tomake the comparison itself. You can tell when one object isbrighter than another object, you just don't know howmuch brighter it is. And then the remainder ofthese stays in the same order, it just falls further down on the list. So length, angle, slope, area, andvolume are a little bit less effective at looking at relative orders of thingsthat don't have quantitative values. And this gap here is filled in withsome geometric mappings that don't make sense for quantitative values, thingslike texture, connection, and containment. And so we wouldn't think of one textureas being different, quantitatively, than another texture, but you can makevalue judgments between textures, and so this is more textured than this is. You can make connections between objects,and these two objects are connected andthis one isn't, and that sets up an ordering that'snot necessarily quantitative. And finally, containment, the fact thatone object contains another object, and a third object is notcontained by that object, is another ordinal organization thatdoesn't make sense for quantitative data. And finally, there's a third categoryof data we can look at, nominal. These are categories of data. Ordinals are values that we can compare,but that we don't know how muchgreater one is to another. With nominal values, theseare categories that aren't comparable, things like comparinga square to a circle. A square isn't greater than a circle orless than a circle. They're just different. And so, again, position is still the strongest indicator of difference fornominal data values, but then what would ordinarily be second? Density, saturation, hue,texture, connection, and containment remains as powerful fornominal axes, except that hue and texture become stronger, and the restof these become a little bit weaker. And so hue, the actual color in a colorwheel, is a good indicator of category, even though it's a very poorindicator of actual quantities. And then, finally, length,angle, slope, volume, and area end up being similarly poor for describing categories as they are fordescribing base sizes. There's an additional value here that youcan map categories to shape, so you can map nominal data to shape, and I've beenusing shape as an example of nominal data, circle, square, and so on, and that's moreeffective than mapping categories to, for example, the length orangle of geometry. So we've learned that there's manydifferent ways of connecting our data to different graphics attributeswhen we display data graphically for data visualization, and we've learned thatsome of these ways are better perceived than others depending on whether the datais nominal, ordered, or quantitative. [MUSIC]"
cs-416-dv,3,3,"[SOUND] So, data visualization canconsist of some very simple charts, but the success of a datavisualization can often depend on how we map our data variables tothe elements of those charts. So we can start with a Bar Chart. And the bar chart hastwo axises typically. You've got a horizontal axis anda vertical axis. And you're usually measuringdiscrete values here, and some either discrete orcontinuous value vertically. And this benefits from the factthat you're mapping a variable, a data variable, to both position,the actual height of these bars, as well as to a length,the size of the bar. And so you do a really goodjob of not only seeing. That, for example, the orange baris larger than the blue bar, but how much larger the orange baris to the blue bar because position and length are both at the topof perceptual effectiveness for displaying quantitative values. And so usually vertically we have somesort of quantitative dependent variable. And then horizontallythese can be categories. And so we have some nominal variable orat least some discreet variable here indicating the individualbars that we're plotting. And this is an independent variable,a dimension. And then this is some kind ofmeasure of that dimension. It's a dependant variable depending onthe value of this independent variable. Similarly, you have a line chart. A line chart has data pointsthat are connected by a line. And sothis is very very similar to a bar chart. These data points are at the samealtitude as the tops of the bars. So they benefit from position butthey don't have the length. That you visually see withthe bars in a bar chart. So you still do a pretty good job of being able to discern quantitative valuesand their relationship of quantitative values in the altitudes of thesedata points in a line chart. And so again we have a quantitativedependent variable vertically that's changing based on some quantitativeindependent variable horizontally. But now the horizontal value is somequantitative continuous variable and the vertical value also needs to bea quantitative continuous variable because we're drawing lines betweenthese data points and these lines imply that there's a continuity of valuesbetween these data points and these data points have a horizontal anda vertical component. These lines have a horizontal andvertical component. So you don't want to use a line chart todisplay data across categories because that's implying that there's in betweenvalues in between these categories and if they're nominal categories,if they're discreet, then there should notbe in between values. Your visualization shouldn't implythat there's in between being values. If we remove the lines,we get a scatter plot. And a scatter plot givesus some other flexibility. When we display a line plot,we're displaying a function. We're displaying some dependent variable that's changing accordingto an independent variable. So that there's one dependent value forevery independent value. So there's basically one measure foreach change in dimension. When we do a scatter plot,we have two independent variables so that I can have the samehorizontal value here and I can have two values associated withthat and so that can be a powerful value. You usually don't connect these witha line unless there is some order in which the data is coming in that youwant to associate with a line and that would be an additional dimensionyou could indicate on a scatter plot. But the line doesn't infer thatyou're plotting a function, because a scatter plot doesn't plot a functionunless the data's organized that way. And so you have two independent variables,a horizontal independent variable and a vertical independent variable. And you're getting an indication ofposition, both, horizontally and vertically, for the quantitativevalues on each of the two axes. You also get some cues based on density ifthese points tend to cluster in certain areas. You can also create a Gantt Chart, which is kind of looks like a sideways barchart, except the bases of objects don't line up with one of the axiseslike they would in a bar chart. And so in this case,we have two independent variables, things that are no longerrelated as a function, but you still get the benefits of position andlength. Gantt charts are usuallyprocessed diagrams that tell you the various stages of a project. And so horizontally a Gantt chartwould usually be some display of time. This may be a quarter, ordate, or some other time axis. And then vertically, this is somecategorical, often a discrete or nominal independent variable here vertically,and this is typically the tasks. So you'll have the first tasks andthen the second task, and the second task may startbefore the first task finishes. And tasks may stop and then start upagain, and so you get this overlap. Again, it benefits from both position andlength. but it operates from twoindependent variables. Again one could be quantitative and onecould be nominal similarly to a bar chart. But in a bar chart you haveone dependent variable plotted over an independent variable. In a Gantt chart you have to independentvariables And, finally, you have a table. In this case, you have two nominalvariables, two categories, for example,they're independent variables. One doesn't depend onthe other necessarily and you're just looking at twoseparate dimensions, and in plotting some value that would bethe entry in each of these table entries. So it really benefits from position only,and again, that position is discrete or nominal. It's not a continuous position,as it would be in a scatter plot. It's in discrete, quantized regions. You might also notice if youlook at this long enough, you can see some flashinghappening at the intersections. And it's, again, important toremember your perceptual psychology to know when you're laying thesethings out to pay attention to contrast to make sure that you don'tget some unwanted perceptual features. So here's a table thatvisualizes the decision you need to make of what chartto use in various situations, depending on the datathat you want to display. Very often you have at least oneindependent variable and then you may have a dependent variable on it oryou may have an independent variable. And your independent variable might bediscreet or nominal, some category, or it might be some some quantitytha varies continuously and your dependent value could similarly becontinuous or discrete, or an independent variable could be a category, orit could be a continuously changing value. Independent of your horizontal axis,and so depending on each of these configurations you could lookup in this table which you want to use. If you have an independent variable and a dependent variable, then mostoften you want to use a bar chart. You can use a line chart, but only whenyou have a continuous dependent variable and a continuous independent variable,because the lines indicate that they're in between valuesboth horizontally and vertically. You want to use a Gantt chart ifyou have a independent variable. That's continuous anda categorical axis vertically or a categorical axis horizontally anda continuous value vertically. Either one of those willform a Gantt chart. If you have two categorical axes,you want to make a table. And if you have two continuousaxes that are both independent, you want to make a scatter plot. So we use the kind of data thatwe're trying to visualize nominal, ordered, quantitative,whether it's continuous, whether it's discreet, whethervariables are dependent or independent. To not only figure out how theymap to chart elements, but more importantly to decidewhich chart best displays them. [MUSIC]"
cs-416-dv,3,4,"[MUSIC] So in our previous discussions,we've been focusing on the relationship between data points, where we'replotting the data points as points. In some cases, you want to display morethan just points, you want to display shapes to represent the data, and in thatcase you want to pack the shapes together. So here's an example called a word cloud,and this is basically the overview text from our data visualizationcourse from the Coursera page. And I've just passed this througha web page called Wordle.net and it created this word cloud whichbasically displays the most common words, the words that are distinctive thataren't everyday words like the, and and. And words that are used multipletimes get displayed larger than the words that are smaller. And then there's a process thatpacks these words into a collection, so instead of just displayingthe words in order horizontally that you might see in a tag cloud,this word cloud is packed together. And there is a spiralalgorithm that it uses to basically find a first openspace to pack a new word in it. Tries to start with the largest wordsfirst and then packs in the small ones later based on that same spiral trial anderror techniques spiraling out from the middle to try to pack the words intothe location that they'll fit into first. And so these techniques tendto use area as a measure, and we've looked at area as a measure before. It's a pretty good quantitative measure,but ordinal, it's not very good. Being able to tell whether two differentshapes have the same area can be quite difficult. So if you have consistent shapes. It's hard to tell if I have the sameamount of area in the blue circle as in this orange square. But if you have consistent shapes,if I have two circles, then I can tell that the areasare the same based on the length. And then if the shapes are aligned,if they share the same base, then I can just look at the heightat the position, at the top and tell if the orange circle is lessthan the blue circle in area. And so area is very difficultto determine, especially when you have inconsistent shapes, butbecomes useful if you have consistent and aligned shapes, because then you'reusing actual length and position. Also area, as the area increases ordecreases, the length is going to increase ordecrease as the square root of the area, and so that also complicates figuring outa quantitative difference between shapes. And so, if we want to plot a bunch of datawithout any coordinates attached to it, you could just pack the datatogether using, for example, area to display the data. So this is a Tableau plotof the population from our WDI database, andyou can see some of the larger items are labeled if the labelwill fit in the region, but here the area is basically indicatingpopulation, total number of people. The area is proportionalto the number of people. And because these are all the sameshape we can do a pretty good job of finding an ordinal. I can tell that China hasmore people than India. India has more people than United States. United States has more people than Brazi,and so on. And so we get a good indication,at least an ordinal indication, of the differences in population,and we don't need to use any other coordinates because we're we're justcomparing the areas to each other. We're not plotting anythinghorizontally or vertically, we're just using the horizontal andvertical axis to keep our shapes disjoint. So another way of plottingthings horizontally or vertically is to display them on a map,and in this case, we've distorted the map into a cartogram. Here's an ordinary atlas of the world,and here's that same atlas distorted based on population, sothat the amount of distortion, the area of the region isproportional to its population. And you can see that certain regions havegrown larger than they were before, and certain regions havegrown to be much smaller. There's not very many people in Siberia,and so that's very small given it's area. India and China have a lot of population, high population density, and sotheir regions have grown much larger. In Africa you can see that Nigeriahas grown to be quite large. And sothis is a a way of indicating with area, certain data that youwould want to visualize. Again, it's relative. It would be difficult to tell ordinallyif the there's more people in for example, France or Germany, based on thisdisplay because the shapes are different, and it's difficult to judge arearelationships between different shapes, but we can certainly tell thatGermany is is roughly the same shape it's just grown larger with a littlebit of distortion than it was before. And so that's the distinction we'retrying to make with the cartogram. So how do you create those cartograms? Well you start, in this case,with a grid of density data. And this is an old algorithm fordoing this. There's better techniques thatare more sophisticated but also much more complicated. This is a nice way to do it. If you just fill a grid in withdensity data such as your local population density so within some unitarea, what is the population density? And so it could be heavilypopulated in this area and lightly populated in these areas. And so you create a grid likethat of rectangular cells and then you just scale eachcell by that density value. So the squares that havea population density of one-half get shrunkby one-half in area. So that's about 70% horizontally andvertically. The areas that have populationdensity one stay the same size, and the areas that have larger populationdensities would grow proportionally, so these squares would become twice as big,these squares become three times as big. And then you can see that what we had asa grid before now becomes disconnected. And so to reconnect it, we want toreconnect these corners to their average of the four corners thatshared that corner. So we want to find a new point for this corner equal to the average ofthese four previous corner points. So we create a new setof grid vertices equal to the average of the four gridvertices that shared them before. Or three or two depending on, orone, if you're on the perimeter. And so we create this new grid andthis new grid is deformed. And so based on this deformed gridWe keep iterating that over and over until we get a final deformed grid,and then we look at the differencesbetween the original grid points and their new positions in the deformedgrid that sets up a deformation. It basically tells us what directionwe should move data points. Then, we take our shape vertices ofthe outline data of our original map and deform them based on these arrows,and we can smoothly interpolate these arrows if we need in betweenvalues in order to create the cartogram. Its basically, we take a vector graphics representationof our data of our map and then deform the vertices based on this type ofdeformation to create the cartogram. So another technique that might bebetter than a cartogram is the ordinary choropleth, andwe see choropleths all the time. It's basically data that'splotted over regions of a map. This is that same population data but plotted inside the originaloutlines of our regions. In here, population is Is just givenby the saturation of the color, and when we're looking at ordinal values, basically we can see that China hasmore population than India does, has more population than the U.S., whichhas more population than Brazil and so on. So you get a better indication of theorder of these things from a chloropleth than you might with a cardogram becausearea is such a poor indicator order, whereas density saturation hue,density being intensity of a color or saturation in hue,are much better indicators of order. So there's several techniques now forplotting data without using coordinate data associated with the data forit's layout. We're deriving coordinate data. In these most recent examples we are usinggeographical data, where we have maybe a region name, or a country name, and thenwe're deriving the actual coordinates of that, perhaps an outline to fill in,or a location on a map to generate. Or we're using some other packing orlayout algorithm to distribute the data. Those are all useful techniques in In thecase where you've got data that you don't want to use the explicit coordinates inthe data as coordinates for their display. [MUSIC]"
cs-416-dv,3,5,"In 2012, Jeffrey Heer and Ben Shneidermanpublished this paper, Interactive Dynamics forVisual Analysis in ACM Queue. The ACM is a professionalorganization for computer scientists, and Queue is a publicationtargeting CS students that shows them howcomputer science is applied toreal-world problems. In this case, the paper shows how interactivity is used to support effectivedata visualization. The interactivityis broken down into a taxonomy ofabout 12 different parts. These parts are organizedinto three broad categories: The interactivity used in setting up a chart and showingsomething on the screen, the interactivity in movingand changing that chart, and then finally,the interactivity involved in sharingthat chart with other people. So we start withcreating the chart, the data in view specification. The first step there is to create the chart to visualize the data. When we create a chart wechoose visual encodings. These are perhapsbest demonstrated by Hans Rowling's chartswhere he plots the income of nations horizontally and then plots the health of theirpopulations vertically. Then you see that there's a trend showing thatpoorer nations, the populations areless healthy and richer nations the populationstend to be more healthy. So this is an exampleof visual encoding, but there's more visual encodings in the glyphs of the chart. For example, the size ofthese circles is corresponding to the populations ofvarious countries. You can see that there'snot much correlation in the population of countriesand income or health. But you can also encodebased on regions. So we can see Africahere contains a lot of countries that have less wealthand are less healthy. But China, and Australia, and Southeast Asia tends to be spread around invarious places in the chart and doesn't have such a correlationwith income or health. So there's all sorts of encodings here and Hans Rowling is also particularly wellknown for using time. He would basically movethis chart through various decades intime and you would see these circles basicallymove in different directions. You can see hownations may become wealthier or less wealthy and the effect onhealth afterwards. So in each one of those cases we're looking atthe visual encoding. The interaction comes inthe creation of these charts, and in the abilityto investigate and learn about the data through the interaction ofadding chart elements, and visually encodingvarious aspects of the data, and understanding whathappens as a result of the interaction ofvisually encoding the data. We'll talk about filteringand sorting later. Finally, there's derived values. So here's an example, at the University ofIllinois computer science is in demand especiallyat the master's level. We've got this increasing numberof students that want to get a master'sdegree at Illinois, and we can only accommodatea small portion of them. In order to understand howwell we're doing that, we can plot derived valuesand interactively we can start to look at what derived values makessense in this case. For example, we can look at the selectivity whichis the number of students we areadmitting that year divided by the number ofstudents that applied, and that selectivity is actually decreasingbecause we have so many students thatare trying to apply and only so many studentswe can accommodate. Similarly we canlook at the yield, the number of students that accept versus the numberof students we admit. That gives us the yield,the number of students that end up on campus, and that's fairly constant. So these derived values that we can generate interactively based on a graph can help us understand some of theunderlying structure in a graph."
cs-416-dv,3,6,"As we have seen already, sorting is an important part of the visualization processbecause it imparts a visual ordering that can help the viewer moreeasily spot trends, correlations, or clusters. Sorting and thevisual ordering of information present ina visualization can be a really powerful toolto reveal structure and patterns in the data as broughtout by the visualization. You can sort along a dimension to reveal trends and correlations, or you can sort a measureand that can prioritize important data and reduce the distraction ofvariants and noise. For example, we had sortedthe total population of all the countries of the world by the sizeof the population, and that showed uswhich countries had the largest populations, and we had a long tail of countries that hada very small population. Whether or not yousort or provide some form of visual orderof your visualization, the viewer will payattention to the ordering of information and ascribesome importance to that. They may not even realizethey're doing it. So sorting and visual orderingare quite important, and we have a couple ofexamples to demonstrate this. Here we have an adjacency matrix. So if you have a graph and there's an edgebetween two nodes, the adjacency matrix basically plots node numberhorizontally and vertically. So you get a symmetric matrix. The matrix has an elementthat's non-zero if there's a corresponding edgebetween this node and this node and sothis symmetric matrix is plotted visually here just bythis colored array of dots. Then you can see that it really doesn't have any structure. But there are methods of sorting. Basically, sortingthe node numbers, and just by reorderingthe node numbers, you can reveal structure in the matrix and in this casecommunity structure, just by reorderingthe node numbers in this matrix representation and that's called matrix ""seriation"", and there's all sorts ofmethods available to do that. Then that's another interactive step of trying to figure out the best way to sort the data in order toreveal that structure, and that's aninteractive process to reveal the structurein your data. A second example ofthe importance of sorting and visual orderingand visualization comes from my own experiencein evaluating students applying for a doctoral programin computer science. Students apply forComputer Science at the University of Illinois, and we have a webpage that they use to select faculty theymight want to work with. So we say, pick up thethree faculty members you'd like to work with, and then we havedrop-down menus with all the faculty and students pick the facultythey want to work with. Then the students havingmade that choice, that information is entered into a database and foreach student applicant, we can see the facultythey might be interested in workingwith and we can start to get an idea foreach faculty member, what students they wouldwant to pick from. So I wanted to plot to see how uniform this wasacross the faculty. So I used a data visualizationtool in order to do this. I came up witha standard bar chart, and horizontally, these are the differentfaculty members. There's some faculty members in our department andsome are affiliated with our department and some getmore students interested in working with them and some get fewer students working withthem and so horizontally, I've got the faculty membershere and then vertically, I'm plottingthe number of students that want to work withtheir faculty member. So we've got some facultymember that are very popular and some thataren't very popular. Again, some of the onesthat aren't very popular or faculty that are affiliated with the otherdepartments and students usually apply to work with them through other departments. I've eliminatedthe faculty names here just to avoid any embarrassment. So we've got this list and then I color-codedthe bar charts. This is a stacked bar chart. I've color-codedthe bar charts based on; blue, if the faculty member was the student's first choice. Orange, if the faculty member was the student's second choice, in green, if the faculty member was the student's third choice. You might see something here. I certainly saw it when Ifirst plotted this data, and that was that students were picking people inthe first half of the alphabet. This is an alphabetical order. Students werepicking faculty from the first part ofthe alphabet first and they're picking people from the second half ofthe alphabet mostly last. What I discovered was that, from this visualization,was that students were picking faculty inalphabetical order. Having made that discoveryfrom that visualization, I was able to work back intothe system and figure out why students were pickingfaculty in alphabetical order. The reason was we had three pull down lists withall the faculty, and so the students wouldgo down the list and find the first facultymember they were interested in and click there. They go down the list andpick the second faculty that they were interested in andthey would select that, and then they findthe third faculty member they were interestedin and click that. Then that would gointo the database. But when we were looking at the database in order to evaluate what studentsmight want to work with us, we were assuming students were picking the facultythey wanted to work with first as the first choice and their secondchoice was second, their third choice was third. When in fact, theywere just finding three faculty members and picking them when theyappeared in the list. So in this case, datavisualization helped us make a decision that we wouldn'thave been aware of otherwise."
cs-416-dv,3,7,"[SOUND]So often we want to visualizemultiple variables simultaneously. And so we generate stacked graphsthat can display several variables changing along the same dimension. For a bar chart, we may want to lookat more than just a single quantitative dependent variable measured acrossan independent variable dimension. One way of doing this iswith a stacked bar chart. In this case we can have two ormore dependent variables plotted. In this case, the blue bars arerepresenting one dependent variable and the red bars are representinga second dependent variable. And so we're using q to representdifferent nominal value, basically the second dependent variable is differentthan the first dependent variable. And then the total amount ofthe contribution of both variables is represented by the height ofthe top of these stacked bars. And the relative proportionis being represented by the proportion of the bar beingone color versus another color. And if we want to emphasizethat relative proportion. We can have the bars stacked to 100% and just plot the percentage ofthe contribution of one variable versus the other variable as the differencein the colors of the bars. This kind of relative contribution isalso represented in a pie chart but in a pie chart you're representingthe contribution based on angle and a little bit by area than on position andlength. Here in a pie chart the relativeproportion of the blue category versus the orange category versus thegreen category versus the yellow category is being represented by the angleas the angle's sum to 360 degrees. It's also proportional to the arearepresented by each of these four regions. That indication can be degradedsomewhat if you try to show a pie chart in three dimensions. And that's because in threedimensions we are still perceiving a three dimensional sceneas a two dimensional image. And the visual cues we're using includeforeshortening and perspective. And those cues can get in the way of ourability to perceive a region, say in the foreground, as being larger or smallerthan a region in the background because of our expectation that perspective is goingto make regions in the foreground larger. Or at least appear larger andregions in the background appear smaller, even if it's just a orthographicnon-perspective projection. So, it's best not to use threedimensions for pie charts and, in fact, it's better just to use a stackedbar chart instead of a pie chart. In this case, each one of these barscan represent a separate pie chart because it's representing the separaterelative contribution to the whole of multiple dependent variables, as theyvary across some independent variable. Also, stacking order matters. I've got the same data plottedin the left and in the right. In the left,I've plotted the blue variable first. And then I've stacked the redvariable on top of that. On the right side,I've plugged the red variable first and I've stacked the bluevariable on top of that. On the right it's easy to see thatthe red variable isn't changing across the horizontal axis. On the left, you can still see thatthe red variable isn't changing, but you have to see that becausethe lengths aren't changing. Whereas on the right,the actual positions aren't changing. Which further verifies that position isa stronger indicator than the length of the actual data values in termsof mapping two geometric and spacial display elements. You can avoid that stacking order problemby using diverging stacked bar charts if you have the red variable creatingbars growing upwards, and the blue variable creatingbars growing downward. But that stops working when youhave three or more variables. Another way to avoid having one variabledominate the other in stacking order is to stack them and then re-centerthe bar across the horizontal axis. So instead of growing upfrom the horizontal axis the bar is just centeredat the horizontal axis. And this give us the samedisplacement on the top and the bottom of the bar chart,but it helps us to pay more attention to the areas of the elementsinstead of their actual position. It's much better if you have continuouslychanging horizontal and vertical axis variables to use stacked line graphs,then stacked bar charts. If we can connect their data valuesby lines instead of having them discrete bars, then it's easierto see when areas are changing versus areas are remainingconstant as they move across even though they're displaced bythe variance of other variables. [MUSIC]"
cs-416-dv,3,8,"[SOUND] So, if we create stacked graph layouts, stacked bar charts, and stacked linegraphs of a lot of dependent variables, then you get some interestingeffects as displayed here. You start to see kind of a stream effect,and you can see the same data variable evolving as itmoves across the horizontal axis. But you also see some otherproblems with this representation. One is that the variantsin the lower variables in the stack can influencethe shape of the higher variables. And it can be harder to see when thingsget sheared towards the outsides, whether they're representing the variablesthe same value or if it's increasing or decreasing. And also you can see that by the timeyou get to the very top of the graph, it's changing quite violently becauseit's the sum of all the changes that have happened below it. We can analyze this byusing a few variables. If we let each variable at a givenhorizontal position on the horizontal axis, each variable be represented by say,f1 for the first variable, f2 for the second variable Andso on, f1, f2, f3. Then we can define, basically,ground zero, g0, to be the baseline level. And in this case, we're just setting g0to be 0, just at the horizontal axis. Then g i is the position of the topof the plot of the i'th variable. And the top of the plot of the i'thvariable g i is just equal to g0 plus that variable andall the variables below it in the stack. And we can start to do some analysis. And because of this analysis, there isan alternative layout called ThemeRiver. And ThemeRiver basicallycentered the vertical plot, the stack of variablesalong the horizontal axis. Set it set g0 to be one-half of the totalheight of the stack of variables. And by doing that, it basically said thatthe way this thing is varying at the top will be a mirror image of the waythis thing is varying at the bottom. And you get more of a, the appearance ofa river that data is kind of streaming by and evolving and it reduced the amountof shearing that was happening, but it didn't eliminate it completely. You can see for example, this regionright here, is creating a large shift in the data around it thatyou'd like to be able to minimize. So just by centering the vertical stackof data around the horizontal axis minimizes the heighth of the chart around that horizontal axis and it also minimizesthe slope at the top and the bottom. There's a Streamgraph layout thatdoes an even better job of this and it does this just by changingwhere the position g0 is. Streamgraph sets g0 equal tothe result of this formula. So you just evaluate this formulabased on your data values f i, and we won't go throughthe derivation of this formula. But by just changing where the baseof this stacked bar chart or stacked line chart occurs andthen stacking based to that new baseline, we get an even smoother appearance andit makes it even easier for us to make comparisons as we movehorizontally across this chart, to determine relative changes toeach of these dependent variables. It minimizes the deviation andthe wiggle, the deviation being how far a variable's plot moves from its previousposition on the horizontal axis, and the wiggle is minimizing the slope,basically the sheer effect that you get from the wiggle. You can also improve the appearanceby changing the order in which you add variables. So, if variables are zero until a certainposition on the horizontal axis, and then they changefrom zero to some value, you can change where they appearin the stack of variables. And so in this case, they're juststacked in some fixed ordering, some arbitrary ordering. And you can see kind ofa stream in the coloring. Coloring each variable differentlyhelps perceive that stream. In this case they're coloredbased on when the variable starts to take on a value other than zero. But you can also add new variables when the variables take ona value other than zero. You can always add them tothe outside of the graph. And what that does,is it takes variables that start out at a certain point on the horizontal axis,as soon as they take on the nonzero value, you add them butyou add them to the outside, so their initial surge in value doesn'tdisturb the other variables. It's happening onthe outside of the graph. And then as it's waning, other variables are taking non-zero values andbeing added to the outside of the graph. And so you get this nice, flowingappearance where you can see the relative change in values of these variables asyou move right on the horizontal axis, even though we're looking at 10 or20 variables in a given stack, vertically. So we learned how to make a stream graph,and how something as simple asstacking a bar chart can become a point of further investigation inorder to how to display it effectively. We also learned that, for example, a pie chart can become misleadingespecially when shown in three dimensions. [MUSIC]"
cs-416-dv,3,9,"[SOUND]So we're used to seeingtwo-dimensional images, and we can use two-dimensional images toperceive our three-dimensional world, and we perceive time as maybea fourth dimension, but then our ability to perceive dimensions islimited to those three or four dimensions. Parallel coordinates offer us a wayof plotting even higher dimensional spaces than that. So in looking at the higherdimensional drawings or higher dimensional spaces,you can think of something like a square. A square consists of twoone-dimensional lines, these two vertical lines,that are separated horizontally. I've taken a one dimensional line,copied into a second vertical line and then connected them up withlines between the vertices. So I've now got a two-dimensional square. If I want to make this two-dimensionalsquare into a three-dimensional cube, I just copy the square, and move it ina direction, and then I just link up the corresponding vertices with lines, andthat gives me a three-dimensional cube. If I want a four-dimensional cube,I just take the three-dimensional cube and I make another copy of it andoffset it in another direction, and then just draw lines betweenthe corresponding vertices and it gives me a tesseract, some two-dimensionalprojection of a four-dimensional cube. And so you can do this, butit's not very helpful for visualization, because you've got allthese different directions that are being projected into a two-dimensional image,and after three dimensions, we're just not used to perceiving the world in fouror higher dimensions, and so we just have difficulty perceiving four-dimensional orhigher data being projected that way. If I have a scatter plot, for example,I can pretty readily see x and y encoded in the positional Cartesiancoordinates of x and y here. If I had a z coordinatethat's being projected, z is being foreshortened asit's being projected here, and I've gotta mentally remember that zis some combination of horizontal and vertical that's different thanthe horizontal x or the vertical y axis. So you can draw these hints thatare basically similar to a shadow, where I'm indicating,with these dashed lines, what the coordinates of this greendot are in three dimensions, because it lacks other visual cuesof where it is in three dimensions. If you try to do this in four dimensions, then it becomes a nightmare tomanage all those dimensions. So there's a better technique for doingthis called parallel coordinates that Al Inselberg demonstrated tome back in the early 90s, when they were first invented, and I wasquite blown away by his demonstration, and I'll try to reproduce it here. So on parallel coordinates, we'regoing to take the Cartesian coordinates. We have a horizontal x-axis anda vertical y-axis, and we're going to take these axes andwe're going to make them parallel instead of orthogonal,at right angles, as they usually are. So I'm going to take the x-axis,I'm going to put it here, and I'm going to take the y-axis andI'm going to put it here. And so I'll label this axis x andthis axis y. So now the two axes are not orthogonal. They're parallel to each other andthey don't extend from the same origin. So the origin here ishorizontally at the bottom and increasing x goes up this axis,increasing y goes up this axis. So now I've got the data points, and Ineed to figure out where these data points occur, so if I take the y-coordinatesof each data point, I can map each point onto itscorresponding position on the y-axis. I'm just basically dragging them acrosshorizontally, because their position in y in this chart corresponds totheir height along this y-axis. If I want to do the same for the x-axis, I'm going to basically takethe x position of each point, and then I'm going to drag it to the correspondingx position on this vertical x-axis, so the horizontal length here correspondsto the vertical length here, and so blue and red are at the samehorizontal x coordinate, and so they overlap each other on the x-axis. And in green is a little bitfarther to the right, so it's going to be a little bithigher on the parallel x-axis. Yellow is a little bitfarther to the right so it's going to be a little bit higher. And then this blue green colordot is the farthest right so it'll be the highest on the x-axis. So now I've got this one set ofpoints that's now appearing as two sets of points, andI've got a correspondence between color, and color's a good perceptualindicator of category, but it's very difficult toperceive what's going on here. So what we're going to do is, instead ofdisplaying these as points on the axis, I'm going to connect thesepoints with lines, and I'm going to delete the original points,and now we get this nice duality betweenpoints in the coordinate system, here, andlines in the coordinate system, here. So, this orange point, here,has an x-coordinate and a y-coordinate. It corresponds to this line connectingits x-coordinate to its y-coordinate. And so each one of these five pointsin the Cartesian x y coordinate system corresponds to a line inthe parallel x y coordinate system here. Some other features are collinearity,so if these three points lie along the same line, then you get this niceconvergence in parallel coordinates. And so you can see some co-linearitythat happens, because lines in parallel coordinates, corresponding to points inCartesian coordinates that are collinear, will basically converge in a pointin the parallel coordinate system. So you can add extra dimensions. If I add a z-axis here, I'd have to addall sorts of three-dimensional queues to these data points to figure outwhere they are in three dimensions. In parallel coordinates,I just add a z-axis here. And then I can connectthe z-coordinates to the y-coordinates, and follow this line from its x-coordinateto its y-coordinate to its z-coordinate. And if I add a fourth w-axis. It's actually easy toadd a fourth axis here. And in fact, all of these points may becollinear in the z w coordinate system, and you can see because these linesare kind of converging to the same point. The point that they converge to doesn'tnecessarily have to be between the axes. But there are some ways of helpinga person to see these points. And so these parallel coordinates arereally useful for high-dimensional data. And you get this correspondence of beingable to follow these lines through the various coordinates andhave them correspond to points here, and it's easier to see some relationships. There could be correspondencesbetween the w-axis and the y-axis, and we wouldn't see those unless wealso had lines from these points on the w-axis to the points on the y-axis,and so you get a bit of a combinatorial nightmare when you try to connectevery axis to every other axis. So there's some decision making thatneeds to happen if you're going to show correspondences between axes, to choose to have those axes next to eachother in the parallel coordinates system. So parallel coordinates takethe orthogonal axes of the Cartesian coordinate system and they lay them out inparallel, and you can have any number of these parallel coordinate axes laidnext to each other, and you can start to perceive higher dimensional datausing these parallel coordinates. Some of the problems of parallelcoordinates are the fact that you're only seeing the pairwise relationship betweenthe axes, but they can be very useful for finding certain features in your data,for example, collinearity. [MUSIC]"
cs-416-dv,3,10,"[MUSIC] So, sometimes we have coordinatedata associated with our data sets. But that coordinate datais high-dimensional. Sometimes, that's not the right coordinatesystem to use to display the data. And we want to find the righttwo-dimensional subspace of that high-dimensional coordinate data in order to display the datain a more intuitive manner. That technique, that approach iscalled dimensionality reduction. And principle component analysis isthe first method we'll discuss for dimensionality reduction. So as we layout data and look at ways oflaying out data based on relationships. It's useful to look at some othertechniques for laying out data even if the data does have variables or attributesor coordinates associated with it. So one of these techniques is calledprincipal component analysis. If I have a data set like this one, each data value has an x coordinate anda y coordinate, but you can really see that the main variance of the data ishappening along this diagonal direction. And the data isn't varying muchperpendicularly to that direction. So, the x coordinate doesn'tentirely capture the data. And the y coordinate doesn't entirelycapture the variance of the data. The variance is actually happeningin a different direction. So we can look at the varianceof the data as being the actual dimension where thingsare changing along the data. And that may not beexactly the x direction or the y direction,it may be a combination of them. And as we look at higher-dimensional data, we may find that the data's varying insome direction that's a combination of x, y, z, w,how ever many dimensions you have. And so principal componentanalysis helps us find these directions where the data's varying a lot. And also the directions thatthe data is varying a little bit and then we can reorient it so that we can just display the dimensionswhere the data is varying a lot. And ignore the directions thatthe data is varying a little bit and then align those dimensionswith the displayed dimensions. So in order to perform principal componentanalysis, we have to analyze the data. First, we have to take the data andthen we have to subtract the mean. So for each of these data items,we're going to add them up, divide by the total andthat'll give us the total number. And then that'll give us the mean. We subtract that from all the data thatcenters the data about the origin. And then we're going to computethis Covariance Matrix. It's a big matrix. If our data elements are four dimensions,x, y, z and w, then the covariancematrix would be a 4x4 matrix. It'll have these many dimensionby these many dimensions. And it's just the data items afteryou've subtracted their mean, multiplied by the other coordinates ofthe data items and then the average. And that covariance matrix will yielda symmetric matrix because, for example, Yi times Xi is going tobe equal to Xi times Yi. And so you can think of a matrix,a large matrix as something that transforms a sphere in highdimensions into some kind of ellipsoid. It's going to stretch itout in a certain direction. And when you square that matrix andmultiply times its transpose, it's going to create a square symmetricmatrix, similar to that covariance matrix. When we pull out the eigenvectors andthe eigenvalues of that matrix, these are directions that where, when youmultiply the matrix times that direction, that's the same thing as justmultiplying it by a scalar value. So these are directions where,if a matrix is stretching a sphere into ellipsoid, so the directionsof the principal axis of that ellipsoid. And so, if you have a lot of variance,you'll pull out an eigenvector in this direction that's stretchingthe sphere out into a long ellipsoid. There may be two principal directions, in which case that sphere getsstretched out into this cookie shape. And so principal component analysis findsthose eigenvectors the direction where the data is varying the most andthen it ignores the small eigenvectors, the directionswhere the data is varying the least. In that way, we can display the two orthree most varying directions of data without displaying allof the dimensions of the data. So here's an example, I worked with a group of peoplea few years ago on a method for computing the geometry of a scaffolding tosupport bone growth in damaged mandibles. And so we needed some geometry for a mandible in order to figure outhealthy mandible and a damaged mandible, to be able to do a subtractionoperation to find this damaged piece. So, if we don't have an undamagedmandible, how can we find a mandible that was the shape of the damagedmandible before the damage happened? And the way we did this was, we scannedin a bunch of these mandibles and we specifically measured the shapeof these five data points. And so we had this big databaseof forty-some mandibles and each data point, each of those 40data points was 15-dimensional. Because each of these three datapoints was three-dimensional. So, five three-dimensional datapoints gave me 15 dimensions. So, I could describe the shape ofa mandible as a 15-dimensional point. We then perform principle componentanalysis to find three main variations. Three of the directions inthis 15-dimensional space where these mandibleswere changing the most. And they were kind of this direction here,that pulls the jaw down. This direction that causes the jawto kind of extrude in the front and then a direction that causedthe jaw to move to the side. And so based on that,we can plot all of our data points and this is an example of the mandible. All the different data sets of ourmandibles plotted as a single, just overlayed on top of each other. Each one of these corresponds toa position in the space of mandibles. And the three dimensions here thatI'm visualizing, these mandibles in, corresponds to a direction,a variation that I found from that principal component analysis,each of those three dimensions. So each one of these points correspondsto a point in a 15-dimensional space, but I'm plotting them inthe three-dimensional space. According to the directionof their highest variance, as represented by the eigenvectorof that covariance matrix. And so that performs a form ofdimensionality reduction where we can take high-dimensional data and plot it ina low-dimensional space for visualization. By basically focusing on the dimensionswhere we're seeing variation and ignoring the directions where we'renot seeing variation in the data. So principal component in analysisis based on linear algebra, on eigenvectors andeigenvalues of matrices. You don't necessarily have to understandthose details in order to use principal component analysis. If you have a package that willperform principal component analysis, then it will just convert the datafrom a high-dimensional form into a low-dimensionalform that you can display. [MUSIC]"
cs-416-dv,3,11,"[SOUND]So a second method for dimensionalityreduction is multidimensional scaling. It focuses on the distancebetween related items, as opposed to their actual positions. So multidimensional scaling isa form of dimensionality reduction. We previously looked at principlecomponent analysis as a method for dimensionality reduction. We have high dimensional data, and we want to display it ona low dimensional display. On just using maybe two or three axesin order to be able to better visually perceive the data,even if the data is high dimensional. And so we've looked at this interms of visualizing graphs. Graphs encode relationships between datapoints where the length of the edge is irrelevant. But if we want to look at a graphwhere the length is relevant, we might want to look at ways ofplotting the nodes of a graph or data, based on the relationship betweendata points, based on distance. So that we can try to preservethe distance in, for example, a low dimensional, a two dimensionalvisualization of high dimensional data. We may want the distance between datapoints in two dimensions to somehow indicate the distance betweenthe data points in higher dimensions. And so that sets up a metric MDS, metric multidimensional scaling whereyou try to preserve the distance between data points even though you're displayingthem at a lower dimensional space. So here's a nice example. We're not doing dimensionality reductionbecause we'll go from a two dimensional space to a two dimensional space. But it illustrates how we would computea multidimensional scaling to try to preserve the distance between points. So here's distances betweencities in the US, for example. So we've got Boston, NYC, DC, Miami,Chicago, Seattle, SF, LA, and Denver. And so the distance between,for example, Chicago and NYC would be 803 miles in this example. So the lower triangle of this datais showing the distance between the city on the left and the city inthe row, and the city in the column. And so there'd be 0 distancebetween a city and itself. And this will be symmetric, so I haven't bothered to fill inthe upper triangle of this matrix. So given those distances, dij,between data points i and j, between city i and city j, can werecover the positions of those cities? So I'm not takingthe coordinates of those cities, I'm just taking the distancesbetween those cities. Basically taking a graph where I'mspecifying the length of the edges between the nodes. And then I want to find the positionsof the nodes that satisfies those edge lengths. So we can do that byminimizing this function. We basically take the distancebetween the two data points, and then we want to subtract their actualdistance, and we want to minimize that. So if the distance between two datapoints equals the desired distance between those two data points,then this should be close to zero. And we square things here sothat we keep everything positive so that negative values aren'tmessing up our minimization. So we can solve this with any numberof non-linear optimization methods. In the example I'm going to describe,I solved it just using the Excel spreadsheet programwith an optimizer plug-in. So here's the results that I got, I minimized that distancerelationship between the actual distance from my optimized variables andthe desired distance. And as you can see,things are obviously in the wrong place. I've got the East Coast on the left andWest Coast on the east, but that should still preserve distance. And there's a few other differences, but basically I've got thisgeneral east to west trend. Seattle is above SF and LA,Denver is here, Chicago is where it's at. So I've preserved the distance of my points even though I haven't preservedthe exact geometry of the points. And so this can be a useful layout method,especially for high dimensional data when you're trying to preserve the distancesbetween the points without necessarily representing accurately their spatialconfiguration, their original coordinates. So we've thrown away the originalcoordinates of these points. And just using theirdistance relationship, tried to preserve that in a reprojection,in this case in two dimensions. So there's all sorts of ways you canuse MDS, multidimensional scaling. You can visualize affinitiesbetween data points, areas of collaboration basedon co-authorship of papers. If you're visualizing papers or the numberof any other attributes two data points might have in common,anything you can describe as a distance. It doesn't have to be coordinate distance,it can be other similarities. It's used in human-computerinteraction for user interface design tobasically lay out buttons. If you have a dashboard with a lot ofbuttons, you can lay out the buttons by organizing them based on how similarthe buttons are to a given task. And then, let multidimensional scaling compute thecoordinates of where the buttons should be located so that buttons that are usedtogether are close to each other. And it's also used in marketing tocreate what are called perceptual maps that are based on survey data of whatpeople think of different products, what attributes people assignto different products. And then you can figure out what productsare similar to other products and create a map of products that way. So, multidimensional scalingis enabled by optimization. You don't have to write yourown optimization program, you can use pre-existingoptimization packages. You just need something that's goingto minimize a function, and so you need some form of non-linearminimization in an optimization package. I found one that was enabledin Excel in order to do the example I described in the slides. [MUSIC]"
cs-416-dv,3,12,"[MUSIC] So for handling non-coordinate data,non-numerical data, we're often more interested inthe relationship between data items than in the data items themselves. And so we'll use a graph or a network to representthe relationships between data items. And so we'll talk here about differentways of specifying a graph, and different attributes of graphs, and howgraphs are represented computationally. So a graph just consists of nodes andedges. In this case, we have four nodes andwe have five edges connecting these nodes. And sothe nodes will represent a data item and the edge will representa relationship between two data items. If we add this particular edge, then every data item is going to beconnected to every other data item. We would call that a complete graph,or a clique, of these four nodes. Here on the left, I've got I've gota graph of four nodes connected by five edges, and on the right I've gotfour nodes connected by five edges. In fact, these two graphs are isomorphic. They're the same graph,they're just laid out differently. And on the right, I've got a planar graph,which means to nodes and edges are laid out sothat none of the edges cross each other. And then I can speak of a faceas being the region bounded by a cycle of edges starting andending at the same node. And also, we would say that theseare two different embeddings of the same graph orat least these graphs are isomorphic. If we think of this wholearrangement of eight nodes and ten edges as a single graph, then that would be one graph consistingof two connected components, but the entire graph containing both of thesesub graphs would be a disconnected graph. We can also have directed graphs. So in an ordinary graph,or an undirected graph, just has edges, but does not reallysuggest the direction of these edges. We can have a directed graph by usingarrows to indicate a direction, so that we would have a connectionfrom this node to this node, but not necessarily a connection fromthis node back to this node. And you can think of a graph as having acycle, a directed graph having a cycle, so this undirected graph, this ordinary graphhas a cycle because these three nodes. I can start at this node andI can follow the cycle to get back. You can have a cyclic directed graph,because I can follow a cycle here. This graph is acyclic,because there is no cycle. Once I get to this nodeI can go to this node. Once I get to this node,I can't go any place further. So there's no way to getback to any of these nodes by following these edges inthe directions indicated. You can also have trees. Any graph that's connected that has one less edge thanthe number of nodes forms a tree. It's minimally connected. And you can think of having a parent node,but for an undirected graph, any of these nodes could be the parent andyou can think of multiple siblings. Here's some graphs withmany more edges and you can see kind of the way they'relaid out would imply a parent. But I could lay these outin isomorphic methods, equivalent methods, and another potentnode might appear to be the parent. When you have directed graphs,the tree forms a hierarchy and then you can have a parent. So in this case, the child nodes,these child nodes point to their parents. And so I've plotted their parentshigher than the child nodes. And then there is a clear root node. This one parent that's the parentof all of these nodes. You can also have a hierarchythat's not a tree. In this case we havea parent relationship, but this child node has two parents. And soyou've got a definite hierarchy here, but it's not a tree because a tree would haveeach node would have a single parent node. There's also a relationship between the number of edges each node has andthe kind of graph you're looking at. We talk about the degree of the nodeas being the number of edges extending from a node. Directed graphs would have a differentin degree than an out degree. The in degree would be the numberof edges going into the node and the out degree would be the numberof edges leaving the node. And then these, the number ofnodes you have of each degree, if it follows this kind of fall off,it's called a social network. A social network, you can thinkof this as a friend's network or many natural data relationships follow this power law,this social network power law. In this case, this is the numberof interactions of yeast proteins. Each one of these nodesis a yeast protein, and the edges represent interaction betweenthese proteins and, in this case, you have many nodes that have a fewinteractions that are connected to, you know, one or two,you have one or two edges. And then you have many fewer nodes thathave high degree, that have a lot of edges that are connected to a lot ofother nodes by a single edge. And so in this case, the number ofnodes that have a certain degree follows a power-law. If the y is the number ofnodes with that degree, is equal to the degree to some powerwith some constant multiplied to it. And you see this falloff happen quite often. These graphs tend not to be plainer,they tend to be difficult to embed. They also tend to be the most populargraphs that we encounter in real life. Finally, we're going to use an adjacencymatrix to represent graphs. And so in this case we have a graphthat has four nodes, four data items. One, two, three and four, and then we're going to representthis graph using this matrix. In this case, the Adjacency Matrixwill have a one in row one, column two, if there's an edgeconnecting node one to node two. Likewise, it'll have a one in row two,column one, because it connects node two to nodeone and so it will be symmetric. Because this is a non-directed graph. If it was a directed graph, then youwould connect, if it was directed from node one to node two by an edgeleaving node one going to node two. Then you would have a onein row one column two but you would have a zeroin row two column one. So a directed graphwould have asymmetric or could have an asymmetric adjacencymatrix but an ordinary graph, a non-directed graph, would justhave a symmetric adjacency matrix. And these edges could also have weights, in which case you might not just havezero or one, you might have a value here to indicate how strongthe connection is between nodes. And the diagonal is usually zero, unlessthere's some relationship between a node and itself, andyou can represent that along the diagonal. So it might be good to review graphs andthe different kinds of graphs, the different representations of graphs,and the different attributes of graphsto familiarize yourself with them, because we'll be using those forthe rest of this module. [MUSIC]"
cs-416-dv,3,13,"[MUSIC] So when we visualize dataorganized as a graph, we'll likely want tolay it out in a plane. There's a kind of graph thatenables that called a planar graph. So as we saw before we're going torepresent data items as nodes and edges between nodes will representrelationships between those data items. In some cases we'll have a graph,a planar graph for example. A planar graph can be drawn such that theedges connecting the nodes don't cross. That's called a planar embedding. The same planar graph, in this case you've got the same foreignnodes connected by the same five edges. In this non planarembedding the edges cross. And so, the same graph,they're both planar graphs. What we'd like to do is beable to find the embedding, the way to lay out the nodes sothat the edges don't cross, so it'll be clearer to the observer,the relationship between the nodes. So we can do this. We can do this for very large graphs. For example, here's a triangle meshrepresenting a geometric object. It's a triangle mesh, sothe vertices are nodes and the edges of the triangles formthe edges between the nodes. This graph, this large graph of50,000 nodes is not a planar graph, but if I make cuts in it, if I separatethe faces and duplicate edges, I can turn it into a planar graph just bymaking these cuts along the black lines. And as a planar graph, I can then finda way of embedding it into the plane so that the edges don't cross. There's so many nodes and somany edges in this this graph and it's hard to see all of the edges,so trust me that they don't cross. Hopefully you'll be workingwith fewer nodes and edges so that your layouts will bea little more clear than this. So there's a way we can find theseembeddings automatically, and it's called Tutte's Method, and itstarts by picking some of your nodes and defining where theyshould be in the layout. So in this case, I'm going to use eightnodes, 12 edges that make a cube, and I'm going to takethese first four nodes, and I'm going to find positions forthem in for example an SVG Canvass. I'm going to position node 1 at (0,0),node 2 at (1,0), node 3 at (0,1), and node 4 at node (1,1). And then I'm going to try to figure outwhere do I want to position nodes 5, 6, 7, and 8, in order to make a plainerembedding of this graph. This graph is shown ina non planar embedding, because we've got edge crossings. We want to find out how tomake a planar embedding of it. So in order to do this automatically forany graph, we're going to need tosolve a linear system. So we'll create a matrix. In fact, an adjacency matrix, a special kind of adjacencymatrix called a Laplacian matrix. So it's an adjacency matrix which means, I'm going to take one node to another nodein case of node 1 is connected to node 2, so in row one, column two,I'm going to have a non zero entry. In this case, the non zero entryis going to be 1 over the degree. The node one has degree three, it has three edges extending from it,so I'll put a one-third there. And so node 1 is connected to nodes 2,3, and 5, and so I'll put one-third in columns two,three, and five, and node one isn'tconnected to the other nodes. So in those columns including node one,in those columns I'll put a zero. The nice thing about the Laplacianform of an adjacency matrix is the fornode 1 I'm connected to three items. My degree is three. The value I'm putting inis one over the degree. So if I add all of the elements up forrow one I get one. So no we're going to create a specialmatrix from that Laplacian adjacency matrix. We'll just call this matrix A. First thing I'm going to do is I'mgoing to take that Laplace matrix, and I'm going to zero out all of the rows forthe nodes I've already assigned. So I've already assigned Node 1,2, 3, and 4 a position. So I'll just zero those outin my adjacency matrix, subtract the whole thingfrom the identity matrix. The identity matrix just has ones downthe diagonal and zeros everyplace else. That will give me this matrixA that I'm going to solve in order to find the placements fornodes 5, 6, 7, and 8. So I need to set up a linear system,and so we'll set up a linear system forthe x coordinates of our nodes, and then we'll set up a separate linearsystem for the y coordinates. So the linear system we use forthe x coordinates looks like this. It's my matrix A, andthen I've got times x, a column vector that's just the xcoordinates of my nodes, and then finally I've got the answer, whatthe x coordinates actually should be in the case of the first four nodes andthen zeros for the nodes I want to solve. So this is saying that one times x one,the x coordinate of node one, one time x one andzero times everything else equals zero. Because that what I've assigned. So here I've got node 0,node 1, nodes 2 and node 3, these are the x coordinates of node 1,2, 3, and 4. And so here I've got one, this second row. 1 times x2 is equal to 1, and the xcoordinate of a sine for node two is one. So that's what I putin these first entries of this bx column vector, andthe remaining entries are just zero. The nice thing here is thatif we look at row five, row five says that the xcoordinate of node 5, x5, one times x5 minus one-third times x1, minus one-third times x6, minus one-third times x7,is going to be equal to zero. That's basically settingup a linear equation so that wherever I position nodes 6 and7, they're going to impact node 5. Once we do that for rows five, six, seven,and eight corresponding to nodes 5, 6, 7, and 8, and when we solve thissystem we'll have positions for nodes 5, 6, 7, and8 that solve that relationship. Do the same thing for the y coordinates. So now my by column vector is equalto the y coordinates for nodes 1, 2, 3 and 4 that I've placed, andthen add zeroes for everything else. Node 1 has zero for the y coordinate. Node 2 has zero for the y coordinate. Node 3 has one for the y coordinate, andnode 4 has one for the y coordinate, and this is basically saying that y1 = 0,y2 = 0, y3 = 1, y4 = 1. But it's saying that the y coordinate for node 5 is equal to whatever the ycoordinate is for node 5, minus one-third, times the y coordinate for node 1, minusone-third the y coordinate for node 6, and minus one-third the y coordinate fornode 7, and so on for the remaining nodes. So I've basically setup theseequations for the x system. I said x1 is equal to zero, x2 isequal to one, x3 is equal to zero, and x4 is equal to one. And then I've set up these equations forthe x coordinates of the remaining. I've set up similar equations forthe y coordinate. So if I group these together,I basically position x1 and y1 at the xy coordinates fornode 1 at (0, 0), node 2 at (1, 0), node 3 at (0, 1),node 4 at (1, 1), and I've set up these relationships forremaining coordinates. Now, you might notice something here. I'm basically saying that node 5It's going to be node 1's position plus node 6 position plus node7 position divided by three. So basically the position of each node isgoing to be the average of the position of its neighboring nodes that it sharesan edge with, and that's true for all the remaining nodes. And so we fix the position,we set the position for some of our nodes andthen the remaining are just going to be set to be the averageof the nodes that are connected to. And so if I solve that system, if Isolve that linear system you can use any numerical methods you like to solve thatmatrix problem, you get the solution. So that node 5 is at (1/3, 1/3). Node 6 is at (2/3, 1/3). Node 7 is at (1/3, 2/3), andnode 8 is at (2/3, 2/3). And so each of these nodes ends up beingat the average, the position of node five is equal to the average, the centroidof node one, node six, and node seven. So I take node 1 plus the position ofnode 6 plus the position of node 7, divide by three, and I get node 5. So we've solved forthese positions of node 5, 6, 7, and 8 to find this planar layout of thisgraph shown here in a non planar embedding by solving a matrix problem. You don't have to solve a matrixnecessarily to do this. You can do the same thing by basicallystarting with your nodes 1, 2, 3, 4 in their fixed position, andthen for all the remaining nodes, you can assign them random positionsto begin with and then iteratively, replace those node positions withthe average of their neighboring nodes. And if you continuallyreplace every node's position with the average of it'sneighboring node's position, those nodes'll work intoplace where they belong. And so you can solve this systemwithout setting up a matrix just by constantly doingthat iterative procedure of moving every node to the averageof it's neighboring nodes. So Tutte's method tells us that we cansolve a linear system to embed a planar graph, but it's not that difficult. You can do the exact samething by nailing down a few of your nodes into fixed positions. And then the rest of the nodes, you just replace their position withaverage of their neighbor's positions, the positions of the neighborsthat share an edge with that node. If you keep doing that over and over,those nodes will relax into the exact same position that tutte'smethod will converge to. [MUSIC]"
cs-416-dv,3,14,"So tut's method tells you how to lay outthe nodes of a planar graph in a plane so that the edges don't cross. If you don't have a planar graph, if youhave an arbitrary graph, you need to use a more specific methods to lay out a graphso that it could be easily visualized. So as we saw in the last set of slidestut's method for laying out a planar graph places each node at a position that'sthe average of it's neighboring nodes. And so each of the nodes is applyinga force to the current nodes position and so that's an example ofa force directed layout. Then there's quite a fewdifferent force directed layouts, a popular one is called Gem and that'swhat was used to layout this example. This is a interaction graphbetween yeast proteins, there is about 1500 yeast proteins andabout 2000 interactions and so the yeast proteins are indicated by nodes andthe interactions are indicated by edges. So in the GEM algorithm forforced directed layout, we're simulating the forceof a spring system. So each edge between two nodescorresponds to a spring, and that spring has a rust length if the nodesare farther than that rest length and they'll be attracted to each other. If they're closer than that rest lengththen they'll be repelling each other. And also, if you have high degree nodes,if you have a node with a lot of neighbors, then Jim will makethat rest length of the springs larger to try to push away thoseneighbors since there are so many of those neighbors sharingan edge with that high degree node. There's also a repulsion between nodes even if there are notconnected by a spring so that nodes don't end up on top of each othereven if they're not connected by an edge. And that repulsion force is basicallyequal to one over the distance between the node, between any two nodes. And then finally all the nodes experiencea force of gravity towards the center of the graft to keep thingsfrom wandering too far away. And a small random perturbationforce just to make sure that nodes take unique positions. As we talk about the layout of graphs,by placing nodes, it's good to think ofthe centrality of nodes. And centralities are ways of analyzing you know where a node shouldbe positioned in a layout. Should it be more central orshould it be on the periphery here? So these nodes that are bluein color are central nodes. These nodes that are red incolor are non-central nodes. There's many different centrality metrics. Ways of measuring how centrala node should be in a layout. Node degree is a simple method whereyou just count the number of edges that a node has. And so nodes with high degree shouldbe placed towards the center and nodes with low degree mightappear on the boundary. Page rank is a kind of centrality measurewhich Google uses to find popular webpages by basically counting the numberof incoming links from other webpages. Another is based on the isolation metric. You can think ofthe distance between nodes. For example this green node here to this red node has three edges betweenthat, so it's distance would be three. It's the length of the shortest number or edges to get from one node toanother node is the distance. The isolation matrix basically adds up the distance from a given node toall the other nodes in the graph. And if I add those up thenI'll get a smaller result for nodes towards the center of this graph. And a larger result to nodes inthe periphery of this graph, in this particular layout. And so you can think of closenesscentrality as basically the inverse of that isolation metric. So that nodes with high closenesscentrality would have low isolation metric. And you can also think of graphcentrality, which is like the isolation metric except I'm not taking the distanceto all the nodes and adding them up. I'm taking the distance between the nodeand the farthest node away from it, and one over that gives me what'scalled the graph centrality. And finally,there's between the centrality. Between the centrality basicallylooks at every pair of nodes, and finds the shortest pathbetween that pair of nodes. If I take the shortest paths betweenany node and any other node, all of those shortest paths, how many of thoseshortest paths go through a given node? That tells me the betweennesscentrality of that node. And so, if I take every shortest pathbetween every node and every other node. Those shortest paths will likely gothrough these central nodes more than they will go through these peripheral nodes. And so that's what we have plottedhere is nodes plotted based on their betweenness centrality. How many of the shortest paths between anytwo nodes go through each of these nodes. It's high forthese nodes that are blue in color and low for these nodes that are red in color. And you can also compute that for an edge, how many shortestpaths go through an edge? So, we can use use thatbetween the centrality or any of these centralitymetrics to simplify a graph. And so, for example, if I computethe between the centrality of edges of my network of yeast proteins. Edges that have lowbetween the centrality, have few shortest pathsgoing through them. And edges with high between the centralityhave a lot of edges going through them. And so a lot of shortestpaths going through them. And so if I remove edges that havefew shortest paths going through them you might think the impact would be lessthan if I have a lot of shortest paths going through that edge. So I've plotted the between the centralityof edges here, and I basically removed the lowest between the centrality,the ones that are very dark blue and left the highest between the centralityedges, the ones that are in red here. And I've not removed any edge ifit creates a disconnected graph. So the result is a graph that has as many edges as it has nodes,about 1500 edges and 1500 nodes. So I guess something like a treethat's minimally connected, but have retained the highbetween the centrality edges. And removed the lowestbetween the centrality edges. And so what's left is kind ofthe communications backbone, the most often used edges when I'm findingthe shortest path between any two nodes. And that simplifies the layout. Now I've got fewer edgesin order to compute spring distances when I use the samegem layout, force-directed layout for this graph on the right than I'm using forthis graph on the left. The nodes spread out morebecause I've got fewer springs. And the nodes are freer tomove around to unique places. And you can visually see the relationshipbetween nodes better in this layout than in this layout. You're also looking at fewer edges, so you could always add backin the edges as necessary, to add back in those low between thecentrality edges to see a more complete view of this graph, but the nodes wouldstill be positioned better than they are. When you try to compute the layoutusing all of those edges. So there's other techniques forlaying out graphs. Here's an example technique called edgebundles where we just put all of the nodes in a ring and we make a circle, a radiallayout of nodes and then we draw the edges in the region inside this disksurrounded by this circle of nodes. And we can start to group edges together if we have some way of measuringthe similarity of edges. We can create basically wire bundlescalled edge bundles that simplify the visualization of this graph. In this case, these are emails thatwere part of the litigation for a company called Enron between132 employees back in 2001. And here's a couple different layouts. This is a force directed layoutof these node positions and then instead of having the edges,just go straight line. Connecting each node we've triedto bundle the edges together so you can kind of see main communicationpathways in these representations. How do you find two edges to be similar or not similar to each other sothat you can bundle them? Well this, forthat task it's useful to form communities to try to cluster nodes togetherin communities that are sharing similar edge behaviors,similar edge connections. And so in order to find communities wecan remove edges in order of decreasing centrality, or decreasing betweenthe centrality, which is what we used. So instead of removing the lowcentrality edges like we did for node layout before,now we're removing high centrality edges. So now we're moving the edges that arepart of the main communication pathways. What that does is that disconnectsa graph and it isolates nodes. It isolates nodes in clumps and those clumps form clustersthat form communities. So you can think of a communityof nodes communicating through another community of node, across a highbetween the centrality edge, and so by removing our edges from the highestbetween the centrality to the lowest centrality edges, we're creatinga hierarchy of these communities. As we remove those edges, we can createnodes that represent where those edges connected communities to and so we can usethe resulting communities to figure out the order in which to lay our verticeson the periphery on this circle. And then by merging those clumps togetherwe can place vertices in the interior to denote higher level communities. These are called community notes,they're not part of the original graph. They're just used torepresent communities, to represent mergings of clumps of nodes. And so here's some examplesof Edge Bundle Communities. In this case we took every football gamebetween colleges in the United States. University of Illinois isone of these colleges. And by just taking, any time anycollege played another college, we would connect that with an edge andby looking at the final graph, removing the high between the centrality edges,they started to isolate communities and we were able to figure out a layout thatrepresented these low-level communities. We basically rediscovered the conferences. And so, just by looking at allthe games played between, games of football played between one university andanother in the US we were able to discover the conferences that organizedthe games between colleges in the US. And then we can use that communitystructure to figure out an edge metric. So, edges from onesection of the community to another section of a community can thenbe clumped together and form these edge bundles by basically attractingthem to these edge node positions. So here's an example of the actualcommunities from Enron, and here's the communities we discovered. These don't lie in the sameposition as the rest of these. For example here's wherethe president of Enron is and he's listed here in the CEO section here. But we found the same actualcommunities were discovered by basically removing high betweenthe centrology edges from the graph of all emails from one employee to anotherended up revealing these communities. And as we'll see later it's usefulto be able to filter out and look at details interactivelyin these edge bundle examples or in any large data set, so we can click forexample on the CEO of Enron, Kenneth Lay. And see all the email that was sent inthis data set to all the other employees, to all the other communities. For example I can click on Illinois, and I can see all the other Big 10conference games that it had, but I can also see the games it had toother places outside of its conference. So there's a variety of methods thatwe've just seen to visualize graphs. Not necessarily planar graphs,but any graph. Social networks and other graphs. And they're based largely on analysesthat are enabled by that centrality, the definition of some kindof centrality of the nodes. [MUSIC]"
cs-416-dv,3,15,"[MUSIC] So when we want to visualizehierarchical relationships, there's a particularly good method fordoing that called a tree map. So as we visualize graphs, and graphsrepresent relations between data items. One relation that we'll want to visualizeoften is hierarchical relationships, things like parents andchild relationships, ancestry and descendant relationships. And soin an undirected graph if you have a tree, any of these nodes couldserve as the root. But often in layouts wecan identify a root node. And we can look at ancestry anddescendants relationships based on that root node based on the edgedistance from a node to that root node. In a directed graph, we can set up a more definitive parent childrelationship using a directed edge. And we can have child nodespoint towards parent nodes, or the other way aroundif we set it up that way. We can specify a rootnode that has no parent, in this directed edge representation. And we can create hierarchies wherea node could have multiple parents, where we could make them an actual tree, inwhich case each node has a single parent. And then we can alsospeak of a tree depth, which is basically the number of parents anode has at the bottom level of the tree. Or the number of descendants, the maximumnumber of descendants a root node would have along one chain of these edges. And sowe often need to visualize these trees. This is an example from genetics of fish,and the nodes in this caseare laid out radially. And then the tree structureis shown on the inside. Here's another examplefrom the Tree of Life. And you can go to this website andzoom in on this image and see a lot more detail than is shown here. But this is a layout that showsa hierarchical relationship as well. One of the methods for displaying hierarchical relationshipsthat I've found quite useful is treemaps. Treemaps were introduced byBen Schneiderman in 1992. And, they map hierarchical organizationsof quantities into areas, so it's a very visual method for displayinghierarchies based on subset operations. And so, we have an example of thisin a utility called WinDirStat, which displays a Windows file system. And it represents the entirefile system by this rectangle. And then that root directory issubdivided into smaller rectangles representing sub-directories. And each smaller rectangle is subdividedinto smaller rectangles which are sub-directories of that subdirectory. All the way down to individual rectangles, where each of these individualrectangles represents a single file. And so it gives you this nice broadoverview of your file system, but it also shows you how the file system isdivvied up into individual directories and sub-directories and files. And so it's a good indication ofquantitative views, in this case, of disk space. It's telling me how my files and subdirectories are taking updifferent amounts of disk space. And so it's mapping disk space to area. And I can get a pretty goodquantitative measure of that disk space based on the area relationshipof each of these sub-rectangles. So how do we compute a tree map? Well, if you're given some hierarchy,some tree. And for a tree map, it has to be a tree. It can't just be a regular hierarchy. Each node has to have a single parent. You have some value,usually at the bottom level. For that previous example, this would be the disk spacetaken up by each individual file. And so we first need to do a bottom upprocedure that takes our tree nodes, and for the parent nodes of our tree, adds up the child nodes inorder to assign them a value. So now each of the parent nodes isequal to the sum of its child nodes. This parent is equal to 4. It's 1 plus 2 plus 1. This parent is equal to 11,because it's 4 plus 4 plus 3. And finally 16, which is the sumof all of these adds up to 16. And then we're going to use thatparent-child relationship and the values that it represents in orderto subdivide some rectangular region. In order to display how these smallquantities at the bottom are organized, as portions of this full 16 valueat the top, at the root node. And so the first thing we do is we needto subdivide our rectangle into portions representing the first set of descendants,11 and 5. And so we subdivide the rectangle intoa portion that is eleven sixteenths and five sixteenths. And so, we subdivide it horizontallywith a single vertical division at eleven-sixteenths of the wayfrom this edge to this edge. Into a portion that'seleven-sixteenths and five-sixteenths. Now we have two portions, and these portions are actuallytaller than they are wide. So we're going to subdivide themvertically instead of horizontally for the next step. So now we've got this portionhere on the left representing 11. We need to subdivide that intoportions representing 4, 4, and 3. So we subdivide them into a portionthat's four-elevenths along the way, another four-elevenths along the way,and then finally another three-elevenths along the way,representing each of these three nodes. And now these are widerthan they are tall, so we're going to subdivide them horizontallyinstead of vertically at the next step. And so we do this again, and wesubdivide this top node into one-fourth, two-fourth, andone-fourth portions horizontally. We do this with all the rest of the nodes, and we get this tree map ofall of the nodes we see here. And so here I've got 1, 2, 1. One-fourth, one-half, one-fourth ofthis rectangle representing four units. I've got 1, 1, and2 of this rectangle representing 4 units, and then 2 and1 of this rectangle representing 3 units. All together, those represent 11 units. And on this side, these 5 units,that were five-sixteenths of the entire triangle are beingrepresented by 3 and 2. And I've got a slightly thicker linehere differentiating between them. And then this 3 unit isby individual units here, and this 2 unit is byindividual units here. And so, we're using area anda hierarchy of area into finer and finer rectangles in order to divvy up thislarge rectangle into its component pieces. You'll notice that for example, this rectangle here iseleven forty-eighths wide because it's one-thirdof eleven-sixteenths. It's one-third of the eleven-sixteenthson this side of this thick black line. So one-third times eleven-sixteenthsis eleven forty-eighths. This bottom, the height of thisis equal to three-elevenths. It was three-elevenths of this whole unit. And so we have three-elevenths timeseleven forty-eighths is equal to one-sixteenth, which is exactlythe portion one-sixteenth. Where is it? Right here, one-sixteenth of the entirearea that we wanted to represent. And so that gives you a goodvisual connection between the area of this tree map layout asa portion of the whole. And the organization of thisnode representing one unit of this root node representing 16 units. Can also see some of the difficulty indoing ordinal connections between area. This is 2 units of area, andthis is 2 units of area. If I didn't have these numbers here,it may not be as easy to see that this rectangle is the same area asthis more square region is. So area's not that good atrepresenting ordinal amounts. But it is a pretty good tool forrepresenting quantities in general. So the tree map is a good method forvisualizing hierarchies because it relates the region of that hierarchy to a regionon the screen, associating it with area. That is,we've seen area can be a decent but not great method forordinal relationships. [MUSIC]"
cs-416-dv,4,1,"When visualizing large amountsof abstract information, Ben Schneiderman has some really goodadvice on how to do that effectively that he's formulated into a mantra. And this mantra goes like this,you overview first. Then, you zoom and filter. And then provide details only on demand. And so overview first,you provide a big picture of the data. You plot all of the data butdon't show any details about the data, giving you some big picture to be ableto understand the dataset as a whole. And then once you cansee the whole dataset, you can start to be curious andfocus on a particular area of the data. And so we zoom and filter in order tofocus on a particular portion of the data, subset of the data. And then, once we've foundan appropriate subset to look at, we can provide details only on demand. So details are the individualdetails of a single data item. But only when they're requested,otherwise plotting that many details about many items can easilybe visually overwhelming. And so this mantra has been shown towork really well on a lot of examples of large amounts of abstract information. So here's a example ofBen Shneiderman's mantra being used on the World Bankindicators to display for example, life expectancy and population forevery country in the world. We start with an overview,we've got a plot of every country in the world plotted over population andlife expectancy. I've zoomed in on the appropriateareas and ranges of the data. So the population goes froma minimum to a maximum. The life expectancy goes fromits minimum and maximum, and we can zoom in further as we need to. I've filtered out data, for example,you can filter based on continent, and I've used that filter to then colorthe data based on continent, and that reveals other information. For example, Africa in generalhas some low life expectancies. And Asia, in green,has some high population countries. And then you've got details on demand. For example, here is one country inAsia that has low life expectancy, and that's Afghanistan. And so by providing those detailson demand, they don't obscure or overwhelm you visually. They're the result of interactivityjust as a result of a query. And so this is a good example of how youcan use Ben Shneiderman's mantra for visualizing abstract informationfrom large datasets."
cs-416-dv,4,2,"So the mantra beginswith an overview to provide an overviewof the data first. So this overview is often a scatter plot ofthe entire dataset. In this case, I'veshown an example, it's a scatter plot of allthe countries of the world, which is a large amountof information and I've plotted it by populationand life expectancy. So this is every single country, I've omitted the labelsof each country, the name of each country because that would bea bit overwhelming, so we just have a data pointfor each country. By using these axes, we get a high-level viewof all the countries. We can get our headaround this dataset, and we can already start to see some interesting issues thatwe could be curious about. For example, someof the countries have low life expectancy. There doesn't seem to bea correlation between population and life expectancy. The challenge forthis broad overview, this ability to spread outyour data and see all of it at once is to be able tofind axes that do that. So in this case, I wantedto look at life expectancy, but then I've chosen population. This choice of population is a bit of an arbitrary choice, but it's one that spreadsout the data well. So for life expectancy, you'll notice wedon't go from 0-85, we go from 40-85because we've chosen tight minimum maximum bounds that better spread out the datain this overview. For population, I haven't chosen a linear scalefor population, I've chosen alogarithmic scale for population thatbasically displays orders of magnitudestarting from a billion going down toa 100,000 and less. So this is a logarithmicscale because that better spreads outthe data in this case. So here's an example ofplotting an overview of your data that allows you to see all of your data inone big picture. This is Tableau, and we're looking at the WorldBank indicators. We want to look at forexample life expectancy. So if I drag life expectancyover here as a field, I get an average life expectancyover all countries. Now, if I want to seewhat the life expectancy is by country, I can just drag the countryregion in the columns, and now I get individuallife expectancy average over the years that weremeasured by country. But this is quite a lotof countries, and so you don'tget a big picture. This is observablebut not displayable. I can observe all of the data, but I can't displayall the data in a single picture and stillsee all of the countries. So we'd like to providea broad overview of the life expectancy dataover all of the countries. In this case, we don't needto see every country's name. So instead of doing that, we can just dragthe country region over here into the marks, and now I've gotan individual mark for every country projected onto a single line which doesn'tspread the data out. So we need a way ofspreading it out. So we can pick for example population as a way of spreading out the datahorizontally. So now, I've plotted the data by life expectancy vertically and bypopulation horizontally. There are a fewcountries for example, India and China that have around a over abillion people each, and that's skewing the scale of the rest of these countries. So in order to spreadthis data out, we don't want touse a linear scale. We only use a logarithmic scale. So I can edit this axis andchoose a logarithmic axis. Now, when I plot logarithmically, the data gets spread out moreevenly across population, because I'm displaying populationby order of magnitude, factors of 10instead of linearly. But now the data is clumped up inthis upper right quadrant. So in order to seethe data more clearly, I want to spread this outacross the entire display. So life expectancy, insteadof going from 0-85 years, I want to change thatto about 40 years. So I can set theminimum life expectancy to some lower bound valueof say 40 years. Now, I can usemy display resolution to display more of the itemsin the separate more of the items by thatsetting tighter bounds. Again by population. If I'm set the population it looks like it goes fromabout 10,000 upto one billion. So we'll set the lower endof population to 10,000 instead of zero. Now, that spreads out the data, and I've gotthis nice overview of every single country inmy dataset that's spread out. It's plotting life expectancyand it's using population, actually the logarithmof population, as a way of spreadingthe data out so I can get a nice big overviewpicture of the data. So I can further investigate it."
cs-416-dv,4,3,"So the next step of the mantra is zoom andfilter. So let's look at zooming. What does zooming do? Zooming helps us focus ona portion of the plotted data. It removes extraneous data that'soutside of our zoomed in area. So here's an example ofa plot of cloropleth of plot of data across the United States. And in this case, we've zoomed ona portion of the west to the Midwest. And then, we've zoomed in again ona portion of northern California. And by zooming in at each of these levels,we're removing a large portion of our data set, so we can focus ona smaller portion of our data set. And by expanding that smaller portionof our data set to the entire screen, it allows us to use the resolution of theentire display to display more details. So at the highest level,we're just showing the state boundaries. But as we zoom in,we can start to show county boundaries. And then, at the finer level wherewe have enough room to where we can add the names of counties andother data about each of the counties. By focusing on a small portion of ourdata, we can ignore large amounts of data, and we can add more detail aboutthe data we're specifically looking at. And so, there's two ways of zooming. There's this way where we're lookingat a portion of our data set, that portion of the displayedcoordinates of our data set by, for example, clicking a window. Or we can also expand access fields. So for example, if you're planning data over time,you might be looking at it by year. If you want to focus on the year 2014, then you could click on 2014 andexpand that into quarters. And we could look at, say, the first threequarters of 2014 and ignore 2013 and 2015. So we can use our resolution to moreclearly see what's happening at each quarter. And then, we may be interested in quartertwo, and we could expand that and look at the individual months April,May, and June of quarter two. And ignore, for example, the monthsin quarter one and quarter three. So in each of these cases, zoomingallows us to take the displayed data and to expand it, to be able to focus onthe portion of the displayed data and ignore the rest of our data set that'snot in that subset of our displayed data. So here, we've used Tableau to plot ourlife expectancy data versus population for every country in the world. And some of the data clumps up. We have a big collectionhere of a bunch of countries that have similar populations andlife expectancy. And there's not enough resolution to beable to know which one we're pointing at if we're looking at that. So if I want to find out in these clumpyareas what's going on, I would zoom. And so, I would show the zoom controls. Go over here, and zoom in on this data. And that will spread things out. And I can see, for example,this clump was Norway, Singapore, and New Zealand that all have similarpopulations and life expectancies. And some of these otherclumps can be differentiated. For example, West Bank, and Gaza, and Lithuania have similar life expectancy andpopulations. Or over here,we've got Argentina and Poland. And so, by zooming in on the data,we can use the resolution available on our display screen to betterdifferentiate similar items. And then, we can always zoom back ifwe want to see where things are in the original context. When we zoom in on a portionof our plotted data, we're completely ignoring the portion of thedata that's outside of that zoom window. Often, that can cause usto get lost in the data. We're looking at a fine level, andwe forget where that fine level of data that we are focusing onsits within the larger dataset. So there are tools that help usto combine the focus of zooming with the context of the bigpicture of the entire dataset. And these are called lenses. They're interactive lensesthat you can drag around. Sometimes they're called magiclenses depending on what they show. And they allow you to zoom in on details,for example. Here's a data set, andwe can drag around the zoom lens and it uniformly magnifieseverything underneath it. But you can still see where youare relative to the the large dataset. But in this case,we have a uniform level of magnification. So the data that's underneath theperiphery of this lens is being obscured. We're magnifying this portion of the data. But for example, we've really lostthe detail of the region of data away from that magnified edge because it'sbeing obscured by the magnifying data. There are fisheye distortionlenses that help fix that problem. In a fish eye lens, you've got a lot ofmagnification at the center of the lens. And as you move away from the center,you've got less magnification. And as you get towards the peripheryof the lens, you get no magnification. The magnification goes to 1. And so, you basically,as you cross the lens, you don't see any change in magnification. That way, we can push away some ofthe details from the center, and we get this seamless transitionfrom the outer context into the zoom detailsthat we're looking at. In this case, these three spheres,these three light green spheres, are the three large spheres that we'rebasically occluding everything here. And as we push them away,because they're away from the center, we can start to magnify five very, very small spheres that were beingoccluded by these large spheres. And for example, these three spheresare basically these three spheres. They're uniformly distributed one,two, three here. And you can see that the first spherehere is farther away than the other two. That's because the magnification of thissphere is greater than the magnification of these spheres. So the distance is getting magnifiedmore here than it was here. And so, these fisheye distortionlenses give us a nice combination of the ability to zoom in on data, butto see it in a context of the big picture. So the D3 JavaScript libraryhas some good tools for doing distortion thatnicely demonstrate it. So here, for example, we have a graph. And you can see that the graphcontains a lot of clusters of nodes. And it's hard to see what's going on withall the edges inside those clusters. But if I put the mouse near it,then we get a magnification. You can start to see the differences. There's still a lot of lines there,but you can magnify the positions of the nodes, spread the nodes outwith this fisheye distortion lens and see what's going on withthe edges while still connecting to the context ofthe remainder of the graphs. So you still get your bigpicture of the whole graph, but you can zoom in on individual elements. And later on in the page, they showan example of how this distortion works. It's basically creating a small vectorfield that offsets the vertices or pixels, depending on if you're lookingat primitive data with vertices or if you're looking atimage data with pixels. And then You can do the lensing tothe entire screen in a rectlinear way. Or you can just do it toa small cylindrical or a small disc shaped portion. And here's some other data. In this case,you've got data that's plotted by area. And there's a lot of overlap anda lot of small details and large details. And as you drag over these regions,by distorting the entire field, then you can see the intricaterelationship between the data in your focus while stillmaintaining the context of where that data lies in the contextof the rest of the data set. So these fisheye lenses provide a nicecompromise of both focus and context."
cs-416-dv,4,4,"So filtering allows us toremove extraneous data. Zooming is a kind offilter where we remove extraneous data based on the plotted coordinatesof the data. When we select a zoom Window, we choose to focus onsome of the data based on its displayed coordinatesand to eliminate data outside of a Window based on itsdisplayed coordinates. That's a filter based ondisplayed coordinates. But a filter can be applied to other attributes of the data besides thedisplayed coordinates. So in this case,we've got our plot of all the countries based on population and life expectancy. We can filter out allthe countries that are in other continents and focus just on the countriesthat are in say Africa. So by filtering on Africa, we only plot thecountries in Africa, and the data is spread out nicely and there'smuch less data here, so we've got room to be able to plot the names ofthe individual countries. You can start tosee the subset of the larger dataset thatcorresponds to Africa, and you realize that manyof the countries that have low life expectancyare in Africa, but also there'sa separate band of countries in Africa that havehigher life expectancies. By plotting the names, you've now got room tosee more of the details. Because of this filter, you can see that those countries are the ones locatedin Northern Africa. There's different waysof selecting data by applying filter if you have ordinal or nominal datafor example, continent, then youcan click a check box, you can show all ofthe different categories, and select the ones you wantto look at and the ones that you don't wantto be plotted, or if you want to look atquantitative data for example, if we want to look atcountries based on gross domestic product orother quantitative fields, then we can set a range witha minimum and maximum value, and filter out dataabove or below these extremal values in order to focus on certain subsetsof a quantitative range. So in table it's quite easy to filter data based on attributes. You would just drag theattribute into this filter box. So in this case, if we have our countriesdisplayed here differentiated by putting thecountry in the marks box, we may want to filter thecountries based on continent. So I drag the region dimension in the filter box and thatwill filter based on regions. So now, since region, these continents are basically a category they're presentedto me in checkboxes. So if I want to only look at a subset of thecountries say Africa, I can just click ''Africa,''and then click ''Okay.'' Now, I'm only looking at the countriesassociated with Africa, the countries inthe continent of Africa. You can see there's a bigdifferentiation here between these countries at the top and the countries at the bottom. In fact, we can zoom in alittle bit further to give us more screen resolutionto take a look at them. Then if we want to knowwhat those countries are, then I can drag countryinto the label box here, and we can get labels foreach one of these countries. You can see that the countries in Northern Africa tend to have a higher life expectancy than the countries inthe rest of Africa."
cs-416-dv,4,5,"We don't want to overwhelma display of a lot of information with further details especially when thosedetails are extraneous and distract us from general trends that we should beseeing in the data. A very common way of doingthis is with a tool tip, if your mouse is sittingover a data item, then a little pop-upWindow will come up and give you the detailbased on that data, and maybe some items thatyou can select only or you can exclude that item and look at other operationsbased on that item. A second way offinding details on demand is a field selection, you can select a field of your dataset and you canfind the data points, that that field correspondsto or a certain setting of that field correspondsto that get highlighted. So there's all sorts of interactive details that you can make available to a user. You just want tomake them available only when they're requested and not all at once so as tonot overwhelm the viewer."
cs-416-dv,4,6,"So the key interactivedynamics for manipulating a datavisualization are, one, selecting items forfurther investigation. Two, navigating scalesfrom overview to detail. Three, organizing a dashboardof multiple charts, and four, coordinatingthe charts in a dashboard. Here we have a dashboard constructed basedon our WDI data. On the left, wehave our overview. It shows the averagetotal population of each country and the averagelife expectancy. The total population is logarithmic to betterspread the data out and this gives us a broad overview of all of thecountries in our database. We've addedsome visual encodings. For one, the region iscolored for each country. So the countries from a different regionare different color, and you can see that onthe map on the right as well. Also, we've encodedthe income group. Low income, low-middleincome, upper-middle income, and high-income have iconsdisplayed in the overview. That helps us get a bit more information withoutbeing too overwhelming, and you can see someof the trends in the data where most of the high-incomegroup marks appear, versus the low-incomegroup marks, and you can see a general trend, like higher income leadsto higher life expectancy. One of the things thatTableau allows you to do is, in the tool tip for each mark, is to insert another chart. So in this case, we createda chart for CO_2 emissions, life expectancy at birth,and total population, and we've inserted that so that when we went details on demand, we can just mouse overeach of these countries, and see you thoseresulting details. Those are general sketches, a fuller dashboard wouldallow you to click on each of these countries and get a full report onthe individual country. We can also select the income group usingthe highlighter. So if I highlight low-income, it shows me thelow-income entries here, and grays out the others. Similarly, high-incomewill highlight the higher income countries. If I select regions, for example, I could select Europeand Central Asia then I've set an actionin this database, such that selectionsof one chart are also under the selectionsin the other chart. Similarly, I can select points in the overview and see where the resulting countriesare in the world map. For example, if wewanted to look at the countries whoseaverage life expectancy was less than 50 years, we could highlight that and we can see whichcountries we're looking at. Similarly, the countries with the highest life expectancy, you can find that out in the map. Also, we can look at the total populationand the countries with the population over100 million are these. We can also highlightthe countries with populationless than 100,000, but they tend to be quitesmall other than Greenland. Also we have the year andthanks to hands rolling, we can do things like wecan select the year and use the slider and see what theseaverages look like in 1960, and then as weincrease from 1960 on, we can start tosee general things like the lifeexpectancy increasing. Well, also countriesare becoming more populous and the worldpopulation is growing, and this really dependson the interactivity of Tableau and the underlyingengines that can do all the databasequeries needed to go through the 200in some countries, and all of the indicatorsin order to grab this data, so that it can appearinteractively like this."
cs-416-dv,4,7,"Another importantinteractive dynamic for view manipulationis the ability to organize multiple charts into a dashboard such as the one shown to the righthere from Datazen. Datazen is a tool thatallows you to create multiple charts and drag them interactively into a dashboard, into a particularconfiguration of charts that can help yougain insight into data, interchange those charts as necessary to gain other insights. If we click on oneof these charts, we can find more detailand drill down. It provides other tools forlooking at the context of your data and to be able to pull up different segmentsof your data. Another important aspect ofdashboard design is layout. A few years ago, Iwas able to work with the graduate college at the University of Illinoisat Urbana-Champaign, under graduateeducation dashboard. This dashboard focused onpresenting a lot of data and relevant details aboutgraduate education on the Universityof Illinois campus. We can see it here. We're seeing this data visualized for all ofour campus graduate programs. Things like admissions,who gets in, how many students apply, are admitted and endup getting enrolled? This is separated intomaster's and doctoral students. The same thing fortime to degree. How long does it takeyou to get your degree on average at theUniversity of Illinois? Things like enrollmentdemographics. How many studentsare in the program? What's the breakdown interms of international versus domestic students,male versus female? How many under-representedminority students are there? Also, the support of students. How many studentsare supported with assistantships or fellowshipsin each of the programs? Then a visualizationof the tuition. How much tuition is paid and where doesthat tuition come from? We're able to lay outthis information across several different categoriesin different boxes. That layout helps visuallyorganize the data and group it so that you'vegot admissions data together, time to degree data together, and also a consistent coloring so that you can tell masters from doctoral programs as youwalk through the dashboard. You can also drilldown into details. For example, we can look at the Department ofComputer Science and its graduate programs and see the specific data forthe computer science program. We can also see where the students that get a University of Illinoisdegree end up. Here too, the layout helpsinform the State of Illinois, what the rewards are of a University ofIllinois education? As well to the United Statesand to the entire world."
cs-416-dv,4,8,"Finally, if you've organized multiple Windows and work-spaces, you can coordinatethose views to provide a linked multidimensionalexploration into the data, such as thiscross filtering example. In this example, we'relooking at multiple views, multiple charts ofa data-set on flights. We have the time of daythat the flight departs, the arrival delay, the distancetraveled, and the date. We're looking at a subset of those dates and then below it, we have the list of the flights that are containedin that data-set. This is an exampleof cross filtering. So we can look at an extended set of these dates and look at the effects of thetime of day that the flights aredeparting and so on. We can also filterbased on the time of day and by usinga technique called brushing, we can drag those filters across. You can see that flights that are leaving early in the morning, tend to havean on-time arrival but flights leaving later in the evening tend tosee longer delays. You can see if the distance traveled has any effect on this, and shorter flightsversus longer flights. You can also try to figure out which flights are the latest, which flights havethe longest delays? We can see that there'sa lot of delays here. There's probably some snowstorms or other weather, some tips place here aroundJanuary 12th or 13th, maybe it was a Fridayof the 13th."
cs-416-dv,4,9,"The final set of interactive dynamicsfocus on processes of learning through data visualization, and the communicationof that knowledge to others. As well as ensuring the fidelityof the data and its visualization. Finally, we can look at the process andprovenance of interactive dynamics for visual analysis,ways of sharing information and handling the information we gatherfrom the process of visualization. It's not science if you don't document it. And so,ways of being able to record the insights that you get from your visualization aswell as communicate them to other people can also be a very interactiveprocess in visual analysis. And we can start just by looking atrecording the analysis that you get. And for example, in Tableau, thereare mechanisms to record the different visualizations, the different charts thatyou form as you're investigating data. So that if you want to goback to a previous chart, you have that ability toin this worksheet history. Similarly, annotating is an importantway of taking notes both for your own reference for later perusal, or if you're going to share somediscovery with somebody else. Being able to circle certain items orbe able to record a box to be able to put these little annotations isquite valuable in communicating and messaging a visualizationfrom one person to another. Or from yourself toyourself at a later time. And finally, being able to share views so that other people can seethe visualizations that you create. And this was introduced by a system calledmany eyes that's no longer in existence, but it was the ability to createvisualizations that you could share with others and look at messagingacross those visualizations. What ideas are you trying tocommunicate to other people? And other people could lookat your visualizations and explore with your visualizations, and also use them to find their own insightsthat they could share with others. The modern form of this now,you can see with Tableau public. And finally, the ability to be able toguide people through the visual analysis so that you can communicate yourunderstanding with somebody else. In addition to just having annotations andthe ability to share a chart on the Internet with somebody else,there's a technique that you'll need to develop in order to be ableto effectively guide somebody. That's the idea of a story. Data journalism, for example,Tableau stories, is one example. Ways of telling stories with a sequenceof charts and also the accompanying annotations combined with the abilityto explore further on your own. So the key interactive dynamics forthe process and provenance of data visualization are, one, recording the steps towardsproducing a data visualization. Two, annotating a visualizationto document and highlight interesting aspects of the data. Three, sharing these with others. And four, ensuring others canreach similar conclusions."
cs-416-dv,7,1,"So in this week, we'll learn how to dodata visualization, using web pages, and we'll need to learn two main things. The first thing is howto program a web page, how to develop a web page that canshow data visualization in a chart. And the second thing we'll need tolearn is the tools we have to do that, namely d3, which is a JavaScriptlibrary that we use for data visualization, but is designedto integrate data into a web page. [MUSIC]"
cs-416-dv,7,2,"Data visualization isa powerful tool to communicate informationto an interested user. It uses the computer to gather that information ina visual display in order to convey that visually across the high bandwidth channelthat the user can use to receive a large amount of information and to helpunderstand that information. There are several modesof visualization that one can use to convey this informationto an interested user. One of these modes isinteractive visualization. This kind of visualizationis used for discovery, and it's intended for a singleinvestigator or group of investigators thatare collaborating to be able to sit infront of the computer, and to ask questions, and then to be able to usesome tools such as Tableau or another datavisualization tool to access the data,organize the data, and then be able topresent the data visually throughsome form of a chart in some visualrepresentation so that the investigators can getthe answer to their question. The focus here isn't onthe quality of the visuals, the focus is on being ableto use the visuals to accurately represent theinformation to the investigators. So the result isn't going tobe too glitzy or glamorous, but it will be informative. Another mode of visualization is presentation visualization. This is a visualizationintended for a large group or a mass audience. Typically, you would seethis kind of visualization used for documentariesor museum shows, but it might also be preparedfor site visits from the funding agencies involvedin scientific explorations, or the general publicas a method for outreach in order to communicate the results ofscience or other data in away that's appealing and understandable tothe general public. This form of visualization isgenerally not interactive. It doesn't support user input, and so it's designed to be a unidirectional communication of the information from the source, from the investigator,or the producer of the information to the consumerof the information. Since it's generally produced so that it can be seenby a mass audience, it will be highly polished. So in between these two extremesof visualization, where you have prototyped quality interactivevisualization, designed to informa single investigator or small group ofinteractive users, at the other end of the spectrum, you've got presentationvisualization that's highly polished butnon-interactive. There's a middle ground, and that middle ground isinteractive storytelling. This uses the powerof the Internet, and especially theInternet Browser, to be able to present data to interested users usinginteractive web pages. It's a presentationvisualization and its narrative, so it's using thedata to tell a story in the same way thatpresentation visualization does. But it's also interactive, and it can invite the viewer to explore the datafurther to answer questions that theymight have in addition to what they've just learnedthrough the narration. Because of this narrativestorytelling mode of using data to tell a story, it's also been a very popularmedium for data journalism."
cs-416-dv,7,3,"We can learn a lot aboutinteractive storytelling with data visualization by looking at examples of data journalism. Recently data journalism has included interactive web pages that tell a story by presenting datavisualized in charts, as a narrative visualization. They also let the readerinteract with the charts to explorethe data further. Here are some examples. This first one ison the hype cycle. It's a hype cycle foreducation developed for the University ofMinnesota that's no longer available.What is a hype cycle? A hype cycle is a model developed by the consultant firmGartner Incorporated. And it showshow new technologies, new innovations areintroduced to society and the stages that they gothrough before they're finally adopted into general use. So this looks atthe hype cycle for education but it's the samehype cycle for anything. New technologies, new ideas, they start out with some trigger that builds excitement and then that excitement leads to this peak ofinflated expectations. Then we come crashing down into the trough ofdisillusionment. When the technology orthe innovation doesn't live up to those inflated expectations. Then we climb out through the slope of enlightenmentand finally reach the plateau of productivity and that's when the technology or the innovationbecomes of general use. You can look at a few of these. These are focused on education. So they're educationaltechnologies. Things like quantumcomputing are still interesting but haven'treally taken off yet. Other items such as open micro credentialsand flipped classrooms are just at their peak of heightened expectations and some other thingslike gamification. In this slope of enlightenmentwe reach the plateau of productivity and thingslike self-publishing, cloud email for staff and faculty, E-book readers, mashups. Things that werequite exciting years ago disappointing are nowbecoming commonplace. This story is told through what's called adrill-down visualization. So at each one of these cases, you can click onthe item and you can see a description of more detailabout that technology. This narrative visualization also allows us to explorethe data further hanging and ignoringsome of the ones in the middle or some otherones that are not assessed. This hype cycle example is a good example ofnarrative visualization. It's implemented asa reactive webpage. So it's available to everybody on the internet througha standard URL. It displaysan ordinary data chart but it does so inan engaging way, and it allows user interaction. That's a good platform fora narrative visualization. It uses the data to tell a story. In this case that story ishow technologies go from their initial trigger stage to a peak of inflated expectations, through a trough ofdisillusionment, a slope of enlightenment and finally a plateauof productivity. So it plots the data pointsalong that hype curve to tell that story and letsthe data do the talking. But it also allows the userto investigate further. They can drill down and mouseover each of these items, clicking on them tofind more detail about the technology or they can filter items out to see how different technologies at different stagesof their lifetime, where they belongin the hype curve. Another example is the story of how bad the drugoverdose epidemic is and it uses data visualization ina very interactive way. It asks since 1990 the number of Americans who have diedevery year from car accidents. You can try to predictwhat that looks like and then it will compare your prediction to whatthe actual data is. It has some narration here, some messaging aboutcar accidents. The number of Americans whohave died every year from guns and then it shows youwhat the actual result is. In this case, thereis a decline between the 90s and the 2000s butthings have slowly risen. Number of Americans who havedied every year from HIV. You might expect that to climb, when in fact it peaked in the nineties andthen dip significantly. Then finally drug overdoses. This is probablysomething like that and in fact it's growingquite drastically. So in each one of these cases, we balanced the messaging, the article that's being written by the journalist with theopportunity for interactivity. Those two are combined to use data to present the storythat's being told. Then a final example is from the Wall Street Journaland it tracks the Nasdaq, but it does so usinginteractive virtual reality. So we can do this on the screen, you can do this onyour phone using a VR viewer if you want and it givesyou some explanations here. We'll discuss in class later what purpose theseexplanations serve. So we begin here. We'll click ""Start Ride"" and this one happens to be in 3D. In general we'll be lookingat two-dimensional examples. This will be one of the few three-dimensionalexamples we use. There's some narrationhere, the annotation. So this follows the form of an annotated chart but it does so in 3D topretty good effect. So it starts outback in 1994 before the boom and we're writingthe roller coaster and we get some some annotationsof the Nasdaq chart here indicating where we are as we ride the roller coasterof the stocks. We're climbing up to the top, closing up above 5000 andthen the bubble bursts. March 10th, 2000, thisbubble burst and so we fall. You can see why we'refalling here then in early 2000s we're down in the valley and there's some annotation andnarration here, and then we startto climb out of it. You can seesome other annotations the years that are labeled here. So are always labeling our axesand a good visualization. But adding these other messages,telling the story. There's another recession here. That's I thinkthe real estate bubble bursting fall down here. Then we do this slow climbout of the whole thing. Closing up above 3000 in 2013. Twitter changes its listingand then we climb up above 5000 and then we cando the whole thing over again and we can lookat where we've been so far. These are all examples ofnarrative visualizations. Interactive webpages that convey a message throughcharts of data but also allow the reader to engage with the data throughinteraction with the charts."
cs-416-dv,7,4,"We'll be using HTML to design interactive webpages for narrative visualizations that display data in charts and allow users to interact with them. We'll use this Tryer Outer to show how to write HTML and to show how it appears in a web browser. So we'll be using HTML to design web pages to support narrative visualization and interactive storytelling. And so we'll use this Tryer Outer to be able to write web pages, and web pages are in HTML. And so we start out with an open and closing HTML tag, and so everything in the HTML will appear here and we can start for example, with a body of the HTML and then we can enter a paragraph and I can say this is a paragraph, and then I run it and I get a paragraph. You can also have headings, and then they can run it and see the output here on the right. You can also add things like a button and other interactive elements to a web page. So in general, we'll write our HTML descriptions here including JavaScript code and then hit run, and then the result is here on the right. We'll need to focus on narrative visualization and can't spend much of the course on each HTML basics. You can learn more about HTML tags and Cascading Style Sheets at the w3schools website."
cs-416-dv,7,5,"We'll be using HTML to design interactive web pages for narrative visualizations that display data end charts and allow users to interact with them. We'll be using this Tryer Outer to show how to make interactive web pages using HTML and JavaScript and show how it appears on a web browser. So we can create an HTML file and they can put in a paragraph that says pi equals and then include some JavaScript. So anything in between these scripts is JavaScript, but we need to insert text there so we'll use document.write, and then that will write the value of pi into our HTML document. Sometimes helps to debug these things and so if you hit control shift i in Google Chrome you can get a console log and also look at elements. And so you can click on these elements to find more information about them, you can inspect them. If you go to inspect, it will inspect the element and tell you all sorts of information about that element and that helps to debug webpages. You also have a console here, and so, I can get rid of this and simply have a little bit of script, JavaScript that says console.log.pi and that will just write the value of pi into the console log. And this console log is helpful especially when you're debugging web pages because you can output to the console without having it affect the web page itself. And it doesn't matter if you have a paragraph here or here, you'll still get the output to the console log here. And so those are some tools that can help you code in JavaScript and be able to see the output interactively and then be able to debug your programs using console and elements in order to investigate the elements using inspect. We'll need to focus on narrative visualization and can't spend much of the course on HTML to JavaScript basics. You can learn more about HTML tags, cascading style sheets, and other web based technologies in any number of tutorials available where else, but on the web."
cs-416-dv,7,6,"We'll be implementing interactive narrative visualizations in dynamic web pages using javascript. The web page can allow javascript programs to access it, to update elements at displays through its document object model. The document object model abbreviated DOM, is a hierarchical co-representation of the tags in a web page. So web pages and HTML, so the contents are in between HTML tags and opening HTML tag and then its closing tag here, and that forms a node in this hierarchy. And inside the HTML, we separate the document into a head and a body and those correspond to head and body nodes in this hierarchy. The head has a title and that title has some text. The body has two paragraphs, and each those paragraphs has some text as well. And this paragraph also has an ID and this is an attribute that also belongs to that paragraph. So the javascript code will manage the DOM through objects, through the document object and so we can create expressions that modify the DOM based on that object model. Hence the term DOM, document object model. Here's the document, it's an object and we use that to modify the model of the current web page. So the document is at the top of the hierarchy and the HTML node corresponding to the opening and closing HTML element is a child of that document. Instead of working our way down that hierarchy to a desired element, instead we often use this getElementById and it looks like the id attribute of the paragraph element and can find that instantly without needing to traverse the hierarchy. And then once we're there, we can find this inner HTML text and manipulate it if necessary. And so here's an example, we have our document and here's some Javascript, and this javascript can can go in the head, can go in the body, they can even go outside the head or even anywhere you want. But this javascript is located between opening and closing script tags and so inside is some javascript and it's referencing the document and it's okay to have a new line, we can have all sorts of whitespace and we're looking at the getElementById method of this document object. And we're looking for the id ""x"" and that's here in this P element, paragraph element, and then that will return an object that references this P element and then we look at the method inner HTML of that object returned by a getElementById and that gives us a reference to this internal text. So here is the document, and now we can change the inner HTML that we've referenced the getElementById and we can change its contents to something else. And so what happens is that this inner text will dynamically change to something else due to this script. So you can write javascript that's going to manipulate the HTML that's being displayed on your web page. You can use the document object model to walk through each node in the hierarchy of an HTML Web page, but we'll be using other methods to access these nodes."
cs-416-dv,7,7,"We'll use scalable vector graphics todraw the charts used to visualize data on interactive web pages. Vector graphics describepictures as filled outlines instead of as an array of pixels. This way the images can be rescaled to fitany display without worrying about image resolution. And scalable vector graphics can beembedded in an HTML file for a web page. They're embedded using the <svg> tag. When we specify an SVG drawing canvas, we'll need to specify its width and itsheight, and those are usually in pixels. But you can also make them inpercentages of the screen size. The origin for an SVG drawingcanvas is in the upper left corner. Often in graphics,we'll use a lower left origin but for SVG, that origin is in the upper left. And then we position items using tagattributes, such as x,y positions. So if I want this blue rectangle here,I say rect, I'd use the rect tag. There's a closing rect tag butthere's usually no contents of it, and then we position it at x = 50, y = 50. And so we move here 50 pixels anddown 50 pixels. And so that's x = 50 and y = 50 pixels,and that gets us to this point here. We set the width of 100, so that'll be a width of 100 pixels,and a height of 200 pixels. And that gives us this rectangle here. We can get this second rectanglein a different manner. Notice that this is a rectangle also. And we've set the width to 100, andwe've set the height to 200, but we haven't set an x and a y. And so we need to set the x and the y,and so if you don't specify what x and y are,then they're both going to be equal to 0. But that means that thatrectangle should be here. And so what we've done is we'veperformed a translate command, we've embedded this rectangleinside a group node. That's this g, closing g tag. And then we can implementa transformation here, in this case, a translate by 200, 50. And that creates a transformation,a translation by 200, 50. That means any x andy values that are used to generate this rectangle get 200 addedto their x coordinate and 50 added to their y coordinate. And that means when we goto draw the rectangle, that's 100 pixels wide and200 pixels tall. It's actually all of those drawingcommands are getting translated by 200 horizontally and 50 vertically,to put the rectangle in this place. And so you can use these groupnodes to position elements. And you can describe an element once andposition it several times in a scene just by drawing it relativeto a given origin and then placing it in various group nodes. So let's try out some SVG. So I'll start by creating an HTML file andthen the body we'll add an <svg> tag. Now, this will create an SVG canvas,so we need to give it a size. So let's make its width 200 pixels,and its height 200 pixels. If I run that,we still don't see anything, but I can add some CSS styleto show the border. There, and this is our SVG canvas. And we can create things inside there,I can create a circle of radius 50 pixels. And you'll notice that the originof the circle is here on the left. Also, we've got this DOM here,and it's showing everything inside the <svg>tags in this rendered frame. And soright now all we have is a circle here. I can add another circle,of increasing radii. And we would see those circles andtheir associated tags here. And they're just pilingup on top of each other. So if we want this circle inthe middle of our canvas, we can enclose it in a group node. So there will be a group node,opening and closing. Now this group node canhave transformations. And so let's set that transformation to bea translate into the center of the canvas. And so it's going to takeevery drawing command and it's going to add 100 to its xcoordinate and 100 to its y coordinate. And that'll put the circlein the center of the canvas. And you'll notice that ourDOM has the group node here, and below it inthe hierarchy is the circle. And so everything that happens inside,below this group node, has this transformation applied to it. Most of our lessons will usevery basic SVG elements, mostly rectangle elements and group nodes. However, SVG can draw more complexshapes using the path element. And you might want to look up the detailsof the path element to see what it can do. [MUSIC] [SOUND]"
cs-416-dv,7,8,"This is d3, which stands fordata driven documents. It's a system for incorporatingdata into HTML documents, and it facilitates datavisualization in web pages. As you can see its designedto be interactive but requires a fundamental understanding ofthe principles of data visualization, as well as skills at web programing. So d3 is a way to visualizedata from a web page, and it's a web page running on the clienton an ordinary browser and it facilitates all sorts ofinteresting ways to visualize data. You can see a lot of these on thiswebsite on the right, bl.ocks.org. And this is an example by one ofthe authors of d3, Michael Bostock, and it shows a bubble diagram of some data. But if you look at that website,once you finish this course, you'll know enough about d3 that youcan make your own visualizations. And you can demonstrate them to otherpeople by putting them in your own directory on this bl.ocks.org website. So d3 is a JavaScript library, and it'sdesigned to work alongside the document object model that your HTML page,the elements of your HTML page, and the CSS, andthen the SVG elements you might have. And it works with the document objectmodel to be able to display visualization. So here's an example on the rightof what you would have to do if you didn't use d3 for a visualization. It's a very simple bar chart and as youcan see you can use HTML in the document object model to represent a visualization,but it's difficult and limited. And so d3 makes that process much moresophisticated, much more powerful, by associating a data elementwith each HTML element. And in doing so, it adds datato the document object model and that's why it's d3,data, driven, documents. Using the d3 library andadding data to HTML elements, it actually empowers the browser to makesome very impressive visualizations. And you can see some examples hereof network visualizations, and various map visualizations, andbar charts, and so many, many more. Especially on the d3 websites. But the fundamental contribution is notjust the functions that they create the charts. It's the integration of data intothe HTML documents that's the fundamental contribution. And the way that data is integrated isto not treat the document object model as a document,as a hierarchy with a head and a body, and the body having several elements, andeach element having sub elements. D3 doesn't really treat the document asthat outlined hierarchy organizations. D3 treats that document as a database and it integrates the data that you want todisplay with the data in the document using sophisticateddatabase methods joins. And so on in a way that is quiteeffective to enable HTML web pages and the web browser to display data. So d3 was designed to support datavisualization and interactive web pages. But it's fundamental contributionis to add data to web pages and to treat the web pageitself as a database. [MUSIC]"
cs-416-dv,7,9,"D-3 stands for data driven documents as can be seen by this info this 2011 paper that introduced it. It incorporates the data you wanna visualize into the web page used to visualize it. It also provides methods for accessing the web page not as a hierarchy but as a database. As we saw in the last video D-3 treats the HTML document, the document object model not hierarchically but as a database. And and it merges that with a second database that is data that you wanna visualize. And the way it does this is with the new operator it's an attribute for elements of the HTML document and that attribute is the data attribute. It's.data and then you feed in the data array. And so an example of the data, the external data that you might want to bind to the elements of HTML document is shown on the right the numbers 4, 8, 15, 16, 23 and 42. And so you would bind those using this.data attribute on elements in the HTML Document object model. And so your HTML file might have it svg canvas that has six blue squares on it and you can grab all six of those blue squares in D-3 using a select all method and give it a query string and you say select all the blue squares in my svg document and it will return a structure that gives you all of those blue squares and then you allow D-3 to bind each of those data items what each datum. For example four as a datum and you're going to bind each one of those to each mark, each HTML element in your- in your document object model. In doing that binding you're going to set the height of each of those squares to a setting and that setting is going to be controlled by the datum. And so you know the first rectangle will be four units high and the second will be eight and 15 and 16 and so on. And then you've turned your six blue squares into a bar chart of that data. So D-3 can do this for you but in order to understand how D-3 works you have to speak D-3. D-3 has specific ways of articulating the programs, some methods that it uses and expectations and so it's good to understand those. So in order to use D-3 to associate data items with HTML elements. We have to understand how to speak D-3. We have to understand the conventions of the D-3 libraries so we can program in and effectively. One of these conventions is methods and what those methods return. In D-3, there's a select method. I can select rect and that will basically do a database query of the HTML document for a rect element and it will return one of the rectangles in the svg region of the HTML document and it'll give me a reference to that. Then I can call the attribute method of that reference and set the height of that rectangle to 42. The interesting thing is that attribute method has a return value but that return value is not a reference to the height attribute. It's a reference to the original rectangle. And that's because we can use chaining as a convenience. And so if I wanna select that rectangle I can set its height to 42 and that attribute doesn't return a reference to that height attribute. That attribute method returns a reference to the original rectangle so that I can then set the attribute, basically it's width to 20. And that method is being applied to the original triangle and that method is not being applied to just the height attribute before it. Also methods on selections of multiple objects will be applied to each object in that reference. So I can select all of the rectangles by saying select all rect and that will give me all of the rectangles in an svg section of my document and then if I called the attribute method I can set the width of all of those rectangles to be 20. That starts to get at the power of D-3 because I can also set the height of all of the rectangles. But instead of sending them all to the same value I can set them to different values and that's because that attribute function, the value that you're setting for the height doesn't need to be a constant. I can set it to an object. In this case that object is a function. Functions are objects in javascript and so I can use this anonymous function basically. A function of d,i. And if D-3 sees that I'm passing a function as that second parameter to the attribute method then it will call that function and will always call that function with at least two parameters. The first parameter being the data value and the second parameter being the order of the element that is currently processing which data item we're currently using. And so in this case we want to set the height to be the data value and so we can just return D for the height. So here's an example of D-3 in action. If you put a D-3 program here. First we do is we load the D-3 library and then we create an svg region with a bunch of rectangles in it. And then we run our D-3 program and we define some data here. The D-3 select all is going to select all of our rectangles using the select all method and it's going to associate our data array with those rectangles because they're using the data method on that selection of all the rectangles and then we can set the attributes sled all the- the width to 19 and then we can set the heights to be a function of the data value. And then we can set the x coordinate to be a function of the index and the y coordinate to be a function of the data value. And if we run that, we get our magic numbers 4, 8, 15, 16, 23 and 42 plotted as the height of these rectangles that we were essentially empty rectangles and the svg specification here. But we assigned heights by this line here, because we've essentially associated the data values here with the rectangles here. So D-3 creates charts to visualize data in a web page by associating each data item with an svg element used to display it."
cs-416-dv,7,10,"The key aspect of D3 is that it treats thedata you want to visualize as a database. And the webpage of elements youwant to use to display the data, also as a database. Such that the association of the dataitems with the SVG elements used to display them becomesa database join operation. So if we start with our six rectanglesthat we're going to use to visualize the data, we can use d3 to accessthem using the selectAll method and that will select all the rectangles inthe svg section of the HTML document. And now I'll just abbreviate thoseas a blue rectangle there, and now I can use the data method to associatedata with those blue rectangles. In this case the data is the vector[4,8,15,16,23,42] and that basically creates an association of the firstdata item with the first rectangle, the second data item withthe second rectangle, and so on. And once we have that associationI can call the attribute method on that association. And it will set the heightof each HTML element with the corresponding data valuethat's been associated with it. So in this example we have an HTML file. It has an svg section withsix rectangles in it, and we're creating a data arraywith six values in it. And the first thing we do,is we take this paragraph, and in this region we're going toinsert those data values. And if I run that you seethose data values here. Now, we can insert some d3 code,and we're doing a d3 selection. We're basically doing a selectof all the rectangle elements. And then we're using the data method and that's going to take our selection ofall six of these rectangle elements and it's going to associate thesesix data values with them. And so the first rectangle element willget the first data element and so on. And that creates the selection with pairing up the HTML elementswith the associated data items. And then as we call each method,for example, setting the width to 19 of a rectangle,that's going to be applied to each one of these rectangles,same with all these other attributes. The height attribute,instead of calling it with a number, we're going to call it with a function. And when you pass a functionas that second parameter, then that function gets called withthe data value as the first parameter. And so the height will be set to the datavalue, or a function of the data value. In this case, the number of pixelsis equal to 10 times the data value. The x coordinate of the rectangle isgoing to be set with this function, and when it gets called, it gets called withthe data values the first parameter. But the functions all get called witha second parameter and a third parameter. The second parameter is the orderof the index of the data item. So the second rectangle would getthe second data item and so i, there, would be 1. So the first item will be 0,second item's 1, third item's 2. And so that tells us where horizontally toput that rectangle, 20 times the ordinal. And then the y value is just going to beset with a little bit of arithmetic to be the base of the rectanglein the svg coordinate system. So now if I run this, I get a bar chart. The first rectangle isgoing to be 40 pixels tall, 80 pixels tall and so on,following this function. This is basically usingthe semantics of a database join. We have two databases,the database of data items and the database of html elements. And when there's a one to onecorrespondence between those two, we call that section the update section. That's the section that's returned whenyou have the selection, the select all of all the rectangles and use the datamethod to associate data with that. The result is that association ofthe data with the selection and that association basically formswhat's called a selection. And once we have that selection, in thiscase the update area of that selection, then we can set the attribute of theheight of each element to the data value. In some cases, we might have moredata than we have HTML elements. So, in this case, we have sevendata items and six HTML elements. There's an additional item that doesn'thave an associated HTML element. And so for that, that goes intoa different area of the selection. That's called the enterarea of the selection. And so we can access that usingthe enter method of that selection. And we need to figure out whatto do with those elements, those data items that don't havean associated HTML element. And so, in this case,we would append a rectangle and set its attribute, the attribute of itsheight to the corresponding data value. And finally there might bethe case where we have more HTML elements than we have data. If we have six rectangles but only fivedata items, we need to figure out what to do with that sixth rectangle thatdoesn't have an associated data item. That goes in the exitsection of the selection, and we can access that with the exitmethod of that selection. And in this case, we can simply removethat item from the HTML document. And so we can use the notation from the algebra of relational databasesto look at these database joins. Here's an example withan arbitrary database. You might have a store selling fruit, andyou might have several bins that you can put the fruit in andthose bins have prices. So we'll say the bin number is the key. And you might be selling apples, pears andbananas and that'll be in database A. And so database A tells youthat the apples are in bin 1, the pears are in bin 2, andthe bananas are in bin 3. You might have another database that hasthe prices associated with each bin. So bin 2 might have a price of $4,bin 3 has a price of $3 and so on. And then you can join these twodatabases based on the key field. And the natural join of A and B gives you all the elements of a whosekey matches the key in Database B. And so in this case,pears are in bin 2 in database A, and bin 2 has a price of $4 in database B. So the natural join would showyou that the pears are $4, and it would also show youthat the bananas are $3. There's also the antijoins, so A minus B. These are the elementsin A that aren't in B. In A we have apples in bin 1 andin database B, we don't have an entry for bin 1. We don't have a corresponding entryin database B with key of 1 and so apple will be associatedwith the price of null. We don't have a price for apples andlikewise for an antijoin B minus A. These would be elements in database B who key does not havean associated value in database A. And soin B minus A we have the $2 bins and the $1 bins don't have anyfruit associated with them. There's no fruit with key 4 or key 5that's associated with price of $2 or $1. So, we can use that to expressthe three areas of the join of the data items with the HTMLelements that D3 produces. So, in D3 you've gotthe data D on the right and that has an index of each data item, andthen the datum, the actual data value. And, on the left, you've got the nodes,you've got the HTML elements. And so you've got the index N, basically,the ordinal value of that HTML element. And then you've got the node,the actual HTML element. And so you want to join those formatching index, node index, in, that matches data index, id, basicallythe order that these things come in. And sothe update region is the natural join, that's when you've made an associationbetween a node and a datum. And then the enter region Is D minus N. It's the region where you've got dataitems that don't have a corresponding HTML elements. And the exit region is the antijoin Nminus D where you have a node that doesn't have basically an HTML element thatdoesn't have a corresponding data item. The select and select all methods act likedatabase queries to give you ready access to the elements in a webpage, includingany data items associated with them. [MUSIC]"
cs-416-dv,7,11,"So the real power in D3 isin the selection methods. That join the database of your data items with a database of HTML elementsyou want to use to display them. The first of these selectionmethods is select. And it'll find the first elementin the HTML document that matches the selector string. You can also select all of the elementsthat match a selector string and you can combine these. So you can find all of the elementsthat match a selector string s1, and of those elements you can find the firstone that matches a selector string s2. You can use a function to filtera selection to refine it. And you can create the unionof two selections. So you can select allelements matching s1 and you can then select allelements matching s2. And merge that with the resultof the elements matching s1. And that gives you the unionof those two selections. And once you have a selection, there's avariety of functions that D3 provides that allow you to manipulatethe elements of those selections. For example, we've seen how youcan change the attribute of all the elements in a selection. You can also assignclasses to a selection. You can change the style ofevery element in a selection. And you can change the text or the inner HTML of all the elementsof a given selection. And there's some helperfunctions with selections. You can iterate through eachelement in a selection and of each element, you can applythe function f using the each method. And you can, given a selection,you can register a call back by calling the function f on that selectionalong with parameters p1 and p2. There is a predicate that tellsyou if the selection is empty, the selection size method tells youthe number of elements in a selection. And selection nodes returns the arrayof elements in a selection. So these selection methods are morepowerful and easier to use than the HTML dom methods to access the contents ofa webpage and to update them dynamically. [MUSIC]"
cs-416-dv,7,12,"Design Patterns are an approach to programming that collects a sequence of instructions into a motif that can be reused over and over. In D-3, the methods used to join data with HTML elements form such patterns and understanding these patterns makes it easier to program D-3. So the selections in D-3 create a join between the database of data items and the database of HTML elements used to display them. And so we can use this variable join and we've defined it to be the select all of the rectangles, the rectangle HTML elements joined with the data 4,8,15,16,23 and 42. So that creates three possible sections. There's the update section and that's the section where we have an HTML element matched up with the data item. And in that case we can just set the height of the HTML element to be the value of the data. In the intersection, that's the data items that don't have a corresponding HTML element. And so those are represented by join.enter for each of those data items we have to append a rectangle to the HTML elements and then we can- we have a paired up data item with the corresponding HTML element and we can set the height of the HTML element to the data item. And finally we have the exit region and these are HTML elements that don't have a corresponding data item. And so we can just remove those HDML elements. But we can see some patterns develop for one. What we're doing in the updates section is similar to what we're doing in the intersection it's just in the intersection we're also appending in HTML element. So that sets up the merge pattern. In this case, we don't do anything in the update section and in the intersection we append an HTML element for each data item that doesn't have an HTML element. And then we merge the update elements by saying merge join. Join without anything after just returns the elements and the data items in the Update section. And so now we have the union of the update and the intersection and we can just set the height of all the HTML elements there to the corresponding data items. So we can see what happens here, I've got six rectangles so I can select all six rectangles and join that with the data 4, 8, 15, 16, 23, 42 and 84. So there's seven data items and only six rectangles. So the update section contains the first six rectangles paired up the first six data items but the seventh data item 84, doesn't have a corresponding rectangle so that ends up in the intersection. We have this.enter at the bottom that represents the elements in that intersection. And so we append a new rectangle that gets paired up with it. And so now the intersection has an HTML item paired up with its data items and we can merge that with the update section. And now we can change the height of all seven pairs of HTML elements. We set the height of HTML elements according to their corresponding data values because all seven of them have been paired up. But you might notice that it's kind of a pain to have to manage existing HTML elements and creating new HTML elements when- when there's not enough of them. It's actually much easier to do what's called the selectall_data_enter_append pattern. And in this case we start out with no HTML elements in the selection. And so typically you'll have svg tag and then closing the svg tag and nothing inside the svg and just create everything in javascript using D-3. And so now you'll say D-3 select all my rectangles and I don't have any rectangles. So the selection is empty and I'll now associate a bunch of data 4, 8, 15, 16, etc.. And so all of the result of that join will be nothing in the Update section and everything will be in the intersection. And so when I look at the enter section of that I've got all of the data that needs to be paired up with HTML elements and so I need to append an HTML element for each data element in that intersection. And now I've got everything paired up and I can just set the height of all of those HTML elements to their corresponding data items. And this is what that looks like. There's nothing on the left in the svg. There's no rectangles. The select all returns no rectangles and everything will end up in the intersection. The six data items end up in the intersection without any corresponding rectangles. And so we have to append those corresponding rectangles to that section. Append is going to append a rectangle for each data item in that section and then we can set the attribute of those rectangles to the corresponding data values. In this example we have an HMTL file, but the svg block is empty. There are no rectangles and so we'll do a D-3 select of the chart area and when we go to select all the rectangles there are no rectangles. So that selection will be empty. We're gonna associate the data. We have these six elements of data that are going to be associated with no rectangles and so that means nothing will go in the update area. Everything goes in the entire area. So in order to address that we're going to add the line Enter. Use the Enter method to get to the enter area and for each element in the entry area that has a data item but no associated HTML element for each one of those we're gonna append to a rectangle. So we're going to create a rectangle element here. That's going to be used to display the corresponding data item. And now, after we've done all- we've appended all of those items we can set the attributes as we did before and as we run this we get this same bar chart but now we're creating the rectangle HTML svg elements that are used to visualize each of the six data values. So the most common sentence in the D-3 language starts with selectall_data_enter_ append and the rest of that sentence describes how you're going to encode the data values in the attributes and style of the HTML elements."
cs-416-dv,8,1,"When drawing a chart you have to manage two sets of measurement units. The first is the system of units of the data values and the second is the system of units used to draw the charts. In d3, there's a set of built-in objects called scales to convert between these two units. So scales are functions that map from the data domain to the visual range. The data domain is the values that the data take on and they're abstract. They can go from zero to one or, you know, take on arbitrary values. The visual range are the values in pixels that are used for the display of the marks that represent the data. So, for example, in a bar chart the heights of the bars correspond to the pixel values used to display the data. So as the data varies across its values, say, from zero to one, the number of display pixels that those values correspond to can range from 0 to 500. Similarly, if we have five bars the data indices will range from zero to four but the number of display pixels needed to represent each bar could be a range across from 0 to 100 or more. And so, in d3 we have these objects called scales and they're generated by functions, d3 methods called scaleLinear and scaleBand. On the left we have the y-values of a bar chart and they'll correspond to those black dots. Remember that in SVG canvas the origin is in the upper left. And so we need a mapping from a domain, in this case, the data domain is, the data values are gonna range from zero to one, and we're gonna map those to SVG y-coordinates ranging from 500 back down to 0. And so, if I then call the object y as a function with the value zero, it will return 500. If I call y as a function with parameter one, it'll return zero and if I call y with a data value of 0.5, it'll return 250. And so 500, 250, and 0 are pixel values that you get by passing in data domain values from 0.5 and 1. Similarly, the x-coordinates can be generated by a scale object resulting from scaleBands. And so we call scaleBand, that's a mapping from a domain. In this case, the domain are nominal values, a, b, c, d, and e. We're mapping them to a range from 0 to 500 pixels. The result of that is, and this kind of an associate of map, if I call x with the value of a, I get zero. If I call x with the value of c, I get 200 and if I call x with the value of e, I get 400 and those are basically the left edges of the corresponding bar a, bar c, and bar e. You can also use scales to control other axes besides the spatial axes, x and y. You can use scales to control radius of circles or the various shapes or any other mark attribute you want to use and one of those mark attributes might be color. And so, you can create a scaleLinear that maps data values ranging from zero to one in the data domain and have the output range interpolate from blue to orange. And another one might be if you have negative values, you could map the data domain -1 to 1 to go to blue to orange but pass through white for 0, depending on if you want a centered color map or not. And D-3 actually has a bunch of built-in scales that you can use for color, in case you have a continuous variable or a category variable that you want to use, and there's many more of these that I didn't include, but those are built in scales and they're just a really handy function in d3 to be able to convert items from the data domain to the visual range. So scales in d3 convert values from the data domain, the units used to represent the data values, to the visual range, the units used to display the data, which for web pages are usually measured in pixels, but can also be colors or the size of marks in a chart or any other visual attributes."
cs-416-dv,8,2,"As we discussed earlier,SVG includes a path element that encodes the outline of an objectwith a string of path instructions. D3 has methods for automaticallygenerating axes, pie charts, and many other charts and decorations,by creating a string of instructions for these SVG path elements. So in the previous lesson, we learnedabout scales, and scales are maps that D3 generate that allow us to convert fromthe data domain to the visual range, where the visual range is inpixels using the screen's coordinate system to plot markscorresponding to the data. So now, we want to display axes,and axes are a way of communicating the corresponding data domain values forthat visual range. The first step to generatingthese axes is to create margins, to create room in our SVGcanvas to plot the axes, and so we'll do that bybasically moving the origin. Recall that the origin for an SVGcanvas is in the upper-left corner, and we want to move the origin to the rightby a certain number of pixels, and down by a certain number of pixels, tobasically leave room for us to plot axes. We can do this using a translation, and we implement a translationusing an SVG group node, g. And the attribute for that groupnode is going to be a transform, and the value's going to bea translate by x and y. In this case,the amount added to the x coordinate and the amount added to the y coordinateare just the margin values. So now that we've movedthe origin inside and created a margin inside our SVG canvas,we can then generate the axes, and the way we do that isby calling a function. After we've added that group node and translated the origin to the currentpoint, we can call a function, and D3 has a call method thatwill just simply call a function. And the function we call is the D3 method,axisLeft, And that generates, basically, a vertical axis on the left side ofa chart, and we pass to that the y scale. Remember, y is a variableholding a scale object, and that scale object isthe mapping from the data value domain to the y coordinatesof the chart in pixels. And sofrom that scale object in the y variable, D3 has everything it needs in orderto generate a SVG path specification to give this y axis going from0 to 40-some in a data domain. And similarly, if we create a groupobject that moves the origin down to the lower-left inset corner, inset by themargin, then we can generate an x axis. So after we've moved the origin tothat location, we can create an x axis using the d3.axisBottom method,and then passing in the x scale. Remember, x is variable holding a scaleobject corresponding to the category axis, in this case, values going from 0 to 5,corresponding to the indices of the data. And that automatically generatesthe path data for that x axis"
cs-416-dv,8,3,"D_3 can generate axis, x-axis and y-axis renderingsfor a visualization by converting the scaleobjects into SVG paths. Similarly, d_3 cancreate other paths. For example, to create apie chart from data values. The elements aregoing to be paths, but there's somehelper functions is a d_3 method called arc. An arc is a method that maps data into an SVG path outlineof an annular sector. An annulus is like a washer. It's a disk with amissing middle portion. So the parameters ofthe arc function r, it needs an inner radius and outer radius startangle and angle, and if you pass thosefour values to arc, it's output willbe the SVG string that will trace out the pathof that annular sector. We can create a pie chart using that arc function usingthe d_3 method pie. We start by defining pieas being a d_3 pie object, and then we invoke it byusing pie a a function. If we invoke pie withthe data values 4, 8, 15, 16, 23, and 42, it's going to returnanother array of basically the parameters for the arc objects corresponding to each of those data items. So here, the first value 4, it's converted tothe parameter values start angle 0 and angles 0.23 in radians forproportional sector of a pie chart correspondingto the value 4. If we use those as thedata and pair them up with arc values usingan S_3 selection, we're basically joininga database of arc values generated by pie as thedata with the arc elements. To create this, you'll notice that we've selectedall path elements. We don't have any path elements, so it'll be empty, and then we associatethat with data. But instead of sendingthe data values 4, 8, 15, 16, and so on, we run pie on them. So pie converts those sixdata elements 4, 8, 15, 16, 23, 42 into six data values. Each data value consists of the parameters tothe arc function. Now in the intersection, we're going to appenda path element, and the attribute ofpath is going to be d, which is the data for the path, and the data for thepath is going to be generated by the arc function. We call that theattribute method, when you call it with a function instead of with the constant, that function getscalled with the data. The data you've pairedup with that element, and the data we've pairedup with that element or the parameters forthe arc function. So that arc functiongets called with the corresponding starting an angle generated by the pie function. In addition, the bar chartsand pie charts d_3 can generate a wide varietyof other chart styles, will need to move onto the fundamentals of interactivevisualization on our way to learning aboutnarrative visualization. But you've learned enoughabout d_3 now to be able to experiment with the many otherchart styles on your own."
cs-416-dv,8,4,"We've learned how to visualize data. And the next step is to communicate those visualizations through interactive storytelling, so that others can understand the data and draw similar insights. In order to do this, we'll need to understand the fundamentals of interactive computing. And so interactive computing is computing that's involves a user, it's not batch computing that requires no user intervention. These are programs that talk to a user so that the user can express various controls over the program. And this is a user and this is a computer, it's and it's an old Cray supercomputer. But the interesting thing about this picture, this is one of the favorite pictures of Larry Smarr, who founded the National Center for Supercomputing Applications at Illinois, was that this big supercomputer was useless without a terminal because you needed this user interaction between the user and some display device with a keyboard and an interaction device in order to make that supercomputer useful. So interactive computing creates a dialog between the user and the computer. And in this dialog, the user will speak in the language that the user speaks is in terms of clicks, drags, and key presses-- it's all through this input device. And the computer's going to hear these events, hear a key click, hear a drag, hear a key press, and then it's going to send that information for processing. And then the computer's going to speak to the user and the computer speaks in display objects, displaying something on the screen or creating a beep or some other indication that then the user can hear, or the user can see or perceive in order to find that information and react to it-- and then the whole cycle starts again with further interaction. So in order to make this work effectively, you have to investigate user interface design, and user interface design is a very important course for anybody doing software development to have. As much as 50% of any software developer's time is often spent developing the user interface. And so if you don't know anything about how humans work or how humans work with the computer, then you're really missing out on a significant aspect of software development and programming in general. User interface design is a really important aspect of any software development project especially for data visualization used for interactive storytelling. And so the two components that we'll focus on, are task analysis and dialog design. And we'll start with task analysis. Task analysis, you study of the way a person, not necessarily one using a computer, best completes a task. And so that's that's actually studying humans in the absence of a computer some time. And once we understand how people actually perform tasks, how they expect to get information and use that information to perform a task, we can decompose the task into individual user steps, and these are called actions. And then the results of those actions can be shared often among multiple tasks. So as you analyze different tasks, you'll find that the same actions are used for multiple tasks and you'll start to get a library of actions that you can implement user interfaces for. Once you have those actions, you can start to focus on dialog design. If those actions require some cooperation between the user and the computer, then you need to figure out how the user and the computer are going to communicate, how they'll coordinate in order to accomplish that task. And so in order to figure out the language of that dialog, you have to do things like choose display elements, choose user interface widgets. And then once you have those things organized, you can start to connect the actions to the widgets so that the user can understand what action each of those widgets is going to perform and those widgets can be presented as new actions become available. So effective interactive computing is based on user interface design, which is based on task analysis and dialog design."
cs-416-dv,8,5,"In order to understand interactive computing, we'll look at a design pattern called Model View Controller, abbreviated MVC. Model View Controller is actually a software pattern and there's many libraries that set up their software architecture based on Model View Controller object models and systems. We're not going to be using Model View Controller in that sense, we'll use it in a very high level sense, just to understand the roles of the various aspects of an interactive system. We have each of the three complements the model- the model is your application. It's for example a database- interface with the database using SQL queries. The view is what you use to see the result of an SQL query. It formats and displays information, and in fact you can have multiple views. You may have a single monitor, but you may have multiple windows each with a separate view that you're displaying. And finally the controller. We're going to use the controller to represent the input device, but the controller often is the input device as well as the system that's controlling all of the operation of these elements, often through what's called Message passing. So it's going to initiate model and view actions, but it's also going to process inputs as events that then notify the model and view to perform various tasks. So let's use the example of- in HTML of a button, a submit button, you filled out a form and then there's a submit button. The dialog design for the submit button is that you'll move your mouse to the submit button and you'll- then you'll say click to the computer, by clicking the mouse button. The controller is going to sense that you've clicked the mouse button, and that will be called an event. And then the controller is going to notify the view to re-render the butt- and before the button was rendered raised, and now it's rendered using these hints that it's been depressed instead of raised. And then the controller will notify the model to change the state- that's the form has been submitted and it will transmit the data from that form to the model so that the model can update itself with the information from the form. And then the model will notify the view to re-render the button so that it's ready for another key press. And also it may update the view with any information as a result of submitting the form to let you know that the form has been submitted. So we can use the Model View Controller pattern to understand the components of an interactive system and how they communicate."
cs-416-dv,8,6,"So the first multimedia web browser was Mosaic invented here at the University of Illinois in 1993. And we can understand how it and modern browsers work using the model, view, controller design pattern. So here's our model view controller organization that we're using to sort of analyze interactive computing at a high level. We're going to use that to understand how the web browser works from an interactive computing standpoint. And so, a web browser is a client that runs on a client computer, a personal computer, a tablet, a phone, any handheld device, anything that a user is going to be using and it communicates with a web server that provides information from some source and that communication is through this HTTP protocol. You notice we've got the model over here on the web server side. If you have a large database you're not going to keep the large database on your client, you're going to keep it in the cloud or some remote location that's served to the web browser through the web server. But the web browser contains the view when you render a web page, you get the view and it has the controller basically user interface devices that you're using to interact with the web page that are generating events that update the model through HTTP or update the view through the web browser itself. And so, specifically for a web browser that controller is basically running JavaScript and that's effectively doing the job of the controller, user interface device is sending events and it's updating a view. Your view is basically rendering HTML, often controlled by cascading style sheets and scalable vector graphics, SVG. You'll notice that this organization is a bit more sophisticated because now we have two models. We have our model at the other end of the web server. That's our database or other information held at the web server and we have this document object model. This is the HTML and the document object model is just a hierarchical organizational structure for the HTML code. And then HTML code also includes style and SVG elements and so on that are used for the view. Often it's easy to get confused if you're using a more strict model, view, controller architecture on a web browser. Are you working with the model in the document object model, are you working on the model with the web server? For many actual MVC, architectures the model is here and the web server. But for most of what we're going to be dealing with, the data will be pulled into a local cache, we'll pull the data that we need and then it's accessed here. So you can think of this whole thing as the model but really the document object model is this part here. The document object model is really just a representation of what's going to be viewed. So you can think of the document, object, model really as an organization of the view but you can also think of it as a cache of the model held at the web server. So the DOM is an internal representation of the view. Some text in human computer interaction would refer to the DOM as the entire observable as opposed to the subset that is currently displayed. The DOM is also a local cache of the model in the HTTP server. The precise role of the DOM with respect to the model view controller pattern is not important so long as we understand what the DOM is and its relationship to the elements of the MVC."
cs-416-dv,8,7,"When writing an interactive program you often don't write the main function that is run from start to finish. Instead, this main function is a controller implemented by an interactive framework and your program is implemented as functions called by this controller. This is how javascript programs are written for the controller implemented by a web browser. The browser is a program that's running and we interact with it by writing javascript in web pages and that javascript registers callbacks that process the events as they happen. And so let's look at the example of a button press. Here we have an HTML file and we might say we call the button here and if you click on the button it will call the Javascript function Submit. And what does that mean? That means that some place in the browser where it's got the submit button, it's going to store a callback to submit so that when the button is pressed it will call the Submit function. And that's a callback function. Later on in our program and it's somewhere else in our program, we're gonna define what that Submit function is and in this case it'll perform an SQL query, perhaps a HTTP server the web server it'll query some database over there. It's an action that we define but our interface is basically by defining the submit function that we registered as a callback for when you click the user interface element, in this case, a button. If we click the button, we're going to look up for the submit button what the callback is that will get called and then that select SQL query will occur. So we program the browser by registering callback functions that respond to various user interface events."
cs-416-dv,8,8,"Since we're using JavaScriptto control the webpages that host our narrative visualizations, it'sgood to understand how JavaScript runs. In a browser, JavaScript is event driven. That means JavaScript functionsare responding to various events from the browser,like a button push or a page load. And those events are calledfrom an event processing loop. This is something comingfrom the controller. So when you write a JavaScript programin a browser, you're basically writing callback responses to eventsthat happen in the browser. And so that event loop lookslike something here on the left. While forever, we wait for something toappear in the queue, and this queue is just an ordinary array, items are addedin the back and popped off the front. And so, when there's an element,at least one element in the queue, then we pop the queue and that goes into ourevent and we currently process that event. And so we're listening forthese events and when those events happenthen we call their callback. And so event is an object that gets placedon the queue and has a callback function. And that callback is a listener forthat event and so when that event happenswe call its listener. And so we call event.callback, andthen we do the whole thing again for the next event. And so you could have a mouseover event ora button event, a button press event. And if the mouse happensto pop over a button then we will have a popup thatdescribes what the button does. If the mouse presses the button, then we'll call a function calledloaddata that loads some data. And so these two items might bein our queue because we have the mouse over the button,and then we click the button. And so the event processing loopis going to first call popup, and then it'll call loaddata. But the trick is that it won't callload data until pop-up is finished so here is the example thatwe're talking about. We have a load button on the left andour mouse is over that, as shown by that arrowdenoting the cursor. And then we have a little pop-upthat says press to load data and so the pop-up function creates that pop-upand displays it and then returns. And sothat way when we click on the load button, we can call the load data callback andprocess both of those events in order. And the trick here is that popup displaysthat popup, and then returns, so that we can then call loaddata. And that's because JavaScriptis single threaded. Each of our event listeners,each of these callback functions, needs to be run to completion beforethe next event listener is called. And so while we have our eventprocessing loop on the left, it's calling event.callback. And it doesn't return to the loopuntil event.callback returns. And sothat means that popup needs to run and then as soon as popup iscompleted then loaddata runs. So for example popup doesn'tdisplay a window and just wait for the window to go away before exiting. Popup generates the window. And has the window displayed andthen immediately returns so that other events can be processed,such as a button click to call loaddata. And the reason that this is singlethreaded that we don't call both of those functions at the same time in a parallelprocess is that the document object model, the DOM is not thread-safe. That document-object model might havea button that's the ""load button"", it might have another window, the ""popupwindow"", and it might have another window, the ""data window"". And the popup function might accessany of those three objects and the loaddata might accessany of those three objects. And if we put our mouse over a button andwe click the button and we start running loaddatabefore popup is finished, popup might be manipulatingone of the objects at the same time that loaddata ismanipulating one of the objects. And because the document object modelisn't thread safe, that means that one of those functions might be changingdata that the other is depending on. And so we need to completely runpopup first before we run loaddata. And so this can be a problem forexample with server requests. Say we want to load some data andthe data's available on the web. Say it's from some place file.csv comma separated valueslocated someplace on the web. We can just load that using d3.csv whichis going to read the comma separated values file andreformat that into a JavaScript array of objects with each field of the objectdenoted by the first role of the csv. The title of each column of the CSV. But in the process of doing that, anytimeyou're making a web access to a resource, that resource might notbe immediately available. And that resource may take sometime to transfer across the web, across the internet. And so there's going to be some blockingthere that needs to happen and so, that data may not be available andthen when you go to process that data, that means no other userinteraction can happen. And the page becomes unresponsive. Single threaded JavaScriptwould ordinarily stall when you are loading this kind of data butwe use the event loop. The event processing loop and event drivenJavaScript to avoid this kind of stalling. So the way JavaScript wouldhandle something like this or some other event is that wewould register a call back so that when d3 is loading the commaseparated value file, it also registers the callback process so that as soon asthe file is done, then process can happen. ds3.csv immediately exitsafter it's called because it's initiated the load of file.csv andthen it can return processing to the event processing loop sothat other events can be responded to. Then when the file is completely loaded, another event is registered sothat process gets called. And you are not waiting for the fileto load before you do something else. You can be processing other eventsbefore the data is processed, and it's convenient just to embed thatfunction instead of naming that function separately just to embed thatfunction as an anonymous function. So instead of creating a process function, we just had declared that functionas the second argument d3.csv. And that would process the data. And that's fine,except that can lead to callback hell. And so for loading one data file, we needto process that data before loading, for example, a second data file, saythat first data file hasn't include and includes data from a second file,then we need to load the second file. And in the process of doing that, we have these anonymous functions thatcreate this kind of pyramid shape, embedding code inside embeddingcode inside of embedding code. And the code gets very hard to read. It's not very linear. So promises are a method in JavaScriptto handle this kind of event driven asynchronous processingin a sensible manner. The promises that are used hereare from version five of d3. And so what I'm describing here won'twork in previous versions of d3 but works in the current version. Which at the time I'm recording this,is version five. So d3.csv now returns a promise object. And a promise is an object thathas some code that gets run. In this case d3.csv gets runat the time it's called. In order to set up the promise, and in order to open the filethrough an XML transfer. But then, when the code gets loaded, it's going to call this functionthat we have declared here, in the .then member function of d3.csv. Since d3.csv returns a promise, that promise includes thisthen member function. And that member function basicallycontains a function that we declare just an anonymous function thatgets called when we process the data. But because we're using this chaining so that .then is a member of d3.csv. We can just change things along andthe code becomes a little bit more readable because it's top to bottomlinear without all the indentation and the pyramid structure and all the tabbing. So a little bit better,makes a little bit more sense. Under the hood a promise is an object,you can create one by saying new promise andyou pass promise a function. And that function will set up the code that creates the promiseit returns a promise. This object but you pass it witha function that itself has arguments that are two functions, the resolvedfunction and the reject function. And in JavaScript functions orobjects just like anything else. And so then you have codethat sets up a process and sets up resolve as a call back. What actually gets passedto resolve is whatever you then declare in the .thenmember function of the promise. So I've created p as a new promise. And then I say p.then is some function. That then function that I've declaredbecomes what gets passed to resolve when the promise is set up. And you can also have, if an error occurs,so you can have a second function, reject, and that gets passed along inthe p.catch number function. And so then when you actually invoke p,that's when the promise gets executed. And the events that occur whenthe promise gets resolved or rejected get called by whatever you'veregistered as a .then or a .catch. So that's what's happening here. d3.csv is setting up the promise for you. It's loading the file andif the file loads successfully it calls whatever gets registeredas the .then function. And if the file doesn'tget loaded properly, it'll call whatever functionyou pass as the catch function. But the layout is a bit easier to readwhen you write it using these promises. It can still be a little hellish, in thatyou have this .then and you might have some other files that you want to read andthen and then under there, you can actually chain these captains andadopt then actually returns a promise which you can then use so it's not quiteas hellish if you do this properly. But there's still an easier way. And that way is called await. Await is a keyword that you canplace in front of a promise. Or in this case in front ofa function returning a promise. Await will pause the execution ofthe current set of instructions, until the current instruction in thiscase d3.csv resolves its promise results. And in that way code afterthe d3.csv instruction doesn't get executed untilthat d3.csv function results. But while the codeexecution here is paused, await is ensuring that other events arehandled through the event processing loop. In order for await to work, it has to becalled within a asynchronous function. So it has to be inside of a function thathas the async keyword in front of it. And so we need to describean asynchronous function, then you can have an await that willallow the awaited instruction to pass execution of that asynchronousfunctions commands until that, in this case, the d3.cs function returns. And so you have to declare the functioncontaining the await keyword as an asynchronous function. That's easy enough to do in a web page. For example, if you declare yourbody element with a callback for on load that says after the body'sbeen loaded, you call main or some other function that thathandles the processing of that. And then you can declaremain as an a sync function. So here's some d3 code that readsa CSV file using d3.csv, and it reads a CSV file placed on the internetand reviews the await keyword. And the reason we canuse the await keyword is because we've embedded this in an asyncfunction, an asynchronous function. And this function is init. And I've set body to call initonce the html has been parsed. We load the csv file, andthen await says that we don't process any further untilthe file has been loaded. And then when the file has been, we callthe select function that selects body, selects all the paragraph elements,there are none, and it joins the data. And then it calls enter,which identifies all of the data elements that don't haveaccompanying HTML elements. And then for each one of those,it appends a paragraph element, and then it calls this function thatinserts into the HTML of the paragraph element this corresponding text thattells you the item and the number. And so if we run this, we get all these paragraph elementsthat says ""item 1"" 42 is 8 and so if you want to see what's going on,then we can always insert here. We can send out to the console, the datavariable, this various variable here. And so doing that if we look atthe console log is an array, six elements. Item is 1, number is 4 and so on and in fact we also get this extra element,columns, that describes the item and the number and these are pulled fromthe first line of that CSV array. And we can see that the first row ofour CSV file are the column headers, item and number. And then below that, for each line,we have common separated values for each row of the data."
cs-416-dv,8,9,"So, we've learned howa web browser works and we've learned a little bitabout user interface design. So, now we can apply them bothto the design of web pages for interactive data visualization. We'll look at user interface design andthat will start with task analysis. The first step of task analysis isgoing to be to study the way a person will want to work with the information yourgoing to present on the web page. You don't have to start by presentingthe information on the web page, you can just have the informationavailable to a user on a piece of paper. Because you want to study what informationthe users going to want to see, what additional informationthe user's going to want and how they're going to want to identifythat additional information. And that's going to lead tothe identification of specific tasks, and those tasks,you can decompose into individual steps. And each of those steps,of course, bound to an action. But multiple steps mightresult in the same action. So this first task, it's first stepmight correspond with this first action. But, that same action is the firststep of the second task, but the second step of this first taskmight result in a different action than the second step of the second task. Once you have these actions isolated and compartmentalized, you can startlooking at the dialogue of those tasks. For example,a task might be user initiated, and so the user will click to start the action. And then,the system will see that click and issue an event, that event will createa callback to your user to find function. The result will be displayed, andthen the user will observe that, and that completes one ofthose actions of a task. And so, then, you can switch to dialogdesign, and look how the design, the dialog, the communication andcoordination between the human and the computer for each of those actions. The first thing you might want to ask is,who initiates the dialog? Did the user click onsomething like a button? Or, did the system initiatethe dialog asking the user for something through a pop up? And then, once you've organized the actionand figured out who's initiating it, you can start to connectthe action to interface elements. For example, if it's a user Initiateddialogue, you've got a button and the user is going to click the button,then you can start to work through the connection of the button elements andthe call backs required to do that. We can define the widget andwe're going to use a button widget for this particular action. We're going to click on the button and that'll be the event that we use toprocess the action, and then the system effect will be to do an SQL query asa result of that click on a button. So, here's an example of taskanalysis applied to a specific case. One task you might have is tofinish working on a document. And in that case, you might want to savewhat you've done so far, and then exit. And that save step involves an action,and that is to press a save button. And then, the exit step might involvea different action, which is also a button press, but a different kind of buttonpress to exit the application. We've decomposed the finishtask into those two. We might also look at the taskof saving your current results, your current progress,in a different file than the default file. If we want to save as, the first stepis going to be to save our work, and that might involve pressinga similar button for saving. But since we're saving it in a differentlocation, we have a second step that is to select the file that we're going to saveit in, and that's going to require a new action, creating a file popup anda separate dialogue for that. If we save our new file with a filenameof a file that's already existing, we need to confirm that we want tooverwrite that previous file, and so there might be a step three. And that results in another action withanother pop up window confirming whether or not we want to overwritean existing file and then the subsequent dialogueinvolved in doing that. As before, we've got two tasks, and we'vedecomposed those tasks into their steps. And, in this case here, two of those stepsinvolve pretty much the same action. And so, you can start with tasks andend up with actions. And then these actions help you figureout the dialogue involved with performing each of these actions. Task analysis decomposes humantasks into interactive actions, and dialogue design decomposes theseactions into the generation and processing of individual events. Effective interactive data visualizationon the web, thus involves, decomposing the tasks required forthe user to visually understand data down into the individual eventsassociated with the HTML elements. [MUSIC] [SOUND]"
cs-416-dv,8,10,"We'll use d3 to support interactivevisualization by creating reactive web pages that respond to user input. In our model view controller scenario, the input from the user comes throughthe controller in the form of events. And so, we process events in d3using the on method of a selection. So, the selection is the data that we'redisplaying as well as the elements we're using to display that data. And if an event occurseither on those elements or with respects to those elements,then the listener function will get called if you've registered bothusing that on method. And so, we'll start with the eventthat we're listening for and that's just a string describingthe event such as click or mouseover. And if we get that event,then listener will be called. And that's the functionthat processes that event. And that function will becalled with the data value. It will be called withthe index of that data in the order of elements that you have. And then the group within the selectionthat the element may belong to. And then there's a pointto the element itself, and then the actual event in case youneed some details about the event. And so, some of the details you mightneed about the event are the coordinates that the event occurred at if it'sa mouse click or a mouse over. So we can demonstratethat with a tool tip. And so here's an example of a tool tip. You move your mouse over a bar and then you want to display informationabout that bar, such as the data value. Since we've got no access here, we don'tknow what values these bars are at, and we can display thatby using a tooltip. And so in order to do this with d3, weneed to set up a box to hold that tooltip. And so, this box is going to be a div andwe'll call it tooltip. But you'll notice, we make it clear at thebeginning, it's opacity is equal to zero. And then what we're going to do is whenthe mouse is over one of the bars, we're going to set that opacity to one. And so to do that, we're going to createour selection, to create our bars. And then in that selection, we're going tosay if the mouse is over one of the bars, we're going to call this function andwe do that with the on method. And the first thing that function does isit sets the opacity of the tooltip box, that div to one. And then we need to positionit where the mouse is. And the mouse is going to havecoordinates d3.event.pageX, and that's from the upper leftcorner of whatever window we are in. And so it will be page X here andpage Y there. And so we can position that usingthe style left and top elements. And then we can place the textinside to say that we're looking at item number two and a state of valueis d, and those have been called. Here is the data value in the index. You can also use d3.mouseto get the coordinates. So here's an example ofthat tooltip in action. And soas I put the mouse over any of the bars I can see which itemnumber it is in the order. This is the fifth item starting withitem zero here and the value data value. This first item is data value four,second one is 8, 15, 16, 23 and 42. And those are all reported usingthat ON method of the selection. And the code to do that is down here. First thing we do is we create a style forthat tooltip. And notice that opacity here is zero. Then we set up our area for the chart,and then we create a div for the tooltip. And then we go down andsay select all of the elements and then we display them as bars andwith the appropriate data values. And then we have a mouseover as our on method event. And the function sets the opacity to oneand sets the position of the tooltip. And then the contents of the tooltip here. And then when the mouse leaves the bar,we have a mouse out event and that calls a function that simply setsthat tooltip's opacity back to zero, so it's no longer displayed. So d3 creates a reactivewebpages by registering event call backs with HTML elementsthrough its selection methods. And the callback invoked by an eventon an HTML element Has access to both the element andthe data associated with it. [MUSIC]"
cs-416-dv,8,11,"Tool tips are an important element of anydata visualization system because they provide details on demand. And as a user interface element,we should analyze their effectiveness and usability in the datavisualization process. For example, in task analysis,we might consider what information a viewer might need in the datavisualization process. What details should be brought up ondemand and would be placed in a tooltip? And then,the dialog design of the tooltip. One big example is how does the viewerknow that the tooltip is available? That's the first step in the dialogis starting the dialog a nd the viewer needs to know howto bring the tooltip up. That leads us to affordances. And affordances are hintsin a user interface to let the user know howthe user interface works. And in this case, how a tooltip isgenerated, that a tooltip exists. And that the user can mouseover orclick and see more information,more details on demand. So one of the problems with tooltips iscommunicating to the user that tooltips exist, especially if the user doesn'tmouseover one of the data items. And a strategy for showing the user thatthese tool tips exist is to use larger tooltip regions to mouse over. And so we can form what's calleda Voronoi diagram of the data points. In a Voronoi diagram, if this isthe data point here, a Voronoi cell Is all of the points closer to thatdata point than any other data point. This red region is allthe points closer to this central data point than any ofthe neighboring data points. And so we can use those Voronoiregions as our mouseover regions. And if we place our mouseover those Voronoi regions, it'll bring up the tooltip associatedwith the closest data point. And that hints to the user,tooltips are available and can be used to provide details ondemand for the closest data point. So here's an example ofa Voronoi Tessellation. And I can dynamicallymove the mouse around and it's adding a point at the mouse positionand it's creating a Voronoi cell, a red Voronoi cell aroundthat mouse position. And all the points in that red region arecloser to the mouse position than any of the other data points. And so here's an example using thoseVoronoi regions to open up a tooltip to the closest data point tothe current mouse position. So the mouse doesn't have tobe exactly on the data point. It can just be near the data point. And that's enough to create affordancesfor the user to understand that those tooltips are available toprovide additional details on demand. In this case, the country name. So to review, the concept ofan affordance is an important user interface design conceptthat allows a user interface element to provide a hintto the user of its function. [MUSIC]"
cs-416-dv,8,12,"D3 can also visualize relationships,in the form of networks or graphs with nodes and edges. But does so with HTML element animation,using internal timing events. And these events allow d3 toupdate the display dynamically and that can be used to visualizeinformation that's changing over time. Or it can be used to change thevisualization over time to better display information. We use that latter configuration tohave d3 lay out the nodes of a graph. So here's some nodes using our datavalues we've been using so far. And we've also added these edges,relationships between those nodes. And the challenge of laying out thesenodes is you want the nodes to be nearby other nodes that they share an edge with. But you don't want the nodes to benearby other nodes that they don't share an edge with. And so d3 can help layout these nodesusing a force directed graph layout. And it treats those edges as springs,and the nodes as mass points. And so these springs are usedto hold these nodes, close to each other at a fixed distance. And a lack of a spring betweenthose that don't share an edge, means that those nodes aren'tnecessary going to be near each other. So, to implement that forcedirected graph layout, we have to simulate the physicsof a mass spring model. So, we use Newtonian kinematics and that mean F = ma,force equals mass times acceleration. And from that, we get the accelerationis the force divided by the mass. And by the acceleration, we canfigure out where to move these nodes based on the forces thatare being applied to those nodes. In order to do that, we need to figureout how to get the position from the acceleration and the position is goingto be the second integral of acceleration. And we don't like to integrate twicebecause that causes numerical problems. So we use a trick. Instead of keeping track of justthe position of these nodes, we also keep track of their velocities. And that trick is called phase space. That's just a fancy word. That just means we're also keepingtrack of the velocity of the node in addition to its position. Since we have the velocity, the positionis just the integral of the velocity and we can figure out the velocity. It's the integral of the acceleration and the acceleration is equal tothe force divided by the mass. And we can use a technique called Eulerintegration, which says that the new position is equal to the old positionplus the velocity times the time step. And the new velocity is equal tothe old velocity plus the acceleration times the time step and that accelerationis the force divided by the mass. And so given a node and its positionwe need to figure out all the forces that are acting on the node, add them up,and we'll get one final force that we can use to update the velocity andthe position of that node. So spring forces. We have a spring representing an edge. And we can representthat using Hooke's law to figure out the forceson a node due to a spring. And so a spring is going tohave a rest length and that rest length is l and that'sthe length that the spring want to be in. And so the force on node A towards Bis going to be length of the spring vector from either A to B the lengthof that vector minus the rest length. And so that difference is thenmultiplied times Hooke's constant K and then that's multiplied times a vector,a unit vector, from A to B andthat gives you the force on A towards B. And then the force on B towardsA is going to be an equal and opposite direction. We can also simulate gravity on nodes. And we just have a gravity vector. A vector G in a downward directionthat could be equal to say, 9.8 meters per second squared. And you multiply that timesa direction that's down. Actually, it's plus ifthe direction is up here. So we also want to keepnodes from overlapping. And this can happen, for example, if you have a node A connected withan edge at a given length to node B. And node B is connected by an edgeof a given length to node C. But there is no edge between A and C. And so C is free to orbit aroundB at this given length, and the system is at equilibrium. And so C could very well overlap node Aand not violate any of the spring forces. It could be at equilibrium. So we use an electrostaticforce to avoid that. And the trick is to treat node A andnode C and all the nodes as atoms and to compute the electrostaticrepulsion force between two atoms. And so to do this,we create a vector from A to C. And that vector will have a length,and that length is basically r and so we have a 1 over r squared force. So as C gets close to A,1 over r squared gets very large and this term becomes quite large. And that force is applied in the ACunit direction, but negated. And so that will send A further away fromC and the amount that it will send away is based on the charges of A and C whichwe can just assume are equal to one. So d3 implements this dynamic simulation, this physical simulationof a mass spring model. Using the d3 force simulation object. And it's based on Verlet integrationinstead of Euler integration but it's similar in nature. And you need to tell the simulation allthe forces that are acting on nodes and those forces get added up. And used in the integration. And so we tell the forces,we give each one of the forces a name so we can refer to it later andthen a function that applies to the force. And those are as a method.force to the force simulation. And for example, there's a d3 force centerthat allows you to pick a point and all of the nodes try to stayclose to that point, and that keeps the graph from flying off thescreen as it's relaxing in the position. We also need to specify the nodes, weassume that the mass is equal to one for all the nodes. And then we implementthat electrostatic force. To keep all the nodes apart fromeach other even if they don't have an edge between them. And so that uses this forceManyBodywhich is the electrostatic force and that uses a technique called Barnes-Hut. The Barnes-Hut approximationwhich says that this node is keeping all ofthese other nodes away. So, it needs to keep track of the distanceto all of these nodes and that's fine for a small graph, butif you have a graph with a million nodes, that each node is going to have tokeep track of a million neighbors. And there's a million of these nodes andpretty soon you have squared computation to make, and sothe Barnes-Hut approximation takes groups of nodes that are further away andtreats them as a single atom. And that makes things runa little more efficiently. And finally, you have this edges whichare going to simulate spring and so you need to specify the edge andthose are specified with these links. With the source node id,and a target node id. And then we specify the ideal springlength, using the distance method. So, here on the left,is a d3 program that will render a graph, using the force directedlayout algorithm for d3. It starts with some CSS style code, thatsays that the edges will be black lines. The nodes will be circles. And then there'll betext inside the nodes. We created an svg canvas here and then we create the force directed layoutusing this d3 force simulation object. Then we describe these nodes withvalues 4, 8, 15, 16 23 and 42, and then pass those to the simulation throughthe nodes method, and we define links. And the links are fromthe source to the target. Ordinarily these would be identifiersthat but we can also use node values, and that's basicallycontrolled by this id function that we've used to set up the links forthe forces. You notice also we've seta distance of 75 pixels between the nodes connected by an edge,so that's the desired spring length. So we go ahead and run this, andwe get a layout of those six nodes connected by these links. We create this node selectionthat selects all of the nodes. We create a circle foreach one of those nodes and we also create text labels foreach of those nodes. The simulation consists of the edgeforces which are the spring, the Hooke's law spring forces. There's also a many body charge and that's what keeps the nodesapart from each other. And also a four center that keeps thegraph towards the middle of the display. Now you'll notice that node 23 andnode 4 are pretty close to each other so you can increase the strength ofthe electrostatic charge force. And soI can change it's strength to -100 and then that will forcethe nodes farther apart. Also, when we created the nodes, we added some events,we have this d3 drag function and we have a on event,drag event, and end event. And that allows us to drag the node. So we can grab a node and move it around. And you'll see that the simulation isstill running as we drag the nodes around. And that's controlled by these functions. Here and there is this item .fx and fy which are fixed x and y coordinates. So as the soon a node is being dragged, as soon as you click on a nodeto drag it's fixed x and y coordinates are set to it'scurrent x and y coordinates. And then as we drag the nodewe're changing it's fixed x and y coordinates to the x andy coordinates of the mouse. And then when we end the dragging,we set the fixed x and y coordinates of the node to null, and thatallows the simulation to reposition it. So, that's how dragging is implementedthrough those user events. When we run the layout, we see the layoutstart in some fixed position and then it relaxes into place. We don't have to watchthe graph relax into place. And we can change that by thissimulation on tick event. So this is the clock that'srunning the simulation. And every tick that's runningruns this ticked callback. And that's basically updatingthe display position of the nodes according to their positionsin the simulation. If I replace tick with end, and rerun the simulation, then the nodesstart out in a fixed position. And then end up in their correct positiononly after the simulation has run. And then we can no longer drag the nodes. But using that internal clock tick makesthe page run at a given refresh rate. So d3 can layout the nodes of a graphusing a forced directed layout method. D3 has many other layout methods thatwe won't have time to discuss now, but you can explore further on your own. [MUSIC]"
cs-416-dv,8,13,"D3 uses transitions primarilyto smoothly blend charts when changing the data being visualized. So D3 uses an internal clock to create a tick event that allows itto update the display dynamically. We saw before that the dynamic updateof the display can be used for rendering and visualizing graphsusing a first directed node layout. But D3 can also use that tick event to update the view tocreate other animations. One of these is transitions. And a transition is a way forD3 to dynamically update data, either due to changing data values, orto even changing visualization methods. We can use that to some effect, say, if we wanted to look at yourdata visualization knowledge. Right now, you know some visualizationthings, some things about visualization. But after this course,you will know lots about visualization. We get this dynamic change inthis bar from its initial y value to its after y value. And that's transition andD3 automatically changes this rectangle to this rectangle using these key frames. D3 interpolates between the beforey value and the after y value. It can automatically interpolate numbers,colors, SVG paths, transformations and strings, includingdetecting numbers when they're in strings, and also the attributes andstyles of elements. The interpolation is basedon some key functions. There's a D3 interpolate function andit returns a function f(t) that will interpolate between values a and b givenan input value t, ranging from 0 to 1. It's linear, it starts out with a linearfunction, so if I pass the t value equal to one-half, it will returna value halfway between a and b. And basically return a + b over 2,the average of a and b. And it's based on this ease function. And so e is a function of t that maps something from the domain0 to 1 into a range 0 to 1. And it can be any function,it can be linear, it can this, it can do all sorts of things. And so there is an easeLinear. And that just returns t, and soif I put in t here going from 0 to 1, I basically get t out goingfrom 0 to 1 in the range. But I can also use an easeCubic, and this gives me a cubic function that easesme in and eases me out this way, and that cubic function is basedon a piece-wise cubic in t. And it's basically if tis less than one-half, it's giving me a cubic function in t. And if t is greater than one-half,it's giving me a reflection of that. If I wanted to go from t0 to t1,arbitrary t values from t0 to t1, and return some corresponding value a tob using, for example, that cubic, then I can pass this ease function intothe function returned by interpolate a, b. And if this ease function is d3.easeCubic,then I will get this cubic interpolation that nicely starts outanimations slow speeds them up and then slows them down beforethey get to the end. So here's an example ofthe D3 ease functions. Time will be occurring linearlyacross the horizontal axis and the output of the ease function iscoming out of the vertical axis, and so you can think of that as altitude. This black circle that's rising isrising linearly and you'll notice it comes from the bottom instantaneouslyand hits the top instantaneously. And that's using that linear ease functionand showing us a linear interpolation. If we switch to the cubic interpolation,we get this profile. And so now we can see that time isrunning linearly across the bottom, the x value is running linearly,but the y value, the output is starting out slowgetting faster, and then slower again. And so that eases in andout of the transition. And that's less jarring,more natural motion for animations, and interpolations, andtransitions in general. And so these transition use thoseinterpolation functions and those ease functions automaticallyto interpolate attributes. And so d3 you specify yourselection that's joining all of the data with you elements. And then you specify the before attributesand styles of those elements, say, the initial y value of the rectangle. And then you'll have a transitionfunction, this transition method. And then the attributes and style that are specified afterthat transition are the after. And this transition typically takesplace over about three seconds and so this will specify the animation ofsome beginning bar into some ending bar, by doing some interpolation fromthe initial y value of the specified. Here's an attribute, to the endingy value specified as an attribute. And you can also specify transitionsat all three stages of a selection, you've got the enter stage, and this will initialize newmarks to the before settings. You've got the updates section, and this transitions existing marksinto their after setting. And if you do this asa separate statement, it will include the new elements before. So, you just want to make sure you appendthose elements and then, these attributes are set to the before setting, and thenthis transition happens at the update. And so, these attributeshappen after the transition. And then finally, if you have any elements that are disappearing afterthe transition, you remove them here. So here's an example of a D3 programon the left with a transition. It's our same bar chart of these datavalues and so we're going to create a selection and associate thatdata with rectangle elements. But we're going to start those rectangleelements out with a fixed height of only 20 pixels. And then we have a transition method. And then after the transition method,we're going to change the height to our function that's dependenton the values of the data. And so if I run this, the bar graph grows,but it grows awfully quickly. So you can specify moreinformation about the transition. For example,the duration should be about 3 seconds, 3 seconds is 3,000 milliseconds. And so now the bars grow into place but at a more visually recognizablerate instead of so instantaneously. We can also set a delay. So I can set the delay to be 1second which is 1,000 milliseconds. And now when I run the program, therectangle started their initial values, and they grow into place. And you can see both their initial values,as well as growing into place. While transitions are designed to smoothlyadapt the chart to changes in the data, they can also be used to smoothly changethe chart type from one kind to another. It's pretty easy to transition a bar chart into a scatter plot withlittle square marks. It's also possible to transition froma bar chart into a doughnut chart but it's much more complicated andworth looking at if you're interested. [MUSIC]"
cs-416-dv,9,1,"So we've learned how to create reactive web pages that can display charts fordata visualization and make them interactive, make them responsiveto user input. Now, we'll learn how to use those reactive web pages to create narrativevisualizations. We'll start with learning about narrative structure and welearned that from the field of filmmaking and other methods for storytellingthrough visual media. We'll take that on through its application todata visualization, and using data visualizationto tell a story."
cs-416-dv,9,2,"Narrative visualizationis about telling a story using data visualization. So it's important to understand visual narrative structures, these include how toset up your scenes, how to walk somebodythrough those scenes, how to allow someone to wanderoff into their own scenes, and how to communicateyour story to them even if they do wander off. As we seek to tella story with the data, we can learn a lot from other visualcommunication methods for telling stories mostly film, and the methods that film uses in order to tella story visually. Those methods arebased on scenes. As you told a story, you're going to besetting up scenes, and in data visualizationyou're going to be setting up specific scenesinvolving data, and moving from sceneto scene as basically moving from data visualizationto data visualization. As you do this, you'regoing to have to pay attention to structure. How is the datapresented in a way that makes sense to the useralmost immediately? Then you're going tohave to highlight portions of that scene to draw your attention tothe caution of the visual that you want the personto pay attention to. Then pay attentionto the transitions as you go from scene to scene, as you move from one setting of the data to anothersetting of the data, you don't want the userto become disoriented. In film making, the structure helps the userunderstand the scene. For example, this scene was filmed from a user's perspective, an ordinary humans perspective, they're looking at it fromabout six feet tall and it's a country road goingoff into the sunset. We're not filming itfrom a bugs view, we're not filming itfrom a giant's view, we're filming it froma typical person's view. In doing so, it'sa more familiar scene. We could also do thisfrom an aerial view for an establishing shot and that'salso familiar to people, but we're not doing it fromthese unfamiliar views. The same with data, we have to makesure that the data is presented in a way that the viewer can conceive thedata in its proper context, either an overview or for drilling down that we understand the portion wherewe've drilled down into how that relatesto the content. So we want to make sure that the user can seewhat the scene is. In this case, it'sa very familiar scene and you may want to play up the familiaritywith data and start with a very simplesetting of data, and make that morecomplex in order to help the usernavigate the scene. We can demonstratethe elements of the visual narrativeby looking at this example of data journalismfrom The New York Times. It's an article titled Extensive Data Shows Punishing Reach of Racism for Black Boys. It's about the effects ofrace on income for youth, especially for individualswhose economic situation changes from youth to adulthood. One of the elements of visualnarrative is the structure. It helps the usernavigate the scene, and in this casethe scene begins with some text and then belowthe text is a graphic, and that graphic isanimated and it's organized so that there'sa flow from left to right. That structure of the flow from the left to the right along with a color legend that's showingwhat the data depicts, helps the user identify the scene and understand the structureof the visualization. In this case, thevisual narrative is important so that the user can immediately understand what the visualization is depicting. The second is drawingyour attention to a portion of the scene. When you presenting data, it's easy to get lostin a mass of data. So you want to make sureyou focus the user on the particular region of the data that youwant to emphasize. In this particular scene, it's divided up by the ruleof thirds into nine squares, and the square hereis the highlight. That's because it'sthe brightest. We have basically lines that meet ata vanishing point there, and you can use similar effects, you can make somethingflash, or be brighter, or higher contrast, or emphasized in some fashionas you present the data to draw the user's attentionto the part of that data that you wantthem to focus on first. This example fromthe New York Times, demonstrates the effect of highlighting for thevisual narrative. The graph at the bottomis animated, which quickly drawsthe attention of the viewer towards the animation andthe data that it depicts. But the animationdoesn't start instantly, so that the viewergets a chance to see the chart in the contextof the entire article. But once the animation starts, then the viewer's attentionis drawn to the chart. Then there's the transition, as you zoom into the data, you drill down into the dataor you pop out of the data, you want to make sure that the user does notbecome disoriented, that users stillmaintain some context of where they are inthe data visualization and doesn't get lost. So if you can usethe D3 transitions that animate the elements as youmove from scene to scene so that the user canvisually follow those, and you get somecontinuity between the scenes to avoiddisorienting the viewer. Our example fromthe New York Times, demonstrates these transitions. Keeping the useroriented between scenes. Here we havea first scene showing a graph of childrenthat grew up rich, and then ending upas a rich adult, an upper middle-class adult, the middle-class adult,the lower middle-class adult, and a poor adult andseeing the differences in the distributionof those children as they reach adulthood. As you move to the next scene further on down the article, we simply scroll down andwe get other elements, in this case large income gaps. You'll notice thatthe colors that are used are consistent withthe previous chart, so that the transitionfrom this chart to this chart is more familiar because the samecolor legend is used. As you move to the next chart, we still get aconsistent color legend. Then here we see a multiple, the same chart just flipped over, this is now children that are growing up in poor conditions, and as they reach adulthood their income increasesor stays the same, and you can seethe distribution there. So it's followingthat same familiar structure as in the first chart, but now in thisanimated chart yet we're flipping the incomecondition of the children. So that familiarity helps the transition as you'reworking through these charts. Here we see another chart using that same color legend as before, and here we seean animated chart that uses those same colors as before, but adds a few other raceclassifications to the data. In here that samecoloring is used for all of the different racial and ethnicclassifications, and that familiarity in the color legend helps thetransition as you work through the different scenes in this data journalism exampleof narrative visualization. Here we see a final example that's a little less informative, but visually quite stunning. That shows the childrengrowing up in various income levels andwhere they end up as adults. Again, using thatsame visual narrative that we saw in the first chart, that's been heldconsistent so that the transition from the firstchart to the last chart is easier for the viewer to understand and help the viewer keep track of the relevant data."
cs-416-dv,9,3,"The visual narrativedescribes how we lay out each scene ofa narrative visualization. The elements of narrativestructure tells us how to order those scenesso they tell a story. For example, we can have a linear ordering and thisis the most common, this is what we do in videosand slideshows and so on. You can move forward or back. I can move to the next bookmark or I can move tothe previous bookmark, and those arethe only interactions you have for example in a slideshow. That gives the authortremendous control over the presentation of thedata to the viewer. But that might belimiting to the viewer. So there's user-directedorderings of the scenes of data, and in thoseuser-directed orderings you can providemore options to the viewer. So I can go tothe next bookmark or the previous bookmarkas a viewer or if I'm interested inwhat I'm looking at, I can drill down intomore detail and then you can have a presentation of some additional detailabout that data, or if I'm trying to figure out what am I looking atit the current dataset, how does that fit inthe larger picture? If I'm focused ona particular subset of the data, how does that fit inthe larger mass of data? So you can pop up toan overview for example. In each of these cases, you're followinga predefined path that's been set up by the author but you've given the viewer more options. But it's still somewhatconstrained because the author is definingall the paths that you can take, you can just take multiple paths. But it does give the authora lot of control on messaging and being able to deliver the appropriate message. Finally, there's random access. Here's an example from Gapminder. That's a visualization systemthat starts by showing life expectancy based on income per person fora variety of countries. It's a very effectivepresentation, but it's presented initially in this configuration and gives the user complete control to, for example, choose what countrythey're looking at, continents they're looking at, and world regionsthey're looking at, whether they're plottingthings based on population, they can zoom in to a portionof this and a variety of other attributes that they can examine intheir visualization. So none of this is constrained by the author except for the choices of attributes that you can look atin the visualization. It's a very free form version of interactive datavisualization through the web, but it still effectively tells the story that the authorwants to convey. Those are the orderings ofthe narrative structure. There's also the interactivity that the user has available for navigating throughthe visualization and manipulatingthe visualization. So the interactivity hastwo basic components; one is the number of ways the user can manipulatethe visualization. Can you just advance from the next bookmark ora previous bookmark? Or can you drill down? Can you change fields? How many different ways canyou change the visualization? More importantly,how does the user understand that they canmanipulate the visualization? One of the challenges ofinteractive computing is communicating the different ways that the user caninteract with the system, and we'll look more aboutthat later in the lessons. In this updated version ofthe Gapminder bubble chart, you can start to seehow interactivity in the visual encoding ofdata values can be used. So a user can determine whether family size has an effecton life expectancy, or how family sizeaffects income, or carbon productionrelates to income, or how carbon productionrelates to family size. Finally, there'smessaging. That is, when you put data on the webfor a user to visualize, you can use that data to tell a story and you cancontrol what story is being told in the waythat you present the data to emphasizesome aspect of the data, something that you'velearned that you want others to learn as well. The messaging is how those lessons that you'velearned are communicated to the user and how the user of your system could maybe learn that lesson andmaybe others as well. So for data visualization, we set up scenes consisting of charts and dashboards of charts, focusing on the structure, highlighting points of interest, and making transitionsbetween charts continuous so they don'tdisorient the viewer. If we do this right, then we can useordering to effectively direct the viewerthrough multiple charts, and we can use interactivity to allow them to interact with the data and messaging to ensure that the viewer learnssomething from the process."
cs-416-dv,9,4,"As we look at putting data visualizationinto interactive webpages for interactive storytelling, the individual charts will form scenesof the story the data is trying to tell. In order to do this effectively, we need to learn from the othervisual narrative genres. The first of these is video, and in video,you can basically play or pause. Those are the classic controls forvideo, and if you're watching a movie, you can just play, or you can get up outof your seat and walk out of the theater. But this is used to greateffect in the web pages, interactive visualizationof human mortality. And in this visualization, it's showingyou what people might die from, and it's a bunch of gray dotsrepresenting a population. And when somebody in this population dies,the gray dot turns a color. And that color indicates what they diedfrom, and then you'll get a bar chart here showing you the relative proportionsof what people have died from so far. And it's a very powerful message and ithelps you know what you should be afraid of andwhat you don't need to be afraid of. And it uses the videogenre going from scene to scene basically years 39, 42 years old. And if you use pause, you can pause andthen you can hit play and those are really the controls you have forthe pacing from scene to scene of this. And those are the same controls you havein life, you can step back and pause for a moment, take a look at your life andthen you can just continue aging and you certainly can't rewind. This mechanism of video playing and pausing is used to greateffect with this data set. Another genre we can lookat is the magazine article. In this case, it's why so many Americansare in jail and in prison, and it's basically a bunch of text andfigures interspersed. And you have, as a reader, the choicewhether to read the text or to look at the figures, and you can bounce back andforth and you have control over that. And we can do the same thing with datavisualizations that are embedded in webpages of a lot of text. These figures can be interactive andadd control, but then the user still has the control whether to look at the textand understand the story from the text, or then jump to the visual andunderstand the story through the visual. Another genre for visual ordering that'sbeen very effective is the comic strip, going from frame to frameto frame in a comic strip. And the layouts of those framesimplying a visual ordering of the scenes of the storythat's being told. In this case here's an example of the lifecycle of a Japanese beetle that's been used by Tufte and others to illustratethis method of communication. You can see the life cycle of Japanesebeetle starts as a larvae stage, and then it matures and becomes a beetle,and then it mates and reproduces, and then there's more larvae andthe life cycle continues. And the process of doing this is beingshown through a timeline because you have each individual frame going from timeto time in a comic book fashion. This is certainly not realistic, this is sort of a comic visualization, noteven quite linear, but it's showing that the larvae can start here in July evenwhile other beetles are out mating and reproducing all the waythrough to the end of August. And so it's done veryeffectively using the familiar visual ordering of a comic strip byvisualizing some data, in this case, of the life cycle of the Japanese beetle. Similarly, a flow chart also conveysthat visual motion of going from frame to frame, but it does so with scenes andthen arrows like this arrow here. It's done quite well with thisinteractive visualization, this interactive website showing howbusinesses can benefit from different space choices as they go fromstartup to ramp up, to speed up. And so we can look at this for the example of the launch whenyou're launching your business. And you get a little visualization herecomparing the use of executive suites or full service shared spaces orleasing full office space for your company in that early stage. And so that color coding is used here toplot on these bars across cost, up front cash, efficiency, and other issuesthat you might have in your startup. And so that's an example. We're using a flow chart as a genre forvisual ordering can be quite effective. And you're basically doing that anytime you're using arrows, boxes, and arrows in your visual ordering. Another common visual orderingof genre is the slide show. And in the slide show,you're pressing an arrow to advance. And this is done to a great effectin the New York Times' visualization of the medal count from the Olympics. And sohere's the results of Rio 2016 Olympics. And we can press this arrow and movebackwards in time, and it's using D3's transitions to animate between theprevious display and the current display. And so it's paying attention to thosetransitions from scene to scene to help you keep your bearing. And you can progress further andfurther back, and see the relative change in metal counts, and the effects ofother issues, for example, boycotts. But basically we're either movingbackwards or forwards by advancing slides, and in doing sowe're using the slide show genre for visually ordering those scenes,basically those years of the visual data. Another visual ordering is the partitionedposter, and you see these most often with research paper posters that youwould present at a conference. But here's an example from the XKCD comicshowing money in its various scales and it's partitioned. We've got a region here showing dollars,and here we have thousands of dollars and youcan see a slight arrow here that's showing how a single dollar lookscompared to a thousand dollars. And we go from thousands to millions,and millions to billions, and from billions to trillions. And so each one of those isa separate partition of the poster. And it sets up a certain flow as you gofrom one to the other based on the layout. And so layout is very importantin a partitioned poster to be able to infer that visual ordering. And you've got some largetext here to try to draw your eye toward the beginning of itsince there's a lot of detail. This is also an exampleof a zoomable interface. You've got very, very fine text that youwould have to zoom out in order to see. And so this kind of scalable interfacewhere you zoom in to see the detail is also a very effective genre. It's just not a genre usedmuch before computing. And finally, there's the annotated chart. And this is one of the most popularvisual ordering mechanisms for ordering the scenes of the data that are inthe story that you're trying to present. In this example, it's just an imagefrom a New York Times interactive visualization of a weight tracker for somebody that was keeping a diary oftheir diet and weight loss plans. And you get various annotations here,and scenes shown here. And you can advance from scene to sceneby clicking on the appropriate location in the graph. And that will take you to that scene, andexplain more detail about that scene. So each of these genres,the linear video, the static magazine, the visual and flow implied by a comicstrip or a partitioned poster, or made explicit by a flow chart, orthe explicit sequencing of a slide show or annotated chart can all be appliedto narrative data visualization. And the choice of which to use depends onthe form of the data visualization and the message you're trying to convey. [MUSIC]"
cs-416-dv,9,5,"The design of anarrative visualization is influenced primarilyby the messaging. Narrative visualizationslie on a spectrum with author driven magazine stylevisualizations at one end, and completely free-form reader driven visualizationsat the other end. But there are manyinteresting examples of narrative visualizationsbetween those two extremes. So as we saw before in the elements ofnarrative structure, that we looked at the ordering, whether you have a linear,or user directed, or random access ordering, and that makes a difference whether the ordering isdesigned by the author, or if that ordering iscontrolled by the user. So if we look at those orderings, they form a spectrum. Orderings aren't necessarilycompletely author driven and they aren'tcompletely reader driven. Those are the ends of a spectrum for narrative visualization. So we can look athow the message is conveyed in anarrative visualization. Is the visualization designedto be message focused? If the narrative visualization is designed to tella specific story, you want a narrative structurethat's more author driven. On the other hand if youwant to present the data, let the data tell thestory and let the viewer interact with the data to figure out what storyshould be told, then you can makea more query focused interface that's less structured and less focused on the message. Author driven storiesare often linear. You start at the beginning and you work your way to the end. There may be multiplepaths to get there, but they are very linear atthat far end of the spectrum. Reader driven storiesare more free-form. You're given the data andyou can manipulate it in any way you like to learnyour own lesson from the data. Author driven stories areoften non-interactive. They're designed to letthe author take you from seeing the scene ina very prescribed way. Reader driven storiesare interactive. Sometimes there maynot be a structure, the author sets upthe environment and then the reader is free to make their own story by working their way any way throughthat environment. Some examples of author driven media are cinemaand commercials, where you have no controlof the pacing, you can't even pause. You watch something verylinearly from start to finish, and some examples of reader driven stories areGoogle at the extreme, where you just query anythingyou want, or Tableau, where you get a basic databasevisualization system, and it's up to you tofigure out what you want to visualizein that database. So the author driven and reader driven sides ofthe spectrum are extremes. What we see in the middle are what we find in mostdata visualizations on the web used for narrative visualization forinteractive storytelling. Those are hybrid structures that effectively mixthe messaging that you get from an author driven story with the flexibility and interactivity you get from a readerdriven story. These follow the structuresof a martini glass, an interactive slide show, and a drill-down story. So the three most effective narrative visualizationstructures; the martini glass, the interactive slide show, and the drill-down story, each combine author driven and reader driven structuresin a unique way."
cs-416-dv,9,6,"So the Martini Glass structureis a martini glass, but in order to see the structure youhave to rotate it 90 degrees. And the author driven content is at thebeginning and following this stem shape. And then you follow a prescribedpath from seeing the scene. And then you get to a point here whereyou can start to explore in a bunch of different directions, andthat's the reader driven content. And that's a good mixture of theirauthor-driven content first and then the reader driven content second. But you can understand better whatyou can do to what the data is and what you can do to interact with the data. The author can providequestions observations, the author can writean article effectively. That's taking the reader downthis this very narrow path, many hit a jumping off point, and thenthe reader can do things like highlight certain things that they're interested in,they can filter and zoom into the data find the particularsubset of the data they're interested in. And they can also choose which path thatthey want to follow while the authors maybe not let him to go in any directionbut letting them go in any one of multiple directions thatare more author controlled. The author for highlighting the authorcan also highlight certain example data sets that the reader mightchoose to start with. And so an example of this isdollar street from Gapminder. This starts with an author defined path. It's basically showing various familiesand their incomes around the world to help bring their situations to light andit starts with a quick guide. That's a very narrow stemalong the martini glass, you starting by first seeingthat there is this spectrum from the poorest to the richest familiesthat everybody lives there. And it's talking about families and even though these families are fromall incomes around the world. They're still families andthat in each country, each of these different familiesmay have vastly different incomes. You can pull these sliders back andforth to filter out richer or poorer families to narrow the focusof what you might want to look at. And then you're ready to start and thenwe can do things like we can filter out, look at just a subset of families. And you can pick any one of these and findmore information about that family and where they are in the world, andmore details about their situation. That's following that martini glass,we get the lessons in the stem. And then we are able to move ina variety of different paths somewhat controlled by the the website, butin any direction that we wanted to go."
cs-416-dv,9,7,"Another hybrid structuremixing author led and reader led structures isthe Interactive Slideshow, and here you follow an author directed paththrough a slideshow. During each slide, you can maybe drill downinto some details, or investigate some tangent that you might be interested in, or you can choosejust to continue on if you're not interested inthe details of that slide. So as a good example ofthis in this New York Times website visualizingbudget forecasts and how they compareit to reality. So it's following a slideshow. We've got six slides hereand a slide advance here. We can advance fromslide to slide, and it's showing that now thatthere is a forecast here, and it differs fromlast year's forecast, and going to the next slide. It starts to plot. At each point in time, what the forecasts were planning for and what actually happened. If you're interested thatall the forecast seemed to be positive or even though reality was notperforming as well. You can start to look atthese details with a mouse-over, and as you can drilldown into some of those details in each slide, and then you can advanceto the next slide, and you can see that there's even more disparity betweenthe forecasts and in reality. So that's a good exampleof moving from slide to slide that being able toinvestigate further in-between."
cs-416-dv,9,8,"A third structurethat's effective for narrative visualizationis the Drill-Down Story. You get your datapresented in a scene, but you get many options inthat scene to drill down and to follow a narrative as you drill downinto the details. So an example of that is also available on the NewYork Times website, looking at various bear markets and this was publishedOctober 11th, 2008, right in the middleof one the real estate bubble burst and I think the idealistic comparethose times with the previous times. So you have all the datapresented here in an overview andyou can drill down. You can start to look atthe other markets crashes. This one's 1942, this was1932 and how long they took, how far they dip down, and you can you canuse that to compare. So you're drilling down inmany different directions to find more details outabout each of these, but you're startingfrom a single overview as that one sceneto drill down from. At this point, youmight notice that the Interactive Slideshow and the Drill-Down Storyare quite similar. In both cases, youget the option to investigate further to drilldown into some detail. The difference betweenthe two is that the Interactive Slideshow gives you the opportunity to drill down in a particular slideand then it takes you to the next slide after you've drilled downinto a detail. The Drill-Down Storyon the other hand, gives you one large overviewwith many options to drill down and you can select which option youwant to drill down. So the messaging isusually stronger in an Interactive Slideshow and Drill Down Story is moreof a free form interface. So the three most effective narrativevisualizations structures: the martini glass, the Interactive Slideshowand the Drill-Down Story, each combine author driven and reader driven structuresin a unique way."
cs-416-dv,9,9,"In 2014, Arvind Satyanarayan and Jeffrey Heer published a paper on a systemcalled ellipsis, designed to generatenarrative visualizations. The system isn't quiteready yet for general use, but the theory that theybased it on serves as a good fundamental model fornarrative visualization. This model fornarrative visualization consists of fourcomponents: scenes, annotations within each scene, the parameters that are the variables used forthe narrative visualization, and then triggers that arethe actions that occur as a result of various interactionswith the scenes. We can demonstrate these withthe example of the budget forecasts datajournalism article. In this case, the scenes are shown here the different slides, slide 3 inside slide 4 and you go from scene 3 hereto scene 4 here. Then in each scene,you have annotations, and this is the messaging your drawing out a certain aspect of the data in each scene and you dothat with annotations. These are all describedby parameters. The difference betweenscene 3 and scene 4 is that scene 3 takes place here in 1995 and scene 4takes place here in 2008, and these are controlledby a parameter setting. So as you move from scene to scene that seem to setting up the various parameters that control the charts in the scene, and those parametersbecome the variables of the narrative visualizationthat control all aspects. As those parameters change state, it'll cause other parametersto change states. So if the scene parameterchanges state, then that's going to cause the gear parameterto change state. One parameters changedbecause other parameters to change and you start toget a state machine, and you can go fromstate to state and some states will affectother variables and so on, and you createa finite state automaton. So the scenes are analogous to slidesin the presentation. This is scene 4, demonstrated by slide 4. Each of these scenes is usually consist ofa single chart. Sometimes they'll havea coordinated set of charts. You could have for examplea timeline over here, and all of these scenes, scene 1 corresponding to slide 1, scene 2 corresponding to slide 2, follow the same template, follow the same visual form. That is not only providesa consistent visual structure, it keeps the viewer orientedthrough transitions and that's an important aspectof narrative storytelling, is to avoid disorienting the viewer whenyou switch scenes. The annotations areused for messaging. They draw attention tocertain aspects of the data, and again they followa consistent form. The annotation onthe left is the same as the annotation on the rightin terms of its form, its text with a drop line, and the yellow dot in both cases. These annotations are usuallycleared between scenes, but they can remain betweenscenes when you're using the scenes as a sequence tobuild towards a conclusion. The parameters are the variables that are used in the chart to control the scene and the elements in the chart,and everything else. As we showed before, the slide number isa parameter and that controls the current state andthe state machine as you go from slide to slide. The chart here isa parameter controlled by that slider whetheror not to include these forecasts as a parameter, and these forecasts areleft out in some of the slides and whether to animate the chart or whetherto just show the chart at once is another parameterin this example. Finally, the triggers, that the connections betweenthe parameters. When one parameter changes, it can cause another parameterto change as a result and that first parameter is triggering a change inthe second parameter. For example, if slide number 4, if the current slideis equal to 4, then we change the animateparameter to true. We set the year to 2008, and then we leave the previousannotation from slide 3 where we drawnew annotation for slide 4. If we go to slide 6, then we don't animate and weset the year to 2010, and then we clearall the previous annotations. So the four fundamentalelements of narrative visualization are the scenes consisting of charts, the annotations of those charts, the parameters used tocrave those charts, and the triggersused to transition between charts and interactwith the visualizations."
cs-416-dv,9,10,"In Tableau, a narrativevisualization is called a story, and the scenes are called story points. And each of those scenes is representedby a sheet which forms the chart of that story. These can also be multiple coordinatedcharts in a dashboard as well. And so here, we see a single chart. We have the slideshow, and these controlsstate variables that give you the sequence as you're working through the story. And in Tableau, annotations also followa standard template, as shown here with the arrow, so that each time you addan annotation it's in a familiar form. And that helps orient the viewerfrom seeing the scene. You can also use Tooltips, andthat creates another kind of story, a drill-down story as opposedto an interactive slide show. And Tableau has parameters that cantrigger other parameters as well. In Tableau, you have fields,especially calculated fields, that play the role of parameters thatcontrol things like the current year or the current state of the presentation. And Tableau supports default actions. For example, advancing a slide froma previous slide to another slide. But there are ways of hacking it the sportmore customized actions, and you can go, for example, to this websiteto see some other examples. So here's an example from Tableau Public,using that model of narrative visualization to tell the story of whatmaternity leave is really like, and raising a toddler from infant to crawling. We get sequence starting with week one,and it lists, for example, the number of diapers andthe number of feedings per hour. And as each week progresses,you can see how these change. And here's an exampleof a drill-down story. This is generated on Tableau stories,but embedded in a website. In this example,you see an atlas of the world, and you see disks corresponding tothe number of cardinals in each country. And if you mouse over each disk, you cansee more information to drill-down to the details aboutthe cardinals in that country. And then, you can select one if you want,or you can do multiple selections. So here's a narrative visualizationthat we've been using as an example all throughout this course. It compares budget forecasts with reality. And it's based on a New York Times articlewritten by data journalist Amanda Cox. In this case,we've implemented using Tableau stories. And so, each of the scenes isset up as a Tableau story point. And we can flip through each of the scenesusing those Tableau story points. And you'll see, as we flip throughthe scenes, we have annotations. And these are ordinaryTableau annotations. And so, they follow the same template andprovide consistency. And you can build on previousannotations to build up a story. And then, as you complete that story,you can remove those previous annotations. In order to see the parameters, we can open this Tableau story up inTableau Public and look at the details. Here's the story interface, and the slides are basicallypointing at individual sheets. For example, here's the firstsheet that's used for scene one. And we have both a real budget and in theforecast budget involved in the same plot. And so, that's basically a dual axis plotwhere the axes happen to be the same. We have other parametersincluding forecast years. And as we change years,we go through different charts and we use separate filters in orderto indicate the years that we're interested in for those particular charts. And so, budget year is one of the parametersthat changes as the scene changes. And also, the scene change is a triggerfor the budget year to change. And so a change in the scene parameteralso triggers a change in the filter parameter."
cs-416-dv,10,1,"So we've learned how to createnarrative visualizations. Visualizations of data thattell a story with the data. That allow viewers to interact withwebpages that are displaying data, to learn something about the data, and then allow them to explorefurther with the data. Now we'll look at modern toolsthat have been developed. Very modern tools, that are just on the cutting edgeof data visualization on web pages. That have been developed tofacilitate neuro devisualizations, these are based onDeclarative Programming Languages. Programming languages that focus on,what a page should display and not how it displays it. And so we'll learn about thoseDeclarative Languages, namely Vega, Vega Light and Ellipses. And how they've been developed tohelp people tell stories with data in a more regular approach. [MUSIC] [SOUND]"
cs-416-dv,10,2,"The Grammar of Graphicswas a book published in the '90s by Leland Wilkinson,a statistician. It revolutionized the way wethink about plotting data, about visualizing dataespecially data from a database. It approached the visualization of data in the same waythat we would speak in describing the data using sentences andthose sentences form a grammar. When we're presenting a chart, a visualization of the data, we're speaking aboutthe data and that chart is itself using a grammarto formulate the data, to describe the data, to communicate the data,using well-established rules, in this case rules ofvisual communication, whereas a sentenceis using rules of written communicationor oral communication. So here's an exampleof graph that was used in the book tomotivate this grammar. It consists ofseveral components. It's a graph of world data. Horizontally, wehave the birth rate. This is the number of birthsper 1000 people in a year. Then we have the death rate, the number of deaths per1000 people per year. This is from the year 2017, and I pulled this from theWorld Development Indicators. So for every country, we're plotting its birth rate, and then we're plottingits death rate in this scatter plot. Some countries havea high birth rate but also a high death rate. Some countries havea low birth and death rate, and some countries have a higher death rate thanthey do a birth rate. So we've got thisreference line here that Leland Wilkinson inserted into the graphic called theZero Population Growth. Countries above this line, to the left of this line, are countries wherepeople are dying faster than peopleare being born. Countries to the right areincreasing in population because you've got more peoplebeing born than are dying. The birth rate is greaterthan the death rate. So Leland Wilkinson used this for the Grammar of Graphics as an example todemonstrate how you can think grammaticallyabout communicating that population growth in the form of a birth rate anda death rate as a chart. What grammar goesinto constructing a sentence tobasically communicate that data where that sentence is a chart and the componentsof the sentence, the nouns, the verbs, the prepositional phrases are the components of the chart. So at the very top, we have a chart. That chart consists of a guide, a frame, and a graph. The frame is simply what's the data that's being displayed. In this case it'sbirth rate and death rate. So the frame of the chart, it's using those two variables; the birth rate andthe death rate. Then we have a graphon the right. We have two graphsactually superimposed. One is a scatter plot andthat is a graph of points. Each of those pointshas a symbol, in this case, a dot, and then a label, so that we can see which dots correspondto which countries. So the dots are labeledby their country. But we also have a contour graph, and I've rendered that graph using color forthat contour graph. That's basicallya density plot of the countries where red indicates a denser region of countries where you'vegot more countries, and the yellow, green, and blue areas are where there are fewer and fewercountries in the plot. Those two graphs are superimposedtogether to communicate the layout of countries in that birth rate versusdeath rate scatter plot. So that forms the graph. Along with that graph, in order tocommunicate that data, we not only havethe data in the graph, the data is expressedin the frame and then the graph actually plotsthe geometry of that data. But we need to understand that, we need more to the Grammar of Graphics than just the data andthe graphing of the data. We need guides. Those guides are in the formof an axis and a form. The axes are a horizontal axisand a vertical axis. So they're describedby these rules, the lines and the tickmarks, the scale, so that you know what'sthe minimum and maximum, extent of the axis, and then the labels for the axes. For example,the horizontal axis is the birth rate and the verticalaxis is the death rate. In addition, we havea second guide this form. That's a line of equal birth and death rate values that extends essentially atthat even one-to-one rate to indicate where you wouldhave zero population growth, and we indicate that zeropopulation growth with a label, an annotation that goesalong with that form. So each of these elements becomes part of the sentencethat we use to communicate the populationgrowth of all of these countries in the form of a chart visualizing that data. So the Grammar ofGraphics decomposes this process in theseven basic components, actually even more, butseven basic components. We can start to think more systematically aboutcommunicating with graphics, about tellinga story with data by thinking grammatically aboutwhat those components are. The seven componentsthat will concerns Grammar of Graphics consists of, at the highest level, the first one is data, and that data isbasically expressing variables that correspondto the data sources, basically labeling each of the data sources witha variable name. So I'm expressing thathere as variable name equals data source inthose angle brackets, and those are inside parentheseswith an asterisk after that inregular expression syntax, meaning you couldhave any number of those variable name equals data source assignments You also have data transformations. These data transformationsare things like sorting your variables so that the datawithin them is sorted, figuring a rank of variables, the top 10 elements ofa variable, for example. Then we express the frame. The frame defineswhat's being plotted. If we're doing a scatter plot, what is the horizontal axis? What is the vertical axis? So we have a variable operator,a variable combination. Where we have two variables, a horizontal variableand a vertical variable, and then an operation. That operation istypically cross-product. So we're looking at something plotted versus something else. The first variable plotted versus the second variable,horizontal versus vertical. Those are examplesof cross-products. Wilkinson also introducedsome ideas that were later implemented in visualizationpackages like Tableau, things like combining axes or nesting axes asdifferent operations. Wilkinson also discussthree-dimensional plots as an expression of the frame, where you'd have variable crossvariable across variable. Then each of these axes is going to definea coordinate system. For a scatter plotlike we saw before, those are two linearcoordinate systems. You could havepolar coordinates systems or a spherical plot. So this coordinate sectiongives you a chance to say whether you are dealing witha linear coordinate system or a polar coordinate system. The scale section describes any coordinatetransformations. For example, in our plot, we had two linear axesbut they could have also beenlogarithmic or some other transformation tothe variables as we're assigning their data values to actual spatialcoordinates for plotting. Then we specify the graph. The graph is specified, as we specified inthe previous example, as points. So a point would be usedfor a scatter plot. We could also connect consecutive points withlines and do a line plot. We could do an areachart where we filled in the region betweenthe line and the axes, we could do a bar chartwhere each of the individual points hasan area below it filled in. Here there's an eighth component that's actuallya subset of the graph, and those are the aesthetics. So within point, youmight have some marks, some other visual attributes that you can define, for example, color based on a variable, shape based on a variable, size based on a variable. Those are embedded withinpoint line or area and so on. Also, a point line andarea can be combined, you can get worldswithin worlds by having points within points and so on. Then finally guides. These are the annotationswe saw before, both the actual axes andthe labeling of the axes and any additional annotations we want to add to the graph. For example, azero population growth line. So the other takeaway from the seven components ofthis Grammar of Graphics is that this leadsto what's become a declarative method forprogramming a visualization. We'll describe what declarativevisualization means that it's a different mode of programming than whatwe've seen before."
cs-416-dv,10,3,"In order to understanddeclarative programming, it's first important to understand imperativeprogramming. Imperative programmingis ordinary programming. The kind of programmingyou learn in your first computerscience course where you learned howto program a computer. You write a program as a list of commands and those commandsare run in sequence from first to last and so there's a general control-flowfor your program. Each command somehowchanges the system's state. Here we have a sampleprogram on the right. It declares a variable nand then it sets the variable n to be a listof values from one to five. So we have three system states in that imperative program asthat program is being run. State one is the initial state. In the initial state the programhas been initialized. There's no variableson the stack. Then state two, the variable nhas been placed on the stack so there's room on the stack for variablen. Then in step three, an array is constructed and the sign tothe space on the stack allocated variable n. So at the end of eachof these commands, the system state has changed from the initial state and that's the idea ofan imperative program. It runs from top tobottom using the rules of control flow with each statement changing the state of the system. So the order ofcommand is important. If a command comes before another command inthe control flow, then we know thatfirst command gets executed first andthe later command gets executed afterwards and thatsets up a sequence of state changes thatwe could use as a mental model for howthe program operates. In a declarative program, we're not setting upa sequence of commands that will change system state in order to produce some solution, we're going todescribe the solution. So declarative program is more like a specificationof a desired system than a recipe an ordered list of instructions forthe computer to execute. So it's less ofa linear program that has less control flow and it looks more like a database,a specification. As a database orderis less important, because the databaseelements are being read in simultaneouslyeffectively and not in any order of fields. So HTML and especially HTML with Cascading Style Sheets is a declarativespecification of a webpage and this declarativespecification can be thought of asa program that creates a webpage and so we mayhave three elements of our webpage denotedhere by the blue div, the red div, andthe green div and the CSS might place those three divsin different locations. So we have a CSS positionstatement for the blue div, a CSS position statement for the red div and a CSS position statementfor the green div. Ordinarily, if you just ran the HTML withoutthat cascading style sheet, you would see blue contents, the blue element, a blue paragraph followed bythe red paragraph, the red element, whatever it is, and then the green element. But because of that CSS the actual page is laid out where the green element isin the upper left, the red element isin the upper right, and the blue element is in the bottom and that'sa completely different ordering. So HTML with thoseCascading Style Sheets becomes less of an orderly flowof information that gets output to a webbrowser but more of a specification ofthe content and that content is controlled by the Cascading StyleSheet to determine where it appears in a particular renderingof that webpage. So these three divs are really three database elementsof information for the webpage and how thosedatabase elements are used in the rendering ofthe webpage is controlled by the by thisCascading Style Sheet. Also the order doesn't matter for the Cascading StyleSheet and you can specify styles for variouselements in any order as well. That makes HTML andCascading Style Sheets more of a declarative approach toprogramming a webpage, because you're actually justspecifying the webpage. HTML events are also declarative. We use HTML events to specifythe webpage behavior, how the webpage reactsto a given user event, the user input event. As we're addingelements and creating the database thedocument object model, the part of thatdocument object model includes events for each of the elements and specifically the event handlersfor each of the elements. For example, on click, if you click an element then you might call a select function. On double click you might call an act function and havesome action result. If you place your mouse overa button then you might call a describe functionthat describes the element. It doesn't matter which order these event listeners have been specified in as callbacksfor these event handlers. The point is that if eachof these events occurs, then the appropriatecallback gets called. So order is alsoless important here. We're specifying the behavior, the specification of the webpage in terms of its behavior and not necessarily animperative order or recipe of how to handleeach one of these events. So event streams cause a resulting actionand because of that, we're specifyingwhat actions occur as the result of an eventin a stream of events, this is an example ofreactive programming. So the Grammar of Graphics, we had seven items ofspecifications for a chart or for a graphicthat's going to convey some information goingto communicate some data. So we had data, the transformations ofthe variables of the data, frame that defines which variables formthe axes of the data, the coordinates, howthe other variables form the coordinates for plottingthe data, the scale, how those coordinatesmapped to the screen or to the display and thenthe actual graph specifying it as a bar chart or a line chart and then guides things likedisplaying the axes, labeling the axes, and the tick marks and any additional annotationswe might have. That becomes a declarativeprograms specification for something that wouldcreate that graphic. The Grammar of Graphicsdescribes this in a conceptual form and it ended up getting implementedin some forms through Tableau and through D3and other tools sense. In the way that thisgets implemented is from that declarative program describing what the graphshould look like but then be compiledand produce code. Then that code wouldbe executed and then would run and generatethe desired graph. So the code would bean imperative program, some kind of standard sequence of instructions thata computer could process. But in order to create that code, you are writing aseparate program that gets compiled into that executable code and that's separate program is adeclarative program. It's declarativebecause it doesn't matter which order you'respecifying these elements, the data, the transformations, the frame and so on. You're basicallydescribing what kind of graph you want togenerate and letting the compiler deal withthe details of how that becomes imperative code that can be executed on a computer. The next question is how do we integrateinteractivity into this? The Grammar of Graphics waswritten to generate graphics, to generate charts that a viewerwould see but these were passive charts and didn't really involve any user interactivity. In order to incorporate user interactivity intoa declarative program, we have to look back atimperative programming and we have control flowfrom top to bottom. The first instructionsget executed first and laterinstructions get executed later and each one of these instructions is changingthe state of the system. Then we think ofReactive Programming. The difference betweenimperative programming and Reactive Programming is that control flow inan imperative program flows from the beginning of the program to the end ofthe program and there may be some loops and some conditionals and so on affectingthe control flow. Reactive Programmingassumes there's some kind of data stream. Some kind of stream of databe that input data from a sensor or it could beuser interaction events. Each time data comesthrough this stream, there's a reaction and these reactions are inthe form of event listeners, callback functionsthat get executed to react to each of these inputelements in the data stream. So when we create adeclarative program, we're creating a specificationfor the desired output. The desired outputcan be the form of a chart that's beingdisplayed but also it can be in the form of user interaction events and how those user interaction eventsshould be handled. So a declarative program generates code and that codecan be in the form of the callback functions that are reactions to individual eventsin that data stream. The callback functionscould be imperative but in this case theyare declarative and the action that occursas a result of each of these user eventsin the data stream, gets implemented through this declarativeprogram callback."
cs-416-dv,10,4,"The next generation of tools for interactive datavisualization and webpages are based on an even more extensivedeclarative programming model. One which focuses onthe specification of the visualization andleads us to consider grammatical modelsfor visualization. So Vega is a visualizationsystem built on top of D3. That was launched in 2013 by Jeffrey Heer whenhe was at trifecta. It's designed to be a grammar for specifying visualization, and it's intended to describeboth the visual appearance of a visualization as wellas its interactive behavior. So Vega describesthe visual appearance and the interaction using adeclarative programming approach, and this is different than an imperativeprogramming approach. An imperativeprogramming approach describes howa result is computed, but a declarativeprogramming approach describes what result to compute and that'sthe programming language figure out how to compute that result. So this is a new approach to programming webvisualization before we had the web server andthe data was in a database, and the HTML document, the DOM, was organized hierarchically and JavaScript would manage that. With D3, that DOM was turnedinto a database itself, and so you merge that downdatabase of web elements with a database of items you want to visualize and that merger in D3, those selections createdthe web document. In Vega, we're taking the program itself and turningthat into a database, into adjacent dataspecification and then the Vega runtime parsesthat specification and determines the best wayto merge the elements on a web page with the data items that it'sintended to display. So we have the difference in approach between D3 and Vega. D3 and Vega are bothdeclarative languages, but D3 is lessdeclarative than Vega is. D3 includes some declarativefunctions so that you don't have to worry about how a function is implemented, you just specifywhat you want to do, and D3 has some methodsthat will compute that result most efficientlybased on the data. Vega expects declarativespecifications for everything and doesn't rely on low-levelimperative programming for any of its operations. So the coding for a D3 programis quite low-level. There are some declarative high-level operationsyou can do in D3, but there's stillsome very low-level operations such as assigningstyles and attributes, and opening SVG areasin an HTML document. In Vega, those are specified at a very high-level and the runtime it manages all thelow-level detail. So D3 is a JavaScript library and you write your code inJavaScript, whereas in Vega, you specify yourvisualization system in JSON and JavaScript is a runtime system thatparses the JSON and executes all the functions necessary to implementthat specification. Finally, event handling. In D3, you have callback codeand often that's quite imperative in natureregistering callback functions. Whereas in Vega, there arenew stream operators that are much more declarativeto manage the interaction. Vega can be used ina variety of settings, there's hooks for Vega in Python and a variety ofother programming languages, plus you can use Vega to add reactive visualizations forinteractive storytelling in a Wikipedia page, such as this one right here. Vega is a powerful languagefor specifying a data visualizationand can be used in a variety of platforms,including Wikipedia. But as Vega simplifiessome elements of visualization, it's programs become quitelengthy and complex, as we'll see when we tryto make a chart with Vega."
cs-416-dv,10,5,"We'll demonstrate the declarativeprogramming of Vega by using it to create a bar chart, the same bar chart we'vebeen using in our other examples. So let's make our familiarbar chart using Vega. And sothis is what a Vega program looks like. The chart itself is implemented as a JSONobject, and that's right here Here, and then this is how that JSON objectis embedded into an HTML file to be displayed as a reactiveweb page using this Vega embed. So the first thing we have to do is wehave to import the Vega library and then call the embed functionin that Vega library, and that places the chart thatwe've generated in a div. And that chart is describedby that Vega specification. And so this is what one of thosecharts specifications looks like. First, we have the schema and that's just some boiler plate code todescribe the database in the JSON file. And we describe things like the width and height of the canvas we'regoing to be drawing to. We have a specification of the data,that we might name the data. And then we specify some values. This looks very similar to how wespecified it before this some scales, axes, and marks. So there's the schema. So as in a D3 program,we have to describe scales. What's the coordinate system for theheight, and what's the coordinate system for the width of these bars and so forexample, the X scale, it's a band type. The range, is the width that we've allready defined, appeared to be 200. And then the domain,is the i field of the lost data. And then the y axes is similarlydescribed as a yscale, that's linear. It's range is height which is up here. And it's domain is the data field,D of our lost database. And once we've described those scales we can use them to create axes'such as this axis right here. And so we'll have on the bottomthe scale corresponding to xscale. Vega will look up xscale. And that specificationautomatically generates this axis. And we do the same thing for the vertical axis as well referring tothe yscale we already defined up here. And then we need to describe the marksthe actual bars of the bar chart. That specification says that we'regoing to use rectangles for the marks. The data from the marks are going tobe from the lost data file. And then we're going to encodethis data into these marks. And instead of specifyingthe attributes here. We'll specify each individualattribute in terms of the scale in the field,including these y values. And then the update section, in case wealready had some elements in the enter section, andwe can also specify some interaction. Such as that we are going to changethe color of the fill value for the element if we hover. And so here on the left Is a HTML webpage that runs the Vega specification for that bar chart. The first thing it does is it loadthe Vega run time and the specification, is here in the variable spec. And it's JSON specification andwe'll use this div here labeled ziz and then we use the Vega embed instruction. The embed method of the Vega object. And if we run it,we get this bar chart and so it's creating a canvass of about 400pixels wide and 200 pixels high. It's creating some data here. The name of the data set is lost. And the values are labeled i asthe tag and then d as the data values. And then we're creating two scales,a horizontal scale that's a band and its using the i value used from the data. And it is creating a y scale that'slinear corresponding to the height and that's using the d values from data. Then it generates axis on the bottom andto the left based on the scales and then it creates these marks thatare rectangular from the data set lost as we specified before. And then when we hover,we change the fill color to orange and as we update we then changefill colored steel blue, so you can see that we're specifying a littlebit of interaction with this Vega file. So Vega's declarative programming makessomething simpler by specifying the chart instead of listing the instructionsof how to construct the chart. But the specification is extensive and requires a working knowledge ofwhat its chart grammar expects. [MUSIC]"
cs-416-dv,10,6,"One of the major advances of Vega was notonly its declarative specification of charts, but also its declarativespecification of user interaction and it's an example of what'scalled reactive programming. And Vega implements something calledevent-driven functionally reactive program, EFRP. And that means that values only change ina reactive program in response to user input or other discrete events. And Vega sets up a system whereevents can be processed as streams. And so the events coming from thecontroller in our model view controller system, instead of being processed ascallbacks, are processed more like streams, like input streams oroutput streams in C++ programming. This is more akin to the dialogue designwe use for user interface design. We think of streams of events as clicks. The system hears that as an event, callssome callback and then responds to it. But the specification of these clicks,Vega provides a new interface for describing these clicks andcharacterizing these clicks and how they trigger those responses. Reactive Vega is reactive because ofthe way it models the user input, and it has a declaration of way of specifyingthe input and then responding to it. So it uses a grammar tospecify the interactivity and this grammar is a way of expressing inputas a stream instead of as simple events. And then it transforms thatstream into a signal, and that signal is what causes othervariables in the system to change and to create the reactionin the visualization. And so we'll use these diagrams andwe could have a sequence of events like a mouse could movefrom here to here to here. The first mouse move is quite grey andit's older in time, and then the more recent mouse moves are more saturatedin color, so they're more recent. Gray means older, as I get older I realizethe older I get, the grayer I get. And so each one of thesemouse moves is an event and the sequence of mousemoves forms a stream. And so we can use an event grammar tospecify what kind of sequence of events can create a trigger, andthose triggers are called signals. The syntax is very similarto what's used in CSS. The words in the event grammar are simpleevent names such as mouse move. There's punctuation used alongwith those event words and then we specify the selector. For example, if we're representing ourdata items using HTML elements rectangles, then we would say rect as forthe rectangle and that mouse move is the eventon that rectangle. And so these mouse moveswould be recognized as rect mouse moves because the mouseis moving over rectangles. And so you can do things likeyou can concatenate two streams. So if I do a mousedown first andthen a mouseup and another mousedown, this is the mousedown stream andthis is a mouseup stream. And the mousedown,mouseup stream is the mixture of those two streams into a single stream based onthe timing of when those events occurred. And so mousedown is a word in our grammar,mouseup is another word in our grammar and then we have punctuation,in this case the comma. And so when we say mousedown,mouseup, we mean all mouse down and mouse up events in orderas a single stream. You can also filter events and filter thembased on attributes about the events. You could look at the y coordinateof the mouse when a click occurs and if that y coordinate islower than 300 pixels from the top of the canvasthen this click would occur. And you can combine multiple filters. So here we have if you're clickingon data and the price that you're clicking on is less than 500, then youcould let this click event go through. And because we've specified both ofthese together, this is an and, so the click only goes through ifthis is true and this is true. And so again, the words here are click andthen we have some filter operations and the punctuation are these brackets,and these brackets filter events so that not all events end up in the stream,only the ones that passed these filters. So here we've got a bunch of click events,click click click click click and here we've clicked where the price was300, we've clicked on a data item where the price was 300 andthe y-coordinate was at 500. 500 is greater than or equal to 300 andthe price is less than 500, so that click goes through. Here we've clicked and the price was 750, the y value the y coordinateof the mouse was 310. 310 is greater than 300 but750 is not less than 500, so that click doesn't go through. Using these filters,we can just capture the events of clicking the mouse on elements with data itemswhose price is less than 500 but on the lower part of the page, lower than300 pixels from the top of the canvas. We can also pre-filter. So recall that these bracketsmean we're filtering events, but we can filter events say we're onlyinterested in mouse move events, we can set a pre-filteron mouse move events. If I put brackets after the mouse move, then it would be filtering based on what'shappening at the time of that mouse move. If I put brackets before the mousemove and then use this greater than sign the stream symbol, this will restrictthe mouse move events I'm getting to only mouse move events that occur after thefirst event and before the second event. The stream that this describes are themouse move events that happen between mouse down and mouse up events. And so if we have a mouse down eventhere and a mouse up event here, then these mouse move eventsbecome part of the stream. But this mouse move event does not,because it happened after a mouse up but not between a mouse down anda mouse up event. You can also use time intervals forfiltering and so more punctuation, we'll use these curly brackets, and sowe'll filter the mouse moves by only mouse moves that happen between three andfive milliseconds apart from each other. So here if we have as time goes on we havea mouse move here at zero milliseconds and another mouse move hereat two milliseconds. And then another mouse movehere at four milliseconds and a mouse up at six milliseconds anda mouse move at eight milliseconds. And so we're only allowingthe mouse moves that happen more than three milliseconds apart andless than five milliseconds apart. So of this mouse move wasn't here,this mouse move wouldn't be here either, because these two mousemove eventsoccurred more than five milliseconds apart from each other. And so we can we can think to ourselves, you can come up with different streamsentences in this stream grammar and try to figure out what event stream ischaracterized by these expressions. So here we have mousedown commamousemove leading to keydown keyup leading to mouse move fivemilliseconds ten milliseconds. So it's a stream of mouse move events. In this language we look to the rightfirst because these are pre filters. And those mouse movesare occurring between five and ten milliseconds from each other andthey're occurring between key down and key up events, that's the prefilter forthese mouse moves. And those key down key up events that needto happen around these mouse move events need to happen between mouse down andmouse move events. And so if we want to use these events totrigger reactions in our reactive Vega visualization specification,then we need to create that reaction. And that's a reaction to a signal andso these signals are triggers and those triggers are pulledwhen a stream occurs that matches the event grammar we've described. And that results in a signal name signalname and these signals are variables and so those variables have value. And so we start with an initial value,and if the signal is triggered, we update the signalvalue with a new value. So we can use this for a tooltip. We can create a signal,the signal will be a tooltip. So we'll name it tooltip andits initial value will be null. If our mouse isn't over any element,then we're not going to show a tooltip. And then we specify the eventsthat will trigger the signal. So the first one is if we havean element we're displaying and then the mouse is over that element, we'regoing to create the tooltip signal and so that means that we're going to updatethe tooltip value to the datum. So if we have a rectangle andthe mouse is over that rectangle, that rectangle has an associated datumthat happens during the selection operation when we pair up HTMLelements with data values. That datum will be associatedwith that element, that symbol. We update the value of our tooltipsignal with this datum value, with this data record. And when the mouse leaves the objectthat's a symbol mouseout, when the mouse leaves the object,then we update this value with null again. So here's that same Vega codethat generates the bar chart, but now we've got tooltips. As I mouse over each of the bars,its color changes to orange from the hover action, but we've also added this tooltipat the top of the bar with the data value. And so that's specified here as a newsignal and that signal's name is tooltip. Its value starts out as null andwhen we mouse over one of these rects, we sense that event with a rectmouse over, and that means we update the value of our tool tip to the datumassociated with that rectangle. And when we mouse out,we sense that with this rect mouse out and we update the tooltip value to null. And then we react tothat signal down here. In our marks section, we have theserectangles, those are the bars, but we also have these texts andthe text are what occur above the bars. And so we in the enter section we definesome values regarding the text and in the updates section, we specifythe X and Y position of the text. The x position comes from x scale and we're sending it the signal tooltip.i,the index value, and X scale's going to return an x coordinatecorresponding to that i value and then that's offset in the band byone-half halfway through the band. Now we refer to this variable as tooltip,and ordinarily tooltip will be null exceptwhen we get a rect mouse over event and then tooltip gets the data value, the datarecord, corresponding to the rectangle. And that data record will have an i value,and it'll correspond to the position on the horizontal axis that the x scalewill convert into an x coordinate. In similarly forthe y value we go through y scale. We send it the tooltip value, the datarecord associated with that rectangle and look at its dvalue, its data value, andthat gives us the vertical value so that these tooltips are placed witha y-value two pixels above the bar, above the top of the bar. And then the text, the value that we passto the tooltip, is just that data value. The reactive Vega event grammar givesus a way to describe user interactions, even when they are in sequences, withouthaving to manage careful state-based transitions between events the wayan imperative language would. [MUSIC]"
cs-416-dv,10,7,"Vega-Lite is a simplified version of Vegathat's less expressive than Vega but easier to use. It simplifies the specification ofan interactive data visualization to the point that we can create a more formalgrammar for describing the charts and the interactions. So Vega is a powerful decorativelanguage for specifying visualizations. And it's very expressive andcan do a lot of things. But it's also very complex and requires a lot of JSON specification foreven simple charts. Vega-Lite greatly simplifies Vega's declarative approach by automatingmany of the specific choices, but that also limits the flexibility andexpressiveness of Vega. Vega-Lite also expands on Vega'sdeclarative approach using the JSON format for specifying a visualization and stillfollows that same grammar based approach. So here's a much simpler Vega-Liteversion of that bar chart that we've been using as an example. And it consists of three basic sections. The data section, the mark, and herethe marks for the chart are simply bar. And then the encoding, that says that thex coordinates are pulled from the field i of the data, and they're justan ordinal type, so a categorical access. And that the y values are fromthe d field of the data, and they're type quantitative, sothat they can use real numbers. And Vega-Lite also further exploresthe interaction and the grammar for interactions. Specifically forthings like user selections. So if user is going to select data,for example draw a rectangle or drag the mouse over some data toselect these three data points. Vega -Lite figures out where thatdata is in the data domain, and so it maps out those threevalues in the data domain. And it does so,using what are called backing points. And so, if we're sweeping across andselecting an interval of points. It would store the endpoints in the data. Specifically, items 3 through items 5, and describe all of these data pointsusing those two limits, as a compact way of representing which data values and datarecords have been selected by the user. So it can select single data points. It can store the min and max pair of datapoints if you're selecting an interval. Or it can also use a what's calleda predicate function that is run on each data point to determine if it's inside oroutside the selection. So here's another specification fora chart in Vega-Lite. In this case, the data we're using is on cars we're going to makea scatter plot using circles and so the x axis of the scatter plot willbe the horse power of the cars. And that's of type quantitative andthe y axis will be miles per gallon. And that's type quantitative. We'll also use color toindicate the country of origin. And that's of type nominal. And then the size will justbe set to a constant of 100. This is the size of a circle. And so the circles will contain 100 pixelsas opposed to specifying their radius. And there's the result. Here we have a scatter plot. We have horsepower horizontally,miles per gallon vertically, and the country of origin. We automatically get a legendbecause that's a type nominal here. And then each of the cars is plottedin the scatter plot using that x, y and color coordinate. So this specification of a scatterplot of car data in Vega-Lite uses a selection andwe can use Vega-Lite's grammar for selections in order to specifyan interval selection by brushing. And so when you run it, we get the scatterplot of cars plotting horsepower and miles per gallon using colorbased on the country of origin. But now we can brush, and soby brushing I can select a point and brush rectangle across the data,and using this condition that, if the selection is not in the brushregion, then the color should be gray. Otherwise, the color is basedon a nominal origin field, basically the country of origin. So in comparison here is a Vega, nota Vega-Lite, but a Vega specification for this scatter plot of car data wherewe're plotting horsepower horizontally, miles per gallon vertically, andthe country of origin in color. This is significantly more complex. But it also allows more flexibility. For example, we get a much more expressiveabilities in describing the marks. And we can also specifythe interaction and you can see some of Vega's interactivestream grammar specifications. Here's a mouse up. Here's some other events. Mouse down on the x axis. Here's a catenation of two mouse,a mouse down and a mouse up. And so the result of that interactivity,is that you can select along the x axis and you can also select bycountry of origin in the legend. As you can see, Vega-Lite is significantlysimpler than full blown Vega but still contains the event grammars andother tools that allow it to support most chart types with verysophisticated interaction techniques. [MUSIC]"
cs-416-dv,10,8,"We looked up the ellipsis system last weekfor the model of narrative visualization based on scenes, annotations,parameters and triggers. This week we have studiedthe declarative grammatical approach to specifying datavisualizations through Vega. Ellipsis also provides such a declarative grammatical approach to specifyingnarrative visualizations. So the same group that gave us declarativelanguages for specifying charts and interactivity in Vega andVega-Lite, develop tools for creating declarative specificationsof narrative visualization. These tools form the Ellipsis package. And Ellipsis is basically a language forspecifying narrative data visualization. And it's based on JavaScript,it's declarative. And, in general, it's a DSL, a domainspecific language which means it's a little language with a few high levelcommands focused on a particular subject. In this case,producing narrative visualization. And you can specify the narrativevisualization either in this Ellipsis language or as a WYSIWYG editor,but we'll focus on the language. So here's an example of an Ellipsisas an API in JavaScript for creating narrative visualizations. You create an object, andthis object is an instance of the Ellipsis vis object because it'sgoing to be a narrative visualization and when they meet budget forecast. So we associate somedata with that object, with that narrative visualization object. And then a stage, and a stage is whatwe're going to render our scenes onto. So we have a render(function() thatwill render scenes onto our stage, and the stage is just the SVG elementthat's going to display the scenes. And then we need to create someparameters, as we learned before, narrative visualizationsare based on parameters. These parameters are calledstate variables in Ellipses. And so we have in our budgetforecast we have the year, and that's the current yearthat is being displayed. All of the forecasts up to that year. And whether to plot the forecasts orthe actual in the plot. And so here's an example. Ellipsis Scene Specification forthis scene right here. And so it would be scene 1,we'll set the year to 2010. And we don't have the littleforecasts porcupining off there. So plotForecasts is set to false and thenwe'll add an annotation here and here. And these annotations followa consistent template in Ellipsis. And so we'll name them onesI highlightedPoint template. And it's basically a little bit of codeagain using the same semantics we're using in D3 and Vega and Vega-Lite in orderto specify highlighted points and the associated text for an annotation. So it lets us create a grammar fornarrative visualizations. These narrative visualizationsconsist of four components, a specification of scenes, annotations,and specifically annotation templates, parameters, the state variables ofthe narrative visualization and triggers first changing state. So the scenes, in each scene we have theparameter settings specific to that scene, that create that scene andinstances of any annotations. As well as any UI controls, forexample to move to the next scene. The annotation templates, each havean enter, transition and an exit function. Enter is creating new elements. And exit is removingelements in an annotation. And the transition is updating elementsin case the annotation Is being animated in the scene or updatinga previous annotation in a previous scene. The parameters the state variables ofthe narrative visualization can be constants or they can be variables and change as the narrative visualizationis evolving through its story. They can also be functions, for example,transformations from coordinate systems. And finally, the triggers, andthe triggers can be keyed on a parameter. So if a parameter changes values, somepredicate function will sense that and will fire a trigger,setting a new parameter value. Triggers can be timers. And a certain duration aftera new scene has appeared, an event could happen orcould trigger a state change. And finally, there are event triggers. And these are state changes,or parameter changes, that can happen based on user input. The Ellipsis Grammar ofNarrative Visualizations is based on its model ofnarrative visualization. But defines these elements in more detailand implements them as a domain specific language for the declarative specificationof narrative visualizations. The ellipsis tools are not yetready for release. But all of these systemsare developed on GitHub and you know enough about them now toparticipate in their development and be part of the next generation of narrativedata visualization tools on the web. [MUSIC] [SOUND]"
stat-420,1,1,"Hello and welcome to STAT 420, statistical modeling in R.My name is David Unger, and I'll be your instructorfor this course. I am a senior instructor in the Department of Statistics at the University of Illinoisat Urbana-Champaign, where I teach courses on predictive modelingdata management and analysis, andapplied regression. Broadly speaking, this course will havetwo primary focuses. One, the basic theoryof linear regression, and two, the practice of applying linear modelsto analyze data. We'll actually begin by first learning practical skillsneeded to analyze data, specifically the open sourceR programming language. While R started asa language written by statisticiansfor statisticians, it has grown to be one ofthe most popular tools for computational statisticsand visualization in the data science world. Companies like Facebook,Google, Microsoft, and Twitter, not only use R, but also contribute to the Ropen source projects. Since R was developedwith data in mind, right out of the box, it is ready to performa large number of analyses. However, in addition toour standard capabilities, the community of R usershave provided a number of enhancements mainlythrough the use of R's packaging system. We'll make heavy use oftwo of these enhancements, RStudio and R Markdown, to assist in performingdata analysis. RStudio is an integrateddevelopment environment for interacting with R thathas become extremely popular. While there are many waysto interact with R, Rstudio has some of the best support andinteractive features. In particular, RStudio contains built-in support forworking with R Markdown. R Markdown is an R package for the creation ofdynamic documents. Essentially, it combines both the R statisticalprogram and Markdown, a simple markup languageused to enhance output. R Markdown allows RStudio to seamlessly integrateboth our code for analysis as well as our practical interpretationsof that analysis. Keeping these twotogether allows us to communicate results ina reproducible manner. When introducing R, this course will start fromthe very beginning. We won't assumeany previous R knowledge, but we will assumeyou are familiar with basic programming concepts. R, RStudio, and RMarkdown will be the first conceptscovered in detail. Then as we movethrough regression, we'll continue to useand learn R concepts. While R will beour practical tool, linear regression willbe our theoretical tool. Linear models are by far the most widely usedstatistical models, and they're used innearly all application areas. Linear models are often simple, but also very flexiblewhich makes them extremely useful for making predictions and explaining relationships. For example, which variables are significant predictors forthe success of a restaurant? What factors make fora fuel efficient car? Can you makeaccurate predictions of the opening weekend grossof a new film? In each case, a linear modelmay perform very well. We will discuss howthese models are fit to data, and R will be our tool for carrying out thenecessary computations. Obtaining these regressionresults is a useful skill, but not without the abilityto interpret the results. We will carefully examine the various results ofthese models being sure to consider themin the context of the real-world problemsthey aim to solve. We will pay particularattention to the statistical propertiesof these models. In order to account for variability in fittingthese models to data, we'll create confidence intervals and perform hypothesis tests. To do so, we will need to utilize several results frombasic probability. Often, instead of directlyderiving a result, we will performsimulation studies in order to justify the results. It turns out that R isa perfect tool for that job. Before beginning,it is important to acknowledge ProfessorDavid Dubois , a colleague of mine atthe University of Illinois and a collaborator onthe development of this course. He is the author ofthe online textbook we will use, the person featured onmany of the videos and the primary architect of many of the recent enhancements tothis long standing course. As a statistician,linear regression and R are probably two ofmy most commonly used tools. After completing this course, you will havethe ability to use both. So let's go aheadand get started."
stat-420,1,2,"[MUSIC] For our first lesson, we'll dive rightinto using the R programming language. Throughout this course,R will be our tool of choice for working with data and models. In this lesson, we'll start from the verybasics of running a simple command and interacting with the R languageusing the RStudio IDE. An integrated development environmentallowing users greater ease for interacting with R. We'll then build up to somebasic programming concepts. In particular, how the use of vectorsis fundamental to the R experience. After completing this lesson,you'll be on your way to mastering one of the most common tools fordata analysis in use today. Don't worry if you don't understandevery detail of R right away. This lesson, as well as the next few,will serve as a whirlwind tour of R. We'll continue to highlight the importantbits throughout the entire course. Personally, I'm still learningnew things about R all the time, including from my students. So let's get you started withyour R journey right now. [MUSIC]"
stat-420,1,3,"[MUSIC] So to begin talking about probability andstatistics with R, we'll talk about R. R being a programming language developedspecifically for statistical analysis. So here I have the R GUI. This is a piece of softwarethat is distributed with the R programming language on both Mac andWindows. So here we see the Console here. So alternatively,you could open up a terminal, type R and you would be insidean interactive R environment. R being an interpreted language, so I candirectly run commands in the console and get an immediate result. One thing I notice here, forthis video, I'm on R version 3.3.2. We'll always try to stayon the most recent version, which will have some sort of clever name,like Sincere Pumpkin Patch, in this case. But for what we see here,this will be the last time we see it. The R GUI is an older way of using R,it's perfectly valid. These days, most people use RStudio. At first glance, this looksrather similar to the R GUI, but RStudio is a much more feature-richIDE for working with R. Some of those features will be ratherinvaluable to what we try to do here. So notice, we have the console here,which was more or less exactly what we saw in the R GUI. But then we have someadditional panels here. I'm actually going to open up one more. So this will be what RStudio looks likemost of the time we interact with it, we'll have these four panels here. So the console we've already seen, where we can throw commands at it andit'll give us a result. This script file up here, we could alsotype commands here and then run them, which sends them to the console andruns them. We also have two additional panels,which at this point are basically empty. So our Environment panel, we havenothing stored, so there's nothing here. A history,we can see a history of what we've done. And then we have another panel herewhich we can display files, plots, which we don't have any yet. Packages, which we'll use a lot of. Help, where we'll find a lotof additional information. And Viewer, which is blank now,but we will learn what this is for at a later time. Before we go further, obviously you'llwant to have both R and RStudio installed. R can be found at r-project, and in particular crayon is wherethe releases are stored. You'll find a mirror host soyou can download R, and RStudio can be found at rstudio.com. You want to make sureto have both installed. You need RStudio to interact with R. RStudio cannot run by itself. [MUSIC]"
stat-420,1,4,"[MUSIC] Okay so now that we have those R and R Studio installed we can beginto discuss the basis of R. First you might notice that I have loadedhere a quick little R script that I wrote. R scripts end in .rfiles. And we can also see that it now exists,I have it saved in my home folder and we can see it over herein the Files panel. So, often we rarely type commandsdirectly into the console. Most of the time we'll be typing them intoa script, or some other file that will keep a record of what we're doing andso we can reproduce it later. So if we want to be able to runfrom a script, to the console, so we can use R interactively. Most of the time we use R,we'll be using it and wanting to see an immediateresult as part of an analysis. To do this we can clickthis Run button here. You can see though as I mouse over it,it tells me that the keyboard shortcut for running an individualline is Ctrl + Enter, which I will use here insteadof clicking the Run button. So running those four lines we see basicmathematical operations, in particular here this division we see that by defaultR is using double precision numbers. So we can also do things like exponents. We see here we can use parenthesesto induce an order of operations. Note here though this is essentiallythe square root of 2 for which we actually see there is a functionwhich gives us the same results. We can access certainmathematical constants like pi. E will not give us anything however the function E to the 1will give us the value E. So if I take log of 1, that gives me 0. But at this point,it's probably ambiguous what log is doing. So any time we want to know moreinformation about something in R, be it a function or another object, you simplytype question mark and then that function. And what will happen is over here, we willget documentation about that function. So here we see that log by defaultcomputes the natural algorithm and then we can verify this ourselvesby taking a log of E and it is 1. Over here in the help window, we seea quick description, some use cases, details of your arguments,the values that are output. We'll discuss functions in more detail,but know that any function in R will have documentation, and then alsothere's usually examples at the end. A quick note about assignment in R, soI can assign to a variable a the value 5, so notice it's not affected in anotherpanel, which is the environment panel. So it's telling us now thatthe variable a has a value of 5 in our global environment. So we can operate on a stored variable. We can store another variable. And we see that's also output here. We could add variable scatter. We could drive a new variablefrom the original two. A quick aside here that in R, there's sort of a long historyof the assignment operator. You'll see that I use equals sign, which puts me in somewhatthe minority of R users. Historically in R,assignment would look like this. So let's modify this. And there's actually an interestingreason for this, and a long history of why this is. I feel that it's more important tobe consistent first with yourself, then with any othersthat you're working with. So just know most of the time I will beusing the equal sign but code you might see on the internet or elsewhere mightbe using this assignment operator. It's a choice you make for yourself. There are some very, very, very, very fringe cases where itmakes a slight difference. But for our purposes it will beokay to use equals for assignment. And we'll talk about this morewhen we discuss functions and the difference between assignment andargument passing. One of the strengths of R is that anytime you want to do something in R, there's a good chancesomeone's already done it. And instead of writing code yourself,you can just their code. To do so,we would have to use the packaging system. So often,we will want to install packages. In this case,I installed a package called mosiacData, and we should be able to seethat in my user library here. So now I see that the Mosaicdata package is installed. So I could use this operatorhere to access things inside of the package without loading it. So I'm actually going to delete this. And, so one thing you'll note that Rstudio very often will try to auto complete things. So these are all objects that liveinside of the Mosaic Data Package, particular I was interestedin this Galtan data set here. So if I run this, this is a data setwhich we'll talk about in the future. But so this was something thatwas previously not in R but now I have it in R. Often in R we will load the entire packageinto our current working environment. So here I use a library command, we seehere over back in the packages tab that Mosaic Data Package has been loaded andnow instead of using this syntax, I can simply directlyaccess the Gaussian data. Along the way here,I've been using a few keyboard shortcuts, most notably the Run Linekeyboard shortcut. Another useful one is to ableto clear the console here, which is Ctrl + L on Windows. But in general, there's a large, large,large amount of keyboard shortcuts in R. I will try to highlight some ofthe more useful ones along the way. But if you're interested,R Studio has developed a number of cheat sheets here which willoutline generally working with R and highlight some of the certain workflowsand keyboard shortcuts along the way. Also there's this keyboard shortcut herewhich will pull up a big list of all of the possible keyboard shortcuts. Obviously, at first, this is overwhelming. So a number of these,I will highlight along the way. We might come back and reference this to make sure we're usingsome of the better work flows in R Studio. [MUSIC]"
stat-420,1,5,"[MUSIC] Okay, sojust a quick note on assignment in R. Why do we have two ways of doing this? So first for the vast vast vast majority of casesthey will do the same exact thing. For example, both these lines of codeassign the value ten to the variable x and anything we do it willnot make a difference. There are some very, very, very, verysmall edge cases where this would matter, but for the vast, vast, vast majorityof things it won't make a difference. But R users have this preference forthis two character assignment operator. Why is this? The main reason is history. So if we invented a time machine andwe went back to the year 2000 and we tried to use equal as the assignmentoperator it just wouldn't work. Until 2001, it wasn't available. We had to use this weird twocharacter assignment operator. Another reason we mightchoose to use that, is because our keyboardallows us to type that. Back when S was developedwhich was the precursor to R, this was something that somepeople had on their keyboards, and it's sort of a relic fromthe APL language in the 60s. So it's just this historicalthing that got passed down. We no longer have thiskey on our keyboards, but back when the beginnings of Rwere being written people did. So nowadays, in our studio, there'sa keyboard shortcut that does this, which on Windows happensto be alt minus sign. Those two are the sort of the bigreasons why we still use this. Now, it has a couple advantages. One is that it keeps assignment andargument passing separate. Argument passing will be donewith an equals sign, but assignment will be done with this goofytwo character assignment operator. Another reason you might want to use thisis just because it's considered more idiomatic R. It's what most R users do. Maybe you're coming from another languageand you really just don't like it. You hate the way it looks, you don'twant to have to type two keys to make an assignment, and every otherlanguage you use uses an equals sign. And if that's the case,you can use equals for assignment. We should not run into a casewhere it will make a difference. If you're interested you should researchthose cases if you want to use equals, but they're very very very few andfar between. So what it comes down to is you deciding. What's your background? Are you a statistician who's goingto be working at R very often or working with other R users? You should probably usethe R assignment operator. Are you someone who's just using R fora single project and you use equals all the time foreverything else? Maybe use equals. Consider your audience as well. If you're going to be writing code that'ssubmitted to CRAN where R code is kept, or maybe interacting with other R users, you probably want to considerthe R assignment operator. So these are just things youneed to be aware of, but the most important thingis to be consistent. Pick an assignment operator anduse it for everything. [MUSIC]"
stat-420,1,6,"[MUSIC] We'll quickly talk about three data typeswhich are numeric, logical and character. Numeric actually has a number ofpossibilities in particular double and integer. Here we'll see an example of a double, 42.5, and we can verify thatit's both numeric and a double. R has a lot of these functionswhich are is.something and it checks if the input is that thing,which we see here. Another data type is logical, with valuestrue and also false, we can verify that true is indeed a logical value, it happensto return a logical value to TRUE and FALSE come up so often that R hasshortcuts for them, in particular T and F. But you should never use these forthe following reason. I could try to do something evil likeset the value of TRUE to be FALSE, but R won't let me do this. What R will let me do is setthe value of T to be FALSE. So, if you try to use T as a shortcut for true you'd actually be getting FALSEwhich will cause some problems. The other data type that we'll seepretty frequently is characters, so here's an example, statistics byputting in quotes it makes it a character. If I simply ran statistics withoutthe quotes I would say, well, I haven't found this because we didn'tdefine a variable statistics and we can just quickly verify that witha quote it is indeed a type character. Often we want to work with groups ofdata types together, so data structures, the most common data structure beinga vector, R uses vectors quite often. A vector is a one dimensionalhomogenous data structure. So, let's use an example of that. First, I'm going to just run two andit returns two, but I'm going to note that there's this onehere and we're going to come back to that. Again, a vector is one dimensional andhomogenous, so we want to combine together a bunchof elements of the same type. So, that's what we're going to do here. And we have all these numeric values hereto combine them into a vector and I do so with the c function, which stands forcombine, so I group them together and now this is a vector,which I will also store. But also note that again,this one up here, its actually appeared a few other times sofar. Let's figure out what that's doing. To do that, we're going to run this line,which introduces a number of new things. So first, over here, if I run just thispart, it gives me the same thing, so this is creating a vector from 1 to100 and everything in between, so 1, 2, 3, 4, 5, and so on. This is a new operator we have now,it's the colon operator. I would also note that most of the timewhen we're using a binary operator like equals, plus, minus,we put spaces around it. The colon operator is one ofthe minor exceptions to this, where the arguments are directlyadjacent to the operator itself. And so, this line is storing this in y, which normally wouldn't return anything,but because we put parentheses around it, it actually also outputsto the console as well. And instead just this one beingoutput we have a number of other numbers being the output,what does this mean? Essentially it's just keeping trackof where we're inside of that. For example, this 24 says thatthe first element of this row is the 24th element of y. These happen to be the same numberbecause these are all the numbers between 1 and 100. Similarly, the first element of thisrow of output is the 47th element of y. Returning to this example of two. Well, this says that the first element of this row is the firstelement of this vector. So, that tells us that two is actuallya vector and there's actually no scale or data type in R,there's simply vectors of length one. We mentioned that vectorsshould be homogenous, so what happens if we try to combinedata types that are different? Here we had a numeric, a character and a logical,they all end up becoming characters. Similarly here we a numeric anda logical, but they all become numeric. Because vectors must be homogenoussome type coercion was happening here. There's specific rules forhow this happens, but we'll save that for reference material. So far, we've seen mostly numeric factorsthat happen to see this one character vector here, so just we couldalso create a logical vector Z. A couple other ways we're creatingvectors are the sequence function and the repetition function,or repeat function. The sequence function creates a vector,starting from a particular value, ending at a particular value and everythingbetween according to some number here, we'll see numbers that increaseby 0.1 from 1.5 to 4.2. And we can do this withoutspecifying the argument names. Here's a sequence from 129 by 2. This is pretty close to our X vector,but I think X also has eight. Similar is repeat, sothis will say repeat A ten times. So, this is a vector thatcontains the value A ten times. Instead of giving singleelement to this function, we can also just give it a vector, so thiswould repeat the vector x three times. Here we see x repeated three times. Those are a number of waysof simply creating a vector. And now that we have vectors created,we'd like to be able to subset them. The most important thing to rememberhere is that R has index starting at 1, not at 0. Just a refresher,x is this vector here, so here we will extract the first elementof x, using the square bracket notation. Here, we'll extract a third element. Here, we'll extract everythingexcept the second element, we're leaving out the second elementof the vector with the minus syntax. Here, what we're doing is essentially, instead of you giving the square bracketsa single number, we'll give them a Vector. So, in this case it's the vector 1, 2, 3. So, this would extract the firstthrough third elements of x. We happened to give ita sequence of numbers here, we can give it any arbitrary vector. So, here we'll extract the first,third, and fourth number from x. These are essentially, because again wesaid a single number is just a vector, these examples here are allusing a vector to subset x. We could also use a logical to subset x. So, remember z wasTRUE TRUE FALSE TRUE TRUE FALSE, so if I do x subset by z,I will get the first, second, not the third, the fourth, the fifth andnot the sixth element of x return. This will come in very handy later. Sort of throwing everything all together,we can mix and match using C for combine, repeat sequence and sub setting. So, here I create sort of a vectorusing many of the things we've seen and then, I use a square bracketsyntax to subset it. So, essentially I'm going totake this vector here and subset according to the even indices. [MUSIC]"
stat-420,1,7,"[MUSIC] Using logical operators we can actuallydo some more interesting sub setting. We'll return to ourusual vector acts here. And now quickly introducelogical operators. Here we see we can compareacts to a certain value or turn to a access greater in that value. We can return when accessless in a particular value. We can return when x isequal to a particular value. And we can return when x is notequal to a particular value. We could throw some of these together. For example using the and,and or operators. So here we're saying x is equal tothree and x is not equal to three. So I think that should return false for everything because you can't be bothequal to and not equal to three. Similarly here it's x is equal to three orx is not equal to three and that should basically always be true. We saw that we can subsetusing a logical vector. So we see here for example x greater than three is actuallyreturning a vector of true and false. So I can take that thento subset x originally. So this says, take x andsubset only where x is greater than three. And what we see is exactly the valuesof x that are greater than three. We could instead say give me the valuesof x where x is not equal to the three. And we see again we have here returned thevalues of x that are not equal to three. We can do some moreinteresting things with this. Again x greater than three returnsa vector value of true false. So what happens if we sum those up? We get the value four, which is essentially counting howoften this expression is true. So it's true exactly four times in x and those corresponds tothese true values here. So what's actually happening is there'ssome coercion happening to numeric from logical. So I could explicitly do thatcoercion by using as numeric. So we know that this is logical value but if we make it as numeric we see that falseis becoming zero and true is becoming one. So that's why when we sum thisfactor since we can't sum up through a false values at first coerces themto be zero one, then sums them up. Another we can do is extractthe indices where something is true. So using this which function wesee here that this is not true. This is not true. These are true. So we want the indices here,which are three, four, five, six. Which is what we see here. And we could again use that to subset, since we know we can subsetby a vector of indices. This is a little redundant because we'veseen that this does the same thing a little easier. We could also extractthe maximum of a vector. And we could combine that with the whichnotation and the logical operator equal. So I want to know the indices ofthe vector x where it's equal to it's max value which happens to be sixthe last value of that vector Or the last indices of that vector. But this is sort of a veryodd thing to write so we see a function which.maxwhich does exactly that. It says the max, which in this caseis nine is the sixth element of that vector andthere are similar functions for minimum. [MUSIC]"
stat-420,1,8,"[MUSIC] We'll return to some familiarvectors here x and y. And now,we'll perform some operations on x. So here, we say x + 2. And what does that do? It adds 2 to each element of x. We'll take the log(x),which takes the log of each element of x. We'll take the sqrt(x), which takesthe square root of each element of x. And we'll square x,which squares to each element of x. This is an idea of vectorized operations. So many, many,many operations are vectorized. These are just a few. But often, you can perform an operationdirectly out of vector and it'll perform that on eachelement of the vector. We can conceptualize this asadding to each element of x. That's not strictly what's happening here. What's actually happening here is R issaying well, x is a vector of length 6. 2 is a vector of length 1. This is shorter than x. And what R wants to do is elementby element-wise comparison. So what it does, to perform thisoperation, is it repeats this vector until it's the same length as this vector,which is essentially what I'm doing here. So here, x is a vector of length 6. Repeating 2,6 times is a vector of length 6. And when I add those two together, I'm adding together this element plusessome element to get this element. This element pluses element to getthis element and so on and so forth. So that's really what's happening. And that's what was happeningwith logical operations. So here, it sort of seems like we'recomparing each element to the value of 3. But what's really happening is we'recomparing the vector x to the vector 3, repeated 6 times and then elementby element comparing the values. What, then, happens if we compare oroperate on x and y together? Something happens and there's no error. But there is a warning, because x andy have very different lengths. So we can use the length functionto obtain those lengths if we don't remember them. x is of length 6, y is of length 100. And as this warning messageis sort of saying, well, 6 does not divide 100 evenly. So what's happening here is R isattempting to repeat x to match the length of y butit doesn't do so in exact way. So this code here,think about what this does. What we see here is, actually, what is truly beingadded to y in this line of code. So here, we see x. Here, we see x repeated,up to here that is. x repeated again and so on and so forth up until the end where most of xis repeated, but not the whole thing. And R is, essentially,just alerting us to that fact. So if I change the length(y), sonow instead of being 100 being 60, I can perform x + y with no warning. Now again, x and y have differentlengths than each other but the length(x) / length(y). So this is no problem. So, essentially, y is of length 60. If I repeat x 10 times,it's also of length 60. So I can perform this addition. We can verify that x + y isactually doing repeating x 10 times + y using this all function here. So inside here, again, logical operator. So it's looking at this vector comparingto this vector element by element and we see that all those are all TRUE. And then the all function simply says,well, are all of these TRUE? And just verifies that for us. A shortcut to that would beusing the identical function, which says is this, here,identical to this, here? We can look at these two things andsee that they truly are. But obviously, [LAUGH] we can't always sithere and compare element by element for hours on hours. So we run this function and we see that, yes, those two thingsare actually identical. There's a number of functions like this,all is complimented by any. So instead of are all of the values true,are any of the values true? all.equal is similar to identical, but allows fora tiny little bit of machine error due to numeric computation issues. [MUSIC]"
stat-420,1,9,"[MUSIC] So one of the usual, most basicprogramming constructs is the if and else statement. We see an example of that here in R. So I'm going to initialize a couple valuesand then we say if x is greater than y, we'll run this code,else We will run this code here, so when I run that I would see herey is indeed greater than 1. So, the else should run here andwe run this. We see that it says x is less than or equal to y which isexactly what we thought. We also see that z now has a value stored by doing this computationinstead of this computation. This is useful for programming, but often in this if else statement willbe something that's even more useful. So what's happening here? We say if else some condition,which in this case is true, so these remaining two arguments say, so if this is true, return this value,if this is false, return this value. So we run this, we get the value once. So that doesn't seem very useful initself, but if you remember that are has many factorized operations,it becomes extremely useful. So here I quickly store some ofthe numbers from the Fibonacci sequence in fib, andnow I can compare each of them to six. And if it's greater thansix I will return true, if it's less than six I will return bar. And that's what we see here. So if else is this sortof very nice vectorized if else construct where we canmake a comparison to a vector and return one of two valuesdepending upon the comparison. So like any language R has four loops. So here what I'm going to do isinitialize a factor x between 11 and 15, I will loop over the values 1 to 15 andthe each time I will change a particular element ofx to be double of what it was previously. So if I run this loop and then return x we see that the originalx values have been doubled. So this is just a quick example of the for loop syntax but this is actually not a forloop that will ever run. That is not to say that forloops are bad but this is just a for loop you should not do because R hasa much faster way of doing this which is Initializing the vector rememberingthat vectorized operations exist and then returning the value. So these two are the same things but we'll never actually writethis particular for loop. We've been sort of using functions so we haven't talked much about themmostly to say how arguments work in R. For example this sequence functionhas three arguments from to and by and they all, more orless, need to be specified. So we run this, we get a result. We can run this withoutthese named arguments and R will essentially match themappropriately, in this case in order. There is some very specificrules to how this works for if you have some named arguments and some unnamed arguments, again we'llleave that for reference material. But for the most part,it's a good idea to name many of your arguments unless it's very,very clear what's happening. There's also the ideaof default arguments. So if I run just sequence, something happens even though I haven'tgiven any arguments, and why is that? Well, if we look at the documentationon the sequence function, we see that many of the argumentshave default values here. So the default value from is one,that's why we started here. The two value by default is one whichis very ended here and by default we sort make even steps throughout thesequence and that's what we've done here. We've been using functions,we just haven't talked about them, we're used sequence, repeat,square root log, but it's more interesting towrite our own functions. Here's a generic syntaxto writing a function. We have the name of the function isequal to we use than used function, we then in parenthesis herespecified the arguments. So here we have three arguments, argumentone, argument two, and argument three. And I'm specifying a default forargument 3. So once we have our arguments set up we use an opening curly brace,we end with a curly brace over here and everything inside of here isthe body of the function. And what's happening here? So importantly there is a differentbetween the last line and everything else. This last line is what will be returned unless you are to store thislike this, we'll come back to that. So by virtue of this being essentiallyoutput, this would be what is returned. So, we don't need a return statement,and that will simply return. So, this is just forforming a few operations here. And then we're returning of factor that'sbased on some of these values here. To really use this function,we first need to run it. Now that we have run it, we can callit in number of different ways. So here I am specifyingall of the arguments. Here, I leave out argument three and it still runs because I hadgiven it a default value of 42. So we see that byspecifying the value 42 for argument three we get the sameoutput here and here. And again we could run this withoutspecifying the argument names and it'll use these in the order thatthe arguments were initially specified. A minor note,like any programming languages, there's certain styles thatare more accepted than others in R. So for example,everything here is very bad style. And I want to correct it very quickly. So the issue with this line here,is that I would much prefer to have spaces after commas andaround binary operators. In some languages they would preferno space around a binary operator and a function call. In R, it's idiomatic to put spaces there,because serve a lot of work to go and manually fix something like that. So thankfully,RStudio has a nice way to do this. If I go to Code,let's see if I can find it here. Yes, Reformat Code. It reformats code automatically. So here's another example of that,except now instead of clicking this, I will use the Ctrl+Shift+Akeyboard shortcut. And now I go from ugly code,to stylistic R code. [MUSIC]."
stat-420,1,10,"[MUSIC] Now that we can use R at a basic level, it's time to startinteracting with some data. In this lesson,we'l discuss data types and how they are used in the data structuresof the R programming language. After completing this lesson, you’llunderstand the ways data can be organized in R, and how to both create andimport data. [MUSIC]"
stat-420,1,11,"[MUSIC] So if our overall goal is to analyze data, we should probably have somedefinitions about data. In particular, we want to define a datamatrix, which is a collection of individuals and measurements anddescriptions about these individuals. Each individual will be a row in thismatrix, and each possible measurement or description will be called a variable,and be arranged in the columns. Oftentimes, people will referto this as a data set, but sometimes data set can bemore broad than something that's specifically arrangedin a data matrix like so. So here we have a famous data set fromFrancis Galton about heights of children and their parents. So the blue row highlightedhere is one child. This child happens to be female,69 inches tall, and parents, a mother 67 inches tall, anda father 78.5 inches tall. Interestingly, at least the subset of thedata we see here, everyone's pretty tall. But so the columns are the variousdescriptions of this particular child. So it's from the first family, sothe family variable here indicates which family, sothese four children are all related. Father describes the height of thatchild's father, mother describes the height of that child's mother,sex describes the sex of the child. The height is forthe height of that child, and the number of kids is the numberof kids in the family. So we see four individuals forfamily one, so we see 4 over here. So a row is an individual andthe various data points here are from particular variables that describethat individual in a particular way. So we see that there's a numberof different data types here. We have some numeric values here,we have male and female for sex, seemingly, number of kidswould be integer valued. Family is represented by integers but probably these aren't meantto have numeric value. So we need to have some definitions ofthe type of variables that we have here. So variables can be broadly broken downin to two categories, numerical or categorical. People would often define theseas quantitative and qualitative. Oftentimes people will also findthe word numerical a little funny and just call it numeric. So numeric variables are numbers. And we can have two examples of those,continuous and discrete. Most, most, most often we'll imaginethat variables are continuous, more on that in a second. Non-numeric variables, categorical variables essentiallyplace an individual into a category. And there's two types of these,be it nominal or ordinal. So sometimes categories have a specificordering, sometimes they do not. A few examples. So age if we measure it inyears is numeric and discrete. So if we actually measure the age ofsomeone's life, we could do it down to, say, seconds or nanoseconds andthat would be a continuous variable. But because here we're only allowing forinteger values, this would be a discrete set of values. Letter grade would be an exampleof something that's categorical. So a letter grade splits studentsinto different categories, but they have a defined order. A is better than B,is better than C, and so on. HDL here, a measure of cholesterol,this would be a numeric variable. And it's continuous. So here we're rounding tothe first decimal place, but anytime a variable couldbe essentially any number, provided that you had a measurement toolthat could measure as precise as possible. That would be considered continuous. So the only reason we don't have moreprecise measurements here is because we're limited in how we measure HDL. Otherwise, we could have any real numberbetween whatever the reasonable numbers for HDL are. A genotype would be an example ofsomething that's categorical and nominal. So there's no ordering of these genotypes,but they are different possible genotypesany given person could have. So those are some data types, a naturalquestion that we're going to answer now but discuss much more later iswhere does this data come from? So generally, the way we'll think about this is there'sa population that we're interested in. And a population contains every possibleindividual that we could be interested in. An individual being a person or a place ora moment in time, any of those things that we could be interested inwould be considered the population. And because it's also often notpossible for monetary or time reasons, we can't look at everysingle one of those things. So instead of doing that,we'll take a sample from the population, preferably a random sample. And we'll talk a lotmore about that later. And this sample could besimply observational, or maybe we're experimenting out ofthe population, which would be nice, because we prefer experiments. Much more on that later as well. But that sample then will representthe data that we will usually analyze. This isn't the only way to obtain data,but this will be our general framework for thinking about data. So just recapping here with the Galtondata set again, so here we see that father, mother, and height are allnumeric continuous variables. Sex would be a categorical variable,it would be nominal, there's no ordering to the genders here. Number of kids would bea discrete variable. It's numeric, but we don't believe thatpeople could have fractional children. You could either have one,two, three, or more children. Family is a bit of an oddball variable,it would actually be categorical, and in particular nominal. So these numbers here, they're justsort of an index for the families but they are not actually rankingthe families in any way. [MUSIC]"
stat-420,1,12,"So now, we move from a one dimensional vector to a two dimensional matrices. So again, recall that vectors are one dimensional and homogeneous. So we see only numeric values here. So similarly, a matrix will be two dimensional, but again it will have homogeneous elements of the same data type. So, to create a matrix we'll use the matrix function. We will specify the vector that we like to turn into a matrix and we'll say how many rows and how many columns we would like. So if I run this, I'll put it. We see that we have moved from a one dimensional vector to a two dimensional matrices. Also, note that I have little ""x"" and ""X"" and these are both valid variable names and they're different. So just to note that ""R"" is indeed, case sensitive. You can see that this vector here, it's sort of snaked down through the columns here. Sometimes we all prefer to think of it as going through the rows, so we can use this by row equals true argument and now ""Y"" will be similar to ""X"" but instead of going down the columns, the vector is placed across the rows. So we can see that ""Y"" here is simply the transposed of ""X"". Another way to create a matrix is instead of giving the matrix function a vector, we can give it a single value. So we say create a matrix of zeros with two rows and four columns which we see here. OK! So like a vector, we can subset a matrix. So, for clear council and start with the X matrix here. So I could directly access certain elements. So, for example, using the square bracket operators. This is the row, this is the column. So we have row-column-column. So in this particular case, we're going to take the second column of the first row which should give us four. If we leave the column index blink, we'll get in this case, the first row and if we leave the row index blank, we'll get the second column. So you see ""four-five-six"", ""four-five-six"". We could combine these. So here, we want the second row and the first and third column of that row which are the elements ""2"" and ""8"". So, we could also create matrices, a couple of other ways using this ""cbind"" for column bind or ""rbind"" for row bind. So here, I have three vectors: X, reverse X, and repeat one, nine times. And if I column bind to them, it will use those four columns of a matrix. So here we end up with X as our first column, reverse X is our second column, and one repeated nine times as our third column. And the way I passed this to the C bind function, I also happened to give names to the columns. Had I not had these argument names here, it would have been the same matrix but they would have had no names except for ""x"" which oppose from the vector X. We can perform matrix operations. So again, I'm gonna take my vectors X and Y here and if I do X plus Y. What that does is element by element addition. So for example, this six is a result of four plus two. I could do subtraction, so this two results from four minus two. Now, where this gets a little tricky is I'm multiplying X by Y but this is not matrix multiplication. This is element by element multiplication. Similar for X divided by Y. This is X, element ""Y"" is dividing by elements of Y. To perform matrix operation, I need this operator here which is percent sign, the usual multiplication sign, percent sign again. So this is matrix multiplying X by Y. So, when the time I made this multiplication, the next obvious thing to talk about is the inverse of a matrix. So I'm gonna create a new matrix Z, which looks like this. And to get its inverse, I need to use the solve function. So solving that gives me the inverse of Z. We can verify this. So if I take the inverse of the matrix and matrix multiply by itself, I should get the identity matrix. This at first glance is not the identity matrix which would look like this. We can use the diagonal function to get us a diagonal matrix that is three by three. But in reality, we will consider this the same thing. So I'm gonna use the all equals function which we'll say is this, here. So the inverse times itself or the inverse times the original matrix. The same as the identity matrix. But, it will allow for tiny errors due to numerical issues, and this says that they are equal. So we can see here that this is essentially one and this is very, very, very, very close to zero. So that's what we have here. Also note that X does not have an inverse as it is singular. Just a few more things about matrices. So again, let's return to our X matrix here which is three by three and there's some quick summary information we can get it off his matrix. We can get its dimension which is three by three or if we wanted those information separately we could take the rows which are three or the columns which are three. We could also, some of the rows. Same thing for columns or we could take the means of each column. And we could do the same for rows."
stat-420,1,13,"[MUSIC] So like a vector, a list is a onedimensional data structure. Unlike a vector which must have allelements of the same data type, a list can contain differentdata types as its element. In particular, here we have a list madewith the list function of a numeric value, a character value and a logical value. And we see them outputteddown here in the console. The double bracket one here indicatesthat this is the first element of a list. The second element of a list,and the third element of a list. And r is again outputtingthe single square bracket one indicating that this is actually a vector. So the first element of this list isa vector of length one which starts in numeric value 42. The second element of the listis also a vector of this time, a character value andthis last vector here is logical. To further expand upon listwe'll make a larger list and we've done a few things differently here. So first we stored the list,second when creating the list, we gave it vectors another vector. We also gave it a function anda matrix and we stored them in different, in this case, letters. So if I return this list we see thatnow instead of having element numbers, this is so the first element. But now also it has a name which is a and it's a vector of numericvalues of length four. We see b is also a vector, we see c is also a vector similarto what we saw with four. Now d, the fourth element, is a functionand e, the fifth element, is a matrix. So we also see that now it'sstored we can see that the list is contained in the valuesin our global environment. And we can see different things storedinside the list by expanding the list. So one thing that we'll want to do,like with a vector, is be able to subset a list. So the most frequent thing we wantto do is access a particular element on the list. So here I use the dollar signoperator to extract a named element. So here I'm extracting this fifth element,e here, from the list. And we see the matrix is returned. Now we could subset the listto say give me back part of the list instead of a single element. So to do so we use the single squarebrackets much like we did with a vector and I'll give it a vector ofindices in this case list element. So this will return a list of length twowhich has the first two elements in it. So this here, extracting the first elementof the list, is now what this does. What it does is returns a listwith the first element in it. So we see here that this is stilla list because it has its name element which happens to be the vector. If we wanted the vector,not a list which contains a vector, we would need to usethe double square brackets. This gives me the list element. This gives me the first list element,which is a vector. Instead of giving a vector of indices,I could also give a list of names. So here I am creating a listwhich contains element e and a. Similarly here,I'm creating a list with the element e. However if I wanted toreturn what's stored in e, I would need to again usethe double bracket notation. So one thing that's a little bit differentabout this list is that contains let's see, a vector, a vector, a vector,a function and a matrix. This function we can returnby extracting the element. So this is the functionthat's stored there. It takes a single argument, 42,it does nothing with that argument. It simply prints Hello, World!, because wehaven't written Hello World yet in our. So to run that I simply need to add theparentheses to make it a function call, give it an argument, andhere we go, we have Hello World. [MUSIC]"
stat-420,1,14,"[MUSIC] So a data frame will tietogether a list and vectors. Data frame is simply a list of vectors. So we can create a data frameusing the data.frame function and here I have a vector, another vector andanother vector of numeric values, character values and logical values. And I'm giving them names x, y and z. So if I run this code hereI'll put what was there and we see our data frame returns as rows andcolumns and so our data frame finallyties everything together and we now have rows which are observationsand columns which are variables. So we said that a dataframe is a list of vectors. So, we could rerun this code here butmake it a list instead. But, these sort of loses the rows andcolumns, observations and variables structure. So, that's where we use the data frameit will be sort of we think of as a data matrix but unlike an actual matrix in R,it will not be required to have all of the same data type by givingit different vectors we can use different data types inside those vectors whichwill be necessary for data analysis. Because a data frame is a list of vectors,we can extract a particular vector like we would ina list, using the dollar sign operator. So we see here, we're extracting the xvector from the data frame example data. And just to quickly verify, we see here that all of thesevectors are of the same length. And that's basically a requirementbecause, again, we need observations and variables, rows and columns so we needto have vectors of the same length. When we have a data frame we'lloften want to look at the structure using the str function,tells us that this is a data frame, it has 10 [INAUDIBLE] and three variablesand it shows what those vectors are. We might notice that well, up here itlooks like this vector is numeric, but here we're calling it a factorvariable, we'll come back and touch on that in a moment. So we could directly extract some ofthis structure information, for example, the number of rows or the number ofobservations, the number of columns or the number of variables orwe could get both of those together. [MUSIC]"
stat-420,1,15,"[MUSIC] So to further explore data frames,we want to look at a larger dataset. So for example we'll use the Galtondataset from the mosaicData package. I'll quickly store that inmy global environment and we will attempt to printthat to the console. And we see that it's sortof immediately overwhelmed, RStudio actually restricts howmuch of it it will show to us. So to get a look at the data, we'll usethe View function with a capital V. This is actually an RStudio specificfunction because when I run it, it opens up a data viewer in RStudio. We could also open this byclicking the dataset name or data frame name that isin the Environment tab. And this just gives usa quick overview of the data. We have the ability to scroll andwe're not restricted by the console here. We see that all the data is here actually. We can do some quick filtering, say, we could sort height lowestto highest, highest to lowest. We could filter to a particular gender,if we click Filter here. Maybe we find really tallboys in this dataset. It's just a really quick way toget a broad overview of your data. If you wanted to do that without having toopen up the viewer, if you wanted to maybe just get an idea of what variables are inthe dataset, you could use the head function and you could tell it how manyobservations you would like to see. So here we see the first 10 observations. So we get an idea of whatvariables we're dealing with, and what are some typicalobservations here. Again, we can use the structure functionto get the structure of the data frame, which can also be seen by expandingthe data frame in the environment window. And again, we see these factor variables. So for now, you should just know thata factor variable is essentially how R is dealing witha categorical variable. So in this case, the sex variable takestwo possible values M and F, male and female in this case, andthen R says that there are levels M and F. So we could extract those directly. And that's essentially saying thisvariable can only take those two values. If we try to change one, if we try to adda third or fourth possible category here, R would not let us do that. The thing to realize right now is that data frames will automatically makecharacter values into factors. That's what we saw happen backwith our example data here. So this vector, as a vector,it's a character, but when we introduce it into a data frame,it becomes a factor variable. Much like every other datastructure we've seen before, we would like to potentiallysubset our data. So to extract a particular row andcolumn, or that is, a particular observation from a certain variable, wecould subset this sort of like a matrix. We could leave the row blank andextract a certain column. Maybe we forget whatvariable this column is, so the names function will help us out here. That is, we use, so we took the secondcolumn, which is the father variable. So the heights ofthe fathers in this dataset. And we could also leavethe column information blank and get rows of the dataset. So here is the firstobservation of the dataset. So this is sort of subsetting a dataframe as if it were a matrix. We could also subset a dataframe as if it were a list. So I could say, give me the fifthelement of the list, which should be, one, two, three, four, five,the height of the children. So notice that this givesme back a data frame. That might be difficult to see here,but this is a data frame. We can tell it's not a vectorbecause these aren't in brackets. Whereas back here, say for example, if I took the second column this way,this was a vector, so keep that in mind. So I could also say,get the first two elements of the list, which in this caseare the first two columns. And again, this is a data framethat's being returned here. We already saw that we can use a $operator to extract named elements from lists, andthose are the columns of a data frame. So here we're extractingthe heights of the fathers. Again, this is returning a vector. This is the same thing, but, again,so this, using the $ operator, gave us a vector. But using the 2 for giving me the second element of a list,that returns a data frame. So similarly here, instead of using 2, using the name, andthe sort of list subsetting syntax. This would also give me a data frame back. But if I want the element of the list, the vector itself,I would use the double bracket syntax. We can sort of do somemore complex subsetting. So, for example, here what I'm doing is,first I'm creating a logical vector, finding where the gendervariable is female. And then subsetting to those rows andthen finally selecting the heights. So these are the heights of the femaleindividuals in the dataset. Similarly, I could usethe subset function. So this says subset, the Galton dataset. And the subset I want is whereheights are greater than 70. So it's automatically because thisis all inside the function call, it knows I want to dealwith the Galton dataset. And I wrapped that with the headfunction so l see the first ten rows. So these are the first ten rowswhere the height is greater than 70. So, so far we've been using a data frame. Recently in R,there's a new package called tibble. Now, tibble is essentially a dataframe with two differences. We would say, it prints in a sane manner,and it subsets slightly differently. So first, I'll coerce the Galton dataset,which is currently a data frame, to be a tibble. And then I will print it to the console. And we see this is vastly differentfrom what happened when I just print a data frame. It automatically only printssome of the observations and it gives me some additionalinformation here. So it essentially prints onlythe first few observations and it also gives methe structure information. So I see the number of observations andthe number of variables. I see the variable names, and in addition, I see the types of the vectors forthese variables. So I see that family is a factor vector,I see that mother and father are both doubles, I see sex isfactor, height is double and so on. So that's the first differencebetween a tibble and a data frame is that it printsin a more sane fashion. But it also subsets slightly differently,and you just need to be aware of this. So remember that right now Galtonis a tibble, not a data frame. So when I tried to subset using, forthis example, the sort of list syntax. This gives me back another tibble. If I used the $ operator, that means,give me the element of the list. This will return a vector. Now, before we saw that these,with the data frame, when I did things more like a matrix,it reduced down to a vector. But here, we'll see that the resultremains as a tibble in both cases. So, you could think of this as, this isalways sort of operating like a list. So the only way to get the element isto specifically request the element. Otherwise, it's going to give us backa list, or in this case, a tibble. If we switch back to data frame real fast,so we see that if I use the list syntax here, this will give me back a dataframe, which prints rather poorly again. If I use the list syntax and say give me the name element,it will give me a vector. But now because we're back toa data frame, when I subset sort of like a matrix, these will reduceto a vector and, again, a vector. One minor note here is,in some older R references, you'll see this attach() function used fordata. Sometimes, you'll be told to attacha data frame, and that would essentially allow you to directly access the variableswithout needing to use the $ syntax. You might think that this makesthings easier and code look nicer, but you should never do this. For example I'm going to,for no particular reason, create a height variable here. And then I'm going toattach the Galton dataset. And I get a little warning here,but let's see what happens. So if I look at the father variable,I can now access it directly. I don't need to use the Galton$father. While it's not directly inmy global environment here, we see that Galton is now loaded, soI can directly access the variables here. Now there's a slight problemthough because height is in my global environment. So when I make this comparison,it works with no complaint because this happens to be of the correct length, thatwhen l repeat it it's the same as father. But I am not comparing father from theGalton dataset to height from the Galton dataset. I'm comparing father from the Galtondataset to height, which is right here. What l am actually trying to do is this,but I'm not getting it because my workspace is not what I expect itto be because I've attached Galton. So bottom line,never use the attach function. [MUSIC]"
stat-420,1,16,"[MUSIC] To import external data into our,for example, this CSV file here in my home directory. We'll use the Import Datasetfeature within RStudio. So we'll select import dataset from CSVwhich will work for most text files, not just CSV. We'll be introduced to this new windowwhich is the import text data window. So we'll first browse tothe file we're interested in. And we'll see a number of things happen. So right away, RStudio populates some codethat it wants to use to import this data and it gives us a preview of what thatdata will look like if we run this code. So a few things, I like to changethe name that it will result in. We'll do ex_from_csv. So this data we already have is theexample of data we saw before when I first made the data frame and I'm going to want to compare reading itin like this to what we had done before. I also want to note that RStudio had intelligently guessed someof these options here. So for example,it realized that the delimiter was comma. If we change it to tab,we see the code changes and the preview does not look like wewant the data to look like anymore. So comma was correct. We can also see here a guess that thefirst row here was indeed the names for the columns. If we uncheck this, it uses them as dataand gives default names to the columns. But this doesn't look right either. So this is what we want our data tolook like, so we'll go ahead and run this code by clickingthe import button. And because it runs view, it immediatelytakes us to the data viewer here. So we see now that we have two data setshere which was example data which was our original data frame that we used earlier. And now, example from CSV,which is what we just dragged in. And we see that theymostly appear the same. But I want to highlighta couple differences. So first,I want to note that read underscore CSV is a relatively new functionfrom the reader library. And we saw that that's whatRStudio decided to use. When we write in this data andread_csv is basically a new version of read.csv which RStudio usedto use to import data. The main difference here is that,read.csv will import a data frame, whereas read_csv will read in a tibble. Another thing I want tonote is that RStudio, it gave us a somewhat absolutereference to our data. So it said read in example datafrom my home directory whereas this line of code here, which we'll alsorun and work and does the same thing, just says simply read in the exampledata from you working directory. So I've mentioned a workingdirectory a couple times now, but I'm going to ignore it for a while longer. When we talk about our mark down and reproducible analysis, we'll sortof deal with working directory and relative references to data tomake our code reproducible. But fornow we'll just be okay with this, but note that we're going to preferrelative references all the time. So let's look at what we just read in,which is a tibble, and compare it to our previous exampledata which was a data frame. So these look mostly the same,except here we see now that this y variable in the tibble ischaracter, whereas we can't see it here. But if we look at example data and extract the y variable,we see that it's actually a factor. So this is a difference data frames andtibbles. So tibbles will allow us to keepcharacter information as character. Whereas data frames will keep anycharacter information as a factor. And this is sort of a remnantof R was developed for statistical analysis and factors are veryuseful for statistical analysis. So often times if you had characterinformation, you would've preferred it as a factor whereas now Ris used for a broader range of things. So sometimes you wantcharacters to be characters. So now we allow when we read in data forit to remain a character and we will only force it to be a factor if we would likeit to be a factor for later analyses. So recapping,read_csv is newer, faster, and will import a tibble, which we see here. And read.csv is slower, andwill import a data frame. And also, by default,will coerce characters to be factors, there is options that we can add here thatwould allow characters to pass through. But by default, it will make character vectors suchas this one here into factors. [MUSIC]"
stat-420,1,17,"While R is aprogramming language, we will focus onits ability to be used as an analysis tool. Now that we can createand import data, we will begin to analyzedata using simple summaries. After completing this lesson, you will be ableto summarize data both numerically and visually, which in practice is thebeginning of any good analysis. First, we will see how to calculate basicsummary statistics. Then since humans are very good at detectingpatterns visually, we will introduce someof the most common data visualizations.Thesetools will be reused andrecontextualized as we introduce methods throughoutthe entire course."
stat-420,1,18,"[MUSIC] So import a new dataset, the miles pergallon dataset from the ggplot2 package. And the first thing we should doanytime we import a new dataset is look at the data. So we'll do that using the view command inour studio and here we have our dataset. We see it's information about cars,we have some information on what year they were made,some information on the engines, and in particular some informationon fuel efficiency. So we can do things like, say,sort city fuel efficiency and we see here some low fuel efficiency and that corresponds to some largertrucks which sort of makes sense. And if we sort to higher fuelefficiencies we see some smaller cars. We already understand a littlebit about this data, but one question we should ask ourselves is,where did this data come from? What is this data? So to do that we shouldpull the documentation. So we see that is data fromthe EPA from 1999 to 2008 for 38 particular models of cars. And those were models that had a newrelease every year between 1999 to 2008. We should note that this maynot be representative sample of all possible cars in that time butit's specific to these 38 models. I want to summarize some of this data. One of the variables we're going tolook at is the city miles per gallon. To summarize the numeric variable,which city miles per gallon, here it is. We'll want to know whereis most of the data, so where is it centered,and how spread out is it. So a number of things that we'reabout to look at are statistics. And we'll use them now forsimple summaries of data, but we'll discuss them inmuch greater detail later. So first, some measures of center,we would have the mean and the median. So the mean is often referredto also as the average. So we could sort of compute this ourself. We could say, well, we want the sum of thedata divided by the length of the data. So what we would commonlyrefer to as average. And we see that that is the mean. The median is a point thatsplits the data in two. So 50% of the data is below this point. And 50% of the data is above this point. We can try to do this ourselves. So first we would needto sort the data and now we need to find the midpoint ofthe sorted data, so this is a vector. So we'll take this vector andwe will subset it to be, let's see, this would be the last element that's 35. So we want halfway, sowe see that 17 is indeed the median. We see that the mean andmedian here are fairly close together so they're both giving us an idea wherethis city fuel efficiency is centered. And we'll come back to the fact thatthese are very close together right now. We can move on to spread and thereare a number of ways to measure this. We could look at the variance,the standard deviation, the range, which is actually two numbers,the minimum and the maximum and the IQR. So let's further talk aboutstandard deviation and IQR. So standard deviation,the name sort of tells us what's going on. It gives us more orless the average deviation. So first we should talk about a deviation, which how far away each datapoint is from its mean. So these are the deviations here. So this is how far away eachdata point was from the mean. And sort of,we want to average these things. But let's note that if we were to averagethese, I was going to do sum, but let's do mean. We that it's a very small number,which is essentially 0. So we need to average something elsethat's not the actual deviations, but instead, the squared deviations. So if I take the average of these,what do we have here? We have 18.03, andI'm going to do one more thing. We'll just take a square root,which is because I squared things here. Now, I'm in units squared, sothis is miles per gallon squared. I would like a measure spread thatis in unit miles per gallon so I take a square root. And I have this number here which, let'scompare it to the standard deviation. So it's very close butit's slightly different, and this is detail that we'lltalk about much later. But just know that the standarddeviation is very close to the average of the squared deviations. And then, again,taking a square root to put it back into this tells us an average deviationin terms of city miles per gallon, not city miles per gallon squared,which is actually what the variance does. Because the variance is justthe standard deviation squared. We also saw that the IQR was 5, butwe don't know what that is yet. To understand what that is, we should lookat what we call the summary of the data. So the summary function inR will come up quite a lot. It has many uses depending on whatwe give it as an argument here. Let's find out what happens when we giveit a single vector of numeric values. We see a number of things here. So this sort of summarizes the data. We have the minimum and the maximum,which is the range that we saw before. We have the Median and the Mean, which were the measures ofcenter that we saw before. We also have the 1st quartile andthe 3rd quartile. So these are similar to a median. So if the median says, 50% of the datais below this number and 50% is above, the 1st quartile for quarter,25% of the data is below, 75% is above. The 3rd quartile,75% is below 25% is above. So the IQR, the inter quartile range, we see that that is the 3rd quartileminus the 1st quartile, for 5 here. So that tells us wherethe middle 50% of the data is. So we mentioned that the mean andthe median were rather similar here. So I want to see whathappens if magically, we make one of these cars get 500miles per gallon in the city. Unlikely, but let's see what happensif we do that to our data here. So, let's see what happens to the mean andmedian. We'll actually reproduce the summary here. So the median stay the sameit was not affected by this crazy fuel efficient car here butthe mean increased. Because of this we say that median isa robust statistic whereas mean is not. So mean was affected by thissort of massive outlier whereas the median was not. This is why we use median for thingslike average salary, average home value. So a few high earners or a few very expensive homes don't sort ofthrow off our measure of center here. I'd also note that before the standarddeviation was roughly 4.2, now if you look at the standard deviation it is muchlarger where as the IQR did not change. So we would say that IQR is robustwhereas the standard deviation is not. That's the number of numeric summaries fornumeric variables. We could also come up with numericsummaries for a categorical variables. So, for example, here we have the drive tray variable whichtells us if the car was front-wheel drive, four-wheel drive, or I believe thereare some rear-wheel drive cars in here. Yeah, here's one. We can use the table function to quicklysort of summarize this data, so there were 103 four-wheel drive cars, 106 front-wheeldrive cars, and 25 rear-wheel drive cars. This is a simple count,if we divide by the size of this dataset we could get proportionsof those three drive. I also return herethe different classes of cars. And I would note that we have a lotof pickups and a lot of SUVs. And I think that sortof explains the not so very fuel efficiency that we havebeen seeing in this dataset. [MUSIC]"
stat-420,1,19,"[MUSIC] Returning to the miles per gallondata set, we will now summarize some of this data visually insteadof with a single numeric value. First to summarize numeric data,we'll use the appropriate graphic, which is histogram. So in this case, we'll summarizethe city fuel efficiency data. And in R, to create a histogram,we'll use the hist function. So R studio places the plot in this bottomright panel here under the Plots tab. So what does this tell us? So first, to simply read a histogram,we say well, between a city fuel efficiency of 10 and 15 miles per gallon,there were, let’s see, over 80 cars. Similarly, between 15 and20 there were over 80 cars. Down here between 30 and 35 there were,maybe not even 5, very few. What else do we see? Well, the vast majority of fuelefficiencies are between 10 and 20 here. Further, we can sort of see how thismatches what we same numerically before. Recall, that we had a mean of 17, roughly,and a standard deviation of about 4. So 17 is in this bar here, and17, if we go 4 in this direction, we're about here, if we go 4 to the right,we're about here. We see that the vast majority ofdeviations are sort of within four of the mean, and that's where the vastmajority of the area of this histogram is. We see that the histogram sort of matchesthe numeric summaries we saw before. But it tells us all this in a single plot. It also shows us, for example, thatthe range is roughly between 5 and 35. So we have one quick visual way ofrelaying a lot of information about this data here. This plot isn't particularlyvisually appearing. And in particular,some things just look very ugly. For example, passing through this,our variable name here, automatically. So we can fix this with some additionalarguments to the hist function. We see a number of them here. So here we are going to modify, say, this label,here we're going to give it a title. Let's run this andsee what happens though. So a few other thingshave changed as well. So instead of there being 6 bins,I now have 12, and I gave it some colors,which I happen to like. We'll see here by increasingthe number of bins, here, we see the information is still the same,17 is sort of a very common value, most of the valuesare within four of that. We do see a little bit more informationwhere there's very, very few cars out here and they're sort of almost separated fromfrom the other data, so these are sort of, I don't want call it outliers, butwe'll call them unusual values. Here, we had a numeric variable,if instead we wanted to summarize a categorical variable,visually, we would have to use a bar plot. In R, we first would have tosummarize the data, in this case, the drive variable, as a table,and then pass that to bar plot. And we get a bar plot. This isn't the most usefulgraphic in the world, it doesn't really tell us anything thatin the numeric summary didn't tell us. But again, we can clean this upa little bit with some additional arguments to the bar plot functions. Bar plot and histogram were ways ofvisually summarizing a single variable. But often, what we would be interested in,is the relationship between two variables. For example, if we wanted to understandthe relationship between a numeric variable and a categorical variable,we would use box plots. So let's first create one. This is actually multiple box plots, one for each of the possiblevalues of the drive variable here. We also are our y axis now isthe highway fuel efficiency. So, a number of things. First, were using new syntax here. So instead of saying mpg$ andthen extracting a variable, the box plot functionhas this data argument. So it's saying, essentially, in this function call, we knowwe're going to be using these data. So then over here,in what we'll call the formula, we can directly access the variables. So this says, I want to understand the highway variableas it relates to the drive variable. So then, another way to think of this is,this variable here will be the Y variable, and this variable here will be,in this case, the X variable. So for each possible value of drive,four wheel drive, front wheel drive and rear wheel drive, we have box plots. So this gives us a quick way ofcomparing highway fuel efficiency for the three different drive trains. A quick note about box plots,so what is a box plot? The most important part of a boxplot is the box, and so in this box, we have the thick line,here, is the median. So that's telling us wherethe data is split in half. The lower part of the boxis the first quartile. The upper part of the boxis the third quartile. Depending on the strictdefinition of a box plot, how the whiskers are createdcan be slightly different. But know that this tells us wheremost of the rest of the data lies. And then sometimes you'll see dots outsideof the whiskers, and those represent data points that are sort of very far fromthe box relative to the size of the box. So we can sort of figure thisas rather unusual observations. So before talking about the box plus more, I'm going to again rerun box plot witha number of different arguments here. We'll talk about some of them. So again, modify some colors,I modified some labels. PCH stands for plot character, so I modified how these dotswere being created here. I also increased their size with CEX. So again, this gives us a quickway to compare these things. So we see that there's a veryclear difference between say, front wheel drive and rear wheel drive. In general, the front wheel drive has a much betterfuel efficiency than the rear wheel drive. So while say, comparing fourwheel drive to rear wheel drive, the median is lower for four wheel drive,their boxes are sort of very similar. So they're not all that different. Lastly, one of the plots that we'lluse most often is a scatter plot for comparing two numeric variables. There's a good chance you'realready familiar with this. So here we're saying placethe highway variable on the y axis, the displacement variable,how big the engine is, on the x axis. And then we have each individualdata point plotted here. Again, this is not the prettiest plot, sowe can give additional arguments again. And here we've added some labels,a title, and we've changed the color, plot character,and size of the individual points. And we see sort of this decreasing,maybe it's like curved relationship. So the bigger the enginethe poorer the fuel efficiency. Everything we've done here would beusing what we would call the base R plotting features. So these are all functionsthat were directly in R. There are a number of differentplotting systems in R. For example, there is the latticepackage which has X Y plot, and by default a scatter plotwould look like this. There's also the GGplot2 package,this is becoming probably one of the most increasinglypopular plotting systems inside of R, and by default it looks likesomething like this. I will most often use andshow the base R plotting system, but you're free use anyplotting system you chose. Also, I would note that alongthe way I was very quickly using different arguments here andsort of saying, what they did. But note, this are never exhaustive listof possible arguments to a function. So say, for example, plot,if you look at the documentation here, there is a wealth of informationabout how you can modify plots. So just be aware of that. Often, you'll have tosort of dig around and try a few of these things before youget the exact plot you're looking for. [MUSIC]"
stat-420,1,20,"Here's an analogy frommy colleague, David Dalpiaz. If a tree falls in a forest and no one is around to hear it, does it make a sound? Or in the practice of statistics, if you perform a data analysis, and you can't communicateyour results, did you really performa data analysis? Worse still, if youperform a data analysis, and it cannot be reproduced, does any of it even matter? In this lesson, we'll solve these issues byintroducing our markdown. In the spirit ofliterate programming, a concept introducedby Donald Knuth, R Markdown combines both R and Markdown intoa single document, which can be used for both data analysis and communication. The ability to bothanalyze data using R, and explain the analysis in natural language using Markdown, makes R Markdown apowerful tool that is becoming increasinglypopular in practice. We will see how RMarkdown can be used to both interactively analyze data, as well as create rendered reports in a numberof different formats. After completing this lesson, you'll be able to useRStudio to create report documents that aregenerated using R Markdown."
stat-420,1,21,"The first thing we need to do to get started with our markdown is install our markdown, which happens to be itself in our package. So, we'll install it using install packages R Markdown. Now, that we have.R mark down installed, we can do some things inside of our studio without even using R Markdown document. For example- I'll hide the council here. I can take this file.R file I had written, so this R script - this was about plotting - and I can create report from it. So I'll use the compiled report button here, or control shift K in Windows, and our ass, or our studio asked, what output format I like. I will select HTML. So, some work is on the background here and the one we see here - I'm going to hide my environment Paint - and we see in this viewer here there's a resulting HTML file, which is a nice compiled report based on the code in this script here. So you see not only the code but also the output intermixed in this document. So that's, in and of itself, fairly nice. But if we want to consider writing our code to perform data analysis, so not writing our code for software development but for data analysis where our goal is to convey information to someone, both of these documents - so the R file or this resulting document here this HTML file - are both sort of lacking. So, the R file by itself is obviously lacking because it doesn't have output. So, if we gave this to someone else, they would have to rerun all of our results and, on top of that, they would have to reinterpret the results. Over here, while we now both have code and output, we're sort of limited to writing up our results and explaining our work via comments. So we'd like a more expressive way for summarizing our analyses and sort of conveying what we've actually learned from an analysis. So, to do this we will introduce our mark down documents which combined are, and Markdown, in sort of a literate programming paradigm to produce reproducible research. So, we will start our first R Markdown document by creating one in our studio using File New R Markdown. I'll say, will title it, 'my first R Markdown'. My name is David, so that'll work for author and we click okay. So, R Studio gives this sort of this skeleton template for R Markdown document. So what I'm going to do right away is save it. I'm going to get in the habit of saving it into, inside of a folder that sort of represents the analysis or inducement. We'll call this 'my first R Markdown', then side of there I'll call the document the same thing, 'my first R Markdown'. Alright, and now before we even discuss what's happening here, I'm simply going to click the knit button, which will take what I have here and create the final output from an R Markdown document. The season worked on in the background and we see the resulting HTML file here which is considered the report from this analysis. So, we've created our first R Markdown document, now we're going to need to discuss some of the details of what's happening here."
stat-420,1,22,"So now we have our first R Markdown document, and the result of compiling it into a report, in this case, an HTML document. So to better understand how an R Markdown document creates the final report, we're going to start from scratch and see what happens along the way. I'm actually gonna delete everything except for the header. So the header, we can see, is some sort of metainformation about this — it creates a title, the author, the date. And in this case, when using the Knit button, we are knitting to HTML, we could actually change this to various formats including PDF and Word, but we're mostly gonna stick with HTML. OK. So, Markdown is just sort of a shorthand language for producing HTML but also other formats, but you sort of think of it as a shorthand notation for HTML. So, we can use hashtags or pound signs to create headers. So here's header one. I'll create another header here. We can also do subheaders by adding additional hashtags or pound signs. See, we'll call this a subheader. And, let's just put some text here, to sort of get a better idea of what spacing will look like. And we'll knit this, and we see the resulting headers, a subheader which is slightly smaller than the overall header, and a text interspersed. So, I'm gonna create some more text, but now let's try to stylize some text. So for example, we could write bold text. We could also write, let's see, italic text. If we knit that, we see the resulting bold and italic text. So bold is created with two stars on each side, italic with one. Another useful thing, because we'll be talking about code very often, is the ability to have monospace font. So, here we see a monospace font. Knit that and see what happens. What we see, it sort of is – I'll put stylize as code, so that will be useful. So another useful thing to be able to do is write a list. We'll do that fairly frequently. So, the single dash and then a list element. There's a spelling button here in case I screwed up. Oh I did. OK. And then I'm just gonna copy and paste this a few times to get a list going. And we see list. Another useful thing to do is to be able to link. So, links looks like this. So here we have the text of the link. So let's say we can link to Google, and I need to put the actual link address here, www.google.com. Let's see, what are some other useful links. So, here's a useful web page here, which is the R Markdown web page from RStudio. There's a number of resources you might want to be aware of here. This Getting Started link here has a number of tutorials for getting started with R Markdown. I'm just interested in the link right now since I just needed another link to talk about. So here we'll say R Markdown Documentation and the link here. So if we knit this, if I click the button, we see some links here. I should probably space them appropriately, maybe I could put them as a list. And now, we have a list of useful links here. OK, so one thing that will come up often is creating tables inside of R Markdown. The syntax for this is a little tricky. So, one thing that I often do, well, I'll use a Markdown table generator which you can generally find by just searching for one in Google. I have a three-by-three table here. Just show you I have a three-by-three table. Let's see, A B C, 1 2 3, Do Re Mi. I think that's right. So if I generate this table, I can copy this text and then place it into my document here, and knit that, and we see the resulting table and the output. So at this point, we've only done Markdown, so everything here is Markdown. This is a quick way to express what we want to see in the resulting HTML. But now, the power of R Markdown will be the ability to, as you guess, introduce R. So to do so, we want to insert what's called an R chunk. So let's see if I remember how to do this. Code, Insert Chunk. But this is something you're gonna do so often. You'll absolutely want to remember the keyboard shortcut, which for Windows happens to be Control+Alt+I. So I'll do that here. So, now, we see by using the shortcut we have this syntax here, this essentially says anything between here and here will be R code. So let's just sort of maybe create a couple of vectors. I'll create a vector of x and maybe I'll create y which is based on x. We'll make it three times x. And now, when I knit this, we see the results here, which is to show the R code, but I realized that nothing was output here, so I didn't give you an example of output. So maybe we'll add something like x + y. So I think this, in the end, should be four times x. And we see now the output as well as the code. Let's create a few more chunks, so I'll do another one here. Maybe this one I will want to plot, no, let's create a histogram of y. Let's see what happens when we do that. OK. And it results in a histogram of y, as well as the code used to generate this. And I'll make just one more chunk. Let's see, maybe create some more data here, we'll call it z. Let's get some sort of arbitrary data here, and then maybe returns z + x. So, we might notice a lot of these chunks depend on previous chunks. So, we see here that, oh, I made z and x of different lengths, and I'm sort of getting a warning here. So, first, let's deal with this. Sometimes we don't want to see warnings like this. So now we can talk about chunk options. So, for example, sometimes, you know, we don't want to see the code that displays this histogram here. So, what I can do is say echo = FALSE. Similarly, maybe I don't want to see this warning. We can sort of see here there's these buttons off the side here. I'm going to click this one which is a setting. So for this particular chunk, I'm gonna say I don't want to see warnings and I don't want to see messages. And actually, I'm gonna come back to this chunk here too and say, maybe I want this histogram to be bigger. I don't really want to be bigger but, for the sake of example, let's maybe make it rather large. And we see that I've now modified some more chunky options here. And if we knit, we see that now the histogram is gigantic and no longer is the code that generate this plot being displayed. And lastly, the warning message about z and x not being of the same length has suddenly disappeared. I'd also note that, well, all this code here depends on x, so or at least all the code depends on this chunk here. Back when I first started generating analyses, you'd have to run your R code, sort of copy and paste your code to another document, and then copy and paste the output into the document and then, gosh, if something changed, you'd have to go through the whole process again. But here, and this is gonna seem sort of trivial now, but I could, for example, just change this to be maybe a longer vector, rerun my code, and everything is totally fine. So you might be thinking, well, we just created a bunch of extra work for ourselves because now every time we want to see what's happening here, we have to click on it and then find it in the resulting document and see what happens. But that's not actually the case. These buttons over here are sort of indicating — hey wait, we can actually run this code, so I can run this chunk with the play button here. And we see it runs and outputs directly inline inside of the Markdown document. We could also use our usual Control+Enter for running a chunk. We could use Control+Shift+Enter for running an entire chunk instead of just one line. So now, this is gonna get annoying with this being so big – let's change it. So now, with the ability to both run inline and then knit a final document, we see that R Markdown is a really nice way to work on an analysis. So, before, we had seen that our environment was not being used at all, but now, by working inline, our environment is being populated with what we're doing inline. So, we should also be able to import data in analysis like this, so let's do that real fast. We'll import from a CSV file. I believe I actually happened to have slipped some data into this folder that we've been working on. We'll do input. Yup, that looks good. We'll important that. And now if I go back to my Markdown file, I can do things like, let's say, example_data$y. Just y. Maybe I'll take the mean of that. Actually that wouldn't make sense because that's a numeric variable – not a numeric variable. So I'll just do this, and we'll knit it. And we have a problem here, because while example_data is inside of my global environment for working interactively, there's no instruction inside this document for where to obtain that data for where I'm knitting. So I need to be sure to include a chunk that actually reads in the data. So I'll steal this code here. I'll remove this information that I don't need. I'll change this because I want to, and I will knit this. And we'll see that now the document runs. So now, we can be both running interactively and it will have the ability to knit to a final file. I will note a couple things. We don't necessarily want to see this, so, again, we could modify our chunk options here. No warnings, no messages. I believe this is a message, not a warning. And I'll also note that while this works, this will only work on my machine or any machine where the data is in this exact directory. So I would prefer to use a relative reference. So, this means that we want the data file to be in the same directory as the Rmd. And this will still knit because that is the case. You'll also note that if you're working interactively, your working directory is` automatically the directory that contains the Rmd file. So if I wanted to just import the data without having to use Import Dataset button, I'll put a quote here, tab complete. And these are the files in the same directory as this file. So here's the data I want. I knit, and everything sort of works out. So, in general, you just need to be aware of — am I working interactively or am I knitting? When I'm knitting, everything has to just read in order from the top to bottom of your Rmd file. When you're working interactively, things are being affected by the global environment here. So note that, for example, things can be run out of order now. So I could, for example, continue to run this chunk here and actually let's do this. We'll say now that maybe we'll get a little crazy here, say that y is equal to x here. And this will continue to change my working environment the entire time. This would not knit because when it gets to this line, it'll say that it doesn't know where y is, so it doesn't work. So I have to return to what we had originally. But you just always need to be aware of am I working interactively or am I knitting a final document? If you're knitting, you need to be able to run from top to bottom. If you're running interactively, you're operating inside your global environment."
stat-420,1,23,"[MUSIC] So, to talk about these few morefiner points that are mark down. Instead of doing everythingfrom scratch and watching me make someerrors along the way. We'll use a pre-written mark down file,and I'll actually distribute this entire directory so you can try thingsout with this file as sort of a template. Have sort of a list of thingsI want to go through here. First of which is again,we see headers in this file. There are quite a few of them. This is a much longer file. And so, a few things about that. Our Studio Server provides this niceoutline here which we can actually toggle. This will let us seek through the headerinformation sort of rather quickly to get where we want to go toin a larger document. Also, when I knit this file. We see that the header informationwas automatically used to generate a table of contents. I did this by adding the table ofcontents to this header information here. Now there's a number ofoptions we can specify here, which is detailed inthe mark down documentation. But some of them, there's no need toremember because we can use the Setting button here and the Output Options. And for example,I simply include a table of contents here. We could also do some things like say, changing the overalltheme of this document. I'll sort of pick one at random here. And we could also changethe syntax highlighting. Again, I'll sort of pickone at random here. And we see that automaticallymodifies the header information here. And when I knit his document we'llsee the style will change and we'll have a slightly differentsyntax highlighting going on here. Let's see I think I can scroll down andfind a chunk in this document. Here we go here's one. One thing you might notice is,instead of the default syntax for a chunk here, I have a little bit more. So if I start with r, space, andthen some name, we can have named chunks. This isn't necessary, but if we'redoing some more advanced r markdown, it's sometimes useful to be ableto reference a previously chunk. So sometimes you'll see our markdowndocuments with named chunks. Again, we can run this. And we see the output is inline inside of this document. We also see that it is being put intothe council, but we're not actually seeing this here and it's populatingour global environment here. I would note that sometimes people don'tlike to see the output in line here. So we can actually modify how our markdownoutputs using the settings again here. Maybe we want, instead of inline,we want things to the console. R's going to want me to remove what'salready there and that's fine. So now when I run this, it's onlyrunning in the console or I should say, only outputting the console,and we don't see inline. This is a personal preferencethat you can decide. I happen to sort oflike the inline output. This is running R in a chunk. I think one thing we didn't talkabout last time was inline R. So here we see we have some values storedin this variable called test sample here. And here,I have an example of running R inline. So it will run this function and then place result of it insideof this exposition here. So if we look at where that isoutput in this document here, we see it's outputting the resultof this r command here. And that can sort of be rather usefulwhen we're writing an analysis. And then again if we change this here. So this is based on somerandom number generation. So let's say I changethe see here to be one and I renit my document thiswill no longer be 4.7. It'll be a new number which is,let's find it here 2.6. So again, it could be sort of niceto write your exposition based on results of code that you ran. We briefly touched on chunk optionsbefore, a little bit more here. In particular one that we didn't mentionbefore, I believe we talked about echo and messages and warnings. But we didn't talk about evaluation. So one thing we can usefulto do is to display code. Let's see if we can find this here,Chunk Options. So here's some code that's being displayedbut it's not run so this is not evaluated. This can be useful if you wantto display code that would. Throw an error,because if code resulted in error, the marked down document simply won't run. So you could display it but not evaluateit which is what we're doing here. And these are three lines wesort of briefly touched on that you would not want to run. Or at least you touched on the firsthere which is this line wouldn't case a problem,though the document would still knit. But it just adds time to the knitingbecause you're installing this package every time you knit the document. These two commands there fineif we run them interactively and actually rather useful torun interactively, so if I run this line here with control enter itpulls up some documentation that's good. If I run this line here,it pulls up the data viewer. Nope, I ran into a commoninteractive mistake, which is that I'm running interactively,and I did not run the data that imported, orsorry, the line that imported this data. So it's somewhere up aboveme in this document here. I could go andfind just that line and run it. But the other thing I could do issimply click this button here and it will run every chunk above this one,so I'll do that real fast. Okay, sonow everything above this line was ran. So this, I believe is now loaded,via I think a package, and now I can run this andI've taken the data viewer. Which again,this is fine to do interactively. But these two lines would potentiallycause problems when knitting the document, because again, when we knit the document,essentially what happens is a brand new our session spawns outside of ourstudio and we run this document there. So that'll cause a problem herebecause best case scenario, it'll spawn a browser windowwith this information. Worst case scenario something will beset up right and it will simply break. Same thing here. This view command is actually somewhatspecific to opening the zero window in our studio. Best case scenario, opens up sort ofthe old school data viewer and our. Or again, worst case scenario,something's not set up right, and it will simply break. So these are number of lines thatwe would want to not evaluate when we're running a mark down document. If I return to the viewer window here,and I think go down a little bit, we see we can write Latek insideof our mark down documents. If you're not familiar, it's not necessarily somethingyou need to know how to do. We'll see here this LaTex is a languagefor type setting mathematics. We also see inside of our document is sortof automatically generates the output here and we can see what happens if wechanges thing in the phi I believe. Yeah, that equation doesn't make sense,we'll change it back. But sort of nicely inside ofour mark down and our studio, this will automatically populate. So we sort of see a what we see is what weget sort of editor, which is rather nice. One thing about the result here isthat when I'm knitting into html, this simply works, and this isdisplayed as what's called, math jacks. So if you wanted to learn LaTek,a nice thing is you can right-click any math equation, and a resulting document,click Show math as tech command. And it will open up a window with theequation that we had seen written here, and you could copy and paste that into an R mark down document Tosort of get started learning some tech. And again, it can be used inline sortof similar to how r was used inline. Here we actually see we'reusing both tech and r inline to generate a mathematical equation witha number that was being pulled from r. Another thing I would mention here isthat to use tech and knit to html, you actually don't haveto have LaTex installed because it's simply using math jaxwhich is JavaScript in your browser. Whereas if you like to nitthis to a pdf you would indeed have to have tech installed ahead of time. Lastly again, this file will be providedto you or, a file very similar to it. It would be a good thing to just try itout, make sure you can knit it yourself. Make some changes. Try to understand what theydo to the resulting document. We'll get plenty of practice withour mark down along the way. [MUSIC]"
stat-420,2,1,"[MUSIC] For our first lesson this week, we will introduce the simplelinear regression model. Perhaps the most common andimportant model in statistics. The simple linear regressionis a simple model. But also often a very effective way toexplain the relationship between two variables. We'll discuss both how andwhy we fit a line to data. In addition to simplyfitting a line to data, the idea of simple linear regression asa probability model will be introduced. After this lesson, we will have laidthe foundation that we will build upon throughout the remainder of this course. [MUSIC]"
stat-420,2,2,"[MUSIC] So here we have some datawe'd like to analyze. So first we should say that this is a dataset inside of R, which is called cars, and it's a default data set. So you can simply type cars andyou get back this data set. Which we'll do eventually. So, we see the data presented in two ways. On the left, we have the sortof data frame representation that shows the actual numeric values forthe two variables. Whereas on the right,we have the graphical representation. And again,we said there's two variables here. In particular, speed, in miles per hour,and distance, in feet. This data is,cars were driven at certain speeds and then we measured their stopping distancein feet after the brakes were applied. You might notice that these cars werenot driven particularly fast and they took a decent amount of time to stop. That's because these are not modern cars,these were cars in the 1920s. So this is a bit of a classic data set. A bit of additional nomenclature aboutthis situation before we proceed. With most plots we would call speed the xvariable and distance the y variable. But in this setup, we want to give slightly more specificnames to these two variables. So in particular the y variablewill be called the response, otherwise known as the target orthe outcome. And then speed, the x variable in thiscase, will be called the predictor. What we would like to do now isdevelop a model for this situation. We have some data here that tells us a bitabout how speed and distance are related. We could make some general guesses about, we drive a car at a certain speed,how long will it take to stop? We want to though, developa specific model for this situation. That model will look something like this. We want to say that the response, Y, issome function of the predictor variable, but we also want to allow for some error. Oftentimes we sort of think ofthis as a SIGNAL + some NOISE. Sort of looking at this picture,we can see here, there's sort of this overall trend. Where as speed increases,the stopping distance increases, and that is explaining what wesee here with this data. But, it clearly doesn't explaineverything because, say at a speed of 20, we see a number of differentpossible stopping distances. So we can think of the signalas what is explained, while the noise is what is unexplained. In this case we see thatspeed is explaining some of the relationship with stopping distance,but there's definitely more going on here. There could be different cars, whichexplain the different stopping distances. Maybe these cars were being drivenon different road conditions, or maybe we're simply measuring poorly andthat's why we see some variation here. But clearly, speed cannot totallyexplain the stopping distance. Another way to think of that is wehave an overall trend in the data, but individual deviations from that trend. So the question sort of becomes,what do we use here. What is the trend portion ofthe relationship between the predictor and the response. One thing we could do,which is not great in this situation, is ignore that relationship and simplyuse a constant value for all values of x. So maybe that would look somethinglike this in this situation. But this is actually notappropriate in this situation. We could actually say thatthis is underfitting. So while this is a model and it doessort of summarize the information here, it only summarizes Yusing the Y information. It's ignoring the X information. But this leaves us witha pattern in the noise. We see that for high speeds, thedeviations are always above this line, or not always, but often. And for low speeds,the deviations are often below the line. So we could clearly do better here. So another thing we could do is considera very complicated function for the signal, here. In this case we've chosen sortof a rather wild polynomial. This is another example of a bad model andhere we're overfitting. It's clear that comparedto our previous model, this signal portion is going through many morepoints and is closer to many more points. Sort of off to the edges here, we see it appears it's almostgoing through these points. It's not quite quite, butit's getting very, very close. And sort of in the middle here,it's sort of following the points closer. It can't go through them all because thereare, for example here, four points for the same X value, but it's getting very,very close to the data. This doesn't really seem appropriateeither, so, for example, if we wanted to consider new observationsat a speed of 21 miles per hour, we would probably expectit to be maybe here. Or maybe vary about it,something like this. But instead, our model is sayingit's somewhere way up here, I'm not even sure how farup that polynomial goes. But clearly, if a stopping distance isaround 60 feet to 20 miles an hour. It shouldn't be that much more for21 miles per hour. So, we'd like to consider a model that'ssomewhere between these two extremes. The one extreme here being overfitting andthe other extreme here being underfitting, and to do so,we'll use what's called a linear model. So as the name suggests, the signal portion will now be a linearfunction of the predictor variable X. So that'll look something like this. So just by convention,we use a Beta 0 for the intercept and we use Beta 1 for the slope. So we want to consider lineswith some variation about them. So maybe in this case it willlook something like this. So this line now gives us sort ofthe average trend in the city. We see sort of on average. As you increase speed, the stoppingdistance increases and then we see, well, there's still somevariation about that line. But that's fine, and I could sithere filling all these but I won't. But we see now compared to,back a few slides here, where there was clearly a trend in the noise, which sortof suggests that it isn't actually noise. Over here there's less of a trend. Maybe even no trend. We see that it really is justsort of variable about this line. But there's sort of no rhyme orreason to where that variability is. So we truly have have noise, now. So this seems like a prettydecent model for this situation. Linear models are probablythe most popular models for data. So we're going to spend a decentamount of time talking about this specific situation here. We should note though that while a linearmodel is useful for this situation, we do see this increase in stoppingdistance as we increase speed. We have to pause andponder a quotation from George Box. At least attributed to him, he saida few more words around this quote, but he said, all models are wrong,some models are useful. A model being sort of an approximationto reality, do we think that the truth is this exact linear relationshipbetween speed and stopping distance? Probably not. But it will serve a purpose. It will provide a model for the situation. We could say then make predictionslike this and we can sort of use it to generally explain the relationshipbetween speed and stopping distance. While the previous two models,the overfitting and underfitting model,are just clearly wrong for this situation, the linear model is sort ofa good approximation to reality. [MUSIC]"
stat-420,2,3,"[MUSIC] So now that we've decided to fit a linearmodel, and again, a linear model looks something like this, wherethe signal is now a linear function of x. The question we have to ask is, for aparticular dataset, so again the stopping distance versus speed data, what isthe line that best summarizes this data? So if we first say, we restricted ourselfto the case where beta 1 is 0 so, no slope. Even in that situation we would say, well, this is probably a prettybad line and this is a bad line. In this situation maybe a goodline is right about here. Again, that would be if were restrictingthe slope parameter to be 0, and that's not the case. We'd like to have different slopesas well as different intercepts. With that in mind, I think maybe a linelike this is pretty obviously terrible. We could consider maybe lines like this,or like this. Again, getting better, they sort of havethe right direction, but sort of now, the wrong intercept. So maybe a good line issomewhere right about here. And we can just sort of see this visually,and it sort of makes sense. We're going to want to quantifythis a little bit more. So what we want, is for the errorsthat this line is making, to be small. So if the true value of a point is y, and the associated point on the line fora particular value of x, looks like this, we want these twoquantities to be close to each other. So another way to say that, is we wantthe difference between these two, to be as small as possible. Essentially, we want to makeas little error as possible. So sort of quantifying this a littlefurther, let's say that we have data and their pairs of data point (xi, yi). And we'll say we have a sample of size n,so we have n data points. Essentially, we want to find a slope andintercept. So a beta 0 and a beta 1,it gives us a line with small error. But how we define small erroris sort of ambiguous right now. So there's a number ofways we could do this. So the first here,we could find the slope and intercept that gives usthe smallest maximum error. So in this case, let's consider two lines. So maybe, this line here andthis line here. So, the bottom line has a maximum error,I believe here, whereas, the top line, its largest error maybelooks to be from here to this point here. If you're using this as our metric,this line would be the better line. This isn't the only possibleway to minimize error. Instead, let's now consider a new line,maybe something like this. Now, instead of only consideringone error, the maximum error, we want to add up all the possible errors. So we could draw those,something like this. And we would do that for every point, butnot going to do that for every point. So essentially, we could find the slopeand intercept that minimizes the sum of all these errors, and the errorswould be after taking an absolute value. So negative and positive errorswill cancel each other out. And this is a perfectly validway to find a good line. It's a line that has a small total error. But if you think about these twomethods of keeping errors small, you might think to yourself, how do Iactually find that slope and intercept, that beta 0 and that beta 1? They're not a very easyoptimization problem. If instead,we consider this last option here, and instead of simply looking at the absolutevalue of the errors, we square the errors. So we want to find the slope and the intercept that give usthe smallest squared error. And this is calledthe Method of Least Squares. This has a number of advantages. One is compared tothe absolute deviations. Since we're squaring the deviations,it guards against sort of larger errors. It's also much, much,much easier to find the beta 0 and the beta 1 that minimizethis expression here. So for that reason, historically, it's been the method thathas been used most often. We'll also see along the way, that this method has a numberof advantageous properties. So now, we need to find the best slope andintercept for a particular data set. This line here is notnecessarily that line. We'd actually first need to deriveexpressions for the best beta 0 and beta 1. So to do that,we'll actually perform the minimization. Remember, we have data (xi,yi), and we know the data. We have the data, so, that's known. Whereas, we do not have the bestslope invest intercept that those are unknown model parameters. So our goal here is to find the beta 0 andbeta 1 that minimize this expression. In a sense, we're looking forthe beta 0 and the beta 1. I'm writing this as a functionof those two parameters, and that's what we want to minimize. How do we do that?Well, we first take a derivative with respectto the two parameters of interest. We then, take those derivatives andwe set them equal to 0, so now we have a system ofequations that we can solve. Along the way,after some algebraic manipulation, we'll arrive at these two expressions,which are called the Normal Equations. We won't deal with these right now,but they will come up again. And maybe if you've takena linear algebra course, you've probably seen similarexpressions to these. Or you've potentially seen them andnot realize that, because you saw them in matrix form,which we'll see later. Soon we arrive at the best values forbeta 1 and beta 0. And because these are a function ofthe data we've seen, and our estimates for these true unknown model parameters,we put a hat on top of them. And we say that theseare estimated from data. So these are now sample statistics. So on the left here, we have the modelthat we assumed for the data. And, on the right here,we have the fitted line. This line here, with beta 0 hat and beta1 hat, which were estimated from data, is our sort of best guess forthe signal portion of the model. So now, returning to the familiar car dataset, we want to take this data here and plug it into these two expressions here. And if we did so,we would arrive at beta 0 hat, and here we have beta 1 hat forthe cars dataset. Obviously, we haven't even looked at thisfull dataset, except for graphically. And these expressions are sortof cumbersome and annoying. We wouldn't want to pluginto those by hand. So eventually, we will see howto get these two values from r. So now that we have the fitted line, and we could call this our estimate ofthe signal portion of the model, and then we see that there iserror about this line. This line doesn't go through every point,obviously. We can use this line to make predictions. So if we wanted to consider an x valueof 15, we would predict a value here. If we wanted to consider an x value of 21,we would maybe consider a value here. If we maybe extended our axisa little bit here to about 30, we would extend the line andpredict a point here. But in each case, so for example,at 15 we wouldn't say, well, a car traveling 15 miles an hourabsolutely will stop in about 40 feet. That's just our best guess. We think that,based on the error we see about this line, it could sort of varyabout that prediction. But also, so 15, 21 and30, here, this was 21. These all have slight difference in their interpretation of the typeof prediction we're making. So let's talk about that real fast. So first, for x values that have beenseen in the data set, so for example, we saw x values at 15 here. We see a bunch of x values at 20 here. The predicted valuesare called fitted values. This point here is the point 15, 54. So a car was travelling 15 miles an hour,and it took 54 feet to come to a stop. So if we were to considerthe x value 15 and plug it into our fitted line,we obtain the value, let's see 41.37, andthat is the point here. So this is called a fitted value foran x value of 15. And so, not only does this point havethat fitted value as does this one, and this one which also have a x value of 15,but the three points here, here, and here all have different,what we would say, actual values. So for each data point in the data set,there is an x value, the true value of y, andalso the fitted value of y. So in addition to fitted values, whichare predictions at x values we've already seen, we can make predictionsat x values we haven't seen. One that I like to do is the point here,which is 21. When x is 21, the predicted value,we'll use our fitted line here, andwe get a value of 64.95, which again, is right on the line here. And this seems like a perfectly,reasonable, prediction. Because we're dealing with a value thatis inside the range of data that we've all ready seen, this is interpolation. We're fairly confident in this prediction. Obviously, we think, again, that possible values could vary aboutthat line because of some noise here. But this value of roughly 65here is where we sort of expect to see stopping distances fora speed of 21 miles per hour. We could also look to makepredictions outside of the data. So maybe if I extended the axis hereto maybe 50's, somewhere over here, and I also extend the line, soI would predict a value here. So when x is 50,if I plug into my expression again, I get a value of 178.92. And now, because we're outside ofthe original range of the data, this is extrapolation. And we're much less certainabout this prediction here. Without looking it up myself,I actually didn't know whether or not cars can travel 50 miles per hour. And remember this data is from 1920,we don't know if our model would even apply to this situationof a speed of 50 miles per hour. I believe cars could actuallytravel that first in 1920. We sort of see a similar issue here,if we consider x is 0, then it's pretty easy to seethat the predicted value, because we haven't seen the valueshere on the dataset, is minus 17.58. And this should be sort of very surprisingand something you shouldn't expect. This essentially says, that for a car that's not traveling at all, ifyou apply the brakes you move backwards. And this brings us back to ourstatement that all models are wrong, some are useful. So that's sort of predicting overhere somewhere, and again, this is outside of the range of data we see,so the model doesn't really apply there. For x values that we've already seen,we are fairly confident in our model. And for x values that we haven't seen,the further we get away from our observed x values, the lessconfident we are in our fitted model. So this is the only model we have. We will extrapolate if we need to and make a prediction at say, an x of 30. But we simple prefer not to. [MUSIC]"
stat-420,2,4,"[MUSIC] So far we've been consideringthe generic model, where Y is a linear functionof X plus some noise. But we haven't reallydiscussed the noise at all. We sort of mentioned that we'd like itto be noise and sort of random about this line, but we haven't made anyfurther statements about that. So now we'd like to talkabout the SLR model, or the Simple Linear Regression model. The S for simple has to do with the factthat we only have one predictive variable. X later will have more than one, linear. Because we have this linearfunction of X here, and regression because we're looking atthe relationship between two variables. Now forthe full simple linear regression model, we need to add a little bit ofinformation about the noise here. So now we'll consider xi and Yi because we want to apply this model toa data set where we have end data points. So the i's will representthe indexes of the observations. So all I've done here is added indicesto the individual variables here. Now, notice that Y is capital,while x is lowercased. And that's to say that in this model, we're assuming that the xiare fixed known quantities. But now, because we're going to introducea distributional aspect to the noise, the Yis are going to be random. So the xis will simply be constants thatare fixed and known ahead of time but the Yis will be random variables. This is because we're now goingto assume that these errors are what we call IIDnormal zero Sigma squared. So this symbol here essentiallymeans the epsilon i's follow a normal distribution with meanzero variance Sigma squared. And in particular we would say that theyare IID, the first i for independent, the second i for identical. So we say that the error variables,the epsilon i's are independent and identically distributed, sothey all have the same distribution. So now we have a model for each responsewhich is both a function of the signal which is this linear functionof its x variable and the noise. Which now we know how itvaries about the line. It varies about the line accordingto a normal distribution. So now that we've added thisdistribution assumption to the errors, that they are independent andnormally distributed. We now have a probability model instead ofthis more generic model where we simply said, it's signal plus noise and we didn't have any additionalinformation about the noise. So another way to think of this is, Y has a conditional distributiongiven a value of X. And that's how this reads here, this readsthe random variable Y conditioned on a value of the random variable x. Again, we're not going toconsider x random, that's why we're going tohave a fixed value over here. So essentially we havea different distribution for y given a different value forx, but in a particular way. So, for any value of x,Y follows a normal distribution. But, for different values of X, we havea different mean, or signal portion. But, for any value of X, we have the samevariance, or sort of noise component. So again, the expected value of Y for a particular value of X is that linearfunction, and does depend on x. Whereas, the variance of Y forany x is simply Sigma squared and does not depend on x. So now, how does this look visually? So we see that Y condition on x,the mean is that layer function. So that's what this line here is,so this is the expected value of y given a particular value of x,and that is beta 0 + Beta 1 x. So then at each value of x there'sa different normal distribution, again with the mean according to the line,but they all have the same variance. So that's why we see these threenormal distributions here, all have the same shape. But, so let's say that this isthe value x equals, I don't know, let's say 5,then this is a normal distribution. With a mean Beta 0 + 5 times Beta 1,with a variance of Sigma squared. Whereas if this over here is x value 10,this is now a normal distribution with a mean of beta 0 + 10 beta 1 andstill a variance of Sigma squared. So essentially, this is saying, weare trying to model some data as there is the sort of, signal component which is themean of all these normal distributions. And then about this signal we expect, say over here, most of the data tobe within one standard deviation. So data is going to vary aboutthis line sort of like this. I mean, we can see observations sort ofout towards the tails of these normal distributions. But most of it will be sort of closeto the line according to the norm of distribution. Sometimes we'll want to directly discusswhat we call the assumptions of this particular model,the simple linear regression model. Which technically,when we specified the model, this sort of implicitlystates all the assumptions. But sometimes we like to sortof summarize them in a shorter, more concise way to talk aboutspecific aspect of them. We can do that with LINE. So, LINE is a quick way to rememberthe assumptions of the linear model. These stand for Linear, Independent,Normal, and Equal variance. So Linear, because the signalis a linear function of x. Independent, because we are assuming thatthe errors are independent of each other. So an error at one point does notinfluence an error at another x value. Normal, because we assume that the errorsfollow a normal distribution, so that observations will be spread out about theline according to a normal distribution. And we're assuming Equal variance. So at any x value the points varyabout the line, the same amount. [MUSIC]"
stat-420,2,5,"Now that we've seenhow simple linear regression is a probability model, we take a minor detour to discuss the normal distribution. We have seen thatthe normal distribution is explicitly used in the simplelinear regression model. However, as you'veprobably seen before, it is by far the most importantprobability distribution. In this lesson, we'll quickly review the basics ofthe normal distribution. We'll also see how Rcan be used to perform many probabilitycalculations related to the normal distribution. In particular, we will introduce the fourcore functions for all probability distribution-basedcalculations in R. We'll utilize these forthe normal distribution, but they also exist for each of the probability distributions available in R. Soafter this lesson, you'll not only be ableto use R to perform calculations forthe normal distribution, but also for many of the most commonprobability distributions."
stat-420,2,6,"[MUSIC] As an example of a continuousrandom variable, we'll talk about a normal randomvariable and the normal distribution, which is probably the mostpopular distribution of them all. Normal distribution is given by bothits PDF and its possible values. So here, a normal variable will takevalues anywhere on the real line, and have this density function. Which we see is controlled by twoparameters, mu and sigma squared, which also happen to be the mean andvariance of a normal random variable. We can summarize this entire situation,we say a random variable X has a normal distribution with mean mu andvariance sigma squared. So this tells us the possible valuesare between negative infinity and infinity, and it also tells us that thisis its probability density function. Let's take a look at allpossible normal distributions. And we can do soby drawing a single bell curve. You're probably familiar withthe normal distributions, also called the bell curve orthe Gaussian distribution. And again, we said that it's controlledby its parameters, mu and sigma squared. Sigma squared being the variance andsigma being the standard deviation. And the standard deviation sigma has arelationship to the plot that we see here. See this, I've labeled the axes fortwo different distributions. First I've labeled it for a generalnormal distribution with mean mu and variance sigma squared, but alsoa particular distribution with mean 0, and standard deviation or variance 1. And that's called the standard normal. First we notice that in both, most of the density is sort ofcentered around the mean here. So the mean is where the areaof highest density is. We also see that as we get standarddeviations away from the mean, so over here, say, two standarddeviations away from the mean, this is an area of low density,as is going two standard deviations away from the mean in the other directionis also an area of low density. We actually notice that this curveis symmetrical about the mean. So the area here, so to the right oftwo standard deviations away from the mean is the same as the area here, which is the area to the left of twostandard deviations below the mean. So that means that these are twoareas of equal probability. It's equally likely that we seean observation two standard deviations above the mean, or further away,as it is that we see an observation that's two standard deviations below the mean,or further away. There's actually some wellknown areas under this curve which are related tothe standard deviations. For example, between -1 and1 standard deviation, we have roughly 68% of the area,and that's the same for any curve. In the standard normal curve between-1 and 1, they're 68% of the area, but also with any curve -1 standarddeviation to 1 standard deviation, between those two, about the mean,they're 68% of the area. Similarly, for two standard deviations,we have about 95% of the data, and lastly, for within three standard deviations,we have about 99.7% of the area. So we see that the vast, vast, vastmajority of the area under the curve for any normal distribution is withinthree standard deviations of the mean. We could write that like this, andagain, these are just approximations. They're not the exact value, but sometimes we'll see this referredto as the 68-95-99.7 rule. And this tells us quickly forany distribution, that is any normal distribution, sort of where the area lies with respectto the mean and its standard deviation. So instead of viewing everynormal curve as the same curve, we could visualize differentmeans of variances. So first, here are three normal curves,all with the same variance, but with different means. So this center curve here isnormal with mean 0 and variance 1. So the other two curves also have variance1, but they have different means, so the left curve has a mean of -2, andthe right curve has a mean of 2. And we see that by virtue of themall having the same variance, they all have the same exact shape. So the area has the same relationshipto the mean in all three curves, but the mean is simply different. So similarly, here are three curvesthat all have the same mean, but different variances. So this middle curve here is again thestandard normal, the lowest curve here, this has a higher variance. And the sort of sharp,tall curve has lower variance. The lower the variance, the tighterall of the area is around the mean, whereas the higher the variance, the morespread out the area is about the mean. And this just sort of makes sense,because again, we said that within one standard deviation of the mean was roughly68% of the area for any normal curve. So to make that true, so when we reducethe variance we need sort of taller densities closer to the mean to make surethat that area is still roughly 68%. We've been avoiding actually doing anycalculations of the normal distribution because we're going to do themwith R instead of by hand. For some normal distribution wehave this PDF here, which is rather unfriendly to work with, so we have somenice functions of R that'll do it for us. The dnorm function takesa particular value, x, and plugs it into the PDF for us. We should note, though,that when we're writing the PDF, and often when we're writing a normalstatement, we're giving it the mean and a particular variance,whereas R expects the standard deviation. So that's just something to be aware of,and you just have to remember the standard deviation is the squareroot of the variance. So there's also the pnorm function,which is the CDF, so it calculates probabilities for us. So it's essentially calculatingthe integral from negative infinity to a q of that density function. There's also the quantilefunction called qnorm, we haven't talked about this specifically. But so what this does is,if we draw a normal curve here, we input to this functionof probabilities. So let's say a probability, I'm going toconsider this area to be, let's say 0.15, I want to know the value here, q, that makes that true fora particular normal distribution. And then lastly,there's the rnorm function, which will generate random observationsfrom a particular normal distribution. [MUSIC]"
stat-420,2,7,"[MUSIC] To test out the normal distribution in R,we'll consider this IQ score example here. So IQ scores is our tactically designedin a way such that they follow normal distribution with a mean of 100,and a standard deviation of 15. We can define a random variable X tobe the score obtained after a randomly selected person takes an IQ test. X has a normal distribution with a meanof 100 and a variance of 15 squared, we define the standard deviation to be 15. That particular normal distribution lookssomething like this, it has a mean of 100. We see here 85 as a standarddeviation below the mean, and 115 is a standarddeviation above the mean. I created this graph usinga function I wrote real fast here. It's sort of complicated andnot important, but I'll just be using that to createvisualizations along the way. So first, we want to know what isthe probability that a randomly selected person has an IQ below 115? First we'll visualize that. So this is the area under the particularnormal curve that we're interested in, that we like. And that is equivalentto this integral here. We don't want to calculate that integral,so luckily there's a function in Rthat will do exactly this for us. So this is the cumulative distributionfunction and to do that we will use pnorm. So we'll give it the valuewe're interested in, and the parameters of the normaldistribution we're talking about. And we can run that, and we obtain the value of this integralhere which is a probability. Note here that P here stands forprobability, that's where the P comes from. We'll talk about D when weget there in a second, but first I want to talk aboutstandardizing a variable. So if we take a normal distribution thathas some mean and standard deviation, and we subtract of mean over standarddeviation, we're left with a normal 0, 1 variable otherwise center normal. So we can use that to alsocalculate probabilities. I'll take the value I'm interested in,subtract off the mean and divide it by the standard deviation, andI see that this is not correct because it should be 115, soI'm one standard deviation above the mean. So I could use the pnorm function now,with simply the value of 1. And we see I get the sameanswer as I got before. And that's because a pnorm functionby default uses a mean of 0 and a standard deviation of 1. And we were looking at standarddeviation away from the mean, instead of the original units. So again,the pnorm function gives us probabilities. The dnorm function, what does it do? Here we want to answer the question, what is the height of the densitycurve at an IQ of 115? So that would be plugging the value15 into the density curve. The dnorm function willgive us exactly that. So we say dnorm of 115, with a meanof 100 and standard deviation of 15. This is exactly the height of the curveat, I did it again, 115 not 155. So this is the height of the curve at thevalue 115 which we can see plotted here. This means that D here stands for densityand it gives us the height of the curve. So in discrete distributions itgave us the heights of bars and those happen to also be probabilities,in general P is only used for probability for pnorm and P binomial. Because it's always probabilitywhereas D is sometimes is a density, so that's why we use D. So let's calculate a fewmore probabilities. So now we want the probability that arandomly selected person has an IQ between 100 and 115. Mathematically that boils down to thisintegral here, visually we want this area here, the area between 100 and 115under at this particular density curve. We'll start with a pnorm, so first I'mgoing to find the area to the left of 115, and we've already done that Itwill look something like this. So this gives me all the areafrom 115 all the way over to negative infinity over here. So what I'd like to do now issubtract off the area from 100 over. You might realize right away, that becausethe normal curve is symmetric about the mean, that it's going to be 0.5 butwe'll use a pnorm function all the same. So we do that andwe obtain the answer, which was 0.34. You might also realize that this areahere is half the area between 85 and 115, which is within 1 standard deviation. And we said before that'sapproximately 68% of the area, so half of that is about 34%. So we see that worked out well. There's another way we can do this. Because like most functions in R,the pnorm function is vectorized. So I could use pnorm, andI can give it two values 100 and 115, and we'll say mean again of 100,and standard deviation of 15. I'll run this and I see I get,this is the area to the left of 100, this is the area to the left of 115. So I can use the diff function nowto take the difference of this and we see we get the same answer. So somewhat similarly if we want tothe probability that are randomly sight to person as IQ above 130. That looks something like this, this should be a ratherintelligent person over here. So again recalculating this integral here,but of course we don't want to do that. We'd like to do this somewhat directly,so I can say p norm of 130, a mean of 100, a standard deviation of 15. And I'm going to look at one more argumentwith pnorm function which is this lower tail. So by default is true, so that meansby default is calculating area is to the left, but I want the area to theright, so I'm going to say lower.tail is false and that should give me the answer,which should be the same as utilizing first calculating the area to the leftof 130 and subtracting off that from 1. So I could do 1- pnorm of 130 with a meanof 100 and a standard deviation of 15. And that should give me the same answer. So now we'd like to work in reverse. We'll say what IQ is needed to be inthe top 5% of intelligence mathematically, we want some value C here such thatthe probability that x is greater than it results in a probability of 0.05. I've cheated andalready got the answer here. To plot this, sowe see that 5% of the area is over here. So I want to find this valuehere that we can't actually see. It's right here. But we want to calculate it withoutme just telling you what it is. I'm going to do this a few ways. The opposite of pnorm is qnorm. So they say qnorm of 0.05 with a meanof 100 and a standard deviation of 15. I'm going to get not the answer, because we've noticed that R isalways sort of thinking to the left. It's areas to the left of some value. In this case what it's saying is, this is a value 75 such that 5%of the area is to the left of it. That's close to what we want, butwe want on the other side of the mean. So to do that essentially this area hereis 0.05, this area here has to be 0.95. So I can change this a little bit. And I can say 0.95 here andthat will get us the correct answer. And this makes more sense. This being below the meanwe knew it had to be wrong, this answer seems much more correct. But we could do this another way as well. Remembering that there's this lower tailargument to some of these functions, so I could say qnorm of 0.05 with a meanof 100 and a standard deviation of 15. Again, this would give me somethinglike 75, if we calculated this area, well not area over here, but the valuethat gives me that area over here. If I now say lower trail is FALSE, it'll give me 5% in the upper tail,and result in the correct answer. To tie some of that together, we want to answer the question what isthe probability that someone has an IQ more than two standarddeviations from the mean? This picture won't be exactly what I want. I want people who are two standarddeviations away from the mean or more. So that the IQs at 130 andabove, or IQs of 70 and below. So I want the white area here,not the blue area here. We are already familiar with how tocalculate the blue area using pnorm function. So I could do pnorm,and give it 70 and 130. So 70 and 130, 70 is two standarddeviations below the mean. 130 is two standard deviations above. I can give it the mean of 100 andstandard deviation of 15. And it'll give me the two values I need,and then I can take the difference of those. So that gives me the blueshaded region here, but I can subtract that from one,and that should give me the answer. Now I'll note that if I goback to the blue shaded area. This is, again we said within two standarddeviations on any normal curve is about 95%. This is the exact numberthat we calculated here. And this would be exactly true forany normal distribution. We just say 95% when we're talkingquickly and approximately. So again this is one minus. We could do this in few other ways. So one thing we could realize is thatthis white area here is the same as this white area here. So the two tail areas because the normaldistribution is symmetric are equal. So first I can calculate this area herewith a pnorm call to 70 with a mean of 100 anda standard deviation of 15 again. And then I could multiply it by 2, because this area here isthe same as this area here. So if I run that,I should see the same answer. I could say, let's look at pnorm of 2. And not give it any mean andstandard deviation information. So say we're using a standard normaldistribution which essentially deals in standard deviations. And I'll say lower tail is false. So what this is calculatingis exactly this area here, which I would need again to multiply by 2. So we've seen the dnorm,pnorm and qnorm functions. So lastly,let's look at the R norm function. So for example if I wanted to generate20 IQs at random from the correct distribution I would say R norm 20, soit will give me 120 observations from a normal distribution with a mean of100 and a standard deviation of 15. And here, we'll see 15 randomly generatedIQs from the appropriate distribution. Just a note on how vectorizedsome of these functions are. So this line of code here is,using pnorm I'm giving it a vector here, a vector of means anda vector of standard deviations and I run this and I get three results. So I get a probability here, here, andhere and what are those probabilities? So for example with 0.5 it'susing a mean of negative 1 and a standard deviation of 2. WIth 1 here, it's using a mean of 0 anda standard deviation of 1. And with 0 it's using a mean of 1 anda standard deviation of 0.5. I can verify that down here, we seethat its the same result as up here. But we can sort of get clever with thistoo and maybe I wanted to calculate the probability that,let's see here, we're maybe below 0. Or maybe we'll just do 1 forthese three distributions. So if I run this, that's why I didn'twant to do one, I'll do zero instead. There's a coincidence there. So yeah, this is the probability thata number distribution with a mean of 1 and a standard deviation of 2 is less than 0,that's here. This is the probability that a normaldistribution of the mean of 0 and a standard deviation of 1is less than 0 and so on. And we can verify that down hereif I changed all these to 0, we see that It's getting the same results. So you can play around withthe vectorization of these things to sometimes short circuit some multiplecalculations you might need to do. That's using the d, p, q, andr functions with normal distribution. But there are many other continuousdistributions, for example, exponential t, f, and chi square. And they all also have a d,a q, a p, and an r function. Again much like with how binomial wasan example of a discrete distribution. And once we knew how it worked in R,we could use any other discrete distribution as longas we have the parameters. Same thing here, so the exponentialdistribution has a rate parameter, and so that's all we need to specify it. So this would be the PDF for an exponential distribution with a rateparameter of 0.5 at the value of 2. This qt function wouldgive me the quantile for a 2 distribution such that 97.5%of the area is to the left of it. And it has one argument called degreesof freedom and so on and so forth. And we can run this andwe can see that we get results and it's sort of what we expected. So here's a bunch of randomobservations for the R function, here's a probability for the p function,here is the value for the q function and a hydro curve for the d function. This tf and in chi square functionsare the ones that we'll see later when we start talkingabout hypothesis testing,. And then just to wrap up this video, I'llclick the knit button to just say that Hey, hi I was using a mark down file andit'll produce a nice looking document and everything, I did here at the end. [MUSIC]"
stat-420,2,8,"[MUSIC]. In this lesson, we'llbegin to discuss some of the details of thesimple linear regression model. First, we will discussthe estimation and interpretation of the parameters of the simple linearregression model. That is, what doesapplying this model to data actually tell usabout the real world? Second, we'll look atthe errors made by the simple linear regressionmodel and use them to create a new statistic that describeshow well the model is reducing the uncertaintyof the response variable. Next, becausesimple linear regression is a probability model, we'll introduce an alternativemethod to fit the model. Finally, we'll begin to use R to explore the simplelinear regression model. [MUSIC]"
stat-420,2,9,"[MUSIC] So to fit this line to this data,we said that this intercept and this slope give usthe smallest squared errors. And then we gave additionalnames to these two numbers. We called the intercept beta 0 hat andwe called the slope beta 1 hat. And at this point, we essentially saidwe're placing hats above these two values just to indicate that theyare calculated from data. So now we want to give a littlebit more meaning to these two. So we return to the simplelinear regression model. So now we're assuming this model forthe data, and this model has three parameters,beta 0, beta 1, and sigma squared. Beta 0 and beta 1 being parameters forthe signal, and sigma squared being a parameter forthe noise. So beta 0 and beta 1 are model parameters,whereas beta 0 hat and beta 1 hat are sample statisticscalculated from data. And we'll see now that these are estimatesof the model parameters beta 0 and beta 1. So the model parameters beta 0 andbeta 1 have specific interpretations. So if we visualize the model like this,at a particular value x here, the mean is beta 0 + beta 1x,which is here. Now if we increase this by 1 andmove to x + 1. So we go over 1, we must go up beta 1. So beta 1 is how the mean or averageof y changes when we increase x by 1. Similarly, if we extend the linedown here to where it crosses 0, this is exactly beta 0,which is the mean of y when x is 0. So importantly, these parameters tellus what happens to the mean of y, not any individual observation. So again, these are parameters forthe signal. So now, we've essentially seen andalready said how to estimate these things. So beta 0 hat is an estimate of beta 0, so a statistic is an estimateof a model parameter. And similar for beta 1 hat is an estimateof beta 1, the model parameter. And then together, beta 0 hat + beta1 hat times the particular value x, this is an estimate forthe mean at some particular value of x. So we've been using y hat to denotes this,but that's sort of a shorthandnotation to say that, well, we have the expected value of y giventhe x takes some particular value. And I put a hat on top of that, sowe have down here, an estimate for a particular mean. So now, returning to the cars' data,we want to reinterpret the situation, both in the context of the cars andsort of speed and stopping distance, but also, the simple linear regression model. So we want to talk about beta 0 and beta 1 in the context of boththe situation and the model. So to do so, we say that this 3.93is the estimated increase in mean stopping distance foran increase in speed of 1 mile per hour. So that is that the beta 1 hat is 3.93,and this is an estimate of this here,the increase in mean stopping distance. So the statement here that I've bracketed,that's exactly beta 1, under this model in this situationwhen we're using this model for cars. Similarly, -17.58, that's beta 0 hat,that is the estimated mean stopping distance when a caris traveling 0 miles per hour. If I bracket here, this is exactly beta 0. So again, importantly, everythingwe've seen here is talking about mean, it's talking about whathappens on average. Beta 1 tells us what happensto the mean when we change x. And beta 1 hat is an estimate of that, andbeta 0 is a particular mean when x is 0. And beta is 0 hat is an estimate of that. Of our three parameters, beta 0,beta 1, and sigma squared, we have estimates for beta 0,we have estimates for beta 1. But what about sigma squared? So before we estimate sigma squared,we first need to talk about residuals. So a residual is simply what's left overif we subtract the predicted value or, in this case,fitted value from an actual value. So recall that for each observation in thedata set, we could talk about the fitted value, which is plugging in the x valueand obtaining the value of y on the line. So for example, when x is 15,the true value of y is 54. But the fitted value of y, which Ibelieve we calculated before was 41.37. So the residual is the distancefrom the true value to the line. So in this case, the residual,which we'll denote by an e, is 54- 41.37 andthat works out to be 12.63. Again, so the yis are the actual values. The yi hats are the fitted values,and the eis are the residuals. For each observation the data set,we'll have the x value, the true y value, the fitted y value,and also a residual. And we can see that in this case,the residual happened to be positive, but that's because the pointwas above the line. For points below the line,the residuals would be negative. So now we'll use these residuals to comeup with an estimate for sigma squared, the variance of our model. Again, what we want estimatenow is sigma squared, and we'll do so with a newstatistic called S sub e squared. So what do we have here? So here we have the actual values,and here we have the fitted values. Here together, we have the residuals. So now what we want to do is, we said we want to look atan average of the residuals. But before we do that averaging, we werefirst going to square each residual. For the reason being, and now I'm going totalk about use of i's for the residuals. If I added up the residual for each point,we would actually find that they sum to 0. So an average of thesewould give us 0 again, so that wouldn't really bea useful metric for anything. So instead, we use square each of them. Now after squaring them, we sum them,and then to average, we divide by n- 2. And that might seema little bit surprising. The reason we do this is because we hadto estimate two parameters along the way. So you might recall the usualsample variance looks like this, if we had just a a single variable x. And so here we an n- 1, and that's becausethis x bar here is an estimate of mu. So we estimate one parameter, sowe say we lose a degree of freedom. So we had to divide by n- 1, so that's similar to why wehad to divide by n- 2 here. Because we had to estimate two parametersbefore calculating the estimate of the variance. For this dataset that we'vebeen looking at here, we find that S sub e squaredworks out to be 236.5. But we need to be careful here,this is in units feet squared. because again, sothe original units was feet, but we have been squaring observations or,well, in this case, squaring residuals. So often, we'll talk about S sub e,so it's the standard deviation of the residuals, which is simplythe square root of S sub e squared. So this is roughly 15.38, butit's back in the original units feet. So often,we might have a slight preference for talking about S sub einstead of S sub e squared. So now we've seen a way to estimateeach of the three model parameters. So again, we have model parameters, those were beta 0,beta 1 and sigma squared. And then we have estimates for those,these are estimates using data. And so, because they're functions of data,these are considered statistics. So we had beta 0 hat and beta 1 hat,which we found via least squares. And then, now, we've seen S sub e squared,which looked at an average of the residuals as an estimate forsigma squared. And also we can considerbeta 0 + beta 1 times x, which is the mean of y fora particular value of x. And we could put together some ofour estimates as an estimate for that particular mean. [MUSIC]"
stat-420,2,10,"[MUSIC] To see how we can think of a linearmodel as reducing variability, we'll first take a lookat this equation here. So on the left hand side, we haveinformation only about the y variable. So we have a yi soa particular y value of an observation and we're subtracting off its sample mean. So the sample mean is the horizontal linehere and if we consider just one point, maybe this point here that expressionis for this distance here. So that's yi minus y bar. To obtain the right hand side of thisequation we've essentially added 0. We've subtracted and added y i hat,which are the fitted values. So those are obtained from a line here, which is essentially y hat forall possible values of x. If we consider it at the x value thatwe're considering for this value, that would be the point here. We can decompose this expression into thetwo expressions on the right hand side. So first, he we have the residualwhich we can see here. And over here, we have this distance here,which is a little harder to see, so I'll label it, sothat's yi hat minus y bar. So we effectively decomposed thisexpression that does not depend on x into two different expressions that have to dowith the line now via the fitted values. So currently, this only considers onesingle point, so what we like to do is consider this left hand expression forall possible observations. Interestingly, if we wereto take the equation and square both sides, andsum them up over all possible values, you might notice if we do that over here therewill become this additional term here, but interestingly, this term goes to 0 andwe're left with this expression here. We have three sums of squares here andthese, each have a particular meaning. First, on the left hand side, we'll callthis the total variation, it does not consider the relationship between line xand we're only using the sample mean of y to summarize the y variable, sothat will be the total variation. In the middle here we havean expression we've seen before. This is actually the residual sum ofsquares as these are the observations and their fitted values. So we'll call this the error and lastly, here this last sum of squarewe'll call sum of squares for regression. Again these are all sums of squares, sowe'll call this the sum of squares total is equal to the sum of squares error plusthe sum of squares for the regression. And the way to think of this is wehave a total amount of variation. And that get's decomposedinto what's unexplained or the error plus what isexplained by the regression. So we can try to geta sense of this visually. For the total we're looking athow far away each observation is from the sample mean of the y value,so that looks something like this. So we'd be squaring allthe distances I just drew. So for the error, this is consideringhow far away each observation is from the regression line,so that looks like this. And again, we'd be squaring each of these. And then, the last one fora regression, we're looking at how far away the regression lineis from the mean of the y values. And that looks something like this. So for each point, you consider itspredictive value to the sample mean. So we started with a total amount ofvariability which is visualized here. But after we applied the regression model,we ended up with a lot less variability because we say that a large portion ofit which we see via the regression, is explained by the regression line. And so, we can actually quantify this,so what we would like is, we would like this sum of squares to belarge and we'd like this sum of squares to be small and we actually have a statisticthat quantifies this, that statistic is called the coefficient of determinationor most often we simply call it R squared. So R squared is exactly,if we go back a slide, a the proportion ofthe variation explained. So that is the sum of squares for regression divided bythe sum of squares for total. So that's the proportion ofvariation which we say is explained. So that's what we see here,it's the sum of squares regression, divided by the sum of squares total,we have that written out. But remember, the sum of squarestotal is equal to the sum of squares error plus the sum of squares regression. So we can do a little bit ofmanipulation and get here, so this is 1 minus, the sum of squaresair divided by sum of squares total. So if we think of this asa proportion explained. That's the same as 1 minus, what we havehere is a proportion that's unexplained. So again, we say that R squared is theproportion of the variation in y which is explained by the linear relationshipwith the predictor variable or x. So a few things again, it's a proportion. So R squared is the sum of squares regressions divided bythe sum of squares total. And it only deals withlinear relationships. We'll make a note of thatat the very end here. And we should also say that Rsquared can go between 0 and 1. One, if sum of squares errorhere would have to be 0, so essentially the points would haveto fall exactly on a line and 0, if we had complete noise and a linewould do nothing more than just a simple horizontal line would forexplaining the relationship. Visualizing that again, soI want to draw total line here and that looks like this andI want to draw error here. In this case here, the linear regression has clearlyexplained a way a lot of the variation. So we started with onthe left a lot of variation. And on the right, we are left witha much smaller amount of variation. We also see that on the left here, there'sa sort of obvious pattern where, for low axis, the horizontal lineis summarizing too high and for large axis, the horizontalline is summarizing too low. So this is an example of somethingthat would be high R squared. If we actually break this downa little bit further, too. So I have two different data sets, oneon the left, and one on the right here. On the left is what we just saw, andso the bottom left panel here is for the sum of squares total. So on the top,I have regression and error. And here we see that,just what we saw before, using this line, the variation is greatly reduced, andin this final panel here, I have both in solid orange the regression and dash bluethe error and we see that the regression makes up a much larger portion ofthe total line than the error does. So this is a high R squared, inparticular, this is an R squared of 0.92. This is something wewon't calculate by hand. We'll do it by using R eventually buton the right here, I have a different data set. So the bottom left panel on the rightshows the total variation and on the top if we began broken it downbetween regression and error and then finally, in the bottom right whenwe plot it all together we see that the total bars are,there's a much higher proportion of them, which is the dash blue,which is the unexplained. And again, remember that we're consideringsquares of all these, like here and here, that blue portion isgoing to make up the blue dash, that is will make up a much largerproportion of the variation there. So this would be an exampleof small R squared. In particular this is an R squaredof 0.19, so our left dataset the linear regression is explaining a muchlarger proportion of the variation. Just one more note that wementioned that R squared works for a linear relationship, soit tells us the proportion of variation explained bya linear relationship of x and y. We say this because if we look atthis data set here there's a very clear relationship in this data. It looks something like this but becauseit's not a linear relationship when we fit loose squares, we probably get a linethat looks maybe something like this. And this would result in Rsquare that's very close to 0. So while R squared is 0,so the linear relationship doesn't explain what's going on here,there is still a relationship. [MUSIC]"
stat-420,2,11,"[MUSIC] Here we have the usual simplelinear regression model. And again, we said that we couldwrite this conditionally for a known value of Xi. The Yi's are assumed to havea particular role as solution and then we said about estimating beta 0,beta 1 and same as squared but so far the way we did these wasusing a method of B squares. So first we essentially found the linesthat minimizes the square root errors and after doing so we have beta 0 half and beta 1 half which estimatedour two model parameters. And then we used thoseto obtain residuals and then we sort of average those toobtain an estimate for sigma square. In obtaining this estimateswe actually know or utilize the facts that wehave a probability model. So we didn't use the facts that we'reassuming the y value follow and normal distribution given an x value. So now we want to look at an alternativeway of finding estimates for the three parameters using the factthat we have a probability model. Given the model parameters and, in thiscase because we have a conditional model, a value of Xi,we have the density function for each Yi. Now also because we're assumingthat the errors are iid, and again the first i standing forindependent, and also here it's just the fixedsignal part of the model. The Yi's are alsoindependent under our model. When we have independent random variables, we can multiply their densitiestogether to obtain the joint density. So that is what we have here. This is the joint densityof the Yi's in this case. Now when we're talking about densities, we assume we know whatthe model parameter is. But now we want to go the oppositedirection and say, well, we know the data andwe want to find the mono parameters. So when we're doing that, when we'reconsidering data fixed instead of random like we would with densities,we have a likelihood. So, what we want is we want to findthe values of beta 0, beta 1 and sigma squared that give us a high likelihoodof obtaining the data that we saw. So essentially it's the oppositeof what we normally do when making probability calculations. Instead of making a probabilitycalculation assuming parameters are known. Now we're calculating a likelihoodassuming the data is known. So what we'd actually like todo is maximize this likelihood. We're going to find beta 0, beta 1 and sigma squared thatmaximized this likelihood. We've done a little bit of rearrangementof the multiplication that we've done here. And then this is a particularlyeasy function to maximize. So instead of directlymaximizing the likelihood, we'll maximize the log likelihood And thenfrom here it's fairly easy to proceed. We'll take derivatives with respectto each of the parameters and we obtain these three expressions. Setting of them all equal to 0 then. We obtain a system of three equations andwe have three unknowns. Now we should immediatelyrecognize two of these equations, these are the same two equations weobtained via the method of least squares. So our estimates for beta 1 and beta 0 are actually going tobe exactly the same as least squares, but now in relation to these twoestimates we also have a third equation. Which after plugging in our estimates forbeta 0 and beta 1 into this expression,we can solve and obtain a new estimate for the variance which we'llcall sigma square hat. The difference between the previousestimate, s(v) squared and this new estimate sigma hat squared,is somewhat minor. So we'll first see that they both havethe residual sum of squares within them. The difference is what we're diving by. In the previous approach,we divided by n- 2. And now we're simply dividing by n using the estimate foundin VM maximum likelihood. So there's a small, butsubtle difference here. So we're attempting to estimate sigma,the model parameter. So if we look at the expected valueof Se squared it turns out that it is exactly sigma squared andthis is what we call unbiased estimate. Whereas if we look at the expectedvalue of sigma hat squared. And we won't actuallyderive either of these. But just not that in this case,it is not equal to sigma squared, so we would call this a biased estimate. Further reasons we'll see later willalso have a slight preference for this Se squared. But for now just note that fora very large, and there wouldn't be a big differencebetween these two things. Overall we saw that maximum likelihoodwill give us the same estimates for beta 0 and beta 1 and sort of a very similar estimate forthe estimate of sigma squared. So now I'd like to make an attempt todraw this maximum likelihood approach. What I have here is some data. I've limited it to three particularvalues of x to sort of simplify it. And then we have someobservations at each of those. Now, if I consider this line andthen again in our probability model, we have a normal distributionabout the line at each value of x. So, I'm just going to drop forthe three that we have. This probability model that I have drawnIt's probably unlikely that the data that we see plotted here camefrom this distribution. We see that the mean at thisvalue of access over here but the data is all very far above thata similar situation over here to the mean of y is up here where the data down below. It's sort of a k forthe middle value of x but it's very unlikely that the data came from thismodel, so this will be low likelihood. Instead, let's consider this line, sothis is the mean of y for each x and now it's pretty good, that would begiving us a good beta 0 and beta 1. Now, what value of sigma do we use? That controls the shape ofthe normal curve that's at each x. So I'm just going to consider this x fornow. So if I draw this normal curve,we have to ask ourselves, does it look like that data codecame from that normal curve? Maybe, but maybe there's a few too many points outtowards the tails of the distribution, so maybe it should have instead beensomething like this, a little bit wider. And we plot that everywhere andwe see that it fits the data pretty well. So we would say that this data is sortof something we would expect to see from this probability model. So this is a high likelihood model. I have no idea if I've drawnthe maximum length of it here. But let's just illustrate that we'retrying to find the probability model that makes it as likely as possibleto have seen the data that we observed. [MUSIC]"
stat-420,2,12,"[MUSIC] So I still have stored here all thecalculations we previously performed to do a simple regression are, sort of by directly performing thiscalculations all the way here. But now we want to returnto the car's data set and you simply the way it wouldactually use it in practice with R. That's using the LM function whichstands for on your model, so the very first thing we'll do is use LMand we'll see here that LM has a very similar syntax to the plottingsyntax we've seen before. We're saying we want to use the carsdataset and then within that dataset we want to use the distance variable as ourresponse or [INAUDIBLE] variable and the speed variable as our predictivevariable as our X variable. So I'll run this line and store it ina variable called stop this model. And the first thing we do is justreturn what our we call that variable. And we see that it givesus two coefficients and this values look familiar to us. But if we print the valueswe calculated before, we notice these are more precisethan what has been printed here but note that that is just an artifact ofhow R is performing the printing here, we'll see that we can actually extract afull percentage numbers in a minute here, so we would like to sort of unpackwhat's stored in this variable here and this is one very simplecommand that we've run and stored information in this variablehere but there's a lot stored here and we'd like to extract some of that andsort of try to understand it. The first thing I'm going to dohere is plot the data again. And now we'd like to addthis line to the plot. And previously we saw we could useabline and we gave it the A and the B, the slope and the intersect. But now,if we give it the variable that we stored. The results of this LM command. It'll actually extract the A andthe B, the two coefficient and use that to add a line to the plot. In our environment window here we canactually see that this stop distance model is a list of length 12. So we can use the names command to seethe named objects of that list and a few of these should jump out at us rightaway as ones that we're interested in. We see here we have residuals. Fair values and also coefficients. Like we said when we simply print this,these aren't the full coefficients. If we actually want to get at thosevalues, we could use stop distance and then usual with syntax wouldextract that variable. So here we're looking atthe actual coefficient's fit. Simple linear regression. And similar we will extract the, for the values and the residuals,we could do a little bit of checking here. We could see that, if you tookthe cars data and the distance, which was the response andthe subtract of the fitted_values, that should give us the residuals again. So these values should be the same asthese values, maybe we'll even check that. We'll say are these equal to the residualsand we see that it looks like they're not. Here it is, here it is,here it is, here it's not. But this simply a minornumerical issue here. So if we instead use the all equalfunction, so do all that equal and the first argument is the residualsthat I calculated which are there and then the second argument is the residuals,from the model we see that they actually are all the same, there'sjust some very minor numeric issues so they're not exactly the same zero surein the computer but if we allow for a tiny little bit of tolerance there,we see that they are actually the same. We can extract those elementsby using the less syntax. There's also functions called coef,fitted and resid that will extract thosefrom the stored model as well. That's just a syntactical issue. Sometimes I find myself preferring these,to using the dollar sign syntax, over here I was checking that the Y values are equalto the fitted value plus the residual. Here we actually, interestingly,don't see that same numeric issue. We see that this indeed true foreach observation. So, LM will be probablythe function we use the most often. Another function that we'll use almostas often is the summary function. We can use the summary functionon an object of type LM and that will give us a lotof useful information. We'll eventually know whateverything here means, so first it first tells usthe way the model was fit. It gives us some summaryinformation about the residuals. Now it gives us informationabout the coefficients. So right now we recognize thisvalue here and this value here. We've actually talk aboutwhat all these are here. And then there's two more client is inhere that we have already discussed. One is the residual standard error,where's looks like this. So this is the square root of SW squared. So we see previously we havecalculated that, that was 15.39 sorry, 379 and that's roughly what we see here. And again, we can extract to this value,we'll do that. And here we see the R square which issimilar to what we calculate before. Again, R is doing some rounding andit's, as printing. This here is actually justwas being printed but summary of this other here actually SOSand initially call them names of that. There are some things here thatwe like to directly extract for example R squared and sigma. Sigma is what it's using tocall residual standard area. So for example I can extract R squared. And I can extract sigma,again sigma being what here it's calling the residual standard error and what wewould call the square root of SE squared. Another very useful functionwill be the predict function. This will take the place ofus directly plugging in X values to a fitted model andit will, instead, do that for us. Syntactically, it says okay, we want to predict usingthe model we fit on some new data. And sobecause we fit this model to a data frame, it expects that the newdata has the sameformat as the data this model was fit on. So I in-place here. I'm creating A data framewith a variable speed and simply one observationwith a speed of eight. This is just because when we call predict,it needs to know what to put into here. If I run this, it will essentially performthis calculation with an X-value of eight. You would sort of expect thatthis is probably vectorized, so I can give it a larger data frame so inthis case I give it three speed values, so when I call predict here it'spredicting for each of These values. Now, these two data frames that I'vemade only had a speed variable. If I gave it a data frame, for examplethe original car's data frame which had two variable speed and distance,they would automatically in a okay this is the X variable so I want to know makepredictions on each of this X values, so when I do that here, what I'm obtaininghere is essentially the fitted values. It's making prediction for each of the Xvalues that were already in the data. So that would be came to also performingthis count to that line here. So this would be in place creating thedata set with only the speed variable, but again our knows, because this model wasfit with a speed, as the X variable, it knows when I give the cars to onlyto look at the speed variable for X. Also, this is here is whathappens in default, if we don't provide a new data argument, it'll simplyreuse the original data and again. So, this is basically anotherway to obtain the fit values. [MUSIC]"
stat-420,2,13,"We have seen that we canuse our knowledge of simple linear regressionto fit a line to data in R. But now we willintroduce methods that will automate andabstract this process. In particular, we willintroduce the LM, predict, and summary functions. The LM function is usedto fit a line to data, while also storingadditional details about the simple linearregression model. The predict and summary functions will be used toextract those details. We will also beginto use R as a tool for performingsimulation studies. Instead of fittinga model to data, we will use R's functionsfor generating random observations to create datasets generatedfrom a true model. This simulated data willbe useful for verifying that methods we discussare working correctly. So after this lesson, you'll have two new tools youcan use for data analysis, fitting a simple linearregression model and performing simulationstudies, both using R."
stat-420,2,14,"[MUSIC] Now we'll use r to performthe calculations associated with fitting a line to data. So first we've been talkingabout the cars data. This is a default data set in R,so I can simply type cars, and the data frame appears. If we prefer a nicer viewer, we can usethe view command inside of RStudio and here we see the data thatwe've been working with. We see there are 50 observationswith speeds ranging from about 4 to 25 miles per hour, and then our responsevariable of distance is stopping distance. And also, we could use the documentationto learn a little bit more about this data set, in particularthis data was from the 1920s. We've seen commands here, structure,dimension, rows, and columns. Structure being the most useful, it tellsus there's 52 observations, 2 variables, and it gives us the names and variablesand a preview of their observations. But the best thing we could do to checkout this data would be to recreate a plot that we've seen a few timesusing the familiar syntax y to the x. In this case,using the variable names distance and speed, andreferencing the data frame cars. And then we simply use someadditional arguments to make the plot appearvisually a certain way. So now I'll do something that wewouldn't do in practice, but for the sake of matching expressions andequations that we've seen along the way. I'm going to take the speed variable fromthe cars data set and store it as x, and similarly for the distancevariable I will store it as y. So now I have variables x andy inside of R that I can work with. So the first thing we're going todo is perform least squares, that is performing this minimization hereand when we did that we obtained similar expressions to these here, the beta zeroexpression is what we've seen before. Beta one I've written a little bitdifferently, the previous way we've seen beta one was this expression heredivided by this expression here. They have alternative formsthough which I prefer, and makes the expression herelooks a little bit nicer. You can actually after seeingwhat we do here verify that this expression is the same asthis expression fairly easily. I'll quickly calculate all thoseheavily on vector as operations. So here I'm subtractingthe mean of every observation, every x observation that is makingthis part of this expression here. Similarly here, I'm looking at subtractingthe mean of y from each y value, that's this expression here. So inside of here,I'm multiplying those together for each value, and then summing those there. So this gives me exactly thisexpression here which we call s sub xy. So I can calculate allthree of those very easily. Again, heavily relying onvectorized operations, and then I can report those like this. So this is just a quick way toreport a vector in the council here. So then I can go about actuallycalculating my estimates beta one had and beta zero had, and I'll do that here andthen we will return those. And these are numbers thatwe've seen a few times here. This is sort of just going through, performing the calculations insteadof simply me writing them down and saying these are the answerswe'll return to the plot now. And now I want to add the linethat we fit here to this plot. So we'll use the abline function, it addsa line with intercept a and slope b. In this case,beta_0_hat is our intercept and beta_1_hat is our slope,so we can add that. And I use similar additional argumentslike I have in previous plots. Here, I'm changing the color andthe line width. That brings us to up to fitting a lineto data, again, we are throwing things in x and y to make our calculationshere match the expressions we've seen, but later we'll see how to simplywork with the original data. Once we have this fitted line,we can make predictions. We can use the fitted model now,we can get fitted values, and fitted values are the predicted yvalue for x values we've already seen. One thing we notice here this data thatwe have repeat a lot of x values so I can use the repeat function to extract onlythe unique values in this vector here. Also we can use this n operatorhere to check whether or not a value is in this list here. Obviously with such a small amount of datawe can pretty easily check, for example, that 8. Is in this data set, but if you hada bigger data set you might want to see, well, is this value in that data set? You can use the in operator. So here we see that 8 is indeed insideof the data we've already seen. So if I plug that into the expression for a fitted value, here is the fitted yvalue for an x value of 8, so we can also obviously make predictions whenthe value is not originally in the data. So here for example,21 is not seen in the original data set. But we also note that it is biggerthan the minimum we've seen, and it's smaller than the maximum we've seen. Now we can do that together with and, this is within the rangeof data we've seen already. So 21 is somewhere around here, so this isinterpolation, so we're okay with that. So again plugging into our fitted modelhere we predict a value of 65, so if we go back to the plot that's here and that'ssort of a very reasonable prediction. We could also try tomake a prediction at 50, but we see that 50 isoutside of the data range. So while we can make a prediction there, this is sort of a value that wedon't have a lot of confidence in. Now that we can make predictions,we can also get residuals. For example we like tofind the residual for this prediction we madeat an x value of 8. So first we need to see whichobservation had an x value of 8. And there's only one andit was the fifth observation. So we can find the y value byreturning just that observation. So it had a speed of 8 anda stopping distance of 16, we could have skipped this part by simplytaking this expression and subsetting. And again, we would have seen,again, stopping distance of 16. The residual is the true valueminus the fitted value, and so here we see the residual formaking a prediction at an x value of 8. And if we look at the plot, let's see,this is 10, 9, so this must be 8. And we see it's a positive residualas the point was above the line. Now, it sort of,this is something we would never do. We would never actually plug one value in,we'll see later that we can obtain fitted values and residuals veryeasily if we were going to do that and sort of just by doing the calculationswe'd say use vectorized operations. So here I can quicklyobtain the fitted values. For all of the x values, and then I'llstore them in a vector called y hat. Something I might do real quickis I'll plot the x values and the y hat values, and this shouldbe a line, which we see that it is. So obviously when we'remaking predictions on previously seen x values these are fittedvalues, and they fall exactly on a line. Which we can see indicatedby this line here, but I'm just sort of verifying thatit is indeed exactly aligned. Now that we have all of the fitted valuesstored in a vector, we can compare those to the actual live values and obtainthe residuals for every point very easily, so now we have all of the fittedvalues and all of the residuals. We said earlier that there are 50algebrations in this data, but instead of directly coding, say, the samplesize n as 50, we'll extract that by taking the length of the residuals,because there are end residuals. So we see again that that's 50. So then, we can use that tocalculate s of e squared, which was an estimate ofthe variance here in our model, applying this formula here inside of R,we obtain s sub e squared. Later we'll see that r will often directlywhat it calls the residual standard error, which is the square root of s sub esquared, so we'll calculate that real fast as well, and these are the twonumbers that we talked about before. We also talked about decomposingvariation, so we'll do that really quick. So again, we'll simply applyvectorized operations inside of r to calculate these expressions here. So here we're calculatingthe sum of squares total. Here we're calculating the sumof squares regression, we're using the mean of y andthe fit of values as we see here. And sum of squares error which islooking at the sum of squares residuals. So we also call this RSS forresidual sum of squares. And we use a vector to output those,and we see the three values there. We should note that if I takethe sum of squares regression, and add to it the sum of the squares error, we get back from it the same thingas the sum of the squares total. Because as we said, we're decomposing thevariation that those quantities add up. Also, sum of the squares error can beused to calculate s of new squares. So if we take sum of the squares error and divide it by n-2, we see that we getthe same result as we had previously. And then lastly, now that we've decomposedthe variation like this, we can very easily calculate r squared whichis the sum of the squares regression, divided by the sum of the squares total. I don't recall if we actually talkedabout this value for the cars dataset, but we see that 65% of the variationin stopping distance Is explained by a linear relationship withspeed in miles per hour. [MUSIC]"
stat-420,2,15,"[MUSIC] So at this point, everythingwe've done we've known the data. We have some data. We have data points (xi, yi). We have any of these data points. And we want to fit somemodel to this data. What we're trying to do is uncover sometruth that we don't actually know. And so for example, we want to maybetry to uncover the true relationship between the x and the y variable. We don't know the truth, andthat's what we want to find out. So what we did is we assumed a model. In this case, we assumed that y followeda normal distribution when conditioned on a particular x value. And the mean looked like this, andit had a variance of sigma squared. We've boiled the unknown down tothree parameters, beta0, beta1, and sigma squared. And what we did was, from the data well,we calculated beta0 hat, which is an estimate of beta0,the true unknown model parameter. We calculated beta1 hat,which is an estimate of beta 1. And we calculated S sub e squared,which is an estimate of sigma squared. But what if we knew the model,what if we knew the true model? So we knew the truth butdid not know the data. So what we would do now,given a true model, we want to generate or simulate data from that model. So for example,we could assume that, let's say, the truth is that condition on x,the mean of y is 5- 2x. And the true sigma squared is,let's say, 9. So here we have beta0 = 5, Beta1 = -2, and sigma squared = 9. We're saying this is the truth. Then, what we want to do is we wantto generate data from this model. And we'll do that via simulation,which R will be our tool for doing so. And we'll actually do this, eventually. So then, what we'll have is somedata set that looks like this. I don't know, I'm not good at simulation. I can't make up values very well. I'll let R do that, eventually. But we'll get some data, andthen what we'll do is we'll calculate from that data, beta0 hat,beta1 hat and S sub e squared. And then what we'll do is we'll say, well,how well do those values match the truth? And in this case, we know the truth. So we can use it as a way to check, well, does our procedureactually work correctly? Does the method of least squares andthe estimate sigma, the estimate for sigma squared, S sub e squared, are theyactually close to what they truly are? So here I just have a pictureto illustrate this, and we'll actually create this in R. But here,the dashed line is the line 5- 2x. Again, this is beta0, this is beta1. And then these data points in grey here, are scattered about this lineaccording to a normal distribution. So in this case, the Ei's followa normal distribution with mean0 and variance 9, orotherwise standard deviation 3. And so then, ignoring the dashed blueline which is the truth, and just using this data that we simulated from thetruth, we obtain the solid orange line. And this is a line, let's see, it's 4.84- 1.83x. So this is beta0 hat,and this is beta1 hat. And we can see that, well, they're notexactly 5 and -2, they're not too far off. And we'll verify this with simulation,we'll do this ourselves. And then we'll see what happens. Well, what if we keep this truth andthen regenerate the points about the line,what happens to these? And we'll see that, well, they vary. And then eventually, we can talk abouta distribution of the estimates that we obtain when fitting a line to data. [MUSIC]"
stat-420,2,16,"[MUSIC] Now we'd like to generate some of ourown data following the simple linear regression model. We would like to generate dataaccording to this model here, which we can think of as either the meanportion plus some random noise, or we can think of it asa conditionally normal model. To do this, we're going to considerthis sample here where the mean is 5- 2x value and we have a variance of 9. So, the first thing I'm going to do, and we're going to decide to generate21 observations from this model. I'm going to first settle some parameters. So we want 21 observations,the true beta_0 will be 5. The true beta_1 will be -2. And the true sigma, so I'm going tostore the center deviation, for reasons we'll see in a second, so that'llbe 3 which is the square root of 9. In this simple linear regression model,we say that the xis are fixed, so they are not random. So I'm going to decide ahead oftime what those are going to be. And I'm going to do a sequence of valuesfrom 0 to 10 and 21 of them along the way. So if I run this, we see that thisis a nice sequence of values, and I used 21 instead of 20 so that Icould have these nicely go up by 0.5. Now we have our x values. So the next thing we needto do is create our errors. And our errors are random and theyfollow a particular distribution here. So what I have here is code that willgenerate 21 hours observations from a normal distribution with a mean of 0 and a standard deviation of whateverstored's in sigma which is 3. We are using the rnorm function here tocreate random observations from a normal distribution. So R has a lot of these functions, placeyour favorite distribution name here, and R likely has a function that willgenerate random observations from that distribution. So one thing to note here is oftenwhen we talk about rnorm distribution, we talk about a mean and a variancewhich we call mu and sigma squared. R uses mean for the mean andstandard deviation instead of variance and it has the argument sd. What I'm going to do is I'm going to runthis line repeatedly and then I'll store it in variable epsilon, but because I'vewrapped it in parentheses it will print. So I'm just going to justrun this line repeatedly. I'm going to use Alt + Enter becauseit'll keep my cursor on the same line. And we see that each time it'sdifferent because we are, as we said, trying to generate random observations. But I would like to make sure that I knowwhat's going to happen here so, here, I've set a seed which essentially controlsthe state of the random number generation. So provided that I alwaysstart from this line here, I know what to expect the rest of the way. So if I run these two lines repeatedly,despite the fact that we're performing random numbergeneration, I always get the same results. So now, we have our x values andwe have our errors, so now we simply need to createthe Y values by adding together the, essentially, deterministic portion whichis just based on the true beta_0 and true beta_1 multiplied by x values. And then add to thatthe randomly generated noise. So we'll do that. So now we have x values that we decided, epsilons which are random whichwe generated using rnorm, and then the y values that ties it alltogether according to the specified model. That was the process of simulating data. So now that we have simulated data, we could now, well, fit the model to thesimulated data and see how well it works. So what I've done here is we'll use the tlm function to fit a linear model, and then we'll extract the coefficients. And we see that these aren'texactly the true values. But they're not too far off. So we believe that simple linearregression is working pretty well. We could also plot this andadd the real line. Now, I've done a few thingshere that I don't like. One is that I sort of have x andy stored as just variables in my global environment, and this plotis not particularly nice looking. So, I would like to clean some of that up. So the first thing I'm going to do isI'm going to write a function to perform the simulation, because simulation issomething that we'll often want to repeat quite a lot, so if I had a function to doit, it will make my life a little easier. So that's what I have here, this functiontakes as arguments a vector x and then true beta_0,a true beta_1 and a true sigma. It then determines the lengthof the x factor that we give it, then generates noise of the same length,mean, 0, and sigma according to the argument we give it,then combines that to create a y variable. And the other thing that it does thatis better than what we've done before, is it returns a data frame. So it will turn a data frame wherethe x variable is called predictor and the y variable is called response,which sort of just makes sense. So I'll store this function andI'll try it out. I'm going to set the sameseed that I did here, and so because when I run this,we'll get to the essentially same random number generation as I had before,I should get the same results, it will just look differently becauseit will now be stored in data frame. So when I run this,we see that sim_data is a data frame and it has two variables,predictor and response. And I think if we compare,let see, sim_data$response, should be the same stored in the yvalues that we had done previously and we see that they are the same. So now,instead of doing this which I don't like, which is fitting a linear model butwithout using a data argument. I can now recreate that, butusing a data argument here, and then the two names of the variables,and then we can extract the coefficients as we hadbefore and we see it is the same answer. Now I'll clean up this plot as well,so I will plot it like this and this is a plot that we've more orless seen before, I've just changed the data, I'm using somearguments that you've seen me use before. I'm going to add two lines to this plot. The first is the model fit to this data,so that was the result of this here. We see that I'm adding the line forthat fit to the simulated data, but then I'll also add the true line. So with the true beta_0 and the true beta_1, and we see that it'sdifferent but they're fairly close. And I'll actually also adda legend to that plot. We see that this blue line is the truth,the gray observations are simulated about that line according to a particulardistribution, this distribution here. And then the orange lineis fit to that data, the orange solid line is fit to that data. And we see that while it's close tothe truth, it's not exactly the truth. But had we had differentrandom number generation, this orange line would havebeen a little bit different. So if we repeated this processwith 21 new observations, we would expect it to be a similar line,but it will be a different line. So let's do that real fast. So I'm going to take some ofthe code that I've already written. So here is the code to generate the data,and again, we're keeping the x data the same eachtime, we're only creating new y data. And then I'll fit to that data. And then I'll create this plot again. Essentially, now what I want to do isI want to repeatedly make this plot. We simulate new data, fit to that data andthen recreate this plot. And let's see what happens. Now we see the blue line is the same. And actually, we notice thatthe axes changed a little bit. Because I'm going to makethis graph a number of times, what I want to do iskeep the y-axis the same. So I think points will vary roughlyaround -15 to let's say about 10. So now what I'm going to do is I'mgoing to repeatedly run this code. So the blue line isalways staying the same. Each time I ran it, I get a new set ofy values, varied about that blue line according to a normal distribution with amean of 0, and a standard deviation of 3. And then, because of the variabilityin generating that data, the fitted line will bedifferent every time. And we see that, it always fairly close tothe blue line, but it is varying about it. And you might notice a couple of things,which is, there's sort of more variabilityfurther away from the center here. And you might notice that it'ssort of equally above and below. So, sort of on average, it's correct but any given one is going to varysome amount from the true model. That will get us starting tothink about the fact that these estimates that we are deriving fromthe data are something that's sort of has its own distribution depending onthe true model that we're assuming. [MUSIC]"
stat-420,3,1,"When simulating from a true simple linear regression model, we began to see howour estimates for the model parametersare variable. In this lesson, we'llquantify this variability. First, we'll discuss the exact distributionof our regression estimates under the assumptions of the simple linearregression model. Instead of derivingthese results mathematically, we'll instead use R to complete a simulation study thatverifies the results. While simulation willallow us to generate many datasets to help understand the variabilityin our model, in practice we will onlyhave a single dataset. Because of this, we'll look at a statistic used inpractice that allows us to estimate the variability in our model usingthe available data, and how performingthis estimation affects the samplingdistribution. After completing this lesson, we'll have the mathematicalresults needed for two new methods thatwill develop this week."
stat-420,3,2,"In our discussion of the simple linear regression model, we said that there were three parameters: beta_0, beta_1, and sigma_squared. We said that beta_0 and beta_1 for-for the signal, they control the line. In particular, beta_0 is the intercept, beta_1 is the slope, then sigma_squared controls how much the observations will vary about that line. We can sort of consider that the noise parameter. Now, with some data, what we wanted to do was estimate those parameters. So, for beta_1 and beta_0, we used either the method of least squares or maximum likelihood estimation to obtain beta_1_hat, which was an estimate of beta_1, and beta_0_hat, which was an estimate of beta_0. We also then said that s_sub_e squared, which is this s sub e here, squared, is an estimate of sigma_squared. Now, in particular for beta_1_hat and beta_0_hat, we justify these via least squares by saying these are found by literally minimizing the residual sum of squares. But now we'd like to talk about some statistical properties of these two estimates. So before we do that, we quickly want to take a look at the estimate for beta_1. We originally derived this top expression here, and in particular, we said that S_xx is this expression here. Now, through a little bit of algebra, we can obtain this new expression here – we'll have a slight preference for this in the moment because, now, x-bar is just some number for any particular dataset as is x of i just some number for any particular observation. So, this entire expression here is just some number. The way we could think about this, we essentially have the sum of some constants times the y_is for each observation, and then this will be useful for us going forward. So now using this new expression, I'll rewrite the estimate for beta_1. And, at first glance, it might appear as if I've done it twice – once on the left and once on the right. But there's a subtle difference here, where I use a lower case y_i on the left, and a capital Y_i on the right. And I want to talk about the two ways we can interpret these expressions. So, on the left here, this is meant to represent a sample statistic, that is, this is an expression that I mean to apply to a particular dataset. So, I evaluate this expression on some data, and maybe, for example, for some dataset we obtain a beta_one_hat of, say, 5.1. So, that is, I expect this expression to evaluate down to a number. And that's why we call it a sample statistic because it was applied to data. On the right though, I wanted to now consider beta_one a random variable. As we've seen when we first simulated simple linear regression, if we simulate from the true model, our estimates will vary each time. So for a different dataset, we'll get a different value of the estimate. And to discuss this, we want to, instead of considering the actual data, consider the process. So, we predetermine x_i values and we understand then that the Y_is will be random, that is, according to the true model where the errors follow a normal distribution. That's why we want to now consider beta_1_hat and beta_0_hat as random instead of particular values for a particular dataset. Another way we would say this, we would say that beta_0_hat and beta_1_hat on the right-hand side are estimators, and on the left-hand side, when applied to data, they are estimates. An estimate is a particular value of an estimator for a given dataset. So when you apply the estimator to a dataset, you get an estimate. What we're doing on the right here is essentially we're saying, ""What values of beta_0_hat and beta_1_hat could we obtain through a random sample of size n according to the simple linear regression model?"" So now that beta_1_hat and beta_0_hat are random, we can talk about their properties as estimators. So, in this situation, when we are attempting to estimate the slope parameter and the intercept parameter of a simple linear regression model, we find that the least squares estimators, which are beta_1_hat for beta_1 and beta_0_hat for beta_0, are what we call the best linear unbiased estimators. Often when we will say that they are BLUE for best linear unbiased estimators. We already understand that they are estimators because we're considering them random and we're considering the process, not just applying it to a single dataset. But what do we mean by linear, unbiased, and best in that order? So to define best, we first need to discuss linear and unbiased. The fact that these are linear estimators is not due to the fact that we are estimating a line, it's due to the fact that they are linear combination of the random Y_is. Due to the rearrangement we saw earlier, we can write beta_1_hat here as a linear combination of the Y_is, where the coefficient in front of each one of them is given here, so that c_i we sort of defined earlier, and then this for any particular dataset will just be some number. And this is also true of beta_0_hat. So, beta_0_hat, if you remember, is, so capital Y_bar now again because I'm considering this random minus the beta_1_hat we already calculated or already considered an estimator times x_bar. We already said the beta_one_hat is a linear combination of the Y_is and then x_bar is just some constant, so this whole quantity here is also a linear combination of the Y_is. And well, Y_bar can also be written as a linear combination of the Y_is, simply putting a coefficient of one over N in front of each of the Y_is. So, this is also a linear combination of the Y_is. When we subtract a linear combination of the Ys from a linear combination of Ys, we can have a linear combination of Ys. So, we say that beta_1_hat and beta_0_hat are linear estimators. These two estimators are also what we call unbiased. In the strictest sense, that means that their expected value is the thing that they are trying to estimate. So, the expected value of beta_0_hat is beta_0 and the expected value of beta_1_hat is beta_1. So what does that mean? So if we consider a number line here, and we say that the true value of beta_1 is here, so if we take a random sample from the simple linear regression model, and then we fit linear regression to it, we obtain a beta_one_hat here. And then maybe we do the process again, and we get a beta_1_hat that's here. And we keep repeating that process, and we see something that looks like this. So maybe we see a bunch of beta_one_hats, and they sort of — they're never exactly the true beta_1, but on average, they're essentially correct. So, while any one beta – so this is some beta_one_hat for a particular dataset, so it's not correct. But if we repeated this an infinite number of times and average these, they would, on average, be estimating the correct thing. And we would generally like that to be true; we'd like the thing we're using to estimate a parameter to be, on average, correct. So now, we've discussed linear and we've discussed unbiased. So now, if we restrict ourselves to estimators that are both linear and unbiased, we will say then that the regression estimates we've seen via least squares are the best. And in particular, we define best to be the estimators with the lowest variance. We won't actually derive these variances and we also won't prove them, but so these are the variances for the two estimators, and under the conditions of the simple linear regression model, these are the smallest variances we can obtain. And it should sort of be intuitive why we like low variance. So, let's consider the sort of same set-up as we had before. So, here's beta_1 – so that's the true beta_1 – and then maybe we repeatedly take random samples from the simple linear regression model, and we obtain a bunch of beta_one_hats and maybe they look like this. So, we have a bunch of them and, again, they're sort of never really exactly the value of beta_one_hat, but on average, they're okay. So, we like that, on average, the beta_1_hats here are what they're trying to estimate. But if we maybe use a different estimator, so maybe the one I already drew was not least squares, but now, every time I generate a random sample of size n and now I'll use least squares to find estimate each time. So now my beta_one_hats maybe look like this. So, they are both, again, on average correct but they vary much less. So, we like this top estimator here because it's, on average, correct, but we prefer the bottom estimate – sorry estimator – to the top one because while it is also, on average, correct, it varies a lot less, and that's good because, again, in practice, we're not going to repeatedly get datasets. We're gonna get just one dataset and apply our method to it, while we're gonna resign ourselves to not always estimating the exact true value. Because of that, since we only get one of these, if we randomly pick one of these versus we randomly pick one of these, we have a better chance of being closer to the true value in the bottom case."
stat-420,3,3,"[MUSIC] So we'll return to thishopefully familiar picture. Again, so the blue dash dashlinehere is the true regression line, which we determined, actually notdetermined, but decided to be 5- 2x. And then we simulated observationsabout this line, according to a normal distribution with a mean of 0 andin this case, a variance of 9. So in this case, we have a simple linearregression model where the true beta 0 is 5, the true beta 1 is -2 andthe true sigma squared is 9. So we decided on some xvalues ahead of time. I believe these are equallyspaced between 0 and 10. And then we simulated randomobservations according to those random y observations according to the simplelinear regression model that we defined. So with these simulated values,we then fit linear regression to them. And then we obtain the solid orangeline here, which is the estimate. And in particular there, we found thatbeta 1 hat is approximately -1.83. Now -1.83 is obviously not -2,there's some variability here so we don't expect it to be right every time. But now based on our discussion ofthe Gauss–Markov theorem we believe if we repeated this process over and over again,beta 1 would on average be -2. So we sort of saw that when we didthe simulation set point regression we saw that each time wesimulated new y values and fit regression the slope andintercept changed a little bit. And soif we repeated that process over and over again on averagethis value would be -2. But each individual one will not be -2. And so since we understand now thatthere are some variability there we also wrote down the variability here. So the variance of beta 1 hat it turnedout was sigma squared divided by s sub xx. So sigma squared whichwe know in this case and s sub xx is a functionof the x values here. Which we've pre-determine essentiallyjust a quantity that says how variable are those things. So we know the mean of beta 1hat should be the true beta 1, and we know the variance, butthat doesn't tell us everything. We'd like to go one step further and say, well, what is the fulldistribution of beta 1 hat? It turns out perhaps unsurprisinglythat beta 1 hat also follows a normal distribution. So recall in the simplelinear regression model here, we said that the errorsare assumed to be independent and normally distributed with a mean of 0 anda variance of sigma squared. So that means that the Yi's are alsoindependent, normal random variables. And we said that beta 1 hat isa linear combination of the Yi's. And a linear combination of independentnormal random variables is also normal. So it turns out that beta 1 hat and beta0 hat also follow a normal distribution. In particular, for example, beta 1 hatfollows a distribution with a mean of beta 1 because again it's unbiased. And the variance that we had seenpreviously when we were discussing the Gauss–Markov theorem. And similarly for beta 0 hat it's again, anormal distribution with a mean of beta 0, because it's independent. And a variance that wehave seen previously. So if you're familiar with the theoryof normal distributions and how sums of independent normal andvariables behave, it might not be too hard to derivethe distributions we see here. But we would like to not dothat derivation and instead use a tool at our disposal, R, to sort ofjustify this without actually proving it. So to do that in the end, what we're going to do is we'll actuallycreate these two pictures here. But soour general strategy is going to be, okay, we're going to assume the simplelinear regression model. And we are also going to predeterminetrue values of the three parameters of that model. So then what we'll do is we'll decideon two things which are the x values. Which again, in the simple linearregression model are assumed known because this is a model that'sconditioned on the x values. And we'll also decide on n,which is the sample size, and then we'll repeatedly generateYi's under this set up. So for a particular sample size wellgenerate a number of different data sets. So we'll get particularvalues of these random Yi's. So for each of those data sets we'llthen apply simple linear regression and obtain a beta 1 hat. So for example, if we did that about10,000 times, what we see here this histogram is a histogram ofthose simulated beta 1 hats. And we can sort ofconsider the histogram as the empirical distributionof those things. So essentially each beta 1 hatis given probability 1 divided by the number of samples we generated orthe number of data sets we generated. And what we see here isthat in a plot form. Overlayed on top of that we see the truesampling distribution of beta 1 hat. So we've plotted the true normal curve,the density curve, for normal distributionwith a mean of beta 1. In this case 6 and a variance ofsigma squared which is 4 divided by s of xx which is a functionof the x values we have here. And we see that this true normal curve,the true sampling distribution of beta 1 hat, matches what weobtained via stimulation. So that tells us that this distributionthat we did not derive is more or less justifying. The plot on the rightessentially focuses on the mean. And this is sort of to indicate to us thatwhile this idea of using simulation to verify distribution rules only worksif we perform enough simulations. To sort of match the theory we would haveto sort of consider an infinite number of simulations, so all possible data sets for those x values witha particular sample size. But because we're not goingto run that many simulations, we essentially need to runa large number simulations. So here, for example, the true mean is 6. And We see that for a small numberstimulations, the empirical mean is actually already very close to x sowe only get sort of between 6.04 and 5.96. But we see that it sort of starts tostabilize around the true mean as the number of simulations increases. And we'll dive deeper into this when wesort of look at this exact situation in r. [MUSIC]"
stat-420,3,4,"[MUSIC] Now, we'll jump into R anduse simulation to justify these sampling distributions that we stated butdid not derive. So, here we have our familiarregression estimates we've seen before. But again, now we're understandingthat if we repeat this process, so obtain new data, and fit regression again,we'll get different coefficients here. And so now, we'll considerthe Yis as random variables, and thus the two regressionestimates are also random. And under the assumptionsof the simple linear model, we find that they have thesesampling distributions here. Essentially what we want to do is, we want to assume that the simplelinear regression model is true. And in particular, we'll say that we know ahead of timethe values of the three parameters. So we know the true model. Then we will generate the data fromthe true model repeatedly, and store our results andcheck if their empirical distribution matches the distribution here thatwe stated but did not derive. So in addition to knowing the threeparameters, the beta 0, beta 1, and sigma squared, we see here that the two samplingdistributions depend on n explicitly here in the case of beta 0 implicitlyhere, in the case of beta 1 hat. And also a sub xx because again, the simple linear regressionmodel is a conditional model. So the y is random, given some x values. So we'll need to also, additionally,decide on a sample size, and decide on a set of x values, we will keepthroughout the entire analysis here. So that's what I'll do. So I'm first going to set a seed sothat this analysis is reproducible and I will actually ahead of time know someof the numbers we're going to see. And I will determine that wewill use a sample size of 100. And again, soI'm calling it a variable sample size, but it is this n that we see here. And then I'll just create some x data. I've chosen to use a sequencebetween -1 and 1 and it happens to be equally spacedwith a hundred observations. And then so again, we'll need this Ssub xx here in the variance of the two sampling distributions sowe'll calculate that and store it. So, I'll also set up the three parameters. Because the normal distribution functionsin R standard deviation set of variance, I'll specify sigma, which isthe square root of sigma squared here. So, we'll have the 3, 6 and2, which are the 3, 6, and square root of the 4 that we see here. Okay, so now we haven't generated any ydata yet, but we actually already know all of the things we need to know to determinethe true sampling distributions here. So it turns out that both theseestimates are unbiased, so we have the means of both of them. We have them right here, 3 and 6. And then we just need to simply do somecalculation here to get our true variance of beta 1 hat andthe true variance of beta 0 hat. And because we have all theseintermediate values now, we can very simplycalculate both of those. So here, I calculate andstore the variance of beta 1 hat, which works out to be this value here. And I calculate andstore the variance of beta 0 hat here, which happens to be now this value here. And I wrote this down here, so the truesampling distribution of beta 1 hat is a normal distribution witha mean of 6 and this variance here. And the true sampling distribution of beta0 hat is a normal distribution with a mean of 3, and this variance here. We've written that out explicitly here. Now, what we're going to do is we'regoing to repeatedly generate datasets from this true distribution here. So I'm going to determine that we'regoing to generate 10,000 datasets. And what I'm going to do is I'mgoing to preallocate the vectors where we store the beta 0 hat for each ofthese datasets and the beta 1 hat for each of those datasets. One thing that you should never do insideof R when you're looping is grow a vector. So it's always a good practiceto preallocate the vector that you are going tobe storing results in. Growing the vector each time iscomputationally inefficient, because it will copy that previous vectorinto the new vector of the new length. So I'll go ahead and preallocate those. And then what I'll do is,I'll start this for loop running, so it'll take a secondwhile I talk through it. So what happens inside this for loop? So, we will generate noise accordingto the true sigma squared here. And so the noise always has a normaldistribution with a mean of 0, and that's what we're doing here. So we're generating random observations,we're generating 100 of them with mean 0 and a standard deviationof the true standard deviation. So then what we'll do is,we take that noise, we just generate it andwe add it to the true signal. So these x values are predeterminedbeta 0 and beta 1 were predetermined, or actually, they're justthe parameters that we decided upon. So we're just adding noisearound the true signal, and storing that as our generated y values forthis particular dataset. Then what we do is, we fit regression for that particular dataset that we'reconsidering at that time, store it. And then what I do is, I extract beta 0and store it, extract beta 1 and store it. So now, what's stored in beta 1 hatsare the fitted value of beta 1 for each of the 10,000 simulationsthat we performed. So now interestingly,if I look at the mean of those, it's incredibly close to the true mean. So we've essentially used the empiricaldistribution that we generated, the empirical distribution of beta 1 hatsit is, to verify that, yes, this mean is what we get when we continuallyobserve data from this model here. We also see that the variants ofthe observations we've generated is extremely close to the true variantsof that sampling distribution. But so far this is only verifyingthe mean and the variance, we haven't actually verified thatthe assembly distribution is also normal. You could have another distributionwith the same mean and variance but not actually be normal. So what I'm doing here is,first I'm plotting a histogram of the simulated beta 1 hats, so we could saythat this is the empirical distribution. Then what I do is on top of that, I adda curve for the true normal distribution. So this orange curve is not in anyway based upon these blue bars here; it's a normal curve for a mean withthe true mean, which is beta 1 and the true standard deviation, which is thesquare root of the sampling distribution, so using the square rootof this value here. And we see that these match rather well,actually. Now, we could go further andsay well, so for example, we could calculate probabilitiesfrom this particular normal distribution. So what I'm going to do is, I'm going to say, I want the probabilitiesthat a normal distribution with that mean, and that standard deviation is less than,let's say, 6.2, which is somewhere here. And I could also say,well let's use these simulator values of beat 1 hat andcalculate that probability using those. So what I'll do is first this here says,okay, which of those simulator valuesis beta1 hat or less than 6.2? And then if I course this to be a 0 and 1 which will automatically happenwhen take the mean of these. So this will be one proportion of thosesimulated values are less than 6.2, which we actually can't seethe value calculated from normal, we see that those are ratherclose to each other. And we can pick any value,maybe we can instead pick 5.5, so I put 5.5 here, and 5.5 here. And calculate those two again, andwe see that they are extremely close. So, we've more or less used simulation toverify, in particular, this result here. Now, the sampling distributionbeta 1 hat s indeed normal with the mean of the true beta 1 andthe variance that we see here. Now we've really only verified it forthis particular situation, but we could change these values to beany three parameter values we want, you use any x values,we want to use any sampling. Any sample size we want, and we wouldstill obtain the correct information. So, we could also repeat this process for beta 0,which we've already done via storing it. And we can check that well, the empiricalmean was very close to the true mean. The empirical variance was veryclose to the true variance. And we can, again, create a similar plot. So, the mean here was 3, andwe see that both the histogram and the true curve are standard at 3 andthey have a very similar shape, so now we've also verified this result here. So, we notice that thesevalues are close but not exactly the same, because the samplingdistribution what would happen if, considering every possible sample here butin the simulation study, for essentially time purposes, we'reonly considering ten thousand samples. So even with 10,000 samples,we can see we're very close here. But to really, truly verify this was all,you'd have to consider an infinite number of samples,which we wouldn't actually want to do. One thing we can do,though is I'll create a plot real fast. So here is, looking at the empirical meanof beta 1 hat, so looking at the mean of the simulated beta 1 hats, as a functionof how many simulations I had performed. And we can see that there's somewhaterratic behavior at first, but the more and more samples we see, thecloser and closer we get to the true mean, which is this horizontal line here. And while yes,it looks pretty erratic at the start, it's actually noting the axishere it's only 5.96 to 6.04. So we're already pretty close andthen the more and more simulations we get, the closer andcloser we get to the true mean. We can also make this plot for beta 0 hat. You can take a look at the codeused to make these plots. One thing I note that, I did a littlepre-processing of the plotting window here and changing the margins otherwise, this hat on the beta 0 here wouldhave not displayed correctly. It would have been cut off. [MUSIC]"
stat-420,3,5,"So now that we have the sampling distribution of beta_0-hat and beta_1-hat, we'd like to look at one more result before we move on to calculating interval estimates and hypothesis testing. So first, anytime we have a normal distribution, if we subtract the mean and divide by the standard deviation, we have now what's called standard normal, where we have a mean of zero and a variance of one. So the standard deviation of beta_0-hat here is simply the square root of the variance here and similar for beta_1 here. So we can take both of these estimators, subtract their means and divide by their standard deviations - and now we have standard normal. So with that, there's one issue, which is that within the standard deviation, we don't actually, in practice, know sigma-squared. So we're going to need to estimate sigma-squared. So recall that we use s sub e squared as our estimate of sigma-squared. So the way we can think of this now is we'll use s sub e as our estimate of sigma. So if here on the left we have the standard deviation of beta_0-hat and this is the true standard deviation of beta_0-hat, if for some reason we knew the model parameter sigma, this and x-bar and s sub xx are based on sample sizes and predetermined x values; so the only thing that we wouldn't know in practice is sigma. So when we estimate sigma with s sub e, we no longer call this a standard deviation, we call it a standard error. So a standard error is an estimated standard deviation. So we can do that with both beta_0-hat and beta_1-hat by simply using s sub e to estimate sigma in both cases. So these are two quantities that we can actually calculate, given a dataset. So these are what we will have in practice as sort of our measure of variability of the two estimates that we will obtain. So now, with those standard errors, instead of dividing by standard deviation in the standardization process, we're gonna divide by the standard errors. So while we don't know, say, beta_0 and beta_1, we're going to be doing things where we either assume a value for it or creating an interval for that value. So the fact that we don't know beta_0 and beta_1 is okay, but the fact that we don't know sigma will be a problem and that's why we're using standard errors here. So we - for example here, we want to know the distribution of this thing, because by virtue of estimating this parameter here, we're not dividing by the standard deviation anymore and it is no longer normal. It turns out it follows a t distribution with what we say is n minus two degrees of freedom. So - and that works for both beta_0 and beta_1 after doing this sort of new standardization procedure where we divide by the standard error. So there are two natural questions. And the first is well, why does this thing follow a t distribution? And the second is well, what is a t distribution? So first, on the left here, we have a definition. So if we have a random variable t and if it's a ratio of, on top here, Z which is a standard normal random variable and on the bottom, the square root of a chi-squared distribution with d degrees of freedom divided by d, then it turns out that this thing follows a t distribution with d degrees of freedom. So we write that like this. This reads t follows a t distribution with d degrees of freedom. So d here, the degrees of freedom, is the one parameter of a t distribution. So we'll just very quickly look at the justification for why this follows a t distribution. So again, here I have simply just repeated the definition of a t distribution which we'll need in a second. And also, one more fact that we'll need, which is that s sub e squared times n minus two divided by sigma-squared actually follows a chi-squared distribution with n minus 2 degrees of freedom. We'll potentially see chi-squared distributions again later, but for now, just know that it is a distribution with one parameter, the degrees of freedom. And in particular, this here follows a chi-squared distribution with n minus two degrees of freedom. So we'll start with the quantity that we are interested in and then just simply writing out the definition of standard error, we move forward. So the first thing we're gonna do is sort of the usual trick in math where we multiply by one. So this quantity here is one. I could have simply multiplied by sigma divided by sigma, but sort of by doing this, both on top and bottom here, these are the standard deviations of beta_1-hat. Okay. So in the next step, all I do is I simply swap places of two quantities here and we can do so freely. That's still equal. And then also now I will, to get to the next step, cancel out the square root of s sub xx quantities. And then in the next step, I'll also - so I'll square these two quantities here, flip them and then take a square root. And so after I do the flip, I need to take a - do division instead of multiplication to keep equality. And then I'll sort of use my trick of multiplying by one again and sort of put an n minus two here and n minus two here, so keeping equality again. And then what do we have? Well - so we said that this thing is standard normal, which we see here, but we said that this thing here follows a chi-squared distribution with n minus two degrees of freedom and we're dividing by n minus two and we have the square root in the division. So it turns out that what we started with here can be written as this, which is a ratio of a standard normal to the square root of a chi-squared distribution divided by its degrees of freedom. So we've justified that this expression here, which we'll see a lot going forward, indeed follows a t distribution with n minus two degrees of freedom. So now, this also works for beta_0-hat, but the more interesting question now is well, what is this t distribution? So what I have plotted here are three distributions. The solid black curve is standard normal. Again, standard normal is a normal distribution with a mean of zero and a variance of one. And then the two other curves are both t distributions with different degrees of freedom. So the dotted orange line - so the lowest curve here - is a t distribution with one degree of freedom, where the dashed blue line - so the slightly taller curve - is a t distribution with 10 degrees of freedom. So there's a number of things we should notice immediately. First, all three of these distributions have a mean at zero. So a t distribution with any degrees of freedom will have a mean of zero. Another similarity between the two t distributions, actually all t distributions and the standard normal, is that they are symmetric. So all of these distributions are symmetric about zero - the curve is the same on the left and the right-hand side of zero. The other thing we should notice, which is the difference between the standard normal and all the two distributions are out here in what we would call the tails. So the t distributions have heavier tails. So while a normal distribution has more of its density closer to the mean, the t distributions are more spread out. And how spread out they are is dependent on their degrees of freedom. So we see when we have only one degree of freedom - that's this curve here - it's more spread out than the degrees of freedom with 10, which is here. So as the degrees of freedom increases, the tail has less and less of the density and more and more of it is closer to zero. It turns out that as the degrees of freedom increases, the t distribution becomes more and more and more like a standard normal distribution. So we should quickly discuss some intuition of the differences here. So if we consider beta_1-hat, we subtract its true mean and divide by its true standard deviation, we said that this is a normal distribution with a mean of zero and a standard deviation of one, otherwise known as standard normal. If instead we took beta_1-hat, subtracted its true mean and divided by its standard error, this then follows a t distribution with n minus two degrees of freedom. So first, the similarities here. So both these things have mean zero. So on the top, since the expected value of beta_1-hat is indeed beta_1, this quantity, in both times, on average, is zero. Now over here - so when we divide by the standard deviation and it becomes standard normal - so it says that essentially, we can say, for example, 68 percent of the time, beta_1-hat will be within one standard deviation of the true beta_1. And all the other properties of standard normal apply to this quantity here. So for example, if we look at this plot here - and again, the solid black line is the standard normal. So most of the values of this quantity here should lie between negative three and three; so within three standard deviations. But now, if instead we consider the quantity on the right - so instead of the true standard deviation, we have the standard error now - essentially, this quantity is a little bit more variable because instead of using a true value, we're using an estimated value. So on average, we still expect this thing to be zero because of this here, but because we're a little bit less sure of what we're dividing by - so we're estimating the amount of variability - this overall quantity is a little bit more variable. And how much more variable is dependent on the particular t distribution. So just in general, we see here that out in the tails, there's a little bit more density for either of the t distributions. And again, that's just by virtue of using an estimate instead of the true value, but then the other thing that affects it is the sample size. So the degrees of freedom is the sample size minus two. So as the sample size grows, the degrees of freedom grows and the t distribution we're considering becomes closer and closer to standard normal. So as the sample size increases, we're a little bit more confident in our estimation here and that this quantity will become closer and closer to this quantity here."
stat-420,3,6,"Now that we can quantify the variability ofour regression estimates, we can use this to our advantage. In this lesson, wewill move from making simple point estimatesto interval estimates. First, we will create interval estimates forthe model parameters. We will discuss howthese are created, but also what they can tell us about our data in practice. In addition to estimatesof the model parameters, we will introducetwo new quantities that we would like to estimate. Again, we will createinterval estimates for these quantities and discuss how they are useful in practice. Of course, to actually performthe necessary calculation, we'll use R. We will return to the predict functionthat we have seen earlier and see how it can be repurposed for this new task."
stat-420,3,7,"[MUSIC] Now that we know the sampling distributionof our regression estimates, we can move from simply using point estimates towhat we call, interval estimates. So we can see the real line andsome true value of beta1. Now this is some unknown quantity thatwe would like to try to estimate. So we obtain some data, we apply thissimple linear regression model and perhaps we get a beta1 hat that's here. So we understand that we won'tnecessarily get exactly beta1 for estimate because somethingthat Sort of close. But we realize now that if we take adifferent sample of data, we might obtain a different beta one hat, ormaybe even something here or here or here. So if we continue to do this on average,these would be the true beta one. And we would then say,maybe take an average of these and that'll be our estimate for beta one. And it should probably be prettyclose except that's not what we'll do because we don't have multipleasymmetric to beta one. We'll simply have B1 butnow we have what we called the standard error of beta one hat which isa measure of variability of beta one. Now we can use this whichinformation from a single data set to tell us about the variability of beta1. And now instead of simply using a pointestimate, which is what we call this, which is a single number toestimate the unknown parameter, we now like to use what wecall an interval estimate. So, now, instead of saying we believe thetrue value of beta1 is our estimate, we'll maybe say something like we believe thetrue value of beta1 is in some interval. And we would like this intervalto have certain properties. It should be somewhat intuitive thatthe larger the standard error is, so the more variability in our estimatethe wider this interval should be. The less variable the narrowerthis interval should be. And then we'd also like thisinterval to be narrower or wider depending on how confidentwe want to be in our estimate. The steps that we'll do now is sort offigure out how to create this interval based on the distribution ofresults that we've already derived. So these interval estimates wouldbe called confidence intervals and they'll take this form here. It'll be an estimate which is the pointestimate or our single best guess, plus or minus some margin of error to account forthe variability. So the margin though can bebroken down into two quantities. One which we'll callthe critical value and one, which we'll call the standard error. So we already know whatour estimate will be. So say if we wanted to make a confidenceinterval for beta one, our estimate would be beta1 hat, our standard error wouldbe the standard error of beta one hat. And we simply need to say,what is this critical value here? And another thing to mention is wehave no control over beta one hat and the standard error beta1 hat. These are two values thatare simply estimated from our data. But we will see that we do havecontrol over the critical value. So the length of the intervalwill be somewhat determined for us by how variable webelieve our estimate to be. But we can also control itvia this critical value. The critical values will be determinedfrom the distribution of our estimate. So we saw that the quantity,say for example, beta one hat minus beta one dividedby this standard error of beta1 hat, follows a t distribution with nminus two degrees of freedom. So this probability statement here saysthe probability that a t distribution with n minus two degrees of freedom, is greater than Some valueis equal to alpha over two. So this value here is what wecall the critical value for a t distribution with n minustwo degrees of freedom. And this definition tells us thatthat value should be here, so I'll draw t sub alpha over two. I'll suppress the degrees of freedom. We'll just say that's sort of implicit. And this says that this area here then,should be exactly alpha over two. So we're going to eventually use this factto derive the actual confidence interval. You'll potentially see a lot ofdifferent notations for these things. But so here,I'm saying that this is a random variable, this is a random variable that has a tdistribution with [INAUDIBLE] degrees of freedom, andthis here is a critical value. So it's a value that hasthis particular property, where there's alpha over twoarea to the right of it. So, continuing with that setup, so if here is 2 sub alpha over two,in this area here is alpha over 2. We know the t distributions is likea standard normal r symetric, so we look at negative t sub alpha over two then the area here to the left of thatvalue should also be alpha over 2. So we're left with inthe middle here 1 minus alpha. So the area between thesethe critical value and the negative of the criticalvalue is 1 minus alpha. So we can write the followingprobability statement. Which says the the probability that a tdistribution with n- 2 degrees of freedom is between the positive andnegative critical value is 1- alpha. So this will be the probability statementwe use to create the confidence interval. So it just so happened that this quantity here,follows exactly this distribution here. So we can plug that intothe probability saver we just had. And we can perform soalgebraic manipulation within here and arrive at following probability statement. So this statement now says thatthere is one minus alpha probability that the true parameter betaone is between two values. So we would call thisone the lower bound and we would call this one the upper bound. And we see they takeessentially the same form, except one is the estimateminus the margin. And the other is herethe estimates plus the margin. And now this margin is the criticalvalue which is the critical value from a t distribution with Mi's2 degrees of freedom and dependent on the alpha value we haveright here and the standard error. So we should be clear here thoughthat in this probability statement, Beta one hat is a random variable. We don't have the value of beta1hat talking about a potential value of beta one have, after obtaininga data set of size n, if you run sampling. So, right now this quantityis a random variable, so you can make a probability statement. When we actually apply this method todata, we won't be able to do that, and we'll be explicit about that in a minute. So, the derivation we did herealso applies to beta zero. So, actually we've derived two conferencecircles, and we will call those 1- alpha times 100% confidence intervals forthe left-hand side here, beta 0. On the right hand side here, beta 1. So we can create the complements forbeta 0, but that will, as we'll see, sort of not be very interesting. But the confidence interval for beta1 willbe something that we focus quite a lot on. And again it's an estimate, plus or minus a critical value times the standarderrors we've seen several times before. So now we want to talk about thisidea of this percentage as applied to the confidence interval. So we say that we have a 1 minusalpha times 100% confidence interval. So this alpha is later what wewould call the significance level. And one minus alpha is whatwe call the confidence level. So there are three very commonvalues we would use here. So we'll talk about 90%, 95%,and 99% confidence intervals. So this would bea confidence level of 0.9. This should be a confidence levelof zero point ninety nine(0.99). A ninety five percent (95%)confidence level would correspond to a significance level of alpha equalszero point zero zero five (0.005). This one in particular is rather special. It will be the default in R. And this alpha of zero point zero zerofive (0.005) will come up quite a lot. Mostly for historical reasons but this will be sort of our go to ifwe don't specify anything else. Now we see that alpha here, which is determined from the confidencelevel affects the critical value. So, if we're using an alpha of 0.05 fora 95% confidence level, that means that alpha over 2 is 0.025. So our critical value would be herewhere this would be t sub zero point zero two five from a t distributionwith the correct degrees of freedom, and this area here would be 0.025. If instead, we maybe selecteda 99% confidence interval, that would be an alpha of 0.01. And an alpha over 2 of 0.00 05 and so we want less area over here,so may be that value was already appear as t sub 0.005 weonly have 0.005 area there. So, these will have the larger criticalvalue that's a larger interval. So, we see that for a larger confidence level,we'll have to have a larger interval. And that just sort of makes sense, to bemore sure of how often we're capturing the true value of the parameter,we're going to need a larger interval. So, the width of the intervalis based on two things. The standard error,which we don't control. That's simply something wecalculate from the data set and is an estimate of howvariable our estimate was. And the critical value, which we have somecontrol over via the confidence level and then the differentresulting critical values. Returning to the cars examplewe've seen several times, so this fitted regression line forthis particular dataset here, results in a Beta 1 hurt of 3.93. So this is a point assumingof that parameter beta1. Now instead we can calculatea 99% confidence interval and that results in an intervalestimate of 2.81 to 5.05. So we a lot of bound of 2.81 andan upper bound of 5.05. We could also write thisa slightly different way. We could say this is 3.93,our point estimate plus or minus our margin, which is 1. 12, the margin here being the widthof the interval divided by 2. and we'll see how tocalculate this in R two ways. One which is sort of automaticallybased on the object of type LM. And the other where we actually directlycalculate the standard error and critical values. Critical values will befound using the qt function. But so here, the first thing we should notice is thatthe value 0 is not in that interval. That we'll be somethingwe come back to but that should sort of tellus something right away. So if 0 was in this interval,that would tell us that, well 0 is a possible value of betaone instead of this Say 3.93. And if beta 1 is zero,there's no relationship between speed and stopping distance. So that should just be somethingthat we want to think about for now. So one thing we want to be careful abouthere is we have a 99% confidence interval. We might be tempted to, because ofthis probability statement here, say something like Well,there's a 99% chance, or there's a probability of 0.99, that thetrue value of beta 1 is in this interval. That is not the statement we should make. The statement we should actually make isthat we are 99% confident that the true change in mean stopping distance for anincrease in speed of one mile per hour Is between those two valueswhich were 2.81 and 5.05. So what I want to say is that we shouldnot attach a probability statement to these two numbers. So this 2.81 is not random. This 5.05 is not random. Furthermore the unknown valueof beta one is not random. There's just some true unknown value butwe don't know what it is. But it's not random in any way. So once we've applied the confidenceinterval to this particular data set, we have these two valuesthat are no longer random. So the procedure of creating a confidenceinterval works in this case, let's say 99% of the time. So if we repeatedly wentback to the 1920s and generated this data set again by drivingthese cars and doing this measuring. 99% of the time, if we applied a confidence interval tothat data, the true value of beta 1, assuming the Linear regression modelholds, would be in those intervals. But when we have a singleinterval like we have here, we can not make a probabilitystatement about it. Because neither the unknown parameter, or those two sample statisticsare something that are random. [MUSIC]"
stat-420,3,8,"[MUSIC] Now that we have the general framework forcreating a confidence interval, we can actually createa confidence interval for a number of things in the simplelinear regression model. So we can talk about the mean response. Again, recall that the simple linearregression model is a conditional model that is for a given value of X. The line portion ofthe regression model or the single portion isthe expected value of Y. And previously we said, well,get our predictions for beta 0 and beta 1, beta 0 hat and beta 1 hat. And then that is our estimatedline which is an estimate of the single portion of the noise. And we use this as our best guess forthis. I've modified the notation here slightlyto indicate that y hat, the predictive value of a particular value of x isindeed a function of that value of x. So here we have the true mean of Y fora particular value of X, and here we have the estimated mean of y fora particular value of x. And well,we know that beta 0 hat is variable and will often not be the true value. We know that beta 1 hat is variable andoften will not be the true value. So this predictive valueof y is also a variable. In particular,it also follows a normal distribution. So we say that the expected value of y hat is exactly the thingwe're trying to estimate. So it is unbiased, we like that. We could derive the variancewhich we see here. But then so beta zero hat isa linear combination of the y's, beta one hat is a linearcombination of y's. So this whole thing together isa linear combination of the y's. So somewhat unsurprisingly, we run intoa normal distribution again with the mean that we're trying to estimate andthe variants that we have here. So, now we again havethis thing that's normal. And if we estimated the variance andtook a square root, so that it is used S sub u squared to estimatesigma squared take as square root. That would be the standard error, and we would arrive at the t-distributionthat we've seen before. So we could use that information tocreate a confidence interval for the mean response. So essentially,we're making a confidence interval for the expected value of y,at a particular value of x. And this takes a familiarform we have our estimate. We have our critical value, which againhappens to follow a t-distribution with n-2 degrees of freedom, andwe have our standard error,. The thing you might notice now isthat the standard error actually depends on the value of x at whichwe are interested in the mean of y. So depending on the valueof x we're predicting at, we're going to havea different standard error. And in particular, the further awayfrom the sample mean of the x's, the larger the standarderror is going to be. So now back to our familiar cars data set. And we're going to be interestedin the mean of y that is the stopping distance when x is 21. So first, we want to calculate y hatthe predictive value when x is 21. So we would plug in to the regressionline, we would obtain the value 65. So that's the value hereon the regression line. So that's our point estimate forthe expected value of y when x is 21. But we've said previously, well,the beta 1 parameter is variable, and the beta 0 parameter is variable. That is our estimates of thosetwo parameters of variable. So this point here, well, is also variabledependent upon the data set we see. So we like to createan interval estimate for this. So if we apply this procedure here, and in particular we will calculatea 99% confidence interval again. We obtain an interval estimate forthe expected value of y, given x is 21, of 56.52, 73.5. So if we draw that on this picture,I think 73 should be about here. And there's some metrics, so56.5 should maybe be about here. So this is an interval estimate here for the expected value ofy given that x is 21. Later we will see the mechanics ofhow to obtain these values in r for different percentages ofthe confidence interval. But one thing we should notice right awayhere is that this is an interval for the mean of y. It is not an interval that issupposed to capture potential values. So we see here that maybe sort ofextrapolating a little bit here, we could potentially see values out here. And we could potentiallysee values out here. And notice that our intervalis not capturing that. What our interval is capturing is where wethink this regression line would be for an x of 21. So the next thing we'll do is talk aboutan interval that will capture all of the potential observations fora particular value of x. [MUSIC]"
stat-420,3,9,"[MUSIC] So now we know how to createan interval for the mean of y and particular value of x. So, when x was say 21,our point estimate was here and we had an interval estimate thatmaybe would look something like this. So, it's symmetric router point estimateand we said that we were maybe in this case, 99% confident that the truemeaning of y was within this interval. But this is only forthe expected value of y given that x = 21. What if instead of just for the expectedvalue we want an interval that captured possible observations at x is 21? So while maybe with we believein the means reasonable, it's totally reasonable that we seeobservations scattered about that mean more base on this datathat we're seeing here. So maybe an interval will looka little bit something more like this in order to capture not just the mean butnew observations at an x value of 21. One thing that should maybe be intuitiveright away is that this value here is still our best guess for new observation. But now, there's more variabilityin new observations than there is in the mean at a particular value of x. So this new interval that we'd like tocreate is called a prediction interval for a new observation. And it follows sort ofthe usual confidence interval form where we have ourEST plus or minus a CRIT value, which will again be from tdistribution with MI to freedom. And then lastly,over here we have what we call a SE. And the standard errorshould look rather familiar. It's very similar to the confidenceinterval response with minor difference, which is this one right here. To understand where that 1 comes from,we want to compare and contrast a prediction interval andconfidence interval. So, here is the simplelinear regression model. Now, a confidence interval for the mean response, essentially is tryingto make an estimate of this here. We want to a confidence interval forthis part of the model. Essentially we wanta confidence interval for the mean of y at a particular value of x. In other words, we're trying to createa confidence interval for the signal. Now what a Prediction Interval seek to do,is capture all of these here. So a Prediction Interval is notsimply interested in the signal but also how much the points varyabout that according to the noise. Now since the expectedvalue of these errors is 0, estimating this as well will not changethe estimate or prediction that we make. But it does add some variability, becauseagain we said that these follow a normal distribution with a varianceof sigma squared. So we'll continue to use y hat ofx to estimate this portion here. Except now we'll considerthe variability of y hat of x as well as these random errors. So, the variability when we add thosethings together is the original variability of y hat,plus the variability of these errors and that is exactly sigma squared. So, we arrive at this new variance andwhat you know y hat we said was normal, these errors alsofound a normal distribution here so this new quantity here alsofollows a normal distribution. So everything we've done beforewhere we have the variance we estimated what the standard error and then we'll arrive at T distributionlike we have seen several times before. So now we'll apply this procedure here,a prediction interval for a new observation to the cars dataset we seen several times before. So what I have drawn here alreadyis the confidence interval for the mean response wehave already seen before. So we're 99% confidencethe true mean of y for an x value of 21 isbetween these values here. But now what we'll do is calculatea 99% prediction interval. So for a new potentialobservation at an x value of 21. And it turns out that is, let's see, 22.9 to 107.1. So let's see here, 107 is maybesomewhere all the way up here and 22.9 is maybe all the way down here. That's my best attemptat getting those values. But we see this is a much largerinterval than the confidence interval. So the prediction interval again, we want to be 99% confident that a newobservation will fall in this interval. And from the data we see herethis also seem reasonable, there is a descent modelspread about the sign, so this interval should be fairly largeto capture a potential new observation. And again we see that theyshare the same point estimate, it's simply that the predictioninterval is wider due to the extra factor of sigma to deal withthe errors about the line. Just recapping that,that's what we see here. So this first line here is fora confidence interval this second line is here for predictioninterval is this extra 1 here which is a result of back here thisextra factor sigma squared for the error terms aboutthe predictor regression line. So sometimes we'll like to create a plotthat looks like this that shows both the fitted regression line, the confidenceinterval for the mean response for any particular value x, as well as predictionintervals for a new observation. So for example at an x value of21 that example I keep doing here is our predictive value so the confidence interval formean response is from here to here. And a prediction interval for a newobservation is between here and here. And these bands were drawnwith 99% confidence, so it's a 99% confidence interval anda 99% prediction interval. So again, what we saw is that the predictionintervals are much wider everywhere. So the prediction intervals forany given x, we're 99% confident that a newobservation will fall in there. And a couple of the things we shouldnotice here is that it's sort of subtle, but the confidence interval aswe get further away from x bar, which is marked here. This is the point x bar, y bar. So, the further we get from this,the wider the confidence interval is. And this is true ofthe prediction interval as well. It's much harder to see here, and that'sbecause this extra factor of 1 here sort of drowns out the differenceis happening here. As we move further away from x bar,the standard error grows. So the effect of that is greaterin the confidence interval. But in both cases, the interval is at itsnarrowest when we're predicting at x bar. And the further we get away from that,the wider the interval gets. And that just sort of makes sense. Because over here, there's sort of less data nearthe point that we're predicting at. But at x bar there is sort ofequal data on both sides of it, so we're sort of more confidentin our estimates there. [MUSIC]"
stat-420,3,10,"[MUSIC] Jumping back into R, we'll now useresults from a model fit VLAM, to automatically obtain all ofthe interval estimates that we just saw. We'll see three new functions,confint for confidence intervals, qt because not only will we generatethe intervals automatically, we'll sort of verify how thesecritical values are being obtained via the distribution results we already sawand I suppose it's not a new function but we'll use the predictfunction in a new way. We'll return to a familiar dataset the cars data set and we have fit this regression before so here are theestimated coefficients here and we'll also just refresh our memory with this plotthat we've seen a number of times here. The first estimates we want to do wereinterval estimates for beta zero and beta one andwe'll just focus on beta one here so the way to do that is withthe confint function. What I have here is I want conferenceintervals for the model that I fit and I want them to be at 99%, so I wanta level 0.99 so it's a confidence level. I do that and I see it actuallyreturns two estimates, or two interval estimates that is. I guess four estimates if youwant to think about it that way. So here I have an interval estimate forbeta zero and here I have an interval estimate forbeta one. We see that we have these values 0.5% and99.5% so we notice that between value thoseare 99% and there's 0.5% above this and 0.5 below this sothat is exactly a 99% confidence interval. One thing you might want to do hereis extracts particular values here. And we will note that thisis actually a matrix. I can do something like this,where this will extract this value here. The other sort of commonthing we might want to do, is only obtain an interval estimate forone of the parameters. The syntax for that, we could use this param argumentto the confidence interval function. So here, I am getting the confidenceinterval for the intercept. Here, I'm getting the confidence intervalfor the slope which is the coefficient for our speed in this case. I have been using level of 0.99 so99% confidence interval. I can change this to anything I want,for example 0.95 and we'll notice that I will get,as we expect, a shorter interval. So we see that the lower bound is greater,and the upper bound is smaller. So switching back to the 99%confidence interval, now, what I'd like to do is verify thisautomatically calculated interval, with the mathematics thatwe sort of derived earlier. So first recall that the summary commandgives us a lot of information about the model that we fit. In particular one value of interest to mehere is here we have the standard errors for both of the parameters. This 0.4155 here is exactly this standarderror for beta one that we see here. We can quickly verify that too. Well actually first of all, verify thewhole interval and then I'll come back and verify the standard error. Actually I'll do a littlebit of both along the way. First notice that I am interested in betaone hat so I need that coefficient and I just want to store it in a variablethat sort of more illustrative and then I want to extract this value here. So to do so I'll notice that within thesummary information I can directly extract this table here by doingdollar sign coefficient. And then this is a matrix that I canextract to whatever value I want for example this one is the second row andsecond column. And I'll store that in beta one hat c forthe standard error of beta one hat. We'll also look at that value, which we'veseen before, is 0.4155 and some change. We'll also verify that it is s of e,our estimate for sigma divided by the squareroot of s of xx. First I would need to calculates of xx for this data, and then we've seen before this s of e,otherwise known as residual standard error, is something that isgiven to us in the summary information, actually, right here,as the residual standard error. And r calls this sigma, so it's the sigma which is storedinside the summary information. So that is s of e. So I'll store that in s of e. And then the standard error is s of edivided by the square root of s of xx. And we see that that matches what rwas automatically calculating for us. So the other thing we want to do iscalculate this critical value here. This is a critical value for a t distribution with n minustwo degrees of freedom. And we want alpha divided by two areato the right of that value on that two distributions. And that's exactly whatthe qt function does for us. Again, this is based on alpha whichis one minus the confidence level. So, that's we have here. And because of the two sided interval and it's symmetric, we want half ofthis area to be on either side. So, we'll divide this by two. We want a value such that there's0.005 area to the right of it. But r thinks to the left often, sothat's where I get this 0.995 value from. So this going to give me the quantile forour t distribution for a probability 0.995 andthat matches what we're seeing here. Maybe I should say here forthe speed coefficient. And I need to give it the properdegrees of freedom so I can obtain n quickly using the numberof rows in my data set and subtract two. When all that is said and done my critical value is about 2.68 andI will store that in the critical value. I have my estimate, my critical value,and standard error so, I could obtain my lower bound here,I could obtain my upper bound here or I could return it all togetheras a vector, like so. And if I go back and look at that[INAUDIBLE] four beta one again, we see that it is indeedcalculating what we thought. So for a quick reset of the console here,we'll varify how qt and pt work together. So again, both these functions are for thet distributions, because of the t in them, q is for quantile, p is for probability. This portion of the code here is whatwe use to get our critical value. So this says this is a value suchthat this is the area under the curve to the left of this value. The curve being a t distributionwith this degrees of freedom. What we should get back then is, if wecall pt on this value here with the same degrees of freedom, we expect to get backexactly this value here which we see here. And if we do one minus this, this wouldgive us exactly the alpha over two here that we were using for the critical value. The two other interval estimates wemade were confidence intervals for the mean response andprediction intervals for new observations. The first thing I'm going to do is,I'm going to quickly make a data frame. And this data frame onlycontains one column. And soone variable speed with values 5 and 21. I believe we've seen this before. And I'll quickly store that anda variable called new speeds. What we'll do now is we'llcreate point estimates for predictions at these two values. I'm actually not surehow this level got here. We'll see why that's there in a second. But so,this code we essentially have run before. So we want to predict usingthe model we fit and this new data. These are the predictive values forx is 5, and x is 21. And, there should have beenno level attached to that. But sonow if we take this predict function and add one argument whichis called interval and we say we want confidence, what it willcreate are confidence intervals for the mean response at the valuesthat we are giving it here. So if I run this,we see here is a confidence interval for the mean response which is stoppingdistance at a speed of five. Here we see the forthe confidence interval for the mean response when x is 21 andthose are both at a level of 0.99. So those are 99% confidence intervals. So we see the lower and upper,we're also given this column here for fit. And we can see that these hereare exactly the point estimates. This is the point estimate and then thisis the point estimate minus the margin this is the pointestimate plus the margin. That gets us confidence intervals forthe mean response. The only thing left to do then iscreate prediction intervals for new observations, and so the code forthat looks remarkably similar to this, the only thing we change is we movefrom confidence to prediction. Everything is exactly the same. But I run this code,the results look remarkably similar. We actually again see the same pointestimates, because, again, estimating a mean, estimating a new observationwe use the same point estimate. But now because observationsare more variable than a mean, we see wider intervals thanthe confidence interval for mean response. This is an interval forthe mean of y at an x of 21, this is an interval that we hope tocapture new observations at an x of 21. And again this is done ata 99% confidence level. We could change that and again so if we're less confident wecan get a narrower interval. The more sure we are we need a widerinterval to actually be more sure. The last thing I want to do isquickly create a plot that sort of demonstrates these. So these sort of just look likenumbers to us right now but it's sort of often moreillustrative to look at a plot. And we also see that this predict functionis already working in a sort of vectorized manner so I gave it a data frame here andit had two rows in it. And I get two rows of output here. So what I'm going to do is, I'm going tocreate a grid of possible x values between the minimum x value we observedand the maximum x value we observed. So this is just a big gridof potential x points. So I'm going to makeconfidence intervals and prediction intervals ateach of these values. So to do that I just takethe code I had and now, instead of giving it this data framethat only had a couple values in it, I give it this new data frame thathas all of these values in it. So here I'm storing confidence intervals. And now here I'm storingprediction intervals, so if I look at what's stored there. So here are confidence intervals ateach of those x values, I give it. There's quite a lot of them. But each time I have an estimate andthen the interval. And same for the prediction interval. So now what I'd like to do is goabout the business of plotting those. One thing that I'll needto do right away is so because the intervals are going to be muchbelow this line here I'm going to need to adjust my y limits, because I know thatthe prediction interval is going to be where the bands are sort ofthe biggest and smallest. I'm going to adjust my y limitsto be based on the smallest of the prediction intervals andthe largest of the prediction intervals. So I create a new plot andit looks something like this. So this is the same data, I just havea slightly different y axis here. And then what I'll do is I will create aline for the x values and I'll extract so what I have here is these are the lowerbounds for each of the x's I saw there. I'll give it a color, I'll give it a widthand I'll give it a particular line type. So here I'll add the lower bound forthe mean response and here I'll add the upper bound forthe mean response. And then here I'll add the predictionintervals respectively. And the last thing I'lldo is I'll add a point, which is the x bar comma y bar point. And I'll zoom in on this plot here. We've seen this plot before. But again, sowe see that the confidence intervals, they're sort of narrowestright around x bar. And the further away we get from x bar,the wider those intervals are. And we see the same thing for theprediction intervals, it's just much more subtle, the further away from x bar weget, the wider those intervals are. We also see that the 99%prediction intervals capture almost all of the datathat we observed here. Whereas the confidence intervals forthe mean response, they don't contain a lot of the data butagain, these are just intervals for the mean at a particular x value,not a new observation at each x value. [MUSIC]"
stat-420,3,11,"When creating interval estimatesfor the slope parameter, we hinted at how we could use this interval to determine if there is a relationship between the predictor and the response. In this lesson, we'llformalize this idea by introducing hypothesis tests for simple linear regression. Because there is variabilityin our estimates, our relationship betweenthe two variables might appear just by chance. We will briefly recap hypothesis testing and see how to apply it toregression models. In particular, we'll introduce the significance ofthe regression test, which will allow us to make decisions about the regression, despite the uncertaintyin our estimates. After completing this lesson, you'll be able to perform hypothesis tests forthe regression using R and interpret the resultsto make real-world decisions."
stat-420,3,12,"[MUSIC] Now that we have the sampling distributionof our regression estimates in addition to interval estimation. We could also perform inferenceotherwise known as hypothesis testing. In the regression setting,hypothesis testing will more or less follow the same procedureas you've probably seen before. We just need to talkabout a few specifics. In most hypothesis testing procedures, we first have to overallassume a probability model. And in that case that'll be exactlythe simple linear regression model. And then we're going to makehypotheses about this model. So here we're going to hypothesizevalues of either beta 0 or beta 1. So we'll perform testing about the beta0 parameter or the beta 1 parameter. We'll be more interested in the beta 1parameter, but we'll see the testing for beta 0 would be remarkably similar. From there, essentially, the procedure is what you've likelyseen before in hypothesis testing. We'll develop a test statistic whichyou've basically already seen before. But we'll discuss the distributionof that test statistic, which, as you might guess, is t. And then to carry out the procedure, we simply need to select a significancelevel and then calculate a p-value. So now we'll talk through each ofthese steps in the simple linear regression model. So, just a refresher, here's the model. And in particular,our errors follow a normal distribution. And now what we want to do iswe want to make inference about one of two model parameters here. So either beta 0 or beta 1. Now to do so, we'll have to createhypotheses about those parameters. Here we have written two different tests. So for example, here we're talking abouta test about the beta 1 parameter. So here we have the parameterthat we're testing about. And here we have what wecall the hypothesized value. Beta 10, essentially this is some specificvalue of the parameter that we are going to assign to what we call on this lefthand side here the null hypothesis. Whereas on the right hand side thealternative hypothesis is what we would call, and this will allow forany other value of that parameter And again, we will call thisthe alternative hypothesis. In this notation we'll use H sub 0 torepresent the null hypothesis, and H sub 1 to represent the alternative. Now to actually carry out these tests,we'll need to first calculate a test statistic, and the test statisticwill look like a test statistic for most other hypothesis tests about means. Where we will say the teststatistic is the estimate minus the hypothesized value dividedby the standard error. So for example here we havethe test statistic for this test here about beta 1, sowe see our estimate of beta 1, beta 1 hat, our hypothesizedvalue which again we can pick. This will be some specific value of theparameter divided by the standard error. So first, on top here we have how far away is ourestimate from our hypothesized value? We would expect it to be close, hopefully,and then we divide by the standard error. So we sort of normalize accordingto the variability of our estimate. We can also talk about the distributionof the test statistic. If we assume the null hypothesis is true,so under the null hypothesis the true value ofbeta 1 is this beta 10 here. Well back here then, that means the expected value ofbeta 1 hat is exactly beta 10. And then dividing by the standard error, well then this thing followsa distribution that we've seen before. Which is what we see here, it follows a tdistribution with n- 2 degrees of freedom. And we see that that'll hold forbeta 0 as well. So in both cases for both parametersthe test statistic assuming the null hypothesis is true, that iswe know the true value of beta 10 or well we're assuming thatthe value of beta 10. So under that assumption both these teststatistics follow a t distribution with n-2 degrees of freedom. Under the null hypothesis,we expect beta 1 hat to be often very close to beta 10 andvary according to the standard error. When we have our test statistics thenwe expect, often if the null hypothesis is true we expect the value ofthe test statistic to be close to 0. And for example, we would rarely expectto see values of test statistics. In this case, this happens to be a tdistribution, 10 degrees of freedom. So if we had a sample size of 12,it would be really rare that we see a value of the test statisticsay somewhere approaching the value 4. So we'd like to quantify this ideathat we don't expect to see values of the test statistic out in the tails here. But we often do expect to see values ofthe test statistic close to 0, that is estimates close to our hypothesizedvalue under the null hypothesis. So the thing that does that is a p-value. And while p-values don't havethe best publicity these days, they're still a very useful concept,provided you understand the concept. So a p-value is a probability,p for probability. So what it is, it's a probability,assuming the null hypothesis is true. So this probability is obtained whenwe assume the null hypothesis is true. And it's the probability that we obtaina value of the test statistic that is the same as what we observe, ormore extreme that what we observe. Let's unpack that a little bit. So say we observe a valueof the test statistic for testing beta 1 of, let's say, 2.5. Now everything to the right of 2.5is a value that is more extreme. So more extreme is something that'sfurther away from what we expect, which in this case is 0. When the null hypothesis is true,for example, we expect beta 1 hat to be beta 10, andthe test statistic would then be 0. Now there's also some morevalues that are more extreme. So 3 is more extreme. But also -3 would be moreextreme because again that's further away from what we expect. So if we look at, say, -2.5 here, theseare also values that are more extreme. So this area under this curve now, which again is a t distribution with n- 2degrees of freedom is exactly the p-value. It's the probability if we assumethe null hypothesis is true as obtaining a value of the test statistic that isas extreme as we saw or more extreme. We can write this mathematically then, so on the left hand side we havethe probability that this particular t distribution is less than negativethe magnitude of the test statistic. And then over here on the right hand side,we have plus the probability that that t distribution is greater thanthe magnitude of the test statistic. But what you might realize is that whilewe've said that a t distribution is symmetric, so really, we could focus thenjust on the area over here on the right, and simply multiply by 2 becausethe area on the left is equal. So the p-value would be 2 timesthe probability that the particular t distribution is greater than the magnitudeof the test statistic away from 0. So now we have the p-value forthis particular two sided test. One thing we need to be careful aboutis that this particular probability is not the probability thatthe null hypothesis is true. That is the maybe not common but that's the really bad mistake wecan make in interpreting a p-value. What a p-value is, is a probability ifwe assume the null hypothesis is true, of seeing a test statisticthat is extreme. That is assuming the null hypothesisis true the probability of seeing these extreme values. So a small p-value would tell usthat under the null hypothesis, it's unlikely that we would have obtainthe observed value of the test statistic. So it's unlikely we willobserve that particular data. So a low p-value gives us evidenceagainst the null hypothesis. So the last thing we need then is a wayto make a decision based on a P-value and to do that we will usea significance level. The quantity alpha which wehave actually seen before, we call the significance level. So if the p-value is less than alpha, we say that we reject the null hypothesis. So a very, very small p-value means thatif we assu,me the null hypothesis is true, there's a small probability that weobtained the test statistic that we saw or something more extreme just due to chance. Now conversely ifthe p-value is greater than alpha we say that wefail to reject the null. So we're saying we fail to rejectinstead of saying that we accept, to illustrate the idea that, just because we have this large p-valuedoesn't mean the null hypothesis is true. It just doesn't mean that we haveenough evidence against it to reject. [MUSIC]"
stat-420,3,13,"[MUSIC] So now that we have the basic ideaof hypothesis testing in the simple linear regression model,we want to talk about one particular test. So, that test is calledthe significance of regression and what it does is a test forsignificance of regression. So, here's our simple linearregression model here. Now what we want to do is had a veryspecific null hypothesis that is we want to hypothesize a value of beta 1,that is 0. Now if we do that andwe sort of write in the model now, well, that would be a model where y is beta 0plus, well, nothing here, plus some error. So, this is a model where there is nota linear relationship between x and y. So essentially, the null model says,no linear relationship. Whereas the no hypothesis still allows fornonzero values of beta 1. So over here, there would bea potential linear relationship. So essentially, this is a test for whetheror not we need to use a line to model the data which would be the scenarioin the null hypothesis or whether or not there's no significant relationshipbetween x and y in a linear sense and we can just use a singlevalue to summarize y. So thinking about this in terms of ourusual data set here, the cars data set. Essentially, we want to know could thisdata set here have been generated by this estimated null model here where there'sno linear relationship between x and y? So what we do is we fit regression, whichI think may have been something like this. But had we done that,we would've gotten a beta 1 hat of 3.93. And we want to say, well,if we assume the null hypothesis is true. And soif we assume that beta 1 is actually 0, is it possible that this numberjust springs up due to chance? Because we know when we ranwith sample of the data, we're not going to get exactlya beta 1 hat of 0, but it should be close to 0 with respectto how variable the estimate is. Well, so then the other thingwe need to know is well, what's the standarderror of that estimate? And it turned out to be something like0 point roughly 416 and then we can tie all that information together, becausethe test statistic t is our estimated value minus the hypothesized value whichwas 0 divided by the standard error. So it's simply this divided by this and it works out to be a very large number,which is 9.45. So to calculate the p-value, we woulddraw a t distribution with the correct degrees of freedom which is the numberof data points here minus 1. So, and then we have here is 9.45. I'm being very generous withwhere I'm drawing that and then here's -9.45. And so, the p-value is thisarea plus this area here. Again, this is very much not to scale,because it turns out the p-value is then approximately 1.5times 10 to the negative 12th. Under the null hypothesis,it is very, very, very, very, very unlikely that we would haveobtained this beta one hat. In other words, very unlikely that we would have obtainedthis value of the test statistic. So say we had decided to performthis test at an alpha of 0.01, we would definitely then,based on this p-value, we would reject the null hypothesis. It's very, very, very, very unlikelythat we would have seen this data if, in fact, the null hypothesis was true. We briefly hinted at this before. But we can perform this exact test here,this significance of regression test without actually calculatingthis test test here. And to do so instead of doing that, wewill calculate this confidence interval, but in a specific way. So if we wanted to perform this testat a significance level of alpha, we would look at a 1 minus alphatimes 100% confidence interval. We sort of briefly mentioned that if 1minus alpha is the confidence level, alpha is the significance level. What we then look at is one value inparticular is we check whether or not the value zero is insideof this confidence interval. So if zero is inside of this interval, that means it's a value we sort ofbelieve we could expect to see, then we would accept or not accept, butfail to reject the null hypothesis. Whereas if zero is outside of thatinterval, we reject the null hypothesis. So if zero is in that interval,we fail to reject the null hypothesis. Whereas if zero is outside of thatinterval, we reject the null hypothesis. So in the cars example again, this orange line here wouldbe the estimated null model. So, I think the actual regressionmodel is something like this. So again, this was a beta 1 hat of 3.93. So say, we wanted to perform the significanceof regression test at an alpha of 0.01. Well, so instead of doing what we didbefore and going through this procedure, we could instead look ata 99% confidence interval. And we saw before that a 99%confidence interval for beta 1 worked out to be 2.81 to 5.05 and we see that the value 0is not in that interval, so we would reject the null hypothesis. So two different ways of accomplishingthe same thing using the actual test, we get a p-value which is a very specificquantification of how likely it would be to have seen this valueother than hypothesis. Whereas the interval estimate givesus sort of a range of possible values for beta 1. In the end, both of them would allow usto make a decision about the hypothesis. So here, we see three simulated data sets. So, the first two were simulatedwith a true beta 1 of 0. And in this final data set here,beta 1 is not 0, it's exactly 0.5. So, we simulate some datain each graphic here. So in the first one, we see that sothe true value of beta one is zero. So this is the true regressionline in dash blue here and the estimated rational line isnot all that far from 0 and it turns out that having performedthe regression test here, we get a practical value of about 0.52. So let's say, we're going to performall these tests at an alpha of 0.05, we would fail to reject andthat is the correct decision. Now here in the second data set andagain, so this data was simulated froma model ware beta 1 with 0. So no relationship between x and y, but the estimated regression linehas a bit of a slope to it. And it turns out that if wego through the procedure from the hypothesis testing here,we get a p value of 0.03. So at this alpha, we would rejectthe null hypothesis and that's a mistake. We wouldn't actually know that it'sa mistake most of the time in practice, because we wouldn't be simulatingthe data from a known distribution, but that's something we need to be aware of. Now in this last panel here,we see that beta one is not zero. The estimated beta 1 is also very far from0, but so performing the hypothesis here. We're assuming this null model here. In this case,an estimated null model is true. So it's very, very, very unlikely thatwould have seen this particular data from that particular model. And it turns out the p-valuehere is incredibly small, something like 3.7 times 10 to the -12th. So, we would reject the null hypothesis. But now, we've seen here that's hypothesistesting certainly isn't perfect. And in particular alpha, this significancelevel tells us how not perfect it is. This table here will tell usall the possible outcomes. So in practice know the reality, whetheror not the null hypothesis is true or false. But in reality,it is either true or false. But based on our hypothesis testing,we'll make a decision. We either fail to rejectthe null hypothesis or will reject the null hypothesis. Now when the null hypothesis is true,we would like to false reject. So, that would be a gooddecision in that situation. Similarly when the null hypothesis istruly false, we would like to reject the null hypothesis like wedid in the third data here. So, that would be also a good decision. So now, there is two mistakes we can make. If the null hypothesis is true,so there is indeed not a linear relationship between x andy like in the second data set here. But we actually, unfortunately,reject the null hypothesis. That's what we would call a type I error. Also, this would often becalled a false positive. Now in the procedure that we've developed, the probability of thisoccurring exactly alpha. So alpha, the significance level is alsothe probability of making a typo error that is rejecting in all hypothesis whenthe null hypothesis is actually true. So essentially,when we do these procedures, we're going to ahead of time determinea level of alpha that were okay with. So very common ones are 0.10, 0.05 is generally the default andan alpha of 0.01. So these are very common ones, but you could choose any value of alpha thatyou feel comfortable for your situation. But then with that value of alpha,we carry out the test. So just with any of level alpha test, we have to willing to accept thatproportion of type I errors. Now the other area we can make isthat if hypothesis is actually false, that is there is similar inrelationship and we fail to reject. So we sort of saying,there no any relationship. That would be a type II error. Otherwise, known as a false negative andthe probability of making this error is beta andthe way that have set up all these tests. We won't drive or prove this. But for a given value of alphawhich we set ahead of time, if your test is made in a way thatit minimizes this type II error. One last sort of hidden errorthat we need to aware of is this data here hasa very clear pattern in it. It looks something like this. So, there's a very obviousrelationship between x and y. However, when we fit the simplelinear regression model to it, we obtain this solid orange line here. So if we did a test then for whether ornot beta 1 equals to 0 as a null hypothesis, it would turn out thatwe would fail to reject here. And this is because this test forthe null hypothesis of beta 1 is 0 is based upon a test statisticthat is based upon beta 1 hat. So this is just a warning to say that toapply these testing procedures, we have to make sure that the simple linearregression model actually makes sense in the first place and we are specificallytesting for linear relationships. So we can figure this test asdetecting linear relationships, but it will fail for a number of nonlinearrelationships like we see here. [MUSIC]"
stat-420,3,14,"Coming back into R again, we'll quickly see how to perform hypothesis testing in the regression setting using R. We've basically already done it before, we just need to point out where we had done it. So, we'll return to our usual dataset and fit the usual linear model, and we will return the summary information as we have a number of times before. And we'll notice that, here, we see our estimate for beta_1 which was beta_one_hat, which we've seen before. Here we see the standard error, which we again had seen before. Now, here we have the t-value. So this is a test statistic, and again, it's a test statistic for a particular test, that is, the significance of the regression test. So, this t-value here, this test statistic, is for testing whether or not beta_1 is zero or some other quantity that is not zero. This t-value is exactly the beta_1_hat we've seen before, minus zero, divided by the standard error that we've seen before. And here we see exactly that. And then the last thing that R for us immediately is it calculates a p-value. So, it won't complete the test all the way through to, say, give us an accept or reject, but if we had pre-specified an alpha here, say 0.01 – this is an extremely small value; it's also less than an alpha of 0.01 – so we would reject the null hypothesis here. And based on the picture we've seen before, it's pretty clear that the speed here has a significant linear relationship with stopping distance. A few things about the details of what's going on here. Often we may want to extract these particular values here instead of just displaying all of the summary information. To do that, we can first use the names function to see what's actually stored in the summary information. And we've seen before that we want to look at this coefficients variable within the summary, and if we do that, we see that we get back exactly this here. And then we can extract things as if this were a matrix. So I could say to get this p-value here, I could request the second row and the fourth column, like I have here, or I could instead say I would like the row which is named speed and the column which is named this expression here which indicates the p-value, and that would also get me the p-value. So, I'll do that to store the value of the test statistics here and here. So this is the test statistic for the test of whether or not beta_0 is zero instead of beta_1. And here, this will store the test statistic for testing whether or not beta_1 is zero. I'll do that and store those real fast, and we can just verify that what's stored in those two variables are these two numbers here. Now what I'd like to do is verify the p-value calculations. We'll do so for beta_1_hat first, and then sort of simply slip in beta_0 and verify that it is also the same. So the first thing I want to do is say that the degrees of freedom that we will be using here is 48 because there are 50 observations. And I'm quickly gonna run this line here and simply note that well, yes, this is the value of the p-value, but I want to break down what we've actually done here. So, we're using the pt function because, again, we know that under the null hypothesis, this test statistic follows a t-distribution with n-2 degrees of freedom. So, to calculate probabilities about a t-distribution with n-2 degrees of freedom, we use the pt function. So, if I explicitly want to calculate this quantity here, I'll take the absolute value of my test statistic, which is actually just the test statistic again. I'll look at the negative of that and I will use n-2 degrees of freedom. That gets me this quantity here. Now to get the p-value, I can do one of two things. I could add this quantity here, or I could simply multiply what I have by two. So I'll take this and I will multiply by two, and we see that that is exactly the p-value. Had I done this the other way, I can say, OK, this again is this here. And then I can add to it something similar. So we'll look at the positive test statistic now. And now, instead of the lower tail, which is what this calculates by default, I will add an argument here for lower tail = FALSE. They will put this on two lines. And if I run all of this, again, I get the test statistic. This initial line that I had here had done the same thing. So, this here was calculating exactly this quantity here and then because of symmetry, we simply multiply it by two to get back the p-value. And again, so these absolute values are working for us because sometimes we're gonna have positive test statistics, sometimes we can have negative test statistics like we saw with beta_0 here. So now, since I have those absolute values there, I can simply change this to zero, and this p-value should indeed be this p-value that we already saw here – just here, it's using scientific notation to display it, and here it is not – so we've verified those p-values. It's always nice to use R to do a little bit of simulation, so that's what we'll do now. I want to consider simulating from a true simple linear regression model with certain parameters. So, first, we'll simulate from this model here. And when we do this, we're gonna now do test about this parameter beta_1 at an alpha of 0.05. Here's my usual function to simulate simple linear regression given a set of x values. I'm going to set a seed here. I'm going to decide on some x values and then simulate some data. So now I'm going to fit the regression model. The way I've simulated this data, again, beta_1 is zero and sigma_squared is one. The true regression model, now, has no relationship between x and y because this parameter is zero. We would like to, when we perform this test – the test of significance for regression – we should fail to reject the null hypothesis, and that is exactly what we do here. And if I make a plot here, it's sort of clear that while, yes, this line is not exactly the true line, which is the dashed blue one, it's pretty close. But now I'm going to repeat this process a few times. We failed to project again, that's good. We failed to project again, that is good. We failed to project again, that's good. But now, we run into a bit of a problem here despite the fact that we simulated from this true model here. We see that we get a fairly negative slope here, and it turns out we actually reject the null hypothesis. But remember that's something that we simply have to accept when we're performing hypothesis test. This alpha value controls exactly how often that's going to happen. So if I sat here and repeatedly ran this code, we would see that from time to time, specifically about five percent of the time, we would accidentally reject the null hypothesis even though essentially, in this case, the null hypothesis is true. And what we can do now is I'm gonna demonstrate that the opposite can also happen. So I'm gonna now simulate data where the true beta_1 is not zero – so now, it's 0.5. And now, I'm going to make sigma_squared 49, in other words, sigma is seven. We'll run this code here. And we see that, so this blue dashed line here would be plotting if beta_1 had been zero, and then we fit this line here. So, it's no surprise that we reject the null hypothesis. So now I'm gonna repeat this process over and over again. Now, we would expect to reject the hypothesis or hope to expect the null hypothesis often because now the null hypothesis is indeed false, and we know that because we're simulating, and this beta_1 is truly nonzero. But again, if I do this over and over again – I sort of chose these values so that it would work out this way. But we see that here we actually make a mistake. So, despite the fact that we're simulating from a true model that is not the null hypothesis, the slope of this line is not particularly large just due to chance, so we accidentally fail to reject the null hypothesis. So we just need to be aware of the fact that hypothesis testing isn't a guaranteed procedure, but something that is dealing in probabilities."
stat-420,4,1,"The simple linearregression model was a nice introductionto regression models. While we do like simple models, it is rare that wewould either only have a single predictor variable or that only a singlepredictor variable would be needed toexplain the response. So now we'd like todevelop a model that can accommodatemultiple predictors. In this lesson, we'll introduce the multiple linearregression model. Right away, we'llsee that this is a more general version of the simple linearregression model. Many of the conceptsand tools that we learned for the simplelinear regression model will only needminor modifications to work in the multipleregression setting. After completing this lesson, you'll be able to apply the multiple linear regression model in R and interpret the results."
stat-420,4,2,"[MUSIC] So here we have three different viewsof the same multi-variance data. So we have a single response Y and now wehave two predictive variables X1 and X2. We limit ourselves to X1 and X2 here because it’s hard enough to plotthree dimensional data in two dimensions. So anything above thatwouldn't be possible. It might be difficult to see at thispoint that both X1 and X2 are useful or that this we can see a relationbetween both of them and the response. In this third panel here,we can see that X2 is sort of very good at explaining what happens to yas we increase X2, y goes up. We'll also find that as we increase X1,Y goes down. So when we add a fitted model to thisdata, that would be a little bit clearer, and you may be able to see that basedon the coloring of these data points. So the blue points are the lowest points. The darkest points up here,the dark orange, are the highest points. But we'll sort of make that a littlebit clearer on the next few slides. So now we like to create a model for this response variable Y thatutilizes both predictors X1 and X2. So we want to understand the relationshipbetween both predictors and the response Y. So the model we'll start with will bewhat's called a multiple regression model. This probably looks somewhat familiar. For example, if we ignore this for a second, this would just bea simple linear regression model. We'd have our predictor x1 andthe slope parameter in front of it. By adding this x2, sonow we have two predictors. We have a predictor x1, so x sub i1 is the ith observation of the x1predictor and over here we have x2. So x of i2 is the ith observation of x2. And now in front of each predictorwe have a beta parameter. So now this model has three betaparameters and two predictors. In the end though, we essentiallyhave a very similar looking model. We have our new mean portion of the model. Which now is a linearfunction of both X1 and X2. And then we have our usual errors whichwe can think of it as noise and they follow a normal distribution with the meanof zero and a variance of sigma squared. We can also rewrite this modelas a conditional model so we could say why Conditioned on xwhich is now a vector value actually, follows a normal distribution withthis new mean function here so beta 0 + beta 1, x1 + beta 2 x2 and variance sigma squared. So now to fit this model tothe data we essentially want to estimate the mean portion which would meanfinding the beta 0, beta 1 and beta 2. They give us the best fit, and the wayI think of that is finding essentially the best place through these data. So now they were in higher dimensionsinstead of simply aligned, we would need a place. So if we did that for this data andwe'll talk about how we did this but first we'll just discuss results,it would look something like this. So again,this is three pictures of the same data. I want to sort of focus on the third onebecause I think it's easiest to see. This plane here, now, as x 2 is increased, we see that there's a trendwhere y is increasing. X 1 here is sort of coming out at us. Now for any value of x 2,this plane is sort of sloping down. As x 1 is increased, for any value of x 2,we see that y is actually decreasing. So while x 2 did explain a lotof the relationship with y. It appears that X1 isalso helpful as well. Had we ignored X1 completely, I believe the results in claimwould look something like this. It will still be a plain but it will becoming directly out at us and this would actually result in larger errors inhow to weigh used both X1 and X2. What do I mean by errors? Well I mean the distance inthe y axis from the points to any of the planesthat we're considering. So if I take this third panel and rotate it a little bit morewe see something like this. So now we're not looking directlyat x 2 it's sort of rotated a bit. And this picture sort of startsto look really familiar. In the end what we've donehere is we've found a plane that has sort of minimized errors. And you probably see wherewe're going with this. We minimized errors ina squared errors sense. So that's what we'll talk about now. So to arrive at this point, that is, tofit a plane to this data that we see here, what we want to do is minimize squarederrors, here we have the Y values for our data and then beta0, beta1 andbeta2 control the possible points. So what we have here is where ourdata is on a particular point for the values of the predictors we have. And again, we want to thenfind the bata 0, bata 1 and beta 2 to get this asclosest possible to our data. So we want to minimize this function withrespect to beta 0, bata 1 and beta 2 so that three derivatives we need totake with respective we need to take with respect to those three parameters. Set them equal to zero,solve along the way. We'll arrive at something that willstart to look sort of familiar, these being now the new normal equations. We can see that they'restarting to get complicated. We also start might to notice somepatterns, like there's a square here, a square here an x 1,x 2 here, an x 1, x 2 here. But we can see that right away ifwe added more dimensionality here, these equations are going toget much more complicated. And so we'd like to look at another wayto think about this is going to make our life easier. The first thing we'll do isintroduce the more general multiple linear regression model. Which looks something like this. Now instead of two predictors, we'll have some arbitrarynumbers of predictors P minus 1. We're going to say we haveP minus 1 predictors, so we have 1, 2- all the waythrough p minus 1 predictors. And that's where the multiple comes from. So we're going to talk aboutmultiple linear regression. So in multiple linear regression, we have multiple predictor variables, inparticular, we have p minus 1 predictors. Now, linear will no longe refer to a line. It will refer to a linear combinationof the available predictors. And we are still performing regression. So, in the end, we are still tryingto explain the relationship between the response y and some predictors. Now we just have multiple predictors. One minor note our convection note, so we are going to say that wehave p minus one predictors. With p minus 1 predictors becausewe are also using sort of an intercept here as beta 0,we'll have p beta parameters. Some references used thesetwo values differently, which is 4 convention we're goingto use p as a number of data's. And p minus 1 as a number of predictors. When we apply these models of data asidethat we've been indexing about i as entire time to allow for the fact thatwe're going to have an observations. So the YI's, they'll be n of them,and those will be placeholders, essentially forthe potential data that we could have. So in other words, we're going tohave this expression, n times for each value of i, i from one to n. We can write that a little bit morecompactly with some notation that we have here now. So for example, the second observation,so Y2 will take the second row of this matrix we've created here,and multiply it by this vector here. We've created it, then add on thislittle arrow at the end here. So at the end, we arrive atsomething that looks like this. So we have Y2 is equal to 1 timesbeta 0 is beta 0, plus the value of the second observation forthe first predictor times beta 1. And so on and so forth. Ending with beta P minus 1 timesthe value of P minus 1 is predictor for the second observation. Then, one of the IID error variables. Written even more complexly than. We can use this notation here. We have a vector of random variables forthe resulting ys. We have what we'll call our x matrix orour design matrix. Design matrix can also becalled the model matrix. And it essentially combines allthe predictors into a matrix. Here we have a column forthe values of the collector X1, here we have the column for the previousvector X2, here all the way going there. And we have a column forthe predictor Xp minus 1 and in addition to that it addsa column of 1s that we see here. And these are essentiallywhat are going to for each observation introducethe beta 0 term. And that's what we saw happening here. So this one here brought thisbeta 0 into the expression here. Now we have a beta vector that has eachof the p betas in it and now epsilon is a random vector of length n that hasthe IID error variable stored in it. So we can quickly do somedimension checking, so y is n by 1 X is n by p beta is 13 same as 0here this is p by 1 and epsilon is And by 1, importantly whenwe do the matrix multiplication here, we'll do n by p multiply by p by 1,we end with something that n by 1, so that the dimensionalityof everything checks out. We've been dealing with the model sohere we've had a capital Y for the random variables,Y you consider as a vector value. We also have data. Lower case y. And that's what we want to fit to. Modifying what we'd seen before,we essentially want to perform this minimization now sohere are the ys in our data and here is the linear combinationof the available predictors. We'd like to find beta0 allthe way through beta p- 1. That minimizes this expression,which results to taking p derivatives, and that's going to get really messy,so we don't want to do that. So instead, we're going to moveback to our matrix notation here. Y was a vector of random variables there'suppercase Y, lowercase y will be our data. We want to compare thatto the mean portion here. This value we want to bevery small all the time. And so we want to sort of square this andminimize it. But then that make sense the major sensethis is the notion of the expression. What we actually want to do is lookat this transpose time Y minus x beta so this expression you want tominimize we essentially want to take a derivative with respect to the betavector set that equal to zero and solve if we do that we'll end up with first herewe have a normal equations but now they are written in a nice compact metric formso actually everything we've seen before. As long as we have the correctmentionality of X, it would satisfy this expression here. And then at the end we have our estimates,beta head is now a vector which contains estimates foreach individual beta parameter. So that beta hat we just found, again it's purpose is to estimatethis entire beta vector here. So we could write betahat is also a vector. And we could refer to the individualelements of that vector as beta zero, hat beta one, hat all the way through. Beta sub p- 1 hat. And each individual element of this vectorestimates the corresponding quality and the theta vector. So each beta hat sub j estimates beta sub j and it turns out thateach of those is also unbiased. So the expected value of betaj hat is exactly beta j. We've taken care of estimating beta whichagain in the three dimensional sense, finds the best plane. But remember that the simple linearregression model was a probability model. As is the multiplelinear regression model. So don't forget that these epsilonsare assumed to be normal with mean 0 and variance sigma squared. So the question now is wellwhat about sigma squared? It turns out that we just need to findfitted values, find residuals and then combine them in the correctway again to get a good estimate. Y hat here Will be a vector of fittedvalues plugging in beta up here so this is the mean portion,this is the estimated mean portion, and those are predictedvalues now as a vector. So we consider the vectorof data subtract of the. Predictive values and we arrived atwhat we have here is the RESIDUALS. Essentially, once we got the predictedvalues these became exactly what we have seen before. The fact we are in multiple madeno difference because this is just a factor of predicted values. So how do we use that though,out estimate for again so. These are normal mean zerovariance sigma squared. So our estimate for sigma squaredhere changes just a little bit. This should look rather familiar. On the top here we have the sumof the residual squared which we could also writeIn a matrix notation here. So the thing that changesis this n-p here. The way to think about this is thatthese yi's, they're our n observations. And here the yi hatsare our predicted values. And they're predicted froma model that used p parameters. In particular, p beta parameters. But we needed to estimatethose p beta parameters Before we could estimate sigma squared. That's why we're dividingby n-p here instead of n. So this will be what we callthe degrees of freedom for the multiple linear regression model. And so the reason we do this is exactlythe same reason we divided by n Minus 2 previously, because in the end, this makesit such that the expected value of this estimator is exactly the thing that we'retrying to estimate, so it's unbiased, and it will also allow us to obtainsome distributional results later. You might realize to that well inthe simple linear regression model, P is exactly 2 because we havetwo beta parameters, one for the intercept and one for the slope. So this expression is justa generalization of exact that we seen before but now it takes into the accountthe number of parameters and the model [MUSIC]"
stat-420,4,3,"So, we've discussed how a multiple linear regression model is fit to data, but we actually haven't seen it happen, and we're still gonna delay that a little bit further. We'll do it in R, eventually. But now, we want to talk about what fitting that model to data tells us about the data, so we're gonna discuss some of the interpretations of the multiple linear regression model. So to do so, we're gonna need some data. In particular, we're gonna use this dataset, which we see some limited output of here, which involves cars from the 70s and early 80s. In particular, we have a number of characteristics of cars. The one that we're gonna be most interested in is this mpg variable here, which has the city miles per gallon of all of these cars. And we'll also look at two predictors in this case; we'll focus on the weight of cars and the year the car was built. We want to create a multiple linear regression model that uses weight and year to explain the fuel efficiency of the car. When we actually look at this dataset in R, we'll also pay attention to the horsepower and acceleration variables, notably the acceleration variable is the time it takes to reach 60 miles per hour. But we'll return to that when we see this dataset in R. The model that we'd like to use is a multiple linear regression model with two predictors. So, our response here is going to be the miles per gallon, and again, that's city fuel efficiency and it's in units miles per gallon. We'll have two predictor variables. The first, x_1, will be the weight. It happens to be in pounds, but because we're not actually gonna fit to the data, that's not super important. And we also have the year of the car. We want to first discuss in the model setting what these various parameters mean. So, first, we could talk about the beta_0, which is the mean miles per gallon, so it's the average fuel efficiency when the weight is zero, that is, zero pounds and the year is zero, and that would actually correspond to the year 1900. Again, this is an average because we're dealing with estimating means with this model. But in particular, beta_0 is a particular mean when weight is zero and year is zero. Now, this is a quantity that we're not all that interested in. The more interesting model parameters are beta_1 and beta_2. They tell us how weight and year affect the response miles per gallon. Focusing on beta_1, beta_1 tells us how Y changes when x_1 changes. It's the mean change in miles per gallon when you increase the weight by a pound but in a particular situation, that is, for a particular year. So, here, would have been the interpretation under the simple linear regression model. But now, we have this extra predictor and, thus, parameter in the model. Year is explaining some of the fuel efficiency and weight is also explaining some of the fuel efficiency. When we increase the weight by one, that's beta_1 is how much Y increases. But we have to consider that year is already in the model, so it's for a particular value of year. Beta_2 then has a similar interpretation. It's the main change in fuel efficiency when you increase year by one for a particular value of weight. Let's now take a look at the fitted model. We'll see when we get into R that the fitted model ends up being something like this. So, our fitted values are -14.64 + -0.0066_x_x_1 + 0.761_x_x_2. Right away, we see that beta_0_ hat is a little silly. So this is beta_0_hat, which is an estimate of beta_0. So this says that when weight is zero, year is zero, the fuel efficiency is -14.64. So, that's a little absurd because fuel efficiency shouldn't be negative. But again, we're trying to estimate a quantity that also is just of no interest – when weight is zero and the year is 1900. I don't believe any car that weighed zero pounds is made in 1900. So, it turns out that this is extrapolation, so we just wouldn't expect the model to work there anyway, and we're just not all that interested in that parameter. So here, beta_1_hat we see is negative. Now, again, remember beta_1 was weight, beta_2 was year, so this says for a car of a particular year, as the weight increases, the fuel efficiency goes down. And that just sort of makes sense. Heavier cars are harder to move, so they're less fuel-efficient. Similarly, beta_2_hat, which is an estimate of beta_2, here we see it's positive. So, for a car of a particular year – I'm sorry, for a car of a particular weight – as the years go on, we see an increase in fuel efficiency. So, keeping weight constant throughout the years, cars were getting more fuel-efficience. So, if we compare a simple and a multiple linear regression in the sort of same setup we had here, so consider a simple linear regression that looks like this. So, and again, we'll let Y be miles per gallon, x here, x_1 importantly is weight and x_2 is the year. And we'll see the same in the simple model that we have here as well, so Y is miles per gallon, the single x, which I'm labeling one just to have a similarity here, is also weight. The thing I want to say here is that beta_1 in the multiple model and beta_1 in the simple model are not the same parameter. Again, we said beta_1 in the multiple model tells us how x_1 affects Y with x_2 in the model, whereas in the simple model, beta_1 tells us how x_1 affects Y but without beta_2. So, beta_1 in the multiple model is, in effect, for a particular year, whereas beta_1 in the simple model is the effect sort of averaged over all years. So that's different. We would note that, in this case, when we're talking about weight here, we would expect our estimate here to be positive, and we saw our estimate here was positive. But just because something is positive in the simple model does not necessarily mean it has to be positive in the multiple model. It is in this case, but when we get into R, we'll find an example, even in this dataset, where we can have a positive value of the estimate in the simple model – so, sort of averaging over another parameter – but an opposite sign in the multiple model. So we can take this idea and transfer it to larger multiple regression models. So if we want to discuss the effect of, here, x_1 and beta_1 on Y here, so if we increase x_1 by one, beta_1 is the change in mean of Y for particular values of each of the other predictive variables. So, essentially we just need to always remember that when we're talking about the effect of a single predictor, the fact that all of the other predictors are in the model changes what that effect is compared to a simple linear regression model. Now, one thing you might notice here is that no matter what values of these other predictors that we have, the actual effect of x_1 is the same. So, the fact that these things are in the model changes the effect of x_1, but no matter what values of these other predictors we have, the effect of x_1 is the same. In more advanced regression classes, we'll talk about ways to complicate that relationship and allow x_1 to have a different effect on the response when the other predictors take different values."
stat-420,4,4,"[MUSIC] To discussMultiple Linear Regression in R, we'll interactively work throughin our Markdown document. So, the first thingwe'll need is some data. We'll actually go out and get the datathat we saw when we were talking about interpreting the model which can be foundat the UCI machine learning repository. We have the direct link to the data here. So, we use the read out tablefunction to read that data in. Now what we want to do is take a lookof the data that was read in and then you'll see what we're goingto do here is modify that data. And we'll take a look at what thesemodification step do along the way and sort of why we're doing that. So I'll use the view command inside of ourstudio to pull up a viewer of the data. So at first glance, this looks like a lot of numbersthat don't make any sense to us. The final column are names of cars,so that one is immediately clear. But these other ones, we wouldn't be toosure about unless we actually went and looked at the documentation of this data. But one that I think I canrecognize right away is V7 here, appears to be the year of the car. But we would have to go andobtain that documentation, and we'll assume that we had. And then we can determine whatthe column name should be, and we can add those to the data set. And I should have left my viewerWindow up so we'll bring it back up. And now we see that the column nameshave been attached to the dataset so for example we have the fuel efficiency thatis the city fuel efficiency which will miles per gallon, we have year,we have name of the vehicle. I think we'll also be interestedin weight and horsepower. Another thing we might noticeright away is, for example, for this '71 Ford Pinto here,we have some missing data. We also want to pay attention tothe fact that this missing data took the form of a questionmark which is a character. That will come up in a secondbecause later on I'm going to be interested in the horsepower variable. I'm going to go ahead and remove any rowsthat have a horse power that is missing. And when we switch back to our viewerwe see that that particular Ford Pinto is now gone. The Plymouth reliant causes someissues in this data set because there are two identical Plymouthreliants with respect to year and number of cylinders in the engine. So I'm simply going to remove it for simplicity's sake because I would likeone row for one model year of car. So now I have this column forname but I'm not going to want to use this as a variable forin most point of regression model. I would like it to simply describethe row for me as a human. So I'm going to take this information andmake it into a row name. But something that's going to causea problem is the fact that for example here we have multiple amcgremlins for different years. So I want the row name to indicate boththe year and the name because row names need to be unique whereas entriesin the column don't need to be. It also turns out and then for some vehicles the year,there's two different engine types. So I'm also going topull in the cylinders. So that's what we do here,I paste together the information for the cylinders, the year, and the name. And we'll see what that does,so now I have this row name. It's not a variable in the data set,but it's a descriptor of each row. So with that,I can now remove the name variable. And I'm also going to take out the originvariable just because we don't want it for the analysis that we're doing. And that's what I do here. If we go back, we see that now eachcar is described in the row name, we no longer have the name as a variable. So one last thing I'm going to do, which I'm going to skip for now, ismodify the horsepower variable, because, as it currently stands,the horsepower variable is character. And that's because originallythere was some missing data and it was coded as a question markwhich is a character, so it turned everything into a character variable butwe're going to want this to be numeric. So I'll go ahead and do that. And then when I checked the file structureof the data we have all numeric variables and we have nicely named rowsthat described each observation. One last thing I'm going to do is I'mgoing to remove this command I used to pull up the viewer because that'ssomething that we don't need when we finally knit this document. It would just clutter the report becauseit would be this command where we don't see any output because it'san RStudio specific command. Also, it would pop this up everytime you knit the document, which you don't want to happen. So to fit the multiple linearregression model in R, we will return to the familiar LMcommand that we had seen before. So almost everything will be the sameas simple linear regression. We'll have the response variableto the left of this tilde, and the formula that we have herewill give it a data argument that passes through the data frame that wewould like to use in this regression. But the only thing that changes nowis on the right side of the tilde, we list the variables that we'dlike to include in the model. And we use a plus sign because theyare going to be in an additive form. This notation here says fit this model, where y is miles per gallon,X1 is weight and X2 is here. So if we fit that andextract the coefficients of that model, we see the fitted values herethat we had talked about before. We could write them like this to indicatethese are how to obtain predictions for a critic of X1, which is weight andX2 which is here. One command that we used very frequentlywith the results of LM was this summary function here, so we'll run this andwe see this column for estimate here in the resultsare exactly these values here. So we understand these already, but there's some new things here inparticular these values for testing. They look similar to whatwe have seen before but we'll see exactly how they differ we'llalso talk through this bottom line here which is the f test statistics andwhat that does for us. We'll also talk about multiple r squaredwhich it turns out is exactly the same r squared as we have seen before just with aslightly different interpretation now that we're doing multiple linear regression. So we saw how this model can be fit andinterpreted in terms of a matrix approach. So we'll verify that now. So we're going to considerthe model like this. So to do so, we're going to need n, which is the number of observations, p,which is the number of beta parameters. So essentially what I did to obtain p was,well, it's 1, 2, 3. But to obtain it programmatically,I said well, I want the length of this vector here, becausethese are the estimated coefficients. So I need to create what we would callthe X matrix or the design matrix. So it has a column forweight, a column for year but importantly we said it also has a columnof all ones which essentially introduce the intercept into the model andI'll also define my Y variable. So I'm going to run this. And then, just to verify what the X matrixlooks like, I will use the viewer again. And we see that, here is the column forweight, here's the column for year. But we also have this columnup front here that is for introducing the interceptor in the model. So, with this X matrix andthese Y variable here, we could obtain the estimatedcoefficients, simply using some, when your algebra here so I'll actuallysort of break this down a little bit. So to apply this expression here,I'm going to start with my X matrix. I will transpose it, and then importantly,matrix multiplied by the X matrix. So that'll get us what's inside of here. And we verify that that's a 3 by 3 matrix. And then to obtain the inverse,I will use the solve function. Which will be anotherthree by three matrix. And then I can continue on andmultiply by X transpose again, and then by Y, andthat's what we have here. And running that gets methe same result as I had before, this is the new beta hat,which we had found via lm. So just verifying that thoseare exactly what we had seen before. So, I have here the expressions forthe fitted values and the residuals. So, fitted values are obtained here. Residuals are what they've always been,the original data minus the fitted values. So we'll store those, andthen we'll see how they can be used to calculate the SW squaredwhich is our estimate for sigma squared in the multiplelinear regression model. So first we'll performthis calculation here, so we're looking at the sum of the areasquared divided by the degrees of freedom. I'm taking a square root here so I'm actually calculating S of E not S ofE squared and we'll see why in second. So we see a 3.43 orI can use the matrix version which will sort of automatically take care of summingand squaring via this notation here. And again I'm taking a square root andwe see these two are equivalent. I'm taking the square root becauseby default if I use the output from r it is getting me the estimate ofsigma not the estimate of sigma squared. And we see here that weverified that that is the same. This is what r calls the residualstandard error which we've seen before in the summary information. So we want to talk about multipleR-squared real fast which we could just call R-squared. R happens to be labelat multiple R-squared. So here I'm going to fit a simpleintegration model which I'm just going to call a smaller model that only uses year,not year and weight. So we run that, so I have that stored. And right away one thingthat we talked about is the estimated coefficientshere are different. So in the full model with both weight andyear the estimated coefficient for year is rather different thanthe estimated coefficient for year in the simple regression model. because again the interpretation ofthese parameter estimates change due to the fact that weight is in this model. So we can obtain R-squared forthe larger model, simply using these same codethat we had done previously. But so, this is the portion or variationin miles per gallon that's explained by the linear relationship withboth weight and year now. So I want to sort of completethis marks-on document. And the first thing that I'm going to dois I'm going to put this value into here. And we see this in-line tech display andit's updating correctly. So also obtain R-squared for the smallermodel and we see this is smaller. And that's something that should sortof make sense so using fewer variables, less of the variation in milesper gallon is explained. And In this case, it's sort of I think almost anyonewould say is a large drop-off. But we're going to build towards is sayingsort of, well, is that a significant difference, because, all else being equal,we prefer a smaller model. So we're going to need some way todetermine the difference between these two models here andwhich one we actually prefer. Okay, so I'm actually going to goahead and nick this document here. And hopefully it works. Yep, there it is. Okay. So, we see that nicely thishere is sort of a nice, quick way to write up aboutwhat's happening here but there is sort of a minor issuehere which is if for example. Let's say I changedyear to be acceleration in the smaller model andmaybe in the larger model as well. Let's make this year acceleration. And I nick this document. So the problem is these valuesI have sort of hard coded. It actually hard coded these values aswell which are also no longer wrong. I won't fix these right now butI will fix these. So instead of simplycopying pasting the values we obtained when we run saythis code here, I'll also note that this was run by knitting thishad been run interactively previously. So to actually get back to where we were Ineed to run this code but what I'll do is I'll actually run everything abovethis using this button here. And now I have my updated code soI'll get my updated R-squared here and I'll get my updated R-squared here. Now I could be lazy and copy this intohere and then re-run this document. What I'll actually do is be smart andI'll take this R code and I can actually put it intothe stack code here so I'll do this will allow me toanything I put now here will be run before rendering the task. For example,if I put this R code here now. And I'm actually going to dothe similar thing here, so unfortunately the inline update doesn'twork well but now when I run this. Oops I forgot the r here. Let's do that again. We see that my resultsare updating to be what they were. So if I were to go and change these modelsagain, these R-squared values here, it would automatically update. So just something to think about whenyou're writing on Markdown documents, is you like to write them in a way whereif you rerun it with new models or new code your narrative resultswill all automatically update. So one last thing I want to mentionis I'm going to consider two models. A model with just miles per gallon beingexplained by acceleration but also a model where miles per gallon is beingexplained by acceleration and horsepower. So I'm going to run these and take a look at the estimatedcoefficients in front of acceleration. And, interestingly,the sign of them has changed. So previously when we had looked at,say, maybe this model here, just the magnitude of them has changed. But in this model, not only doesthe magnitude, but the sign, changes. And again, this is because there'sa difference in interpreting a parameter inthe Simple Linear Regression model, and a parameter inthe Multiple Linear Regression model. So this is simply, okay,as acceleration increases, how does that affect ounce per gallon. And something to keep in mind hereis that the acceleration variable is the time to 60 milesper hour in seconds. So a larger value is a slower car,a smaller value is a faster car. So, it sort of make sense thatconsidering no other variables as you increase the time 60 miles perhour fuel efficiently goes up. So its a sort of think of slower carsoften as sort of maybe more fuel efficient like spots car are generally not veryfuel sufficient but they're faster. Now, in the Multiple Linear Regressionmodel we have to considered that horsepoweris in the model. So the interpretation of the parameterin front of acceleration is now for a particular horsepower. So consider a large horsepower,so there's sort of maybe two classes of vehicles that would fitinto something that has high horse power which is maybe sports cars and maybelarger trucks or maybe really large cars. But so for a particular horse power,if the acceleration is small then we're probably dealing with somethinglike a sports car that's fast. Where is if the acceleration is largethat is a long time to 60 miles per hour we're probably dealing with a truck ora a very large car. And between those two, it makes sensethat the lighter sports car probably, actually, has better fuel efficiency. So when we move from a lowacceleration to a high acceleration, we actually see a decreasein fuel efficiency. So that's just something we have tobe aware of when you're interpreting Multiple Linear Regression models. It's not as simple just considerthe acceleration variable. You have to consider the accelerationvariable with whatever other variables are in the model such asin this case horsepower. [MUSIC]"
stat-420,4,5,"Unfortunately, multiplelinear regression doesn't magically solve the issue of the variability of our estimates. In this lesson, we'll updatesampling distributions, interval estimates, andhypothesis tests for use with the multiple linearregression model. We will discuss the minordifferences in how to apply these concepts tomultiple linear regression. But more importantly, thedifferences in interpretations. After completing this lesson, you'll be able to createinterval estimates and perform hypothesis tests formultiple linear regression using R. You might also realize that you mostlyalready knew how to do both, based on our past lessons."
stat-420,4,6,"[MUSIC] So we've introduced this multiplelinear regression model and we said that we can write it inthe more compact matrix notation. So we'll note that in this model eachindividual observation Y sub i is normal. Then, if we consider the entire vectorof these random variables Y sub i, this vector happens tobe multivariate normal. And then l'll come up again in a second, we saw how to apply this model to data andobtain an estimate for the beta parameters,in particular this beta hat vector. Based on what we saw in simple linearregression, we probably believe, well, if we sample some more data,apply this estimated to the data, we'll get a different answer. Here we use lower case y torepresent the data and here we use upper case Y to represent a randomvariable that's associated with the model. We want to talk about the samplingdistribution of beta hat. So again, beta hat is this vector thatcontains the individual estimates of the individual betaparameters in the model. And again, now we're consideringit to be random instead of fixed, as we had when we wereapplying it to data. Turns out that the results willprobably not be at all surprising. So the expected value of that estimatoris the thing it's trying to estimate. And in particular it'sunbiased which we like. And the variance of that vectorturns out to be this here. And we're going to call this thecovariance matrix because not only does it contain the variances of each individualbeta hat sub j, it will also contain the covariances between any twoof those individual elements. So we have the mean andthe variance of this estimator, and then the shocking reveal is that it turnsout its distribution happens to be normal. Again, we said that Y ismultivariate normal And what we have here is essentiallya linear transformation of this Y vector, that vector variables,and it turns out that a linear transformation of a multivariatenormal is, again, multivariate normal. Beta hat follows a multivariate normaldistribution with this beta is the mean vector and we said that the variancehere we call the covariance matrix. Because we'll be interested in particularelements of the covariance matrix, we'll quickly define some notation. So we'll define X transposeX inverse to be C. And then we'll denote the elementsof that matrix as we see here. The first row and first columnwe'll call C sub 00, for example, the third row andthird column we'll call C sub 22. So, again, this matrix here,this C matrix is p by p. Recall that X is an n x p matrix,and X transpose then is a p x n matrix, so X transpose X is p x p. We've essentially started at 00 sowe can end at (p-1)(p-1). And we'll see why thatis on the next slide. We want to talk about the distributionof individual parameters in the beta hat vector, forexample, beta hat sub 2. The expected value of each of these isthe thing that it's trying to estimate. The variance, then, is sigma squared times the appropriateelement of the covariance matrix. In particular the element onthe diagonal that corresponds to the beta hat that we are interested in. So, for example, beta hat 2, it's the one,two, third element of beta hat. So it will correspond to the third row, third column of the covariance matrixwhich we have labeled C sub 22. The 2 here corresponds to the 2 here. That's just a convention we use. And so similarly, for example, if we wereinterested in beta 1 we would pull this element here because beta 1 is the secondelement here, it's the second row second and we just label that C11 toindicate that it corresponds to beta 1. We have it's mean we have it'svariance and then it's for really no surprise each individual elementhere has a normal distribution, so now we are back to univariate normalwith a mean of beta sub j and the variants extracted fromthe covariants matrix. Of course,we don't actually know sigma squared, so we saw that we can use S sub esquared to estimate sigma squared. Well, this is the true variance fora given X matrix. In practice we won't know this value sowe'll have to use S of e squared to estimate sigma squared and then if we takea square root we arrive at the standard error, which again is an estimateof the standard deviation. And as we've seen before if we takean estimate, subtract off its mean and divide by a standard deviation,that would follow a normal distribution. But because we don't knowthe standard deviation, we'll divide by the standard error,which we see here. And that results in a t distribution. Now the difference is wesaid when we created this estimate here that we were dividing by n-p, because that's the degrees of freedom. And that's what we see here. We see a t distribution withn- p degrees of freedom. If we went back to talking about simplelinear regression, we would have p = 2, and we would be back to the distributionrules that we saw before. But now, in the more general case,where we have p- 1 predictors, thus p beta parameters,we need to use n- p degrees of freedom. And this is the result that we'll need tothen create both interval estimates and perform hypothesis tests inthe multiple linear regression setting. [MUSIC]"
stat-420,4,7,"[MUSIC] To verify the distribution resultsfrom multiple linear regression, we'll perform a quick simulation study. We're going to consider this truemultiple regression model here. In particular, we have predictors x1 andx2, true beta 0 of 5, a true beta 1 of -2,and a true beta 2 of 6. True variance, sigma squared,is 16, so that's the error. And we are going to consider data sets ofsize 100, so we have 100 observations. We said that the beta hatfactor is multivariant normal. And then each individual beta, forexample beta 2, is also normal. So we're going to investigatethat in particular. So the distribution of beta 2 hat in thismodel, we set a 1,000 normal distribution with a mean of beta 2, which in this caseis 6, and a variance that looks like this. So the variance depends ona particular element of the C matrix, the C matrix being X transpose X inverse. And that's dependent on the x values,which we get to decide. Once I set the values of x1 andx2 that we'll be using for this entire simulation study,we can obtain the true distribution of beta 2 hat, which will workout to be what we see here. So with a mean of 6 anda variance of 0.0236 and some change. Then we'll simulate from this situationrepeatedly and verify this result. To get started with the simulation, we'llessentially set up some parameters of the simulation which are the true values. So we're going to considerdata sets of size 100. p is 3 here because of our twopredictors and three beta parameters. And sigma, being the squareroot of sigma squared, is 4. So the first thing we need todo is set up our X matrix. We're going to need three columns. One that's all ones for the intercept,and then one for x1, and one for x2. And I've sort of arbitrarily decided thatx1 and x2 are going to be random numbers. Or not random numbers, a sequencebetween 1 and 10 that's shuffled. I'll create all of those andthen combine them into an X matrix. And let's just take a lookat that real quick. So here we see a column of all 1s forthe intercept. And then some numbers between 0 and 1,and some more numbers between 0 and 1, which will be the x1 and x2 that weuse throughout this simulation study. Again, we said that C is X transpose Xinverse, so we'll quickly create that. We will want what we define to be C sub 2,2. Now again, we labeled the first row andfirst column C 0, 0 to correspond to beta 0 andso on and so forth. So C sub 2, 2 would be the third row andthird column. So that's what we'llneed to extract there. And again, remember R is indexed at one,so I'm using 3, comma, 3. So we can think of this as shifting 2,comma, 2 by 1 to deal with the difference inour notation and how R stores things. So to obtain this variance here,we take our true sigma squared. To get sigma squared,we multiply by the appropriate value. And we see that result here in R. And then we'll look atthe standard deviation, which is taking the square root of that. Because we'll obtain this value later. Okay, sowhen performing simulation studies, it's not strictly necessaryto create a data frame. For more complicated simulation studies,it might be helpful. But we're going to besomewhat lazy here and simply work in ourglobal environment here. This is not something you should dowhen you're actually modeling data. You'll always want to keep things indata frames and refer to them that way. But for now,we're going to just deal with x1, x2, and then create another y inour global environment. So I'll do that. I'm going to create a y that will storeour simulated values from this model. We're going to perform simulationstudies of size 10,000, so we're going to generate 10,000data sets from this example here. And I'm going to pre-allocate a vectorof the fitted values of beta 2 hat for each of the simulations. Okay, so essentially what wewant to do is 10,000 times. We're going to simulate some noise, add it to the true mean function. And then once we've done that we'llhave our simulated values of Y. We'll fit a multiple linear regressionmodel and extract and store beta 2 hat. And remember there's three coefficients,beta 0, beta 1, and beta 2. So beta 2 is the third coefficient andthat's what we are extracting here. So we'll run this and I believe it'lltake roughly seven to ten seconds. We'll find out why I know that in minute. Okay, so now we want to verify. So this again is the true distribution,assuming this is the correct model. But so in beta hat 2 we havea bunch of simulated values. And if we look at the mean of those wesee that it's incredibly close to 6. It's 5.998. And again, if we had performed moresimulations it would get closer and closer to 6. And then, if we look at the variance, we see that this is alsoextremely close to the true value. So at this point, we more or less believethe mean and the variance are correct. But that doesn't necessarilymean it's normal. So the last thing we'll do is we'll createa histogram of our simulated values. And to this plot we will add a curve forthis true distribution. And to do so, we do something like this. And we see that these match rather well. We could go on and verify someprobabilities to make sure that the empirical distribution that wesimulated matches the true distribution. But we've done that before, andwe see that this is rather close. So we're believing this result here now. One thing you might have noticedis that inside of the for loop here we're alwaysdoing the same thing. We're always generating new noise,adding it to the true mean function, fitting a multiple linear regression,and extracting the coefficient. The only thing that changes isthe value stored is different. So we can consider rollingthis into a function here. So that's what I'll do real fast. So I'll say I'm going to writea function called sim_beta_hat_2. And it'll be a functionwhich takes no arguments, because we'll be pulling thingsout of our global environment. You might want to consider thatwhen you're doing this yourself. And that function will insteadof store the estimated value of beta 2, it'll just return it. So I'll load that function,I'll run it to test it. So we see that this is essentiallyperforming what was going on inside of this for loop. So we could take this function andsneak it into this for loop here, essentially replacing these lines and thishere, and that would have a new for loop. Another thing we can do isuse to replicate function. So the replicate function willessentially repeatedly run some code. So I have to give it an argument forthe number of times I want to run it, which will be the number of simulations. And the thing I want to repeatedlyrun is this function here. So then I'm going to want tostore the results of this. So I'll create a new variablebased on the old variable. We'll say beta_2_hat_alt, because I'mdoing this in a slightly different manner. And we'll run that. So this is actually not fasterthan what we'd done before. But it is a little bitclearer what we're doing. We're repeatedly simulating beta 2 hat and we're doing that this number of times andthen storing it. So if we look at the results now, we see that they are indeedestimated values of beta 2. So we can actually verify to ourselfthat performing the simulation study this way versus the for looptakes roughly the same amount of time. To do that,we'll use the system.time function. So the system.time function doesexactly what you think it does. It times how long an operation takes. So first we'll time the for loop here. So essentially inside of system.time,we just gave it an R expression. And it will time how long that takes. So I will run that, and while that'srunning I will set up the next one, system.time. And we want to do that forusing replicate. Okay, so the forloop took about 5.9 seconds. So this is the expressionI'd like to time. This gives me a chance to highlight thefact that this is the one of the places where I need to be carefulusing an equals sign. Because what this looks like,it's a function call, and it looks like this is an argument andthis is what I'm passing. So to tell R that this wholething is an expression, I simply need to wrap it in curly bracketsjust to make sure that R knows this whole thing is an expression that'sbeing passed to this function. And we'll run that. And we'll get the time on that one just toverify that while this code looks nicer, it's not actually a speed-up of any kind. I'm actually curious whatthe result is myself. It's actually slightly faster. But having practiced this a few timesmyself, sometimes it's actually longer. So what this is doing inthe background is, essentially, very similar to this exact for loop here. So one thing that we note that youshould not do is something like this. So if you're coming fromthe other languages, you may have in the paston something like this. So instead of pre-allocating a vector, what we could do is update the vectoreach time through the for loop. So combine what is previously stored withwhat you calculate each time through. And this is a perfectly normal thingto do in some programming languages. But in R, let's find out whywe don't want to do this. So I want to times something. And I'm going to want curly bracketsto tell it this whole expression is what I want to time. I'll paste that in there andwe'll run this. And essentially what's going to happenis this is going to take more time. The reason being is that this line here issomething you wouldn't want to do in R. Because it's going to dealwith this in memory poorly, making a lot of copies andtaking up more time that way. So we see that it didindeed take more time. And the more simulations that we do,the worst this would actually perform. Because it would have tobe copying over longer and longer vectors as this operation grows. So that's just to say that we'velearned two new things now. The system.time function, so you can time things in R to see ifyou're sort of writing good, fast code. And the replicate function, which is a nice way to sort of abstractsome of these simulation ideas. Because we're going to be repeatingprocesses over and over again. [MUSIC]"
stat-420,4,8,"[MUSIC] So now that we have some distributionalresults for the multiple linear regression model, we can first begin toperform hypothesis tests. So the very first thing we'll dois perform hypothesis tests for a single data parameter in the model. This test will be carried out very similarto how tests had been performed in simple linear regression, but in the end,it'll have a slightly different meaning. So in terms of carrying out the test,for example, we could perform a test about beta 2 here,and the null hypothesis would be that it's 0 and the alternativehypothesis would be that it is not 0. To carry this out we need tofind a test statistic and then find the distribution of that teststatistic and then calculate the p-value. Well, it turns out, because of what we sawbefore, this test statistic is more or less the same as what we saw inthe simple linear regression case. We consider the estimate that we derivedfor the parameter we're interested in. The null hypothesis is going togive us the hypothesized value, which in this case is 0. So when we perform this test, we'll beassuming the null hypothesis is true, so we'll be assuming thatthis is the true mean. And then we divide by the standard error. because again, we don't knowthe true standard deviation, so this will be our estimate ofthe standard deviation from data. Now we have a way tocalculate the test statistic. And then, also, we saw previously thatif we subtract the true mean and divide by the standard error, and we considerbeta hat here to be a random variable, this thing would then follow a tdistribution with n-p degrees of freedom. Since we have the distributionof the test statistic, we have a way to calculate a p-value. And then with that p-value, we couldgo and make decisions about whether or not this parameter is 0 or not. But we want to pay close attention to whatit means for that parameter to be 0, or for it to be not 0. We'll consider a simpler multiple linearregression model with two predictors, x1 and x2. We'll perform a test about whether ornot this parameter, beta1, is significant. So the null hypothesiswill be that it's 0, and the alternative hypothesiswill be that it is not 0. We can discuss the modelswithin each of the hypotheses. In the null model, we have a regressionmodel with only x2 as a predictor, because we have forcedbeta one to be zero. However, in the alternative hypothesis,because we allow beta1 to be non-zero, we now have two predictors in the model,x1 and x2. Back in simple linear regression,had we performed this test here, it would've been what we callthe significance of the regression test. Because if we fail rejectthe null hypothesis, there would have no relationshipbetween y and the predictors. However, in this test,in the multiple linear regression model, because in both the null andthe alternative, this x2 variable exist, this test is nolonger a significance of regression test. So it's not testing for a relationshipbetween y and the predictors. It's testing is there a significant effectof x1 given that x2 is in the model. So that's an important distinction. This test is not about an overallrelationship between y and the predictors, it's aboutthe relationship of a specific predictor, given other predictors are in the model,in this case, x2. We'll see later how to develop a newsignificance of regression test for the multiple linear regression setting. But importantly, we can stillperform single parameter tests, they just have a slightlydifferent meaning. [MUSIC]"
stat-420,4,9,"In addition to hypothesis tests we can also perform interval estimation in the multiple linear regression model. And the first thing we'll do is create confidence intervals for model parameters, in particular one of the betas from the multiple linear regression model. Again, confidence intervals will take the form of an estimate plus or minus a critical value times a standard error. We already have our estimate for the parameter of interest. We have the standard error of the parameter of interest. And so the only thing that's really going to change between multiple and simple linear regression is the critical value. It still follows a t distribution. We'll still utilize Alpha to get us the correct level of the confidence interval. So the only thing that's changed is the degrees of freedom of the t distribution that we're extracting the critical value from. And actually it hasn't really changed. It's just become more general. So again we could use this expression in the simple linear regression case just remembering that in that case there's one predictor, so P would be two because there's two beta parameters. We can also create confidence rules for the mean response. So now we're talking about making a confidence rule for the expected value of y given that X takes some particular value. This value now is vector value because we have multiple predictors in the model. So this X Sub-Zero will be a vector that looks like what we have here. Here is the value of x 1. Here is the value of x 2 and so on and so forth. Here is the value of x sub p minus 1. Together these values of the different predictors along with 1, because we'll need that for the intercept, this is essentially, we would like a mean at this particular x value. So if we plug those values into the true model, what we see here is exactly the expected value of y given that X takes those values that we've defined here. Also though, if we take those x values and plug them into the fitted model we have a predictive value at these particular values of the predictors that is an estimate of the true mean of y given those values of the predictors. This is our estimate for the quantity that we would like to make an interval about. And we see that it's unbiased. So the only other thing we need is a standard error for this quantity here which we see here. So we note that it's a function of three things. S of V, which is the square root of our estimate for Sigma squared, x transpose X inverse which we had previously defined to be c, and x sub zero, the vector of x values that we are predicting at. So if you think back to the simple linear regression case, the standard error for the predicted values here, which we're using as an estimate of the mean of y, we're dependent on the single X and we noted how the further way that single X was from it's mean, the larger the resulting interval was. You might think about how that relates to this scenario and how X sub zero will control that as well. But in the end we end up with a confidence interval for the mean response. Here we have our estimate. Here we have our standard error and we have the usual critical value. And again, importantly, in the multiple case we're using n minus p degrees of freedom. So then the last thing we want to do is look at a prediction interval for a new observation and very similar to the simple linear regression case, we just need to make one minor modification. Here was the standard error for a confidence interval. Then this here will be the standard error for a prediction interval. The confidence interval for the mean response was dealing with a mean but the prediction interval is dealing with a new observation while y hat at the x zero that we're predicting at is an estimate of the mean, we also need to take into account the variability of the observations about that mean. And so, because we have to deal with that, that's where this additional one comes from, for the same reasons and derived the same way as we had in simple linear regression. So in the end our estimate is the same. It's just that our standard error is different. In the end we see that prediction intervals are slightly wider versions of confidence intervals to the mean response that also capture new observations instead of just the mean."
stat-420,4,10,"When performing tests about a single predictor, as well as creating interval estimate, the difference between simple integration and multiple integration in R is very minimal. We'll return to the fuel efficiency data that we had seen previously and we'll fit the model that we had used previously where we have two predictors; one is the weight of the car, the other is the year of the car. And the first thing we'll do is we'll check the summary information. This is actually something we've seen before and it actually looks very familiar. So the only difference between this summary output and what we had seen previously in simple integration is the addition of a row here in this coefficients output. We'll actually look at specifically that. Now essentially, each row here corresponds to a test about a single predictor. For example, this row corresponds to a test about the predictor for weight and this row corresponds to a test about the predictor for year. And again, they have the slightly different interpretation than they did in this simple integration sense. So now, testing for weight is not a significance or regression test like it would have been if it was a single predictor; now it's about whether or not weight is a significant predictor when year is in the model. Each one of these rows is about a single predictor when all of the other predictors considered in the overall model are being used. We're using the list syntax here to extract a particular element of the list which is stored in miles per gallon model here and we could go a step further and say maybe we wanted the p value for testing for beta two here, in particular whether or not zero. So we could extract both the row and the column to obtain exactly this value here and we see that's what happens. Moving on to interval estimates, we'll first create interval estimates for the beta parameters of this model here. And to do that, we'll use the exact same syntax we used in simple integration, but which is using the continf function and the fitinf model. And we can specify a level. And now, instead of simply two rows - one for the intercept and the predictor - now we have multiple rows; one for the intercept and then the remainder for the predictors of the model. But one thing we might notice here is that - so for example, if we focus in on the interval for a year here, we notice that it does not contain zero. So that's equivalent to saying well, if we performed the hypothesis test about beta two at, say, an alpha of - in this case 0.01 - we would reject because it does not contain zero. And that's the same decision we would have made here in the test because this is much less than 0.01. To create confidence intervals for mean response and prediction intervals for new observations, we will have to essentially create new observations. So the new observation should be a data frame that looks similar to the data frame for the original data. So if we look at the data frame of the original data, in particular, it needs to be a data frame that has columns for each of the predictors. So in this case, it was weight and year. We could specify any data frame that has those two predictors, but the only two that are necessary are those two predictors. So I'll first create a data frame that looks something like this. So we're going to make a confidence rule for the mean response; that is, miles per gallon for a car that weighs 300 to 500 pounds and is from 1976. And additionally it, we'll do the same for a car that is 5,000 pounds and from 1981. So once we've created this data frame and it has the necessary variables for the model that we fit, everything is the same as what we did in simple linear regression. So we use the predict function, we give it the model, we now pass through the data frame we just created, we specify the confidence in a rule because we want a confidence rule for a mean response and we can specify a level. So these are 99 percent confidence rules for the mean at these x values. And we see again that R gives us both the point estimate - so that's the y hat value at this x value - as well as the interval estimate for the specified level. And then the only thing we need to change to get a prediction interval instead of a confidence interval is change the interval type to prediction. And just like simple integration, we see that the estimates are the same, but the intervals are wider because again, as in simple integration, an individual observation is much more variable than a mean. Need to be - we need to be a little bit careful about how much we trust these estimates. So something we did before is look for the extrapolations - and we didn't like those - and we're sort of a little less certain about using those intervals. So here, if we look at the weights of the cars that we've created here, they are both in the range of weights of cars in the data set. Similarly, if we look at the years of the cars that we created, those are also in the range of the data set. However, if we create a plot - so this is not a plot of the response of the predictor, these are both predictors. We see that - sort of this region here, this sort of blob of blue data - is points where we have observed data. So if we add the two points that we are predicting at, we see that one is sort of very much contained within the blob of data we've seen, whereas the other is very much outside of the data. So this, sort of, very large interval - to begin with, we're actually somehow uncomfortable with it because it is an extrapolation we see here and this is actually a fairly low value. So we maybe have been suspicious about it to begin with. A commonality in both the tests about a single predictor and each of these confidence rules was a t distribution with n minus p predictors - sorry, p is not a predictor - n minus p parameters. So when there are - in this case - two predictors, we have three beta parameters. So our p would be three. Let's go ahead and take a look at this t distribution; in particular, let's obtain the critical value that's being used in these intervals at the 99 percent confidence level. So first what I want to do is create the confidence interval, specifically for the - let's see, which parameter do we want to do? Let's use the weight parameter. So here we are creating a confidence interval for beta one here and we're doing it at the 99 percent confidence level. So we can easily obtain both the estimate and the standard error for beta one hat using the summary information. So if we bring back the summary information - in particular, the information about the coefficients - we see we have our estimate here and our standard error here. So what I'm going to do is I'm actually going to extract both these values and save them. So our estimate is the summary information and in particular, the row for weight and the column for standard error. And I'll do something similar - oh, actually, no - there should be estimate and the standard error will be what I had incorrectly typed previously. So we can see that if I do this, I'm extracting here the estimate and if I do this, I am extracting the standard error and I'll run and store those. So now what I want to do is create the confidence intervals using this estimate, this standard error, but I want to myself create the critical value. So the first thing one needs to do is obtain and - which I could extract using the rows of the original data set autompg - and I also need p, which I incorrectly stated earlier - but in p is the number of beta parameters - and so to do that, I could look at the length of the coefficients of that model. And we see that - oh, that's the data set, not the model - so that output makes sense, actually; so we see three. So if n is 390 and p is three, the degrees of freedom is 387. So I want to consider a t distribution and I want a value from the t distribution corresponding to a particular probability. So here I need to put the probability and here I'll specify the degrees of freedom. So if we're saying that the confidence level is 99, the significance level is one minus that, so one minus 0.99. And then I need to divide this by two because of the two sided confidence rule. So this is the probability that I'm interested in here. And this would be my critical value, but by convention, we like critical values to be positive. So I'll simply take the absolute value of this and there is my critical value. I'll store it in a variable called critical and then I'll make confidence intervals like this. I'll do estimate minus critical time standard error and actually, I'll display them both at once so I'll create a vector with the lower bound that we have here and the upper bound here. And I'll change this to a plus and we'll see the resulting confidence interval should be exactly what we had seen previously in R. So there we have it. We essentially verified that we are correctly obtaining the critical value according to this two-distribution width and minus p degrees of freedom."
stat-420,4,11,"Now that we areconsidering models with multiple predictors, for any given dataset, we have a much larger number of models that wecan choose from. In this lesson, we'llconsider both a specific and a general hypothesis test in the multiple linearregression setting. The specific test willanswer the question, are any of thesepredictors useful? The general tests willallow us to compare two different multiplelinear regression models. We will discusshow previous tests for both the simple andmultiple regression models are examples of these, and how the simple andmultivariate cases have slight differences. We'll introducesome new functions, but also reuse a fewfavorites from the past. After completing this lesson, you'll be able toperform all these tasks using R and interpretthe results."
stat-420,4,12,"[MUSIC] In order to build towards a test forthe significance of regression, we need to go back and talk aboutthe decomposition of variance again. Which we saw in simplelinear regression and is actually almost no differentin multiple linear regression. So if we consider the multiplelinear regression model here, we see that we get the sameexact decomposition of variance. Again, so we say we have the sum ofsquares total is equal to the sum of squares error plus the sum of squares forthe regression. So the only thing that's changed is nowour predictive values are from a multiple linear regression model instead ofsimple linear regression model. And so our sum of squares error quantifiedthe unexplained variation in the response variable while some ofsquares regression quantified the explained variationin the response variable. Since we have this decomposition ofvariation, we can talk about R squared for the multiple linear regression model. And it's exactly the same as whatwe had seen in the simple linear regression model. It is the sum of squares for the regression divided bythe sum of squares total. So it still tells us the proportion ofthe response variable that is explained. But now it's being explainedby a more complicated linear relationship using multiplepredictors instead of a single predictor. So back in simple linear regression,we saw how we could test for the slope parameter and that determinedthe significance of the regression. So if there was no slope, there was nolinear relationship between x and Y. And we can do a similar testin multiple linear regression. So we determined, we could say, well,do we need this x2 predictor here, and we used a test that looks like this. So whether or not beta2 was zero,but this doesn't tell us about the overall regression, thisonly tells us about a single predictor. So even if we fail to reject the nullhypothesis here and we say that x2 is not needed in this model, we stillhave all the other predictors say, x1, x3 through x sub p-1 in this model. And there's still some relationshipbetween Y and the predictors. So the question we want to ask nowis are any of the predictors useful? So we want to know overall,does this regression help explain Y, or does the regression tell us almostnothing about Y and is not necessary. So we'll now develop the significanceof regression test for multiple linear regression. So to do that, essentially we needto have a new null hypothesis that doesn't set just a singleparameter to zero, but sets all of the parameters in frontof the predictors equal to zero. So our null hypothesis looks like this,we're resetting each of the beta parameters in fronta predictor equal to zero. So then we can talk about the null model, which is the model underthe null hypothesis. So since all of these were set to zero, we simply have Yi isBeta0 plus some noise. And in particular,if we estimated this Beta0 here, it would turn out to be justthe average of the Y observations. So I'll write y hat sub0i is equal to y bar. So instead of just yi hat, I put a zero here to indicate this isthe fitted value from the null model. But again, in this particular null model,because we have just an intercept, we're essentially just modelingy as the average of the y's. And importantly, this model showsno relationship between y and x. The alternative then will allow someof those parameters to be non-zero. They don't have to all be non-zero, butat least one of them has to be non-zero. So then we'll write the full model as thefull multiple linear regression model that we had seen. So we allow for any of the predictors to have a non-zerocoefficient in front of them. And we'll write y sub 1i hat as thepredicted values when we fit this model. Just so now we can differentiate betweenthe fitted values of the full model and fitted values of the null model. So now that we have this idea of the fulland null models, we're going to use the decomposition of variation tocreate what's called the ANOVA table. Which we'll then use todevelop the test statistic and complete the significanceof regression test. So here we have what's called an ANOVAtable for analysis of variance. Now we see some familiarquantities on this table, and then along the way we are goingto derive some new quantities. So the first one that you shouldnotice right away is here is sum of squares total, here is sum of squareserror, here is sum of squares regression. So these first two add up to the sum ofsquares total, as we have seen before. So we're sort of justwriting the decomposition of variation in this column here. Now along the way, we talked about using ssub e squared to estimate sigma squared, and we talks about the degrees of freedom. So we said that s sub e squaredis exactly the original data minus the fitted values squared,all over n-p. And we called this n-p quantitythe degrees of freedom, which we see pop up right here. Again, we're just using y sub 1i now just to indicate that theseare fitted values from the full model. And the way we said we could thinkabout this is yi was just data, and there were n data points. yi hat was summarized byp beta parameters, so we had p fitted values there. And so that's how we lost p degreesof freedom from the original n, and that's how we obtained this n- p. So similarly for sum of the squares total,there are again n data points here. And y bar is the estimatedbeta0 in the null model. And that was using one beta parameter,so we see n-1 here. And then a similar argument is made forsum of the squares regression. There are p beta parameters here,there was one beta parameter here, so we see p-1 degrees of freedom. So we also see that the degrees offreedom add up in a similar manner to the sum of squares. So p-1+(n p) equals n-1. So moving to the right of the table now,we see that the mean square is simply the sum of squares divided by the degreesof freedom for both regression and error. And this sum of squares error dividedby n-p, which we'll call the mean squared error is exactly what we have beenusing to estimate sigma the whole time. So we've actually seenthat quantity before. But then we have thisfinal quantity called, and it's the mean squared error's regressiondivided by the mean squared error. So directly writing off the F statistic,it looks something like this. So question we want to ask ourselvesis when is this statistic large, and when is this statistic small? So the first thing you mightnotice is that this statistic will always be non-negative. So because we're squaring this quantityand we're squaring this quantity, we're going to end up with positive valuesfor this test statistic everywhere. We need to remember that this quantityhere is the sum of squares for regression, and this quantityhere Is the sum of squares error. And because they add up to sum ofsquare total, when one is small, the other is large. We also need to remember that this isthe prediction from the full model and this is the predictionfrom the null model. So the F test statistic will be small whenthe prediction from the full model and the null model are verysimilar to each other. Whereas the F test statistic will bevery large when the full predictions and the null predictions are veryfar from each other. That'll also correspond to the fullmodel having a small error. It turns out that under the nullhypothesis, so if we assume that this statistic here is random, that thisstatistic has an F distribution with p-1 and n-p degrees of freedom. So this specifies a particularF distribution, and that's what we will useto calculate the p value. So our p value now will bethe probability that this particular F distribution is larger than the observedvalue of the test statistic. So let's draw an example,so again we said that the F distributions will be starting at zero,and then take values larger than it. So there are many differentattributions which are specified by the two degrees of freedom. But often they look something like this,where they're right-skewed. Now suppose that we obtain a valueof the test statistic that is here. So now what we want to think about is what values are unexpectedunder the null hypothesis? What values of the F test statistic wouldbe more extreme than what we observed, assuming the null hypothesis is true? So if we assume the null hypothesis istrue, and we fit the full model here, we would expect most of these estimatedbeta parameters would be close to zero. And then in the test statistic here,we would expect the predictions from the fitted full model to the nullmodel to be very close to each other, which would result ina small F test statistic. So that means that values over here are ones that we expectunder the null hypothesis. Whereas if we fit the full model andwe obtain large estimates for some of these beta parameters, that'sunexpected under the null hypothesis. And that would correspond to then thepredictions from the full model being far from the predictions of the null model. And again, if we assume the null hypothesis is true,we don't expect that to happen. So that results in a larger F statistic,and that's these values over here. So a p value being a thing that sayswhat's the probability of seeing something more extreme than what we've observed, ourp value is exactly here under the curve. So, and again, as always, if the p valueIs less than our pre-specified alpha, we would reject the null hypothesis. Because again, p value is small when Fis large, and that's exactly when our predicted values in our full model are faraway from what we expected under the null. So summarizing the significanceof regression test then, again, we want to know are any ofthe predictors useful for explaining the relationship with Y. We hypothesize that noneof them are useful, and our alternative is that atleast one of them is useful. So we carry out the test, and if we decide that we fail toreject the null hypothesis, then there is no linear relationship,linear being defined like this here. And if we reject the null hypothesis,there is some linear relationship. Meaning at least one of these beta1,2, through beta p-1 is non-zero. So in this situation,if there's no linear relationship here, this regression is not useful forpredicting Y. Whereas if we rejectthe null hypothesis and we say there is some linear relationship,that means the regression is helpful for either explaining or predicting Y. So we will see how toperform this test in R, and we'll utilize the fuel efficiencydata we had seen previously. [MUSIC]"
stat-420,4,13,"[MUSIC] Let's consider two models here. The first model here has p-1 predictors, so it has p beta parameters. The second model here has q-1 predictors,thus it has q beta parameters. So let's say that in this setup,p is bigger than q. So then we say that thismodel here is more complex. The more parameters a model has,the more complicated it is, the more difficult it is tointerpret any given predictor. So a question we might ask ourselves iswell, which of these two models is better? Now, it turns out that just dueto the way these squares work, the residual sum of squares forthe larger model will never be worse than the residual sumof squares for the smaller model. Similarly, because they're essentiallyjust related quantities, the R squared for the larger model will never be smallerthan the R squared for the smaller model. So because of that, we don't really want to use eitherof these to choose between them. Because if we did,we'd always simply pick the larger model. We like models that are both small,and thus interpretable. So unless a larger model hasa significantly lower residual sum of squares, or a significantly increased Rsquared, we'll stick to the smaller model. Because it has most of the sameperformance in the predictive sense but it's smaller and easier to interpret. And it could be just that the smallerresidual sum of squares in the larger model is simply due to chance,and so we want to develop a testto compare two models. It turns out if we considerwhat are called nested models, the significance of the regressiontest that we just performed needs to be only slightly modifiedto allow us to do exactly this. We say that a smaller model is nestedinside of a larger model if each of its predictors is containedin the large model. So its predictors are simply a subsetof the predictors in a larger model. So for example,if we consider this model here. If I write a smaller model say Y = beta0 + beta1 x1 + beta2 x2 plus some error. We would say that this model is nestedinside of the larger model because this predictor is here, this predictor is here. Whereas if I writea different smaller model, say Y = beta0 + beta1 x1 + let'ssay beta5 x5 + some error. This predictor does not appear inthis model so this is not nested. It is a smaller model butit is not nested inside of this, we'll call full model here. So if we return to this setting,again, here p is bigger than q. We'll now refer to this firstmodel as the full model. And by setting particular parametersin the full model, in particular, this one here as well as those that are notin the what we'll call now, null model. We can use this hypothesis todifferentiate between these two models. So essentially, the null hypothesis tellsus which parameters in the full model does that equal to 0, andthen the resulting model is immediately nested inside ofthe full model and it is also smaller. Much like the significanceto the regression test, the alternative here is that at leastone of these parameters is non-zero. Carrying out this test is essentiallygoing to be just a generalization of the significance or regression test, and to finish off the test we needto again create another table. So now instead of regression, error andtotal, we'll have difference, full and null as our sources of error. So here we have the predictedvalue from the full model. And here we have the predictorvalue from the null model. So notice that we essentiallyhave two errors now, the errors made by the full model andthe errors made by the null model. So essentially what we're doing here iswe're now decomposing the variation again. But instead of decomposing the totalvariance in y, we're decomposing the error made by the null model becausethe full model will explain more of that. But what we want to figure out is itexplaining a significant amount more. So the things that change here, again remember the full model we wereusing p-1 predictors for p parameters. And in the null model we were usingq-1 predictors for q parameters. So we have this new degreesof freedom which is p-q. And then again, so similarly we have p parameterssummarizing to get these predictions. And we have q parameters summarizing here. So we have n- p andn- q degrees of freedom. So essentially, we've changed both the sumof squares that we're considering, and the degrees of freedom we're considering. But it's generally the same setup. If we consider a null model,that only had q = 1 predictor, we would see that this would boil down to exactlythe significance of regression tests. So if we carry out this tableall the way to the right, it will result in the new F Statisticwhich we can look at here. So this F Statistic will operate inthe same way as the previous one. So if we assume the nullhypothesis is true, this statistic follows an F distributionwith p-q and n-p degrees of freedom. But sort of importantly, again, we have to ask ourself, okay,when is this test statistic large? And when is this test statistic small? So it'll be again, small,when the predictions from the full model are very similar tothe predictions from the null model. And it will be large whenthey're very far apart. And that would be unexpectedunder the null hypothesis. So, we obtain a p value in the same way, and then make a decision by comparingp to our prespecified alpha. And then in the end, we can decide to say,fail to reject the null hypothesis, or we can reject the null hypothesis. So if we fail to reject the nullhypothesis, we prefer the smaller model. And if we reject the null hypothesis,we prefer the larger model. So this gives us a general setup forcomparing two nested models. And because we have this preference forsmaller models, it would take a significant difference forus to choose the larger model. So it turns out there's some similaritiesbetween what we just did and a bunch of previoustests that we had seen. So, let's consider this fullmodel I have written here. Let's assume we only have two predictors. We saw how we can perform a test for,let's say, beta1 here. So that test looked like this. The null hypothesis was that beta1 = 0. The alternative hypothesis wasthat beta1 was not equal to 0. But let's write out now the null andalternative models in this set up. So the null model would be y is theintercept plus beta2 x2 plus some error. And the alternative would be y is well,just this full model, okay? So the way we had done thispreviously was with a t test and this we see, well,these are nested models. So we now know that we cando this with an F test. So we don't have two different tests. It actually turns out thatthese are equivalent. So if we perform the test either way and obtain the p value,we'll get the same p value. And it turns out the t teststatistic if we square it, it is equal to the F test there. So a test about a single parametercan be thought of as a t test or it can be thought of as a nested model wherethere is a difference of parameter of 1. So in this case p = q + 1. Now, we can also do somethingwhich is I'm going to say, well, let's say we only had onepredictor which was x1. So I'm going to get rid of beta2 x2 hereand so these hypothesis say the same, but I'll need to get rid of beta2 x2 here, andI'll need to get rid of beta2 x2 here. So it turns out that well, look, this iswhat we had called the significance of regression test now in the simplelinear regression setting. And now what we have here are nestedmodels and actually this is also a significance of regression testbecause in the null model there is no x. So it turns out that the t test and the F test, in the case of simplelinear regression, are equal. Again, they would giveus the same p value, and the t test statistic squaredwould be the F test statistic. [MUSIC]"
stat-420,4,14,"Now we'd like to actually perform significance of regression in R. We'll return to the dataset that we've been using, which I already have loaded here and we see here. So the significance of regression in the multiple linear regression setting has to do with multiple parameters at once. So returning to the model that we've seen a number of times, here we have a multiple linear regression model with x_1 and x_2, in this case, for weight and year, respectively. We want to test for significance of regression in that model. So the null model, we'll set both beta_1 and beta_2 equal to zero, which will result in a model that only has an intercept. So we'll actually first fit both those models in R, which I'll label the null model and the full model, the null model being fit by using a one in place of any predictor variable - this will fit simply intercept - and here we will fit the full model as we have a number of times. Checking these, we see that the null model indeed only has an intercept and the full model has estimates of the three parameters that we expect. So to perform the significance of regression, we can essentially use the ANOVA command and specify the null model first and the full model second. And we'll do that - and we'll see that what we get back is an ANOVA table that looks somewhat similar to what we had written before, but somewhat different. So this ANOVA table can be used for more than just the significance of regression test. So it's written in a way sort of specific to the fact that we have a null model and a full model. So what we have here are the residual sum of the squares for the null model and what we have here is the residual sum of the squares for the full model. So it turns out that the residual sum of the squares for the null model, a model with no predictors, is exactly the sum of the squares total - that's the total variation in Y - and this residual sum of the squares for the full model is exactly the sum of squares error - that's what's left over from using the full model to predict. So then what we have here is the difference between those two, which we have been calling the sum of the squares for regression. And along the way, we have the degrees of freedom corresponding to each of these. And then it performs the resulting calculations, leading to both the F-statistic, which we said follows an F-distribution with p minus one and n minus p degrees of freedom and automatically calculates the p-value, which in this case is extremely small. So at an alpha of, let's say, 0.05, we would reject the null hypothesis. We would say the regression is significant; at least one of x_1 or x_2 is a useful predictor. It turns out we've actually already seen the results here, this is just a way to organize it in the table. So if you look at the summary information of the full model, it turns out that this p-value here is exactly - let's scroll up a little bit - this p-value from the table here, this F-test statistic is exactly the F-test statistic and these two degrees of freedom are the degrees of freedom for the F-distribution that the test statistic follows, which we see here and here. We can also talk about, while we're doing this, the R-squared for the full model, which again is proportion of variation in Y, which is explained by, in this case, this relationship which we said was sum of the squares regression divided by sum of the squares total. So here we see that being automatically calculated for us, but if we pull out the - let's see, here's sum of the squares regression and divide it by the sum of the squares total, we see we get the same answer. We should also note that in this example here, it turned out that both the individual t-test turned out to be also very significant, but that doesn't have to be the case. It could turn out that, individually, both of these are actually not significant, but the overall regression is still significant. So the fact that these two are significant is not actually what's determining the fact that this p-value is significant. It could be that we need both of them in the model for it to be significant and individually, they're both insignificant with the other in the model. So the significance of the regression test is actually a specific case of the broader idea of testing nested models. So we'll do that as well. What I have here are two models that are nested. I have a model with just weight and year and then I have a model specified here that is using all of the available predictors in the data frame. So this syntax here is automatically fitting this model here, which is the model that essentially, for this data frame - it uses miles per gallon as the response and all other available predictors as predictors. I'll fit those. So we see that this model is indeed nested inside of this model as weight is in the full model and year is in the full model. And then I'll again use the ANOVA command to perform a test about these two nested models. So the test we'll be performing is this here, because the difference between these two models is cylinder, displacement, horsepower and acceleration. So we would be setting all of the coefficients in front of those variables equal to zero. It turns out that here, we obtain a very large p-value and a very small F-test statistic - it's actually a small - sorry, a large p-value because it's a small F-test statistic. And that would tell us that we would fail to reject the null hypothesis; we prefer the null model; we don't believe that these variables are necessary when weight and year are in the model. So in this case, this F-test statistic follows an F-distribution with p minus q and n minus p degrees of freedom. Let's see if we can verify how to calculate this p-value for this F-test statistic. To calculate probabilities about an F-distribution, we'll be using the pf function. We want a probability about this value. And now we need to specify the remaining arguments, which are degrees of freedom one and degrees of freedom two. So degrees of freedom one is going to be p minus q, which is essentially the difference in number of beta parameters, which is also equivalent to the difference in the number of predictors, which we see is one, two, three, four. That's degrees of freedom one. Degrees of freedom two is n minus p, so n being the number of observations, p being the number of beta parameters in the full model. Since I don't remember those off the top my head, we'll do a little bit of calculation. n is the number of rows in the dataset and p is the number of beta parameters in the full model, so that's the length of the coefficients in the full model. And so I have 390 rows - we'll subtract those - so I have 383. Okay. So when I run this command, I get a number that is not the p-value; because again, this finds the probability of a value of an observation less than this particular value, but we said that extreme observations compared to what we saw would be larger p-values - that would be what we don't expect under the null hypothesis. So what we want is actually one minus this probability and we see that this now matches what R was calculating. So the other thing we talked about when we were dealing with nested models was the fact that there is an equivalence between certain t-tests and certain F-tests; in particular, when we have a difference of number of parameters of one. So here I have two models, one with two predictors - weight and year - and one that's larger, it has one additional variable, which is acceleration. So what I want to do now is run the usual nested models F-test. And we get a - what would be a non-significant p-value at a significance level of 0.05 and that's using the F-test. But we also know that there's an equivalent t-test using just this model. So if I look at the summary information here and I want to look at the row for acceleration, we see that - scrolling back a little bit - this p-value is exactly this p-value, because this is dealing with setting the beta parameter in front of acceleration equal to zero, which is equivalent to looking at the two models that we have here. And then the one other thing we said is that we could look at this t-value here and square it and that should be the value of the F-test statistic. Now, we see these two values and we notice that they're not actually the same, so your immediate reaction should be that I'm lying. But what's actually happening is that when R is printing the results here, it's doing a bit of rounding; the same here. So what I'm gonna to do is I'm going to extract the actual value here instead of just copying and pasting it over and then squaring it and see if it becomes this number. So let's see, I need to look at the summary information. I would like to extract the coefficient portion. I want the row for acceleration and I want the column for the t-value. And now we see that we have much more information than here because, again, there was just doing some rounding for nice printing, but now I have access to the full value. And when I square it, this value is now rounding properly to this value of the F-test statistic. So we see that whether we use the F-test or the t-test, we get the same p-value and it turns out that the t-test statistic in this case is, when we square it, equivalent to the F-test."
stat-420,4,15,"Okay, so now that we have atleast one tool for modeling, that is multiple linear regression,we can talk about modeling in sort of a broader sense and see howmultiple regression fits into that. So the general idea when we want to modela response y is that we want to find some functional relationships between y andall of the predictors. And we'll allow for some noise. Because when we're modeling something, we generally don't expectit to fit perfectly. We want to sort of representthe data in a reduced way. But we'll allow for some errors. So we allow for some noise there. But overall, we want to findsome relationship between y and a bunch of predictors. So to sort of break this down,we'll consider families of models. So linear models being oneof those families of models. And then within a family of models we'llconsider different forms they take. We've seen multiple ways we can applylinear regression to a single dataset. And then lastly,once we decide on the family of form, we'll talk about fitting models to data. So once you've done all those things,decided on a family and form of the model and then fit the model,you finally have your model. So I'm talking about families and models. The two very broad categorieswould be parametric models and non-parametric models. We've only seen parametric models and in particular we've onlyseen linear progression. So linear models is sort of considered afamily of models, and they are parametric. So the family of linear models is simply we model y as a linear combinationof predictors and stimulants. So that's the family of linear models. You have a bunch of predictors,you take a linear combination of them. That's how you're modeling y. Those linear combinations havethese coefficients, Beta 0, Beta 1 and Beta 2 in this case. Those are the parameters in the model. So to fit this model, what we have todo is find the best possible values for those three in this case. That would be estimating the parameters. So when you have a parametricmodel you specify it like this. It has these unknown parameters andwe need to find them. So how does that look? Well, on the left here we have somedata for considering a single predictor. And in a parametric sense, sousing one linear modeling here, we want to find the best linethat represents this data. So that would be finding the bestintercept, beta 0 and slope, beta 1. Because we're representing this as a line,we only need to find those two parameters, beta 0 the intercept and beta 1 the slope. On the right here, we have an example ofnon-parametric regression, there are many. Here we're talking about Smoothing. So to find the y value ofany particular x value, instead of having these slopesintercept what we do here is, say if we wanted to findthe y value when x is 3. We consider maybe y values for data we sawthat had an x value between two and four. And take some sort of weighted average forthose things. So essentially,we're just smoothing out the data. So there's no parameters to be found. It's just all based on the data. And some sort of algorithm or way of combining the datato obtain trending values. This is just one exampleof non-parametric. Regression, we consider regression trees. A lot of things you'll probablysee in a machine learning course. Okay so now that we've assumed some familyof models, in this case linear models, we need to decide on the form of models. So far,there's not much we can do with that. So for families in linear models, andwe haven't introduced transformations yet but we will. We haven't introduced interactions yetbut we will. Really the only thing thatwe can modify here is which predictors we'regoing to use in the model. So for example,if we have predictors just x1 and x2, and we're consideringa family of linear models, the possible forms it could take is,well, we could use none of the predictors. We could use one of the predictors. Here I chose x2, it could've been x1. Or we can use both predictors. So those are differentforms the model can take, but they're all in the samefamily of linear models. Right now, these are the only three or four if you consider x1 ina single predictor case. These are the only forms of the model thatwe can sort of envision right now, but eventually we'll allow formore flexibility here. So for example,in this last model up here, x1 and x2, the way that x1 affects y isnot at all dependent on x2. Later on we'll see how to change that and we'll introduce some moreforms of this linear model. Okay, so once we have the family and form we need to actuallyfit the models to the data. So we're going to just consider simplelinear regression here real fast and talk about fitting that to data. Sort of the obvious way to do that is thatthe last one here is least squares, but it's not the only way. Instead of looking at squared distancewe could look at absolute distance, which is the second one here, or we couldhave looked at the first one here which is finding a line that minimisesthe maximum error you can make. Think about how to do that computationallybut I think you'll realise pretty quickly that least squaresis the much easier way to do this. Okay, so to fit the model, again,we have to assume a family and a form. Sort of when you assume that thenthe only thing that's left to do is find the best value for the parameters. In this case beta zero beta one andbeta two. Once you then fit the model you havethe estimates of the parameters in this case 1.5, 0.2 and 2.1 And we put a little hat on top of Y andwe call that the fitted model. So this is what we'd use to sort ofmake predictions with new data and that kind of thing. And how this works in Ris we use the LM command, which is what's telling R we want touse the linear model family of models. And then this formulahere y tilda x1 + x2, that specifies the form of the model,or formula is the short hand in R. And R, by default it willassume we want an intercept so we don't have to put that here, it's sort of automatically assumes we wantto estimate this beta 0 parameter here. Okay.So we've seen a lot of simulations so far. And I want to sort of talk abouthow this all ties together. So, when we've been simulating data, we'veknown a lot of things as we set the truth. We knew the family of the models, so we knew there was a linear relationshipbetween our x's and our y. We knew the form of the relationship, that is, we specified which axisactually had a relationship with y. And on top of that, essentially,we could say we knew the fit, but more importantly, we could say weknew the true values of the parameters. So we knew all that,and then, additionally, we knew how we wanted togenerate those errors. We did that and we created data. But when we're modeling,we're going backwards. So we have the data, andwe want to uncover this truth, that is the family of model, the formof the model, and the fit of the model. Well, we don't have a way right now todetermine what the appropriate family or form of the model is. But we do have a way to obtain,not the actual parameters, but a good guess for them. And that's by, after assuming a family anda form, we then use estimation. In this case we use squares to findour best guesses for those parameters. So again, we have to assume the form hereand then we estimate the parameters."
stat-420,4,16,"Now that we know how to set models,we should probably go back and talk about why we should set models. I'll give you the two reasons. Either you want to explain therelationship between the predictors and the response oryou want to make your predictions. Give me values of the predictors. I'll give you a good guess forthe response. And based on that, there's sort of,maybe different considerations for how we build our model, andthat's what we're going to talk about. So again, explaining is, we want to know how do the predictorsrelate to the response? And sort of, to be able to explainsomething, there's two things we want. We want a small model andwe want an interpretable model. We're kind of are using linear models. We sort of automatically getpretty interpretable models, especially with the restrictionswe currently have on it. We should probably talk aboutwhat a small model is too. So again, how the predictors are relatedto the response is going to be in a linear fashion. And a coefficient will tell us howstrong that linear relationship is. But sort of, right now,the more interesting question for us is, which of the predictorsare related to the response? Maybe some predictors don't have anyrelationship at all to the response, and that's sort of the moreinteresting question. So the question is, how do we pick whichpredictors are related to the response? And the question is answered by a toolwe already have, which is inference. So, for example here, let's considertwo forms of a linear model. We have two predictors, x1 and x2. And we want to compare a model withjust x2 or a model with x1 and x2. So, we're already assuming a familyof models which is linear. We're assuming two differentforms of the models. And now we need to add sort of onemore assumption here, which is that the noise is following a normaldistribution with some constant variance. This is an assumption ofwhen miles we make often. The reason we make thosebecause we need it for inference because when we do a T test orF test, the resulting distribution that allows us to calculate a p value isdependent on this assumption here. We'll take about this inmuch greater detail later. But for now, we just know that if weassume that, we can do this test. For example, we can use anovato compare these two things because their orange model hereis nested inside of blue model. So, essentially what we're decidingis is there a statistically significant differencebetween these two things? If there's not, well,we're going to prefer the orange smaller model because it's a lot easier toexplain the relationship between x2 and y than it is to explain the relationshipbetween x1 and x1 and y. So, any time two modelsessentially fit almost the same, we're going to havea strong preference for the smaller one because that relationshipis much easier to understand. In the future, we'll talk aboutmethods that, sort of, do this for us. It'll all, they'll try a large numberof forms of the model and, sort of, give us a one with a only numberof predictors that we need. But for now, we'll sort of have to justuse the inferential tools we have to pick and choose which models to compare,sort of an ad hoc manner. So, our other goal could be tofind a model that predicts well. That is used to predictors to predict theresponse, which is sort of circular and silly, butit's something we often want to do. We want to make good predictions. How do you make good predictions? Don't make any errors. And that's generally impossible, so what we really want to do is try tokeep the errors as small as possible. So, sort of general,we have to pick a metric and minimize it. In this case, we'll talk about,maybe, root-mean-squared error. We just want to make that as smallas we possibly can, or do we? And that's sort of whatwe're going to talk about. So, on the left here,we have some data that I simulated and I fit two models of the data. The first model, the blue one, is I fit the usually simple linearregression model to this data. The other model, the dashed orange modelis a model we don't know how to fit yet but, we'll just say it's a muchmore complicated model than simple linear regression. Here, it's pretty obvious thatthis orange model is fitting the data much better in sortof a don't make errors sense. The orange model is making farfewer errors than the blue model. But now, sort of because I simulated thisdata, I can go simulate a bunch more data, and that's what I've done on the right. So, now I've plotted these fitted models,so the fits we can see on the rightare the same fits from the left panel. But now, it's a new data, and the data was generated inthe same way that it was before. So now, which model is making more error? Well, it should be pretty clear that theorange model now is sort of random, and makes no sense, based on the data. So, again,we care about vertical distances here. So this, for example, this point here is closer to the blueline than it is to the orange line. We don't care about distance like this. We care about distance like this. So, on average here, the orange ismaking much more errors than the blue. So, if we want a model that predicts well,we really don't want it to predict on the data we have,we want it to predict on new data. So, this is the idea of overfitting. Over here on the left panel,the orange model is overfitting the data. It's fitting too hard. It's essentially fitting to the noise. It's not capturing the signal properly. It's capturing the signal anda bunch of the noise, and it's encoded that in what you thinkis the signal, but it's really noise. Whereas the blue model is sort of nicely,it's capturing the fact that there, yes, there's this increase trend, but all this other variation you see istruly it's random, and that's noise. So, what this ordered mile doing ishere we would call it, it's overfitting. It's fitting the data too hard. So, sometimes you're going tohave to sort of allow for more error to not make errorsin general on new data. We don't want to fit too noise. So, when we talk about root-mean-squarederror then, we can sort of talk about root-mean-squared error onseen data, as well as unseen data. So what we'll do here is root-mean-squarederror, it can be applied to what we'll call the training data, which is the datawe've seen and fit the model to. But it could also then be used on unseendata, which we'll call the test data. So in this picture here,this is sort of my seen data, this is my test datathat I fit the model to. Over here, this is my testing data orthe unseen data, which I only use toevaluate how well it fit. The model is only ever fit to this data. So, essentially when you fit a model todata and then try to predict on it, that's cheating because of course, it's going totry to fit that data as best as possible. So we always have this set of data, orif we can, we try to have this set of data that we set aside that wecan never fit the model to and we call that the test data. And then when we evaluateroot-mean-squared error with the predictions of the testdata from our original model, that's what's called our testroot-mean-squared error. That is the metric wewould like to use to say, this is the best model for predictionof the models we have considered. So again, in general, we can usemodels to both explain and predict. In particular, we can use linearmodels to both explain and predict. Linear models can be used forboth of these tasks. And often a linear model thatexplains well, will predict well. And also a linear model that predictswell, will often explain well. And we'll keep these twoconsiderations in mind going forward."
stat-420,7,1,"Welcome to the secondhalf of step 420, statistical modelingin R. I'm David Unger, a senior instructor inthe Department of Statistics at the University of Illinoisat Urbana-Champaign. The second half is obviously a continuation of what we've learned aboutregression modeling in R, and the R programming language. Will continue to focuson both the theory of linear regressionand the practice of applying linearmodels to analyze data. But in the second half, we will introducesituations that require increasingly complex models and the tools necessary to make sure things don'tget out of hand. Moving beyond the basics offitting a regression model, will discuss the use of categorical predictorsand interaction terms. These new terms willgive our models new interpretations andincreased flexibility. The usual linear regression model makes a number ofassumptions that we rely on to perform hypothesistests about our models. We'll discuss both thevisual and numeric tools to diagnose potential violationsof these assumptions. Our models will also introduce methods to find outliers and discuss how these could be a possible source ofassumption violations. Despite the namelinear regression, we'll see that a linearregression can actually be used to model nonlinearrelationships and data. By introducing variabletransformations for both the responseand predictors, we can fit extremelyflexible models. Introducing these new conceptsof categorical predictors, interaction terms andtransformations will have a lot of options when itcomes to building a model. With all these choices, how do we build a good model? There's no one correct answer. So we'll discuss the artof model-building, learn methods for selectingpredictors from those available and discovertools for comparing models. Will pay special attentionto how the goal of our model can affectdecisions we make, and the model-building process. Will finish the course by generalizing the conceptof a linear regression to a broader class of models called the generalizedlinear models. These models will allow for responses and errors that follow a distribution other thanthe usual normal distribution. To illustrate the idea ofgeneralized linear models, will focus onthe most popular example, logistic regression. Logisticregression is used to model binary responses in the contextof machine learning. As such, it is a powerful toolfor classification. It can be used foreverything from spam detection, totumor classification. We'll see how the vast majority of the concepts welearned regarding linear regression immediately and easily transferred tologistic regression, including much of howto work with both of them in R. As a statistician, linear regression in R are two of my most commonly used tools. After completingthe second half of this course, you will have greatly improved your ability to utilize both."
stat-420,7,2,"In this lesson, we areintroducing the use of categorical predictorsin regression models. Since we use the method of least squares to fita regression model, we require all ofour data to be numeric. So in order to use categoricalpredictors in our models, we need to introducedummy variables. Their name issomewhat unfortunate, as they actually serve asa very clever way to provide a numeric representationto categorical data. By using dummy variables, some of the parametersof our model will have new interpretations thatwe need to be aware of, especially when we introduce interaction termsin our next lesson. For variables that havea large number of categories, we need to create a large numberof dummy variables. Thankfully, R willbasically take care of this through the useof factor variables. When using a factor variableinside a model in R, the appropriatedummy variables will automatically be created for us."
stat-420,7,3,"So here we have the familiar statement of the multiple linear regression model. Here we're indexing things by i and that's simply to say that we're going to fit this model to a dataset where there are observations i = 1, 2, all the way through let's say the sample size, n. The way we've written the model here, there are p minus 1 predictors, so we have x1, x2, all the way through x of p minus 1. And that gives us a total of p beta parameters. So the way we've written the model here, essentially all of the information about the assumptions of the model is more or less encoded in this statement about the error terms. But sometimes we like to explicitly discuss some of the assumptions of the linear model and we have a very quick device to remember those which is the word LINE. So the word LINE here is rather fitting because we're doing linear regression, linear being the first letter here for L we have linear. We say this is a linear model because we're modeling Y as a linear combination of the predictors, which is what we see here. We consider this the signal portion of the model. Now the last three letters of LINE have to do with the error term here or the noise that we're allowing for in this model. So this statement about the error terms we're saying that the epsilon ""I""s are what we call IID for independent and identically distributed. We say they follow a normal distribution with a mean of 0 and a constant variance of Sigma squared. So the I here stands for independent, which is essentially stated here from the independent and identically distributed. The N is for normal because that's the distributional assumption we're making and the E for equal. Variance is always the same. Next week we'll discuss these assumptions in very great detail and also discuss how to check that when we're applying this model to data that these assumptions are still valid. For now we want to talk about a couple implicit assumptions of this model. One is that anytime we're doing what we're calling a regression so far, is we expect the Ys to be numeric. And that's also an assumption that we're placing on each of the predictor variables where, maybe not the predictor variables themselves, but this matrix here, which we'll call the X matrix, we're assuming that the entries here are numeric. This should seem sort of silly and maybe a little obvious but in order to do this calculation to effectively fit the model and estimate all of the betas, we have to have numbers in here. But in practice we'll often have predictors that are categorical. Say maybe a variable for time of day either AM or PM, maybe just a simple yes/no variable, maybe a variable that puts the members of a university into different categories, say student, faculty, and staff. So I can't say have this matrix contain the entries say yes, no, no, no, no, yes, no, yes, no, yes. That doesn't work. You can't do matrix operations that we need to do here on the values yes and no. So instead we have to come up with a way to represent categorical variables in a regression model. To illustrate these ideas we'll use the mtcars dataset from R. For now we'll just talk about it in general, eventually we'll see actually using it inside of R. So this dataset is information released from the US Magazine Motor Trend in the year 1974 with characteristics of various cars. We'll want to keep in mind this is from 1974, because that'll come up here it a bit. But so what we want to do is we want to model the fuel efficiency of a car. Here we have a variable for miles per gallon. We're going to pick two variables that we're going to be interested in. Which is we'll use as predictors the horsepower and it's variable here called AM. So this AM variable is a variable about transmission type. AM here stands for auto slash manual for the two possible transmission types either an automatic transmission or a manual transmission. We might note that that's not what we see here in the actual data. We see zeros and ones and that's what we're going to talk about. Our response and our predictor horsepower are our regular old numeric variables, whereas this AM variable is a very specific type of, right now we see that it's numeric but it's sort of going to do something for us in terms of the two categories auto and manual. I'd also note that in practice if we wanted to model the response here we would use as many of these variables as possible. But for illustration purposes we're going to limit it so we can draw some pictures, which we'll do now. First will simply plot the response against the numeric predictor horsepower and we generally see the trend that we would expect, as the horsepower increases the fuel efficiency decreases. This plot though is about as boring as it could be and it's not telling us anything about the transmission information that we have. So what we'll do is since there's only two categories, automatic and manual, we'll color these points according to their transmission type. And that's what we see here the orange triangles tell us which observations are manual transmissions, the black circles tell us which observations are automatic transmission. One thing that we sort of see here is that for any particular horsepower sort of on average the manual transmissions have a better fuel efficiency than the automatic transmissions. And what we'll want to do is we'll want to fit a model that captures this trend that we see here where there's sort of the on average better fuel efficiency for manuals. The first thing we'll actually try is a simple linear regression. Simple is good. We like simple. If we can get away with a very simple model we should use a very simple model. So again here our response is miles per gallon and the single predictor that we'll try to use here is the numeric variable horsepower. So in this case, β₀ then is the average fuel efficiency, when horsepower to 0, sort of nonsensical but that is its interpretation, and β₁ is the average change in fuel efficiency when we increase the horsepower by 1. If we go ahead and fit this model to the data we have using this mtcars dataset here, there's more data than we see here, but this is just for example rows. So we fit it to this data, we see this fitted line here this fitted line is certainly summarizing this data but we see sort of an obvious trend still. The manual transmissions are sort of almost always being underestimated where the automatic transmissions are very often being overestimated. It's actually sort of by coincidence that this line almost splits the data. Again, so we see a lot of the manual transmissions being, you know, their true values being above the line and then the opposite for the automatic, pretty systematically they're under the line. We want a model that captures this idea that sort of says, okay well, on average the manual transmissions are doing better. To do that we'll introduce the transmission data into the model. The response is still miles per gallon. The first predictor is still horsepower. And this second predictor here x₂, this whole term here is going to bring in the transmission information so x₂ will, in this case, explicitly be that AM variable that we saw. So we need to talk about what this AM variable actually does. This AM variable is what we would call a dummy variable. The name is sort of unfortunate, dummy makes it sound like sort of a silly thing to do but in reality it's sort of a clever trick that will allow us to encode categorical information as numeric values. Here we see the two possible categories. We have manual transmissions and automatic transmissions. These have no numerical values actually associated with them, they're just categories of cars. Some cars are automatic, some are manual. But what we're going to do then is provide numeric representations of them. In other words we're going to code for them with numeric values and those values are sort of very specifically chosen to be 0 and 1. A 0 will represent an automatic transmission and a 1 will represent a manual transmission. In this particular dataset this is already done for us so the AM variable as presented was a dummy variable already but we should be careful and note that we should really only supply values of 0 and 1 to this variable. An x₂ of 3.14 wouldn't really make sense here. What sort of transmission does that represent? That's a problem that we'll sort of deal with later in R with a very specific type of variable in R. But for now we just need to know that a dummy variable takes values 0 and 1 and splits a category up into two possible values. So let's take a look at what this dummy variable actually does to this model that we have here. When x₂ is 0 this term simply goes away and we have the model Y= β₀ + β₁ x₁ plus the errors. And this is for automatics. When x₂ is 1 this term enters the model again, but we're putting 1 for x₂ so we don't really need that we're just left over with sort of β₂. So we have the response Y is β₀ + β₂ + β₁ x₁ plus the errors. And this is for manuals. So effectively what we've done by including this dummy variable in the model is we've created a model for the automatics and a model for the manuals. They have the same slope in this case so they both have beta1 as a slope but they have different intercepts. This β₂ parameter essentially tells us what the difference is between automatics and manuals for any particular value of x₁, which was the horsepower. So if we were to go ahead and fit this model in R, which we'll do eventually, we'd obtain estimates of the three model parameters. So we would obtain estimates, so first we'd have B̂₀ which is roughly 26.58. This is the estimate of the average fuel efficiency when x₁ is 0 and x₂ is 0. In other words, so x₁ again is horsepower, x₂ is the AM variable. This here is the average fuel efficiency when horsepower is 0 and it's an automatic. B̂₁ will work out to be roughly negative 0.059. So this tells us how the average fuel efficiency changes as the horsepower increases. And again it's an estimate because we found it using data. And again this is independent of which transmission type we're using. B̂₂ works out to be roughly 5.277. So this is an estimate of the difference in average fuel efficiency between manuals and automatics. So we're saying on average a manual has 5.277 miles per gallon more than an automatic. One more quantity we might be interested in is B̂₀ + B̂₂ which is 31.86 and so that would be the average fuel efficiency for a manual transmission when the horsepower is 0. B̂₀ is for an automatic transmission when the horsepower is 0. β₀ + β₂ is an estimate, well hat, is an estimate when horsepower is 0 and the transmission is manual. So numbers are sort of boring. We always prefer pictures. So what does it look like visually? We have this here. We said instead of one regression line we essentially have created two regression lines and they have the same slope but different intercepts. So that's what we see here. So both these lines have the same slope, which is the B̂₁ that we had talked about, but they have different intercepts so we can't actually see the intercepts here because there is no horsepower 0 being displayed here. But in particular this B̂₂ can be seen on this plot which is exactly the distance from here to here. So this is B̂₂. So essentially we're saying that, on average manual transmissions are more fuel efficient than automatic transmissions. And we estimate that difference to be on average 5.277. And that relationship is the same for any value of horsepower because again we're dealing with parallel lines. And we can sort of immediately see that this picture looks much better than the simple linear regression picture we've seen before. So now within either of the two regressions, it's one model, but we sort of, we refer to the two regressions as the two lines. So the red dashed line here, the red triangles sort of vary about it without any sort of obvious pattern, and the same thing for the solid black line. The black circles vary about with sort of no obvious discernible pattern or least not anything as terrible as before. So this is capturing the situation much better than that very simple model. So the transmission information was sort of obviously important here. So just visually it looks like this transmission information is sort of obviously significant but we could very easily come up with a test to assess the significance. That test, in this situation boils down to a test about a single parameter. So x₂ being the information for transmission, so if we hypothesize that β₂ is 0 that would result in a model that looks like this, so it would be Y = β₀ + β₁ x₁ plus the errors. And the alternative hypothesis, that would be the model here, which allows for β₂ to be non 0. This boils down to a test of a single parameter and a multiple linear regression model which we could do with a T-test or we could consider this model nested inside of this model and we could use anova F test. And we would carry out these like any other tests that we've done before and we'll see how to do this in R and we'll find out here that it does turn out to be significant which we should have, would have expected, based on this picture here. So sort of summarizing, we saw how a dummy variable allows us to create two parallel regressions with the same slope but different intercepts. There's sort of a number of obvious extensions here and that's what we'll talk about in the remainder of the week. The first would be well what if there's more than two categories? This is an old dataset where there were only manual and automatic transmissions but now we have what are called CVTs, or continually variable transmissions, which we could consider a third category. So how would we deal with that? Another question we would ask ourselves is well why are we restricting ourselves to the same slope? Why can't we have a model that allows for different slopes for manual and automatic? So these are the questions we're going to answer the rest of the week. This lesson will go on to talk about what to do with categorical variables that have more than two possible categories. And then in the next lesson we'll talk about interactions which will allow us to have different slopes for different categories."
stat-420,7,4,"[MUSIC] So now we'd like to look at introducingcategorical predictors into your regression models that havemore than two categories. To do so, we'll look at a dataset fromthe UCI machine learning repository. We'll actually go through creatingthis dataset in R, in the next video. But for now, this is another datasetabout cars and their characteristics. We're going to go ahead and look at again, modeling the fuelefficiency, miles per gallon. And now we're going to look atthe number of cylinders the car has and the displacement of the car'sengine as predicted variables. One thing we see right away is thatthe cylinder variable is something called a factor variable with 3 levels,4, 6, and 8. For now, we're going to ignore the factthat it's what's called a factor variable. We'll deal with that when we jump into R,but we will notice that the possible valuesof this variable are 4, 6, and 8. And that's something we want to talk. The question becomes, with that variable should weconsider it categorical or numeric? Let's consider the model y = beta 0, some intercept + beta 1 x1+ beta 2 x2 + some error. And y will be the responsemiles per gallon, x1 will be the displacement andx2 will be the number of cylinders. The number of cylinders could be 4,6, or 8, those happen to be numbers. Why not utilize themas a numeric variable? The issue there is that it's applyinga very particular structure, and that structure says thatthe difference between 4 and 6 cylinders. Well when we increasefrom 4 to 6 cylinders, we change the mean responseby 2 times beta 2. And then if we go from 6 to 8 cylinders, we also move the mean oflie up 2 times beta 2. So in particular,the difference between 4 and 6 cylinders has to be the differencebetween 6 and 8 cylinders. When dealing with most numeric variables,we like this. because for continuous variableswhen we're modeling them, it's likely we will have a differentvalue for each observation. So instead of having themeach possible value will have a different possibleeffect on the response y. It's nice to impose a structure andhave this parametric model and moving plus 1, always has the same effect. Because here we're dealing witha very small number of categories. We'd like 4, 6, and 8 cylinders to essentially all haveits own possible effect on the response y. So that's what we'd like to do, soessentially at a particular displacement, we'd like to have a different mean for4, 6, and 8 cylinders. But not just different, we'd also like forthe difference between 4 and 6 to be different thatthe difference between 6 and 8. We'd like forthe differences to be different. With that in mind,we're going to go ahead and consider this cylindervariable categorical. When we do that,we're going to need some dummy variables. Since it has 3 possible values, there are 3 possible dummyvariables that we can create. For any particular observation,only one of these will be 1. Like for example, we'll never havea car that's simultaneously 4 and 8 cylinders, butwe do have three dummy variables. But again, only one of them can be 1 forany given observation. So you might be tempted to think,okay, go ahead and just throw these dummyvariables into a model. So let's see what happenswhen we try to do that. I'm going to write a rather large model. So we'll have y is beta 0 + beta 1,x1 for the displacement. And then I'll use a differentGreek letter, I'll do let's say, gamma 1 v1 + gamma 2 v2 +gamma 3 v3 + some error. I'm using a different Greek letter, soI can synch up gamma 1 and v1 just for simplicity sake. So again, x1 is the displacement andthe v1, 2 and 3s are the dummy variables forthe number of cylinders. But let's see what happenswhen we do this, so let's start to write out our x matrix. Let's say we have something simplelike just six observations, so I'll hae first a column of 1s. And then I'll the values of x1,so call them x1, x2, x3, x4, x5 and x6, you know what? Let's actually not index this. So these are six observations of x andlet's say v1 is for 4 cylinders. Let's say the first 3cars are 4 cylinders. So the others ones are not,so v2 needs to be, for 6cylinders let's say cars 4 and5 are 6 cylinder. And then lastly for v3,let's say that last car is an 8 cylinder. With this model, this would be our xmatrix, but we run into a problem here. The issue here is that the columnsare not linearly independent, which is what I've written here. This column is a simplelinear combination, actually in this addition ofthese three columns here. And because those columns are not allliterally independent we can't actually do this here. This results in x transposex not being vertical. We need to come up with a way toutilize these dummy variables. And what we'd like isa model that gives us, essentially similar to what we saw before. We want three different lines forthe three different number of cylinders. So how do we do that? It turns out if we have an interceptin the model, to get three lines, we only need two dummy variables. In general, if a categoricalvariable has K categories, you only need K-1 dummy variables. And again,that's assuming there's an intercept. So the intercept will sort of serveas a reference level, whereas the two additional dummy variables in this casewill be relative to that reference level. Let's take a look at this model here. Let's first say, okay,if we're dealing with a 4 cylinder vehicle that means that v2 is 0 andv3 is 0, so we would have this model here. This is a 4 cylinder vehicle here. Similarly, if we're dealing witha 6 cylinder vehicle, v3 is 0. And v1 doesn't do anything, actuallywe're just not even using v1 here. This would be our model fora 6 cylinder vehicle which we have here. Lastly, if we're dealing with an 8cylinder vehicle, v2 would be 0, so this goes away, this stays. And this is our model then foran 8 cylinder vehicle. Essentially then beta 2 is the differencebetween a 4 and a 6 cylinder engine. And beta 3 is the difference betweena 4 and an 8 cylinder engine. So essentially, we've sort of stateda baseline or reference level, which is 4. And then the two additionalparameters beta 2 and beta 3 tell us how to get to othercylinders based on that 4 cylinder model. So this is once again a model withthe same slope for each cylinder type, but different intercepts. So let's take a look at that visually,and here's what we have. Here, the solid orange line is for4 cylinder, the dotted blue line is for 8 cylinder andthe dashed gray line is for 6 cylinder. So circles are for 4 cylinder,triangles are for 6 and crosses are for 4, 8 cylinders. 4 cylinder was sort ofour reference level. If we fit this model in R which we'll do. We can get an estimate of beta 2,beta 2 hat turns out to be -3.63. So that's going from here to say here,because again this is where 6 cylinder is. And similarly, beta 3 hat is -2.04 and that's going from here to just here,so going from 4 to 8 cylinders. These estimates here,tell us an estimate of the difference between say in this case, 4 and 6cylinders for any particular displacement. These are the same forany particular displacement because we are dealing with parallel lines,they all have the same slope. So you might look at this and say,these lines aren't all that different maybe we can actually just summarizethis situation here with one line. And that cylinders don't makeall that big of a difference. So to do this,we will need to perform a hypothesis test. Starting with this model here, again x being displacement andv2, v3 being dummy variables. To perform a test comparing threelines and one line the null hypothesis would essentially hypothesize thatwe don't need these terms here. So we will hypothesizethat beta 2 = beta 3 = 0. And the resulting model therethen would be this here, which contains essentially noinformation about the transmission type. This first model here has three lines,and this model here has one single line. So to do this test,we can no longer use a t test, because we're consideringa difference of two parameters. So here we would have to consider,well this model is nested inside of this model here, andwe would have to use the ANOVA F-test. When we actually do this in R, we'llactually find that we do need three lines. This is significant, and later, we'llactually look to making this model larger and more flexible to allow fordifferent slopes for each line."
stat-420,7,5,"[MUSIC] Now let's see how we can utilizecategorical predictors when building models in R. To illustrate, we'll work through this,our mark down document that we see here. And we'll begin by simply ignoringthis chunk, we'll come back and talk about it when weactually knit this document. Instead, we'll discuss the data that we'llsee first which is this mtcars data set. This data from motor trend road tests forcars of model year 1973 to 1974. We're going to want to modelthis fuel push to variable, this miles per gallon variable here. We're going to consider two predictors forillustrator purposes, which is the horse power variable andthis am variable. AM here stands for automatic manual. See observations that have1r manual transmissions and observations that have 0rautomatic transmissions. The first thing we should do when we getsome data is attempt to visualize it. So we'll start with a very boring plotwhich is just plotting fuel efficiency against horsepower. And we sort of see the trend thatwe're expect as horsepower goes up, fuel efficiency goes down. We said that we want touse both horsepower, and this transmission variableto model fuel efficiency. So we like to add the information ofalpha transmission to this plot so there's only two transmission types. One way, we can do this is by coloringthe points according to their transmission type. And that's what thisplot seeks to do here and to do this we utilizea sort of common R trick. Which is starting with the plot, we hadpreviously we add a color argument and to that argument, we pass the variablethat we'd like to use for coloring. It turns out though that becausethis variable is initially zeros and ones, if I were to not add this 1 here,we'll see what happens. Turns out that it colors half of the pointwhite and half of the point black. Because if we go back, look at the data,0 corresponds to the color white and 1 corresponds to color black. By adding 1, we shift this variable to be1 and 2 instead, and 2 corresponds to red. Now when we plot this, it works. We're doing something similarwith the plot character itself. We end up with circles and triangles basedon the numbers that we're passing through here, butit's separated by transmission type. We'd like to start modeling thisdata now and we'll start simple. We saw already that it'snot going to work well, but we'll do it anyway becausewe like simple if it works. Here, we have a simple linear regressionmodel where y is the fuel efficiency variable miles per gallon andx1 is the horsepower variable. And we fit that like weusually would in R, and we see our estimated coefficients here. We can very easily add thisline to this plot here. We have this model stored in thisvariable here, so we can simply use abline because it understands whatto do with objects of class lm. And it will add the fitted line by extracting the correctcoefficients from that model. So here, we see that we're underestimatingmanual transmissions very often and overestimating automatic transmissions. And this is sort of what we already saw, we want to now utilize the transmissioninformation inside the model. To do that we're going tofit this model here where x2 is a particular type of variablewhich is a dummy variable. While to us, x2 is sort of representativeof a categorical variable. It's actually a numeric variable still, because x2 here is this am variable,which is 0 and 1. So 0 and 1 is numeric, butthey represent categories. So 1 here being manual transmissions,and 0 being automatic transmissions. This is one way to do this, sowe can directly code a dummy variable. Which is what this variableactually already was in this model, which we can see here. To add this variable and model, we will fit a multiple linearregression model as we had before. We simply added to the model andwe'll store it in a new variable here. Now we see the three estimatedcoefficients instead of the original two, and I think we'll noticethey're all different. This model with the added dummy variable, it sort of splitsthe model into two lines. One line for automatic transmissions andone line for manual transmissions. They both share the same slope beta 1,which is estimated here. They have two different intercepts. So the intercept for the automatictransmission is estimated here. Whereas the intercept for the manualtransmissions differs by this much here. So when we add these two estimatestogether, that is our overall estimate for beta 0 + beta 2, both hats. We can take this information here and use this to add the fittedmodel to that plot we saw. Here, I'm doing a little setup whereI store the estimated intercepts for automatic and manual, as well asa slope for automatic and manual. Which happen to be the same, they're bothestimated by this second coefficient here which is estimated in beta 1,should say beta 1 in this model here. And then I can add those to the plot, sohere's the plot that we created last time. But now I simply add two lines toit with those intercepts and slopes that we had sort of manually stored,and we see that this fits much better. So now there's sort of no obviouspattern in the manual transmissions and no obvious pattern inthe automatic transmissions. We see things are varying aboutthose two different fitted lines. Just looking at this, we can seethat this is probably significant. But if we wanted to test whether ornot we need one or two lines, that boils down to a testabout a single model parameter. So this beta 2, if we go back to ourmodel, this is what made the model have this two line representationinstead of the single line. So if beta 2 was zero,we would have one single line irrespective of transmission type, but if beta 2 isnon zero, we have two parallel lines. This test is something that is nodifferent than any other test in multiple linear regression. It just has the specific interpretationthat we're essentially testing the null which is one line versusthe alternative which is two lines. So we can extractthe relevant information. Here, we have the estimate forbeta 2, this is beta 2 hat and the relevant test statistic, andthe P value for exactly this test. We could've also done thiswith an ANOVA F-test. The two models, we previously fedthe simple linear regression model and the additive model with both horse power. And the automatic manual variablediffered by exactly this variable. So if we do this we see weget the same P-values, so these tests are doing the same thing. This P-value is very small, so at any alpha say 0.05,we would reject the null hypothesis. We say that there is this need forthe two line model. The other thing we have here issort of hard to read right now. This is sort of heavy use of R markdown,so here we have some tac inside of that,we have some R code. We're making a list here andwe're using general markup. So what I'm going to do is I'm going togo ahead and knit this document, and here we have the result. A few things along the way, sofirst of all, where were we, here. So one thing you might noticeis this is a lot of precision, and especially here sortof in this narartive here. We don't really needall these digits here. It's sort of just overwhelmingto even look at and it sort of makes it hard to read. So this is that chunk that Ihad initially commented out. And I'm going to uncomment this andre-init this document for this time find the relevantsection faster. Now we see this is muchmore human readable. Better yet here, it's very humanreadable compared to what it was before. So essentially what I've doneis I've modified some options, in particular the scipen option andthe digits option. Which control here when R kicksin to scientific notation and here this controls howmany digits R is printing. For details on how those work,you'll want to see the documentation for this options function here. And it'll give us information,say about digits, and further down this scipen option here. But then we can also see that, well thissort of wasn't easy to read on this side. It's very nice and easy to read over here andit does some interesting things for us. So for example, this fitted coefficient is beingpulled directly from the fitted model. It's not hard coded in here, so if I change this model thisnumber would change here. We're sort of stylizing thingsaccording to this is sort of code, whereas this is math. And these are the generaliinterpetation of things. So again beta 0 was the estimatedaverage fuel efficiency for an automatic transmission. And beta 0 plus beta 2 hat isestimated average fuel efficiency for a car with a manual transmission. So beta 2 is essentially the differencebetween those things, and then in this model we had a shared slope. So the two slopes here are the same, so it doesn't matter whichtransmission type you have. How increasing the horsepower changesthe fuel efficiency is the same. [MUSIC]"
stat-420,7,6,"Picking up where we left off, we'll now utilize factor variables to create models in R that utilize categorical predictors. The first thing we'll do is read in a new dataset, which we're actually pulling from the web in this case. So I'm doing a bit of data cleaning here. Notably, I am making sure that this horsepower variable is numeric because it was coerced to be character because there was some missing data which I removed. I also remove one particular car that causes some issues and one of the other big things I do is I take out cylinder values of three and five because I only want four, six and eight for a concept that we're going to illustrate here. Here I'm coercing cylinder to be a factor variable, which we will talk about in this video. We can verify the structure of this data or it might be nicer to view it as a tibble. And in this case we see that this fuel efficiency variable is the one we're going to try to model, so the miles per gallon. We'll eventually look at the cylinder and displacement variables. First, we'll look at the origin and domestic variables. The first thing we could do is - this domestic variable is actually numeric and it happens to be a dummy variable for essentially where the car was created. The frame of reference for this data set is United States, so domestic cars are cars that are cars that are built inside the United States, foreign cars are built outside of the United States. Here we have an additive model where Y is fuel efficiency, x_1 is the displacement and x_2 is a dummy variable that we see here. Again, we just saw how to do this, so I can quickly fit that and plot it and so we already knew how to do this. This domestic variable is not a factor variable, it is what we call a numeric variable that happens to be a dummy variable. What I'd like to do is instead of having a variable that already codes as zero and one, instead code it as domestic and foreign. That's something to do here. So I'm going to reuse that origin variable that we didn't talk about and make it essentially the same as a domestic variable, but instead of zeros and ones, it'll have the character string ""domestic"" or the character string ""foreign"" depending on whether the cars are domestic or foreign. That's kind of nice. It will be easier to know. We won't have to constantly be referencing ""Oh, is one foreign or is one domestic."" Also note that this line would have done the same thing all in one. This is just a little more explicit so we know exactly what's going on here. At this point, origin is still not what we are looking into is a factor variable. It's just a vector of characters at this point. So just domestic, foreign, domestic, foreign. So here I'm going to coerce it to be a factor variable. And now we see that it is indeed a factor variable. Factors have what are called ""levels"" so when I rerun this code that was apparently already run, we see that there's two levels here, domestic and foreign. If we actually look at the variable itself - let's do that here - we see that now it sort of no longer looks like there are characters, but they are in a very specific way. We see domestic, foreign and it also, when we return it, shows us the levels. Essentially what's happening here is a factor variable is a vector of integers with corresponding labels. If I quickly coerce this to be numeric, we see that these would be ones and twos. So essentially here, one corresponds to domestic and two corresponds to foreign. And we'll see later how this will actually potentially be helpful for doing some other things, but really, it's just a variable that has two possible levels. Now, this is something we will utilize as well. Because it has these two levels, these are the only two possible values this variable can take. So we couldn't say make one of these be other instead of foreign or domestic. These are the only two values this variable can take. That will also be important later. Again, here's the model we had fit previously using the domestic variable. I'm just going to fit it again. And here we see the estimates. So this is using domestic, which was a dummy variable; again, one for domestic cars, zero for foreign cars. And again, remember - help me out - there it is. One for domestic, zero for foreign, which will more or less match this variable here. Again, origin was a factor variable, so domestic domestic which matches the domestic domestic and so on here. This model - again, this is using the pre-created dummy variable domestic. Now I'm going to use this origin variable, which was a factor. And let's see what happens here. So scrolling back up. The first thing we see here is that the coefficient that was estimated in front of displacement, which was x_1 in both cases, is the same. Now the thing that's different here is both the estimated Beta_0, which is the intercept and the estimated Beta_2, which is this coefficient for the dummy variable domestic - in this case, we don't even know yet because we haven't talked about it. What this is fitting is this model here, which looks the same, but importantly, we have to note that here, R is automatically creating a dummy variable for us. And the dummy variable that R creates uses one to represent a foreign car. And that's what it's telling us here. So this essentially saying that ""Okay. This is a dummy variable based on the origin variable and whether or not it is foreign."" So this tells us which level corresponds to one. Essentially, what's happening here is R is choosing a different reference level than was essentially specified with this domestic dummy variable. And the way R does that by default is it looks alphabetically - because domestic becomes alphabetically before foreign, domestic becomes zero in the underlying dummy variable and foreign becomes one. And that's why these are exactly opposite signs because essentially, the reference level in these two is exactly flat. Again, we said that the dummy variable domestic is one when the car is domestic. So if I use that model that we fit - so the additive model where we were using the dummy variable domestic and I make a prediction for a car with a displacement 150 and is domestic, that's the same as using the model where we instead use the factor variable. So we'll pass through origin of domestic and we'll see if we get the same exact prediction. We can also verify that the fitted values for both of these two models are exactly the same. They're fitting the same model, we just have to understand the slight difference in the interpretation of this estimate and this estimate and here it's just a difference of reference level. One thing we'll note here is that because domestic is a dummy variable already, it's a numeric variable. So there's nothing to stop me from making a prediction for a car with a displacement of 150 and a domestic of 3.14. It will let me do this. It's completely nonsensical because a car can't have a foreign or domestic status of 3.14, but because R views domestic as just some numeric variable, it will let me do this. Whereas when we used origin, which was a factor, if I try to do a level that didn't already exist in the factor variable, R is going to complain. And it's going to say ""Well, I don't know what other is here. I can't make a prediction here. This doesn't make sense."" We like this. Categorical variables, when they're factor variables in R, you can only use the levels that exist, you can't make up levels on the fly like we're trying to do here. We had talked about why we would want to consider the cylinder variable a categorical variable and not a numeric variable. So here we have this cylinder variable, which is a factor variable. There's three relevant dummy variables; one for whether or not the car is four cylinder, one for whether or not the car is six cylinder and one for whether or not the car is eight cylinder. And these are the three possible cylinder values in this dataset. What we want to do is fit a model that will model the fuel efficiency as a function of the displacement and the number of cylinders, or we should say, the category of cylinders. We'll fit this and then we'll see what happens. We see that R estimates a coefficient in front of displacement and also uses dummy variables for whether or not the cylinder number was six and whether or not the cylinder number was eight. That's this model here that we had written before. So again, Y is fuel efficiency, x is displacement, v_2 is the dummy variable for whether or not there are six cylinders, which R is indicating to us here and v_3 is the dummy variable for whether or not the cylinder value is eight, which we see here. Again, R is automatically choosing a reference level for us because four comes before six alphabetically. So that means that four is the reference level here. This Beta_0, which is being estimated here, is the estimated fuel efficiency or average fuel efficiency when the cylinders are four and the displacement is zero. We can do something that we've more or less done before where now - so Beta_2 is how we change the intercept for six cylinders relative to Beta_0, which is four cylinders, Beta_3 is how we change the intercept relative to four cylinders again, which is Beta_0. So we can extract the relevant coefficients and again, in this model, they all have the same slope. I'm actually in a plot this and then we'll go back and talk about it. The interesting thing to talk about here is how we were obtaining this color scheme here. I went ahead and created a vector here of possible colors that I'm going to use on this plot. So now the thing to look at is in particular this here. So I'm going to create a new chunk. And so I can directly reference cylinder inside of this plot call because it knows I'm dealing with an auto mpg dataset, but I'm going to need to consider it like this outside of there. Again, remember this was a factor variable with possible levels four, six and eight. So I want to use that to subset the plot colors essentially. But before I run that, I want to show what happens when I do the following. Again, plot colors were these three colors. So if I do something like this - so If I say let's throw a vector in here, like one, one, two, two, three, two, one. So Darkorange was the first color, the first color; Darkgrey was the second color, second color; Dodgerblue was the third color; Darkgrey was second color and so on and so forth. We're essentially using indices to pull out the colors that we wanted. And we can do that with this factor variable because again, as we had seen before, this is essentially coded as integers underneath here. So it's as if we had coerced this to be numeric before passing it to this subsetting operation. Again, the thing to be aware of is while yes, this is a categorical variable with possible levels of four, six and eight, but when you coerce it to be numeric, it's not going to be eight, six, four, et cetera; it's going to be threes, twos and ones, et cetera. That's just something to be aware of when working with factor variables. Remember, when you coerce it to be numeric, it's going to have integer values starting at one. In this case, you sort of had an odd example where the categories happen to be numbers, so you shouldn't get confused and think that that's the numbers you're going to get when you coerce it to be numeric. Coming back up to the plot, that was what was happening here. Each observation essentially according to what cylinder it was; one for four, two for six and three for eight are the integer values there - it extracts the appropriate color. And then similarly here for the plot character - because again, I can't put something that's directly a factor here, so I have to first coerce it to be numeric. So these are the plot characters one, two and three, respectively. That gives us this sort of nice plot. And you might want to play around with this code yourself to see exactly what's happening here, but essentially, we put the points on the plot, we add the lines and then we add a legend."
stat-420,7,7,"[MUSIC]To create some flexibility in our regression models, we'll introduce to you some interaction terms. Without interactions, our regressionmodel is considered strictly additive. This means that the effect of anypredictor on the response is not affected by or contingent on any other predictor. Through the use of interactions,this restriction is removed and a predictor's effect on the responsecan change based on another predictor. To add interaction terms to a model in R, we will need to introduce some new Rsyntax for specifying model formulas. Since R makes it so easy to usecategorical predictors in interactions, it will be important to understandhow R's formula syntax specifies a particular model. [MUSIC]"
stat-420,7,8,"We'll return to the automobile day I believe we were working with. And again, we're going to try to model the fuel efficiency which is miles per gallon very well here. And then we use two predictors of the displacement variable and also this variable for domestic. This data set is from the perspective of the United States. So cars built inside the U.S. will be called domestic. And cars built outside of the U.S. will be called foreign. We've seen before how to fit an additive model. So in this case the x_2 variable is a dummy variable because foreign and domestic are categories and will use 0 and 1 to code for those categories. Again, x_1 is the displacement variable. And Y is the miles per gallon. This model we've seen before would give us essentially two lines with different intercepts and the same slope. So when we have x_2 is 0, we end up with a particular line which looks like this. And if instead we have x_2 is one. So if a car built in the United States, we have a slightly different line. We'll call it that looks like this. So here we see we have two different intercepts but we have the same slope. This term here essentially it was giving us a different intercept depending on the foreign or domestic status of the car. No matter where the car came from, the effect of changing the displacement is the same on Y. So if we increase displacement by 1, the expected value Y goes up by this β_1. In either case, if we actually fit this model to the data, the result looks like this. And something we might want to ask yourself is, do we actually need the same slope here? I guess a better way to say that would be, why not have two different slopes? If we look at say a foreign cars which are the black dots, maybe for example this line would be more appropriate. The next we want to do is sort of relax this condition that we have to have the same slope. To do that we'll need what's called an ""interaction model"". And what we'll do is we'll introduce a new term which we can see here which is an interaction term. So an interaction term takes the original predictors x_1 and x_2 and multiplies them together. Let's do what we've done before and break this model down into the model for foreign cars and the model for domestic cars and see what happens. So when x_2 is 0, the interaction term and the term for x_2 both go away. And we're left with Y is β_0 + β_1, x_1 + ε. But if instead we have x_2 is 1. Everything stays in the model except we plug in 1 for x_2 so that x_2 themselves sort of go away. They're replaced by 1. So we have Y is then now β_0 plus this β_2 plus. Now we have two terms that still have x_1 in them. So the new slope is (β_1 + β_3) x_1. In other words, we could say that this term here control the difference in the intercepts, and this term here controls the difference in the slopes. By including this interaction term in the model we not only have two different intercepts, we have two different slopes. And if we plot that, it's look something like this. And sort of that line that I suggested before for the foreign cars it's much more, I mean, not much more but it is steeper than a line for domestic cars. Looking at this picture compared to this previous picture here, it might seem rather obvious that the interaction model is significant compared to additive model. But we can perform a test to do this and that test becomes a test about a single parameter within the interaction model. So here we have the interaction model. And if we hypothesize that B_3 is zero, that is the coefficient for the interaction term, we end up with the additive model. Essentially, this test about a single model parameter is comparing the interaction or in this case two slopes model, to the additive, in this case one slope model. And because the additive model is nested inside of the interaction model, it differs by just this term here. We could use an anova F-test to do this or, again, because it only depends on this term here. So the difference of one model parameter we could also use the usual T-test. So far, we've been considering the interaction between a numeric variable x_1, in this case was we said displacement, and a binary categorical variable represented by a dummy variable x_2. But if we wanted to consider the interaction between a categorical variable with more than two categories? So what if we wanted to discuss the interaction between displacement and number of cylinders and we had previously argued that we should use a categorical representation of the number of cylinders? The cylinder will be a categorical variable with three possible categories. Previously, we had seen how we can create three dummy variables based on the cylinder variable, but we only need to use two of them in the model. That gave us essentially a model for four cylinders, six cylinders, and eight cylinders. And this is still an additive model because the effect of displacement, again displacement is x_1 here. The effect of displacement is the same in each of these models. If you increase displacement by one, the average MPG goes up by β_1 in each of these three sort of submodels. But we would like to do is, again, remove this restriction or constraint that they always have the same slope, and create a model where each of these three will have a different slope. To do that we need to do something like this. Going back to the additive model for a second, we see that we have three intercepts and one slope, and to fit that model it took one, two, three, four Beta parameters. So sort of by a similar argument. If we want three intercepts and three slopes, we end up needing one, two, three, four, five, six, six parameters. And what we're going to call an interaction model. So now we're going to essentially create an interaction between the displacement which was x and the dummy variables for a cylinder. First, let's consider a four cylinder car. So v_2 and v_3 are both 0, so we don't need this term. We don't need this term. We don't need this term. And we don't need this term. Okay. That's sort of easy. So a four cylinder will looks like this-- β_0 + β_1x. And that's it. Let's instead consider a six cylinder car so v_1 is 0 but that sort of doesn't matter so I'm not even using it. v_2 is 1, and v_3 is 0. So for a six cylinder car we do not need this term, and we do not need this term. And then the v_2 is a v_1 so we can sort of get rid of those because we're just replacing them with 1. So now the intercept here should be β_0 plus this β_2 here. And the intercept is now the original β_1 plus this y_2. And we can make a similar argument when it's an eight cylinder vehicle. So the dummy for eight cylinder is 1, the other two are 0. And by sort of a similar argument, we don't need this term. We don't need this term. And the v_3 become 1, so there's sort of gone. So for an eight cylinder car, it should be β_0 + β_3 for our intercept. And then the slope is now β_1 + y_3. So now, in this model, we ended up with one, two, three different intercepts. But also now we have one, two, three different slopes. So now with this model, if we increase displacement by one, the change in average fuel efficiency is β_1 + y_2 for a six cylinder vehicle. And this is different. If we increase displacement by one for a four cylinder vehicle, the change in average fuel efficiency is only β_1. So y_2 essentially that difference in change in average fuel efficiency. Also the way we wrote this model, the four cylinder vehicle essentially became a reference level. And then we had β_2 and β_3 which altered the intercepts for six cylinders and eight cylinders relative to four, and y_2 and y_3 change the slopes relative to four cylinder. We'll see when we jump into r why we wrote the model this way. Because when we deal with just the interaction between displacement and the cylinder variable in r, this will be what we call r's default parameterization of the problem. We'll also look at different parametrization of this model and see how in the end they're doing the same thing. But for now if we fit this model to data, we end up with these three sort of fitted lines which represent the entire fitted model. One thing we should note while we're looking at this image is, let's say this point here is the predicted fuel efficiency of a four cylinder vehicle with a displacement of about let's say 250. But this is sort of very clearly an extrapolation. While we have seen four cylinder vehicles in this dataset obviously. There's many of them right here. And we have seen engines with a displacement of 250, there's plenty of them right around here. The combination of a displacement of 250 and a four cylinder vehicle is not ever seen in this data set. As a matter of fact there's nothing even close. Looks like the largest four cylinder engine that we see as maybe at best 150. So now that we needed multiple parameters to express the interaction, testing for interaction in this model where we we're considering the interaction of displacement and number of cylinders, we need to test simultaneously two parameters. So the interaction was inserted into a model using these two parameters here. This full model here is the what we call ""Three slopes"" model. But if we wanted to test for the necessity of three slopes versus the necessity of only one, this is the test we would perform. So when we do this, this brings us back to the additive model. To extra perform this test though because there's a difference of two model parameters not just one like we had seen when we were only dealing with a dummy variable. We're forced to use an anova F-tests because they are nested but it's a difference of two model parameters."
stat-420,7,9,"[MUSIC] In the previous video, we were considering modeling the fuelefficiency of a vehicle in this data set. We considered a numeric andcategorical predictor, and then discussed the interactionbetween those two. And that was sort of nice, because itallowed us to create certain pictures and have this different slopes, differentintercepts, interpretation things. But now, we want to considertwo numeric predictors. In this case, we'll sort of,for simplicity's sake, consider displacement and horsepower,which again are two numeric variables, now, we can consider theirinteraction as well. We can also consider the interactionbetween two numeric variables. What does that look like? To give you an example, somewhat concrete,we'll say that x1 is displacement, and x2 is horsepower, and this new term thatwe've created here is their interaction. And again,the response is miles per gallon. We could also just thinkof the model in general. What I'd like to do now is take thismodel, and sort of rearrange it to create a coefficient in front of x1,or a coefficient in front of x2. So let's do it for x1 first, so in that case I'd have a rearrangementof the model that looks like this. The coefficient in frontof x1 will be beta1 + beta3 times x2 + here beta 2,x2, and some error. Or we could do this the otherway around and say, let's regroup to get sort ofa coefficient in front of x2. So here, this term has no x2 in it, sowe'll go ahead and put it in the model. And then in front of x2,we'll have, let's see, beta2 + beta3 times x1,that whole thing times x2. This interaction modelessentially allows for the effect of x1 to be different,dependent on the value of x2. So in this model, we see that ifwe increase the displacement by 1, the change in average fuel efficiencyis given by beta1 + beta3 times x2. How the average fuel efficiency changes,when we change displacement, is different depending onthe horsepower of the vehicle. Similarly, if we think ofthis the other way around, if we increase the horsepower by 1, the change in average fuel efficiencyis given here by beta2 + beta3 x1. So it's different, depending onthe displacement of the vehicle. This model is what we call,more flexible than the additive model, which looked like this. Here, if we increase displacement by 1,the change in average fuel efficiency is given by simply beta1, which is in no waydependent upon the horsepower of the car. Or, if we increase the horsepower by 1,the average fuel insufficiency is changed by beta2, which again, is inno way dependent on the displacement. So adding this interaction term,essentially allows for greater flexibility in the model. [MUSIC]"
stat-420,7,10,"Let's return to the additive model for fuel efficiency that used displacement, a numeric variable and number of cylinders categorical variable as pictures and here we see the picture of the fitted model. Now when we had talked about this model before it looked like this. That's one way to specify the model in terms of model parameters but we could use what we would call an alternative parameterization which we see here. Now, these are the same model and it has the same number parameters we'll see. So here we have one, two, three, four and here we have one, two, three, four but the parameters have different meanings. In the end both these models result in three intercepts and one slope. It's just that what any one parameter means is different. Let's see what happens when we use this model for a six cylinder vehicle. I have the dummy variables up here on the top right. So if you want to be zero, V2 would be one and V3 would be zero. So in the original model we had seen before that would result in a model that looks like this. Let's see. So V3 is zero. So this term goes away. V2 would be one so it sort of goes away. So we have Y is the intercept is beta zero plus beta two and then we have plus beta one times X. In this new alternative parameterization what do we have? So here V1 is zero and V3 is zero. So I think we're left with that. So we have Y is and CV2 is one that sort of goes away. We have mu two + beta X + E. In the end, in both cases we have a single line for a six cylinder vehicle. Now in the original parameterization the intercept for that line was beta zero + beta two, but in the new parametrization the intercept is simply mu two and then previously the slope was beta one, now it's beta. This beta one and beta parameter essentially mean the same thing in both models. It's the change in average fuel efficiency for an increase in displacement of one unit and it's actually in this case doesn't matter how many cylinders the car has. Beta two in our original parametrization was a difference in the intercept between a four cylinder car and a six cylinder car whereas in this new parametrization mu two is just directly the intercept for that six cylinder car. If we wanted something equivalent to this new parameterization to the beta two parameter in the original parameterization, well, you need to look at the difference between the intercept in a six and four cylinder car. So we need to look at mu 2 minus mu one. This would be the same as the beta two parameter and the original parameterization. If we move to the interaction model, the fitted version of that we see here, we can do something similar. The top model here was how we originally wrote this model down but the bottom line is an alternative parameterization. Let's again consider a six cylinder vehicle. That's one where V2 is one and all of the other dummy variables are zero. The original parameterization, let's see, this term goes away, this term goes away and V2 is one. So we're left with, and this V2 is one as well, so we're left with let's see beta zero + beta two is intercept plus let's see beta one plus gamma two times x plus our error. Again the whole point of the interaction model is that we have different slopes and different intercepts so gamma two adjusted the slope of the six cylinder relative to the four cylinder. And we can do something similar in this alternative parameterization that we see down here. If V2 is one, this term is gone, this term is gone, also gone, gone and the V2s are all one. So they sort of go away and we end up with let's see mu two plus beta two times x plus the error. Similar to what we just seen, this alternative parametrization is much more direct. The model parameters are all either an intercept or a slope directly whereas in the initial parameterization everything was relative to the beta zero and beta one which were the parameters, in this case, intercept and slope for a four cylinder car. Again, in the lower model mu two is the same as beta zero plus beta two and beta two is the same as beta one plus gamma two in the original parameterization. We see that in both models we needed three parameters that controlled the intercepts and the original of that was beta zero, let's see beta two and beta three. In the new parametrization that was mu one, mu two, and mu three and then in both cases we needed three parameters for the slopes and the original parameterization that was beta one, gamma two and gamma three and in the new parametrization that was beta one, beta two and beta three. I only see overall it's the same number of parameters in both model in this case six. The first model everything was relative to a reference level whereas in the new parametrization everything was a little bit more direct. You might think that this direct parametrization is easier to work with and it sort of is sometimes, but we'll see when we start figuring miles in R, it uses this reference level set up by default and that is what you'll most commonly see in practice."
stat-420,7,11,"[MUSIC] So far for the purpose of illustration, we've been limiting ourself to twovariables and looking at interactions. But in practice,we'll want to use all available data. So we'll need to consider whatwe call higher order terms. In this model I have written here, we have an example of what'scalled a three-way interaction. And that's given by this term here. We see,we also have a bunch of two interactions, and three what we would call maineffect or first order terms. Within this model, let's talk abouthow x1 is related to the response y. So to do so, we're going to want to lookat all the terms that have an x1 in them. So I believe that's this first order term,these two, two-way interactions as wellas the three-way interaction. So together,that's let's see beta1x1 + beta4x1 times x2 + beta5x1 times x3+ three-way interactions. And then because I'm interestedin the relationship between y and x1, I'm going to factor out an x1 here. So I'm left with, let's see beta1 + beta4x2 + let see, beta5x3 + beta7x2 times x3, that whole thing times x1. If we increase x1 by 1,the change in average y is this sort of large coefficientwe have in front of x1. Now, that coefficient isdependent on not only x2 and x3, but x2 and x3 in a particular waythat involves also the interaction. To see how this is sort of a veryflexible model, we'll look at what happens if we did the same process,but with a two-way interaction model. Instead of a three-way model, let'slook at a two-way interaction model, so let's ignore this term now. So now the terms in this model thatinvolve x1 are this first order term, and two second order terms. So we have beta1x1 + beta4x1 times x2 + beta5x1 times x3. And factoring out an x1, we are left with beta1+ beta4x2 + beta5x3. So now the new coefficient in frontof x1 is less complicated than the coefficient front of x1through interaction model. To really get a sense of this,we might go a step further and let's say, let's get rid of all interactions, andlet's talk about an additive model. See, so we don't needthe three-way interactions, and we don't need any ofthe two-way interactions. So what we have left hereis an additive model. I'm interested in the relationship betweenx1 and y, so I care about this term here, and let's factor on an x1. Well, that was sort of a silly exercise. Okay, sonow the coefficient is just beta1. So in this model, the additive model, which is the simplest of the three-waywe just discussed, the effect of or the relationship between x1 andy doesn't depend at all on x2 and x3. In the two-way interaction model,the sort of coefficient in front of x1, so the relationship between y and x1 isdependent on the values of x2 and x3, but in a sort of restrictive way. Inside of this coefficient,there's sort of an additive model here. Whereas looking at the coefficient infront of x1 in a through interaction model, we see that, there's sort ofan interaction within that coefficient. So in terms of interpretation,the additive model is by far the easiest. And then as we move up to more and more complicated models, like the two-wayinteraction and the three-way interaction using these models forexplanation becomes much more difficult. It becomes a lot harder to talkabout the relationship between y and x1 in this model, because now thatrelationship is dependent on both x2 and x3, and additionally their interaction. The previous three models we looked at,all respects what we call model hierarchy. This is the idea that, any time youhave a higher order term in the model, you would have all of the correspondinglower order terms in the model. For example in this model,we've been looking at this is x1, x2, x3 as a third order term, and it is of thehighest order in this particular model. So that's x1, x2, x3 andwe see that there are a number of lower order terms thatare all related to this. x1 x2, x1 x3, and x2 x3 are all second order terms thatare lower than this third order term. So I'm going to write those down,x1 x2, x1 x3, and x2 x3. So, any time that thirdorder term is in the model, we would like these threeterms to be in the model. We also have three first order terms,otherwise known as main effects. So those are x1, x2, and x3, and I'm going to say thatthere's sort of a term here. There's just a 1 as the variablein front of the intercept there. Because x1, x2, x3 has x1 and x2 in it,that x1, x2 should be in the model. Because x1, x2, and x3 has both x1 andx3, that term should be in the model, and, same for x2 and x3. Because x1 and x2 is in the model,both x1 and x2 should be in the model. For x1 and x3, well, x1 and x3 should bein the model, and same thing for x2, x3. And then generally,we like to have intercept in the model. This is sort of the hierarchy structurewhen we're dealing with at most, in this case, a third order term. So, you might notice that sothe order here, we have a third order, second order terms, first order andwhat I call 0 order terms. One thing you might notice, if you'remore mathematically inclined that there's one third order term 3 second order terms,2 first order, and 1, what we'll call, a 0 order terms. You might try this out for,maybe let's say, four predictors and considering a four-way interaction. You maybe pay attention to the patternthat results in these numbers, which I didn't write correctly,this should be 3. And I think you might noticesomething interesting. So the next question is, well,why do we impose this hierarchy structure? And the reason is, it providesbetter interpretation of the models. Lets consider a model fortwo predictors, and we'll consider their first order of terms,as well as an interaction term. And we've already discussed thismodel sort of at length and we've broken it down andinterpreted whether or not x2 is either a numeric variable ora categorical variable. And sort of we saw the interpretationof this beta3 parameter. It can be used to essentiallychange slopes in front of particular terms in the model. Now, we could also considerthe following model, which we could fit this model in our,maybe we're going to do that, but now it becomes something wherethis interpretation of beta3 is sort of no longer somethingwith a useful interpretation. It has a mathematical interpretationin terms of this model, but what it actually does forus is sort of hard to talk about. So for that reason, if we'regoing to have this secondary term, we like to have the two forterms in the model as well. With that in mind, we should consider the test that we coulddo with some of these larger models. There's nothing to prevent us fromperforming the following test. Let's say, beta2 is = to,let's say beta5 is = to 0. We certainly can do this test, sothat we get rid of this term, and that will get rid of this term. But then the resulting null model doesnot adhere to this hierarchy principle. Well, this is a test that we can do andit will be very easy to do within ANOVA F-test, this is a test weprobably wouldn't do in practice, because it doesn't tellus anything useful. Instead, a test we mightwant to do is say, well, do we need thisthree-way interaction term? So that would be a test about,in this case, a single, B7, but a single model a parameter. And this is again something we want to do,we could do this with a t-test or again, because now the null model is nested, and also adheres to hierarchy wecould it in ANOVA F-test. Let's say this model comes back, or this test comes back andit's an insignificant result. So we don't need thesethree-way of interaction. So, let's consider a model without it. So now there's a number oftests we could perform. We could perform a test about anyindividual two-way interaction, which we could do with a t-test. Again, because the resulting null modelwould adhere to the hierarchy principle, or we could also simultaneously test forthe necessity of two way interactions. So we could perform a test let's say, beta4 is = to beta5 is =to beta 6 are all = to 0. This would be a test essentiallyasking the question, do we need the interactions or not? Because the null model here isexactly the additive model. That's just something to keep in mindwhen you're testing within larger models. You just sort of ask question, well,what does this test tell me and is the resulting null model adheringto the hierarchy principle? [MUSIC]"
stat-420,7,12,"Now, let's fit interaction models and R. So we'll turn to the data set we're using previously where we were only dealing with categorical predictors but only additive models. We saw that cylinder we were using as a categorical variable therefore we coded as a factor, and we were trying to model this miles per gallon variable. A model we fit last time was an additive model where we model the fuel efficiency Y by x_1 which was the displacement, and x_2 which was a particular dummy variable for whether or not the car was foreign or domestic. And we did that and we ended up with this picture here. This model allows for two different lines but they have constraint which is that they have to have the same slope. Now we want to do is we want to remove that constraint and allow for two completely different lines with both different intercepts but also different slopes. That model we had written like this and we were discussing previously which gives us this line for foreign cars and this line for domestic cars. Essentially, β_2 and β_3 are parameters that change the intercept and slope relative to that of the foreign cars. When we write the model mathematically we have this x_1 times x_2 term here. Now, we could simply create that in the dataset and then fit the model this way. But that's not what we want to do. Instead, what we're going to do is tell r that we want an interaction term based on variables that are in the data set. So it'll sort of create this in place in sort of the model but not modifying the data. That's what this does. So this is a model of a model that has miles per gallon as a response. The first order term is for displacement and domestic, and then an interaction between displacement and domestic. An alternative way to fit that same model we see here. So instead of using the colon operator which this is a specific interaction term, reusing the star operator. This essentially says fitted model with miles per gallon as response, and then displacement and domestic are the predictors. And this basically says ""Okay. I want the interaction between these two as well as any lower order terms"" which in this case would be the first order term for displacement and domestic. I can fit that too. So if we look at the resulting coefficients of those models, they're exactly the same. These are just two ways to specify the same model. This is a shorthand notation for fitting exactly this model here. We could ask ourselves ""Well, do we need parallel lines or lines with potentially different slopes?"". And that's essentially what this line is testing for here. So this line is testing for the interaction between displacement and domestic status which corresponds to testing for β_3 in the initial model statement mathematically. It's a single parameter test. So again, we can use the T-test which is given by the usual line and the summary output here of that interaction model we fit. Or we had fitted the additive model previously so we could do it in anova F-test comparing those because that is exactly the difference between the two. And we again see the same p value for both. We can go ahead and plot this situation as well. So now we'll have a different intercept for both. Actually that's here and here. This coefficient. The third coefficient. So let's see, that's one, two, three. This changes the intercept compared to the reference level whereas this fourth coefficient here changes the slope relative to the reference level. This modifies this coefficient here, and this gets tacked on to the intercept here when the dummy variable takes the value 1. We can do that, just a little pre-processing here, and then we plot similar to how we had before. And we see we have now two lines with not just different intercepts but also different slopes. So again, this was an example of an interaction between a numeric predictor and a categorical predictor which ends up in the sort of nice interpretation of two lines with different slopes. We said we could also consider the interaction between two numeric variables. So if we have this model here but now x_1 and x_2 are both numeric. In this case let's consider displacement and horsepower. We saw how then we could rearrange that model to say, if we increase x_1, which is displacement by 1, how that changes the average fuel efficiency depends now on this value of x_2. This is not the case if we had an additive model. The increasing saves space and by one would not be effected by the horsepower. Again, the additive model is without this interaction term here so the effect of one variable on the response is not affected by the other. Whereas here we have the interaction model that we see here. And we can fit those this β_3. Here is the difference between the interaction and an additive models. So we can see this line here essentially test for the necessity. Again, whether or not β_3 is 0. And as always we could also do an anova F-test to compare those two. And p values are both the same here, but they're so small we can't verify that they're the same without doing a little bit extra work. But rest assured as always when we're comparing the difference of one model parameter, the p value will be the same for the T-test and the F-test. A more interesting example again, returning to the example where we use cylinder's predictor, now will have a factor variable. And more interestingly that factor variable will have more than two levels. Again, we had argued that cylinder's should be a factor variable which is what we have here. So we have these one, two, three dummy variables. Previously, we had fit this additive model here using displacement and cylinders as predictors for trying to model this response fuel efficiency. So we had previously seen this. And we'll return to this plot. And again, so this is an additive model so this results in- And while we have three different intercepts, we only have one slope. Now we want to allow for different slopes for all of these lines. What we have to do is now look at the interaction between displacement and cylinder. We can do that with a model statement that is exactly the same as we had done with any other variable. But now because cylinder is a factor variable with multiple levels, I'll have to be careful about the output and sort of consider what it actually is. And again, we could have expressed the model formula this way but this is just shorthand for the exact same thing. Right away we see that there are one, two, three, four, five, six estimates. And that's sort of what we should have predicted based on what we already talked about with this model. We have an overall intercept and then the coefficient in front of displacement which is this β_1 here and then so these two. So this is the dummy variable for whether or not the cylinder is six. That's v_2 here. This is a dummy variable for weather cylinders 8 - that's the v_3 here - these to change the intercept relative to the reference level which was for four cylinders sort of what we've seen previously. Now, what these variables do here. So these are estimates of γ_2 and γ_3, respectively. So these are the interaction between displacement and the dummy variable for whether or not the cylinders is six. And this is the interaction between the displacement and the dummy variable for whether or not the cylinder is eight. This variable here changes the slope relative to the reference level for six cylinders that is, so relative to for four cylinders. And this changes the slope for eight cylinders relative to the reference level which was for four cylinders. Now we have three different intercepts again so this is the coefficient here. This is the fourth coefficient so that's this one here. And these are changing the intercepts. And then coefficients five and six are here and here. Those are changing the slopes relative to the reference. With that in mind, the rest of this code looks sort of familiar. And we end up with our plot here but now we have varying slopes as well as the intercepts. The thing we might want to do is compare this fitted model to the previous one where we did have the same slope for each category. So to do that now within this model we would need to set γ_2 and γ_3 equal to zero. Essentially, getting rid of the interactions between cylinders and displacement. Again, that's exactly this here. We would want to set those equals to zero. But now this is a difference of two parameters. We can't simply look at the summary information and find a single line from a T-test. We have to perform an anova F-test so we use the anova command give it the additive model which has these two coefficients set to zero. And the interaction model which allows for them to be non-zero compare them. And we end up with a very small p value again so at any reasonable alpha we would reject the null hypothesis, and we would prefer this model here which has the possibility for different slopes. We have talked about parameterization of these models and we said that by default when we give r this formula notation here, it's going to use this sort of reference style. So it's going to have an intercept and then parameters that modifies the intercepts relative the reference level and same thing with a slope in front of displacement. We have these sort of terms that modify it for cylinders 6 and 8. But that's not the only way we could write down the model. We could write it down like this where we have individual intercepts and individual slopes written directly into the model in this by utilizing the dummy variables in an appropriate way. What I have here are four ways to fit the exact same model just with different parameterizations. This version of it will fit exactly this model here. This version will fit exactly this model here and this will fit the same one which is actually neither of these. By just looking at the output will determine what these are actually doing without even needing to write down the mathematics behind it. So I fit this. This is exactly what we had just seen. So we have the intercept for four cylinders and how to modify it, and the slope for four cylinders and how to modify it. So again, that was this model here. Here to the output here. So here we directly have a... an intercept for four cylinders, an intercept for six cylinders, intercept four eight cylinders, and then slopes for each cylinder. These estimates here are exactly estimating these parameters here. For example, we see here this is the estimated β_0 in this model so the intercept for four cylinders. That is exactly what we see here. But so again, this we believe to be the estimated intercept for six cylinders. And the question would be ""Well, how do I get to that?"". Well, let's take this here and well here's how we modify to get to exactly that. So we had that and we see that this does indeed match this. And we can do that for slopes as well. So here is the slope for say, eight cylinders. Well, let's take the slope for four cylinders in that parametrization and add this. And we see that this is exactly this. While this is what we'll see most often because it's the least work to do in r and sort of what are willing us to do. We could think about things this way if we want it. So some alternative ways of doing this. This fits an intercept directly for each of the cylinder types but it gives us a reference slope for four cylinders. And then, how to modify for six and eight. So for example, if I take this reference slope and add to it this here for eight cylinders, we end up with what we had for eight cylinders directly and the other parameterization. If that's not enough to convince you that say, for example, these two are exactly the same. I could look at the fitted values from those two models and we see that they're exactly the same. The models will make the same predictions. It's just their parameters what they mean are slightly different. So far we've only been dealing with two predictors at a given time but obviously in practice we'll deal with more and more predictors. So this will allow for what we call higher order terms and they'll be all sorts of other interactions flying around. Here's a model with three predictors. So now we have one, two, three first order terms. We have one, two, three, two interactions, and one, two, three way interaction. The way we fit this model in r, there's a number of ways we can do this. The easiest is to use the star operator. So what this says is ""Okay. I would like the three way interaction as well as anything below in the model hierarchy."" So all possible two way interactions below this, all possible first order of terms as well. And by default, our use is an intercept β_0. An alternative way to do that would be let's say, add those together and then raise it to the third power. You might be aware that we can fit polynomial models but that is not what this is doing. We'll see how to do that later. What this is saying is ""I want all possible three way interactions between these terms as well as all terms below it in the model hierarchy."" When I fit that model. Let's see we have... Let's see eight terms here and we end up with eight estimates here. Here is the estimate for β_7. The three way interaction. And then we have the two way interactions and the first order terms here. When we talk about hierarchy we said sort of the most obvious test to do here would be ""Well, do we need the three way interaction?"". So that would be testing whether or not the β_7 is zero so whether or not we need this term here. To compare we could simply look at the summary of this model, but instead what I'll do is I'll fit the two-way model and there's a couple of ways to do that. One of which is this here. This is sort of similar to this but now instead of starting with a three way term, and everything below it, this is requesting all possible two way interactions and anything below it in the hierarchy. The other way of doing that is well here is saying ""Okay. I want the two way interaction between displacement horsepower and everything below it."" Plus the two interactions between displacement, and domestic and everything below it, plus the two interaction between horsepower, and domestic and everything below it in the hierarchy. So these are two equivalent ways of fitting the model with two interactions and the first order terms. That's everything but this term here. And what's first thought of the coefficients is to verify that this is indeed doing the three first order terms as well as the one, two, three, two interactions. And then we can use an anova F-test to compare to the full model that had also the third order term. It appears that third order term is not significant. So we would prefer the smaller model with the two interactions. We'll note here that just because it's not a significant difference, doesn't mean that there is a difference. So within this data the big model does have a smaller mean squared error, but it's an insignificant difference as the F-test is telling us. Sort of continuing with the hierarchy. We could say ""Well, we said that this term is insignificant so the other thing we would want to ask us is, 'Well, within that two way interaction model, do we actually need all the two interactions so we can simultaneously test for the need for these terms here?'. And that's exactly this here. So I would need to fit what we call the additive model that has only the first order terms and compared to the two way interaction model here. And that is significant. So we do prefer the model with the two interactions instead of the model with just the additive terms. And because I've been going through a marked down document, it only makes sense to knit this in the end. And there we have it. So there is a nicely formatted resulting markdown document that has everything we just did."
stat-420,8,1,"In this lesson, we'll discuss the assumptionsof a linear model. These assumptions mostly centered around the error termsare used in the creation ofinterval estimates and hypothesis testfor linear models. However, if theseassumptions are not met, those intervals andtests are worthless, as they won't actually liveup to what they claim to do. A common phrase for the ideaof building models with invalidated assumptions isgarbage in garbage out. To make sure the models refit to our data don't violatethese assumptions, we'll look at a numberof tools used to assess specific violationsof model assumptions. These tools will include both visualizationsand statistical tests. Like always, we will utilize R to create these visualizationsand perform these tests. In the following lessons, we'll see some of the sourcesof these violations as well as potential methodsto correct our models."
stat-420,8,2,"So here we have the statement of the usual multiple linear regression model that we've been fitting. I've made the error term rather big here because the error term really directly encodes all of what we call the assumptions of the model. So we're assuming that the epsilon I's are what we call IID random variables. Importantly one of the I's stands for independent, IID again being independent and identically distributed. We assume that they are normal. They have a mean of zero and a variance of σ_2. And these errors are distributed about the mean which we see here, which is written as a linear combination of the, in this case, P-1 predictors, which utilizes a total of P beta parameters. Often when discussing the assumptions of multiple linear regression, a sort of quick and clever way to summarize these assumptions is with the word LINE. The L in line stands for linear. In particular we're talking about the expected value of Y given the value of the predictors and that can be written as a linear combination of those predictors, and that's exactly what we see here. The mean is a linear function of the predictors. So the I stands for independent, which is an assumption that we're making about the errors here. And again, we're saying that the ϵ_i's are IID for independent and identically distributed. So speaking of their distribution, we're assuming that they are normal. That's what we see here, and we're assuming that they have equal variance, which we see here. So importantly, the σ_2 is not a function of say the predictors. A big part of what we're going to do in this lesson in this week is check our model assumptions. But so that begs the question well why do we check the model assumptions. Basically, as long as we can perform this matrix inversion right here, we can fit the model using least squares. We can obtain estimates for the individual beta hats. With the assumptions of the model, in particular the error assumptions here, we learned that each individual estimate of a beta parameter actually also follow normal distribution with some mean and variance. We then utilize this fact to do a number of things. This led us to creating interval estimates, to doing hypothesis testing, creating prediction intervals and so on and so forth. So for example, here I have written down the test statistics for testing an individual model parameter. Let's say maybe we wanted to do the test about whether or not beta 1 is zero or alternatively beta 1 is not equal to zero, we would have first calculated the value of the test statistic and then with this distribution here, which again was a result of this distribution here, we know that the test statistic follows a t distribution with n minus p degrees of freedom. We are utilizing both this derived distribution here but more importantly the original model assumptions to arrive at the distribution of the test statistic, which is how we are calculating the P-value. So for the P-value to be valid and have the correct meaning that we want it to have we need the assumptions of the model to be correct or we should say the assumption of the model to not be violated. We shouldn't suspect that they are not true. So if the assumptions of the model are wildly violated, we can still calculate that P-value, we can calculate the value of this test statistic and then incorrectly assume this this distribution and arrive at a P-value. But it essentially won't have the correct meaning then. We have a phrase for this. We call it garbage in, garbage out. Another way of thinking this or another example with some data that I will return to later. So here I have some data and I fit it at some point of regression and if I were to add prediction intervals for example I think they would be something sort of like this. There should be slight curvature in the X but it's not a big effect here but so we should sort of notice something here. For large values of X the interval is sort of more or less accurately describing the situation. So like if we were to say this is maybe 99 percent prediction intervals, it's capturing maybe roughly that percentage of the points over there but over here for low values of X it's, we can pretty clearly see that it's almost always capturing 100 percent. What we would want here is much narrower intervals for low values of X and much larger intervals for high values of X. But that's not what we get because we're assuming constant variance here because our assumptions here are basically wrong, there's lower variance for a lower axis here. We end up with prediction intervals that don't actually make sense. And that's why we say garbage in, garbage out. So because the assumptions aren't met here, we get a bad result in the end."
stat-420,8,3,"[MUSIC] In order to discuss tools that will helpus assess the assumptions of the models, it'll help to have some models that bothdo and do not violate the assumptions. This first model I have here doesnot violate any assumptions. The mean Y, that is, is a linearcombination of the, in this case, single predictor. And the errors are normally distributedindependent and have constant variance, so this model is good. So this model 2, here, the linearityassumption is good, independent and normal assumption are okay but we have thislittle issue here where the variance is a function of the predictor, so this willviolate the constant variance assumption. So that's going to be a modelthat we hope to detect as having violated assumptions whenwe see data generated from it. And similarly, model 3, all the error assumptions are okay buthere the mean function is not a linear function of the predictors whenwe think of just x as the predictor. We'll actually later see how toovercome this very easily, but for now we're going to consider that aviolation of the simple linear regression model assumptions. So these will be three modelsthat we generate data from and then apply tools to,to assess the assumptions of the model. So in this case because we have a singlepredictor x, we'll be fitting the simple linear regression model, which, again,aside from the linear portion of it just being a smaller thing, this only has onepredictor, everything else is the same. So this is the model thatwe're going to fit to data. So when we fit it to data the fittedvalues for a particular value of x will be beta 0 hat, or our estimate ofbeta 0 + beta 1 hat times x. And then the residuals will bethe original data minus the fitted value at the original predictor values. So we'll spend a lot of time here focusingon the residuals that'll play a key role in all of the tools we use forlooking at the assumptions. Here we have some datathat was simulated from this model 1 that did notviolate any assumptions. The orange line, then, is the resultof fitting the simple linear regression model to the data generated from this truemodel that we happen to know in this case. In practice, we would only havethe data not the true model, obviously. We can actually probably look at this andsay things are looking pretty good here, the data definitely appears to havea linear relationship between Y and x and the constant varianceassumption seems to be pretty okay. We can see that because it'sa simple linear regression, we only have a single predictor. In practice, we'll often havea lot more than one predictor. So this visualization,we will not be able to make. So we need a tool that's more general,that we'll be able to use for multiple linear regressions as well. And that tool will be calleda fitted versus residuals plot. So this plot actually putsresiduals on the y-axis and the fitted values on the x-axis. So you might call it a residuals versusfitted plot, I just have a preference for saying the phrase fitted versus residuals,it rolls off the tongue better. We can check two assumptionsrather well with this plot. Of the linear independence normal andequal variance assumption, we can check the linear and equal varianceassumptions very easily on this plot. First of all,note that the fitted values here, so these are y hat, fora particular value of x. These are an estimate of the expectedvalue of y given the value of x or the value of, say, the vector x for all of the predictors inthe multiple linear regression case. What we want to see then is thatthe residuals are centered at 0, which we more or less see here. And that's why generally where wemake a fitted versus residuals plot, we add a horizontalline at a y value of 0. What we want to see is nopattern in the residuals. They should essentiallyalways be centered at 0. In this case,that does not seem to be an issue. We believe that linearityhere is not violated. The fitted versus residualsplot is also useful for checking the equal variance assumption. We said that the errorsare made about the mean. So at a particular mean,say here, estimated mean of 10, we expect the residuals to be centeredaround 0, and spread about them. But we would expect that the spreadabout them at, say, 10, or a mean if 10,is the same as when the mean is 20. So, if you look at the rough estimate ofthe variance there is something like this, and over here there is maybe somethinglike this, and those are roughly similar. So we don't see any sort ofpattern in the variance here, it seems to be roughly the same forany fitted value here. So we would say that we don't haveany suspicions about the equal variance assumption, sothat does not seem to be a problem here. Moving on to data generated in a waythat obviously violates an assumption. In this case we're generating data thatviolates the equal variance assumption. So, again,first we plot the usual scatter plot and add the fitted regression line to it. And here we can, again, very clearly see right away that theconstant variance assumption is violated. But again, that's because we'reonly using one predictor so we can't actually visualize this. But in practice, we would need to, again,use a fitted versus residuals plot. Going through our assumptions again,we said that this is useful for checking linearity and equal variance. The linearity assumption here appears tobe fine, which is to say that the mean of the residuals is pretty much zero prettyobviously anywhere on this plot here. The residuals are always sort of equallyspread about this horizontal line here. But the equal variance assumptionis very obviously violated here. So over at an estimated mean of about 5,we have a very small variance. Whereas over here, at an estimated meanof 25, we have a much larger variance. Now moving on to an example thathas a clear violation of linearity. In here, with the scatter plot,we see this very easily. So, if we just saw this data,we'll realize immediately that there's some sort of quadratic trend in the databecause we only have one predictor. We can see this. So again, in general, we'll want touse the fitted versus residuals plot. And here, sowe'll discuss the usual assumptions here. And the one we want tofocus on is linearity here. The issue here is that the residualsare no longer spread about this horizontal line here,there's obvious patterns. So for low fitted values weare under estimating, for high fitted values weare under estimating, and for things in the middle we are oftensort of overestimating. This essentially means that we haveessentially just mis-specified our model and we'll need to correct that bylearning how to fit polynomials, which we'll do later in this week. But that's to say that this modelis an example of something that violates the linearity assumption. So again, you're essentially looking forpatterns in the fitted versus residuals plot, that is, deviationsfrom being centered at 0 on the y-axis. It turns out that this data actually doesnot violate the equal variance assumption. So if we look at a fitted value of 0, we can sort of crudelyvisualize a variance this way. But, if we look at a fitted value of,lets say, 90 here, we can crudely estimatethe variance this way. It looks the same. And that we can sort of do the samething anywhere on this plot. It's just that it's not centered at 0,so that's a violation of linearity. But it does appear that, for any fitted value, the variancedoes appear to be roughly equal. We have this fitted versus residuals plot, and this is a way to check the linearityand equal variance assumptions. So here, you could almost beginto diagnose normality here. So normality of errors, we would expectthe vast majority of them to sort of be close to 0 and then fewer andfewer to get away from 0. And we more or less see that to be truehere, but the next thing we're going to want to do is look at a better way ofvisualizing this normality assumption. [MUSIC]"
stat-420,8,4,"[MUSIC] Here, we have three models thatwe will generate data from and then use that data to assessthe assumptions of the models. And we said that this first irst model, it's data will appear to haveno violation of assumptions. But the second and third models, they will appear to violate some ofthe assumptions of the linear models. Again, those assumptions being summarizedby a line, linear, independent, normal and equal variance. The one that will be interestedin now is this normal assumption, só that will be no problemhere in the first model. The second and third models, the way theyare written here strictly the violations are none constant variance here. And a nonlinear relationshipbetween the mean of y and x here. But it will turn out that becauseof these it will also appear as normality assumption is violated. And that's we want do now isdevelop a tool to assess whether or not we believe the errorsare normally distributed. One of the most general ways to visualizethe distribution of some data is to use a histogram. So what we'll do here is look ata histogram specifically of residuals of a fitted model. So if we say e sub i are the residuals forthe ith observations. We can sort of view those as an estimateof epsilon sub i, the actual errors. Which are supposed to follow a normaldistribution with the mean zero and the variance sigma squared. If the residuals appearto be roughly normal, we could believe that theirtrue distribution is normal. Looking at these three histograms,which are the histogram of residuals for fitting two data generatedfrom these three models. We sort of immediately sort of say,well this third one here, those don't look very normal at all. The second one is sort of questionable andthe first one sort of looks okay. It's sort of hard to say here,we have these sort of longer tails here. And while we can survey easilydrawn normal curve here, I have done a great job with, this isn'tvery accurate way to asses normality. We want a better tools for this job, so we'll look towards is what'scalled a normal Q-Q plot. Q-Q stands for quantile quantile. It's a plot where on the y axis,we have sample quantiles and on the x axis,we have theoretical quantiles. So in this case here, I have three plots, each what we call a normal Q-Q plot forvarying sample sizes from low to high. And this data that we see plotted herewas generated from a normal distribution. So this data we see here wasa sample from a normal distribution. Essentially what we have onthe y axis is the observed data. What we have then on the x axis is more orless what we expect to see. In particular, it's what we wouldexpect to see if the data was sampled from a normal distribution. Then we have a line in additionto the plotted points here. That line is not the regression line. That's actually a very specific linethrough the first quartile of both the theoretical and sample quintilesas well as the third quartile of the theoretical and sample quintiles. We won't actually go into detail in thisvideo about how to create these plots, we're going to let r do it for us anyway. If you're interested,you can read about the details. But what we're going to do is use theseplots to simply assess normality. What we want to see is that the pointsplotted here, fall very close to the line. And that's more or less what we seeover here in the large sample case. In the medium sample size case, we seeroughly that and in the low sample case. We see this one point down here that'ssort of fairly far off of the line here. But again, these re-plots are all whatwe would say are good normal Q-Q plots. Because the way I created this data, I legitimately sampled itfrom a normal distribution. So these are three plots we wouldexpect to see from normal data. So what I have here now is similar threeplots with increasing sample sizes. But this time, this data wassampled from a t distribution, in particular with degrees of freedom 4,I believe. So this is an example of datawith what we call fat tails. So a t distribution is very similar toa normal distribution, it's mean zero, it's roughly bell shaped. But essentially it just has a little bitmore area out in what we call the tails. And we can see that in this plot,the y axis is the actual observed data. Now the x axis is again what wewould expect to see if the data came from normal. And the way we sort of see these fattails is these deviations from the line. This is sort of indicating to ushere that this data has fat tails relative to a normal distribution. So we would not believe thisdata to be something that could have been generated froma normal distribution. It's sort of maybe not extremely clearhere in the sort of median sample size case. And I think it's very hard tosay in this low sample case, because this plot here looks remarkablysimilar to what we saw in the normal case. For another example, we could look atdata that was sampled from exponential distribution, which isan example of skewed data. In the low sample case, it's sort ofhard to assess, but in the medium and larger sample cases,we see the skew sort of show itself here. These v plots,we're going to be sort of say, well it's unlikely that this data wasgenerated from a normal distribution. This is a better tool than a histogram,but its still a tool that sort of asa little more art than science. This picture here is sort ofthe best case scenario here. That's about as good as you'll ever see,a Q-Q plot. But for everything else, you sort ofhave to make a bit of a judgement call. So we can see here, well there's that sortof a somewhat similar to what we're seeing in this tail situation here,but less severe. So one thing we'll do when westart creating these in r, is we'll sort of go throughgenerating a lot of them for normal data to sort of get an ideaof what they should look like. But in general, we're looking forthe points to fall very close to the line. So now we'll use a normal Q-Q ploton data generated from these models. Here, we have the residuals, so that's what we have here onthe y axis is the residuals. As a result of fitting the simplelinear regression model to data generated from this first model here. It should be no surprise thatthis looks extremely good. This might be the absolute bestnormal Q-Q plot that you'll ever see. There's sort of almost no doubt thatthis data could have been generated from a normal distribution. So we're plotting the residuals now, because we're trying to assessthe normality of the errors here. And this gives almost nosuspicion of that assumption. So next, we'll do the same thing forthis second model here. So we'll generate data from this model andthen fit simple linear regression to it. And the resulting Q-Q plot for the residuals of that modellook something like this. And we sort of this largedeviation in the tails again. That might be a little bit confusing,because we were saying that hey, we were generating the errorsaccording to a normal distribution. But it didn't have constant variance, so in the end when we look atthe distribution of the residuals. We're not considering a differentdistribution for use of an x value. We're just considering the residualsoverall and that appears to not be normal. We can sort of think about this by goingback to the fitted versus residuals plot for there,which looks something like this. So while yes,the residuals are centered at zero, and a lot of the data is very close to zero. The larger the fitted values get,the larger the residuals get, so that's resulting in sortof these fat tails and sort of that's why wesee this pattern here. Which are sort of similar to whatwe've seen with the t distribution. And then that third model, the errors for technical generatedaccording to a normal distribution. But because another assumption wasviolated in this case linearity, we again see a normal Q-Q plot thatdoesn't look particularly great. We would essentially questionthe normality assumption, which would essentially tell usto reassess the model in general. We happen to know that the issue hereisn't exactly with normality, but this is highlighting the fact for us,that there is an issue with the model. [MUSIC]"
stat-420,8,5,"[MUSIC] So far in checking the assumptionsof the linear model, we have two diagnostic toolsthat both happen to be visual. We have a fitted versus residual plot anda normal q-q plot. Maybe you're a little bit uncomfortablewith how non-specific they are. They don't give you a definite answer. There'ss more of an art to utilizingthem than an exact science. We could also look to performingstatistical tests to assess the assumptions of a model. So the first one we'll look at,I'm going to call the BP Test, because I'll surelymispronounce this name. And this a test that will help usassess the equal variance assumption. So in the line version of the assumptions,we're looking at E here. Like any other statistical testthat we've seen, we have a null and alternative hypothesis. So often when you see the null andalternative of this test, you'll see the words homoscedastic andheteroscedastic. These are just fancy words forequal variance and unequal variance. We'll omit the details of this test. We'll simply use it when westart checking assumptions in R. But so essentially, what we havehere is if we perform this test, we'll get a p-value andif it's a low p-value, that means that the constantvariance assumption is suspect. Because essentially,like all hypothesis tests well. So what we're going to do is we're goingto assume constant variance and then calculate the probability of ovservingwhat we did or something more extreme. So if we have a low p-value, we've seen it's sort of unlikely underthe assumption of equal variance. The BP test would be an analog tothe fitted versus residuals plot. In a similar fashion, the Shapiro-Wilk test would bethe analog to using a q-q plot. So this is a test for normality. Here, we're going to be performing thistest on the residuals of the model. So the null hypothesis here assumes thatthe data, in this case the residuals, were sampled from a normal distribution andthe alternative is the opposite off that, which is that they werenot sampled from normal. So here if we get a low p-value,this implies that normality is suspect. Because the way we're performing thistest is we're assuming that the residuals are sampled from a normal distribution. And then calculating a probabilityof observing the residuals we got or something more extreme. So again,if we get a very low p-value here, this would make us questionthe normality assumption. So returning to generating data from a model that has noviolation of assumptions. So on the left here,we have a fitted versus residuals plot and also the p-value fromperforming the BP test. So let's go ahead andsay we're going to use an alpha of 0.01. So here, we would fail to rejectthe null hypothesis which, again, that hypothesisbeing equal variance. And that matches what wehad seen in the plot here. So based on both the test and the plot, we have no reason to doubtthe equal variance assumption. Similarly, if we perform the Shapiro-Wilktest on the residuals we get, again, a very high p-value herecompared to our alpha. So we would fail to rejectthe null hypothesis there. So we have no reason to doubtthe normality assumption and that, again, matches what we hadseen in the q-q plot for that data. Now if we now move back to a model thatdid have some violation of assumptions. And again let's say alpha is 0.01 forthe purpose of the testing. Here, we reject the nullhypothesis of the BP test. So the constant varianceassumption is suspect. And that exactly matches whatwe've seen in this plot here. And if we perform the Shapiro-Wilktest on this data, we also reject the null hypothesis. So the normality assumption is suspect and that matches what we seein this q-q plot as well. So here we're getting matching resultsbetween the plots and the test. In practice, however,that might not always happen. In practice, the tests, especially with large sample sizes,are sort of very quick to reject. So in practice, often we saywe sort of defer to the plots. So if the plot looks reasonable,we might go ahead and not worry about the assumptionthat that plot is looking at. The tests will sort of very,very, very often tell us so we have suspect assumptions. But maybe the plot willlook reasonable and we'll maybe conclude that we're not thatworried about that particular assumption. We'll take a look at this when we get intoR and we'll see that sort of happening. But in general, the milder the violationas well, the milder the consequences too. So often, we'll be okay with somedeviation from the assumptions and we'll be mostly on the lookout for very, very large deviations fromthe assumptions of the model. [MUSIC]"
stat-420,8,6,"[MUSIC] Now we'll go ahead andtake a look at some diagnostic tools for assessing the modelassumptions inside of R. To do so we'll need some data. We'll generate some data ourselves so weknow exactly which assumptions have been violated or if the model was simulatedwith no violation of assumptions. This first function here willsimulate data that is exactly the Simple Linear Regression Model, soit'll have no assumption violations. So we see the mean isa linear function of x, and the errors are normally distributedwith a constant variance. The second function here will simulatedata where the mean is a linear function of x, the errors are normal, butthe variance is a function of x. So we'll see that this isnon-constant variance. And this third function here, we'll simulate data that has normalerrors with a constant variance, but the mean is not a linear function of x sowe see this quadratic term here. For now we'll consider this a violationof assumptions, we'll later see exactly how to deal with that and thata linear model can actually handle this. I should probably load these first. The first thing we'll do is generate thatdata without any violation of assumptions. So now what I'm going todo is plot the data we will fit a simple linear regression model tothat data and then add to the plot and this is a perfectly valid plot. Nothing jumps out at us hereas a assumption violation but to double check that, we'll createa fitted versus residuals plot. We are plotting the fitted values onthe x axis, the residuals on the y axis. And we're addinga horizontal line at y is 0, otherwise everything else we see here isjust modifying some graphical parameters. So this is our first example ofa fitted versus residuals plot. In a lot of ways, it's actually a bad example, because ithighlights no violation of assumptions. So what we see here is noise thathappens to be centered at 0. And this gives us no cause for concern. We would not be worried about thisdata violating any assumptions. A better example would be to showviolation of assumptions in the fitted versus residuals plot. So we'll generate some data from thatsecond function which had the non-constant variance issue. So first we will plot the data,we'll fit simple linear regression and add the fitted regression to the plot. You can probably already see thatthere's an obvious non constant variance issue here. We'll move to a fitted versus residualsplot, and that makes it clearer. So we could only create this plot fora simple linear regression, but this plot will also work for multiple linear regression, so,putting the fitted value here. So for the fitted value,which is an estimate of the mean, we expect the residuals to be centeredaround 0 for any fitted value. Which they are here, butthen there's clearly lower variance for low fitted values andhigher variance for high fitted values. This plot would suggest a nonconstant variance issue. Lastly we'll generate some datathat had a quadratic mean. We will first plot the data, sowe'll fit simple linear regression, so we'll attempt to fit a line into this dataand add it here and we can immediately see some issues here and the fitted versesresiduals plot will make that clear. So we are overestimating here,we are underestimating here and here. This actually has probably,roughly constant variance. The problem is that the residuals are notcentered at mean for any fitted value. So we see this clear pattern inthe fitted versus residuals plot. This suggests that the formof the model is incorrect. So we would consider thata violation of assumptions, the mean is not a linearfunction of the predictor. Going back to this data here wherewe had the non constant variance. We also saw a test for this so the bp test, turns outthe package lm test has a function bp test in it which does exactly what youthink it does it'll perform the bp test. If I load that package. So if I perform the bptest, it take asinput a fitted model via the lm function. So if I do it for that first modelthere we get a high p value. Again remember here that the nullhypothesis was exactly constant variance. So this high p value does not makeus doubt that assumption in any way. For the second model, the one we see plotted here,we obtain a very low p-value. So this suggests we have reason todoubt the constant variance assumption which again this plot sortof very clearly tells us. Lastly, for the third data set we created,the BP test actually does not reject. Suggesting that there's nota non constant variance issue. Which again is correct, the constantvariance isn't the issue here. It's the mean is incorrect here. While this plot does suggest an issue,the issue isn't exactly constant variance. It's the non-linear mean function. So the fitted versus residuals plot tellsus about the linear mean function and the constant variance, but we'd alsolike to check the normality assumption, which is rather hard to see inthe fitted versus residuals plot. So one thing you might think to do isto create histograms of the residuals, which we'll do here. Here it might not be a stretch to imaginethat this first one has normal residuals. The second one has maybe normal residualsbut we sort of see fatter tails here. And this third plot issort of rather weird, so we probably wouldn't thinkabout normality here. Histograms are sort of rather sensitiveto the binning selection we create here, and generally it will be sortof hard to tell about normality. We'll want a better tool. That tool will be a qq plot. So here I've written a functionwhich you can also find. In the text that sort of explicitlystates how a Q-Q plot is created and I'll actually load this becausewe're going to check it real fast. So what I'm going to do is I'm going togenerate some data from a normal distribution and then I'm going to createa Q-Q plot two ways just to verify that this function here is the sameas some built-in functions in R. And if you're interested you candig into the details here to see exactly how this plot is created. In R There are two functions,one called qqnorm and one called qqline, qqnorm will create points so again puttingthe observed quantities on the y-axis and what we would expect to see if itwas from normal on the x-axis and then qqline we'll add the linewe expect to that plot and then we'll just verify that,that function up here matches that. It looks like it does. We'll go ahead and use this just to bea little bit quicker than using these two built-in functions in R. Reading a Q-Q plot is a bitmore of an art than a science. One thing that would behelpful to do is quickly look at a lot of different Q-Q plots to getan idea of what you should expect. So, what I'm doing here is I am simulatingdata from a normal distribution of three different sample sizes and,for each, plotting a q-q normal plot. These would be three q-q normal plots for data that was actually sampledfrom a normal distribution. We know that this data came from a normaldistribution because we sampled it. So we can see that inthe third panel here, the data fairly closely follows the line. We see some deviation fromthe line in the second panel. And this first panel isactually not too bad. But one thing you might want todo to get a really good feel for this is do something like runningthis code repeatedly and so all of these plots are plots where the data wassimulated from a normal distribution. So all of them would be qq plotsthat are essentially good Q_Q plots. And you'll see that witha greater sample size, we sort of see that there's generallynot too much deviation from the line, normally it happens sort of at the veryend if at all whereas for a low sample size, we can see some deviation that sortof appears fairly big from time to time. Since these plots are forassessing normality, we should throw somenon-normal data at it. So that's what I've done here, so here I've simulated data from a tdistribution with different sample sizes with a fairly low degrees of freedom,sort of giving it a heavy tail. And let's see what we have here, this issort of easily seen in the high sample size case with the sort of tail behaviorhere where it becomes further and further from the line. We see this one large deviation and the low sample size, this is somethingthat wouldn't be impossible to see had we generated this data froma normal distribution. We saw before that with lowsample sizes we can get one or two fairly large deviations. And this middle plot here, that would be hard to say that it'snot from a normal distribution. Q-Q plots aren't a perfect tool, but it does a fairly good job especiallywith a reasonable sample size. And then we'll also simulatefrom some exponential data, which is something that is skewed. This is fairly clear forthe medium and high sample sizes and even with the low sample size. Again, we see this sort ofobvious deviation here, the tail. Going back to that datawe had been generating. Actually what I'm going to dois I'm going to clear this. Here is the normal q q plot for the data that did not haveviolation of assumptions. And looks pretty good. Again, we only see some veryminor deviations in the tail, but that's somewhat to be expected. When we run this on the datathat had non-constant variance, this actually ends up with whatsort of looks like fat tails. So, again, technically the errorshere were normally distributed. But because their variance was a functionof x when we try to fit a single normal distribution to that it sort of breaksdown and appears to have fat tails. And a similar issue with the datathat had a quadratic mean we see sort of this oddtail behavior here. So this is sort of suggesting thatnormality assumptions is violated. But, again,it's actually a mis-specified model, if you'd like a test instead of a plot,we talked about the Shapiro-Wilk test. If we run it on data that wasgenerated from a normal distribution, we would hope that it would not reject. Again, the null hypothesis hereis that the data could have been sampled from a normal distribution,so here. High p value, that's what we're lookingfor and here I'm generating some data from a very non normal distribution, we geta very low p value so that's what we're looking for, and then again returning tothe data that we had been looking at. So the model with no assumptions. High P value, looking good. Model with a non-constant variance,very low P value, we would reject that, gives us cause for concern. And similarly, that third model, which, if the plot we were just beenlooking at, also a very low P value. So we reject the assumption thatthe data could have been sampled from a normal distribution. A minor note here is thatShapiro-Wilk test isn't a test specific to linear models It'sabout any data in general. And the data that we're giving it isthe residuals of a model fit via LM. So BP test directly tookas an argument a model, whereas the Shapiro-Wilk test islooking at the residuals of the model. And that's just based on howthose two functions were written. So now looking at some real data, I believe this is a modelwe have fit previously. So this is an additive model forthe mt(cars) data, where we're trying to model the fuelefficiency using the horsepower and transmission variable, whether or notit was automatic, in an additive model. So we'll fit that, sowe would like to assess whether or not there's any violation ofassumptions of this model. So we'll first plot a fittedversus residuals plot. Since there's not a ton of data, it would be sort of hard toactually see a pattern here. You might try to say that there's slightlyhigher variance for larger fitted values, but with such a small data set, it wouldsort of be hard to make that call. We could run the BP test andit actually obtains a fairly low p-value. So if you would ahead of time, set youralpha to be 0.05, you would reject and claim that there's an assumption violationhere, specifically non constant variance. Generally, we sort of liketo defer to the plot, and it seems reasonable, sowe probably wouldn't cast out here. Similarly, we can look at the normalqq plot, which looks fairly good. We see a couple deviations here butnothing too wild and I believe the Shapiro-Wilktest will confirm that, so high P value sowe're not too concerned with normality. So moving on to the larger auto MPGdata set that we've seen a number of times I will load that here. Last time we sort of fit this large model. It was a model with three way interactionbetween displacement, horsepower and this domestic variable as wellas all the lower order terms. So if I plot a fitted versusresiduals plot this is a tough one. We sort of don't see an obvious pattern. Maybe if it weren't for these three or so points here you could argue thatthere is some increasing variance. But it's sort of hard to say. If I performed the BP test here,I get a very low p-value. That actually does maybetogether with this plot maybe cast a little doubt onthe constant variance assumption. It doesn't seem to be a horribleviolation but we do see some evidence that there may be increasingvariance for increasing fitted values. And then similarly, the QQ plot here we see this sort ofobvious fat tail behavior going on here. And Shapiro-Wilk test will confirm that. And so in the next couplelessons we'll maybe look for ways to improve these two diagnostics,the fitted versus residuals and qq plot to see if we can maybe by changingthe model a little bit, and maybe doing some other things, improve upon thispossible violation of assumption here. [MUSIC]"
stat-420,8,7,"All observations arenot treated equal. It is not uncommon tofind a small handful of extreme values lying outside and away fromthe bulk of the data. Often, a model analysiswill highlight some rather unusualobservations for us. These may be a few observations that don't fit the model well, if they deviate fromthe model enough, we might label thoseobservations as outliers. Sometimes, identifyingmodel outliers can be the entire goalof an analysis. A few reasonable outliers don't necessarily cause usto question our model, but some extremelyunusual observations might raise cause for concern. If a large enough outlier exists at the right valuesof the predictors, that single point could have a huge effect on the model fit. Sometimes so much so that it appears that the modelassumptions are violated. We'll introducethree metrics for assessing the unusualness ofan observation: leverage, standardized residuals,and Cook's distance. These metrics willallow us to identify outliers as well asinfluential points. Of course R has built-infunctionality for each, so we'll explore using thesemetrics to identify outliers and influential points and to evaluate their effecton our models."
stat-420,8,8,"Here we have a recap of the matrix approach to multiple linear regression, and if we focus on the X matrix here, we see that it contains a column for each of the, in this case, P minus 1 predictors as well as an additional column of ones, which will introduce the intercept into the model, for a total of P beta parameters, making this X matrix an N by P matrix. A new definition now, or just giving a name to a matrix that we've already used, we want to talk about what's called the hat matrix. So here we have essentially performing these squares to obtain the estimate of the beta vector. And then the following line this Y hat vector here is the vector of fitted values. We're going to give a name to this matrix operation here which itself is a matrix. We're going to call it the hat matrix. This is a matrix that's used to project onto the subspace spanned by the columns of X. So we're taking the data vector Y and we're using this matrix to project, we could call it a projection matrix, that is this matrix H here. But the more clever term for it is called a hat matrix because what does it do? Well, if you take the data vector Y and use this matrix it simply puts a hat on top of Y. In other words it obtains the fitted values for us. This hat matrix we'll see here is an N by N matrix, again, so X is an N by P matrix so X transposes a P by N matrix. We've seen before that this entire matrix here, X transpose X inverse, is a P by P matrix. So that does indeed verify then that H is an N by N matrix. The elements that we're going to be interested in are the diagonal elements of that matrix. In particular, we'll use little H sub I to denote those and those will be what we call the leverages. So H sub I is the leverage for observation ""I."" One more thing to note here, the trace of the hat matrix happens to be P. So if we sum up the diagonal elements of that matrix, it turns out that they sum to P. Something we'll want to do is identify observations that have a large leverage. So there's no strict rule for what it is that is a large leverage but a heuristic that's often used is two times the average leverage. So here we are using H bar to denote the average leverage, and again, so that would be summing up the leverages and dividing by N. But remember that the sum of the leverages happens to be P so the average leverage is always P divided by N. So we've defined leverage and we've given a heuristic for when we're going to say leverage is large, but why do we care? Looking at a slightly more concrete example, let's just think about simple linear regression for the moment, it turns out we could perform a bunch of matrix algebra and arrive at this expression here for the leverage of each observation in simple linear regression and it's actually a familiar expression if you remember the form of a confidence interval for the mean response. We see that this expression actually pops up right here. A couple of things to think about here is, one, is that this is a function of only the X values. It has nothing to do with the Y values. The other thing that it does is it gets larger the further away an observation is from the mean of X in this case. So essentially a leverage will show us extreme values in X so not extreme values in any sense of the relationship between X and Y, but it simply says these are the extreme values of the predictor. Here are three plots we're going to see a few times in this lesson. There's three points that are circled, one in each, and those are going to be points we talk about. This first one here, this is going to be a point of low leverage because the mean here is somewhat close to the X value for this point. So I don't recall the exact mean here, but the mean and the X value for that point are rather close to each other. So looking at plugging into this expression here, this quantity here is probably going to be fairly close to zero so that's going to result in a low leverage. This point here, however, is in that data set there, they're going to the point of highest leverage. Similarly here, this point here is also going to be a point of high leverage. When we actually look at this data in R, we can compare to the heuristic and see if we would label them as high leverage. But just looking at these data points here, these two here and here are the, sort of, most extreme data points in terms of X, the predictors, in those two data sets. We'll talk later about what the two different lines here are, but I can ignore them for now because we don't care about the Y values at all. Leverages only depend on the X values. So how does leverage work in a higher dimensional case? So let's say we're dealing with a multiple linear regression and let's say that we had two predictors, X 1 and X 2 so this is going to be a plot of only predictors so there's going to be no responses seen here. And let's say we have some data that looks something like this maybe... Let's draw some data points here. The leverage of each point is going to tell us essentially how far away that point is from the average predictor value. X 1 will be somewhere around, the mean will be somewhere around here. Sort of just eyeballing this, and the mean of X 2 we'll be somewhere, maybe around here. Points that are close to this point here will have low leverage. So this point here probably has fairly low leverage whereas this point out here will have potentially the highest leverage of this data that I've drawn here. Again the leverage simply tells us how extreme an observation is in a predictor sense. And so ignoring the Y value but it just says, ""Okay, this observation here is sort of far away from the average predictors, whereas this point here is close to the average of the predictors."""
stat-420,8,9,"[MUSIC] Now we'd like to talk aboutresiduals a little bit. So here, we have some summaryof the matrix approach to multiplying a regression again. Now here we have ournewly defined hat matrix. So again, the hat matrix takes the datavector y, and it'll put a hat on it. In other words,it gives us the fitted values. So here we have the data, andhere we have those fitted values. So e here is our vector of residuals. We know that the data isjust this y vector, so we can put an identifymatrix in front of that. And we said that the fitted values againare the hat matrix times the data matrix, and we have that here. And we can do a littlebit of algebra here, so you get this new expression forthe residuals. So this new expression will allow us todiscuss the distribution of the residuals, in particular, the expectedvalue of an individual residual, say ei residual is 0,that probably isn't too surprising. Then the variance ofeach individual residual. We're going to need the diagonalelement of this matrix here, which will be a one coming out of here. And what we define is h sub icoming out of the h matrix, otherwise known as the leverage. So here we have 1- h sub i, and thenvariance of y, which is sigma squared. We don't know what's in the squaredin practice, so eventually, we will want the standarderror of epsilon i, which is the square root ofthe estimated standard deviation. So that would be, let's see,1- h sub i times s sub e squared, and then the square root of all of that. So then we'll do the usual thing, andwe'll take each individual residual, subtract of its mean, 0. And then divide by the standard error, which I'll now write as s sub etimes the square root of 1- h sub i. And this will be what we defineto be as a standardized residual. And we'll call that r sub i. We can utilize these standardizedresiduals to identify outliers. We'll view an outlier as a data point that is far away from where we'd expectit to be for the model that we fit. We can use standardized residualsas a way to quantify that. So, how far away was that observation fromwhere it should have been according to our model. For a reasonable sample size, it turns out the standardized residualsare approximately standard normal. We can use this to identify observationsthat we wouldn't expect to see for the model we fit. For example, if the magnitude of a standardizedresidual is greater than 2, this is a standardized residual that we wouldonly expect to see roughly 5% of the time. Similarly, a standardized residualwith a magnitude greater than 3, we would expect to seeless than 1% of the time. This just gives us a way to standardizedresiduals and be able to quickly compare to these values some of the normaldistribution that we're familiar with, to identify points that we couldpotentially label as outliers. We won't give a specific cutoff forlabeling an observation as an outlier, but rather, use this as a tool togive us a general idea, then defer to common sense, and the context ofthe data that you are dealing with. Returning to the three plotswe had seen previously, now we want to talk about the size of theresiduals for each of the circle points. Because a residual doesdepend on the y data, I actually need to discuss whatthe two lines in each plot are. We'll actually, for now,ignore the blue solid line and and focus on the orange dashed line. And the orange dash line is the regressionfit to all 11 points in each plot. If we look at the first plot here, thisis sort of the size of the residual for that point there. In the second plot, the residual isbasically so small, I can't drive, it appears visually that, that pointis basically on the dashed orange line. And then in the third plot, the size ofthe residual looks something like that. It turns out in both cases,in the first and third plot, the standardized visualin each case will be larger than two. So those are rather large residuals, andwe'll see how to calculate that in R. The thing that we're going to talk aboutin the next video, specifically is, what's happening between the orangedashed line and the solid blue line? So the solid blue line is fit to allof the points in each plot except for the one that's circled. And we'll notice that in the first plot,the difference between the residual for the dash line andthe solid line is very small, but in the third plotthe difference is rather large. And we want to, in the next video,talk about why that is. [MUSIC]"
stat-420,8,10,"[MUSIC] We return to the three plotswe've been looking at previously, when discussing leverage and residuals the circle point in each plotbeing the point that we are interested in. So, in the first plot here the circlepoint had a low leverage but a large residual. In the second plot, the point hadhigh leverage but a small residual. And, in the third plot, the point hada high leverage and a large residual. Now we'll define a new quantity foreach observation that was used to fit a regression which is called,it's Cook's Distance. P here is again the number ofbeta parameters in the model. Which the way we've defined the mostpolinear regression model is one more than the number of predictors. r sub i is the standardized residual and h sub i is the leverageof the i observation. So what actually makesa point influential? If we think about this definition ofCook's distance, so we're going to say that an observation with a largeCook's distance is very influential. A Cook's distance is a function ofboth the standardized residual for an observation and its leverage. So a Cook's distance will be largewhen the magnitude of the standardized residual is large, so either a verynegative standardized residual or a large positive standardized residualwill result in a large Cook's distance. Similarly leveraged valuesrange between zero and one. For leverage values thatare very close to one, so high leverage, this quantity here,will be very large. Whereas for a leverage values very closeto zero, this quantity will be very small. Influential observations are observationsthat have the correct combination of a large standardized residual,and large leverage. For a given standardized residual,if the leverage is large enough, it'll be influential. Or conversely, for a particular leverage, there's some standardized residualthat'll make it influential. But generally you want sort ofa combination of a large standardized residual and a large leverage. One common characteristic forlabeling a point as influential is a point with a cook's distance largerthan 4 divided by the sample size. But again, that's just a heuristic. It's not a set in stone rule. Returning to these three plots, they essentially illustrate exactlythe idea of influential points. In the first panel, again, this pointdid have a very large residual, but because the leverage was so low,it doesn't have influence on the model. The opposite is true in the second panel,where this point has a higher leverage, but again, the residual is so small thatit doesn't really influence the model. So, the third panel is the exceptionhere and is a point of high influence. So, it's both a point with a very largeleverage and it also has a large residual so that combination is exactly whatresults in a large coexistence. So this is an influential point and we seethat that's exactly what's happening here. Again, the solid blue linewas fit without point, the dashed orange linewas fit with that point. And we see that those tworegressions are rather different. We also see here the need to consider theleverage when calculating Cook's distance. We see here that, again,the residuals calculated from this point to the regression that would fit using it,but we sort of see that the whole point of a influential point isthat it pulls the regression towards it. And sothe higher the leverage of that point is, the more extreme that occurrence is. And the more the regressionwill be pulled towards it. So that's why we consider both,not just the residual. But also the leverage of thatpoint when determining influence. We've been using simple linear regressionhere to illustrate this concept. We can make a nice plotlike the third panel here, where we see the fittedregression lines changing, but obviously this all applies tomultiple linear regression as well. You just have to considerthe effect of how all of the fitted regression coefficients changed andyou wouldn't get a nice visual representation ofthe influence of an individual point. [MUSIC]"
stat-420,8,11,"So now we have three ways to quantify the unusualness of an observation. This observation here is a large residual. This observation here has high leverage. This observation here is influential. So we've talked about leverage, residuals, and influence. But the question we need to ask yourself is why do we want to find these unusual observations. When we first looked at leverage and we had a heuristic to determine if a point was of high leverage. What if we find some points with high leverage, what do we do? The answer to that is nothing, because high leverage just tells us that there is a potential for an issue because we said the Cook's Distance is a function of both the residual and the leverage and if the leverage is very high in combination with a large residual, that will influence model, but by itself, a point with high leverage is not really an issue. What if we detect a large residual, more specific a large standardized residual? Well, those are points we could potentially deem outliers. What do we do with outliers? Hopefully nothing. Hopefully finding the Outliers by themselves is enlightening analysis. You identified those unique observations and if it doesn't have an influence on the model itself, say you're still getting good diagnostics, you're not violating assumptions, they're really not an issue. And by finding those points with large residuals you've simply found some interesting observations. Now, if we have a large Cook's Distance for certain points they may be deemed influential. Influential points we understand now can have a big effect on the resulting model. So we may want to consider doing something with these. How do we deal with unusual observations? One thing you could do is nothing, and that would be the best case scenario. So again this is a scenario where maybe there's just a couple of outliers. They don't have a big influence on the model. And so you simply have a couple of outliers and you should note that and those might be interesting observations. Another thing you might consider but with extreme caution is removal of say influential points or outliers, but this would be after inspecting them and determining that this is simply unrealistic data. Maybe it was an entry error or some sort of computer error, that data shouldn't even exist. By seeking out these unusual observations you may find some of these things, but in general we don't want to get rid of any data unless there's a very good reason to. In the middle here, I have the suggestion of do something but what are those some things. So, for example, one thing we'll see is when you have a lot of influential points, that will often result also in bad diagnostics, will sort of see violation that are anomaly assumption and maybe if the constant variance assumption. So, we'll see in the next lesson. One thing we could do there is perform transformations on either the response or some of the predictors. And in general we could consider different models. Maybe we need to consider some interaction terms, maybe there's other predictors that we're not considering that by entering the model it would capture sort of some of those deviations we're seeing from the current model and points would be less influential. Often when you have influential points, you might also have some diagnostic issues. So you'll want to consider refining your model. And in the next lesson we'll learn one way to do that."
stat-420,8,12,"[MUSIC] So now we can talk about detectingunusual observations in R. I've already run some code here,that is used to create these three plots. There's 11 data points in each, the 11th data point being the circled onein each, so they all share this data here, but then I have a point thatare these three points here. And then these three data sets correspondto the data set for each panel here. And then I've also fit a modelto each of these data sets here. So notice that the orange dash line isthe model fit to all 11 data points, just different in each plot. The blue solid line is a modelfit to the ten data points excluding the circled points,which is actually the same in each plot. The first thing we'll want todo is calculate leverages. We said that leverages or the diagonalelements of the hat matrix, so that's where this hatvalues function comes from. It's saying, I want the leverages for the observations used to fit,say, this model here. So this first line here will give usthe leverages of these data points here. So they're essentially in order,except for this 11th one. This is the 11th data point here, otherwise these are in orderfrom lowest to highest x values. So we can do the same thing forthe second panel here, and the same thing for the third panel here. These are essentially just numbers to us. They don't have too much meaning. We can compare, like say, thisleverage is higher than this leverage. But are any of theseconsidered large leverage? So to do that, we have this heuristic,where we consider anything larger than two times the averageleverage would be considered large. We can obtain that value foreach of these three examples, and we can compare the leverages to thatvalue that we use as a heuristic. And then here we see that, none ofthese points have a large leverage. Whereas in panels 2 and 3,we can see that both times the circled point indeed is considereda point of large leverage. Another way we sort of assessthe unusualness of an observation, where it's residuals. So, we've seen residuals before, those are easy to obtain foreach of the three data sets here. But the thing we introduced this weekwas the idea of a standardized residual. Instead of the residual function,we'll simply use the rstandard function. And then I'll get a standardized residualsfor each model, which we can do here. And other thing we'd like to do is, detect if any of these are largestandardized residuals. We said that standardized residualswere approximately standard normal. So for example, if we look at themagnitude of these standardized residuals, we would expect about 5% ofthem to be greater than 2. So we would expect thatto happen not too often. Here what I'm doing is, first I'm finding, which of thestandardized residuals are greater than 2? And then subsetting the factorto be those values. So here we see that this point here,the 11th point, we say has a large standardized residual. Large being somewhat arbitrary,we're defining for ourselves that greater than 2 is large. In the second panel here, we actually detect no points witha large standardized residual. And in the third panel, again, the 11th point, this one here hasa large standardized residual. One of the last metrics that welooked at was cook's distance. Cook's distance can be obtainedwith the cooks.distance function. So we can obtain them for the first,second, and third panel here. We had this heuristic that said,Cook's distance is considered large, and thus the point is considered influential. If it's larger than 4 divided by n,the number of observations. So here what I'm doing is,I am obtaining n, so there are, let's see, 11 Cook's distances, so, I'mtaking the length of that and that's 11. So this is, essentially,just 4 divided by 11. And over here,I'm subsetting to the 11th point, so I'm checking in each case isthe 11th point influential. We had said previously that this oneis not, because it's very low leverage. Again, we see the fit not changingmuch whether or not it's in the fit. Here again, it should not be because,while it is a point of high leverage, it has a low residual, and over here,we expect it to be influential. So let's see if thatmatches what we have here. So the first one, not influential,that's what we expected. Second one, not influential,that's what we expected. And the third one, is indeed influential. Interestingly, this point has suchan influence and pulls the regression so close to it. I believe if we look here,just checking each point for whether or not it is influential. Turns out this first point, so that pointhere, is also considered influential, but really it's this onethat causing the issues. It's pulling the regression soclose to it that it's pulling the regression away fromthis point, and it appears influential. Looking at some real data, again,going back to our additive model for the mtcars data set. So here, what I'm doing is, I'm obtainingthe leverages, and seeing how many of them are considered large, and we see thatthere are two points of large leverage. Here, I'm finding how many ofthe points have a somewhat arbitrary large standardized residual,which that is a magnitude greater than 2. And there is exactly one of those. Maybe that resulted in an influentialpoint, so what I'll do now is, I'm going to store the Cook's distances. So I'll calculate and store them. I'll check how many of thoseare bigger than 4 divided by n. So we believe that there are twoinfluential points here. What I'm going to do is,I'm going to store the indexes for those. So essentially, what I had overhere is a vector for true/false for which ones we need. I'm going to store that, and this line of code here will tellme exactly which ones it is. It's a Toyota and a Maserati. So these are two ratherdifferent vehicles here. Toyota Corolla beinga very economical car. Whereas a Maserati Bora beinga supercar from the 70s. So it's sort of interesting that thoseare the influential points here. Sort of the most very basic car,and one of the most extravagant cars in this data set are sortof having a large influence on the model. One thing we could do is,we could look at, okay, so here we have the coefficients of the model we fit sofar, just this additive model here. So what I'm going to do here is, I'm goingto remove the two influential points. We saw previously, that lookingat the diagnostics of this model, we didn't really see any cause forconcern, so we really don't need toeliminate any data points here. We're just going to do thisto illustrate a point. So I'm going to fit the modelexactly as I had before, but then subset according to this factor here, which is true forall of the points except for these two. So it'll fit to this data, butonly where this condition is true. So only observations where this is true. Now if you look at the coefficients,they haven't changed all that much. Those two influential points,while they did have some influence, it wasn't that big ofa difference between the two. One thing we should noteis this plot function here. So we've used plot fora number of different things. But if you call plot on a variable here,which is a model object, what I get here are quickly a number of diagnosticplots, which is actually sort of nice. So here we see exactly the fitted versusresiduals plot that we're used to. It adds the horizontal line and actuallyadds this nice smoother to the data. And we see that this smooth redline is not that far from 0, so that's good, we'd like that. We see a normal QQ plot and it labelsa few of the more extreme points. But this plot here isan interesting one as well. It plots leverage againststandardized residuals. And then what these curves hereare are contours of Cook's distance. Now we can see this Maseratithat we identified and this Corolla here are two ofthe larger Cook's distances, and those are points that welabeled as potentially influential. Moving on to the larger data setthat we've seen a number of times. If I load that, and we fit ourthree-way interaction model again, we had seen that,we weren't a big fan of this plot here, which I should clear this firstto make it the correct size. We weren't a big fan of this, because itlooks like there's some non-normality here, where you're sort of assumingthe normality assumption is suspect. What I'm going to do is, I'm going toobtain the Cook's distances here, and check how many of them wouldbe labelled influential, and it's actually a fairly large number. And recall that this would also theShapiro-Wilks test would reject, so again, that confirms what I'm seeing here. But so now what I'm going to do is, I'm going to remove those datapoints from the model fitting. So again,fitting the model the same way, but now subsetting two pointsthat aren't influential. Now if I repeat the QQ plot, andperform the Shapiro-Wilks test, the results are much better. I haven't actually justifiedremoving those data points, I removed those data points andeverything works out better. Well, we should actuallymaybe do is think about, are there other models wecould be fitting here? Maybe we could consider some additionalterms that we're not considering right now, or what we'll see in the next lesson, is maybe consider some transformationsof these current predictors. Because again, we would rather not removedata, unless we absolutely have to. So while it does fix a problem here, we haven't really justifiedthe removal of those data points. [MUSIC]"
stat-420,8,13,"While you might think that the name linearregression would imply that it can only model straight line linearrelationships, this is actually far from true. Through the use oftransformations, we'll see howa linear model is actually much more flexible and is easily able to model nonlinearrelationships and data. We'll first considertransformations of the response variable. These transformations allow for added flexibility in our models, and we'll see that theycan be used to correct potential assumptionviolations such as the constantvariance assumption. For transformingpredictor variables, we'll focus on the use ofadded polynomial terms. So instead of directlytransforming a single predictor, we'll add new predictors, which are transformationsof the original data. To utilize the added flexibilityof transformations in R, we'll introducesome new formula syntax. Instead of modifyingthe data ourselves, we'll learn how our modelingfunctions allow you to directly specifytransformations. As we did with interactions, we'll pay close attention to exactly what mathematical model the R formula syntax is creating to ensure our interpretationsare accurate."
stat-420,8,14,"Here we have some data on the fictitious company Initech. So for each employee we have their salary information as well as their years of experience and we'd like to understand the relationship between these two things. Generally, we'd expect that salary goes up as experience goes up. So one of the first things we'll try is to fit a simple linear regression model. So that's what was done here. This line is the fitted simple linear regression model and we see that it captures a trend that we expected it to as experience increases salary increases. No big surprise there. It seems to do a fairly good job for this data but we will want to run a few diagnostic checks to make sure that this is a good model. So we'll look at both a fitted versus residuals and a normal Q-Q plot. The normal Q-Q plot isn't too bad. The normality assumption doesn't seem to be too unreasonable here but the fitted versus residuals plot has some issues. The one that should jump out to us right away is that the variance for large fitted values is much larger than the variance for smaller fitted values. Here we believe that the constant variance assumption might be violated. We might also focus on lower fitted values here and notice that the mean of the residuals there doesn't seem to be zero. That actually seems to be a little bit higher. So that sort of makes us suspicious of the actual form of the model. We'll keep that in mind, but we will for now focus on the constant variance issue. The assumptions of the linear model suggests that we should have constant variance. That's what we've written here. So essentially the variance of Y at a particular X should be Sigma squared no matter the value of X that we have here. But what we've just witnessed is that the variance of Y for a particular value of X is different for different fitted values. That suggests that the variance of Y at a particular value of X is some function of the mean at a particular value of X. In the example that we just saw this h here was some increasing function. So this plot here suggests that as the mean increases, because again the fitted value here is an estimate of the mean, the variance increases. So in order to fix this increasing variance issue, we'd like to find what's called a variance stabilizing transformation. That will be some function g here which is applied to the response variable and when we do so, then when we take the variance it's no longer a function of X. It's some constant value. So no matter what X we're at, the variance of Y is the same. So hence variance stabilizing transformation. If we have strictly positive response data, one of the most common transformations is a log transformation. A quick note that when we say log, we by default mean natural log unless we say otherwise. The log function R will also be the natural log unless otherwise stated. What we're going to do now is we're going to essentially take the usual linear model we've been using and then apply a transformation to the response. So that of a model for the original data, it will be now a model for the log of the original data. Essentially this is still a linear model in the sense that the response is a linear combination of the predictors. It's just now that we have a new response which has been transformed. If we take that response and transform it back to the original data scale, we'll have what appears to be now a non-linear model. So the model on the log scale for Y, this would be an additive model in particular the errors are being added in an additive fashion, whereas back on the original scale then we'll have a multiplicative model where we see that the error term is entering the model in a multiplicative fashion. We could actually take this model and break it down a little further where we have the exponential of beta 0 times exponential of beta 1 times X times the exponential of the error term here. So we see that while on the log scale for Y we have an additive model, on the original data scale we have a multiplicative model. So here we return to the data we saw before but now instead of the salary as a response, we have log of salary as the response. So this line again is simple linear regression, but essentially all we did was replace the original data with the log of the original data, and we can see right away that this looks better than what we had seen before. Taking a quick look at exactly what was fit here, we can get the fitted values on a log scale with the beta 0 hat and beta 1 hat obtained which are 10.484 and 0.079, but often we'll want to think about this back on the original data scale. If we exponentiate everything, we can get fitted values on the original data scale. So think about this in a couple of ways. So when X is zero, that means someone has no experience. The fitted value on the original data scale would be e to the 10.484 which is roughly 35,000. So this means that we estimate that the average salary for someone starting at Initech is roughly $35,000. Now what happens when someone gains an extra year of experience? Well, now this is no longer an additive model which we had on the log scale, this is a multiplicative model. So the effect of going from say, 10 years of experience to 11 years experience, we're now multiplying by e to the 0.079 which is about 1.082. That means that for every year of experience you have, your salary is multiplied on average by 1.082. So that means roughly on average an Initech employee can expect to see 8.2% raise each year. So if we go back and plot this again but now back on the original data scale we see that this is no longer modeling a linear relationship. This is our first example of using a linear model for a non linear relationship. So again, it's linear in the log scale, but back on the original scale it is no longer linear and that's what we see here. Now you actually want to verify that doing this solved our initial problem of non-constant variance and we see pretty immediately that that appears to be the case. I don't see any cause for concern in this plot. Here we both see roughly constant variance and now the mean of the residuals is roughly zero for any fitted value and again the normal Q-Q plot checks out. So the log transform is just one example of a response transformation. There are many that we could try. Two that are very similar are taking the square root or taking the reciprocal. They're essentially weaker and stronger versions of the log transformation. So if a log transformation doesn't fix the constant variance enough, we still see increasing variance, might want to go for the reciprocal transformation. This last one that I have written here is what's called the Box-Cox transformation, named after statisticians Box and Cox, and we see here that is actually parameterised by this lambda here. So the Box-Cox transformation is actually a procedure that will try a number of transformations. So a different lambda suggests a different transformation. So essentially we use a method of maximum likelihood trying a number of different lambdas here. Once we find the best lambda that tells us the transformation we should apply and we'll note that a lambda of one half is somewhat related to the square root transformation and a lambda of negative one is related to the reciprocal transformation and as is sort of hard-coded in the Box-Cox transformation self, a lambda of zero is the log transformation that we had just seen. We'll leave the details of this for reading, but it's something that you can try if you want to consider a large number of transformations automatically and all at once."
stat-420,8,15,"So here we have some data that was simulated in a way that we had seen before when we were checking diagnostics. So this is data simulated from a quadratic model. In particular this model here the errors, as always, are normal and the variance here was 25. So the line here is an attempt to fit simple linear regression to this data, and we can already see that it's not doing a great job. We can verify that by looking at some diagnostic plots. The fitted versus residuals plot shows a clear pattern, pattern also happens to be a quadratic, but in particular we're overestimating here and here and we're underestimating in the middle. So this is a bad fitted versus residuals plot and the normal Q-Q plot doesn't look so great either. So clearly here there is something wrong with this model. The way we'll fix this is with a predictor transformation. In particular, we'll use a polynomial transformation. We'll start by looking at a quadratic transformation. And I put transformation in quotes because we're both transforming a predictor but also adding a predictor to the model. We've talked before about model hierarchy and that will apply here as well. So we're going to use what we call a second order term in this model, the quadratic transformation of the X variable, but also the first order term as well. This will be another example of fitting a linear model that can be used to explain a non-linear relationship. This model that we have written here is... The signal portion is still a linear combination of the predictors, it's just that one of the predictors is a non-linear transformation of one of the original predictors. Fitting this model is done the exact same way we fit any other linear regression model. We have our X matrix here, it's just that we have this new column here which stores this new predictor we have here, and then we go about using these squares as we had always done before. So we will obtain estimates for β_0, β_1, and β_2. Doing that for the data that we had just seen, we obtain a new fitted model. So here the fitted values, β_0 worked out to be roughly 3.1, plus β_1 was roughly 0.5, and β_2 was about 5.1x². Here we see that the fitted model is fairly close to the true data generating model here. And right away, without even looking at the diagnostic plots, we see that the form of this model appears to be much better than the simple linear regression that we had tried before. Looking at the diagnostic plots. I don't see any issues. The normal Q-Q plot looks pretty good fit versus literal pot also looks pretty good. You might notice there are sort of just a lot more data over here but that's not a problem that's just due to the fact that there are a good number of fitted values that are sort of closer to zero just the way this data is. That's not an actual issue in the plot. We don't see any patterns in the plot and we don't see different variants at different fitted values. There's no reason that we have to limit ourselves to quadratic terms. We could move on to higher order terms very easily. So,for example, if we wanted to add a cubic term to the model, all we have to do is add a new column to our X matrix with the transformation of the original data and we can go higher and higher. And so we can fit any arbitrary polynomial that we choose. We would just need to add more and more columns to our X matrix. With this new power though, we need to be careful. Here we have some data that was simulated from a quadratic model. So the blue solid line here is fitting a quadratic polynomial to this data. But the orange line is fitting a polynomial of degree 10. There happens to be 11 data points here. And so if you have a polynomial of degree 1 less than the number of data points provided they're all at different X values, you can get a perfect fit. So here, the residual sum of squares for the polynomial of degree 10, it'll be zero. This is an example of what we would call overfitting. So while the error here is exactly zero, if we observe any new data, we would not necessarily expect it to conform to the fitted polynomial of degree 10. So for example a new data point right here would be not out of the ordinary or maybe one right here. And those aren't too far away from the fit the quadratic model but they're rather far away, especially this one, from the degree 10 polynomial. One thing they all seem to be aware of too is with these increasingly higher order polynomials, extrapolation becomes a huge issue. So obviously we don't like extrapolation in general, but extrapolation is wilder and wilder as models get more and more complex. So we see this sort of wild behavior over here, and over here, compared to the relatively normal behavior of the quadratic when it's extrapolating. So we don't like extrapolation, it's just made much much much worse by higher degree polynomials. So here we have a much less extreme example. So here the data was simulated from a degree 4 polynomial, and then we fit two models to it: a degree 2 polynomial, and a degree 4 polynomial. The solid blue line here is the polynomial of degree 2 fit to this data, and the orange dashed line is the degree 4 polynomial that we fit to this data. Now at first glance it might not be super obvious which of these two is better. We know the answer because I said we simulated this data from a degree 4 a polynomial, but we'd like to be able to determine which of these two models we prefer. So the first thing we can do is look at the diagnostics from these two models. So here we have the diagnostics when the degree is 2. The normal Q-Q plot looks okay, but we see a pattern in the fitted versus residuals plot. So if we look at where, sort of, the mean of the residuals is for any given fitted value, we see a pattern that maybe looks something like this. So that suggests that this model is not specified correctly. If we instead look at the diagnostics for a degree 4 polynomial being fit to this data, we see that these diagnostics look much better. The normal Q-Q plot is still pretty good and the fitted versus residuals plot looks much better. We see no obvious pattern in this plot like we did in the previous one. Maybe instead we wanted to consider a degree 4 polynomial and a degree 6 polynomial. We could go through the same procedure, but we'll find that the degree 6 polynomial also has no issue with the diagnostics. So we want another way to compare these two. And one thing we know already is that the degree six polynomial will have a smaller residual sum of squares. But is it a significant difference? So one thing we might notice right away is this degree of 4 polynomial is nested inside of the degree 6 polynomial. The difference between these two models boils down to these parameters right here. So we could perform a hypothesis test... So we can say β5 is equal to β6 is equal to zero. And this is a simple Anova F-test that we've done a number of times before, and this would end up telling us for this data that we just saw that there's not a significant difference and we would actually prefer the smaller model of degree 4 and we'll take a look at that when we jump into R. Sort of similarly too, if we just consider the degree of 6 polynomial, and we want to maybe say test for the necessity of the degree 6 term, we could do a test for exactly that. And that would be the usual T-test that we had seen in the past. So there's a few things to be aware of when using specifically polynomial predictions or transformations. So we talked about extrapolation, especially with higher order polynomials, we don't like to use extrapolation in general, but it's made worse by higher order polynomials. The interpretation of these models now is a little bit more difficult, especially when we start incorporating polynomial transformations inside of multiple linear regression models. So in this video we were only considering transformations of a single predictor, but there's nothing to stop us from adding more predictors, and maybe polynomial terms for those predictors, and some interactions as well. That's something we'll talk about next week, when we start talking about model building in general. But just be aware of anytime we make the model more complicated, we're losing out on some of the very easy interpretation, and this is also true in particular for higher order polynomials. So often we might want to only consider, say, quadratic terms to sort of have flexibility, but not hurt the interpretation so much. We briefly mentioned hierarchy. That's to say that if we're going to consider a third order term, we would put the second order term in the model as well as the first order term. This is something we have seen previously with interaction terms and we'll discuss again next week in general when we talk about model building. And then lastly, there's nothing to restrict ourselves to polynomial transformations. They're just a nice easy one to talk about and are very commonly used. But, for example, like we saw with the response, we could replace any of the predictors say with a log transform or predictor. And oftentimes that could also be a useful transformation."
stat-420,8,16,"So now, we'll discuss the use of transformations in R. To do so, we'll go through this RMarkdown document here. The common themehere will be that, while we aretransforming the data, we won't actuallydirectly modify any data, but we'll specify thetransformations through the use of the formula syntax infitting models with lm. First, we'll check out transformingthe response variable. To do that, we'll load somedata that I have stored here. This is the data we discussed in the lesson so plotting it, recall it looks like this. We saw this generalincreasing trend, but we'll see ina moment that we had uncovered a non-constantvariance issue. First, we'll just fit a regular old simple linear regression, and as we said, it appears that the regressionis significant, but if upon further inspection, we start to see it here, the non-constant variance issue. But if we were to plota fitted versus residuals plot, this becomes very clear, so we see thisobvious pattern here. One thing we might notice here. So I'm attempting toplot two plots at once, but they get smasheda little bit. So one thing we can dowith our markdown is, change the figure size here. So I'm going to makethe width a little bigger and make it about twicethe height will say. So now, if I rerun this, now the patches looka little bit nicer, they're more the aspect ratio we would be like to see here. Again, the fitted verseresiduals very clearly shows the non-constantvariance issue here. Here's the model thatwe like to fit now. Instead of the original response, we're going to logtransform the response. Again, we're not goingto modify the data, we're still going to passthe original DataFrame to the lm function, we're simply going tospecify a log transform of the response inthe formula syntax we've been using before,we'll go ahead and do that. So now, we'll plotthe data but now the y-axis here will be onthe log scale and we see that, this simple linearregression with the new response looks much better and we canverify that with a fitted versus residualsplot which we can do here, again this looks much better. The other thing we could dois, if we wanted to we could transform this back tothe original data scale, so we'll first plot the original dataagain and then we'll add what is now acurve to this plot. So here I am gettingBeta zero hat, here I'm getting Betaone hat multiply it by x. X here is just the x-axis, the values on the x-axis,that's what curve does. It applies a curve which isa function of x to this plot, and here I need toexponentiate as we see here. If I run that, we see that, this is the model then thatwas fit to the original data. We might want to beyond, just the fact thatthe diagnostics are better, look at which modelfits the data better. So we can look at the rootmean squared errors. So first, the simplelinear regression fit to the original data and then the simplelinear regression fit to the log response, and this would appear that, Oh, look this root meansquared error is so much lower thanthe original data, but this is essentiallyjust due to the fact that we are fitting now ona completely different scale. So what we should do to makea legitimate comparison is compare to the original dataand then find fitted values, but when we do so forthe log transformed data, it transform them back tothe original data scale. When we do that, we still see the same thing which is thatthe log model fits better, but now it'sa legitimate comparison. This is happening again becausethere's actually a bit of a curve relationship here which the log transform also adds. So in addition to fixing the non-constantvariance issue, again, this fitted versusresiduals plot suggest that the mean is incorrect here, so the log transformalso took care of that. So we'll also discusstransforming predictors, as an example, we'llfirst simulate some data. We've actually seen thisdata simulation before I just gave ita new function name, so we're simulatingsome data that has a quadratic relationship now, so I'll create that data. The first thing we'll do is, attempt to fit the usual simple linear regressionto that data. So this data here which we should just lookat it real quick. So it has a response y and a predictor x and so we'll attempt to fitsimple linear regression. Again, it appears significant if we just look atthis summary output, but if we look atit all visually, we immediately realized, well, in this scatterplot here, this fitted regression linehas some issues which are made very clear bya fitted versus residuals plot. The mean of the residuals is all over the place fordifferent fitted values. So there's clear pattern in these residuals andthe normal plot looks pretty bad as well. What we'd like to do is, fit a linear modelwhere this mean now is a linear combination ofthe original predictor, but now also thistransformed predictor based on the original one, in particular,the transformation we are performing here isa quadratic function. So not only we transformedthe original variable, we've also addeda new variable to the model. So to do that in R, we're going to takeour formula syntax and use this i-function here. We'll come back and talk about exactly what's going on here, but this syntax here essentially specifies that we're fittinga model with this term here. If we do that, we'llsee right away that, that term is significant, so we already knewwe should use it, but this verifies that. Using the curve function again, we can plot the fittedregression now to the data, and this just the middlelooks so much better and the diagnostic plots willtell a very similar story. We see no obvious pattern inthe fitted versus residuals, the Q-Q plot looks pretty good. Now, what I want to do isfit some much bigger models. The first example we'll do is, we are going to simulatesome data where the mean function hereis again quadratic. So we have this x squared going on here andthen the usual noise, so let's see here. I'll simulate that data and then I'm going tofit two models to it. One model will havethe correct form. It has the first-order term here, but also importantlythe quadratic term, and the second model is going to be a polynomial of degree 10, so a very complicatedpolynomial here. So now, what I'm going to dois plot the data here and then add those twofit to the data. So a few things here. This previous curve syntax that we were using is going to be a bit of a bother here because, here we only had to dealwith three coefficients, but in this model, we're going to needto deal with 11. So instead of doing that,what I'm going to do is, I'm going to create a sequenceof x values at which we're going to make predictionsfrom these two models. So for example, let'sactually go ahead and plot part of this for now. So we see that, we'regoing to need to make predictions at x valuesfrom around 0-10, I've just gone beyondthat just to be safe. Then what we'll do is say, with the correct modelthis one here, we'll make predictionsat each of those points. Then essentially,because we've made so many of them and they'rerather close together, when we make line to join them, it'll be the smooth curvethat we expect. So if I run this now, we see here thequadratic being added, and here's that crazy tenthorder polynomial being added. We said that, this plotillustrated two issues. One is thatthe orange dashed line is very clearly over fittingand then we said that, we need to be very carefulabout extrapolations. We see wild behavior over here with that verycomplicated polynomial model. Here, the choice between these two modelsare very obvious, we would prefer fitting the quadratic modelhere without a doubt. But in general, deciding between two polynomial modelswill not be this easy, so we need some better tools. So now I'm going to simulate some data froma model where there is a quadratic term anda fourth order term, but we're going to consider three differentmodels for this data. One that has up toa quadratic term, one that has up toa fourth order term, and one that has up toa sixth order term. So I'll create that data, I will plot that data. If we didn't know that this was simulated witha fourth order term in it, we might not be too certainwhat's going on here. It might actuallylook like a quadratic could fit appropriately, might not be clear thatthere's this uptake here in the mean and it's actually happeningover here as well. So what I'm going to dois, I'm going to fit both those models and now I'm going to put themin a different way. So instead of usingthe I-function and that syntax, I'm just going to simply say, I want to havea polynomial based on this x variable in this dataset and I wantit to be of degree two, so this will put an intercept, a first-order term, anda second-order term. Here, I'm going tocreate a degree four polynomial so it'llhave a first-order, second-order, third-order,and fourth-order term. I'm being a little bit incorrect there and I'll explain later. But for now, we'lljust assume that this fits the quadratic model we want and this fits the fourth-ordermodel that we want. Then what I'll do is,I'll attempt to add those two fittedmodels to this plot here using this strategy of creating a sequenceof x-variables and then making predictions, and I'm going to runinto an error here. This is a fairly commonR Markdown error where, by default this linesfunction attempts to add the lines toalready-existing plot, but because thealready-existing plot was from a different chunk, it gets confused anddoesn't know what to do. So what I actually need to do is recreate this plot code here. A good rule of thumb is, one chunk for one plot, so you always have to have a plot be completelycontained in one chunk, and for that reason too, it's probably oftena good idea to just have a different chunkfor each different plot. Here, we see the tworegressions fit and again that fourth order model has this bit ofan uptick here and we'd like to determine which ofthese two models we prefer. So here, I'm going to form the diagnostics forthe quadratic mile, the solid blue one here. This is actually enough to suggest that we needa different model. We see this obvious pattern in the fitted versusresiduals plot here. It's a normal plot that'sactually pretty good, but there is a patternin the residuals, so we might need amore complicated model, that more complicated model that we'll try is a fourth order, so the degree four polynomial. So here are the diagnosticsfor that model, and here now, we don't see any obvious pattern in the fitted versusresiduals plot, so we're much more comfortable with the degree four polynomial. We could also do a test here. So if we take a look atthose two models again, this quadratic model isreally just a mile that's nested inside of this degreefour polynomial, so we could performan ANOVA F-test for the null hypothesis is that Beta three and Beta forR equal to zero, and that's exactly whatthis simple ANOVA call will do. We see a very small P value, so we rejectthat null hypothesis, we would prefer a larger degreefour polynomial. We can perform a similarargument with a degree six polynomial or maybeI shouldn't stop there, why not build larger? So now compare degreefour to degree six, and I believe here now, we'll see a very high p-value, so we would not rejectthe null hypothesis, we prefer the degreefour polynomial there. When I fit thesemodels, I've been using this poly function, which as you'd guessfits polynomials, but I still haven'tsaid what it is. The model we were tryingto fit is this model here, so we want a first-order, second-order, third-order,and fourth order term. So I'm going to fitthe model three ways. The last will be using the I-notation which I alsosuddenly didn't explain. So this is going to fitexactly this model. I'm going to usethis poly function and I'm going to specify that raw is true. I'm also going to label this for its argument called degree, and then the firstone, I'm going to not specify raw is true. So now what I want to do islook at the coefficients of these three things, and at this point, you should maybe expect something might be a little unusual here. So when I attemptto look at them, this is extremely annoying output because each ofthese is a vector, but they all havevery different names and some of the namesare very large, so it's hard to seewhat's going on here. So I'm going to throwthe unnamed function in front of all of theseto remove that, and this looks much better for the comparison I'mtrying to make here. It's actuallyremoving this output, so we can see up a bit. So here is the modelthat was fit here, here is the modelthat was fit here, and we see that theyare exactly the same. So these two aredoing the same thing and they are fittingexactly this model here. This one though, we seedifferent coefficients here, so it is not fitting exactly this model here so we need to do a little bitof investigation there. While there aredifferent coefficients here, if we look atthe fitted values of this model and the fittedvalues of this model, we actually see thatthey're the same and we get the same thingfor the residuals. So while the fittedcoefficients are different, it's actually making exactlythe same predictions. Why is that? So what we're goingto look at here is the summary of this using poly, and degree four, andnot specifying raw, versus the summary forusing the I-notation. So the first thingto notice is that, again, we saw thatthe estimates are different. Almost everything else isdifferent as well with one exception which is that, this p-value here for testing whether ornot this Beta four is equal to zero is exactly the same as this p-value here, that's one of the few thingsthat are the same here. I think will also notice that the F-test statistic is the same as isthe R-squared value. What's happening here is that, when we fit the model this way, we are fittingexactly this model here. When we're using thepolynomial function like this, these are not these terms here. So what this actually is, this is a polynomial of degree one that is orthogonalto the intercept. This is a polynomialof degree two, so this has both afirst-order term and a second order term in it, and then we're fitting a coefficient out infront of all of that. This polynomial is orthogonalto this polynomial, and this polynomialdegree is zero, and so on and so forth. This polynomial andthis polynomial both have some third-order term in it, it's just thatthis polynomial is the only one witha fourth-order term. That's why we see that, the p-value fortesting here is the same as testing for the p-valuehere because again, this is the onlypolynomial of these here that has a fourth-order terminate and that'sthe same here. In the end, these two milesmake the same predictions, but the coefficients havevery different meanings. But for the sake of testing, we can essentially view these models prettysimilarly because again, the significance of theregression will be the same and testing for the highest degree term willalso be the same. That's pretty much okay because, as we said before, we generally considera model hierarchy here. So if we're going to havethe fourth order term, we should have the third, second, and first order term as well. While we can perform tests about these coefficients, wegenerally wouldn't. We probably wouldn't take out the second-order termhere if we're going to have thefourth-order term at it. So while we do see a numberof different results scattered throughoutthe lower-order terms because those are going tobe in the model anyway, because we prefer to have models that adhere to hierarchy, we're okay with the factthat just this last line of each summary output isresulting in the same p-value. So they're bothessentially testing, do we need some fourth-orderterm in the model? We also need to talkabout why we need to be using this I-function here. To explain this, we'llgo back down to fitting data to this data that was generated viaa quadratic model, and I'll runthese two lines here. We see that, one of themdoes add the quadratic term, but this one doesn't. This one does not add a quadratic term even though I am taking x and raising itto the second power. So what's the difference here? If you think back toadding interactions, this was a way that we said we could specify interactions. So if we had a bunch of variables and we group them and raise thema certain power, it would put interactionsof that power, whatever that number is andeverything lower like this is specifying formula syntax. What the I-function does, i for inhibit, its saying, okay we're no longerusing formula operations, we're now performingarithmetic operations in here. So this is literallysaying, okay, we're looking atx squared in here because it's inside ofthis inhibit function. Whereas here we're saying, okay, I want a two-way interactionbetween x and x. That's a little silly obviously, but that's what'shappening because there is no two-way interactionbetween x and x, it just ignores it and puts nothing intothe model for here, so we just havethe first-order term here. Whereas here, we're essentiallycreating a new variable inside of the I-functionwhich is the quadratic term. These are all goofy, but they're illustrating the point again, so this is addingan interaction between x and x which will do nothing. This will say, okay, I want the interactionbetween x and x and I'll lower-order terms. This is saying the same thing, I want all two-wayinteractions between x and x and everything below it, and this is whatwe've seen before. So all of these willessentially just fit the usual simple linearregression model because these terms don'treally mean anything. Similarly here, this modelbecause we're saying, I want x and x in the model, we can't have the same variablein the model twice, so it'll just drop it. Whereas here, I'mon the fly creating a new variable which is essentially adding x toitself thereby doubling x, so when I do this, we'll see that one coefficient istwice the other. One last thing is we'll actuallylook at some real data, so the usual autompg dataset here. So one thing we mightask ourselves is well, how do we even knowthat we should consider some ofthese polynomial models? Well, this pairs functionwill be somewhat useful. So what it'll do is, it'll plot scatter plots between all thepossible variables. A fuel efficiency is our response here. Takea look across here. So with here, we seea nice linear trend here, but let's say somethinglike horsepower, we see definitelya curved relationship here, so we would want to consider maybe a polynomial model here. We can make similar arguments on the weight and displacement. We'll take a lookat simply modeling fuel efficiency as a functionof horsepower, initially. So I'm going to fit the modeland then plot the data, add the fitted model, and then look at a fitted versus residuals plot and that'sgoing to be a procedure. We repeat a couple times here. So first, I'm just going tofit simple linear regression. We see an obvious issue. So here's addingthe fitted line to the original scatter plot and here is the fittedversus residuals plot. So we see two issues here. One is that, the meanis simply incorrect, for certain fitted values, it's not centered at zero, and there's this obviousincreasing variance. So we're going to attempt tofix the mean issue first, so by adding a quadratic term and then we'll repeatthe same process. So I'll fit the model,plot fitted, and look at fitted versusresiduals. Looking better. So this curve fits the data much betterthan a straight line, but we still see the increasing variance issue in the fitted versusresiduals plot. Maybe, we'll also consider a response transformation then. So I'm also going tolog the response and have the quadratic termin the model as well. If we take a look at that, this looks much better. So this might not be the best fitted versusresiduals plot, but is doing muchbetter than before. Here, we seethis curve is fitting the now log response very well. Also consider instead of this quadratictransformation here, we could have insteadconsidered, well, let's just take a logtransformation of the predictor, and that actually turns out to work fairly well here as well, but we actually see a remarkablysimilar fitted versus residuals plot to having had used the quadratic transform. This gets us towardsthe idea of model-building, so we're consideringmultiple transformations and seeing what happens and checking the diagnostics and iterating. We'll do that one more time.So we had previously fit this three-wayinteraction model, but we had noted that thenormal Q-Q plot was not great. Last time, we hadtried to fix this by removing some influentialobservations, and we saw that thereis an improvement, but we didn't reallylike the fact that we're throwing out data. What I've done now is, I've added a few quadratic termsand log the response. So now I fit a much larger model, and looking atthe Q-Q plot there, things are much better. It's not perfect, but we'vemade a big improvement. So this is getting us toward this idea of building models. The focus of next weekwill be, well, first, what variables do weactually need in the model, and then, we'll also consider how to decide onsome transformations as well. Just generallybuilding models and thinking about the iterativeprocess of checking the assumptions andalso seeing how well the model fits and making comparisonsbetween models. So we'll talk about a lot of the different toolswe can use there, and transformations will playa role in that for sure."
stat-420,9,1,"In this lesson, we'll introduce the conceptof collinearity, which occurs whenpredictor variables are highly correlatedwith one another. We'll see how to use Rto detect collinearity. More importantly, we will discuss what effects this has onour regression models. In particular, this willmotivate our next lesson in which we will discusswhich variables to include inour regression model. For now, we'll introducesome tools to assess the effect of adding anadditional predictor to a model. These will show how itis often unnecessary to include two highlycorrelated predictors."
stat-420,9,2,"[MUSIC] So here we see some data, and we havea response variable y and five predictors, x1 through x5. We actually see here,it's called a paras plot, and what it does is it creates a scatter plotfor every combination of two variables. This first plot here hasthe response y on the y axis and the predictor x1 on the x axis. We can very quickly see the relationshipbetween the response y and each of the predictorsin this first row here. And we sort of see that there's somelinear trend in each of these plots. Any one of these predictors by itself isprobably at least somewhat useful for predicting y. We want to look at a model thatincorporates more than just one of these predictors. First thing we'll try is utilizingall available predictors. So we'll say that that data was storedin a dataframe called some_data. And we'll fit a model with the responsey and all available predictors. So we'll notice two things maybe rightaway with this summary information here. One is that the p-value for the significance of the regressiontest is extremely small, so we think that at least some combinationof these predictors is useful. Interestingly, none of the individualt tests have a small p-value and this sort of feels at odds with itself. So the regression itself is significant,but none of the predictors individually are significant withall these predictors in the model. So we sort of want to understandwhat's going on here. We'll first contrastthis to a smaller model, where we only have x1 andx2 as predictors. And again,here we see an extremely small p-value for the significance of regression test,but now additionally, with just these two predictors in the model, both oftheir individual t tests are significant. So the question we ask ourselves is,why is this happening? So if we look at the data again, one thingwe'll notice is that if we look at x1 and x2, there does not seem to be anyrelationship between these two predictors. This looks like basically noise. But if we look at some of the otherpredictors, for example, x5, and maybe let's compare it with x3,we see a very obvious trend in this data. So we would say x3 andx5 are correlated predictors. Similarly, if we look at x4, it seems tohave a relationship with both x2 and x1. We have correlation amongst ourpredictors, we have a term for that which is called collinearity. You'll also hear this referredto as multiple collinearity. So what I have here is a matrixof all two A correlations, so for example, any variable withitself has a correlation of 1.00. So x1 and x2 have a very smallcorrelation, whereas, for example, x3 and x5 have a very large correlation. And then we said that x4 iscorrelated with both x1 and x2. So we seem to havea collinearity issue here. Since we actually simulated this data,we can show exactly why this is occurring. So notice that x1, x2, and x3 are all simulated completelyindependently from each other. Notice that x4 and x5 are related to,in the case of x4, x1 and x2, but plus some noise,x5 is related to x3, plus some noise. This is why x4 is correlated with x1 andx2 and why x5 is correlated with x3. And then here, we see the true modelactually only requires x1, x2, and x3, so y is simply a function of x1,x2, and x3. That's not important at this exact moment. But this is just to say that these twolines here indicate why we have this collinearity problem. In practice, we wouldn't obviously knowhow the data's generated, but here, it's nice to see sort ofwhy that's happening. The more interesting thingto us is what effect does collinearity have on our modelling andhow do we quantify it. We'll use what's calleda variance inflation factor to quantify the effects ofcollinearity on our models. We've often talked about the variabilityof our parameter estimates. Here, we see the variance of the estimateof a particular beta parameter. And we have our usual expression forthat variance. But now we're going to rewrite it a bit. We'll leave out the details mathematicallyof how these are equivalent. And in particular,we need to define R sub j squared. Importantly, this 1 / 1- R sub j squared will be what's calledthe variance inflation factor. It's called the variance inflation factorbecause it shows us how much the variance is inflated because of the otherpredictors in the model. So let's explain that a little bit. Here we have the model we wereconsidering with five predictors. So let's say we want toconsider the variance of, let's look at x4 here, so beta4 hat. So with this new expression, it is now sigma squared times (1/ 1-R sub 4 squared) times 1 / S sub x4, x4. So the question is,what is this R sub 4 squared? That is the R squared of predicting x4using the other available predictors. So in other words, it's the proportionof variation in x4 that's explained by a linear relationshipwith the other predictors. Let's now, instead of thismultiple linear regression model, let's consider a simple linear regressionmodel with only x4 as a predictor. So I'll use beta 4 and x4 again. We've seen before that trying toestimate this beta 4 with beta 4 hat, we would obtain a variance, which issigma squared times 1/ S sub x4, x4. So we can see here the differencebetween the variance in this situation, the simple linear regression case, andthe variance in the multiple linear regression case differs by exactlythe variance inflation factor. The variance inflation factor istelling us how much the variance grows by adding these additionalpredictors into the model. So for example, discussing the full modelhere, using all possible predictors, we see this quantity here wouldbe exactly R sub 4 squared. So again, what we're doingis we're regressing x4 onto all available predictors. So here, I'm removing y because thisis not a predictor, it's the response. So we're regressing x4 against x1,x2, x3, and x5. And we see that a large proportion ofthe variation in x4 is explained by those other predictors. So that means that the variance of beta4 hat is going to be extremely large. Sort of the opposite istrue in this model here. So here, in the model with just x1 andx2, regressing x2 onto x1, the other variable predictor, this wewould call, in this case, R sub 2 squared. This is a very, very, very small number. So x1 does a very poorjob of explaining x2. So here,the variants of beta 2 hat will not be much bigger due tothe addition of x1 in the model. So then looking at the actualvariance inflation factor, so again, we're going to look at thisfull model and this smaller model. There are many packages in Rthat contain a vif function, which will calculate the varianceinflation factors for you. Here I'm using one from the car package. As predicted, the variance inflationfactor for x4 is extremely large. Again, let's do the fact that the otherpredictors do a good job explaining x4. Whereas in the smaller model,see the variance inflation factor for x2 is extremely small. That's because x1 does a verypoor job explaining x2. We see sort of a widerange of numbers here. So the smallest varianceinflation factor can be is 1. So these two are extremely small. What a large variance inflation factor is,somewhat subjective. These, everyone wouldagree that 8,000 is huge. One of the usual heuristics you'll hearis a variance inflation factor greater than 5 suggests that multi-collinearityis having a large effect on your model. Others will use 10, butthese aren't strict rules, they're more or less a heuristic. But here,we would say that the full model, there's definite cause for concern here. Collinearity is an issue, whereas in the smaller model,it seems to be there's no issue at all. The difference here is that forexample x4, we saw that it was well explainedby the other predictors. In particular, it has an obviouslinear relationship with x1 and x2. Whereas in the smaller model, x1 andx2 more or less seem uncorrelated, so there wasn't any multi-collinearity issue. While the smaller model doesn'thave a collinearity issue, when we compare it to the fullmodel using an anova f test, we actually see that we would, in the end, prefer the full model,due to this small p-value here. So while the small model doesn't have thecollinearity issues that the full model has, we still prefer the full model. So something will do the rest of thisbecause maybe we're trying to find a model between thesetwo that is significant, but does not have the collinearityissues of the full model. We've talked about collinearity. We could talk about exact collinearity. So instead of just high correlation,an actual correlation of 1. Previously, we said that x1 andx2 were correlated with x4 and the data we've seen. But what would happen if, say, x4 is maybe some linearcombination of another predictor? Say, it's a +, let's say, bx1. The problem now is that whenwe go to create our x matrix, the column for x4 is just a linearcombination of the column for x1. And when we go to try to do this matrixinversion to find our estimates, we won't be able to do this becausethis is no longer invertible. That's an issue that presents when youhave a predictor that is an exact linear combination of some other predictors. One place you might run into this iswhen using categorical variables, if you accidentally includetoo any dummy variables, you'll run into exactly this situation. I'm saying that analytically,we can't perform this matrix inversion. What we've previously seen in R, what youwould see in R if you tried to do this again, is R will simply drop predictorsin order to actually fit the model. But know that this is the reason for that, because we can't have a predictor that'sa linear combination of other predictors. That would result in exact collinearity. We can sort of summarizethe effect of collinearity. It has a sort of largenegative effect on estimation. We said when we have collinearity,the variances or estimates go way up. So our estimates are much less stable. They become extremely variable,so much so that we might even see signs of estimates changing withjust minor changes to the data. And we'll see this when welook at data examples in R. We also see that it hasan effect on inference. Because the variances have gone up due tocollinearity, it's going to be much harder to reject any of the, say, individualt tests about a single predictor. So these two together sort of saythat collinearity has a effect on using a model to explain a relationshipbetween the predictors and the response. Where collinearity sort of has, what I'm going to say isno effect is on prediction. So while collinearity certainly doesn'thelp prediction, we'll see that collinearity in data won't have a bigeffect on using the model to predict. So while our estimates are going tobe highly variable, in the end, the model will still be useful formaking predictions. What can we do to deal with collinearity? One option is to simply do nothing. Having collinearity in the data doesn'tactually violate any of the assumptions of the model. It just presents some difficultlyin using the model for explanation. So long as you're aware of that, technically collinearity is nota reason to simply throw out a model. One thing you could do is nothing, and just be aware of the effectthat it's happening. And this is probably the easiest solution. But often, one of the better solutionsis to remove some predictors. So if two predictorsare highly correlated, maybe you only need one ofthem to be used in the model. This raises some questions though, forexample, which predictors do we remove? How do we decide? And that's exactly something thatwe'll look at in the next lesson. One trap that people fall into sometimesis just looking at variance inflation factors andremoving large variance inflation factors. That's actually not a goodway to go about doing this. Another thing you could considerto remedy some collinearity issues is modifying predictors. So occasionally, we'll actuallyintroduce collinearity, for example, when adding polynomial terms,or potentially, interactions. One immediate fix for that is to modifythe predictors in particular ways. For example, you could look into meancentering the data, or similarly, standardizing the data. Or in the case of polynomials, instead of the raw polynomials, you couldlook at using the orthogonal polynomials. You sort of have to consider trade offs,though, between modifying these predictors and how much that actuallyaffects using these models for explaining andbeing able to interpret the model. Versus having the collinearity inthe model and how much that affects using the model to explain and interpretingthe model with the collinearity in it. [MUSIC]"
stat-420,9,3,"So returning to the status that we had been looking at, we again are looking for a model for y based on these five predictors and we introduced this idea of collinearity because we see some rather correlated predictors for example x3 and x5. We were considering two models. One model that used all the predictors and one model that only used two predictors and we said that the full model had a collinearity issue as indicated by these large variance inflation factors whereas a small model had no such issues. However, despite the fact that the small model had no collinearity issue, we still had a preference for the full model. We can think of this as the small model is not predicting nearly as well as the full model. It's only advantage is it doesn't have this collinearity issue. How can we improve upon this small model to potentially have it compete with this full model and still not have the culinary issues? So what we're going to need to look into is adding another predictor. Maybe we want to ask yourself which of these three predictors should we add. One thing we can look at here is something called the partial correlation coefficient. Rather than defining it mathematically which is actually somewhat cumbersome we'll actually use R to define it based on residuals and correlations. Again remember this small model was regressing y against x1 and x2. So now I've introduced this new model, I'm calling it x3 model which as we can see here regresses x3 against x1 plus x2. In both cases the residuals are essentially what's left over from using x1 and x2 to explain the response. In this case, what we have is x3 that is not explained by relationship with x1 and x2. And what we have here is the y that is not explained by a relationship between x1 and x2. So what we can think of this is as x3 with the effect of x1 and x2 removed and this is y with the effect of x1 and x2 removed and then we look at the correlation between those two. If it's large, so either close to negative one or one, that suggests that the variable we are considering in this case, x3, is a potentially useful predictor. Or if the correlation is small, close to zero, that would be a not so useful predictor. We can look at this for the three variables remaining: x3, x4 and x5. So we see that x3 and x5 have rather high correlations with y and again that is the part of x3 that is not explained by x1 and x2; is highly correlated with the part of y that is not explained by x1 and x2 and so on and so forth. So, x3 and x5 are predictors we could maybe consider adding to the model whereas x4 has a much smaller correlation here. And we can sort of see that again here. We see that the relationship between x3 and x1 and x2 doesn't seem to be much of one whereas x4 does seem to have a linear relationship with x1 and x2. Similarly, for x5 there seems to be no obvious relationship with x1 and x2. We could have also done this somewhat visually instead of partial correlation. We could look at what's called a variable added plot. On the y axis we have the residuals from the original model. So in this case we have the residuals from this model here. We're regressing y against x1 plus x2 and at the x axis we have the residuals from regressing x3 onto x1 plus X2. Our y axis is y with the effects of x1 and x2 removed and our x axis is x3 with the effects of x1 and x2 removed. If we see a relationship here, that would suggest that x3 is a potentially useful predictor. And here we do see somewhat of a linear relationship between those residuals of y versus the residuals of x3. If we do the same thing with x4 instead of x3 we can obviously fit a line here but it's much less steep than a line for x3 suggesting that x4 is not as good of a predictor to be added to this model. Let's go ahead and add x3 to the model and I'm going to optimistically call this a good model and we see a number of things that we like. The regression overall is significant. We actually see small P values for each of the T-tests. So with the other predictors in the model we believe that each individual predictor is significant and looking at the variance inflation factors they're all very low, actually very close to one so we don't have any collinearity issues with this model. Furthermore, when we compare it to the full model we obtain a large P-value so we are then okay sticking with the smaller model. So now this model essentially predicts nearly as well as the full model and the difference between them is no longer significant. If we try to look at another partial correlation coefficient. So this time now we'll consider adding x5 to this model that already exists. So now we're looking at the residuals of, so here we're regressing x5 onto, let's see, x1 plus x2 plus x3. And here we're regressing y onto those same predictors. Now there's a much smaller partial correlation coefficient. So now adding x5 to this model maybe isn't such a good idea and that actually you should make a lot of sense because while x5 was not related to x1 and x2, so that's why earlier we saw maybe a potential variable's add but once we added x3, x5 and x3 are highly correlated. So now it seems it's no longer necessary to add x5 to the model. It seems that we only need one of either x3 or x5 to have a decent model here."
stat-420,9,4,"So now we'll look at collinearity in some real data. That data will come from the faraway package. In particular, this seat position data set. First, we'll look at it visually, a good thing to do with a data set, and here we see a pairs spot for this data. Before we even talk about this, you should probably ask yourself what is this data, in particular what are all these variables? So, we'll take a quick look at the documentation here. Note that I have this chunk set to not evaluate. So I can evaluate it interactively, but if I knit this document, this won't run because loading this documentation would be sort of something we don't want to do. And re-knitting a document would spawn a web browser in the background at best or at worst some situation. So it actually breaks the knitting. This is data about essentially where a car seat is for different drivers. So people adjust the location of their seat. And we want to understand where they place that seat based on their physical characteristics. So our response variable here is going to be this hip center variable, which is a measure of hip's to some location of the car. So essentially this variable gives us an idea of where the car seat is. So where the driver has positioned their seat for their comfort. And then the predictor variables will all be characteristics of the driver themselves. For example, weight and height. So then actually looking at the data, we see a number of the predictive variables have sort of an obvious linear trend with the response depending on your leg size here. Essentially, I think the longer your leg is we see this decreasing trend of this hip center variable. But we also see some very obvious collinearity For example, a driver's height and their height with shoes on is extremely correlated and that makes a lot of sense. We can look at those numerical correlations. When we round them to two decimal places. It actually turns out that the correlation between height shoes and height appears to be one. It's not actually one, but it's very close. It's a very large correlation. So the first thing we'll do is try to fit a model using all possible predictors. So our usual full model using all possible predictors using hip centers the response we fit this model, and we see a similar results of what we did when we were looking at some simulated data I had created. So we see a p-value for this significant regression that is very small suggesting that this is a significant regression. But if we look at the p-values for each of the individual predictors, none of them appear to be significant with all these other variables in the model. We already knew that there was a collinearity issue in this data set just looking at this plot here. But then the fact that the progression is significant, but none of the individual pictures are, again, are suggesting this to us. We want to look at the variance inflation factors for this data. So one thing we'll do is first calculate this R sub j squared, in particular for the height shoes variable. What I'm going to do here is I'm going to fit a model with height shoes as the response. All of the available predictors as predictors, and I'm removing hip center because that is the response. So we don't want to consider it as predictor. And then if I fit that model and look at its R-squared, it's extremely high. So that's saying that this height shoes variable is well explained by the other available predictors, especially in particular, height. We can go a step further and calculate variance inflation factors for each of the predictors. And so with this alone, we would predict that the variance inflation factor for height shoes is going to be extremely high. If we do that, again, I'm using the vif function from the car package. We see some very large variance inflation factors, in particular for height shoes, 307, 333 for height. The others aren't so alarming. The one for seated here is a little bit high. We suggest that five might be a cut off so because we see a number of variance inflation factors that are larger than five here. That sort of suggests that there is a collinearity issue with this data set. But sort of what effect is that having? What I'm going to do here is I'm going to generate some random noise and add that to the response and see what effect that has on our estimated coefficients of our model. So I'll fit that. These will be the coefficients of the original model. These will be the coefficients of fitting the model to the response with some added noise. In particular, this noise had a mean of zero. So we would hope that this doesn't have much of an effect on the estimation of the beta parameters, but what we'll see is it actually has a fairly large effect. One thing we might notice right away is that the estimate of the parameter for height actually flip signs. And that's sort of troubling because that's suggesting a completely different effect when you change height, how that relates to that hip center variable. We see things like the effect of the seated variable being quadrupled. And in generally we just see a lot of change in these estimated coefficients. When we have multicollinearity, what we've been saying and what the variance inflation factors tells us is that the variability in our estimates is very high. And that's exactly what we're seeing here. If we look at the fitted values for the original model and a fitted value for the model where I had arbitrarily added noise, I'm going to plot them against each other, and I'll even add a line with a slope of one. We see that the fitted values are actually not that much different between these two. And that's why we say that collinearity doesn't have a huge effect on prediction, but it does on estimation of the model parameters. Now, what I'm going to do is I'm going to fit a smaller model only using three of the available predictors. We see here again that regression is significant. And now we even see that, for example, the height variable is significant even with the other variables in the model. And if I look at the variance inflation factors for this model, now that I'm not using the car package, actually turns out this vif function is also in the faraway package, so I think I can run exactly this. Yeah, I can. So let's just to say that the variance inflation factor function, it exists in a number of different packages. But here, we see no large variance inflation factors. We don't believe that there is a collinearity issue in this model here. So now if I were to sort of repeat one of those analysis I did where, again, I'm going to add some noise to fitting the same model here. So just again adding some noise to the response, but only with these three predictors. Now, we notice much less change in our estimated coefficients. We see some difference obviously, we were adding noise, so we would expect some change. But the change isn't large, and here we don't have anything crazy, like signs flipping on us and completely reversing effects. So we see here without the collinearity issue, our estimation is much more stable, like adding noise doesn't affect it that much. And it turns out that if we compare those two models, the smaller model is sufficient in this case. Even though in this case the smaller model is sufficient, we'll again try to add some variables to this and see if they would in any way be useful. So the variable we're going to attempt to add is the height shoes variable. And we should -- I already sort of have an idea that because height is in the model, height shoes probably won't be all that useful. We can just verify that. So the first thing we'll will look at is the partial correlation coefficient. So, again, this was the original small model that we had fit. But so now instead of the response variable, I'm going to put the predictor variable that I want to attempt to add into here and fit the similar model. And then if we look at the residuals of those two models, that will be the partial correlation coefficient for height shoes with the effect of these three variables removed. So we're looking at the correlation between hip center and height shoes with the effective age, arm, and height removed. And we see that it's a very small correlation. It's very close to zero. So this suggests that adding height shoes would probably not be very helpful. And, similarly, we can look at a variable added plot. So I'll simply plot the residuals against each other and I'll add a regression line for regressing the residuals of this model onto the residuals of this model. And we'll see a very flat regression line here. So essentially the height shoes variable is not very useful when we're already considering the effects of age, arm, and height. Since I did a markdown document here, I should go ahead and knit it maybe. I had noted that we wanted to set this particular chunk to be not evaluated so we won't get any sort of weird issues with trying to pull documentation while we're knitting. We see that there's no additional output here and everything else was introduced into our markdown document."
stat-420,9,5,"So now we'll perform a quick simulation today to understand the effect of correlated predictors. We're going to simulate from this multiple regression model. Here I have specified the values of all the parameters so that's β0, β1, β2 and σ squared. The two situations we're going to look at is first when x1 and x2 are highly correlated and then second when x1 and x2 are not correlated. We'll go ahead and set up the simulation by specifying the values of all the parameters. For the sake of this simulation study, we'll use sample size of 10 and we'll use 2,500 simulations. First creating some predictors that happen to be correlated. That's what I've done here. We'll see that they are indeed highly correlated. And I've looked at, there are two standard deviations here and we'll come back and look at those later. But for now there are two highly correlated predictors. So doing a little set up before actually performing the simulations. So here is the true mean for the values of the predictors so we'll store that and we'll use the other sides of the simulations and then setting up some variables for storage. So here is a variable that will store β1 and β2 hat. Here is a variable that will store the mean squared error each time. So here we're going to be interested in the distribution of β1 and β2 hat. Here we're going to be interested in the mean squared error, sort of as an assessment of how well the model is predicting or how well it's fitting. Then we'll actually run the simulations, we'll add noise about the true meaning each time to simulate the y values, we'll fit a model. Note that we're being a little lazy here and not utilizing a data frame anywhere and simply storing all our variables in our global environment. Then I will store β1 and β2 hat. So essentially I'm leaving out the β0 hat each time so the first index will correspond to β1 hat and the second of course will correspond to β2 hat and I'm also quickly calculate the mean squared error each time and storing that. So again if I ran this chunk, so I'll run it again, I had not run it. Now I will actually run the simulations. So I have all that information stored and I'm using _bad each time because this is bad because we have correlated predictors. So now instead of x1 and x2, I'll replace them by z1 and z2. These are similar in that they have the same standard deviations as before but now I have a very, very, very small correlation. So we'll consider these more or less uncorrelated predictors. We are repeating the same process now except with z1 and z2 as our new x1 and x2. So I'll set that up again. Again just setting up some variables for storage and then running the simulations. And then so the interesting thing will be to look at the results. The first thing I'm going to plot here is the empirical distribution of β1 hat. So in the left here we have the situation we were simulating with correlated predictors. And on the right here we were simulating without correlated predictors. And we noticed that both of these roughly appear to be normally distributed. Because again β1 hat will indeed follow in a normal distribution. The difference being that the variants, if we take a look at the x axis here, the range here is much larger than what we see in the uncorrelated plot over here. So we have to verify though that the mean both times is very close to three, which if we scroll back is the true value of β1. So indeed in both cases we are still performing on bias estimation but if we look at the standard deviations we'll see that it is indeed much larger in the correlated case which is exactly what we said happens when you have correlate predictors, variances of your estimates go up. We'll see the same thing repeated with β2 hat. Once again they both appear normal again but simply the variance in the correlated case is much higher. The means are very close to four which I believe was indeed the truth. So again still performing unbiased estimation it's just that variances are increased. So the standard deviation and the correlated case is much higher. Maybe a little more interesting then looking at the mean squared error difference. It turns out these look remarkably similar. So the errors we're making are not that different in both cases. It turns out that on average they are actually extremely similar. So that's to say that while collinearity, that is correlated predictors had a large effect on estimating β2 with β2 hat, it does not have a large effect on simply making predictions. Our fitted values were similarly far off in both the correlated and uncorrelated case."
stat-420,9,6,"[MUSIC] All models are wrong, but some are useful. This is a frequently cited quotefrom the statistician, George Box. Its precise meaning is often debated. But we can all agree that we wouldlike our models to be useful. So how do we create these useful models. We have a number of ways ofintroducing flexibility to our models, including choice of predictors,interaction terms and transformations. We also have a number of tools todetect possible issues with our models, such as deviations from assumptions andcollinearity. We'll first focus on selection ofpredictor variables from multiple linear regression model. We'll introduce newmetrics such as AIC and BIC, which measure the quality of a model. Unlike metrics which onlyconsider the size of errors made, such as the residual sum of squares,AIC and BIC account for the trade off between the complexityof a model and how well it fits. When using these metrics, we'll see howwe can quickly prepare a large number of models that use different predictors. This will allow us to find a good set ofpredictors from those that are available. We'll also motivate model selection asa way to guard against overfitting, while introducing cross validation. Putting everything together, we'll discussgeneral model building strategies. Most importantly, we'll discuss how thegoal of a model can drive the decisions we make during the model building process. After this lesson, you'll have allthe tools necessary to create useful linear models in all sorts of situations [MUSIC]"
stat-420,9,7,"[MUSIC] So let's consider some data. We'll have the usual response variable y. And let's just say we have ten predictors,x1 through x10. And for now,let's consider no interaction terms and no transformations of eitherthe response or the predictors. Which predictors we decideto use in our model will be the only way we can change the model. So our selection of predictors willessentially be our choice of model. How do we go about choosing a model? Well let's see, one potential modelwe can consider is simply modeling the response as a function of let's say,x2. Now maybe we can consider another model, that models the response as a function,of let's say x2, maybe x3 and x5. Now we've seen ways tocompare theses two models. For example,we could consider an ANOVA F-test. What if we throw in a third model? Say something like this, maybe x1 + x4 + x6 + x10. The previous two models were nested, so we could do something likeI don't know an F test. But these two models are not nested. How could we possibly compare them? One thing you might think is well let'sconsider their residual sum of squares. Or somewhat similarly,we can consider their R-squared value or maybe their root mean squared error. Using any of these three to compare modelswill essentially result in the same model but also they all sharethe same common flaw. The residual sum of squares is essentiallythe basis of both R-squared and root mean squared error. Recall that we said R-squared isone minus the sum of squares error divided by the sum of squares total. Sum of squares error just beinganother way to write the residual sum of squares andthe root mean squared error is exactly as the name says it'sthe root-mean-squared-error. So we'll take a square rootof the average squared error, the squared error being exactlythe residual sum of squares. So if we pick the model with the saylargest R-squared, that would be the same as picking the model withthe lowest root mean squared error, which is the same as picking the modelwith the lowest residual sum of squares. And this seems like a good thing todo because these are the models that have the the smallest error. But there's some issues with this, in particular, if we do this,we'll always pick, in this case, the model using all possible predictors,if we were considering that. But sort of in general if we do this,we'll pick larger models. Any time we add a prediction to the model,the residual sum of squares cannot go up. So essentially,all three of these metrics, here, would be favorableto larger models always. But in general, we have a fairly strong preference forquote on quote small models. But why do we like small models? For one, they're much easier to interpret. So, the smaller the model is, the fewerfactors that are influencing a response, these are just mucheasier to make sense of. Another thing we saw, is that they areless likely to have collinearity issues. If we always use all available predictors,if any of them are correlated, and this is going to causesome collinearity issues. In particular varianceswill become larger and it will be harder to rejectsingle predictor test. And then one of the big reasons wedon't want to make the model too large is because we don't wantthe models to overfit. We saw an example of this we'relooking at polynomial regression, if we just fit a polynomial with a highenough degree, we'll basically get a perfect fit but we know that's notgoing to generalize to more data. We'll talk about this explicitly later but for now we're just going to saythat we want to guard against this from happening by somewhat limitingthe size and complexity of our models. So, we'll need something that's not based exactly on residual sumof squares like R-squared. So this is where we'll look into metricscalled AIC, BIC and adjusted R-squared. We'll essentially be looking for what we'll say is a small model that fitswell or in other words a large model that fits without Issues likeoverfitting or collinearity. The first metric we'lllook at is called AIC. I won't even make an attempt to pronouncehis name because I know I'll get it wrong. Interestingly, when he first introducedthe idea he called it simply, an information criterion, but over the years the use ofA switched from an to his name. AIC is based on essentially two things, which are the maximizing the likelihoodof the model and the model complexity. So if we Insert the maximumlikelihood estimates for all of our parameters,we end up with the maximized likelihood. Here I have what works out to bethe log of the maximized likelihood. We'll skip some derivation of this, but in particular we do note that it isa function of the residual sum of squares. AIC is defined to be -2 times thismaximized likelihood plus, 2 times p. Now, before even talking aboutwhat that tells us about a model, we're going to be using this to comparedifferent models fit to the same date. One thing we'll notice is that -2 timethe log of the maximized likelihood has this term here that willbe the same for our models. Often we will simply ignore this termhere and only use this term here instead. So, it won't be exactly the definitionof AIC that we have here, but it'll give us the exact samecomparison between models. So the definition of AIC that we'llactually use in practice is exactly this. First, note that that the residual sumof squares essentially tells us how big the errors of the model are. And then p is as alwaysthe number of beta parameters, so we can think of this as a measureof the complexity of the model. Looking at this first term here, it is essentially smallwhen the errors are small. In general,we're going to want AIC to be small. And we can think of the first part asa measure of how well the model fits. You can think of it as goodness it fitsand it fits well when this term is small. This term here we couldthink of as a penalty. So essentially as the model grows,we're adding a penalty term to that model. So while the fit is going to get better,the penalty will get larger. So this essentially creates a trade-off,as a model grows, the errors it makes will be smaller butthe complexity will be higher. And we're essentially penalizing thatby adding this additional 2 times p. So when comparing two modelsfit to the same data, we want to select the modelwith these smaller AIC. Here we see a number ofdifferent models with increasing number of parametersfit to the same dataset. First we see a decrease in AIC for a whileas the number of parameters increases, the rate at which it starts fitting betteris out pacing the increase in complexity. But then the rest of the way here we seethat the penalty term is not overcoming the decrease in errorsthat we're making there. So we would select thismodel here with a p of 4. BIC or the Bayesian Information Criterionis a very similar metric to AIC. The big difference here is goingto be in our penalty term. So it'll use the same- 2 timesthe maximized log likelihood term, but now we have a new penalty term. So if we compare these two,essentially the only difference is exactly the coefficient here infront of p the number of betas. So BIC will have a largerpenalty anytime that log of the sample size is greater than 2. So anytime I believe n is eight or more. So in general,BIC will have a larger penalty, BIC if we used it instead of AIC,we'll select smaller amounts. Another metric we could look at,something called adjusted R-Squared. R-Squared we said will simply growas the residual sum of squares otherwise know as the sumof squares error goes down. So, R-Squared is not good forselecting a model, because it'll essentially alwayschoose the biggest model possible. Adjusted R-squared doesnot have that property. So instead of simply dividing sum ofsquares error by sum of squares total like we were here, we're now dividingthem by n- p and n- 1 respectively. So adjusted R-squared can nowpotentially go down as the number of parameters of the model goes up,unlike R-squared. So it could be used like AIC orBIC, and like R-squared, you would want to selectthe larger adjusted R-squared. But now it doesn't suffer from this trapof always selecting the largest model. [MUSIC]"
stat-420,9,8,"[MUSIC] So now, we have a number of way to comparemodels that don't rely on an F-test or simply picking the biggest modelvia residual sum of squares. So the question we have toask ourselves now is well, which model should we compare? So again, returning to the scenariowhere we have the response y and these ten predictors, x1 through x10. And again, we'll say, we're not going toconsider any transformations or interactions or anything like that. So the choice of a subset of predictorshere is essentially choosing the model that we're fitting and we want to say, well, how do we decide ona subset of predictors? So we're going to utilize those metricslike AIC, BIC and adjusted R squared. But one of the questions, again, that we need to ask is whichmodels do we even want to compare? And the simplest answer to thisquestion is just try them all. Again, recall that each ofthose metrics in one way or another is dependent onresidual sum of squares. So if we keep the complexityconsistent say, fix the value of p, the number of beta parameters. The model with the lowest residualsum of squares in that sense is actually the best. To consider all possible subsets ofpredictors here, we'll first want to find the model with the best residualsum of squares for each possible size. So, the model with one predictor thathas the best residual sum of squares. The model with two predictors that has thebest residual sum of squares and so on, and so forth. Then once we found all those, calculateour metric of choice for each of those. Be it AIC, BIC or just R squared andthen using the AIC, BIC or just R squared pick the best model. I put best in quotes here, because thatwill be the model with say, the best AIC. Whether or not it's the best modelis a much more subjective question, which we'll return to later. The issue with tryingall possible subsets is that's potentially a lotof different models. If the largest model we're going toconsider has at most p-1 predictor variables, that means the number ofsubsets possible ranging from using 0 predictors to all p-1 predictors andeverything in between is 2 to the p-1. So in the situation we've beentalking about when p was 11, that is that we had p-1 is 10 predictors. So for example, there were ten tozero is one model with no predictors. So, just the intercept model. There were ten to one, which is tenmodels with one possible predictor, sort of obviously. And then going on,maybe say, ten choose two. So there are 45 possible models with2 predictors and so on, and so forth. So altogether, there are 2 to the 10 potentialmodels there which I believe is 1024. So depending on the sample size andhow fast you computer is, that may or may not be a daunting taskto fit all those models. With a reasonable sized dataset and moderncomputers, that's probably not a big deal. You can see how as the number ofpredictors grows this is going to get extremely big, extremely fast. We may not want to have tocheck every possible model. So instead of checking everypotential subset of predictors, what we'll do is check a sequenceof potentially good models. So to do that, we'll use what'scalled various search procedures. So we'll start with some initial model, usually it's either a model withno predictors or all predictors. And then attempt to either add or removea single predictor one at a time and hopes that it improves our metric of choice say,AIC, BIC or adjusting our squared. So in forward selection,we would start with a small model, usually the model with just an intercept. And then one by one, attempt to add the predictors thatare currently out of the model. So in the first step, we'll attemptto add any of the predictors. So, whichever one improves the AIC themost is what we will bring into the model. And then at the next step,we'll attempt to add everything. But that one we just included and look forthe one that improves the AIC the most until adding any of the pressureswould result in no improvement, and that would be the model we would pick. We could also go backwards. So, we would start with a large model. So, something like using allof the possible predictors and then we would do the exact opposite. We would attempt one by one to removeincluded predictors in hopes of improving our metric or we could usewhat's called stepwise procedure. In other words, a bidirectional search. So at each step, we will both attemptto add excluded predictors or remove included predictors. Again, one by one until thereis no more improvement and that will be the model effect. So obviously here, we won't check all possible modelswith a series of good models. So it's possible that we missedthe best model that is say, the best model with via AIC. But it should be a reasonably good model. And so, we'll highlight the details ofthese procedures when we jump into R and we'll also see if using theseprocedures will miss the best model or actually hit on the same best modelthat the exhaustive search does. [MUSIC]"
stat-420,9,9,"[MUSIC] So now I’m going to go ahead andtake a look at using AIC, BIC, and adjusted R2 with some real datain order to compare the models. The data we'll use is the C positiondata set from the faraway package. So reload that and take a look atit just to refresh our memory, as we have seen this before. So we're going to want to modelthis hipcenter variable here, as a function of the remaining variables. We had seen previously, and seeing itagain now, these predicted variables are extremely correlated inparticular height and height shoes. And we saw that introduced aswhat we call collinearity. We'll go ahead and fit them all usingall the available predictors, and here we see the fitted coefficients here. Remember we said that thesewere extremely large variants, inflation factors whichsuggest some collinearity. And we saw how by arbitrarily pickinga smaller model, it fixed a collinearity issue but we didn't necessarilyreally justify those models. So now we're actually going to search forthem in a more intelligent way. So we want to find a smaller model. It still fits well andit probably won't have collinearity issue. The first thing we'll do istry an exhaustive search. In other words best subset selection. So to do sowe will need to use the LEAP's package. In particular the regressionsubsets function. We're going to essentially lookat all possible subsets of all available predictors. And I'm going to store the summaryof calling that function in some variable here. And we notice this actually happens prettyquickly despite the fact that we're fitting all possible models, becausethis data isn't particularly large. So first we'll just take a lookat what was stored there. We're using eight possiblepredictor variables. We're not forcing any of them to bekept in model or out of the model. We could add additionalargument here to do so, and this information here is what we're after. We're actually going to look at it in aslightly different using this dollar sign, which shows us essentially whichvariables are included in the models of the various sizes. So, essentially what this function did isit found the model of each possible size. So, in this case, number of predictorswith the lowest digital sum of squares. So, for example, the model that uses onepredictor, so it would have two parameters because it always has an intercept,uses the variable height. Similarly, the best model interms of residuals on the squares uses two predictors,uses height and this leg variable. And then for example obviously,the best model uses eight predictors, uses all possible predictors. We'd like to know the residual sum ofsquares for each of these models, and that is information that is stored here. We know that the largest model is actuallygoing to have the smallest residual sum of squares, which we actually do see. So we'd prefer to use somethinglike adjusted R-squared, AIC, or BIC to actually make a selection here. A number of things are additionallystored in this object here. One of them is actuallythe adjusted R-quared for each of these eight different models here. So for example, this is the adjusted R-squared for themodel that uses height as predictor, and this is the adjusted R-squared for themodel that uses all possible predictors. We want a large R-squared, sowe'll try to find the maximum of these. It happens to be this third one here, but we can programaticallyfind that using which.max. And we do see it as the third,so it's this model here. And we see that we couldactually extract that. So, this is essentially is storingthe index now, which is three. And we see that again age, height andleg are included in that model. We'd also like to look at AIC and BIC. So to do so we'll actually utilizeeffect of these residual sum of squares are stored for each of thesebest possible models of each size. And remember that AIC is a functionof the residuals on the squares, the sample size n and p, the numberof beta perimeters in the model. So I'm going to store both p andn for the largest possible models. So n will be the same for all models, butthis P here will be the P for the largest possible model, that is, the modelthat us uses all possible predictors. And then what I will do is I willessentially calculate the AIC for each. So essentially we're quicklyapplying this formula because operations are now vectorized. This will give me the log of the residualssum of squares divided by n for each of the models stored here. Then we're going to add two timesthe number of data parameters. So the smallest model has two dataparameters, if you recall that's the smallest here, andthe largest will have what's stored in p, which was the number of coefficientsin the largest possible model. So we'll run that real fast. This now stores the AICs forthese eight models. because again if we keep the complexityconstant the model with the lowest residual sum of squares isgoing to have the best AIC. want to know which of these isthe smallest in this case, so I'll use which.min to extract n xthere which happens to be 3 again. And I believe this isactually the same model. It uses the age variable,the height variable, and the leg variable. And I'm going to go ahead and fit thismodel directly and store it for use later. So I actually go ahead andplot the AICs as a function of the number of parameters orcomplexity as we can think of it. After a certain point as the complexityincreases, the AIC just keeps going up. So at first, AIC went down due tothe reduction in errors that we're making, so residual sum of squaresis always going down. But at some point afternumber of parameters are 4, the panel a term is taking over andincreasing over time. This is generally what wesort of expect to see. It won't always be this niceof a depth in an increase, but we are generally searching fora model like this. We say that here our model with fourparameters is a good trade off between size and fit. We could do the same thing with BIC,all we have to do is modify the penalty. Now its log n instead of two, so we just change our code a little bit,to put a log in right here. So now again, BIC we want the lowest. So if we abstract the minimum and then abstract from the whichthat we had stored there. We see that we're takingthe model with one predictor and that predictor happens to be height. And again, I'm going to fit this model andstore it for later usage. That was using an exhaustive search,so again by using regression subsets we checked every subset possible insideof using our possible predictors. And again, it was notcomputationally prohibitive here but it could be with a big enough data set andenough predictors. We'll also try out searchingthrough possible models and not actually trying them all. So the first thing we'll tryis a backwards searching using AIC as our metric. So that's actually what the stepfunction does by default. I didn't actually need to specifythe direction, I believe. So by default,we're going to go backwards. So what we're going to do is we'regoing to start with this model here, which again was a model thatused all possible predictors and then attempt to remove predictors oneat a time and hopes of improving AIC. I'm going to run this andit's going to create a lot of output. And we'll talk about what this means. The first thing it does,it tells me the model I'm starting with, which is the model that Igave the function step. And it tells me it's AIC. In this block of output here, we notethat this AIC is mirrored right here. So this is essentially saying no change. Every other line, which starts with a minus sign becausewe're trying to remove variables. So, this line, for example, is what happens if we removeheight from this model. So we see that it has a different AIC,and note that this model here, which is the same as up here,has the largest number of predictors and it has the lowest residual sum of squares,but it does not have the smallest AIC. Of the possible models considered here, the model removing heighthas the smallest AIC. We see that removingother variables like say arm has a lower AIC than doing nothing,but it's not the lowest. So the first move we'll make isto remove the height variable. So that brings us to this step here. Now we're considering a model thatdoes not have the height variable, and again this is its AIC,which we had seen here. So now we're considering removingvariables from this model and this time if we remove weight,we get the biggest improvement in AIC. So we go ahead and do that. And we continue with this process until,in this step here we removed the thigh variable from this model, so, we are leftwith age, height, shoes and leg. And at this point, if we try toremove any of these variables, the AIC goes the wrong direction,it goes up. So instead, we stick with that model,we make no change. So that is the model resulting from usinga backwards search and AIC as our metric. I'm going to actually go back here andnote that. So this was the AIC for our initial model,which was the largest possible model. So I just want to verify wherethis value of AIC comes from. We'll that note that there'sthis function extract AIC in R. And it will actually return both P,the size of the model, so 9. So it's the number of beta parameters andthe AIC. So 283.624 mashes up here, 283.62. We can also verify the formulathat we had been using. So extract n and p andapply the usual formula and we see that these two AICs match. So ours is by default using the AIC thatignores the additive constant because we're fitting these differentmodels to the same data. And we can quickly take a look at thecoefficients of what´s stored here, again, I guess I should scroll back here. So when I ran this,this is some intermediate output but what´s actually stored here is the modelthat is selected in this final step here, which I´m storing in this variable here. So when we look at the coefficients here, we get the coefficients ofthat model that was selected. So what´s stored here isactually a model object of type LM that has the model that we found. We can actually also go backwardswith BICs as our metric and the only thing that changes,we still use the step function. We still give it an initial model. In this case we're going tostart with a big model. The model that had all possible variables. We're going to continue to go backwards. We're going to change this k. So k by default is 2 and that'll be AIC. So this is the coefficient infront of the penalty term. So this is what we're multiplying by p. So instead of 2, we want it to be a log ofthe sample size, which I had stored here. If I run this, It will look very familiar. Again, we have these minus signs forpotential removals. We'll note that R calling this AIC, but because we modified this k parameter,it is actually BIC. And then the other thing willhappen if we scroll down here, we notice in the end we endup with a much smaller model, in particular a model with just height,shoes. And this is because they havea reasonably large sample size here and reasonably large happens to only be 38,but that's large enough that the penalty term is bigger sowe end up with a smaller model. So instead of going backwards,we can instead go forward. What we'll here is we'll createa new model that we start with and that model will have no predictors,just the intercept. And sowe're going to use the step function. Give it the model I want to start with. Now I'm going to define a scope. So this is where we'regoing to search with n. So essentially this is the largestpossible model that I'll consider. So we're going to add variablesstarting with nothing and end at it most this large andwe want to go forward. We can do this and what's different now,well instead of minus signs, we have plus signs. because instead of subtracting variables, we are starting with this model here withnothing and attempting to add variables. So in the first step, we add height shoesand in the second step, we add leg and so on and so forth. And we can do the same thing with BIC. We won't talk about it, butwe'll run it real fast, because we'll look at it again later. Similar to going forward,we could also perform a stepwise search. So instead of going only backwards oronly forwards, at each step it'll consider bothremoving or adding a predictor. So I'm going to start with the same startmodel I used in the forward models, which is a model thatonly had an intercept. I'll define the largest possible modelwhich would be using all possible predictors. And I'm going to specifya direction of both. So again,the output look really similar and the first step will actually be the sameas the first step of forward because we're starting with a model with no predictor. So it's going to firstadd the height variable. But the second step willbe more interesting. So it's considering three possibilities. Doing nothing or adding predictors thatare not currently in the model, so that would be leg and these here, orremoving predictors that are in the model. And of all those possibilities,adding leg has the best effect on AIC, so we do that and then we continue searching. So in this last up here,we were considering doing nothing, removing the predictors that were inthe model or adding predictors that were not in the model, and at this stephere the choice is to do nothing. And similarly,we could do the same thing with BIC. Again we'll run it, but not discuss it andjust have it stored for comparison later. One thing we noted about stepwiseprocedures is they could potentially miss the best model. Best being, say,the model with the best AIC. It's not necessarily the best model, butit's the best model in terms of AIC. So, I'm going to compare the modelswith the best AIC, chosen if you have backwards slash and with AIC, forwardslash and with AIC, and stepwise with AIC. They all actually have four parameters. And three of them have the same AIC andone of them has a slightly larger AIC. So it looks like backwards AIC selecteda different model than best subset selection with AIC. And it's a very similar model andit actually just used height shoes instead of height, which sort of makessense if we go back and remember, hey look those are actually twohighly correlated variables. So we are probably definitely goingto take one but not the other. And just in the search tothat backwards AIC used, it happened to not pick up height,it picked up height shoes. And we see a similar thing with BIC and again I can use to extract AIC to getBIC by using this k argument here. It looks like three of them havethe same BIC and one slightly different. That won't always be the case wecould have none of them be the same. We could have all of them be the same, butlet's just say with the search procedures, we're not guaranteed tosee all the same things as with using best subset selection. This was a simplistic example inthat we only considered all possible first order terms andthey all happen to be numeric variables. So what happens if we have a data set,which I'll load here, that has factor variables, and what if wewanted to include some quadratic terms. Maybe for displacement, horsepower, weightand acceleration in this data set where we're trying to model this fuelefficiency variable miles per gallon. So, what I'm going to do,is I'm going to fit a huge model. I'm going to log transform the responsevariable, use all first order terms and their two interactions, andsome quadratic terms for displacement, horse power,weight, and acceleration. As we see there's potential for polynomial relationship betweenthose variables and the response. So, we'll go ahead and fit that. So what I'm going to do is I'm going to,starting with this model and go backwards with AIC. I'm going to click this and it's going todo a lot see how long this takes, because we're going to searchthrough a lot of models here. Something you might notice right away isthat of all the variables we're trying to eliminate here, it only attemptsto eliminate certain variables. And what I mean by that is it'snever going to try to just eliminate the displacement variable, but it willtry to eliminate say the interaction between cylinder and displacement ormay be quadratic term for replacement. And this is to say that the stepfunction with respect model hierarchy. So when we have two interactions, bothfirst order terms should be in the model. When we have a quadratic term, the firstorder of terms should be in the model. So when we're using thisformula expression here, R will understand how hierarchy works andit'll respect that. So it won't even attempt to remove a firstorder term if either the quadratic term is in the model or any interactionwith that term is in the model. And that's what we see happening here. And I believe it actually turns out,if we scroll through all the results here, that in the end,it never actually takes out enough second order terms to even considerremoving any of the first order terms. But if we look at the resultingfitted model for example, we have this interaction betweenhorsepower and acceleration. And we see that both horsepower andacceleration are in the model. To really illustrate this,we'll also try going forward. So I'm going to start witha model that has a log transform of the response variable andno predictor variables. We're going to use stepstarting with that model. And now let's just consider going up the largest possible modelhas all two way interactions. So we'll ignore polynomial terms fora moment. And one other some way tofind a scope variable, we don't actually needto specify the response. It will, by default, use the sameresponse as the starting model. And so we're going to go forward, and we're going to see a coupleinteresting things here. Searching through a lot of models,we got to wait a second. Okay, first we notice that while we'reconsidering all possible two way interactions in the largest possiblemodel that we're going to search through, we don't actually see tryingto add any of those yet. So we're only trying to addfirst order terms here. Something else we might notice is that forexample cylinder has this two degrees of freedom here and that's because cylinderis actually a categorical variable that is represented by two dummy variables becauseit's coding for three possible values. So R will actually either add thatentire cylinder factor variable which has multiple dummy variables ortakes them all out. It's not going to ever remove onedummy variable but not the other. So the soon the variable willalways have the same meeting. This first step we addthe weight variable. And the second step,we add this year variable. Now in the third step, because weight andyear are in the model. Now if we wanted to,we could add the weight, year interaction because the twofirst order terms are in the model. So, again, going forward, R is also respecting a model hierarchy byonly adding terms when it's appropriate. So, it will only even attempt to addsecond order terms when the lower order terms are already in the model. And we see that happeningthroughout this search. And in the end,if we go back here a little bit. So, we noticed when R tries toadd variables to the model, it'll have just cylinder here. But in the end when we fit the model, the cylinder say will be coded withthe dummy variables here, the specs. So four cylinder is the referencelevel but six cylinder and eight cylinder are codedusing dummy variables. So R is essentially taking care ofhierarchy and dummy variables for us in this selection procedure so long aswe're using factor variables appropriately and using formula syntax to specify ourmodels and not simply modifying our data. If we added additional terms by modifyingthe data, R would not understand how the hierarchy works in that model, sohierarchy would not be respected then. And how this works withdifferent functions for doing variable selection is different. So you should always look at the model youend up with and think about whether or not it is respecting hierarchy. And one more thing we should mentionis that while this is potentially a decent model, and actually maybe if we look at the one wedid with backward selection, maybe this isn't an even better model because it'salso considering some quadratic terms. This is just a good potential model. Is it the best model? We don't know that for sure. And that's more of a subjective question. But this is a well chosenmodel in terms of AIC. [MUSIC]"
stat-420,9,10,"[MUSIC] So we discussed how both AIC and BICpenalize based on the size of the model, and how this prevents overfitting. We haven't really explicitlydiscussed overfitting. While AIC and BIC implicitly guardagainst overfitting, we'd like to have a metric that will explicitly determinewhether or not we are overfitting. What we have here is a singledata set that's split in two. The grade points here were all originallyone data set, but they were split in two. So in the left panel, we'll havewhat's called the training data. So this is the datawe'll fit our models to. In the right panel,what we have is the Test data. And this is what we willuse to evaluate our models. So we fit two models. The solid line is a simple linearregression fit to this data. The dashed orange line is a complexpolynomial model fit to this data. In the left panel, the complex model is,on average, making fewer errors. But if we look at, now, the right panel, where this data is from the same dataset but was not used to fit the model, we actually see that now the orange dashedline is making bigger errors, on average. We could quantify this usingroot-mean-squared error. But so now we'll have tworoot-mean-squared errors, one called the Trainingroot-mean-squared error, and one called the Testroot-mean-squared error. To fit the model,we'll always use the Training data. But to evaluate the model forTest root-mean-squared error, we'll use the Test data,which was not used to fit the data. So this will give us a better senseof how the model is at predicting. It's somewhat considered cheating to usedata you fit the model with to evaluate how well you're predicting, becausewe've seen that residual sum of squares, thus root-mean-squared error,is going to go down as we have more and more complicated larger models. So that would be a bad metric for decidinghow well a model predicts because we would just say always pick the biggest model. But here, by using this Test set, we'regoing to have data that wasn't used to fit the model as something we can use toevaluate how well the model is predicting. So to obtain Test root-mean-squared error,we'll look at the y-values from the Test set, and then we'll use the modelthat was fit to the Training data to make predictions using the predictorsfrom the Test set, and those will be our new predicted values. In this particular example we see here,we found that the complex polynomial model actually does obtain a lowerTrain root-mean-squared error. But this is expected becauseit's simply a larger model. In the Test root-mean-squared error,it actually performs worse. So it's higher. So it's making bigger errors, on average. And this is exactly overfitting. Here, the model is too complex. So it's essentially fitting to noise. Because there is a model that is simpler, that fits better, we say that thiscomplex model is overfitting. We could also have the opposite situation,where a model is underfitting. That means it's too simple, andit's not actually capturing the signal. So it would have a poorerTest root-mean-squared error, compared to a more complex model that is,thus, predicting better. With the Test/Train split, we have toconsider how to Test/Train split, and what percentage of valuesshould be in the Train set, and what percentage of valuesshould be in the Test set. And this is an interesting question. Also, by doing that our results are goingto be somewhat tied to how we randomly perform that split. And so to overcome this, what's oftenintroduced is a cross-validation. So here we see another example. Here, we have some data that wassimulated according to a quadratic model. So the blue line here isa quadratic model fit to this data, and the orange line isa polynomial with a high degree. We can already see thatthe orange curve is overfitting. Now here's what happens whenthis particular point is not used to fit these two models. We notice that there's very littledifference between the two quadratic fits to all the data, andthe data with one point missing. However, the larger polynomial,its fit is changed quite a lot. We see previously,it almost goes through that third point, but now it's almost avoiding it. And we can sort of think of that singlepoint as a test set, or a validation set, and the other points wereused to train the model. So those would be the training set. What if we essentially repeatedthis process for each point here? Each time we fit the model to all but oneof these points and see when we predict that point how large the error is,and then we can average this. This process is exactly calledleave-one-out cross-validation. Here, I've defined the leave-one-outcross-validated root-mean-squared error. Here, I have these new residualsthat essentially consider the ith point deleted. So yi is the actual value, andyi hat [i] is the predicted value when the ith observationwas not used to fit the data. What we have here is exactlythe squared error for each individual point when itwas not used to fit the model, but we predict it's value. We add those up, we average them,and we take a square root. And that is the leave-one-outcross-validated root-mean-squared error. It might sound like, well, to do thiswe have to fit the model and times. While in general that's true, forlinear models we don't have to do that. It turns out that the previousexpression we had is equal to this expression here,where we take the actual residuals, so without leaving a point out, anddivide by 1 minus the leverage. And it turns out this is exactlythe leave-one-out cross-validated root-mean-squared error, which makes thisvery easy to obtain for linear models. And in general, there's additionalways to perform cross-validation. For example, five fold and ten fold,which as you might expect, you'd have to fit the model five times orten times. And those are used sort of very often,in general and practice. But for linear models we have thisone shortcut that we'll utilize. And we'll leave the discussion ofcross-validation in general to a machine learning class. While AIC and BIC implicitlyguarded against overfitting, Test root-mean-squared error andleave-one-out cross-validated root-mean-squared error will sort ofexplicitly guard against overfitting. That is, we can always compare two models. And if a larger model has a poorercross-validated root-mean-squared error, we know right away that we cansay that it is overfitting. In the next video,we'll use R to see exactly how this works. [MUSIC]"
stat-420,9,11,"So now we'll take a look at using our two detect models that are over and under fitting. Here I'm going to simulate some data so we'll know the true form of the model that is we have this quadratic function here. We'll run then we'll actually make the data. I'm going to fit three models of this data. I'm going to fit a simple linear model, I'm going to fit a polynomial model of degree two, so of the correct form and then I'm going to fit a high degree polynomial, so in this case degree eight polynomial, considered a big, highly complex model. I'm going to go ahead and plot the data and add the quadratic and high degree polynomial to the model. I'm going to ignore the linear model for now just to make the visualization simpler. We can see right away that the crazy, high degree polynomial is overfitting, it's almost going through all of the points exactly. So this is clearly overfitting. But if we evaluate based on a simple root mean squared error, we will do that for the three models right now, it turns out that the crazy polynomial model has the lowest root mean squared error but we said this is not a metric we should use to pick a model. So instead, what we'll do is look to perform cross-validation. So why are we doing that? So here I have set up a way to quickly make this same plot but with one data point removed from the model fitting process. So in this case I'll have the third point removed and we'll see what happens. So now it seems like the blue quadratic curve is making roughly the same errors on average but now the polynomial model it's making actually smaller errors on the data it was used to fit but a huge error for the point that was removed. And I can actually quickly modify this now. So let's see if we remove this point what happens. So now the quadratic model is missing it by much more than it had previously and the blue model, the quadratic one is making roughly the same error. Maybe if we do it for the second point we see, oh wow, so here the quadratic model not making a huge error, not much change. The crazy polynomial is now making an extraordinarily large error for that one point. So what we'd like to do is essentially do that for each point and add up those errors and average them and we'll call that the leave one out cross-validation root mean squared error. So for example the error here for that model, so this data point was not used to fit the model so the error here, originally like this, so the true value here minus the predictive value which is way up here where the predictive value when we fit the model to obtain the value we did not use this point here. That's what we see here. So if we add those up, we'll have what's called the leave one out cross-validated root mean squared error that is so we'll add up those squared errors, average them and take a square root. This would require essentially literally repeating this process and then storing that result and doing it, in this case, I think 11 times. We don't actually have to do that because of the shortcut formula that works for linear models. What I'll do is I'll write a function that does this where the function will take a model as its argument so it will obtain the residuals here, divide by one minus the leverages and then perform the usual squaring, summing, averaging and square root. So I think I should run that function. When I do this for the linear model, the quadratic model and that big polynomial model, what we see is that the quadratic model performs the best. So this is highlighting a case of underfitting and overfitting for us. So because the crazy quadratic model has a higher root mean squared error in the cross-validated sense, that means it's predicting worse but it's a more complex model than this one here, we say that it's overfitting. So there's a model that fits better, that's smaller, so this is overfitting. The opposite is true of the linear model. Based on this metric here, it predicts worse and because it's simpler than this model here that fits better, we say that it is underfitting. We'll also do a quick analysis with some actual data, so reading in the usual fuel efficiency data set. So I'm going to fit a few models here. So first I'm going to fit a model that considers a log response. So I'm going to do that for all the models I fit here and simply all of the predictors and added a fashion. I'm also going to consider another model with a log response. All first order terms, there are two way interactions and a few quadratic terms here. I think what I'm also going to do is start with this model and search for a good model using backwords AIC and I'm going to add this trace = zero argument here so that way it does not show me the intermediate results of doing that, it simply does it and stores it in this very well here. I believe that so the additive model is the smallest of these. This model here is the largest and the one chosen via selection procedure is sort of roughly right in the middle. We expect that this model, the sort of large one, will have the smallest root mean squared error, just the usual root mean squared error where we don't consider test transport or any sort of cross-validation. So we do see that here and again and we can make this comparison because all these models have a log response. So we're going to just conserve the log response the entire time. But now if we look at a leave one out cross-validated root mean squared error for each, we see that is actually the model found via selection that's performing the best here. The initial huge model is actually overfitting relative to this one and the additive model is actually under fitting relative to the model chosen via selection."
stat-420,9,12,"[MUSIC] One of the more famous quotations instatistics states, all models are wrong, some models are useful andthis is attributed to George Box. There's actually a lot of discussionabout how to interpret this statement. I'll go with the simple aspossible interpretation. All models are wrong, simply remindsus that models are simplifications and approximations of reality. So they're never the truth. They're just something simple for us towork with to help explain and predict. Some models are useful, suggests thatwhile they're not exactly correct they're still useful forexplaining and predicting. The fact that some models are usefulsuggests that some models are not useful. The question we want to ask ourself is,how do we actually find a good model? Again, we've sort of been bothlooking at simulation studies and performing modeling. So we're going to take a step back and sort of think about modelingat sort of a higher level. And discuss the art of actuallybuilding a good model. Remember that when we are fitting a model, we're using data to essentiallytry to uncover the truth. In general, for us, we're tryingto model some response variable y as a function of some predictor variables. The function of the predictor variables,that is the signal and then we normally allow for some noise. We want to discover this signal andmodel it appropriately and we would prefer not toaccidentally model the noise. To do this, we consider somefunctions of the predictors usually. So I have here that we want to discussthe family of models possible to fit here, form of models we can express andhow to actually go about fitting them. So in this course, the family of models that we havebeen considering is linear models. So in this case, the functionalform of the model that we consider is some linear combinationof the available predictors. This model has parameters, beta 0,beta 1, beta 2 and so on and so forth. To fit this model,we want to estimate these parameters. And that makes the linearmodel part of the larger family of models which wewould call parametric. So we've specified a modelin terms of parameters. And we want to estimate orlearn those parameters. Now that's not the only way to do things. So on the left here, I fit a simplelinear regression to this data. But on the right, I use what we called a non-parametricregression technique called smoothing. So instead of the model beingstated in terms of parameters and learning them, we simply specify, say, in this case how to smooth out the datato create this fitted model here. Why our focus on linear models? One is that they are extremely popular. They're almost always at leastattempted if not used in the end. Also they're extremely easy to interpret. So interpreting thissmoother on the right, despite it being very simple is much moredifficult than the regression on the left. And then also most of the concepts welearn about linear regression can easily transfer to using otherfamilies of models. That makes them one of the besttools to introduce first. Once we decided on a family of models,again linear, we can consider differentforms of the model. By that, I mean, we have these,in this case, say ten predictors. But by using different substantivepredictors, different transformations, different interactions,we can fit very different linear models. Using our syntax insteadof the mathematical syntax. For example, I could fit a model where Ilook at just a simple linear regression with maybe just one ofthe predictor variables x5. Or I could consider maybea few of the predictors. Maybe I consider some transformations,like maybe I'll take a log transform of the response andmaybe a couple different predictors. Maybe I add an interactionterm somewhere so maybe we consider x1 plus x2 andmaybe their interaction as well. These are all linear models so we're modelling a response as a linearcombination of the predictors. But we can chose different predictors,we can transform variables and add new predictors thatare functions of the old ones. Within the family of models, there's a large number of modelsthat we can consider fitting. Once we've decided on a form of the model,so for example, here just simple linear regression. We can decide exactly it ishow to fit the model, but our go to tool has been least squares. While that's not the onlyway to fit a model, when we're talking about linearregression, we are using least squares. So we're minimizing the squared errorsthat the chosen form of a model makes. When we simulate we know the family form,and we could say we know the fit of the model, that iswe know the true value of the parameters. But now when we're actually performingmodeling we're going to make an assumption about the family of modelthat's appropriate. We're going to pick or specify a formof the model, and then we will fit that model to the data to obtain estimatesof the true unknown model parameters. What does that look like? So here, in the first line,I have an assumed model. That is I've chosen a family ofmodels that is the linear model here because it's a linearcombination of the predictors. I've chosen a sub set of predictors here,x1, x3, and x5. Then I go about the business of fittingthe model and we'll use r to do that so the lm function, essentially,specifies the family of models. And then, inside of there, a formula specificationspecifies the form of the model. So then we obtain the fitted model,which has the estimates. Which give us a good guess forthe model parameters. So we still need to answer the questionhow do we find a good model for our data? Before you outline a procedure, I want tosay that there is no one procedure to use. Finding and building a good modelis more of an art than a science. But this is a general frameworkthat you can sort of consider. So the very first thing youshould do is pick a goal or develop a research question. What is it that you wantto use this model for? Then you should obtain data. Obviously these two stepscould be interchanged, maybe you have data andthen you want to use it. Or maybe you have a question and then you wanted to obtaindata to answer that question. So you're going to need some data,but more importantly, you're going to want some statement ofwhat you want to do with that data. The next thing you should do isexplore that data before even fitting models to data you shouldunderstand this data that you have, then you want to developsome candidate models. There's no one procedure thatsays we can go from step a to step b to step c we'llarrive at a good model. So we're going to want to find a numberof models that could be useful here. And then we'll want to evaluatethose models with our goals in mind. Some comments about someof these individual steps. There's a number of goals youcould have with your model. But they broadly sort of fallinto two categories which is explanation and prediction. For explanation, we want to understand therelationship between the predictors and a particular response variable. If that's our goal, we are going to placehigh value on models that are both small and interpretable, but fit well. For prediction, we just want to be able to with a new observation makea good prediction of the response. For this we'll values models that makesmall errors, but do not over fit. We mention that we need to obtain data. So there's broadly two ways you canobtain data, via experimentation or you could also have observational data. And very, very,very often our data will be observational. With observational comes a warning whichis that correlation is not causation. You have to be very carefulabout using a model for observational data toexplain a relationship. It can be used to understandhow variables are related. But you should be very, very, very carefulabout making statements about causality. A quick example here. This picture shows how fuelefficiency is related to horsepower. Very clearly as horsepowerincreases fuel seems to go down. But is the increase in horsepowerthe direct cause of that? That's something fora subject matter expert to weigh in on. Also this is observationaldata that we had here. So it would be a bad idea to makeany statement about causality. So horsepower and fuel efficiencyare related quantities, but whether or not one causes the other is a statementthat we wouldn't want to make. Once you obtain some data you shouldnot jump right into modeling. You should take a look at thatdata in a number of ways. One thing you should do is youshould read the documentation about that data if it exists. If it doesn't exist youshould ask yourself well, where did this data come from? because you want toknow what is this data? What are the variables, what are theymeasuring, where do they come from? If we don't know that it's very hard toget any sort of meaning out of our models. One of the next things we want todo is check the raw data. How is the data coded? Are some of the variables factors? Are some of them alreadycoded as dummy variables? If so what's the reference level,what's not? Maybe look at some numerical summaries. That would quickly highlight another thingyou should look for which is missing data. Do you have any missing data? If at all possible, it would be niceto just throw up a nice view of just the entire data if at all possible. You could use a viewcommand in our studio. Some things might jumpout at you right away, like missing data, poorly coded data,and things like that. And then the next thing you shoulddo before fitting any models is visualize the data. So one thing I have here real quickis a pairs plot of some data. That would immediately show youthe relationship between all the possible variables. This might highlight the need forsome transformations and as potential visualization tell you a lot aboutthe data without ever needing to model it. Then the next thing we want to do isdevelop a number of candidate models. And this is where there'sa lot of things we can do. We want to consider things liketransformations of variables which may have been suggested fromsome visualizations. We might want to consider someinteractions between variables. And then one thing we really wantto consider is what variables do we actually want to include in our model? A number of things we can do here,we could consider some simple models. One thing you should almost always dois consider an additive model using all possible predictors. And then you can do some things likeuse some variable selection procedures. Maybe build up a big model thatconsiders a number of interactions and maybe some higher order of polynomials. And maybe do a variable selectionprocedure using AIC and a backwards search to findanother candidate model. So you can use a number of differentcombinations of transformations, interactions, and variable selection techniques to comeup with a number of candidate models. Once you have some candidate models,you want to go ahead and evaluate them. And how you evaluate themshould be based on your goals. If your goal is to use this model forexplanation and you want to understand the relationshipbetween the predictors and the response, one of the metrics you coulduse is, literally, the size of the model. If you have two models that are somewhatsimilar in terms of how well they fit, just go ahead and pick the smaller one,because that will be easier to interpret. That's not always exactlywhat you should do. You should also consider some context ifyou understand what the data actually is. But in general if you want to explainthe relationship between two variables, you have a preference forsmall models you can interpret well. With that, comes some considerationsabout model assumptions and collinearity. So if you want to use a model forexplanation maybe you're really interested on one particular variable andyou want to do tests about it. Well then you need to make sureyour model assumptions are met so those tests are valid. Similarly, if you want to use a model forexplanation, and you're interested in the fitted coefficients to tellyou sort of the size of an effect, you need to watch out for collinearity. Because if there's a lot ofcollinearity in the data, you understand that those estimatesare going to be highly variable. So it's harder to use them for making a statement about howthe variables are related. This also suggests that it'spossible to iterate on some models. So maybe based on some candidate modelsyou find some assumptions or violations. And you should go back andtry to reevaluate them and find the model that similar,but does not have those issues. Looking at things like additionaltransformations or interactions. If instead your goal is to usethis model to simply predict, all you really care about is thatmakes the smallest errors possible. So you want to use something liketest root mean squared error or a cross validated root mean squared error,to find the model that's going to make the fewest errors in predictingfuture observations. So model assumptions are not as importantanymore because we're not doing hypothesis tests aboutany particular parameter. We're not really interested inthe fitted coefficients and how variable they could be. All we want to do is makenew predictions well then, the assumptions don't matter as much. And we can just use the modelto make predictions. So all we care about is howsmall those errors are. We need to be careful there though, so if we want to make predictionswith some error bars on them. So if we want to make a predictioninterval then the assumptions would actually matter. You just need to keep context in mind andyour goal in mind, and remember the limitations ofsome procedures we've used. So that outlines some general things tothink about when trying to develop a good model. But you shouldn't get into the habitof following any particular procedure from point a to point b,rather consider the tools you have. Consider the limitations they have. Use that plus the context of the problemwith a dash of common sense and critical thinking to guideyou in finding a good model. [MUSIC]"
stat-420,10,1,"In this lesson, we will begeneralizing the concepts of ordinary linear regression toa broader class of models, which are not so creatively named generalized linear models. Instead of requiring theresponse variables be numeric, and errors followa normal distribution, generalized linearmodels or GLM's, will allow for different types of responses withdifferent distributions. We'll focus our attention on modeling binaryresponse variables, that is a categorical variable with two possible categories. The type of generalizedlinear model used in this case is calledlogistic regression. We'll show how ordinarylinear models and logistic regression bothfit into the GLM framework. This will allow usto see that many of the concepts we learnedabout ordinary linear models immediately transferredto logistic regression with only minor modifications. Most of what we've learnedabout the LM function, also applies to the GLM functionin R. After this lesson, the already large number of situations where we canapply linear models, will be expanded even further."
stat-420,10,2,"In order to introduce a new concept, will first return to an old familiar concept, Linear Regression. Hopefully at this point linear reggressio is pretty familiar, but from now on we'll call linear regression ordinary linear regression, to sort of differentiate it from other linear regressions that we're about to look at. Linear regression model looks something like this, we have a numeric response, and we want to model it as a function of some linear combination of predictors plus some error. Now, we've been calling this an error, but it would be helpful to start thinking about it as a way to introduce a distribution into the model. In this case, we normally say that those errors follow a normal distribution with a mean of zero and a variance of sigma squared. With that in mind, another way to write the model would be like this. So, what we're really interested in, is the conditional distribution of Y given the predictors. So, to create a model for the conditional distribution of Y, we'll use a normal distribution. The mean is going to be that linear combination of the predictors. So, that as the conditional mean of Y given the predictors. We have a second parameter for the normal distribution, this case sigma squared, and this is where the equal. Variance assumption comes from because we're not modifying the variance for different values of the predictors, only the mean. We're going to focus quite a lot on this conditional mean of Y given the predictors, that's going to be a concept that carries through to the other linear regressions were about to do. But what's going to change is the response variable. So, we're using this model when we have a numeric response, but what if we don't have a numeric response? What if we have some sort of categorical response, maybe an integer response? Essentially, it wouldn't make much sense to use a normal distribution, if say for example the response only had two possible values, like say zero and one. So, we're going to focus on what we need to change here in order to make a setup similar to this work but for different response variables. The thing that we're going to continue to use, is this linear combination of the predictors. The things we're going to change that we can see here are in particular this distribution. So, now we can introduce the idea of a generalized linear model. Generalized linear models are often called GLMs. Essentially have three main components: distribution, a linear combination of predictors and something called a link function. So, the distribution is going to specify that conditional distribution of the response given the predictors. For example, we could use a normal distribution like we have previously, we can maybe use a Poisson distribution or maybe we could use a binomial distribution. So, that is something we have seen before in particular with the normal, but we'll have to discuss maybe how we use a different distribution. Another thing that we've certainly seen many many times before, is the use of a linear combination of predictors. So, that concept is something that we automatically understand, and just how we use that will change ever so slightly for some of these different GLMs. The new concept, is going to be this thing called a link function. What is a link function? Have you used one before? What is it? We've been using GLM's as whole time, just one very specific one which is what we're now calling ordinary linear regression, fit uses a normal distribution for the conditional distribution of Y, it use that linear combination of predictors. It may seem like we haven't used the link function, but technically, we have. So, the question remains, what is a link function? The purpose of the link function is to define the relationship between that linear combination predictors that we love to focus on so much, and the conditional mean of Y. So, this g right here, is the link function. Again, all it does is it says, this is how the conditional mean of Y relates to the linear combination of predictors. The link function that we've been using this whole time with ordinary linear regression, is actually just the identity because an ordinary linear regression, the linear combination of predictors is exactly the conditional mean of Y given X. So, we've been using a link function, it just happens to be the simplest possible link function which essentially not only function it's just the identity. Something will also want to look at along the way is the inverse, which will give us an expression for the conditional mean of Y, as a function of the linear combination of the predictors. So, one question that will come up will be how to fit a GLM. We will sidestep this issue a little bit, but we will say that fitting a GLM is something that we've done before. One thing we'll want to do is estimate these unknown model parameters that define the linear combination of predictors. How are we going to do that? Well, we'll return to maximum likelihood. So, we saw that with ordinary regression, we can frame it as a least squares problem, but that was equivalent to maximizing the likelihood. Maximizing likelihood will be the tool for fitting all GLMs and we'll look at a quick example of this when we actually look at a specific GLM. But for the most part we're going to say, R take the way of do this for us, and when you use those results, but it is useful to know that we are under the hood using maximum likelihood to fit these models. As sort of a very, very broad overview of GLMs, here are three examples. So, linear regression is the familiar linear regression that we've seen over and over again. We use a normal distribution for the conditional mean of Y given X, we use it to model numeric data and then this Y this distribution has a support of the whole real line. We said that we weren't really using link function, we were sort of just using the identity function. The linear combination of the predictors is exactly the conditional mean of Y, and then it's also very easy to get the conditional mean of Y, it is simply the linear combination of the predictors. This table is probably very overwhelming, there's a lot going on here, and that should be the case at the moment, you could take an entire course. In generalized linear models, here we're trying to just introduce that, given what we already know, it's a short extension to use this much larger class of models. To help understand the idea better, we're going to focus very specifically on logistic regression. Which we'll use when we have binary data for the response. So, something where we can label one category is zero, another category as one. This will allow us to look at a new link function, that link function is called the logit function. It looks something like this, where this p of X here is the conditional mean of Y given the predictors, which also turns out to be the probability parameter for a Bernoulli distribution, and that's what we're going to talk about next as a way to get a general idea of generalized linear models."
stat-420,10,3,"So, the first and potentially the most important difference between ordinary linear regression and logistic regression is the type of response variable that we're trying to model. In ordinary linear regression, that was a numeric response variable, whereas in logistic regression is going to be a categorical variable, in particular a binary categorical variable. So our response here will be a variable that takes two possible values, say yes no, maybe spam for a bad e-mail and ham for a good e-mail, maybe positive for a positive test result for some disease and negative for a negative test result for some diseases. Even though we're going to have these two possible categories each time, we're always going to assign one of them to be the numerical value zero and another to be the numerical value one. So, our goal will be to find a model that will essentially be able to tell us what is the probability that an e-mail is spam given values for some set of predictors. In other words, what is the probability if the response variable Y takes the numeric value one? A natural distribution to model a zero, one variable is the Bernoulli distribution. So, in particular, we're going to look at the conditional distribution of Y given the values of some predictors, so we're going to be conditioning on that. Then conditioned on that, there's only two possible outcomes, Y can either be zero or Y can be one, and those two probabilities have to add up to one because those are the only possibilities. Now, we're going to focus quite a lot on the probability that Y is one, so much so that we're going to use some shorthand notation for it, just because if we have that probability, we automatically get the probability for Y is zero, so it will be the probability that we often refer to. So, now we can say Y conditioned on X follows a Bernoulli distribution with some probability that Y is equal to one. The question then becomes, well, how do we relate this probability to the predictors? How do different values for the predictors influence this probability. Before we do that, we're going to do a little math first about the expected value of a Bernoulli distribution. Again, recall that Y can take two possible values, zero and one. So, this expected value becomes zero times the probability that Y is zero, still conditioned on the predictors, plus one times the probability that Y is equal to one given values of the predictors, and that is equal to the probability that Y is equal to one given values of the predictors. In our notation that we developed, this is P of X. So, that probability is actually the conditional mean of Y which is something that we already know, we're going to be focusing on in the modeling. Now, we want to go ahead and see how we can relate that probability to the predictors. To do so, we'll finally introduce the logistic model. So, again we're going to focus heavily on this probability that Y is one, conditioned on some predictors, which we now know is exactly the conditional mean of Y. This next line essentially lays out the model. So, on the right hand side, we have the usual linear combination of predictors that we're very used to seeing, and in the GLM notation, sometimes we call this Eta of X. On the left-hand side, we have what is called the log odds. So, inside of the log, that would be what is called the odds which is the probability that Y is one, divided by the probability that Y is zero, and then the whole thing taken together, that would be what's called the log odds. So, we see that this P of X here, don't forget that this is actually the expected value of Y given X equals X. So, we're relating the conditional mean to the linear combination of the predictors which is exactly what GLMs do, we're just using a very particular function to do that and the question becomes then, well, what is this link function that we're using? The name of that function is the logit function, so that's the function that we will supply the conditional mean of X and that gives us back exactly what we have seen here. So, in other words, the logit function applied to the probability that we're very interested in is equal to the linear combination of the predictors. In other words, the logit of the conditional mean of Y is equal to the linear combination of the predictors. So, in some sense, it's a very succinct way to specify the GLM that we're fitting here. We have our linear combination of the predictors, we have this probability here which is a parameter for the Bernoulli distribution, and we have our Link Function. Pretty often, we'll want to have an expression for that probability that we're interested in, so while we have the weighing function here, we also have the inverse link function, which will come in handy for obtaining the conditional mean as a function of the predictors. So, the inverse logit function if we apply it to both sides here, so if I apply that function to that expression, if I apply this function this expression, what will I get? So, I apply it here, I get P of X, and apply it to the right-hand side which I think is easier, I'll just denote Eta of X, we get E to the Eta of X, divided by one plus E to the Eta X. Then we also have this mathematically equivalent expression here, so this would be one over one plus E to the minus Eta X. So, this will be a very useful expression because it relates the probability that Y is one to the linear combination of the predictors. So, we could say, well, here are the values for the predictors, and then we get back the probability that Y is one. That ends up looking something like this. What we have here is an expression for the probability that Y is one given values of the predictors, and we have this expression here that is based on these now, model parameters, beta zero, beta one and so on, and the values of the predictors X1, X2 and so on. This is a probability model. What we want to do is actually fit it to data and thus estimate beta zero, beta one and so on, and then be able to put hats on them and have the estimated values which gives us an estimate for this probability. So, the question becomes, well, how do we actually fit this model? The answer is going to be maximum likelihood. So, first, what we'll do is we'll sort of rewrite things indexing by eye, that way we know that we are fitting two observations, one through n and we're applying this model to them. So, we have the probability that Y is one and the probability that Y is zero, and then what we want to do is write down the likelihood. The likelihood for this beta vector looks something like this, and you might realize down here this P of X is a function of the model parameters which we see here, and this one minus P of X is a function of the model parameters which we see here. So, what we want to do is find the set of betas that maximize this function. Unfortunately, we can't actually arrive at an analytical solution for this, so we'll have to do it numerically which means it will simply let our take care of it for us. Therefore, we can claim that we can fit a logistic regression, which we will do multiple times in the next video."
stat-420,10,4,"So to get a sense for how logistic regression works, what we'll first do is actually simulate some data according to the logistic model. Just a little refresher, we're going to be focusing heavily on this probability here, which we said the logistic model relates to the linear combination of the predictors. We have these two mathematical expressions for that and also recall that this probability is exactly the conditional distribution of Y given X the model that we're going to be looking at only has one predictor to keep things easy, and here we see the linear combination of the predictors and that is equal to the logit function of that probability that we're interested in. Another way to write this though we see on the right-hand side here first we have explicitly written out the linear combination of the predictors, then we've noted for each observation there is some probability that the response is one which is related to the value of the predictor through the linear combination of the predictors, predictor in this case. Then we finally see that Y is simulated via a Bernoulli distribution using that probability for that observation. Thinking about it this way helps us to write an R function to actually simulate data like this. So first we inside of this function simulate the values of the predictors since we're conditioning on the use of these arbitrary then we make our linear combination of the predictors, then for each observation we get the probability that Y is one. Then we finally simulate the Y values. Note that we have this size equal one argument and that's what essentially makes this line simulating from a Bernoulli distribution because the Bernoulli distribution is just a binomial distribution where we're sampling one observation at a time, and for each observation the probability that is one is exactly that p and we return everything as a dataframe. Having a look at just a little bit of the data that is returned here by running that function, we get a sense of how this model works. So one thing that should be fairly clear is that when x is large because of this positive coefficient here the log odds will be large, therefore, this probability that we're interested in will be large, and so there should be a greater chance that the simulated response value should be one. So if we look at this data here, the largest x value in these six observations that we see here is in this line and indeed y is one. I think the smallest is this one here and we see that y is zero. So that sort of makes a lot of sense. But there's still some uncertainty here. So, I think if we have a look at this observation and this observation, we see maybe the opposite of what we expect to happen. So, while this observation is a larger x-value, it actually corresponds to a y value of zero whereas this observation is smaller, but it corresponds to a y value of one. That's because there's only a certain probability that y is zero or one is not a certainty. So that's should help us understand what this model is actually doing. So, when x is larger there's a greater possibility that y is one but it still could be zero, when x is small there's a greater chance of y is zero but it could still be one. So now we want to go ahead and see how we fit a model to this data which is what this line of R code does for us. This line shouldn't look all that unusual. We see we're supplying the data, and we also see we're specifying some formula syntax. Which in this case tells us well we want to use an intercept plus some coefficient times the single predictor. Now the newer things here are the use of GLM and supplying binomial for this family argument. These two things in combination say we want to fit logistic regression, there's actually underneath by default we are using the logit link function when we specify family equals binomial. So the model that we're assuming here and that's trying to fit looks something like this. We can start to see now that the formula syntax, what it does for all of these glms, it specifies what to do with the linear combination of the predictors. This is where we can specify what predictors to use and where we can specify what transformation those predictors to include. Once we run this line we can obtain our estimates for the coefficients, here's our estimate for beta one, here's our estimate for beta zero, and here is in the end our fitted model. So, investigating this fitted model a little bit, we want to look at what happens when we supply an x value of -0.5, 0 and 1. In particular we want to know well, what do we predict the probability that y is one when the predictors have those different values? So we simply plug into the expression for that probability and we see that when x is small, in this case 0.5, we get a small value for the probability that y is one. Whereas as we increase x, so for example when x is large in this case x is one, we see that the probability that y is one is somewhat large. To get a better idea we can look at a plot. So the first first we see on the spot is the actual data. Again, the data is being represented with zeros and ones for the response. So we see zero then one here, this axis will also serve for probabilities but if we're just looking at the black dots which represent the data you can have an x value and then y is either zero or one. So this blue dash curve which I'll extend a little bit is the result of fitting logistic regression and estimating the probability that y is one for any value of x. What we see here should make a lot of sense. So if we look at the data say to the left of zero, we don't actually see any observations with a y of one. We see left of zero, we see estimator probabilities that y is one that are very small. Especially if we get it maybe over to here when we're looking at x is negative two, we see a very, very, very small probability that y is one and that makes a lot of sense. On the other side of zero we start to see some observations where y is one for those values of x. We see that this curve starts to slope upwards, which gives us a larger and larger probability that y is one and that makes a lot of sense. So this model seems to be fairly good for this data. But we also see a line on this plot what is that? So it turns out this is the predicted value of the response if we fit ordinary linear regression to this data. Again, since we are coding the response as zero and one, it's numeric, there's no reason why we couldn't try on linear regression here. It actually turns out that the conditional mean of Y given X is exactly the probability that y is one, when you're only considering values of zero and one. So this makes even more sense except for when you look what happens when we extend this line down here. This would essentially be estimating a probability that y is one of less than zero and if we extended the line to the right, it would eventually be larger than one and that wouldn't make sense either. So this is why we greatly prefer logistic regression the situation is because of the link function we're using we're restricting these probabilities that u is one to be between zero and one because probabilities should be between zero and one. So if we look at a slightly different model that we want to simulate data from again a single predictor x but now we have this negative coefficient in front of that predictor. So that means that if we have large values of the predictor the log odds ratio will be small therefore, the probability that y is one will also be small. So, if we magically simulated some data from this model and then perhaps plotted it, fit logistic regression and plotted the estimated probability that y is one for that model we would get something that looks like this. Again, just intuitively looking at the data this makes a lot of sense because for small values of x say, less than zero there aren't really any observations where y is zero, so we should have large probability that y is one which we see in our curve here, and then for large values of x, say greater than zero we start to see a lot of observations where y is zero and we also see the probability that y is one is falling off, so that makes a lot of sense. So let's take a look at one more model. We'll again only use a single predictor but now we have added a transformation of that variable. So, that linear combination of the predictors this time including a transformation is still being related to the probability that y is one through the logit function but by adding that transformation we've created a more complicated relationship between that probability that we're interested in and the predictor. Here, we write a function to simulate some data from that probability model, we actually simulate some data from that model and then we want to fit a model that contains the ability to capture that more complicated relationship so first looking at this formula syntax here, we understand from what we've done before, this is saying we want an intercept plus a coefficient in front of the predictor, from that term there, plus a coefficient in front of the transformation of the predictor, from that term there. Then again by using glm, and this binomial argument for family, that says that we want to fit logistic regression. Then R would get us the estimated values of these coefficients and then perhaps we'll plot the result, that is we'll plot the estimated value of the probability that y is one given a value of the predictor, and we see something that looks like this. Again, just looking at the data, this should make lot of sense sort of near zero, we see a lot more observations that are zero then one for y, so it makes sense that we should have an estimated value of the probability that y is one, too low, whereas as we move further away from zero for x we have fewer and fewer y values are zero and more and more that one and that's indicated here and the shape of this resulting curve for the estimated probability that y is one. We see that by adding transformations we can further complicate the relationship between the predictor and that probability that we're interested in. The other thing that we could do is add additional predictors. That would make it a little harder to plot, but so we can add more complex relationships between predictors and that probability for the response, and we'll take a look at that via an example with data with multiple predictors in R."
stat-420,10,5,"So, now that we understand that logistic regression helps us relate a linear combination of predictors to the log odds, thus to the probability that the response is one as opposed to zero. We want to start using this model, and before we jump into R and actually do that, we'll briefly talk through some of the things we can do with this model. So, here we have a non-comprehensive list of things we'd like to be able to do when working with logistic regression. First and foremost, we need to be able to specify a logistic regression model in R and we'll use formula syntax to do that. We've already briefly seen how to do this very simple models, but we'll see it for more complicated models when we jump into working with data in R. We also need to understand how to interpret logistic regression models. We've already talked about how to do that as a probability model, but again when we start working with data, we'll talk about some of the finer details there. So, there are a number of other things we had done previously when working with ordinary linear regression, which we'd like to also do with just regression. Those are, for example, testing about a single parameter, so whether or not a particular predictor should be in the model or not, tests about multiple parameters which would also allow us to think about that as comparing to nested logistic regressions. We've often done confidence intervals for parameters and this will probably rather similar to testing a single parameter. We will also want confidence intervals for the mean response. This will be something that's similar to, but rather different than confidence intervals for mean response in ordinary linear regression due to the use of the linear function. Then, we'd also like to do variable selection. So, in logistic regression, a test about a single parameter is called a Wald Test. This is actually a specific application of a Wald Test tool logistic regression, but this is how we'll perform the tasks in this case, and this depends upon the fact that this quantity here for large samples is approximately normal. So, with this in mind, we can think about this as the analog to the t-test in ordinary linear regression and interestingly, to actually carry this out in R, it will be very similar. We will use the summary command, which will get us things like that standard error and the p-value automatically. So, if this Wald Test is the analog to the t-test in ordinary linear regression, then the likelihood ratio test for multiple parameters is the analog of the F-tests that we'd seen before. Here we have two models written down, the first one, we will call the full model. The second one, we will call the null model, and in this case the null model is going to be smaller than the full model. So, we'll have q is less than p, and essentially to compare these two models, we could specify a null hypothesis setting some of the parameters in the full model to be equal to zero. We can write down a test statistic here, which is based on the likelihoods for the two models. So, capital L here is the likelihood, lowercase L here is the log likelihood and it turns out for large enough samples, this test statistic is approximately chi-square with k degrees of freedom. Here, k is the difference between P and Q. So, this setup should feel fairly familiar. It's a new test statistic and a new distribution, but this functions much like the F-test in ordinary linear regression, so much so that to use this in R, we will actually go through the anova function. We will just have to give it a new argument to tell it that we want to do is slightly different tests. To make a confidence interval for a single model parameter, we can use the distributional result from the Wald Test. So, we said that the test statistic was approximately standard normal. So, here, I've drawn a standard normal curve. So, then, we essentially just need to obtain the appropriate critical value z sub Alpha over two, which is a value such that the area to the right of it is exactly Alpha over two and then minus z sub Alpha over two. Would also have Alpha over two area to the left of it due to the symmetry of a standard normal, but then we also technically need this standard error, but we're just going to go ahead and let R calculate that for us. We can find it in the summary information from the model. More interesting than that would be a confidence interval for the mean response and that is a confidence interval for this probability that we're interested in which is the probability that y is one given X. So, to do this we first need to make a confidence interval for the linear combination of the predictors, which we'll call eta of X for short, and then using this distributional result here, which says that this quantity which looks fairly similar to some test statistics that we've seen, has an approximate standard normal distribution for a large enough sample size. We can then rearrange that to create a confidence interval for eta of X. Then, what we'll need to do is apply the inverse logit function to make it a confidence interval for the quantity that we're interested in. We do exactly that. We apply the logit function to the lower bound and the upper bound of the confidence interval for eta, which then makes this confidence interval for the probability that we are interested in. When we start looking at creating these, we should see that we would hope that the upper and lower bounds of these intervals would be between zero and one because of the use of the inverse logit function. Another thing we'd like to be able to do is variable selection. We can certainly include all the predictors and the linear combination of the predictors, but we don't necessarily need to, maybe we're over fitting if we use too many variables. So, variable selection in logistic regression is certainly possible, especially because AIC and BIC are defined as a function of likelihoods, and we can write down the likelihood for a logistic regression model. So, much like we had seen in ordinary linear regression, we simply need to pick a metric. So, say, AIC or BIC, and then a search procedure, either forwards, backwards, stepwise, or exhaustive, that is a best subset selection in order to pick a model. So, our go-to will be to use the step function as we had in ordinary linear regression, which will, by default, use AIC and backwards selection, but we'll also look into the potential for other metrics. We had seen how something like a cross-validated root mean squared error was rather useful in an ordinary linear regression setting, maybe there's something similar in a logistic regression setting, when we are trying to perform certain tasks."
stat-420,10,6,"Now, we'll take a longer look at using logistic regression in R. So, recall that we're dealing with a binary response, something that we can code. One of the value is zero and other value is one. The thing that we'll be interested in is the probability that that response takes the value one given values of the predictors. So the model that we're going to use has on the right-hand side this linear combination of the predictors and then through the use of the logit link function, we'll relate those predictors to that probability that we're interested in. So, we can undo that link function and directly obtain an expression for that probability that we're interested in as a function of the model parameters and the values of the predictors. So, we'll start by fitting logistic regressions to some data that we create and then we'll move to some real data. So, from previous videos we've seen this model before expressed in two different ways and this function that we can use to generate data from this model. So, we'll go ahead and create some of that data. We see we have a numeric predictor x and technically a numeric predictor y but it just so happens to only take two possible values, zero and one. So, because this y is technically numeric, we can go ahead and use ordinary linear regression. So that gives us an opportunity to look at that syntax again, we use the lm function, we use what we call foremost syntax to say, okay y is the response x is the predictor, and then we use the data argument to say, well here's the data we actually want to fit this model to. The formula syntax should use the column names from this data frame. So to instead fill logistic regression, we have to change two things. The things we don't have to change are the formula syntax which again specifies the response and the predictor and the other thing we don't have to change is how we are supplying the data. The things we do have to change are the function we're using, we'll use GLM for generalized linear model because logistic regression is a specific example of a GLM. The other thing we have to do is specify which GLM we actually want? And that's done through the family argument and we'll supply family equals binomial to indicate logistic regression. So, when we're doing this there's actually a little bit of default behavior that's happening in the background. So, this line of code here would actually do the same thing which specifies the link function we're going to use which is the logit function but this is the default behavior for logistic regression, that is family equals binomial. So we don't actually have to write this all out but this is what's happening. So we'll run this code. We'll look at the coefficients from the logistic regression and we see that they're not all that far from the truth. So the true beta zero should have been negative two and we have a negative 2.3, something here and the true beta one should have been three and we have a 3.65 here, it's not too bad. So, this is that linear combination of the predictors that we just estimated. I'll be doing a bit of rounding here just to make things look nicer. So, one thing we often did when we ran lm was use this predict function. So, we can also do that in logistic regression but it operates a little bit differently. So predict function, we've seen that before. We're supplying the model that we fit, we've done that before, we're going to supply some new data. So, here we have a data frame that we're creating because that's what this argument expects and we're going to just create one new observation in place which has an x value of 1.2 because the predict function expects a data frame with a least columns for the predictors that we've used. But then one new thing is we'll specify type equals link. So, when we specify type equals link, what predicts will actually return is exactly this quantity here and this case plugging in 1.2 for x. So we can run that. So this predict function is returning the estimated linear combination of the predictors. I would actually also note that this is the default behavior, this returning type equals link. I could remove this, run this, and we get the same thing. I'm going to put it back there for contrast but at first glance, this might not be super informative. Again, if we go back and look at the model since this is the quantity that we are returning here. Again, this linear combination of predictors we're estimating. So what that is is an estimate of the log odds ratio but that might not be a super intuitive quantity for most people especially if you haven't worked with odds a lot and log odds a lot. So, what we might want instead is this quantity here. So we'd like an estimate for the probability that y is one given this value of the predictor. So we'd like to plug in 1.2 to this quantity here and we could certainly just take the output here and perform this mathematical operation but instead we can use the predict function again except with type response. This is plugging in this case 1.2 into this expression here. So this is an estimate of the probability that y is one given x is 1.2. Note that we're using type equals response. Don't let that trick you. It's not returning responses zero or ones, it's returning probabilities of a particular response which is probably the wise one. Okay, so we'll plot what we just did there. So, this dashed blue line is the estimated probability that y is one given different values of x and then this orange solid line is the ordinary linear regression fitted values for different values of x. So, what's probably a little bit more interesting is if we go ahead and sort of quickly look at a bunch of different models that is changing the true values of the coefficients here, fitting and seeing what happens. So that's what we'll do. So, I'm going to specify the intercepts I want and the slope I want actually outside of this function so that I can use it later on. But so we'll generate different beta zeros, the intercept and different beta ones, the slopes as we would normally call them will fit logistic regression and then check the coefficients and then we'll do a little plotting, we'll talk about. So, I'm going to start with beta zero of zero and a beta one of one. We see that when we do this, the estimated beta zero is fairly close to zero, the estimated beta one is pretty close to one, so things are looking at okay. So I'm planning two things. I'm plotting the true probability that y is one given x and that is done with the orange solid curve here, so that's using the known values here. Then I'm also plotting the estimated probability that y is one given the values of the predictors and again that's that dash blue line here. Okay, so that looks pretty okay. So, first thing we'll do is well what is the effect of this parameter on this output here? So what I will do is I will first change it from zero to two. But before I do that actually on this plot, let's notice that there's roughly the same amount of zeros for the response and one's in the response. So now I'm going to do is change this to two and we see the fitting is that's fairly close to two and this is fairly close to one things are going all right. Regenerate this plot. So, the thing you should notice here is that well now we have a lot more ones in the data for the response and far fewer zeros. If I change this again, so instead of two let's do something like negative two, run this fitting it's okay, and then we recreate this plot. Now we see far more zeros in the data than ones in the data. So that tells us that this parameter here, it seems to have some control over the proportion of zeros and ones in the data. So it changes back to zero and now we're going to look at the effect of this parameter here. So I'll just run this again. Okay, so we have this positive value for the slope here, the beta one and we see this increase in probability here. So let's increase this in magnitude and we plot this, we see a little disparity between the two but now essentially there's a sharper increase in probability along the way. If we continue to increase this, that relationship and this is fitting better- this relationship will increase even faster. So, this beta one influences how sharp this increase is, sort of the curvature of the curve. If we instead made it negative, you might already know what's going to happen and instead we'll see a decrease in probability that y is one as x increases. If we make it say even more negative, it will be a steeper descent to see here. Okay, so let's go to say more of an extreme example. Let's make this 10, make this negative 10, we see some things there. So, first see a warning that says algorithms did not converge and then we see a warning, fitted probabilities numerically zero or one occurred and then our estimates for the intercept are 273 instead of 10 and our estimate for beta one instead of negative 10 is negative 211. And if we plot this, we can start to get a sense of what's going on. So a couple of things. First, the algorithms did not converge. So the algorithm used to fit this model is an iterative algorithm and for the number of iterations it was allowed to run, it didn't quite converge. We'll see in a later video how to overcome that but I would stress that this is a warning not an error. So our models will still fit, it just didn't converge. So, we'll talk about how that affects this, and we also see this other issue, which is that, fitted probabilities, numerical is zero or one occurred. So, we mentioned that to fit this model, we don't have an analytic solution, we have to use numerical methods, and those methods have issues when things like this happen, which is probability is very very very very very close to one. But again, I would stress that this is a warning not an error, so our model was fit, it's just, we have to be careful about this model now, because again, look at how far off these coefficients are from the truth. Now, so if we come down here, again, the orange curve is the truth, but the dashed blue is the estimated. So, what you might notice is, say in this region here, we're doing pretty okay, and in this region here we're doing pretty okay. But in this region here, sort of, we're potentially very wildly off. So, this model in some sense, it does okay in some regions, and terrible on other regions and these values are wildly off. So, we certainly understand that when we are doing estimation, there's some variability in decimation, we've been seeing that. So, we would expect Beta zero to vary about ten, but this is sort of a whole different issue. It's not just variability in estimation, it's literally an issue with the estimation that's causing this to be wildly off. We can actually take a look at changing the c-value for generating the data, see what happens. So, we see that same error occurs. But this time, this is doing pretty okay. Then, we'll change the c again and we get that same warning, and now we're really far off again. So, essentially when you see this warning, you should be very very very skeptical of your estimated coefficients. We'll see in a later video, that we can potentially still use the results of the logistic regression for the sake of classification, because we see that this dashed blue curves still gives us a sense of, should something be zero or one, it's just really bad with these in-between cases, there's not a lot of those in-between cases. But we'll come back to that later. Instead, we'll move on to looking at some real data. We're going to look at this SA-heart data from this package here, and we'll first just look at the data. But that doesn't really tell us much, just looking at it, instead, we'll look at the documentation. What we have here, is data about males that live in a high-risk region and high risk of heart disease. What we're going to want to look at here is this variable CHD, for whether or not an individual has coronary heart disease. Then, we have some additional variables here, which might be useful for modelling that response. I'm not a medical expert, I don't claim to have any knowledge about heart disease, but I think commonly this LDL variable, this low density lipo-protein cholesterol is considered, what is called, a bad cholesterol. I think that's a massive over-simplification. If I didn't know anything else, I would think that it would be reasonable to try to model whether or not someone has coronary heart disease as a function of LDL. That's the model that we'll look at, we'll try to model the probability that someone has CHD as a function of this LDL. I'll go ahead and fit exactly that model. I will get the coefficients, this coefficient being positive, tells us that the higher the LDL, the higher the log-odds ratio, therefore, the higher the probability that someone has coronary heart disease. That might look better in a plot. We've jittered the data so we can see it, there's a lot of it this time. We see that, maybe not a sharp increase, but certainly an increase we estimate in probability of CHD as LDL cholesterol increases. But the question is, ""is that significant?"" The tests that we'd like to do, is test whether or not that coefficient in front of LDL is actually significant. Is this relationship just due to chance, or are we actually seeing a relationship here? We'll go ahead and we have this fitted model here, and what we always did when we were fitting ordinary linear regressions, was run the summary function, so we'll do that. This should actually look fairly familiar. Ignoring down here and focusing here, this actually looks very similar to what we would have seen before. This line corresponds to the LDL variable, here is the estimate of the coefficient, here is that standard error. The difference now is we have a z-value instead of a t-value, and a probability according to this z, that is the standard normal distribution here for the p-value. And that's because we are dealing with this distribution result here, which is approximate for large sample sizes. The p-value for the test that we're interested in is extremely low, we will reject this null hypothesis, we would actually say this relationship is significant. Okay, but again, I'm not a medical expert, I don't know anything about heart disease, but I would imagine that there's probably additional variables we could use to try to model this response in addition to just LDL, for example, how much someone smokes, whether or not they're obese, age, all of these things, actually this is age and [inaudible] that might not be as important for certain models, but some of these variables should be fairly useful for understanding CHD. What we'll do is compare different models. To do that we're going to use the likelihood ratio tests that we had talked about. Two models, one is nested inside of the other, we have this test statistic here, which approximately follows a chi-square distribution, with the degrees of freedom of the difference of the size of the models. We'll go ahead and fit just model with all of these variables here, and we'll call that the additive model. So, now we have two models, one just with LDL one with all possible predictors. So again, small model, big model and we would like to compare them which would be saying to compare these two models the difference is setting a bunch of the coefficients to be equal to zero. So, I could manually calculate this test statistic, it would look something like this and then I could manually calculate the p-value, but I don't want to do that. So instead, what I'm going to do is returned to the ANOVA function which we had previously used to perform an F-Test to compare two nested linear models, but now we have two nested logistic regressions. So, I'm going to specify the smaller model first, the large-model second, and then I need to also say perform a specific test that is the likelihood ratio test and when we do that we have what's called analysis of deviance table. We'll come back to what deviance is in a little bit. But here we see the value of the test statistic that we had actually already calculated ourselves, but then it also automatically calculates the p-value for us which in this case is extremely small so we would reject the null hypothesis. So, we prefer the larger model here which is no roughly no surprise because it seems that LDL does not explain the relationship, as well as, using additional variables. So, we prefer the model with all the variables of the model with just one variable, but perhaps there's a model with some subset of the variables that is preferred. So, we'd like to do model selection. So, what we'll do is use the step function as we had in ordinary linear regression, we'll supply the additive model and trace zero, we'll just going to suppress some output. But essentially, what this is going to do is work backwards and check AICs to arrive at a selected model. Actually, maybe now we'll run it like this. So again, this starts with an additive logistic regression and works backwards eliminating variables according to AIC. We store that model and we end up with some new smaller model here. You could add additional arguments here to perhaps say build, use a stepwise shooter that goes backwards and forwards or simply forwards and you could also change the PIC through the K argument here, but it's often easiest just to use backwards AIC because it's the default in R. So, some texts I don't want there. So, we could compare this model we selected to the additive model that would be another instance of the ratio test as we have two nested models and we see that we still prefer the smaller of those who model so we like this model that we had selected. So, looking just a little bit further into this model here, we could call the summary function on it and this is just to reiterate now with a multiple regression model. We could still do single predictor tests so each individual line here is performing a test about a single parameter given that the other parameters in the model, much like we had seen in ordinary linear regression. So, something we would like to do is perhaps create confidence intervals. So, here we have the estimates of the model parameters, but again, we're performing estimation here, so there's certainly some variability there. So, perhaps we would like to instead of just use point estimates, where to go, here we go, point estimates here, we'd like to create interval estimates. We do this exactly the same way we did in order to ordinary linear regression, use the confident function, specify the level you want to use, run, there we have it. So, here we had an estimate of 1.6 and change for LDL's coefficient or we could instead use this 99 percent confidence interval here. Something that we also like to be able to do is create a confidence interval for the mean response. So, what I mean by that is, so suppose we have this new observation, and I am supplying values for each of the predictors use, so this is a new person. I can predict this person's probability of CHD, so there's roughly 83 percent chance that this person has CHD. But maybe I want an interval estimate here. So, what we saw as we could first make an interval estimates for the log odds. So, the first thing I need to do is get eta_hat which we have here and by using this se.fit equals true, I can also get the standard error for that. Then if I acquire the correct critical value I could get a confidence interval then for the log odds and I didn't run this code though, there we go. So this is a confidence interval for the log odds, but again that might not be as uninterpretable if you're not super familiar with log odds. So instead, we'll use this inverse logit function from the boot library instead of writing our own which will then convert this interval for the log odds to the probability of Y equals one which in this case is presence of CHD. So, we get an interval estimate instead of a point estimate which is often useful. So, one thing we've been doing that we haven't really specifically noted is that we're using formula syntax. So, what we understand now is that formula syntax is how we specify this linear combination of the predictors. So, if I wanted to say fit this model here that corresponds to this syntax here. So, here we have what we call our main effects which we simply just add to the model. Here we have an interaction which we add like this. Here we have a quadratic term which we add like this. So certainly, things that we've seen before but we just want to specifically say that logistic regression and really any GLMs, what we're doing is specifying this linear combination of the predictors via the form of syntax and also specifying well this is the binary response arrange to them. For example, we see here that this term probably actually wasn't useful because we just randomly threw in a quadratic term that we didn't really have any reason to think it should be there. So, one last thing that's a little bit different between ordinary regression and logistic regression is this deviance thing that keeps popping up. So, we have this null deviance, this residual deviance, if we go back up to what we did in likelihood ratio tests, we call this an analysis of deviance table and we have these deviances here. What is that? So, without going into the details of deviance, again, that's popping up all over the place, and so for example here, you can simply think of deviance as the analog to residual sum of squares. So, it's some measurement of how much error there is in the model. We won't go into the details of it, but we will take a look at the deviances of the three models that we fit. So, this was the model with just a single predictor. This was a model with some intelligently selected predictors via model selection and this was a model with all of the possible predictors. So, those are three models that are nested inside of each other growing in number of parameters, and the thing you might notice is that deviance decreases. That is something similar to what we saw with training root mean squared error or residual sum of squares in ordinary linear regression and that is where we'll stop this video but we'll come back to this relationship in the video when we talk about classification in R."
stat-420,10,7,"Is a credit card transactionreal or fraudulent? Is an email spam? Is an image a cat or a dog? These are all examples of the classification taskin machine learning. While there aremany complex methods for performing classification, logistic regression is both simple and often works very well. After introducing the taskof classification, we will see howlogistic regression can be used in this setting. Then, we'll revisitmodel complexity and use cross-validatedmetrics to assess how well logistic regressionperforms a classification task. Additionally, we'lldiscuss a number of metrics specific toassessing classification including sensitivityand specificity. After this lesson, you'llbe able to perform your own classificationtasks in R."
stat-420,10,8,"So, now we've seen how logistic regression could be used to model data when we have a binary response. We saw how logistic regression focuses a lot on this probability, which is the probability that the response takes the value one conditioned on values for the predictors, but what we haven't done yet is been able to predict the response. We've instead simply focused on what is the probability if the response is zero, one. So, what we will do now is take that probability and then use it to create a prediction. It won't be an extremely surprising result, but one that we should focus on just a little bit. Staying us up back, this is something we did back when we were using ordinary linear regression to model a numeric response. So, first we would fit a model, the usual old ordinary linear regression model, then we would use it to make predictions and then we would evaluate how well those predictions were working. We would look at something like a root mean square error where we compared the actual values to the predicted values, and we discussed how we should use data that was not used to fit the model to do this evaluation, otherwise we would simply always select the largest possible model. So, how do we modify this procedure to move from a numeric response to a categorical response? It's actually very simple. We just have to realize that when we create a model, that is a model for the probabilities, and then use those probabilities to classify to the response it has a highest probability. So, if y equals one has higher probability classified to it, otherwise classified to y equals to zero. We call this procedure classification. Classification is really more broadly just predicting a categorical response, but we're going to do it through the use of a probability model. So, that is a step that we will take. So, if we restrict ourselves to the binary response case, not the broad categorical response case, we can use logistic regression for this test. So, step one, we create a model for the probabilities that is the probabilities that y is one and the probability that y is zero and we assume this model here then we fit the model. So, we have hats everywhere. So, we were estimating model coefficients. Therefore, we have the estimated probability that y is zero condition on x, and then we'll use those to make predictions. So, how do we do that? So, we're going to have what we call a classifier and sometimes it's going to output one, sometimes it's going to output zero. Well, when it should output one? Well, it should output one exactly when there is a high probability of one. That is, let's say, it sensors two cases, a greater than 50% chance, and then in all other cases we will predict or classify to zero, and this just sort of does make sense. If we believe there's a high probability that y is one, let's predict one. Otherwise, predict zero. This makes a lot of sense, but why is this? It's actually due to a result called the Bayes classifier. So, if we actually knew these probabilities, that is condition on some X, what is the probability that y can take any possible value, the Bayes classifier says, well, if you want to minimize the probability of misclassifying, that is predicting the wrong class, simply classify to the probability that is the highest. It seems pretty obvious, but that is a known result. So, in the binary case that looks something like this. So, classify to one when the probability of one is greater than 0.5, otherwise classify to zero. We don't know this probability here that's why we estimate it. So, in practice we can't use the Bayes Classifier, we're generally trying to essentially approximate it or estimate it in some sense. So, here we have the classifier that we're going to use and these estimated probabilities, they will come from some model, in this case we'll use logistic regression. So, that's how we make classifications. That is how we classify. Then the question becomes, well, how do we evaluate? So, essentially, we want to not make a lot of errors. In this case, an error would be when the actual value is not the same as the value that we predict or the classification that we make. Essentially, what we have here is we're summing up all in this classifications we make and then averaging according to how much data we have. So, essentially, this is going to be our metric for how well our model is acting as a classifier. We want as few misclassifications as possible. So, here we see a fitted logistic regression that we had observed earlier. Now, I'm going to add a line here for an estimated probability of 0.5. So, we see that it intersects with estimated probabilities from the model right about here. So, I'm going to draw a vertical line here, try as best possible. Essentially, to the left of this line that I have drawn our estimated probability was greater than 0.5. To the right of this line that I've attempted to draw, we have an estimated probability less than 0.5. So, essentially, over here we're going to classify to one, and over here we're going to classify to zero. So, this point here would be called the decision boundary. We won't talk about decision boundaries too much, but that's a concept that will be talked about in detail and in the machine learning class. We're mostly going to focus on just the ability to actually make these classifications, but we see a couple of things here which is that with this classifier created from this logistic regression, we are going to make some errors. These data points here would be misclassified and these data points down here would also be misclassified. It looks like everything else would be classified correctly. So, those classifiers working fairly well. So, taking a look at another fitted logistic regression that we've seen before, this time the model that had data generated according to some quadratic relationship, and then we fit a model that included a quadratic transform of the predictor. I'll again add this line here for a probability of 0.5. We'll note that there's two intersection points with the estimated probabilities now. So, 2x values. So, I'm setting this down here. So, now the decision boundary actually has two points. So, inside of here, any x value between a little less than negative one and a little greater than one, we would classify to zero. Greater than this point here we'll classify to one and less than this point here, we would classify to one. We again see some misclassifications in this data point here, these two data points here, and it looks like these data points here, maybe this one but I'm not sure. I'm not being very precise here. This should make some sense though. We're obviously not going to get to 100% right. So, for example at an x value right around with a little greater than negative one, so right here, we're estimating a probability that the response is one is 0.3. So, there's clearly some uncertainty here. We very much could observe a value of one, which we do here and we unfortunately misclassify, but when there's uncertainty in the model, there is going to be misclassifications. So, what makes a model good for classification? What makes a model bad for classification? Well, as we have seen before when we were making predictions with ordinal linear regression, we don't want to underfit, we don't want to overfit. We're underfitting when the model is too simple and it does not capture the signal, and we're overfitting when the model is too complex and we're essentially fitting to noise. Looking back here, so had we not fit a model that had the x square determinant here, we would have been underfitting, whereas if we added in a bunch of additional transformations here, we would have been overfitting. Sort of in general, we can talk about how model complexity relates to the misclassification rate. So, if we consider the training data. So, if were looking at our training misclassification rate, it will only ever go down, and this is analogous to what happens with training data and root mean square error. The more complex a model is, we're essentially fitting more and more to noise, whereas if we look at the test misclassification rate, eventually, this will alert us to the fact that we are overfitting. So, the model we want to fit is somewhere with a complexity of about here. So, with logistic regression we can control the complexity via the transformations we're adding and generally just the number of characters that we're adding, but in the grander scheme of classification, there is a lot we can do here. We're only scratching the surface, classification is actually a topic that could take up an entire course and it's the big focus of many machine learning courses. So, if this becomes something that you're interested in, I would highly recommend looking into those resources. But we'll pause and ponder this quote from George Box again, which is that ""All models are wrong; some models are useful."" In this case, the usefulness of a model is based on how well it's classifying. So, since we already had a logistic model fit, it made a lot of sense to evaluate it as a classifier. We'll take a look at that in our in the next couple of videos but I do want to emphasize that classification in and of itself is quite a interesting and deep topic you could spend a lot more time on, but hopefully after the next couple of videos you'll have the general idea."
stat-420,10,9,"So now we'll go ahead and take a look at a very quick introduction to classification in R. I would note again that this video will in no way be comprehensive about classification in R or about classification. We're just trying to get the main idea, trying to get a flavor of the classification task. If you find it interesting, I would highly recommend that you follow up by taking a machine learning course where classification will be discussed in great detail. So, we're going to continue to restrict ourselves to the binary case. You could do classification with more categories in just two but for our purposes we'll consider just the two class case. So this probability that we've been working with will be extremely important. So, what we'd like to do is we'd like to know what is probability and if we did, what we do is we will create a classifier that classifies to one if the probability of one is high that is greater than 0.5 in the binary case or if this probability was low that is probability of one is less than 0.5 classified as zero. Makes sense? This is called the Bayes classifier. Unfortunately or just in reality, we will never actually know this probability. So, what we actually need to do is estimate it and we will do so and then use the estimate instead of the truth and that's what we'll use to create a classifier. So, for our purposes we're going to use logistic regression to estimate this probability but you could use any method that would allow you to estimate that probability, many of which you would see in machine learning class. So, to illustrate this idea, we're going to look at this spam data set from the Kernlab library. So, let's take a look at that. At first glance it just looks like a bunch of numbers, so, this type variable is what we're going to be interested in and there are two types in this dataset. There are spam and non-spam. So, we should probably look at the documentation here. So, each observation is an email, the various predictors are characteristics of the email. So, for example, here's a variable for total number of capital letters in the email. Let's see a variable indicating how many dollar signs are used, various bits of information like that you'll want to check out the documentation. One thing that we should note here, let's see where is it in the documentation, does it say it somewhere? So this data set was collected at Hewlett-Packard Labs. We have about 4600 emails, they're either labeled as spam or non-spam. Fifty seven variables again word frequency, character frequency. I think the bit of missing information here which we could track down if we actually went to the original source as this is a rather old dataset. So, this dataset would've been made I believe in the 90s? But essentially, we need to note that email was less sophisticated. For example, I have a feeling this variable here total number of capital letters that's going to be rather important spam email, I believe at the time had potentially a lot of capital letters in it. Whereas now, people sending spam email understand spam better so they try to mask that it's spam all that more. I think there are also some very specific variable. So, let's see if I find it. George. So for example, there's this variable called George. If you're building a general spam classifier or spam detectors, we can call it. This George variable pi wouldn't be as important. So, these emails were collected from emails that were sent to various specific people, so at the time spam would have been very sophisticated. So, someone sending this George person an email probably would have used George and the email to George where the spammer would not. So this could be a useful variable in this data set but it will not be useful for creating a general spam detector. So you just want to keep that in mind. This dataset is useful for illustrative purposes but you would never want to use it to create a general spam detector for use in practice. Okay, but back to the task at hand, we essentially want to predict whether or not an email is spam or non-spam. I might start slipping up and calling non-spam ham because that's something that's done often in these datasets but I'll try to keep saying non-spam but if I say ham, I mean an email that is not spammed. Okay, so we see a difference in this data set than the previous data sets we've used logistic regression with which is that this variable is a factor variable. We will have to pay attention to that, it is a factor variable with two levels non-spam and spam. So what's going to happen is underneath the model fitting this variable will be recognized as zero one and if you remember back to when we first started talking about dummy variables, the first level will correspond to zero, so non-spam will be zero and the second level spam that will be one, so you'll want to keep that in mind. Okay, so because we're dealing with prediction here for some of the same reasons that we saw before we're going to split up the data into a training set and a test set. So, I'll do that real quick. You might notice that the training set only has a thousand observations, whereas the test set has roughly 3600. Now, this is not necessarily what you want to do in practice, you probably just want to reverse this split. Maybe you like 80% in the training set and maybe 20% in the testing set or something like that but for the sake of these models running fairly fast, I'm going to keep this number sort of artificially low so that these don't take a long time to run. Okay, so we're going to look at four different models, one that only uses the capital letter variable, one that uses a reasonably chosen set of predictors. One that uses all available predictors which called the additive model and one that I'm going to ahead of time called the over fitting model. Which uses all the predictors and the interaction between this total variable and all the other predictors. So, one thing to note is that these models are all nested. I'm going to go ahead and fit all of them, when we're going to fit to the training data. So I did the first one. Okay, so we see this warning this time and remember we had said that. Okay, so this warning suggests that if we look at this model, we should be suspicious of these estimates of the model parameters but it's still reasonable to evaluate using this model for a classifier. So, we're suspicious of this model but we're going to evaluate it for the task at hand which is classification. So, while that warning is important to recognize, we're still going to evaluate the use of this model on its merits as a classifier in a second. Okay, the same thing happens when we fit the additive model but again same story, we're going to evaluate it as a classifier and we're not actually interested in the estimates of the coefficients, we're not interested in which variables are significant. So we wouldn't be doing tests that would be unreliable because these coefficients are highly suspicious. We are mot doing inference where interested in making predictions. So, we're going to evaluate how good we are at making predictions. So, that's just something to keep in mind and this last model, you might notice I have this additional argument here called centrally maximum iterations of the fitting. So, if I were to remove this we would get an additional warning which says that the algorithms are not converged because the default number of iterations was not enough to autistic converge. So, essentially I have adjusted iteration's such that it will actually converge. Okay, so we have these one, two, three, four logistic regressions and now we would like to use each of them as a classifier. So to create a classifier with them essentially, we're going to use their predictive probabilities. I make classifications according to this expression here, then what we will use to evaluate them first is the misclassification rate. So, essentially we want to know how often are we making incorrect classifications? Either labeling spam as non-spam or labeling spam as non-spam. So, essentially, what we'll do here is we'll actually take this step-by-step. This line there's a lot happening here, so let's break it down actually. Okay, so the first thing I'm going to do is make predictions. Now, I'm going to regret that so it's going to spit out a lot of values here but while that's happening, we'll discuss what's happening here. So, remember that by default if I just call predict without giving it additional data, it will use whatever data was used to fit this model. So, these are predictions for each of the thousand observations that were used to fit this model but what are these values? They are not the estimated probability that we're interested in, they are the log odds. So, a predictive probability of 0.5 corresponds to an estimated log odds of zero. So, essentially what I want to know is if we have a high log odds here that is greater than zero, we would want to predict spam. Whereas, if we have a low log odds here we want to predict non-spam. We can actually changes a little bit and it would be equivalent and what I can do is say, type equals response. Where did it go? Here we are and now we see these are actually predicted probabilities. It's fairly better to take a look at this one first. So, essentially what we have now is this value for each value of X and so what I want to do is say, well if these are greater than 0.5, I would want to return a particular value. And so what I want to do now is create an if else statement. Let's do it like this though, to start. So, what do we have here? So, if the predicted probability of spam is greater than 0.5, return spam if not return non-spam. So, this is essentially making classifications using this model on the training data. Recall that, this probability which are being returned here is a probability of one which in this case is spam due to the levels of that factor we always talk about. So if we take a look at this, we see all those predictions. I'll wait for that to finish, but then something we'll do real fast is say okay, an equivalent way of doing this would be to not compare to the response, but instead to log odds and those being greater than zero. Here's what we'll do. Let's say are these all the same. So, I will do, was it all dot equal, and we want to compare this to this. So these should be equivalent expressions MAR. So again, these are the classifications for this model, and then we're going to see how many of them are not equal to the true value and take the average of those and we'll do that for all four models. So running this, so this first model the one and only had a single predictor has a miss classification rate and the training of 0.34, the selected model has 0.21, the additive model 0.064 and the huge model with all the interaction terms, 0.063. So the model that has the lowest misclassification rate in the training data is the biggest model. This should sort of be no surprise. This is because the bigger the model is the fewer errors it makes. But that doesn't indicate to us which of these will make predictions on unseen data well. Essentially, this gives us no indication of if any of these models are overfitting. This is the same phenomenon we saw in regression where the bigger the model the smaller the residual sum of squares or training root mean squared error. So, how do we overcome that in regression? We looked at either cross-validated metrics or test metrics. So that's what we'll do here. We will cross validate these models. So cross-validation with logistic regression, we'll want to use the cv.glm function from the boot library. So the way cv.glm works is you have to give it the model you want to cross-validate, you want to give it the data that you need to cross-validate on, and then the number of folds. So back in ordinary linear regression we were able to automatically and quickly do leave-one-out cross-validation due to a nice old trick, but that trick doesn't exist in logistic regression. So we'll want to use either 5-fold or 10-fold cross-validation, and essentially this will require fitting five times so it will be a little bit slower, but it will give us an average of a misclassification rate over five different held out sets. We're skipping a lot of the details here but essentially, this will be a useful metric whereas this training misclassification rate not so much. I'm going to run this, and what do we see? So, the worst cross-validated misclassification rate was for the smallest model, and then it gets better as model gets bigger and then it eventually gets worse. That's why we're saying this model is overfitting relative to the best model which was the additive model, and these two models are underfitting relative to this model. So between these four models, add them all right now, we like the additive models. So misclassification rate is certainly one metric we should look at. We want to be making as few errors as possible especially on unseen data, so either via cross-validation or just using a single held out test set. Which we still have some test data that we haven't used up or are about to. So it turns out that not all errors are created equal. So we have here is what's called a confusion matrix, so we can compare the actual values so either we use false and zero interchangeably, and true and one interchangeably versus the predicted values. So if the true value is one and the predicted value is one, that's what we call a true positive, we like that or if the actual value is zero and the predicted value zero, that's true negatively like that, but there's two types of errors. If the actual value is one and we predict a zero that's a false negative whereas if the actual value is false and we return or predict a true, that will be false positive. So, whether or not a false positive or a false negative is worse depends on the situation. So in this case, the positives, the trues, the ones are spam and the falses, the zeros are non-spam. So, consider the two errors here. First consider a false negative, so that is we have an email that truly is spam, but we label it, we predict, we classify it to be non-spam. That's an error, but this is an error that's not the worst thing in the world, essentially, we have a spam email that will hit our inbox and we are going to have to delete it, hopefully we don't click any links that spread a virus or anything, but it's not the worst error in the world. Whereas suppose we consider false positives, so an email is non-spam, when we label it as spam. What happens with that? Well, that email goes to our spam folder which we're probably not checking and then gets automatically deleted at some point and we never see it. That's a problem, it could be benign problem, it could be an email that's not super important, or it could maybe be an email with an overdue bill notice or a job offer that you really need to see. So in this situation and the email situation, I think we would be a little bit more worried about false positives instead about false negatives. So this is highly situationally dependent and we could easily just also just flip spam and ham and you'd have the opposite result. But it really depends on what you label as true with which one of these errors matters more. So, let's take a look at our classifier that we liked here in terms of these two new errors. So, first we'll do is actually make a confusion matrix, and this is what we were talking about before, a log odds ratio of zero is equivalent to a probability of 0.5 so these two bits of code would both create the same classifier. So when we continue on now, we're only going to look at metrics on the test data set. So, we don't want to use any of the data used to fit these models to select this model via cross-validation, we want to use unseen data to evaluate false positives and false negatives. So, here we're creating classifications on the test data with our chosen model and we're storing those results. So, using this little function I created to create this table, let's take a look at the result here. So we see a lot of truly non-spam emails being marked as non-spam, we see a lot of spam emails being marked as spam. About here on this diagonal we have the errors, so these are true spams that are marked as non-spam that we're not too worried about, and here we have non-spam emails are marked as spam, we don't like that. So we want to introduce three new ideas now, the prevalence, the sensitivity and specificity. So, the prevalence is just the prevalence of the class labeled as one. So, the total number of positives in the dataset divided by the total number of observations. So, the prevalence in this dataset is about 40 percent. So, we have about 40 percent spam in this dataset. So, one thing we need to realize is that there's a really obvious classifier we can create here, which is that, okay, just look at your data and classify to the majority class, which in this case is the non-spam. But so if we did that, if we just said we're going to be real naive here, we're just going to go naive majority rule here, so there's more non-spam in this dataset. We're just going to classify everything to non-spam. This would be our error rate. So, basically, if we don't get an error rate better than this or models pride pretty bad. But so, taking a look at the error rate for our chosen model in the test data, we see that it gets an error rate of about eight percent, which sometimes instead of misclassification rate, people will report the accuracy, which is just one minus that so about 92 percent. So, obviously, this classifier using that additive logistic regression is doing much better than simply guessing based on the majority class, which would have had this error rate. So, this is much lower than this. So, we like fat, that's a decent start. Okay. But so, now, let's consider a new metric called sensitivity, which is basically the true positive rate. So, we're looking at the number of true positives divided by the number of positives in the data set. If you come back up here, so it's this quantity divided by the total number of things in this columns. So, essentially, when sensitivity is high, the number of false negatives is low. So, sensitivity is basically this quantity divided by the total in this column. So, high sensitivity is low false negatives. So, I'll write this function to get the sensitivity based on the confusion matrix that we had created. But before I actually run the function, we'll introduce another metric called specificity. So, specificity is the true negative rate. So, the number of true negatives divided by the total number of negatives, so essentially, back up here, true negative. So, this quantity divided by the total number in this column, which is to say that when specificity is high, the number of false positives is low. We'll write a function that does that based on the confusion matrix. So, we'll get these two values for the classifier we're dealing with. So, we have, let's see here, a sensitivity of about 88 and a specificity of about 93. Okay. So, in this case, because we said we want to guard against false positives, because that would be labeling non-spam as spam, so we want this number to be high. So, it turns off, there's a trade off between these two quantities. You can sacrifice a little bit of specificity to get more sensitivity or you can sacrifice a little bit of sensitivity to get more specificity. We can do this well, with the logistic regression we already have. So, the natural thing to do is to use this 0.5 as a cutoff. If the probability that something is spam is greater than 0.5, label it as spam, but 0.5 isn't the only number. 0.5 is the number if we want to essentially make the misclassification rate as low as possible. But if we're willing to sacrifice this overall rate for one of the two specific rates, we can make this potentially better. So, essentially, we can create a classifier like this, so, instead of using 0.5 as the cutoff for classifying to one, the bolding here isn't quite correct, now, looks better, so, instead of using 0.5 here, we could say perhaps lower the threshold. Maybe we label for something is spam if there's a 10 percent chance or better, which is what we could do here. So, this is creating a slightly different classifier using the same logistic regression. So, we're getting the same predicted probabilities, but now, anything with a predicted probabilities spam greater than 0.1, we will label spam, everything else non-spam. We can create a confusion matrix with that. So, we've greatly reduced the false negatives, right? So, greatly, there's a false negatives, but with a lot more false positives. So, now, as she wanted to do, and this makes a lot sense, right? So, this is essentially just decreasing threshold to label something or to predict something as spam. So, of course, we're going to get a lot more false positives. So, we see that our specificity has gone down a lot at an increase sensitivity, but we want to do actually the exact opposite of that. So, what we'll instead do is we'll increase the threshold for something being labeled as spam. So, instead of 0.5, we'll try 0.9. So, essentially, we have to be very certain of something being spam to classify it to spam. So, we are essentially creating classifications using a new classifier, but the same logistic regression again, and we can make a confusion matrix based on that. So, now, we have a lot more false negatives, but far fewer false positives partially because we're simply predicting less things to be labeled as spam. We'll see that we get much lower sensitivity at the trade-off of much greater specificity. Now, we should sort of note that we did just increase the specificity, but is this a good spam classifier? Again, still probably no. One because in practice, this rate is probably not high enough, and two because again, this dataset was sort of highly specific to both a time, a company, and a few individuals, so we would not want to actually use this in practice. I would also note that a lot of the functions I've written here and sort of ways I've done this are, you certainly can use them in practice, but part of the reason I wrote some of these very specific functions were to illustrate what's going on here, so you can see exactly how these things are being done. Whereas, if you wanted to do machine learning in practice using R, you probably want to work with a framework for doing so. If after this course in this video, you're interested in those kind of things, I would highly recommend looking into the current package. This is a package that would allow you to, not just say, fit things like logistic regression, but also evaluated in a machine learning classification framework, so it would potentially allow you to recreate some of the sections down here, but a little bit faster. The trade-off being, it will sort of abstract away some of what we are doing here. So, it wouldn't be as useful for illustration, but I would highly recommend that you look into that."
