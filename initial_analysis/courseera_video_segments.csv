key,course_id,week_nbr,video_id,timeline_start,timeline_end,segment_nbr,segment_link,segment_txt
cs-410_1_1_1,cs-410,1,1,"00:00:00,008","00:00:04,018",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_1_2,cs-410,1,1,"00:00:09,625","00:00:12,226",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,>> This lecture is about Natural Language
cs-410_1_1_3,cs-410,1,1,"00:00:12,226","00:00:13,732",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,of Content Analysis.
cs-410_1_1_4,cs-410,1,1,"00:00:13,732","00:00:15,569",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"As you see from this picture,"
cs-410_1_1_5,cs-410,1,1,"00:00:15,569","00:00:19,540",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,this is really the first step
cs-410_1_1_6,cs-410,1,1,"00:00:19,540","00:00:22,060",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,Text data are in natural languages.
cs-410_1_1_7,cs-410,1,1,"00:00:22,060","00:00:26,820",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,So computers have to understand
cs-410_1_1_8,cs-410,1,1,"00:00:26,820","00:00:29,380",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,in order to make use of the data.
cs-410_1_1_9,cs-410,1,1,"00:00:29,380","00:00:32,000",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,So that's the topic of this lecture.
cs-410_1_1_10,cs-410,1,1,"00:00:32,000","00:00:33,910",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,We're going to cover three things.
cs-410_1_1_11,cs-410,1,1,"00:00:33,910","00:00:36,430",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"First, what is natural"
cs-410_1_1_12,cs-410,1,1,"00:00:36,430","00:00:41,740",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,which is the main technique for processing
cs-410_1_1_13,cs-410,1,1,"00:00:43,150","00:00:46,420",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,The second is the state of
cs-410_1_1_14,cs-410,1,1,"00:00:46,420","00:00:48,350",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,natural language processing.
cs-410_1_1_15,cs-410,1,1,"00:00:49,540","00:00:53,430",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,Finally we're going to cover the relation
cs-410_1_1_16,cs-410,1,1,"00:00:53,430","00:00:54,900",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,text retrieval.
cs-410_1_1_17,cs-410,1,1,"00:00:54,900","00:00:57,280",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,"First, what is NLP?"
cs-410_1_1_18,cs-410,1,1,"00:00:57,280","00:01:02,240",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,Well the best way to explain it
cs-410_1_1_19,cs-410,1,1,"00:01:02,240","00:01:05,860",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,a text in a foreign language
cs-410_1_1_20,cs-410,1,1,"00:01:06,980","00:01:10,907",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,Now what do you have to do in
cs-410_1_1_21,cs-410,1,1,"00:01:10,907","00:01:13,172",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,This is basically what
cs-410_1_1_22,cs-410,1,1,"00:01:13,172","00:01:17,580",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,So looking at the simple sentence like
cs-410_1_1_23,cs-410,1,1,"00:01:18,730","00:01:22,250",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,We don't have any problems
cs-410_1_1_24,cs-410,1,1,"00:01:22,250","00:01:25,930",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,But imagine what the computer would
cs-410_1_1_25,cs-410,1,1,"00:01:25,930","00:01:27,830",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,"Well in general,"
cs-410_1_1_26,cs-410,1,1,"00:01:27,830","00:01:34,310",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"First, it would have to know dog"
cs-410_1_1_27,cs-410,1,1,"00:01:34,310","00:01:38,410",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"So this is called lexical analysis,"
cs-410_1_1_28,cs-410,1,1,"00:01:38,410","00:01:42,230",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,we need to figure out the syntactic
cs-410_1_1_29,cs-410,1,1,"00:01:42,230","00:01:43,930",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,So that's the first step.
cs-410_1_1_30,cs-410,1,1,"00:01:43,930","00:01:48,060",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"After that, we're going to figure"
cs-410_1_1_31,cs-410,1,1,"00:01:48,060","00:01:50,370",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,"So for example, here it shows that A and"
cs-410_1_1_32,cs-410,1,1,"00:01:50,370","00:01:54,260",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,the dog would go together
cs-410_1_1_33,cs-410,1,1,"00:01:55,730","00:01:59,500",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,And we won't have dog and is to go first.
cs-410_1_1_34,cs-410,1,1,"00:01:59,500","00:02:02,969",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,And there are some structures
cs-410_1_1_35,cs-410,1,1,"00:02:04,470","00:02:09,650",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,But this structure shows what we might
cs-410_1_1_36,cs-410,1,1,"00:02:09,650","00:02:11,850",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,try to interpret the sentence.
cs-410_1_1_37,cs-410,1,1,"00:02:11,850","00:02:13,960",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,"Some words would go together first, and"
cs-410_1_1_38,cs-410,1,1,"00:02:13,960","00:02:15,640",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,then they will go together
cs-410_1_1_39,cs-410,1,1,"00:02:16,640","00:02:20,200",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,So here we show we have noun phrases
cs-410_1_1_40,cs-410,1,1,"00:02:20,200","00:02:21,500",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,then verbal phrases.
cs-410_1_1_41,cs-410,1,1,"00:02:21,500","00:02:23,670",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,Finally we have a sentence.
cs-410_1_1_42,cs-410,1,1,"00:02:23,670","00:02:25,430",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,And you get this structure.
cs-410_1_1_43,cs-410,1,1,"00:02:25,430","00:02:29,400",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,We need to do something called
cs-410_1_1_44,cs-410,1,1,"00:02:29,400","00:02:31,610",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,And we may have a parser
cs-410_1_1_45,cs-410,1,1,"00:02:31,610","00:02:34,880",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,that would automatically
cs-410_1_1_46,cs-410,1,1,"00:02:34,880","00:02:38,220",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,At this point you would know
cs-410_1_1_47,cs-410,1,1,"00:02:38,220","00:02:40,440",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,still you don't know
cs-410_1_1_48,cs-410,1,1,"00:02:40,440","00:02:44,060",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,So we have to go further
cs-410_1_1_49,cs-410,1,1,"00:02:44,060","00:02:47,120",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,In our mind we usually can map
cs-410_1_1_50,cs-410,1,1,"00:02:47,120","00:02:51,330",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,such a sentence to what we already
cs-410_1_1_51,cs-410,1,1,"00:02:51,330","00:02:53,970",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,"For example, you might imagine"
cs-410_1_1_52,cs-410,1,1,"00:02:53,970","00:02:56,800",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,There's a boy and
cs-410_1_1_53,cs-410,1,1,"00:02:56,800","00:02:59,860",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,But for a computer would have
cs-410_1_1_54,cs-410,1,1,"00:03:00,890","00:03:05,232",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,We'd use a symbol (d1) to denote a dog.
cs-410_1_1_55,cs-410,1,1,"00:03:05,232","00:03:10,430",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,And (b)1 can denote a boy and
cs-410_1_1_56,cs-410,1,1,"00:03:12,650","00:03:15,440",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,Now there is also a chasing
cs-410_1_1_57,cs-410,1,1,"00:03:15,440","00:03:19,130",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,we have a relationship chasing
cs-410_1_1_58,cs-410,1,1,"00:03:19,130","00:03:23,909",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,So this is how a computer would obtain
cs-410_1_1_59,cs-410,1,1,"00:03:25,920","00:03:31,590",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,Now from this representation we could
cs-410_1_1_60,cs-410,1,1,"00:03:31,590","00:03:35,960",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,and we might indeed naturally think of
cs-410_1_1_61,cs-410,1,1,"00:03:35,960","00:03:37,470",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,this is called inference.
cs-410_1_1_62,cs-410,1,1,"00:03:37,470","00:03:42,490",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"So for example, if you believe"
cs-410_1_1_63,cs-410,1,1,"00:03:42,490","00:03:46,180",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,"this person might be scared,"
cs-410_1_1_64,cs-410,1,1,"00:03:46,180","00:03:50,880",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,you can see computers could also
cs-410_1_1_65,cs-410,1,1,"00:03:50,880","00:03:54,080",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,So this is some extra knowledge
cs-410_1_1_66,cs-410,1,1,"00:03:54,080","00:03:56,430",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,some understanding of the text.
cs-410_1_1_67,cs-410,1,1,"00:03:56,430","00:04:02,280",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,You can even go further to understand
cs-410_1_1_68,cs-410,1,1,"00:04:02,280","00:04:05,000",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,So this has to do as a use of language.
cs-410_1_1_69,cs-410,1,1,"00:04:05,000","00:04:08,740",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,This is called pragmatic analysis.
cs-410_1_1_70,cs-410,1,1,"00:04:08,740","00:04:13,910",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,In order to understand the speak
cs-410_1_1_71,cs-410,1,1,"00:04:13,910","00:04:18,370",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,We say something to
cs-410_1_1_72,cs-410,1,1,"00:04:18,370","00:04:19,440",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,There's some purpose there.
cs-410_1_1_73,cs-410,1,1,"00:04:19,440","00:04:22,100",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,And this has to do with
cs-410_1_1_74,cs-410,1,1,"00:04:22,100","00:04:24,750",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,In this case the person who said
cs-410_1_1_75,cs-410,1,1,"00:04:24,750","00:04:29,200",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,this sentence might be reminding
cs-410_1_1_76,cs-410,1,1,"00:04:29,200","00:04:31,410",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,That could be one possible intent.
cs-410_1_1_77,cs-410,1,1,"00:04:33,020","00:04:36,500",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,To reach this level of
cs-410_1_1_78,cs-410,1,1,"00:04:36,500","00:04:41,390",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,all of these steps and
cs-410_1_1_79,cs-410,1,1,"00:04:41,390","00:04:46,940",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,these steps in order to completely
cs-410_1_1_80,cs-410,1,1,"00:04:46,940","00:04:49,560",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,Yet we humans have no trouble
cs-410_1_1_81,cs-410,1,1,"00:04:49,560","00:04:51,430",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,we instantly would get everything.
cs-410_1_1_82,cs-410,1,1,"00:04:52,790","00:04:53,760",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,There is a reason for that.
cs-410_1_1_83,cs-410,1,1,"00:04:53,760","00:04:57,430",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,That's because we have a large
cs-410_1_1_84,cs-410,1,1,"00:04:57,430","00:05:01,890",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,we can use common sense knowledge
cs-410_1_1_85,cs-410,1,1,"00:05:01,890","00:05:06,330",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,Computers unfortunately are hard
cs-410_1_1_86,cs-410,1,1,"00:05:06,330","00:05:08,430",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,They don't have such a knowledge base.
cs-410_1_1_87,cs-410,1,1,"00:05:08,430","00:05:12,520",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,They are still incapable of doing
cs-410_1_1_88,cs-410,1,1,"00:05:14,290","00:05:18,430",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,so that makes natural language
cs-410_1_1_89,cs-410,1,1,"00:05:18,430","00:05:21,540",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,But the fundamental reason why natural
cs-410_1_1_90,cs-410,1,1,"00:05:21,540","00:05:25,430",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,computers is simply because natural
cs-410_1_1_91,cs-410,1,1,"00:05:25,430","00:05:26,430",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,computers.
cs-410_1_1_92,cs-410,1,1,"00:05:26,430","00:05:30,960",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,Natural languages are designed for
cs-410_1_1_93,cs-410,1,1,"00:05:30,960","00:05:33,480",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,There are other languages designed for
cs-410_1_1_94,cs-410,1,1,"00:05:33,480","00:05:36,220",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,"For example, programming languages."
cs-410_1_1_95,cs-410,1,1,"00:05:36,220","00:05:38,780",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,"Those are harder for us, right?"
cs-410_1_1_96,cs-410,1,1,"00:05:38,780","00:05:43,690",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,So natural languages is designed to
cs-410_1_1_97,cs-410,1,1,"00:05:43,690","00:05:46,770",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"As a result,"
cs-410_1_1_98,cs-410,1,1,"00:05:46,770","00:05:49,540",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,because we assume everyone
cs-410_1_1_99,cs-410,1,1,"00:05:49,540","00:05:56,250",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,We also keep a lot of ambiguities because
cs-410_1_1_100,cs-410,1,1,"00:05:56,250","00:06:02,020",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,know how to decipher an ambiguous word
cs-410_1_1_101,cs-410,1,1,"00:06:02,020","00:06:05,320",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,There's no need to demand different
cs-410_1_1_102,cs-410,1,1,"00:06:05,320","00:06:08,820",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,We could overload the same word with
cs-410_1_1_103,cs-410,1,1,"00:06:10,460","00:06:14,350",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,Because of these reasons this makes every
cs-410_1_1_104,cs-410,1,1,"00:06:14,350","00:06:17,520",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"difficult for computers,"
cs-410_1_1_105,cs-410,1,1,"00:06:18,780","00:06:22,060",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,And common sense and reasoning is
cs-410_1_1_106,cs-410,1,1,"00:06:23,800","00:06:26,300",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,So let me give you some
cs-410_1_1_107,cs-410,1,1,"00:06:27,505","00:06:29,350",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,Consider the word level ambiguity.
cs-410_1_1_108,cs-410,1,1,"00:06:30,730","00:06:34,510",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,The same word can have
cs-410_1_1_109,cs-410,1,1,"00:06:34,510","00:06:36,780",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,For example design can be a noun or
cs-410_1_1_110,cs-410,1,1,"00:06:39,270","00:06:42,160",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,The word of root may
cs-410_1_1_111,cs-410,1,1,"00:06:42,160","00:06:45,120",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So square root in math sense or
cs-410_1_1_112,cs-410,1,1,"00:06:46,450","00:06:49,464",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,You might be able to think
cs-410_1_1_113,cs-410,1,1,"00:06:49,464","00:06:52,609",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,There are also syntactical ambiguities.
cs-410_1_1_114,cs-410,1,1,"00:06:52,609","00:06:56,932",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"For example, the main topic of this"
cs-410_1_1_115,cs-410,1,1,"00:06:56,932","00:07:01,480",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,can actually be interpreted in two
cs-410_1_1_116,cs-410,1,1,"00:07:01,480","00:07:03,900",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,Think for a moment and
cs-410_1_1_117,cs-410,1,1,"00:07:03,900","00:07:09,560",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,We usually think of this as
cs-410_1_1_118,cs-410,1,1,"00:07:09,560","00:07:13,991",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,but you could also think of this as do
cs-410_1_1_119,cs-410,1,1,"00:07:16,130","00:07:20,440",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,So this is an example
cs-410_1_1_120,cs-410,1,1,"00:07:20,440","00:07:23,190",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,What we have different is
cs-410_1_1_121,cs-410,1,1,"00:07:24,510","00:07:27,480",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,applied to the same sequence of words.
cs-410_1_1_122,cs-410,1,1,"00:07:27,480","00:07:31,730",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,Another common example of an ambiguous
cs-410_1_1_123,cs-410,1,1,"00:07:31,730","00:07:34,480",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,A man saw a boy with a telescope.
cs-410_1_1_124,cs-410,1,1,"00:07:34,480","00:07:37,810",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,"Now in this case the question is,"
cs-410_1_1_125,cs-410,1,1,"00:07:38,820","00:07:42,700",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,This is called a prepositional
cs-410_1_1_126,cs-410,1,1,"00:07:42,700","00:07:45,030",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,PP attachment ambiguity.
cs-410_1_1_127,cs-410,1,1,"00:07:45,030","00:07:50,000",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,Now we generally don't have a problem with
cs-410_1_1_128,cs-410,1,1,"00:07:50,000","00:07:54,340",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,background knowledge to help
cs-410_1_1_129,cs-410,1,1,"00:07:55,380","00:07:57,961",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,Another example of difficulty
cs-410_1_1_130,cs-410,1,1,"00:07:57,961","00:08:03,290",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,So think about the sentence John
cs-410_1_1_131,cs-410,1,1,"00:08:03,290","00:08:07,632",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,The question here is does
cs-410_1_1_132,cs-410,1,1,"00:08:07,632","00:08:10,803",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So again this is something that
cs-410_1_1_133,cs-410,1,1,"00:08:10,803","00:08:12,540",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,the context to figure out.
cs-410_1_1_134,cs-410,1,1,"00:08:12,540","00:08:15,470",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,"Finally, presupposition"
cs-410_1_1_135,cs-410,1,1,"00:08:15,470","00:08:18,110",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,"Consider the sentence,"
cs-410_1_1_136,cs-410,1,1,"00:08:18,110","00:08:20,710",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,Now this obviously implies
cs-410_1_1_137,cs-410,1,1,"00:08:22,430","00:08:27,000",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,So imagine a computer wants to understand
cs-410_1_1_138,cs-410,1,1,"00:08:27,000","00:08:30,750",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,It would have to use a lot of
cs-410_1_1_139,cs-410,1,1,"00:08:30,750","00:08:35,890",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,It also would have to maintain a large
cs-410_1_1_140,cs-410,1,1,"00:08:35,890","00:08:41,940",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,words and how they are connected to our
cs-410_1_1_141,cs-410,1,1,"00:08:41,940","00:08:44,130",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,So this is why it's very difficult.
cs-410_1_1_142,cs-410,1,1,"00:08:45,530","00:08:49,110",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,"So as a result, we are steep not perfect,"
cs-410_1_1_143,cs-410,1,1,"00:08:49,110","00:08:54,240",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,in fact far from perfect in understanding
cs-410_1_1_144,cs-410,1,1,"00:08:54,240","00:09:00,200",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,So this slide sort of gains a simplified
cs-410_1_1_145,cs-410,1,1,"00:09:01,580","00:09:06,640",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,We can do part of speech
cs-410_1_1_146,cs-410,1,1,"00:09:06,640","00:09:09,610",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,I showed 97% accuracy here.
cs-410_1_1_147,cs-410,1,1,"00:09:09,610","00:09:13,830",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,Now this number is obviously
cs-410_1_1_148,cs-410,1,1,"00:09:13,830","00:09:15,680",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,don't take this literally.
cs-410_1_1_149,cs-410,1,1,"00:09:15,680","00:09:18,210",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,This just shows that we
cs-410_1_1_150,cs-410,1,1,"00:09:18,210","00:09:20,320",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,But it's still not perfect.
cs-410_1_1_151,cs-410,1,1,"00:09:20,320","00:09:23,620",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"In terms of parsing,"
cs-410_1_1_152,cs-410,1,1,"00:09:23,620","00:09:27,800",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,That means we can get noun phrase
cs-410_1_1_153,cs-410,1,1,"00:09:27,800","00:09:31,106",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"or some segment of the sentence, and"
cs-410_1_1_154,cs-410,1,1,"00:09:31,106","00:09:33,439",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,this dude correct them in
cs-410_1_1_155,cs-410,1,1,"00:09:34,470","00:09:39,310",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"And in some evaluation results,"
cs-410_1_1_156,cs-410,1,1,"00:09:39,310","00:09:43,140",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,accuracy in terms of partial
cs-410_1_1_157,cs-410,1,1,"00:09:43,140","00:09:46,910",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"Again, I have to say these numbers"
cs-410_1_1_158,cs-410,1,1,"00:09:46,910","00:09:50,300",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,"In some other datasets,"
cs-410_1_1_159,cs-410,1,1,"00:09:50,300","00:09:54,230",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,Most of the existing work has been
cs-410_1_1_160,cs-410,1,1,"00:09:54,230","00:09:59,800",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,And so a lot of these numbers are more or
cs-410_1_1_161,cs-410,1,1,"00:09:59,800","00:10:02,980",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"Think about social media data,"
cs-410_1_1_162,cs-410,1,1,"00:10:05,460","00:10:07,860",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,"In terms of a semantical analysis,"
cs-410_1_1_163,cs-410,1,1,"00:10:07,860","00:10:13,730",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,we are far from being able to do
cs-410_1_1_164,cs-410,1,1,"00:10:13,730","00:10:16,430",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,But we have some techniques
cs-410_1_1_165,cs-410,1,1,"00:10:16,430","00:10:18,880",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,do partial understanding of the sentence.
cs-410_1_1_166,cs-410,1,1,"00:10:18,880","00:10:22,360",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,So I could mention some of them.
cs-410_1_1_167,cs-410,1,1,"00:10:22,360","00:10:27,190",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,"For example, we have techniques that can"
cs-410_1_1_168,cs-410,1,1,"00:10:27,190","00:10:30,310",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,relations mentioned in text articles.
cs-410_1_1_169,cs-410,1,1,"00:10:30,310","00:10:34,766",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,"For example,"
cs-410_1_1_170,cs-410,1,1,"00:10:34,766","00:10:38,606",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"locations, organizations, etc in text."
cs-410_1_1_171,cs-410,1,1,"00:10:38,606","00:10:40,930",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,So this is called entity extraction.
cs-410_1_1_172,cs-410,1,1,"00:10:40,930","00:10:42,950",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,We may be able to recognize the relations.
cs-410_1_1_173,cs-410,1,1,"00:10:42,950","00:10:46,140",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,"For example,"
cs-410_1_1_174,cs-410,1,1,"00:10:46,140","00:10:51,340",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,this person met that person or
cs-410_1_1_175,cs-410,1,1,"00:10:51,340","00:10:54,350",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,Such relations can be extracted by using
cs-410_1_1_176,cs-410,1,1,"00:10:54,350","00:10:57,230",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,the computer current
cs-410_1_1_177,cs-410,1,1,"00:10:57,230","00:11:00,170",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,They're not perfect but
cs-410_1_1_178,cs-410,1,1,"00:11:00,170","00:11:02,015",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,Some entities are harder than others.
cs-410_1_1_179,cs-410,1,1,"00:11:03,040","00:11:05,907",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,We can also do word sense
cs-410_1_1_180,cs-410,1,1,"00:11:05,907","00:11:10,446",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,We have to figure out whether this word in
cs-410_1_1_181,cs-410,1,1,"00:11:10,446","00:11:15,250",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,in another context the computer could
cs-410_1_1_182,cs-410,1,1,"00:11:15,250","00:11:18,200",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,"Again, it's not perfect, but"
cs-410_1_1_183,cs-410,1,1,"00:11:19,530","00:11:21,240",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,"We can also do sentiment analysis,"
cs-410_1_1_184,cs-410,1,1,"00:11:21,240","00:11:25,830",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,"meaning, to figure out whether"
cs-410_1_1_185,cs-410,1,1,"00:11:25,830","00:11:28,940",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,This is especially useful for
cs-410_1_1_186,cs-410,1,1,"00:11:30,410","00:11:33,150",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,So these are examples
cs-410_1_1_187,cs-410,1,1,"00:11:33,150","00:11:37,570",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,And they help us to obtain partial
cs-410_1_1_188,cs-410,1,1,"00:11:38,850","00:11:43,410",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,It's not giving us a complete
cs-410_1_1_189,cs-410,1,1,"00:11:43,410","00:11:44,380",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=703,this sentence.
cs-410_1_1_190,cs-410,1,1,"00:11:44,380","00:11:48,150",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,But it would still help us gain
cs-410_1_1_191,cs-410,1,1,"00:11:48,150","00:11:49,580",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,And these can be useful.
cs-410_1_1_192,cs-410,1,1,"00:11:51,620","00:11:54,730",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,"In terms of inference,"
cs-410_1_1_193,cs-410,1,1,"00:11:54,730","00:12:00,050",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,probably because of the general difficulty
cs-410_1_1_194,cs-410,1,1,"00:12:00,050","00:12:03,390",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=720,This is a general challenge
cs-410_1_1_195,cs-410,1,1,"00:12:03,390","00:12:07,468",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,Now that's probably also because
cs-410_1_1_196,cs-410,1,1,"00:12:07,468","00:12:10,172",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,representation for
cs-410_1_1_197,cs-410,1,1,"00:12:10,172","00:12:11,320",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,So this is hard.
cs-410_1_1_198,cs-410,1,1,"00:12:11,320","00:12:16,540",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,"Yet in some domains perhaps,"
cs-410_1_1_199,cs-410,1,1,"00:12:16,540","00:12:23,340",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,"restrictions on the word uses, you may be"
cs-410_1_1_200,cs-410,1,1,"00:12:23,340","00:12:28,050",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,But in general we can not
cs-410_1_1_201,cs-410,1,1,"00:12:28,050","00:12:31,650",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,Speech act analysis is also
cs-410_1_1_202,cs-410,1,1,"00:12:31,650","00:12:36,600",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=751,we can only do that analysis for
cs-410_1_1_203,cs-410,1,1,"00:12:36,600","00:12:41,193",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,So this roughly gives you some
cs-410_1_1_204,cs-410,1,1,"00:12:41,193","00:12:46,356",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,And then we also talk a little
cs-410_1_1_205,cs-410,1,1,"00:12:46,356","00:12:51,780",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,and so we can't even do 100%
cs-410_1_1_206,cs-410,1,1,"00:12:51,780","00:12:54,700",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=771,"Now this looks like a simple task, but"
cs-410_1_1_207,cs-410,1,1,"00:12:54,700","00:12:59,800",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,"think about the example here,"
cs-410_1_1_208,cs-410,1,1,"00:12:59,800","00:13:04,840",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=779,have different syntactic categories if you
cs-410_1_1_209,cs-410,1,1,"00:13:04,840","00:13:07,600",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,It's not that easy to figure
cs-410_1_1_210,cs-410,1,1,"00:13:10,000","00:13:12,900",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=790,It's also hard to do
cs-410_1_1_211,cs-410,1,1,"00:13:12,900","00:13:16,940",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,"And again, the same sentence"
cs-410_1_1_212,cs-410,1,1,"00:13:18,010","00:13:23,330",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,This ambiguity can be very hard to
cs-410_1_1_213,cs-410,1,1,"00:13:23,330","00:13:27,940",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,where you have to use a lot of knowledge
cs-410_1_1_214,cs-410,1,1,"00:13:27,940","00:13:33,310",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,"from the background, in order to figure"
cs-410_1_1_215,cs-410,1,1,"00:13:33,310","00:13:37,730",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,So although the sentence looks very
cs-410_1_1_216,cs-410,1,1,"00:13:37,730","00:13:42,380",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,And in cases when the sentence is
cs-410_1_1_217,cs-410,1,1,"00:13:42,380","00:13:46,760",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,"five prepositional phrases, and there"
cs-410_1_1_218,cs-410,1,1,"00:13:48,580","00:13:51,650",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,It's also harder to do precise
cs-410_1_1_219,cs-410,1,1,"00:13:51,650","00:13:53,410",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=831,So here's an example.
cs-410_1_1_220,cs-410,1,1,"00:13:53,410","00:14:00,108",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=833,"In the sentence ""John owns a restaurant."""
cs-410_1_1_221,cs-410,1,1,"00:14:00,108","00:14:05,340",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,"The word own,"
cs-410_1_1_222,cs-410,1,1,"00:14:05,340","00:14:10,210",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=845,it's very hard to precisely describe
cs-410_1_1_223,cs-410,1,1,"00:14:11,430","00:14:16,467",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,So as a result we have a robust and
cs-410_1_1_224,cs-410,1,1,"00:14:16,467","00:14:20,860",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=856,Natural Language Processing techniques
cs-410_1_1_225,cs-410,1,1,"00:14:22,490","00:14:25,640",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,"In a shallow way,"
cs-410_1_1_226,cs-410,1,1,"00:14:25,640","00:14:33,600",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,"For example, parts of speech tagging or a"
cs-410_1_1_227,cs-410,1,1,"00:14:33,600","00:14:35,520",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,"And those are not deep understanding,"
cs-410_1_1_228,cs-410,1,1,"00:14:35,520","00:14:39,419",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,because we're not really understanding
cs-410_1_1_229,cs-410,1,1,"00:14:41,270","00:14:45,170",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=881,On the other hand of the deep
cs-410_1_1_230,cs-410,1,1,"00:14:45,170","00:14:50,840",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=885,"up well, meaning that they would"
cs-410_1_1_231,cs-410,1,1,"00:14:50,840","00:14:54,850",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,And if you don't restrict
cs-410_1_1_232,cs-410,1,1,"00:14:54,850","00:14:59,750",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,"the use of words, then these"
cs-410_1_1_233,cs-410,1,1,"00:14:59,750","00:15:04,310",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,They may work well based on machine
cs-410_1_1_234,cs-410,1,1,"00:15:04,310","00:15:08,520",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,that are similar to the training data
cs-410_1_1_235,cs-410,1,1,"00:15:08,520","00:15:13,090",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=908,But they generally wouldn't work well on
cs-410_1_1_236,cs-410,1,1,"00:15:13,090","00:15:14,290",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,the training data.
cs-410_1_1_237,cs-410,1,1,"00:15:14,290","00:15:19,150",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=914,So this pretty much summarizes the state
cs-410_1_1_238,cs-410,1,1,"00:15:19,150","00:15:23,590",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=919,"Of course, within such a short amount"
cs-410_1_1_239,cs-410,1,1,"00:15:23,590","00:15:27,120",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,"a complete view of NLP,"
cs-410_1_1_240,cs-410,1,1,"00:15:27,120","00:15:35,896",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=927,And I'd expect to see multiple courses on
cs-410_1_1_241,cs-410,1,1,"00:15:35,896","00:15:40,960",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=935,But because of its relevance to the topic
cs-410_1_1_242,cs-410,1,1,"00:15:40,960","00:15:45,410",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=940,you to know the background in case
cs-410_1_1_243,cs-410,1,1,"00:15:45,410","00:15:47,340",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=945,So what does that mean for Text Retrieval?
cs-410_1_1_244,cs-410,1,1,"00:15:48,980","00:15:53,254",244,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,"Well, in Text Retrieval we"
cs-410_1_1_245,cs-410,1,1,"00:15:53,254","00:15:56,470",245,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,It's very hard to restrict
cs-410_1_1_246,cs-410,1,1,"00:15:56,470","00:16:00,092",246,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=956,And we also are often dealing
cs-410_1_1_247,cs-410,1,1,"00:16:00,092","00:16:06,730",247,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=960,So that means The NLP techniques must
cs-410_1_1_248,cs-410,1,1,"00:16:06,730","00:16:12,060",248,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=966,And that just implies today we can only
cs-410_1_1_249,cs-410,1,1,"00:16:12,060","00:16:13,550",249,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=972,text retrieval.
cs-410_1_1_250,cs-410,1,1,"00:16:13,550","00:16:14,780",250,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=973,"In fact,"
cs-410_1_1_251,cs-410,1,1,"00:16:14,780","00:16:19,070",251,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=974,most search engines today use something
cs-410_1_1_252,cs-410,1,1,"00:16:20,740","00:16:25,450",252,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,"Now, this is probably the simplest"
cs-410_1_1_253,cs-410,1,1,"00:16:25,450","00:16:29,250",253,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=985,That is to turn text data
cs-410_1_1_254,cs-410,1,1,"00:16:29,250","00:16:33,930",254,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=989,"Meaning we'll keep individual words, but"
cs-410_1_1_255,cs-410,1,1,"00:16:33,930","00:16:37,660",255,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=993,And we'll keep duplicated
cs-410_1_1_256,cs-410,1,1,"00:16:37,660","00:16:39,950",256,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=997,So this is called a bag
cs-410_1_1_257,cs-410,1,1,"00:16:39,950","00:16:45,990",257,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=999,"When you represent text in this way,"
cs-410_1_1_258,cs-410,1,1,"00:16:45,990","00:16:51,020",258,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,That just makes it harder to understand
cs-410_1_1_259,cs-410,1,1,"00:16:51,020","00:16:52,440",259,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1011,because we've lost the order.
cs-410_1_1_260,cs-410,1,1,"00:16:53,870","00:16:57,320",260,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,But yet this representation tends
cs-410_1_1_261,cs-410,1,1,"00:16:57,320","00:16:59,150",261,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1017,most search tasks.
cs-410_1_1_262,cs-410,1,1,"00:16:59,150","00:17:03,450",262,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1019,And this was partly because the search
cs-410_1_1_263,cs-410,1,1,"00:17:03,450","00:17:08,230",263,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1023,If you see matching of some of
cs-410_1_1_264,cs-410,1,1,"00:17:08,230","00:17:12,560",264,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1028,chances are that that document is about
cs-410_1_1_265,cs-410,1,1,"00:17:13,670","00:17:15,775",265,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1033,"So in comparison of some other tasks, for"
cs-410_1_1_266,cs-410,1,1,"00:17:15,775","00:17:20,490",266,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,"example, machine translation would require"
cs-410_1_1_267,cs-410,1,1,"00:17:20,490","00:17:22,680",267,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1040,Otherwise the translation would be wrong.
cs-410_1_1_268,cs-410,1,1,"00:17:22,680","00:17:25,780",268,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1042,So in comparison such tasks
cs-410_1_1_269,cs-410,1,1,"00:17:25,780","00:17:30,670",269,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1045,Such a representation is often sufficient
cs-410_1_1_270,cs-410,1,1,"00:17:30,670","00:17:34,050",270,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1050,"the major search engines today,"
cs-410_1_1_271,cs-410,1,1,"00:17:35,770","00:17:40,240",271,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1055,"Of course, I put in parentheses but"
cs-410_1_1_272,cs-410,1,1,"00:17:40,240","00:17:42,750",272,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1060,that are not answered well by
cs-410_1_1_273,cs-410,1,1,"00:17:42,750","00:17:48,320",273,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1062,they do require the replantation that
cs-410_1_1_274,cs-410,1,1,"00:17:48,320","00:17:51,900",274,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1068,That would require more natural
cs-410_1_1_275,cs-410,1,1,"00:17:52,950","00:17:56,600",275,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1072,There was another reason why we
cs-410_1_1_276,cs-410,1,1,"00:17:56,600","00:17:59,100",276,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1076,NLP techniques in modern search engines.
cs-410_1_1_277,cs-410,1,1,"00:17:59,100","00:18:02,460",277,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1079,And that's because some
cs-410_1_1_278,cs-410,1,1,"00:18:02,460","00:18:05,400",278,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1082,naturally solved the problem of NLP.
cs-410_1_1_279,cs-410,1,1,"00:18:05,400","00:18:09,240",279,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1085,So one example is word
cs-410_1_1_280,cs-410,1,1,"00:18:09,240","00:18:11,060",280,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1089,Think about a word like Java.
cs-410_1_1_281,cs-410,1,1,"00:18:11,060","00:18:13,900",281,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1091,It could mean coffee or
cs-410_1_1_282,cs-410,1,1,"00:18:15,090","00:18:18,230",282,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1095,"If you look at the word anome,"
cs-410_1_1_283,cs-410,1,1,"00:18:18,230","00:18:23,050",283,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1098,"when the user uses the word in the query,"
cs-410_1_1_284,cs-410,1,1,"00:18:23,050","00:18:26,240",284,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1103,"For example, I'm looking for"
cs-410_1_1_285,cs-410,1,1,"00:18:26,240","00:18:31,990",285,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1106,"When I have applet there,"
cs-410_1_1_286,cs-410,1,1,"00:18:31,990","00:18:36,360",286,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1111,And that contest can help us
cs-410_1_1_287,cs-410,1,1,"00:18:36,360","00:18:39,690",287,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1116,which Java is referring
cs-410_1_1_288,cs-410,1,1,"00:18:39,690","00:18:43,710",288,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1119,Because those documents would
cs-410_1_1_289,cs-410,1,1,"00:18:43,710","00:18:48,560",289,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,If Java occurs in that
cs-410_1_1_290,cs-410,1,1,"00:18:48,560","00:18:52,960",290,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1128,then you would never match applet or
cs-410_1_1_291,cs-410,1,1,"00:18:52,960","00:18:56,250",291,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1132,So this is the case when
cs-410_1_1_292,cs-410,1,1,"00:18:56,250","00:18:58,580",292,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1136,naturally achieve the goal of word.
cs-410_1_1_293,cs-410,1,1,"00:19:01,530","00:19:05,920",293,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1141,Another example is some technique called
cs-410_1_1_294,cs-410,1,1,"00:19:05,920","00:19:11,360",294,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1145,feedback which we will talk about
cs-410_1_1_295,cs-410,1,1,"00:19:11,360","00:19:16,938",295,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1151,This technique would allow us to add
cs-410_1_1_296,cs-410,1,1,"00:19:16,938","00:19:21,859",296,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1156,those additional words could
cs-410_1_1_297,cs-410,1,1,"00:19:21,859","00:19:26,155",297,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1161,And these words can help matching
cs-410_1_1_298,cs-410,1,1,"00:19:26,155","00:19:27,680",298,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1166,have not occurred.
cs-410_1_1_299,cs-410,1,1,"00:19:27,680","00:19:32,500",299,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1167,"So this achieves, to some extent,"
cs-410_1_1_300,cs-410,1,1,"00:19:32,500","00:19:35,350",300,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1172,So those techniques also helped us
cs-410_1_1_301,cs-410,1,1,"00:19:35,350","00:19:38,890",301,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1175,bypass some of the difficulties
cs-410_1_1_302,cs-410,1,1,"00:19:40,530","00:19:43,920",302,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1180,"However, in the long run we still need"
cs-410_1_1_303,cs-410,1,1,"00:19:43,920","00:19:47,280",303,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1183,techniques in order to improve the
cs-410_1_1_304,cs-410,1,1,"00:19:47,280","00:19:50,939",304,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1187,And it's particularly needed for
cs-410_1_1_305,cs-410,1,1,"00:19:52,160","00:19:53,390",305,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1192,Or for question and answering.
cs-410_1_1_306,cs-410,1,1,"00:19:55,310","00:20:00,540",306,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1195,Google has recently launched a knowledge
cs-410_1_1_307,cs-410,1,1,"00:20:00,540","00:20:05,220",307,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1200,"that goal, because knowledge graph would"
cs-410_1_1_308,cs-410,1,1,"00:20:05,220","00:20:09,170",308,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1205,And this goes beyond the simple
cs-410_1_1_309,cs-410,1,1,"00:20:09,170","00:20:12,950",309,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1209,And such technique should help us
cs-410_1_1_310,cs-410,1,1,"00:20:14,180","00:20:19,220",310,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1214,"significantly, although this is the open"
cs-410_1_1_311,cs-410,1,1,"00:20:19,220","00:20:24,990",311,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1219,"In sum, in this lecture we"
cs-410_1_1_312,cs-410,1,1,"00:20:24,990","00:20:27,820",312,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1224,we've talked about the state
cs-410_1_1_313,cs-410,1,1,"00:20:27,820","00:20:30,550",313,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1227,"What we can do, what we cannot do."
cs-410_1_1_314,cs-410,1,1,"00:20:30,550","00:20:34,510",314,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1230,"And finally, we also explain why"
cs-410_1_1_315,cs-410,1,1,"00:20:34,510","00:20:38,290",315,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1234,remains the dominant replantation
cs-410_1_1_316,cs-410,1,1,"00:20:38,290","00:20:43,258",316,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1238,even though deeper NLP would be needed for
cs-410_1_1_317,cs-410,1,1,"00:20:43,258","00:20:46,470",317,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1243,"If you want to know more, you can take"
cs-410_1_1_318,cs-410,1,1,"00:20:46,470","00:20:49,070",318,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1246,I only cited one here and
cs-410_1_1_319,cs-410,1,1,"00:20:49,070","00:20:52,976",319,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1249,Thanks.
cs-410_1_1_320,cs-410,1,1,"00:20:52,976","00:21:02,976",320,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1252,[MUSIC]
cs-410_1_10_1,cs-410,1,10,"00:00:00,012","00:00:07,427",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_10_2,cs-410,1,10,"00:00:07,427","00:00:10,460",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is the first one
cs-410_1_10_3,cs-410,1,10,"00:00:14,165","00:00:17,400",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"In this lecture, we are going to"
cs-410_1_10_4,cs-410,1,10,"00:00:18,430","00:00:24,590",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,This is a very important technique for
cs-410_1_10_5,cs-410,1,10,"00:00:24,590","00:00:25,100",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"In particular,"
cs-410_1_10_6,cs-410,1,10,"00:00:25,100","00:00:30,430",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,in this lecture we're going to start with
cs-410_1_10_7,cs-410,1,10,"00:00:31,650","00:00:37,040",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,"And that is, what is text clustering and"
cs-410_1_10_8,cs-410,1,10,"00:00:38,060","00:00:42,610",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,"In the following lectures, we are going"
cs-410_1_10_9,cs-410,1,10,"00:00:42,610","00:00:44,550",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,How to evaluate the clustering results?
cs-410_1_10_10,cs-410,1,10,"00:00:47,060","00:00:48,400",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,So what is text clustering?
cs-410_1_10_11,cs-410,1,10,"00:00:49,500","00:00:52,640",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,"Well, clustering actually is"
cs-410_1_10_12,cs-410,1,10,"00:00:52,640","00:00:55,670",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,data mining as you might have
cs-410_1_10_13,cs-410,1,10,"00:00:56,760","00:01:00,250",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,The idea is to discover natural
cs-410_1_10_14,cs-410,1,10,"00:01:01,250","00:01:05,040",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"In another words,"
cs-410_1_10_15,cs-410,1,10,"00:01:05,040","00:01:09,170",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,"In our case, these objects are of course,"
cs-410_1_10_16,cs-410,1,10,"00:01:09,170","00:01:14,510",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,"For example, they can be documents,"
cs-410_1_10_17,cs-410,1,10,"00:01:14,510","00:01:21,510",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,"sentences, or websites, and then I'll"
cs-410_1_10_18,cs-410,1,10,"00:01:21,510","00:01:26,560",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,"So let's see an example, well, here"
cs-410_1_10_19,cs-410,1,10,"00:01:26,560","00:01:31,560",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,I just used some shapes to denote
cs-410_1_10_20,cs-410,1,10,"00:01:33,450","00:01:39,690",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"Now if I ask you, what are some natural"
cs-410_1_10_21,cs-410,1,10,"00:01:39,690","00:01:47,790",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,if you look at it and you might agree that
cs-410_1_10_22,cs-410,1,10,"00:01:47,790","00:01:51,740",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,or their locations on this
cs-410_1_10_23,cs-410,1,10,"00:01:53,240","00:01:55,110",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,So we got the three clusters in this case.
cs-410_1_10_24,cs-410,1,10,"00:01:56,940","00:02:01,360",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,And they may not be so
cs-410_1_10_25,cs-410,1,10,"00:02:01,360","00:02:06,220",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,these three clusters but it really depends
cs-410_1_10_26,cs-410,1,10,"00:02:07,650","00:02:11,450",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,Maybe some of you have also seen
cs-410_1_10_27,cs-410,1,10,"00:02:11,450","00:02:14,050",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,we might get different clusters.
cs-410_1_10_28,cs-410,1,10,"00:02:14,050","00:02:21,400",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,And you'll see another example
cs-410_1_10_29,cs-410,1,10,"00:02:21,400","00:02:27,440",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,"But the main point of here is, the problem"
cs-410_1_10_30,cs-410,1,10,"00:02:29,200","00:02:34,130",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,And the problem lies in
cs-410_1_10_31,cs-410,1,10,"00:02:34,130","00:02:36,220",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,And what do you mean by similar objects?
cs-410_1_10_32,cs-410,1,10,"00:02:38,160","00:02:40,990",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,Now this problem has to be
cs-410_1_10_33,cs-410,1,10,"00:02:40,990","00:02:44,537",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,clearly defined in order to have
cs-410_1_10_34,cs-410,1,10,"00:02:46,315","00:02:49,295",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,And the problem is in general
cs-410_1_10_35,cs-410,1,10,"00:02:49,295","00:02:53,445",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,that any two objects can be similar
cs-410_1_10_36,cs-410,1,10,"00:02:53,445","00:02:59,119",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"So for example, this will kept"
cs-410_1_10_37,cs-410,1,10,"00:03:00,300","00:03:02,650",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,So are the two words similar?
cs-410_1_10_38,cs-410,1,10,"00:03:02,650","00:03:08,490",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,"Well, it depends on how if"
cs-410_1_10_39,cs-410,1,10,"00:03:11,070","00:03:16,000",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,properties of car and
cs-410_1_10_40,cs-410,1,10,"00:03:16,000","00:03:20,630",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"if you look at them functionally,"
cs-410_1_10_41,cs-410,1,10,"00:03:20,630","00:03:23,650",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,can both be transportation tools.
cs-410_1_10_42,cs-410,1,10,"00:03:23,650","00:03:26,220",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,"So in that sense, they may be similar."
cs-410_1_10_43,cs-410,1,10,"00:03:26,220","00:03:31,599",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"So as we can see, it really depends on"
cs-410_1_10_44,cs-410,1,10,"00:03:32,740","00:03:37,700",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,And so it ought to make
cs-410_1_10_45,cs-410,1,10,"00:03:37,700","00:03:43,050",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,A user must define the perspective for
cs-410_1_10_46,cs-410,1,10,"00:03:44,310","00:03:47,599",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,And we call this perspective
cs-410_1_10_47,cs-410,1,10,"00:03:49,270","00:03:54,180",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,"And when you define a clustering problem,"
cs-410_1_10_48,cs-410,1,10,"00:03:55,340","00:04:00,870",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,your perspective for
cs-410_1_10_49,cs-410,1,10,"00:04:00,870","00:04:05,830",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,the similarity that will be
cs-410_1_10_50,cs-410,1,10,"00:04:05,830","00:04:11,795",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"because otherwise,"
cs-410_1_10_51,cs-410,1,10,"00:04:11,795","00:04:16,870",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,one can have different
cs-410_1_10_52,cs-410,1,10,"00:04:16,870","00:04:19,910",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,So let's look at the example here.
cs-410_1_10_53,cs-410,1,10,"00:04:19,910","00:04:24,210",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,"You are seeing some objects,"
cs-410_1_10_54,cs-410,1,10,"00:04:24,210","00:04:29,730",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,that are very similar to what you
cs-410_1_10_55,cs-410,1,10,"00:04:29,730","00:04:34,430",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,"if I ask you to group these objects,"
cs-410_1_10_56,cs-410,1,10,"00:04:38,040","00:04:42,052",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,feel there is more than here
cs-410_1_10_57,cs-410,1,10,"00:04:42,052","00:04:47,810",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,"For example, you might think, well, we"
cs-410_1_10_58,cs-410,1,10,"00:04:47,810","00:04:53,510",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"can steer a group by ships, so that would"
cs-410_1_10_59,cs-410,1,10,"00:04:53,510","00:04:57,558",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"However, you might also feel that,"
cs-410_1_10_60,cs-410,1,10,"00:04:57,558","00:05:02,380",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,"well, maybe the objects can be"
cs-410_1_10_61,cs-410,1,10,"00:05:02,380","00:05:07,020",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,So that would give us a different way
cs-410_1_10_62,cs-410,1,10,"00:05:07,020","00:05:11,890",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,the size and
cs-410_1_10_63,cs-410,1,10,"00:05:12,910","00:05:16,440",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,"So as you can see clearly here,"
cs-410_1_10_64,cs-410,1,10,"00:05:16,440","00:05:18,860",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,we'll get different clustering result.
cs-410_1_10_65,cs-410,1,10,"00:05:18,860","00:05:23,750",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,So that also clearly tells us that in
cs-410_1_10_66,cs-410,1,10,"00:05:23,750","00:05:27,060",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,we must use perspective.
cs-410_1_10_67,cs-410,1,10,"00:05:27,060","00:05:32,054",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,"Without perspective, it's very hard to"
cs-410_1_10_68,cs-410,1,10,"00:05:36,152","00:05:40,970",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,So there are many examples
cs-410_1_10_69,cs-410,1,10,"00:05:42,330","00:05:48,380",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,"And so for example, we can cluster"
cs-410_1_10_70,cs-410,1,10,"00:05:48,380","00:05:51,269",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"So in this case,"
cs-410_1_10_71,cs-410,1,10,"00:05:52,280","00:05:55,780",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,We may be able to cluster terms.
cs-410_1_10_72,cs-410,1,10,"00:05:55,780","00:05:58,300",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,"In this case, terms are objects."
cs-410_1_10_73,cs-410,1,10,"00:05:58,300","00:06:03,480",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,And a cluster of terms can be used to
cs-410_1_10_74,cs-410,1,10,"00:06:03,480","00:06:08,530",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,"In fact, there's a topic models that you"
cs-410_1_10_75,cs-410,1,10,"00:06:08,530","00:06:13,850",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,give you cluster of terms in some
cs-410_1_10_76,cs-410,1,10,"00:06:13,850","00:06:19,610",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,high probabilities from word distribution.
cs-410_1_10_77,cs-410,1,10,"00:06:19,610","00:06:25,330",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,Another example is just to cluster any
cs-410_1_10_78,cs-410,1,10,"00:06:25,330","00:06:30,970",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,"sentences, or any segments that you can"
cs-410_1_10_79,cs-410,1,10,"00:06:32,100","00:06:36,300",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"For example, we might extract the order"
cs-410_1_10_80,cs-410,1,10,"00:06:36,300","00:06:39,030",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"let's say, by using a topic model."
cs-410_1_10_81,cs-410,1,10,"00:06:39,030","00:06:43,620",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,Now once we've got those
cs-410_1_10_82,cs-410,1,10,"00:06:45,120","00:06:50,850",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,cluster the segments that we've got to
cs-410_1_10_83,cs-410,1,10,"00:06:50,850","00:06:56,908",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,discover interesting clusters that
cs-410_1_10_84,cs-410,1,10,"00:06:56,908","00:07:00,860",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,So this is a case of combining text
cs-410_1_10_85,cs-410,1,10,"00:07:00,860","00:07:03,762",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,And in general you will
cs-410_1_10_86,cs-410,1,10,"00:07:05,140","00:07:09,670",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,can be accurate combined in
cs-410_1_10_87,cs-410,1,10,"00:07:09,670","00:07:14,840",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,the goal of doing more sophisticated
cs-410_1_10_88,cs-410,1,10,"00:07:16,030","00:07:20,070",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,We can also cluster fairly
cs-410_1_10_89,cs-410,1,10,"00:07:20,070","00:07:24,600",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,I just mean text objects may
cs-410_1_10_90,cs-410,1,10,"00:07:24,600","00:07:27,440",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"So for example, we might cluster websites."
cs-410_1_10_91,cs-410,1,10,"00:07:27,440","00:07:31,228",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,Each website is actually
cs-410_1_10_92,cs-410,1,10,"00:07:31,228","00:07:39,065",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"Similarly, we can also cluster articles"
cs-410_1_10_93,cs-410,1,10,"00:07:39,065","00:07:44,573",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,So we can trigger all the articles
cs-410_1_10_94,cs-410,1,10,"00:07:44,573","00:07:45,785",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,clustering.
cs-410_1_10_95,cs-410,1,10,"00:07:45,785","00:07:50,652",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"In this way, we might group authors"
cs-410_1_10_96,cs-410,1,10,"00:07:50,652","00:07:52,770",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,published papers or similar.
cs-410_1_10_97,cs-410,1,10,"00:07:55,150","00:08:00,290",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,For the more text clusters will be for
cs-410_1_10_98,cs-410,1,10,"00:08:00,290","00:08:06,330",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,That's because we can in general cluster
cs-410_1_10_99,cs-410,1,10,"00:08:08,210","00:08:11,750",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,So more generally why is
cs-410_1_10_100,cs-410,1,10,"00:08:11,750","00:08:16,100",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,"Well, it's because it's a very"
cs-410_1_10_101,cs-410,1,10,"00:08:16,100","00:08:18,710",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,particularly exploratory text analysis.
cs-410_1_10_102,cs-410,1,10,"00:08:20,250","00:08:25,690",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,And so a typical scenario is that
cs-410_1_10_103,cs-410,1,10,"00:08:25,690","00:08:30,300",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,let's say all the email messages
cs-410_1_10_104,cs-410,1,10,"00:08:30,300","00:08:32,070",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,"all the literature articles, etc."
cs-410_1_10_105,cs-410,1,10,"00:08:32,070","00:08:35,960",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,And then you hope to get a sense
cs-410_1_10_106,cs-410,1,10,"00:08:35,960","00:08:40,970",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,"the connection, so for example,"
cs-410_1_10_107,cs-410,1,10,"00:08:40,970","00:08:45,910",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,"a sense about major topics,"
cs-410_1_10_108,cs-410,1,10,"00:08:45,910","00:08:49,360",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,representative documents
cs-410_1_10_109,cs-410,1,10,"00:08:49,360","00:08:53,164",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,And clustering to help
cs-410_1_10_110,cs-410,1,10,"00:08:53,164","00:08:59,949",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,We sometimes also want to link
cs-410_1_10_111,cs-410,1,10,"00:08:59,949","00:09:03,960",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,And these objects might be
cs-410_1_10_112,cs-410,1,10,"00:09:03,960","00:09:04,830",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,"And in that case,"
cs-410_1_10_113,cs-410,1,10,"00:09:04,830","00:09:09,560",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,such a technique can help us remove
cs-410_1_10_114,cs-410,1,10,"00:09:10,910","00:09:13,280",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,Sometimes they are about
cs-410_1_10_115,cs-410,1,10,"00:09:13,280","00:09:17,140",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,by linking them together we can have
cs-410_1_10_116,cs-410,1,10,"00:09:19,660","00:09:24,420",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,We may also used text clustering to create
cs-410_1_10_117,cs-410,1,10,"00:09:24,420","00:09:28,840",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,we can create a hierarchy of structures
cs-410_1_10_118,cs-410,1,10,"00:09:31,270","00:09:36,140",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,We may also use text clustering to induce
cs-410_1_10_119,cs-410,1,10,"00:09:36,140","00:09:40,206",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,"data when we cluster documents together,"
cs-410_1_10_120,cs-410,1,10,"00:09:40,206","00:09:44,100",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,And then we can say when
cs-410_1_10_121,cs-410,1,10,"00:09:44,100","00:09:45,790",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,then the feature value would be one.
cs-410_1_10_122,cs-410,1,10,"00:09:45,790","00:09:49,870",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,"And if a document is not in this cluster,"
cs-410_1_10_123,cs-410,1,10,"00:09:49,870","00:09:54,298",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,And this helps provide additional
cs-410_1_10_124,cs-410,1,10,"00:09:54,298","00:09:57,900",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,text classification as
cs-410_1_10_125,cs-410,1,10,"00:09:59,870","00:10:03,320",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"So there are, in general,"
cs-410_1_10_126,cs-410,1,10,"00:10:03,320","00:10:06,218",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,And I just thought of
cs-410_1_10_127,cs-410,1,10,"00:10:06,218","00:10:08,490",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,"One is to cluster search results, for"
cs-410_1_10_128,cs-410,1,10,"00:10:08,490","00:10:12,360",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"example, [INAUDIBLE] search engine"
cs-410_1_10_129,cs-410,1,10,"00:10:12,360","00:10:19,020",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,that the user can see overall structure
cs-410_1_10_130,cs-410,1,10,"00:10:19,020","00:10:22,454",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,And when the query's ambiguous this
cs-410_1_10_131,cs-410,1,10,"00:10:22,454","00:10:26,710",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,the clusters likely represent
cs-410_1_10_132,cs-410,1,10,"00:10:28,630","00:10:33,535",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,Another application is to understand the
cs-410_1_10_133,cs-410,1,10,"00:10:33,535","00:10:34,943",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,"their emails, right."
cs-410_1_10_134,cs-410,1,10,"00:10:34,943","00:10:40,238",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"So in this case,"
cs-410_1_10_135,cs-410,1,10,"00:10:40,238","00:10:44,903",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,then find in the major
cs-410_1_10_136,cs-410,1,10,"00:10:44,903","00:10:51,355",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,we can understand what are the major
cs-410_1_10_137,cs-410,1,10,"00:10:51,355","00:10:57,897",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,[MUSIC]
cs-410_1_11_1,cs-410,1,11,"00:00:00,012","00:00:07,093",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_11_2,cs-410,1,11,"00:00:07,093","00:00:11,830",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about the discriminative
cs-410_1_11_3,cs-410,1,11,"00:00:13,000","00:00:15,840",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,In this lecture we're going to
cs-410_1_11_4,cs-410,1,11,"00:00:15,840","00:00:20,220",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,do text categorization and
cs-410_1_11_5,cs-410,1,11,"00:00:20,220","00:00:24,760",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,This is a slide that you have seen from
cs-410_1_11_6,cs-410,1,11,"00:00:24,760","00:00:29,120",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,where we have shown that although
cs-410_1_11_7,cs-410,1,11,"00:00:29,120","00:00:34,090",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,"the generation of text data, from each"
cs-410_1_11_8,cs-410,1,11,"00:00:34,090","00:00:40,900",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,rule to eventually rewrite the scoring
cs-410_1_11_9,cs-410,1,11,"00:00:40,900","00:00:45,520",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,And this scoring function is basically
cs-410_1_11_10,cs-410,1,11,"00:00:45,520","00:00:50,530",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,"of word features, where the feature values"
cs-410_1_11_11,cs-410,1,11,"00:00:50,530","00:00:55,720",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,are the log of probability ratios of
cs-410_1_11_12,cs-410,1,11,"00:00:57,280","00:01:02,670",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,Now this kind of scoring function
cs-410_1_11_13,cs-410,1,11,"00:01:02,670","00:01:08,570",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,function where we can in general
cs-410_1_11_14,cs-410,1,11,"00:01:08,570","00:01:12,340",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,Of course the features don't
cs-410_1_11_15,cs-410,1,11,"00:01:12,340","00:01:16,280",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,Their features can be other
cs-410_1_11_16,cs-410,1,11,"00:01:16,280","00:01:22,880",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,And we mentioned that this is precisely
cs-410_1_11_17,cs-410,1,11,"00:01:22,880","00:01:27,570",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,"So, in this lecture we're going to"
cs-410_1_11_18,cs-410,1,11,"00:01:27,570","00:01:31,450",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,They try to model
cs-410_1_11_19,cs-410,1,11,"00:01:31,450","00:01:36,990",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,labels given the data directly
cs-410_1_11_20,cs-410,1,11,"00:01:36,990","00:01:41,550",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,to compute that interactively
cs-410_1_11_21,cs-410,1,11,"00:01:41,550","00:01:47,150",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,So the general idea of logistic
cs-410_1_11_22,cs-410,1,11,"00:01:47,150","00:01:52,350",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,the dependency of a binary
cs-410_1_11_23,cs-410,1,11,"00:01:52,350","00:01:56,360",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,on some predictors that are denoted as X.
cs-410_1_11_24,cs-410,1,11,"00:01:56,360","00:02:01,720",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,So here we have also changed the notation
cs-410_1_11_25,cs-410,1,11,"00:02:01,720","00:02:06,120",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,to X for future values.
cs-410_1_11_26,cs-410,1,11,"00:02:07,140","00:02:10,120",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,You may recall in the previous
cs-410_1_11_27,cs-410,1,11,"00:02:10,120","00:02:12,810",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,FI to represent the future values.
cs-410_1_11_28,cs-410,1,11,"00:02:13,910","00:02:18,762",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,"And here we use the notation of X factor,"
cs-410_1_11_29,cs-410,1,11,"00:02:18,762","00:02:23,331",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,which is more common when we introduce
cs-410_1_11_30,cs-410,1,11,"00:02:23,331","00:02:27,640",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,such discriminative algorithms.
cs-410_1_11_31,cs-410,1,11,"00:02:27,640","00:02:29,690",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"So, X is our input."
cs-410_1_11_32,cs-410,1,11,"00:02:29,690","00:02:37,930",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,It's a vector with n features and
cs-410_1_11_33,cs-410,1,11,"00:02:37,930","00:02:42,920",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,And I will go with a model that dependency
cs-410_1_11_34,cs-410,1,11,"00:02:42,920","00:02:44,360",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,these features.
cs-410_1_11_35,cs-410,1,11,"00:02:44,360","00:02:49,897",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,So in our categorization problem when
cs-410_1_11_36,cs-410,1,11,"00:02:49,897","00:02:55,183",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"theta 2, and we can use the Y value to"
cs-410_1_11_37,cs-410,1,11,"00:02:55,183","00:03:00,080",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,"it means the category of the document,"
cs-410_1_11_38,cs-410,1,11,"00:03:00,080","00:03:07,225",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"Now, the goal here is the model, the"
cs-410_1_11_39,cs-410,1,11,"00:03:07,225","00:03:13,465",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,as opposed to model of the generation of
cs-410_1_11_40,cs-410,1,11,"00:03:13,465","00:03:15,985",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,And another advantage of this
cs-410_1_11_41,cs-410,1,11,"00:03:15,985","00:03:19,880",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,it would allow many other features
cs-410_1_11_42,cs-410,1,11,"00:03:19,880","00:03:23,490",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,since we're not modeling
cs-410_1_11_43,cs-410,1,11,"00:03:23,490","00:03:25,830",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,And we can plug in any
cs-410_1_11_44,cs-410,1,11,"00:03:25,830","00:03:31,410",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,So this is potentially advantageous for
cs-410_1_11_45,cs-410,1,11,"00:03:31,410","00:03:34,510",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"So more specifically,"
cs-410_1_11_46,cs-410,1,11,"00:03:34,510","00:03:40,760",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,assume the functional form of Y
cs-410_1_11_47,cs-410,1,11,"00:03:40,760","00:03:46,610",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,And this is very closely
cs-410_1_11_48,cs-410,1,11,"00:03:46,610","00:03:51,290",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,odds that I introduced in the Naive Bayes
cs-410_1_11_49,cs-410,1,11,"00:03:51,290","00:03:56,250",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,of the two categories that you
cs-410_1_11_50,cs-410,1,11,"00:03:57,900","00:04:00,230",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,So this is what I meant.
cs-410_1_11_51,cs-410,1,11,"00:04:00,230","00:04:05,430",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,"So in the case of Naive Bayes,"
cs-410_1_11_52,cs-410,1,11,"00:04:05,430","00:04:11,370",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,eventually we have reached
cs-410_1_11_53,cs-410,1,11,"00:04:12,990","00:04:18,290",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,But here we actually
cs-410_1_11_54,cs-410,1,11,"00:04:18,290","00:04:23,001",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,that we with the model our
cs-410_1_11_55,cs-410,1,11,"00:04:23,001","00:04:27,400",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,probability of Y given X
cs-410_1_11_56,cs-410,1,11,"00:04:29,840","00:04:36,430",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,directly as a function of these features.
cs-410_1_11_57,cs-410,1,11,"00:04:37,580","00:04:46,260",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"So, most specifically we assume that the"
cs-410_1_11_58,cs-410,1,11,"00:04:46,260","00:04:52,790",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,the probability of Y equals
cs-410_1_11_59,cs-410,1,11,"00:04:54,460","00:04:56,580",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,"All right, so it's a function of x and"
cs-410_1_11_60,cs-410,1,11,"00:04:56,580","00:05:00,910",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,it's a linear combination of these feature
cs-410_1_11_61,cs-410,1,11,"00:05:02,390","00:05:06,790",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,And it seems we know that
cs-410_1_11_62,cs-410,1,11,"00:05:06,790","00:05:11,100",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,is one minus probability
cs-410_1_11_63,cs-410,1,11,"00:05:11,100","00:05:16,030",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,this can be also written in this way.
cs-410_1_11_64,cs-410,1,11,"00:05:16,030","00:05:20,020",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,So this is a log out ratio here.
cs-410_1_11_65,cs-410,1,11,"00:05:22,040","00:05:23,250",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,"And so in logistic regression,"
cs-410_1_11_66,cs-410,1,11,"00:05:23,250","00:05:27,490",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,we're basically assuming that
cs-410_1_11_67,cs-410,1,11,"00:05:27,490","00:05:34,570",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,Okay my X is dependent on this linear
cs-410_1_11_68,cs-410,1,11,"00:05:34,570","00:05:39,960",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,So it's just one of the many possible
cs-410_1_11_69,cs-410,1,11,"00:05:39,960","00:05:42,880",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,But this particular form
cs-410_1_11_70,cs-410,1,11,"00:05:42,880","00:05:45,910",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,it also has some nice properties.
cs-410_1_11_71,cs-410,1,11,"00:05:47,760","00:05:53,690",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,So if we rewrite this equation to actually
cs-410_1_11_72,cs-410,1,11,"00:05:53,690","00:05:58,770",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,In terms of X by getting rid of
cs-410_1_11_73,cs-410,1,11,"00:05:58,770","00:06:01,980",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,and this is called a logistical function.
cs-410_1_11_74,cs-410,1,11,"00:06:01,980","00:06:07,030",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,"It's a transformation of X into Y,"
cs-410_1_11_75,cs-410,1,11,"00:06:08,120","00:06:14,090",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,"on the right side here, so"
cs-410_1_11_76,cs-410,1,11,"00:06:14,090","00:06:19,310",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"into a range of values from 0 to 1.0,"
cs-410_1_11_77,cs-410,1,11,"00:06:19,310","00:06:23,280",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,And that's precisely what we want
cs-410_1_11_78,cs-410,1,11,"00:06:24,350","00:06:26,710",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,And the function form looks like this.
cs-410_1_11_79,cs-410,1,11,"00:06:28,170","00:06:31,790",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,So this is the basic idea
cs-410_1_11_80,cs-410,1,11,"00:06:31,790","00:06:34,570",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,And it's a very useful classifier that
cs-410_1_11_81,cs-410,1,11,"00:06:34,570","00:06:39,231",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,can be used to do a lot of classification
cs-410_1_11_82,cs-410,1,11,"00:06:41,750","00:06:47,100",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,So as in all cases of model we would be
cs-410_1_11_83,cs-410,1,11,"00:06:47,100","00:06:50,780",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,And in fact in all of the machine running
cs-410_1_11_84,cs-410,1,11,"00:06:50,780","00:06:54,980",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,set up object and
cs-410_1_11_85,cs-410,1,11,"00:06:54,980","00:07:00,120",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,then the next step is to
cs-410_1_11_86,cs-410,1,11,"00:07:00,120","00:07:02,680",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"In general, we're going to adjust"
cs-410_1_11_87,cs-410,1,11,"00:07:02,680","00:07:06,641",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,Optimize the performance of
cs-410_1_11_88,cs-410,1,11,"00:07:06,641","00:07:13,410",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,So in our case just assume we have
cs-410_1_11_89,cs-410,1,11,"00:07:13,410","00:07:20,810",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,each pair is basically a future vector
cs-410_1_11_90,cs-410,1,11,"00:07:20,810","00:07:23,530",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,Y is either 1 or 0.
cs-410_1_11_91,cs-410,1,11,"00:07:23,530","00:07:29,900",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,So in our case we are interested
cs-410_1_11_92,cs-410,1,11,"00:07:31,310","00:07:36,020",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,The conditional likelihood here is
cs-410_1_11_93,cs-410,1,11,"00:07:36,020","00:07:41,829",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,basically to model why
cs-410_1_11_94,cs-410,1,11,"00:07:41,829","00:07:46,382",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"so it's not like a moderate x, but"
cs-410_1_11_95,cs-410,1,11,"00:07:46,382","00:07:50,787",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,rather we're going to model this.
cs-410_1_11_96,cs-410,1,11,"00:07:50,787","00:07:55,589",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,Note that this is a conditional
cs-410_1_11_97,cs-410,1,11,"00:07:55,589","00:08:00,494",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,this is also precisely what we wanted For
cs-410_1_11_98,cs-410,1,11,"00:08:00,494","00:08:06,266",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,Now so the likelihood function would be
cs-410_1_11_99,cs-410,1,11,"00:08:06,266","00:08:07,383",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,"And in each case,"
cs-410_1_11_100,cs-410,1,11,"00:08:07,383","00:08:12,990",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,this is the model of the probability of
cs-410_1_11_101,cs-410,1,11,"00:08:12,990","00:08:19,960",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,"So given a particular Xi, how likely"
cs-410_1_11_102,cs-410,1,11,"00:08:19,960","00:08:23,228",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,"Of course, Yi could be 1 or"
cs-410_1_11_103,cs-410,1,11,"00:08:23,228","00:08:28,310",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,the function found here would vary
cs-410_1_11_104,cs-410,1,11,"00:08:28,310","00:08:33,374",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,"If it's a 1, we'll be taking this form."
cs-410_1_11_105,cs-410,1,11,"00:08:33,374","00:08:36,276",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,And that's basically the logistic
cs-410_1_11_106,cs-410,1,11,"00:08:36,276","00:08:38,723",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"But what about this, if it's 0?"
cs-410_1_11_107,cs-410,1,11,"00:08:38,723","00:08:45,420",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,"Well, if it's 0, then we have to use"
cs-410_1_11_108,cs-410,1,11,"00:08:48,299","00:08:50,310",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"Now, how do we get this one?"
cs-410_1_11_109,cs-410,1,11,"00:08:50,310","00:08:54,870",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,"Well, that's just a 1 minus"
cs-410_1_11_110,cs-410,1,11,"00:08:55,990","00:08:58,200",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,And you can easily see this.
cs-410_1_11_111,cs-410,1,11,"00:08:58,200","00:09:04,220",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,Now the key point in here is that the
cs-410_1_11_112,cs-410,1,11,"00:09:04,220","00:09:09,340",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,"Yi, if it's a 1,"
cs-410_1_11_113,cs-410,1,11,"00:09:09,340","00:09:13,852",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,And if you think about when we
cs-410_1_11_114,cs-410,1,11,"00:09:13,852","00:09:19,033",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,we're basically going to want this
cs-410_1_11_115,cs-410,1,11,"00:09:19,033","00:09:26,519",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,"When the label is 1, that means"
cs-410_1_11_116,cs-410,1,11,"00:09:26,519","00:09:31,925",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,"But if the document is not,"
cs-410_1_11_117,cs-410,1,11,"00:09:31,925","00:09:36,821",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,and what's going to happen is
cs-410_1_11_118,cs-410,1,11,"00:09:36,821","00:09:40,500",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,small as possible because this sum's 1.
cs-410_1_11_119,cs-410,1,11,"00:09:40,500","00:09:45,670",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,"When I maximize this one,"
cs-410_1_11_120,cs-410,1,11,"00:09:48,070","00:09:53,275",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,"So you can see basically, if we maximize"
cs-410_1_11_121,cs-410,1,11,"00:09:53,275","00:09:58,568",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,to basically try to make the prediction on
cs-410_1_11_122,cs-410,1,11,"00:10:00,957","00:10:04,970",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,"So as another occasion, when you"
cs-410_1_11_123,cs-410,1,11,"00:10:04,970","00:10:07,075",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,"basically you'll find a beta value,"
cs-410_1_11_124,cs-410,1,11,"00:10:07,075","00:10:11,050",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,a set of beta values that would
cs-410_1_11_125,cs-410,1,11,"00:10:12,190","00:10:15,930",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,"And this, again, then gives us"
cs-410_1_11_126,cs-410,1,11,"00:10:15,930","00:10:20,130",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,"In this case,"
cs-410_1_11_127,cs-410,1,11,"00:10:20,130","00:10:22,870",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,Newton's method is a popular
cs-410_1_11_128,cs-410,1,11,"00:10:22,870","00:10:25,050",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,there are other methods as well.
cs-410_1_11_129,cs-410,1,11,"00:10:25,050","00:10:29,270",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,"But in the end,"
cs-410_1_11_130,cs-410,1,11,"00:10:29,270","00:10:34,590",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=629,"Once we have the beta values,"
cs-410_1_11_131,cs-410,1,11,"00:10:34,590","00:10:38,600",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,function to help us classify a document.
cs-410_1_11_132,cs-410,1,11,"00:10:39,620","00:10:40,580",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,So what's the function?
cs-410_1_11_133,cs-410,1,11,"00:10:40,580","00:10:42,399",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,"Well, it's this one."
cs-410_1_11_134,cs-410,1,11,"00:10:42,399","00:10:47,330",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,"See, if we have all the beta values,"
cs-410_1_11_135,cs-410,1,11,"00:10:47,330","00:10:52,810",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,All we need is to compute the Xi for that
cs-410_1_11_136,cs-410,1,11,"00:10:52,810","00:10:58,010",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,That will give us an estimated probability
cs-410_1_11_137,cs-410,1,11,"00:10:59,170","00:11:02,930",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,"Okay so, so much for"
cs-410_1_11_138,cs-410,1,11,"00:11:02,930","00:11:06,710",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,Let's also introduce another
cs-410_1_11_139,cs-410,1,11,"00:11:06,710","00:11:08,230",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,called K-Nearest Neighbors.
cs-410_1_11_140,cs-410,1,11,"00:11:08,230","00:11:12,340",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,"Now in general, I should say there"
cs-410_1_11_141,cs-410,1,11,"00:11:12,340","00:11:17,517",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,a thorough introduction to all of them is
cs-410_1_11_142,cs-410,1,11,"00:11:17,517","00:11:20,169",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,And you should take
cs-410_1_11_143,cs-410,1,11,"00:11:20,169","00:11:23,500",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,read more about machine
cs-410_1_11_144,cs-410,1,11,"00:11:23,500","00:11:27,950",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,"Here, I just want to include the basic"
cs-410_1_11_145,cs-410,1,11,"00:11:27,950","00:11:32,345",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,"used classifiers, since you might"
cs-410_1_11_146,cs-410,1,11,"00:11:32,345","00:11:36,610",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,So the second classifier is
cs-410_1_11_147,cs-410,1,11,"00:11:36,610","00:11:40,830",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,"In this approach,"
cs-410_1_11_148,cs-410,1,11,"00:11:40,830","00:11:45,615",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,the conditional probability of label
cs-410_1_11_149,cs-410,1,11,"00:11:45,615","00:11:49,360",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,So the idea is to keep all
cs-410_1_11_150,cs-410,1,11,"00:11:49,360","00:11:53,900",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,then once we see a text object that we
cs-410_1_11_151,cs-410,1,11,"00:11:53,900","00:11:59,290",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,the K examples in the training set and
cs-410_1_11_152,cs-410,1,11,"00:11:59,290","00:12:03,981",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=719,"Basically, this is to find"
cs-410_1_11_153,cs-410,1,11,"00:12:03,981","00:12:05,700",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,the training data set.
cs-410_1_11_154,cs-410,1,11,"00:12:05,700","00:12:08,314",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=725,So once we found the neighborhood and
cs-410_1_11_155,cs-410,1,11,"00:12:08,314","00:12:14,132",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,we found the object that are close to the
cs-410_1_11_156,cs-410,1,11,"00:12:14,132","00:12:18,620",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,and let's say we have found
cs-410_1_11_157,cs-410,1,11,"00:12:18,620","00:12:21,460",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,That's why this method is
cs-410_1_11_158,cs-410,1,11,"00:12:21,460","00:12:26,230",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,Then we're going to assign the category
cs-410_1_11_159,cs-410,1,11,"00:12:26,230","00:12:28,870",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,Basically we're going to allow
cs-410_1_11_160,cs-410,1,11,"00:12:28,870","00:12:32,050",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,the category of the objective that
cs-410_1_11_161,cs-410,1,11,"00:12:33,560","00:12:38,240",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,Now that means if most of them have
cs-410_1_11_162,cs-410,1,11,"00:12:38,240","00:12:41,790",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,"one, they're going to say this"
cs-410_1_11_163,cs-410,1,11,"00:12:43,100","00:12:47,820",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,This approach can also be improved by
cs-410_1_11_164,cs-410,1,11,"00:12:47,820","00:12:49,240",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=767,of a current object.
cs-410_1_11_165,cs-410,1,11,"00:12:49,240","00:12:53,560",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,"Basically, we can assume a closed"
cs-410_1_11_166,cs-410,1,11,"00:12:53,560","00:12:55,110",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,about the category of the subject.
cs-410_1_11_167,cs-410,1,11,"00:12:55,110","00:13:00,626",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,"So, we can give such a neighbor"
cs-410_1_11_168,cs-410,1,11,"00:13:00,626","00:13:04,650",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,And we can take away some of
cs-410_1_11_169,cs-410,1,11,"00:13:06,120","00:13:08,520",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,But the general idea is look
cs-410_1_11_170,cs-410,1,11,"00:13:08,520","00:13:13,270",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,then try to assess the category based
cs-410_1_11_171,cs-410,1,11,"00:13:13,270","00:13:15,745",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=793,"Intuitively, this makes a lot of sense."
cs-410_1_11_172,cs-410,1,11,"00:13:15,745","00:13:21,170",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=795,"But mathematically, this can also be"
cs-410_1_11_173,cs-410,1,11,"00:13:21,170","00:13:26,870",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=801,there's a conditional probability of
cs-410_1_11_174,cs-410,1,11,"00:13:28,190","00:13:33,640",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=808,Now I'm going to explain this intuition in
cs-410_1_11_175,cs-410,1,11,"00:13:33,640","00:13:40,530",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,emphasize that we do need a similarity
cs-410_1_11_176,cs-410,1,11,"00:13:40,530","00:13:43,874",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=820,"Note that in naive base class five,"
cs-410_1_11_177,cs-410,1,11,"00:13:43,874","00:13:48,160",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,"And in logistical regression, we did not"
cs-410_1_11_178,cs-410,1,11,"00:13:48,160","00:13:52,570",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,"either, but here we explicitly"
cs-410_1_11_179,cs-410,1,11,"00:13:52,570","00:13:57,500",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=832,Now this similarity function
cs-410_1_11_180,cs-410,1,11,"00:13:57,500","00:14:02,288",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,us to inject any of our
cs-410_1_11_181,cs-410,1,11,"00:14:02,288","00:14:07,420",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=842,Basically effective features
cs-410_1_11_182,cs-410,1,11,"00:14:07,420","00:14:12,770",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=847,make the objects that are on the same
cs-410_1_11_183,cs-410,1,11,"00:14:12,770","00:14:16,600",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=852,distinguishing objects
cs-410_1_11_184,cs-410,1,11,"00:14:16,600","00:14:21,100",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=856,So the design of this similarity function
cs-410_1_11_185,cs-410,1,11,"00:14:21,100","00:14:25,340",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=861,of the features in logistical
cs-410_1_11_186,cs-410,1,11,"00:14:25,340","00:14:28,350",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,So let's illustrate how K-NN works.
cs-410_1_11_187,cs-410,1,11,"00:14:28,350","00:14:32,360",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=868,Now suppose we have a lot
cs-410_1_11_188,cs-410,1,11,"00:14:32,360","00:14:38,612",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=872,And I've colored them differently and
cs-410_1_11_189,cs-410,1,11,"00:14:38,612","00:14:43,690",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=878,Now suppose we have a new object in
cs-410_1_11_190,cs-410,1,11,"00:14:43,690","00:14:46,530",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,"So according to this approach,"
cs-410_1_11_191,cs-410,1,11,"00:14:46,530","00:14:50,730",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,"Now, let's first think of a special"
cs-410_1_11_192,cs-410,1,11,"00:14:50,730","00:14:51,620",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,the closest neighbor.
cs-410_1_11_193,cs-410,1,11,"00:14:53,100","00:14:59,264",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=893,"Now in this case, let's assume the closest"
cs-410_1_11_194,cs-410,1,11,"00:14:59,264","00:15:04,191",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,"And so then we're going to say,"
cs-410_1_11_195,cs-410,1,11,"00:15:04,191","00:15:09,250",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,"object that is in category of diamonds,"
cs-410_1_11_196,cs-410,1,11,"00:15:09,250","00:15:11,945",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=909,"Then we're going to say, well,"
cs-410_1_11_197,cs-410,1,11,"00:15:11,945","00:15:17,250",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=911,we're going to assign the same
cs-410_1_11_198,cs-410,1,11,"00:15:17,250","00:15:22,346",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=917,But let's also look at another possibility
cs-410_1_11_199,cs-410,1,11,"00:15:22,346","00:15:24,730",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=922,so let's think about the four neighbors.
cs-410_1_11_200,cs-410,1,11,"00:15:26,060","00:15:31,090",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=926,"In this case, we're going to include a lot"
cs-410_1_11_201,cs-410,1,11,"00:15:31,090","00:15:32,970",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=931,"pink, right?"
cs-410_1_11_202,cs-410,1,11,"00:15:32,970","00:15:38,182",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=932,"So in this case now, we're going to"
cs-410_1_11_203,cs-410,1,11,"00:15:38,182","00:15:41,590",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=938,there are three neighbors
cs-410_1_11_204,cs-410,1,11,"00:15:41,590","00:15:43,020",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=941,"So if we take a vote,"
cs-410_1_11_205,cs-410,1,11,"00:15:43,020","00:15:48,001",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,then we'll conclude the object is
cs-410_1_11_206,cs-410,1,11,"00:15:48,001","00:15:52,252",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,So this both illustrates how
cs-410_1_11_207,cs-410,1,11,"00:15:52,252","00:15:57,021",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=952,also it illustrates some potential
cs-410_1_11_208,cs-410,1,11,"00:15:57,021","00:16:00,867",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,"Basically, the results might"
cs-410_1_11_209,cs-410,1,11,"00:16:00,867","00:16:03,703",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=960,k's an important parameter to optimize.
cs-410_1_11_210,cs-410,1,11,"00:16:03,703","00:16:07,871",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=963,"Now, you can intuitively imagine"
cs-410_1_11_211,cs-410,1,11,"00:16:07,871","00:16:11,800",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,"around this object, and"
cs-410_1_11_212,cs-410,1,11,"00:16:11,800","00:16:16,360",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,a lot of neighbors who will
cs-410_1_11_213,cs-410,1,11,"00:16:16,360","00:16:21,140",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=976,"But if we have only a few,"
cs-410_1_11_214,cs-410,1,11,"00:16:21,140","00:16:25,220",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=981,"So on the one hand,"
cs-410_1_11_215,cs-410,1,11,"00:16:25,220","00:16:26,850",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=985,And then we have more votes.
cs-410_1_11_216,cs-410,1,11,"00:16:26,850","00:16:31,770",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,"But on the other hand, as we try to find"
cs-410_1_11_217,cs-410,1,11,"00:16:31,770","00:16:36,990",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=991,on getting neighbors that are not
cs-410_1_11_218,cs-410,1,11,"00:16:36,990","00:16:40,210",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=996,They might actually be far away
cs-410_1_11_219,cs-410,1,11,"00:16:40,210","00:16:44,520",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,So although you get more neighbors but
cs-410_1_11_220,cs-410,1,11,"00:16:44,520","00:16:47,650",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1004,helpful because they are not
cs-410_1_11_221,cs-410,1,11,"00:16:47,650","00:16:51,150",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1007,So the parameter still has
cs-410_1_11_222,cs-410,1,11,"00:16:51,150","00:16:55,996",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1011,"And typically, you can optimize such"
cs-410_1_11_223,cs-410,1,11,"00:16:55,996","00:17:01,378",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1015,"Basically, you're going to separate"
cs-410_1_11_224,cs-410,1,11,"00:17:01,378","00:17:05,803",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1021,then you're going to use one
cs-410_1_11_225,cs-410,1,11,"00:17:05,803","00:17:10,778",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1025,the parameter k here or some other
cs-410_1_11_226,cs-410,1,11,"00:17:10,778","00:17:15,913",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1030,And then you're going to assume
cs-410_1_11_227,cs-410,1,11,"00:17:15,913","00:17:21,063",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,training that will be actually be
cs-410_1_11_228,cs-410,1,11,"00:17:23,103","00:17:24,257",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1043,"So as I mentioned,"
cs-410_1_11_229,cs-410,1,11,"00:17:24,257","00:17:29,234",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1044,K-NN can be actually regarded as estimate
cs-410_1_11_230,cs-410,1,11,"00:17:29,234","00:17:34,600",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1049,an that's why we put this in the category
cs-410_1_11_231,cs-410,1,11,"00:17:34,600","00:17:39,470",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1054,So the key assumption that we made in
cs-410_1_11_232,cs-410,1,11,"00:17:39,470","00:17:44,027",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1059,of the label given the document
cs-410_1_11_233,cs-410,1,11,"00:17:44,027","00:17:51,620",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1064,example probability of theta i
cs-410_1_11_234,cs-410,1,11,"00:17:51,620","00:17:56,890",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1071,And that just means we're going to assume
cs-410_1_11_235,cs-410,1,11,"00:17:56,890","00:18:01,570",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1076,all the documents in these region R here.
cs-410_1_11_236,cs-410,1,11,"00:18:01,570","00:18:05,260",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1081,And suppose we draw a neighborhood and
cs-410_1_11_237,cs-410,1,11,"00:18:05,260","00:18:10,320",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1085,since the data instances are very
cs-410_1_11_238,cs-410,1,11,"00:18:10,320","00:18:15,530",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1090,the conditional distribution of the label
cs-410_1_11_239,cs-410,1,11,"00:18:15,530","00:18:19,408",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1095,If these are very different
cs-410_1_11_240,cs-410,1,11,"00:18:19,408","00:18:23,136",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1099,the probability of c doc given
cs-410_1_11_241,cs-410,1,11,"00:18:23,136","00:18:24,976",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1103,So that's a very key assumption.
cs-410_1_11_242,cs-410,1,11,"00:18:24,976","00:18:29,481",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1104,And that's actually important assumption
cs-410_1_11_243,cs-410,1,11,"00:18:29,481","00:18:34,820",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1109,that would allow us to
cs-410_1_11_244,cs-410,1,11,"00:18:34,820","00:18:35,730",244,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1114,"But in reality,"
cs-410_1_11_245,cs-410,1,11,"00:18:35,730","00:18:39,560",245,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1115,"whether this is true of course,"
cs-410_1_11_246,cs-410,1,11,"00:18:39,560","00:18:43,610",246,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1119,Because neighborhood is largely
cs-410_1_11_247,cs-410,1,11,"00:18:43,610","00:18:48,180",247,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,If our similarity function captures
cs-410_1_11_248,cs-410,1,11,"00:18:48,180","00:18:51,290",248,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1128,distributions then these
cs-410_1_11_249,cs-410,1,11,"00:18:51,290","00:18:55,240",249,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1131,if our similarity function could
cs-410_1_11_250,cs-410,1,11,"00:18:55,240","00:18:58,110",250,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1135,assumption would be a problem and
cs-410_1_11_251,cs-410,1,11,"00:18:59,320","00:19:01,680",251,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1139,"Okay, let's proceed with these assumption."
cs-410_1_11_252,cs-410,1,11,"00:19:01,680","00:19:03,310",252,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1141,"Then what we are saying is that,"
cs-410_1_11_253,cs-410,1,11,"00:19:03,310","00:19:07,570",253,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1143,in order to estimate the probability
cs-410_1_11_254,cs-410,1,11,"00:19:07,570","00:19:14,230",254,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1147,We can try to estimate the probability of
cs-410_1_11_255,cs-410,1,11,"00:19:14,230","00:19:16,730",255,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1154,"Now, this has a benefit, of course,"
cs-410_1_11_256,cs-410,1,11,"00:19:16,730","00:19:20,340",256,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1156,of bringing additional data points to
cs-410_1_11_257,cs-410,1,11,"00:19:22,660","00:19:25,410",257,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1162,And so this is precisely the idea of K-NN.
cs-410_1_11_258,cs-410,1,11,"00:19:25,410","00:19:29,910",258,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1165,Basically now we can use
cs-410_1_11_259,cs-410,1,11,"00:19:29,910","00:19:33,870",259,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1169,all the documents in this region
cs-410_1_11_260,cs-410,1,11,"00:19:33,870","00:19:40,340",260,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1173,And I have even given a formula here where
cs-410_1_11_261,cs-410,1,11,"00:19:40,340","00:19:44,910",261,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1180,this region and then normalize that by the
cs-410_1_11_262,cs-410,1,11,"00:19:44,910","00:19:49,510",262,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1184,"So the numerator that you see here,"
cs-410_1_11_263,cs-410,1,11,"00:19:49,510","00:19:55,025",263,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1189,is a counter of the documents in
cs-410_1_11_264,cs-410,1,11,"00:19:55,025","00:19:57,910",264,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1195,Since these are training document and
cs-410_1_11_265,cs-410,1,11,"00:19:57,910","00:20:01,394",265,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1197,We can simply count how many
cs-410_1_11_266,cs-410,1,11,"00:20:01,394","00:20:03,491",266,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1201,"How many times we have the same signs,"
cs-410_1_11_267,cs-410,1,11,"00:20:03,491","00:20:07,269",267,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1203,And then the denominator is just
cs-410_1_11_268,cs-410,1,11,"00:20:07,269","00:20:08,981",268,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1207,documents in this region.
cs-410_1_11_269,cs-410,1,11,"00:20:08,981","00:20:12,781",269,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1208,So this gives us a rough estimate of
cs-410_1_11_270,cs-410,1,11,"00:20:12,781","00:20:13,661",270,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1212,neighborhood.
cs-410_1_11_271,cs-410,1,11,"00:20:13,661","00:20:17,539",271,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1213,And we are going to assign
cs-410_1_11_272,cs-410,1,11,"00:20:17,539","00:20:21,821",272,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1217,to our data object since
cs-410_1_11_273,cs-410,1,11,"00:20:21,821","00:20:31,821",273,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1221,[MUSIC]
cs-410_1_12_1,cs-410,1,12,"00:00:00,401","00:00:07,552",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_1_12_2,cs-410,1,12,"00:00:07,552","00:00:10,524",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about
cs-410_1_12_3,cs-410,1,12,"00:00:10,524","00:00:16,097",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,Contextual Text Mining called Contextual
cs-410_1_12_4,cs-410,1,12,"00:00:19,162","00:00:23,930",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,"In this lecture, we're going to continue"
cs-410_1_12_5,cs-410,1,12,"00:00:23,930","00:00:28,990",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,And we're going to introduce Contextual
cs-410_1_12_6,cs-410,1,12,"00:00:28,990","00:00:32,630",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,as exchanging of POS for
cs-410_1_12_7,cs-410,1,12,"00:00:34,390","00:00:40,310",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,Recall that in contextual text mining
cs-410_1_12_8,cs-410,1,12,"00:00:40,310","00:00:42,285",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,in consideration of the context so
cs-410_1_12_9,cs-410,1,12,"00:00:42,285","00:00:46,950",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,that we can associate the topics with a
cs-410_1_12_10,cs-410,1,12,"00:00:48,240","00:00:54,033",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"So in this approach, contextual"
cs-410_1_12_11,cs-410,1,12,"00:00:54,033","00:00:58,487",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,"or CPLSA, the main idea is to"
cs-410_1_12_12,cs-410,1,12,"00:00:58,487","00:01:01,890",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,context variables into a generating model.
cs-410_1_12_13,cs-410,1,12,"00:01:03,150","00:01:06,860",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,Recall that before when we generate
cs-410_1_12_14,cs-410,1,12,"00:01:06,860","00:01:10,730",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"wIth some topics, and"
cs-410_1_12_15,cs-410,1,12,"00:01:10,730","00:01:18,130",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"But here, we're going to add context"
cs-410_1_12_16,cs-410,1,12,"00:01:18,130","00:01:23,500",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,and also the content of topics
cs-410_1_12_17,cs-410,1,12,"00:01:23,500","00:01:27,607",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"Or in other words, we're going to let"
cs-410_1_12_18,cs-410,1,12,"00:01:27,607","00:01:28,900",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,the content of a topic.
cs-410_1_12_19,cs-410,1,12,"00:01:31,172","00:01:37,370",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,The consequences that this will enable
cs-410_1_12_20,cs-410,1,12,"00:01:37,370","00:01:41,320",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"Make the topics more interesting,"
cs-410_1_12_21,cs-410,1,12,"00:01:41,320","00:01:46,120",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,Because we can then have topics
cs-410_1_12_22,cs-410,1,12,"00:01:46,120","00:01:49,070",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,specifically to a particular
cs-410_1_12_23,cs-410,1,12,"00:01:49,070","00:01:50,590",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"For example, a particular time period."
cs-410_1_12_24,cs-410,1,12,"00:01:52,020","00:01:55,639",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,"As an extension of PLSA model,"
cs-410_1_12_25,cs-410,1,12,"00:01:55,639","00:02:01,330",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,CPLSA does the following changes.
cs-410_1_12_26,cs-410,1,12,"00:02:01,330","00:02:05,770",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,Firstly it would model the conditional
cs-410_1_12_27,cs-410,1,12,"00:02:07,110","00:02:12,990",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,That clearly suggests that the generation
cs-410_1_12_28,cs-410,1,12,"00:02:12,990","00:02:16,520",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,and that allows us to bring
cs-410_1_12_29,cs-410,1,12,"00:02:18,230","00:02:22,300",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,"Secondly, it makes two specific"
cs-410_1_12_30,cs-410,1,12,"00:02:22,300","00:02:24,650",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,of topics on context.
cs-410_1_12_31,cs-410,1,12,"00:02:24,650","00:02:28,420",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,One is to assume that depending on
cs-410_1_12_32,cs-410,1,12,"00:02:28,420","00:02:33,630",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,"periods or different locations, we assume"
cs-410_1_12_33,cs-410,1,12,"00:02:33,630","00:02:37,370",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,or different versions of word
cs-410_1_12_34,cs-410,1,12,"00:02:38,540","00:02:42,260",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,And this assumption allows
cs-410_1_12_35,cs-410,1,12,"00:02:42,260","00:02:45,430",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,variations of the same topic
cs-410_1_12_36,cs-410,1,12,"00:02:46,500","00:02:53,059",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,The other is that we assume the topic
cs-410_1_12_37,cs-410,1,12,"00:02:55,150","00:02:56,810",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,That means depending on the time or
cs-410_1_12_38,cs-410,1,12,"00:02:56,810","00:02:59,630",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,"location, we might cover"
cs-410_1_12_39,cs-410,1,12,"00:03:00,670","00:03:03,890",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"Again, this dependency"
cs-410_1_12_40,cs-410,1,12,"00:03:03,890","00:03:08,680",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,capture the association of
cs-410_1_12_41,cs-410,1,12,"00:03:08,680","00:03:14,540",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,We can still use the EM algorithm to solve
cs-410_1_12_42,cs-410,1,12,"00:03:16,280","00:03:22,520",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"And in this case, the estimated parameters"
cs-410_1_12_43,cs-410,1,12,"00:03:22,520","00:03:23,590",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"And in particular,"
cs-410_1_12_44,cs-410,1,12,"00:03:23,590","00:03:29,940",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,a lot of conditional probabilities
cs-410_1_12_45,cs-410,1,12,"00:03:29,940","00:03:33,090",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,And this is what allows you
cs-410_1_12_46,cs-410,1,12,"00:03:33,090","00:03:34,610",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,So this is the basic idea.
cs-410_1_12_47,cs-410,1,12,"00:03:35,750","00:03:41,470",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"Now, we don't have time to"
cs-410_1_12_48,cs-410,1,12,"00:03:41,470","00:03:45,700",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,but there are references here that you
cs-410_1_12_49,cs-410,1,12,"00:03:45,700","00:03:52,120",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,Here I just want to explain the high
cs-410_1_12_50,cs-410,1,12,"00:03:52,120","00:03:55,610",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,Particularly I want to explain
cs-410_1_12_51,cs-410,1,12,"00:03:55,610","00:04:00,330",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,Of text data that has context
cs-410_1_12_52,cs-410,1,12,"00:04:01,550","00:04:05,660",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,"So as you see here, we can assume"
cs-410_1_12_53,cs-410,1,12,"00:04:05,660","00:04:11,410",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"For example, some topics might represent"
cs-410_1_12_54,cs-410,1,12,"00:04:11,410","00:04:14,270",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,donation Or the city of New Orleans.
cs-410_1_12_55,cs-410,1,12,"00:04:14,270","00:04:18,803",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,Now this example is in the context
cs-410_1_12_56,cs-410,1,12,"00:04:18,803","00:04:20,570",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,that hit New Orleans.
cs-410_1_12_57,cs-410,1,12,"00:04:22,915","00:04:27,400",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,Now as you can see we
cs-410_1_12_58,cs-410,1,12,"00:04:27,400","00:04:31,548",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,views associated with each of the topics.
cs-410_1_12_59,cs-410,1,12,"00:04:31,548","00:04:36,530",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,"And these are shown as View 1,"
cs-410_1_12_60,cs-410,1,12,"00:04:36,530","00:04:41,475",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,Each view is a different
cs-410_1_12_61,cs-410,1,12,"00:04:41,475","00:04:44,715",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,And these views are tied
cs-410_1_12_62,cs-410,1,12,"00:04:44,715","00:04:50,125",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,"For example, tied to the location Texas,"
cs-410_1_12_63,cs-410,1,12,"00:04:50,125","00:04:54,475",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,or the occupation of the author
cs-410_1_12_64,cs-410,1,12,"00:04:56,205","00:05:01,560",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"Now, on the right side, now we assume"
cs-410_1_12_65,cs-410,1,12,"00:05:01,560","00:05:04,370",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,So the time is known to be July 2005.
cs-410_1_12_66,cs-410,1,12,"00:05:04,370","00:05:06,710",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"The location is Texas, etc."
cs-410_1_12_67,cs-410,1,12,"00:05:06,710","00:05:11,410",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,And such context information is
cs-410_1_12_68,cs-410,1,12,"00:05:11,410","00:05:13,300",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,So we're not going to just model the text.
cs-410_1_12_69,cs-410,1,12,"00:05:15,100","00:05:20,980",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,And so one idea here is to model
cs-410_1_12_70,cs-410,1,12,"00:05:20,980","00:05:21,920",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,various content.
cs-410_1_12_71,cs-410,1,12,"00:05:21,920","00:05:25,970",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,And this gives us different views
cs-410_1_12_72,cs-410,1,12,"00:05:27,720","00:05:32,360",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,Now on the bottom you will see the theme
cs-410_1_12_73,cs-410,1,12,"00:05:32,360","00:05:39,310",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,according to these context
cs-410_1_12_74,cs-410,1,12,"00:05:39,310","00:05:44,320",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"of a location like Texas, people might"
cs-410_1_12_75,cs-410,1,12,"00:05:44,320","00:05:46,130",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,That's New Orleans.
cs-410_1_12_76,cs-410,1,12,"00:05:46,130","00:05:47,690",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,That's visualized here.
cs-410_1_12_77,cs-410,1,12,"00:05:47,690","00:05:50,930",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,"But in a certain time period,"
cs-410_1_12_78,cs-410,1,12,"00:05:50,930","00:05:56,280",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,maybe Particular topic and
cs-410_1_12_79,cs-410,1,12,"00:05:56,280","00:06:00,980",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,So this variation is
cs-410_1_12_80,cs-410,1,12,"00:06:00,980","00:06:07,685",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,So to generate the searcher document With
cs-410_1_12_81,cs-410,1,12,"00:06:08,695","00:06:14,055",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,And this view of course now could
cs-410_1_12_82,cs-410,1,12,"00:06:14,055","00:06:17,080",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"Let's say, we have taken this"
cs-410_1_12_83,cs-410,1,12,"00:06:17,080","00:06:18,310",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,In the middle.
cs-410_1_12_84,cs-410,1,12,"00:06:18,310","00:06:21,850",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,"So now, we will have a specific"
cs-410_1_12_85,cs-410,1,12,"00:06:21,850","00:06:25,030",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,"Now, you can see some probabilities"
cs-410_1_12_86,cs-410,1,12,"00:06:26,710","00:06:28,830",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"Now, once we have chosen a view,"
cs-410_1_12_87,cs-410,1,12,"00:06:28,830","00:06:34,400",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,now the situation will be very similar
cs-410_1_12_88,cs-410,1,12,"00:06:34,400","00:06:38,860",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,We assume we have got word distribution
cs-410_1_12_89,cs-410,1,12,"00:06:39,870","00:06:43,070",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"And then next, we will also choose"
cs-410_1_12_90,cs-410,1,12,"00:06:43,070","00:06:47,988",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,we're going to choose a particular
cs-410_1_12_91,cs-410,1,12,"00:06:47,988","00:06:55,305",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,"before is fixed in PLSA, and"
cs-410_1_12_92,cs-410,1,12,"00:06:55,305","00:06:57,825",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,Each document has just one
cs-410_1_12_93,cs-410,1,12,"00:06:58,885","00:07:03,925",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"Now here, because we consider context, so"
cs-410_1_12_94,cs-410,1,12,"00:07:03,925","00:07:08,770",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,of Topics can vary depending on the
cs-410_1_12_95,cs-410,1,12,"00:07:10,020","00:07:13,470",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,"So, for example,"
cs-410_1_12_96,cs-410,1,12,"00:07:13,470","00:07:19,090",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,Let's say in this case we picked
cs-410_1_12_97,cs-410,1,12,"00:07:20,590","00:07:23,440",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,Now with the coverage and
cs-410_1_12_98,cs-410,1,12,"00:07:23,440","00:07:26,590",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,we can generate a document in
cs-410_1_12_99,cs-410,1,12,"00:07:26,590","00:07:32,450",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"So what it means, we're going to"
cs-410_1_12_100,cs-410,1,12,"00:07:32,450","00:07:34,880",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,to choose one of these three topics.
cs-410_1_12_101,cs-410,1,12,"00:07:34,880","00:07:38,230",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,Let's say we have picked the yellow topic.
cs-410_1_12_102,cs-410,1,12,"00:07:38,230","00:07:43,450",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,Then we'll draw a word from this
cs-410_1_12_103,cs-410,1,12,"00:07:44,760","00:07:46,880",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,"Okay, so"
cs-410_1_12_104,cs-410,1,12,"00:07:46,880","00:07:50,840",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,And then next time we might
cs-410_1_12_105,cs-410,1,12,"00:07:50,840","00:07:53,640",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"we'll get donate, etc."
cs-410_1_12_106,cs-410,1,12,"00:07:53,640","00:07:55,550",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,Until we generate all the words.
cs-410_1_12_107,cs-410,1,12,"00:07:55,550","00:07:58,550",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,And this is basically
cs-410_1_12_108,cs-410,1,12,"00:08:00,200","00:08:05,220",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,So the main difference is
cs-410_1_12_109,cs-410,1,12,"00:08:05,220","00:08:11,250",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,"And the word distribution,"
cs-410_1_12_110,cs-410,1,12,"00:08:11,250","00:08:16,050",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,in other words we have extra switches
cs-410_1_12_111,cs-410,1,12,"00:08:16,050","00:08:20,950",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,control the choices of different views
cs-410_1_12_112,cs-410,1,12,"00:08:22,010","00:08:25,430",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,And naturally the model we have
cs-410_1_12_113,cs-410,1,12,"00:08:25,430","00:08:29,010",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,But once we can estimate those
cs-410_1_12_114,cs-410,1,12,"00:08:29,010","00:08:33,080",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,then we will be able to understand
cs-410_1_12_115,cs-410,1,12,"00:08:33,080","00:08:36,020",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,or context specific coverages of topics.
cs-410_1_12_116,cs-410,1,12,"00:08:36,020","00:08:38,850",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,And this is precisely what we
cs-410_1_12_117,cs-410,1,12,"00:08:40,450","00:08:42,950",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,So here are some simple results.
cs-410_1_12_118,cs-410,1,12,"00:08:42,950","00:08:44,340",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,From using such a model.
cs-410_1_12_119,cs-410,1,12,"00:08:44,340","00:08:48,240",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,"Not necessary exactly the same model,"
cs-410_1_12_120,cs-410,1,12,"00:08:48,240","00:08:50,860",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,So on this slide you see
cs-410_1_12_121,cs-410,1,12,"00:08:50,860","00:08:54,950",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,comparing news articles about Iraq War and
cs-410_1_12_122,cs-410,1,12,"00:08:56,315","00:09:02,855",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,Now we have about 30 articles on Iraq
cs-410_1_12_123,cs-410,1,12,"00:09:02,855","00:09:08,852",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,"And in this case,"
cs-410_1_12_124,cs-410,1,12,"00:09:08,852","00:09:11,332",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,It's covered in both sets of articles and
cs-410_1_12_125,cs-410,1,12,"00:09:11,332","00:09:17,352",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,the differences of variations of
cs-410_1_12_126,cs-410,1,12,"00:09:18,622","00:09:23,400",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,So in this case the context is explicitly
cs-410_1_12_127,cs-410,1,12,"00:09:25,040","00:09:30,420",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,And we see the results here
cs-410_1_12_128,cs-410,1,12,"00:09:30,420","00:09:36,040",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,theme that's corresponding to
cs-410_1_12_129,cs-410,1,12,"00:09:36,040","00:09:42,260",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,And there is a common theme indicting that
cs-410_1_12_130,cs-410,1,12,"00:09:42,260","00:09:45,630",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,It's a common topic covered
cs-410_1_12_131,cs-410,1,12,"00:09:45,630","00:09:48,970",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,And that's indicated by the high
cs-410_1_12_132,cs-410,1,12,"00:09:48,970","00:09:49,860",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,nations.
cs-410_1_12_133,cs-410,1,12,"00:09:51,160","00:09:54,680",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,"Now if you know the background,"
cs-410_1_12_134,cs-410,1,12,"00:09:54,680","00:10:00,340",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,this topic is indeed very
cs-410_1_12_135,cs-410,1,12,"00:10:00,340","00:10:04,900",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,If you look at the column further and
cs-410_1_12_136,cs-410,1,12,"00:10:04,900","00:10:09,336",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,two cells of word
cs-410_1_12_137,cs-410,1,12,"00:10:09,336","00:10:14,790",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,collection specific variations
cs-410_1_12_138,cs-410,1,12,"00:10:14,790","00:10:16,660",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,"So it indicates that the Iraq War,"
cs-410_1_12_139,cs-410,1,12,"00:10:16,660","00:10:21,060",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,United Nations was more involved
cs-410_1_12_140,cs-410,1,12,"00:10:21,060","00:10:25,710",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,the Afghanistan War it was more involved
cs-410_1_12_141,cs-410,1,12,"00:10:25,710","00:10:29,060",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,It's a different variation of
cs-410_1_12_142,cs-410,1,12,"00:10:30,100","00:10:33,140",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,So this shows that by
cs-410_1_12_143,cs-410,1,12,"00:10:33,140","00:10:36,215",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,In this case different the walls or
cs-410_1_12_144,cs-410,1,12,"00:10:36,215","00:10:40,034",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=636,We can have topical variations
cs-410_1_12_145,cs-410,1,12,"00:10:40,034","00:10:45,250",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,to review the differences of coverage
cs-410_1_12_146,cs-410,1,12,"00:10:46,290","00:10:50,200",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,Now similarly if you look at
cs-410_1_12_147,cs-410,1,12,"00:10:50,200","00:10:52,710",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,"it has to do with the killing of people,"
cs-410_1_12_148,cs-410,1,12,"00:10:52,710","00:10:56,320",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,it's not surprising if you know
cs-410_1_12_149,cs-410,1,12,"00:10:56,320","00:10:59,660",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,"All the wars involve killing of people,"
cs-410_1_12_150,cs-410,1,12,"00:10:59,660","00:11:03,640",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,imagine if you are not familiar
cs-410_1_12_151,cs-410,1,12,"00:11:03,640","00:11:05,120",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,"We have a lot of text articles, and"
cs-410_1_12_152,cs-410,1,12,"00:11:05,120","00:11:10,230",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,such a technique can reveal the common
cs-410_1_12_153,cs-410,1,12,"00:11:10,230","00:11:14,715",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,It can be used to review common topics
cs-410_1_12_154,cs-410,1,12,"00:11:14,715","00:11:19,581",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,If you look at of course in
cs-410_1_12_155,cs-410,1,12,"00:11:19,581","00:11:26,143",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,you see variations of killing of people
cs-410_1_12_156,cs-410,1,12,"00:11:28,279","00:11:31,582",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=688,And here is another example of results
cs-410_1_12_157,cs-410,1,12,"00:11:31,582","00:11:36,440",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,obtained from blog articles
cs-410_1_12_158,cs-410,1,12,"00:11:37,470","00:11:42,320",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,"In this case,"
cs-410_1_12_159,cs-410,1,12,"00:11:42,320","00:11:46,090",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,the trends of topics over time.
cs-410_1_12_160,cs-410,1,12,"00:11:47,240","00:11:52,980",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,And the top one shows just
cs-410_1_12_161,cs-410,1,12,"00:11:52,980","00:11:58,980",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,"One is oil price, and one is about"
cs-410_1_12_162,cs-410,1,12,"00:12:00,060","00:12:06,280",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=720,Now these topics are obtained from
cs-410_1_12_163,cs-410,1,12,"00:12:07,300","00:12:09,395",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,And people talk about these topics.
cs-410_1_12_164,cs-410,1,12,"00:12:09,395","00:12:12,370",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=729,And end up teaching to some other topics.
cs-410_1_12_165,cs-410,1,12,"00:12:12,370","00:12:15,000",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,But the visualisation shows
cs-410_1_12_166,cs-410,1,12,"00:12:15,000","00:12:18,020",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=735,we can have conditional
cs-410_1_12_167,cs-410,1,12,"00:12:18,020","00:12:19,660",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,Given a topic.
cs-410_1_12_168,cs-410,1,12,"00:12:19,660","00:12:23,420",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=739,So this allows us to plot
cs-410_1_12_169,cs-410,1,12,"00:12:23,420","00:12:26,000",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,the curve is like what you're seeing here.
cs-410_1_12_170,cs-410,1,12,"00:12:26,000","00:12:31,560",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,"We see that, initially, the two"
cs-410_1_12_171,cs-410,1,12,"00:12:31,560","00:12:40,010",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=751,But later we see the topic of New Orleans
cs-410_1_12_172,cs-410,1,12,"00:12:40,010","00:12:44,060",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=760,And this turns out to be
cs-410_1_12_173,cs-410,1,12,"00:12:44,060","00:12:49,010",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,"the time period when another hurricane,"
cs-410_1_12_174,cs-410,1,12,"00:12:49,010","00:12:52,470",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,And that apparently triggered more
cs-410_1_12_175,cs-410,1,12,"00:12:54,900","00:13:00,010",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,The bottom curve shows
cs-410_1_12_176,cs-410,1,12,"00:13:00,010","00:13:05,320",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,about flooding of the city by block
cs-410_1_12_177,cs-410,1,12,"00:13:05,320","00:13:11,620",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,And it also shows some shift of
cs-410_1_12_178,cs-410,1,12,"00:13:11,620","00:13:19,150",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=791,people's migrating from the state
cs-410_1_12_179,cs-410,1,12,"00:13:20,570","00:13:25,650",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,So in this case we can see the time can
cs-410_1_12_180,cs-410,1,12,"00:13:25,650","00:13:26,150",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=805,topics.
cs-410_1_12_181,cs-410,1,12,"00:13:27,780","00:13:33,070",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,These are some additional
cs-410_1_12_182,cs-410,1,12,"00:13:33,070","00:13:37,850",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,In this case it was about
cs-410_1_12_183,cs-410,1,12,"00:13:37,850","00:13:41,690",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,And there was some criticism about
cs-410_1_12_184,cs-410,1,12,"00:13:41,690","00:13:42,649",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=821,in the case of Hurricane Katrina.
cs-410_1_12_185,cs-410,1,12,"00:13:44,020","00:13:48,280",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,And the discussion now is
cs-410_1_12_186,cs-410,1,12,"00:13:48,280","00:13:54,260",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,And these visualizations show the coverage
cs-410_1_12_187,cs-410,1,12,"00:13:54,260","00:13:59,610",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=834,And initially it's covered
cs-410_1_12_188,cs-410,1,12,"00:13:59,610","00:14:05,530",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=839,"in the South, but then gradually"
cs-410_1_12_189,cs-410,1,12,"00:14:05,530","00:14:09,760",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=845,"But in week four,"
cs-410_1_12_190,cs-410,1,12,"00:14:09,760","00:14:14,370",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=849,we see a pattern that's very similar
cs-410_1_12_191,cs-410,1,12,"00:14:14,370","00:14:18,700",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=854,And that's when again
cs-410_1_12_192,cs-410,1,12,"00:14:18,700","00:14:22,540",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=858,So such a technique would allow
cs-410_1_12_193,cs-410,1,12,"00:14:22,540","00:14:24,960",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,to examine their issues of topics.
cs-410_1_12_194,cs-410,1,12,"00:14:24,960","00:14:27,280",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=864,And of course the moral
cs-410_1_12_195,cs-410,1,12,"00:14:27,280","00:14:30,980",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=867,you can apply this to any
cs-410_1_12_196,cs-410,1,12,"00:14:30,980","00:14:32,850",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,To review spatial temporal patterns.
cs-410_1_12_197,cs-410,1,12,"00:14:34,460","00:14:37,390",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=874,His view found another application
cs-410_1_12_198,cs-410,1,12,"00:14:37,390","00:14:41,960",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,where we look at the use of the model for
cs-410_1_12_199,cs-410,1,12,"00:14:43,290","00:14:46,370",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,So here we're looking at the research
cs-410_1_12_200,cs-410,1,12,"00:14:46,370","00:14:49,480",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,"IR, particularly SIGIR papers."
cs-410_1_12_201,cs-410,1,12,"00:14:49,480","00:14:53,180",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=889,And the topic we are focusing on
cs-410_1_12_202,cs-410,1,12,"00:14:53,180","00:14:58,440",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=893,And you can see the top words with high
cs-410_1_12_203,cs-410,1,12,"00:14:59,580","00:15:04,290",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,And then we hope to examine
cs-410_1_12_204,cs-410,1,12,"00:15:04,290","00:15:08,290",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,"One is a start of TREC, for"
cs-410_1_12_205,cs-410,1,12,"00:15:08,290","00:15:11,459",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=908,This is a major evaluation
cs-410_1_12_206,cs-410,1,12,"00:15:11,459","00:15:16,722",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=911,"government, and was launched in 1992 or"
cs-410_1_12_207,cs-410,1,12,"00:15:16,722","00:15:20,690",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=916,And that is known to have made a impact on
cs-410_1_12_208,cs-410,1,12,"00:15:20,690","00:15:22,790",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=920,the topics of research
cs-410_1_12_209,cs-410,1,12,"00:15:23,870","00:15:28,680",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,The other is the publication of
cs-410_1_12_210,cs-410,1,12,"00:15:28,680","00:15:31,850",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=928,This is about a language model
cs-410_1_12_211,cs-410,1,12,"00:15:31,850","00:15:36,440",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=931,It's also known to have made a high
cs-410_1_12_212,cs-410,1,12,"00:15:36,440","00:15:39,780",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=936,So we hope to use this kind of
cs-410_1_12_213,cs-410,1,12,"00:15:39,780","00:15:44,090",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,The idea here is simply to
cs-410_1_12_214,cs-410,1,12,"00:15:44,090","00:15:48,585",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=944,And use these events to divide
cs-410_1_12_215,cs-410,1,12,"00:15:48,585","00:15:51,397",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,For the event and
cs-410_1_12_216,cs-410,1,12,"00:15:51,397","00:15:54,417",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=951,And then we can compare
cs-410_1_12_217,cs-410,1,12,"00:15:54,417","00:15:57,875",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=954,"The and the variations, etc."
cs-410_1_12_218,cs-410,1,12,"00:15:57,875","00:16:02,750",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,"So in this case,"
cs-410_1_12_219,cs-410,1,12,"00:16:02,750","00:16:07,120",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=962,retrieval models was mostly a vector
cs-410_1_12_220,cs-410,1,12,"00:16:07,120","00:16:08,800",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,"But the after Trec,"
cs-410_1_12_221,cs-410,1,12,"00:16:08,800","00:16:13,975",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=968,apparently the study of retrieval models
cs-410_1_12_222,cs-410,1,12,"00:16:13,975","00:16:18,440",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=973,That seems to suggest some
cs-410_1_12_223,cs-410,1,12,"00:16:18,440","00:16:22,980",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=978,"example, email was used in"
cs-410_1_12_224,cs-410,1,12,"00:16:22,980","00:16:26,550",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=982,subtopical retrieval was another
cs-410_1_12_225,cs-410,1,12,"00:16:28,200","00:16:32,461",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=988,"On the bottom,"
cs-410_1_12_226,cs-410,1,12,"00:16:32,461","00:16:36,300",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=992,with the propagation of
cs-410_1_12_227,cs-410,1,12,"00:16:36,300","00:16:40,631",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=996,"Before, we have those classic"
cs-410_1_12_228,cs-410,1,12,"00:16:40,631","00:16:44,600",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,"logic model, Boolean etc., but after 1998,"
cs-410_1_12_229,cs-410,1,12,"00:16:44,600","00:16:50,430",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1004,we see clear dominance of language
cs-410_1_12_230,cs-410,1,12,"00:16:50,430","00:16:54,580",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1010,"And we see words like language model,"
cs-410_1_12_231,cs-410,1,12,"00:16:54,580","00:17:00,764",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1014,So this technique here can use events as
cs-410_1_12_232,cs-410,1,12,"00:17:00,764","00:17:03,403",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1020,Again the technique is generals so
cs-410_1_12_233,cs-410,1,12,"00:17:03,403","00:17:07,370",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1023,you can use this to analyze
cs-410_1_12_234,cs-410,1,12,"00:17:07,370","00:17:10,240",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1027,Here are some suggested readings.
cs-410_1_12_235,cs-410,1,12,"00:17:11,940","00:17:20,090",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1031,The first is paper about simple staging of
cs-410_1_12_236,cs-410,1,12,"00:17:21,270","00:17:24,610",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1041,It's to perform comparative
cs-410_1_12_237,cs-410,1,12,"00:17:24,610","00:17:27,410",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1044,extract common topics shared
cs-410_1_12_238,cs-410,1,12,"00:17:27,410","00:17:29,930",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1047,And there are variations
cs-410_1_12_239,cs-410,1,12,"00:17:31,010","00:17:35,540",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1051,The second one is the main
cs-410_1_12_240,cs-410,1,12,"00:17:35,540","00:17:38,830",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1055,Was a discussion of a lot of applications.
cs-410_1_12_241,cs-410,1,12,"00:17:38,830","00:17:44,889",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1058,The third one has a lot of details
cs-410_1_12_242,cs-410,1,12,"00:17:44,889","00:17:47,679",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1064,the Hurricane Katrina example.
cs-410_1_12_243,cs-410,1,12,"00:17:47,679","00:17:57,679",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1067,[MUSIC]
cs-410_1_2_1,cs-410,1,2,"00:00:00,012","00:00:08,850",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_2_2,cs-410,1,2,"00:00:08,850","00:00:12,546",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=8,"In this lecture, we are going to talk about how"
cs-410_1_2_3,cs-410,1,2,"00:00:12,546","00:00:14,288",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,of the vector space model.
cs-410_1_2_4,cs-410,1,2,"00:00:17,448","00:00:22,110",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,This is a continued discussion
cs-410_1_2_5,cs-410,1,2,"00:00:22,110","00:00:26,859",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,We're going to focus on how to improve
cs-410_1_2_6,cs-410,1,2,"00:00:30,259","00:00:32,327",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"In the previous lecture,"
cs-410_1_2_7,cs-410,1,2,"00:00:32,327","00:00:38,155",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,you have seen that with simple
cs-410_1_2_8,cs-410,1,2,"00:00:38,155","00:00:43,889",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,we can come up with a simple scoring
cs-410_1_2_9,cs-410,1,2,"00:00:43,889","00:00:49,440",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,an account of how many unique query
cs-410_1_2_10,cs-410,1,2,"00:00:50,540","00:00:56,862",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,We also have seen that this function
cs-410_1_2_11,cs-410,1,2,"00:00:56,862","00:01:00,226",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,"In particular,"
cs-410_1_2_12,cs-410,1,2,"00:01:00,226","00:01:05,210",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,they will all get the same score because
cs-410_1_2_13,cs-410,1,2,"00:01:06,322","00:01:11,330",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,But intuitively we would like
cs-410_1_2_14,cs-410,1,2,"00:01:11,330","00:01:13,070",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,d2 is really not relevant.
cs-410_1_2_15,cs-410,1,2,"00:01:14,750","00:01:22,600",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,So the problem here is that this function
cs-410_1_2_16,cs-410,1,2,"00:01:22,600","00:01:27,504",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,"First, we would like to give"
cs-410_1_2_17,cs-410,1,2,"00:01:27,504","00:01:31,297",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,matched presidential more times than d3.
cs-410_1_2_18,cs-410,1,2,"00:01:32,520","00:01:37,657",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"Second, intuitively, matching presidential"
cs-410_1_2_19,cs-410,1,2,"00:01:37,657","00:01:42,808",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"matching about, because about is a very"
cs-410_1_2_20,cs-410,1,2,"00:01:42,808","00:01:44,970",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,It doesn't really carry that much content.
cs-410_1_2_21,cs-410,1,2,"00:01:47,480","00:01:48,945",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"So in this lecture,"
cs-410_1_2_22,cs-410,1,2,"00:01:48,945","00:01:53,868",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,let's see how we can improve the model
cs-410_1_2_23,cs-410,1,2,"00:01:53,868","00:01:59,990",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,It's worth thinking at this point
cs-410_1_2_24,cs-410,1,2,"00:02:01,420","00:02:06,600",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,If we look back at assumptions we have
cs-410_1_2_25,cs-410,1,2,"00:02:06,600","00:02:11,645",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,"space model,"
cs-410_1_2_26,cs-410,1,2,"00:02:11,645","00:02:15,200",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,is really coming from
cs-410_1_2_27,cs-410,1,2,"00:02:15,200","00:02:19,391",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,"In particular, it has to do with how we"
cs-410_1_2_28,cs-410,1,2,"00:02:22,380","00:02:25,390",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"So then naturally,"
cs-410_1_2_29,cs-410,1,2,"00:02:25,390","00:02:27,780",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,we have to revisit those assumptions.
cs-410_1_2_30,cs-410,1,2,"00:02:27,780","00:02:34,755",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,Perhaps we will have to use different ways
cs-410_1_2_31,cs-410,1,2,"00:02:34,755","00:02:39,130",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"In particular, we have to place"
cs-410_1_2_32,cs-410,1,2,"00:02:41,690","00:02:45,708",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,So let's see how we can improve this.
cs-410_1_2_33,cs-410,1,2,"00:02:45,708","00:02:50,248",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,One natural thought is in order to
cs-410_1_2_34,cs-410,1,2,"00:02:50,248","00:02:51,266",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,"the document,"
cs-410_1_2_35,cs-410,1,2,"00:02:51,266","00:02:57,270",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,we should consider the term frequency
cs-410_1_2_36,cs-410,1,2,"00:02:57,270","00:03:02,900",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,In order to consider the difference
cs-410_1_2_37,cs-410,1,2,"00:03:02,900","00:03:07,620",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,term occurred multiple times and one
cs-410_1_2_38,cs-410,1,2,"00:03:07,620","00:03:12,010",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,"we have to consider the term frequency,"
cs-410_1_2_39,cs-410,1,2,"00:03:13,130","00:03:18,200",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,"In the simplest model, we only modeled"
cs-410_1_2_40,cs-410,1,2,"00:03:18,200","00:03:25,106",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,We ignored the actual number of times
cs-410_1_2_41,cs-410,1,2,"00:03:25,106","00:03:26,566",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,So let's add this back.
cs-410_1_2_42,cs-410,1,2,"00:03:26,566","00:03:30,592",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,So we're going to then
cs-410_1_2_43,cs-410,1,2,"00:03:30,592","00:03:34,214",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,a vector with term frequency as element.
cs-410_1_2_44,cs-410,1,2,"00:03:34,214","00:03:39,573",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"So that is to say, now the elements"
cs-410_1_2_45,cs-410,1,2,"00:03:39,573","00:03:43,489",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,the document vector will not be 0 or
cs-410_1_2_46,cs-410,1,2,"00:03:43,489","00:03:49,490",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,instead they will be the counts of
cs-410_1_2_47,cs-410,1,2,"00:03:52,140","00:03:55,340",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,So this would bring in additional
cs-410_1_2_48,cs-410,1,2,"00:03:55,340","00:04:00,650",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,this can be seen as more accurate
cs-410_1_2_49,cs-410,1,2,"00:04:00,650","00:04:03,849",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,So now let's see what the formula
cs-410_1_2_50,cs-410,1,2,"00:04:03,849","00:04:05,480",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,representation.
cs-410_1_2_51,cs-410,1,2,"00:04:05,480","00:04:08,920",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"So as you'll see on this slide,"
cs-410_1_2_52,cs-410,1,2,"00:04:10,090","00:04:14,270",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,And so the formula looks
cs-410_1_2_53,cs-410,1,2,"00:04:14,270","00:04:16,310",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,"In fact, it looks identical."
cs-410_1_2_54,cs-410,1,2,"00:04:16,310","00:04:21,178",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"But inside the sum, of course,"
cs-410_1_2_55,cs-410,1,2,"00:04:21,178","00:04:25,855",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,They are now the counts of word i in
cs-410_1_2_56,cs-410,1,2,"00:04:25,855","00:04:30,208",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,the query and in the document.
cs-410_1_2_57,cs-410,1,2,"00:04:30,208","00:04:35,931",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,Now at this point I also suggest you
cs-410_1_2_58,cs-410,1,2,"00:04:35,931","00:04:41,756",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,just to think about how we can interpret
cs-410_1_2_59,cs-410,1,2,"00:04:41,756","00:04:47,710",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,It's doing something very similar
cs-410_1_2_60,cs-410,1,2,"00:04:47,710","00:04:50,501",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"But because of the change of the vector,"
cs-410_1_2_61,cs-410,1,2,"00:04:50,501","00:04:54,038",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,now the new score has
cs-410_1_2_62,cs-410,1,2,"00:04:54,038","00:04:56,118",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,Can you see the difference?
cs-410_1_2_63,cs-410,1,2,"00:04:56,118","00:05:00,995",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,And it has to do with the consideration
cs-410_1_2_64,cs-410,1,2,"00:05:00,995","00:05:03,360",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,the same term in a document.
cs-410_1_2_65,cs-410,1,2,"00:05:03,360","00:05:06,590",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,"More importantly, we would like to know"
cs-410_1_2_66,cs-410,1,2,"00:05:06,590","00:05:08,830",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,of the simplest vector space model.
cs-410_1_2_67,cs-410,1,2,"00:05:08,830","00:05:12,320",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,So let's look at this example again.
cs-410_1_2_68,cs-410,1,2,"00:05:12,320","00:05:16,670",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,So suppose we change the vector
cs-410_1_2_69,cs-410,1,2,"00:05:16,670","00:05:20,620",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,Now let's look at these
cs-410_1_2_70,cs-410,1,2,"00:05:20,620","00:05:24,580",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,The query vector is the same
cs-410_1_2_71,cs-410,1,2,"00:05:24,580","00:05:27,240",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,exactly once in the query.
cs-410_1_2_72,cs-410,1,2,"00:05:27,240","00:05:30,988",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,So the vector is still a 01 vector.
cs-410_1_2_73,cs-410,1,2,"00:05:30,988","00:05:35,472",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,"And in fact, d2 is also essentially"
cs-410_1_2_74,cs-410,1,2,"00:05:35,472","00:05:40,120",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,because none of these words
cs-410_1_2_75,cs-410,1,2,"00:05:40,120","00:05:43,145",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"As a result,"
cs-410_1_2_76,cs-410,1,2,"00:05:45,410","00:05:49,760",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"The same is true for d3,"
cs-410_1_2_77,cs-410,1,2,"00:05:51,510","00:05:57,400",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,"But d4 would be different, because"
cs-410_1_2_78,cs-410,1,2,"00:05:57,400","00:06:02,760",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,So the ending for presidential in the
cs-410_1_2_79,cs-410,1,2,"00:06:04,240","00:06:08,303",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"As a result, now the score for"
cs-410_1_2_80,cs-410,1,2,"00:06:08,303","00:06:09,050",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,It's a 4 now.
cs-410_1_2_81,cs-410,1,2,"00:06:10,130","00:06:13,380",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,"So this means by using term frequency,"
cs-410_1_2_82,cs-410,1,2,"00:06:13,380","00:06:17,720",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,we can now rank d4 above d2 and
cs-410_1_2_83,cs-410,1,2,"00:06:19,250","00:06:23,725",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,So this solved the problem with d4.
cs-410_1_2_84,cs-410,1,2,"00:06:26,190","00:06:32,548",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,But we can also see that d2 and
cs-410_1_2_85,cs-410,1,2,"00:06:32,548","00:06:38,290",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"They still have identical scores,"
cs-410_1_2_86,cs-410,1,2,"00:06:40,420","00:06:42,434",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,So how can we fix this problem?
cs-410_1_2_87,cs-410,1,2,"00:06:42,434","00:06:46,261",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,"Intuitively, we would like"
cs-410_1_2_88,cs-410,1,2,"00:06:46,261","00:06:49,736",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,matching presidential than matching about.
cs-410_1_2_89,cs-410,1,2,"00:06:49,736","00:06:53,028",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,But how can we solve
cs-410_1_2_90,cs-410,1,2,"00:06:53,028","00:06:57,651",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,Is there any way to determine
cs-410_1_2_91,cs-410,1,2,"00:06:57,651","00:07:02,478",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,more importantly and
cs-410_1_2_92,cs-410,1,2,"00:07:02,478","00:07:09,670",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,About is such a word which does not
cs-410_1_2_93,cs-410,1,2,"00:07:09,670","00:07:11,760",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,We can essentially ignore that.
cs-410_1_2_94,cs-410,1,2,"00:07:11,760","00:07:15,110",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,We sometimes call such
cs-410_1_2_95,cs-410,1,2,"00:07:15,110","00:07:18,710",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,Those are generally very frequent and
cs-410_1_2_96,cs-410,1,2,"00:07:18,710","00:07:21,570",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,Matching it doesn't really mean anything.
cs-410_1_2_97,cs-410,1,2,"00:07:21,570","00:07:23,260",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,But computationally how
cs-410_1_2_98,cs-410,1,2,"00:07:24,960","00:07:27,830",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"So again, I encourage you to"
cs-410_1_2_99,cs-410,1,2,"00:07:29,460","00:07:33,000",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,Can you came up with any statistical
cs-410_1_2_100,cs-410,1,2,"00:07:33,000","00:07:34,358",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,presidential from about?
cs-410_1_2_101,cs-410,1,2,"00:07:37,109","00:07:39,691",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,"Now if you think about it for a moment,"
cs-410_1_2_102,cs-410,1,2,"00:07:39,691","00:07:46,170",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,you'll realize that one difference is
cs-410_1_2_103,cs-410,1,2,"00:07:46,170","00:07:50,764",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,So if you count the occurrence of
cs-410_1_2_104,cs-410,1,2,"00:07:50,764","00:07:55,852",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,then we will see that about has much
cs-410_1_2_105,cs-410,1,2,"00:07:55,852","00:07:58,990",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,which tends to occur
cs-410_1_2_106,cs-410,1,2,"00:08:01,000","00:08:05,887",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,So this idea suggests
cs-410_1_2_107,cs-410,1,2,"00:08:05,887","00:08:09,396",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,the global statistics of terms or
cs-410_1_2_108,cs-410,1,2,"00:08:09,396","00:08:14,660",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,some other information
cs-410_1_2_109,cs-410,1,2,"00:08:14,660","00:08:20,568",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,the element of about in
cs-410_1_2_110,cs-410,1,2,"00:08:20,568","00:08:24,754",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,"At the same time,"
cs-410_1_2_111,cs-410,1,2,"00:08:24,754","00:08:29,278",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,the weight of presidential
cs-410_1_2_112,cs-410,1,2,"00:08:29,278","00:08:34,284",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,"If we can do that, then we can"
cs-410_1_2_113,cs-410,1,2,"00:08:34,284","00:08:39,036",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,score to be less than 3 while
cs-410_1_2_114,cs-410,1,2,"00:08:39,036","00:08:42,996",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,Then we would be able to
cs-410_1_2_115,cs-410,1,2,"00:08:45,138","00:08:47,320",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,So how can we do this systematically?
cs-410_1_2_116,cs-410,1,2,"00:08:48,730","00:08:52,030",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"Again, we can rely on"
cs-410_1_2_117,cs-410,1,2,"00:08:52,030","00:08:57,218",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,"And in this case, the particular idea"
cs-410_1_2_118,cs-410,1,2,"00:08:57,218","00:09:01,425",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,Now we have seen document
cs-410_1_2_119,cs-410,1,2,"00:09:01,425","00:09:04,030",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,the modern retrieval functions.
cs-410_1_2_120,cs-410,1,2,"00:09:05,800","00:09:08,500",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,We discussed this in a previous lecture.
cs-410_1_2_121,cs-410,1,2,"00:09:08,500","00:09:10,859",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,So here is the specific way of using it.
cs-410_1_2_122,cs-410,1,2,"00:09:10,859","00:09:15,910",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,Document frequency is the count of
cs-410_1_2_123,cs-410,1,2,"00:09:15,910","00:09:21,000",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,Here we say inverse document frequency
cs-410_1_2_124,cs-410,1,2,"00:09:21,000","00:09:22,700",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,that doesn't occur in many documents.
cs-410_1_2_125,cs-410,1,2,"00:09:24,890","00:09:30,544",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,And so the way to incorporate this
cs-410_1_2_126,cs-410,1,2,"00:09:30,544","00:09:35,477",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,is then to modify the frequency
cs-410_1_2_127,cs-410,1,2,"00:09:35,477","00:09:39,918",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,"the IDF of the corresponding word,"
cs-410_1_2_128,cs-410,1,2,"00:09:39,918","00:09:46,044",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,"If we can do that,"
cs-410_1_2_129,cs-410,1,2,"00:09:46,044","00:09:50,401",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,"which generally have a lower IDF, and"
cs-410_1_2_130,cs-410,1,2,"00:09:50,401","00:09:56,138",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,"reward rare words,"
cs-410_1_2_131,cs-410,1,2,"00:09:56,138","00:09:58,078",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,"So more specifically,"
cs-410_1_2_132,cs-410,1,2,"00:09:58,078","00:10:03,025",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,the IDF can be defined as
cs-410_1_2_133,cs-410,1,2,"00:10:03,025","00:10:08,845",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,where M is the total number of documents
cs-410_1_2_134,cs-410,1,2,"00:10:08,845","00:10:15,058",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"document frequency, the total number"
cs-410_1_2_135,cs-410,1,2,"00:10:15,058","00:10:18,596",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,Now if you plot this
cs-410_1_2_136,cs-410,1,2,"00:10:18,596","00:10:23,430",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,then you would see the curve
cs-410_1_2_137,cs-410,1,2,"00:10:23,430","00:10:28,273",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,"In general, you can see it"
cs-410_1_2_138,cs-410,1,2,"00:10:28,273","00:10:30,704",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,"a low DF word, a rare word."
cs-410_1_2_139,cs-410,1,2,"00:10:34,220","00:10:38,680",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,You can also see the maximum value
cs-410_1_2_140,cs-410,1,2,"00:10:40,952","00:10:45,158",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,It would be interesting for you to think
cs-410_1_2_141,cs-410,1,2,"00:10:45,158","00:10:46,900",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,this function.
cs-410_1_2_142,cs-410,1,2,"00:10:46,900","00:10:48,368",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,This could be an interesting exercise.
cs-410_1_2_143,cs-410,1,2,"00:10:50,918","00:10:55,238",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,Now the specific function
cs-410_1_2_144,cs-410,1,2,"00:10:55,238","00:10:59,470",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,the heuristic to simply
cs-410_1_2_145,cs-410,1,2,"00:11:01,528","00:11:05,800",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,But it turns out that this particular
cs-410_1_2_146,cs-410,1,2,"00:11:07,340","00:11:12,221",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,Now whether there's a better
cs-410_1_2_147,cs-410,1,2,"00:11:12,221","00:11:14,939",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,the open research question.
cs-410_1_2_148,cs-410,1,2,"00:11:14,939","00:11:19,665",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,But it's also clear that if
cs-410_1_2_149,cs-410,1,2,"00:11:19,665","00:11:22,945",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,"like what's shown here with this line,"
cs-410_1_2_150,cs-410,1,2,"00:11:22,945","00:11:27,200",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=682,then it may not be as
cs-410_1_2_151,cs-410,1,2,"00:11:29,110","00:11:34,270",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,"In particular, you can see"
cs-410_1_2_152,cs-410,1,2,"00:11:35,940","00:11:39,870",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=695,and we somehow have
cs-410_1_2_153,cs-410,1,2,"00:11:41,110","00:11:45,770",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,"After this point, we're going to say these"
cs-410_1_2_154,cs-410,1,2,"00:11:45,770","00:11:48,180",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,They can be essentially ignored.
cs-410_1_2_155,cs-410,1,2,"00:11:48,180","00:11:52,110",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,And this makes sense when
cs-410_1_2_156,cs-410,1,2,"00:11:52,110","00:11:57,310",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,let's say a term occurs in more
cs-410_1_2_157,cs-410,1,2,"00:11:57,310","00:12:01,700",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,then the term is unlikely very important
cs-410_1_2_158,cs-410,1,2,"00:12:03,120","00:12:05,150",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,It's not very important
cs-410_1_2_159,cs-410,1,2,"00:12:05,150","00:12:10,145",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=725,So with the standard IDF you can
cs-410_1_2_160,cs-410,1,2,"00:12:10,145","00:12:12,285",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,they all have low weights.
cs-410_1_2_161,cs-410,1,2,"00:12:12,285","00:12:14,020",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,There's no difference.
cs-410_1_2_162,cs-410,1,2,"00:12:14,020","00:12:16,413",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,But if you look at
cs-410_1_2_163,cs-410,1,2,"00:12:16,413","00:12:19,206",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,at this point that there
cs-410_1_2_164,cs-410,1,2,"00:12:19,206","00:12:26,123",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=739,So intuitively we'd want to
cs-410_1_2_165,cs-410,1,2,"00:12:26,123","00:12:31,450",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,of low DF words rather
cs-410_1_2_166,cs-410,1,2,"00:12:32,990","00:12:37,972",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,"Well, of course,"
cs-410_1_2_167,cs-410,1,2,"00:12:37,972","00:12:43,168",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,validated by using the empirically
cs-410_1_2_168,cs-410,1,2,"00:12:43,168","00:12:46,920",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,And we have to use users to
cs-410_1_2_169,cs-410,1,2,"00:12:48,580","00:12:52,948",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=768,So now let's see how
cs-410_1_2_170,cs-410,1,2,"00:12:52,948","00:12:55,000",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=772,So now let's look at
cs-410_1_2_171,cs-410,1,2,"00:12:56,100","00:13:00,530",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=776,"Now without the IDF weighting before,"
cs-410_1_2_172,cs-410,1,2,"00:13:00,530","00:13:05,810",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,But with IDF weighting we
cs-410_1_2_173,cs-410,1,2,"00:13:05,810","00:13:09,520",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,by multiplying with the IDF value.
cs-410_1_2_174,cs-410,1,2,"00:13:09,520","00:13:14,150",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=789,"For example,"
cs-410_1_2_175,cs-410,1,2,"00:13:14,150","00:13:19,680",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,in particular for about there's adjustment
cs-410_1_2_176,cs-410,1,2,"00:13:19,680","00:13:23,980",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,which is smaller than the IDF
cs-410_1_2_177,cs-410,1,2,"00:13:23,980","00:13:28,930",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,"So if you look at these,"
cs-410_1_2_178,cs-410,1,2,"00:13:28,930","00:13:34,390",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=808,"As a result, adjustment here would be"
cs-410_1_2_179,cs-410,1,2,"00:13:37,190","00:13:44,035",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,"So if we score with these new vectors,"
cs-410_1_2_180,cs-410,1,2,"00:13:44,035","00:13:48,752",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,"of course,"
cs-410_1_2_181,cs-410,1,2,"00:13:48,752","00:13:54,830",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,"campaign, but the matching of"
cs-410_1_2_182,cs-410,1,2,"00:13:54,830","00:14:01,250",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=834,"So now as a result of IDF weighting,"
cs-410_1_2_183,cs-410,1,2,"00:14:01,250","00:14:06,460",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=841,"because it matched a rare word,"
cs-410_1_2_184,cs-410,1,2,"00:14:06,460","00:14:10,156",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=846,So this shows that the IDF
cs-410_1_2_185,cs-410,1,2,"00:14:12,798","00:14:19,434",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=852,So how effective is this model in
cs-410_1_2_186,cs-410,1,2,"00:14:19,434","00:14:23,438",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,"Well, let's look at all these"
cs-410_1_2_187,cs-410,1,2,"00:14:23,438","00:14:28,100",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=863,These are the new scores
cs-410_1_2_188,cs-410,1,2,"00:14:28,100","00:14:32,580",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=868,But how effective is this new weighting
cs-410_1_2_189,cs-410,1,2,"00:14:33,770","00:14:38,520",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,So now let's see overall how effective
cs-410_1_2_190,cs-410,1,2,"00:14:38,520","00:14:39,490",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=878,with TF-IDF weighting.
cs-410_1_2_191,cs-410,1,2,"00:14:40,630","00:14:44,330",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=880,Here we show all the five documents
cs-410_1_2_192,cs-410,1,2,"00:14:44,330","00:14:45,720",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=884,these are their scores.
cs-410_1_2_193,cs-410,1,2,"00:14:47,000","00:14:49,760",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=887,Now we can see the scores for
cs-410_1_2_194,cs-410,1,2,"00:14:49,760","00:14:56,410",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=889,the first four documents here
cs-410_1_2_195,cs-410,1,2,"00:14:56,410","00:14:57,650",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=896,They are as we expected.
cs-410_1_2_196,cs-410,1,2,"00:14:58,740","00:15:05,710",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=898,"However, we also see a new"
cs-410_1_2_197,cs-410,1,2,"00:15:05,710","00:15:10,490",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=905,which did not have a very high score
cs-410_1_2_198,cs-410,1,2,"00:15:10,490","00:15:13,270",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=910,now actually has a very high score.
cs-410_1_2_199,cs-410,1,2,"00:15:13,270","00:15:15,110",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,"In fact, it has the highest score here."
cs-410_1_2_200,cs-410,1,2,"00:15:16,850","00:15:19,002",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=916,So this creates a new problem.
cs-410_1_2_201,cs-410,1,2,"00:15:19,002","00:15:23,080",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=919,This is actually a common phenomenon
cs-410_1_2_202,cs-410,1,2,"00:15:23,080","00:15:25,570",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,"Basically, when you try"
cs-410_1_2_203,cs-410,1,2,"00:15:25,570","00:15:27,960",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=925,you tend to introduce other problems.
cs-410_1_2_204,cs-410,1,2,"00:15:27,960","00:15:32,674",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=927,And that's why it's very tricky how
cs-410_1_2_205,cs-410,1,2,"00:15:32,674","00:15:39,658",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=932,And what's the best ranking function
cs-410_1_2_206,cs-410,1,2,"00:15:39,658","00:15:41,170",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,Researchers are still working on that.
cs-410_1_2_207,cs-410,1,2,"00:15:42,360","00:15:47,530",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=942,But in the next few lectures we're going
cs-410_1_2_208,cs-410,1,2,"00:15:47,530","00:15:53,030",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=947,ideas to further improve this model and
cs-410_1_2_209,cs-410,1,2,"00:15:55,920","00:16:00,740",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=955,"So to summarize this lecture, we've talked"
cs-410_1_2_210,cs-410,1,2,"00:16:00,740","00:16:04,340",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=960,"model, and"
cs-410_1_2_211,cs-410,1,2,"00:16:04,340","00:16:08,470",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=964,the vector space model
cs-410_1_2_212,cs-410,1,2,"00:16:08,470","00:16:13,573",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=968,So the improvement is mostly on
cs-410_1_2_213,cs-410,1,2,"00:16:13,573","00:16:18,673",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=973,give high weight to a term that
cs-410_1_2_214,cs-410,1,2,"00:16:18,673","00:16:21,790",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=978,infrequently in the whole collection.
cs-410_1_2_215,cs-410,1,2,"00:16:23,630","00:16:26,210",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=983,And we have seen that this
cs-410_1_2_216,cs-410,1,2,"00:16:26,210","00:16:29,440",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,looks better than the simplest
cs-410_1_2_217,cs-410,1,2,"00:16:29,440","00:16:33,268",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=989,But it also still has some problems.
cs-410_1_2_218,cs-410,1,2,"00:16:33,268","00:16:40,448",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=993,In the next lecture we're going to look at
cs-410_1_2_219,cs-410,1,2,"00:16:40,448","00:16:50,448",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,[MUSIC]
cs-410_1_3_1,cs-410,1,3,"00:00:00,000","00:00:03,655",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_1_3_2,cs-410,1,3,"00:00:07,739","00:00:14,807",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about Evaluation of
cs-410_1_3_3,cs-410,1,3,"00:00:14,807","00:00:19,490",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"lectures, we have talked about"
cs-410_1_3_4,cs-410,1,3,"00:00:19,490","00:00:22,120",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,different kinds of ranking functions.
cs-410_1_3_5,cs-410,1,3,"00:00:23,550","00:00:27,040",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,But how do we know which
cs-410_1_3_6,cs-410,1,3,"00:00:27,040","00:00:28,390",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,"In order to answer this question,"
cs-410_1_3_7,cs-410,1,3,"00:00:28,390","00:00:33,000",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,we have to compare them and that means we
cs-410_1_3_8,cs-410,1,3,"00:00:34,790","00:00:37,000",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,So this is the main topic of this lecture.
cs-410_1_3_9,cs-410,1,3,"00:00:40,462","00:00:42,730",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,"First, lets think about why"
cs-410_1_3_10,cs-410,1,3,"00:00:42,730","00:00:44,290",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,I already give one reason.
cs-410_1_3_11,cs-410,1,3,"00:00:44,290","00:00:48,770",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,"That is, we have to use evaluation"
cs-410_1_3_12,cs-410,1,3,"00:00:48,770","00:00:50,330",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,works better.
cs-410_1_3_13,cs-410,1,3,"00:00:50,330","00:00:54,310",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,Now this is very important for
cs-410_1_3_14,cs-410,1,3,"00:00:54,310","00:01:00,173",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,"Otherwise, we wouldn't know whether a new"
cs-410_1_3_15,cs-410,1,3,"00:01:00,173","00:01:04,710",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"In the beginning of this course, we talked"
cs-410_1_3_16,cs-410,1,3,"00:01:04,710","00:01:07,200",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,We compare it with data base retrieval.
cs-410_1_3_17,cs-410,1,3,"00:01:08,440","00:01:14,390",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,There we mentioned that text retrieval
cs-410_1_3_18,cs-410,1,3,"00:01:14,390","00:01:18,240",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,So evaluation must rely on users.
cs-410_1_3_19,cs-410,1,3,"00:01:18,240","00:01:22,210",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"Which system works better,"
cs-410_1_3_20,cs-410,1,3,"00:01:25,020","00:01:28,850",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,"So, this becomes a very"
cs-410_1_3_21,cs-410,1,3,"00:01:28,850","00:01:32,510",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,because how can we get users
cs-410_1_3_22,cs-410,1,3,"00:01:32,510","00:01:35,281",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,How can we do a fair comparison
cs-410_1_3_23,cs-410,1,3,"00:01:37,208","00:01:39,970",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,So just go back to the reasons for
cs-410_1_3_24,cs-410,1,3,"00:01:41,210","00:01:42,660",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,I listed two reasons here.
cs-410_1_3_25,cs-410,1,3,"00:01:42,660","00:01:47,060",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"The second reason, is basically what I"
cs-410_1_3_26,cs-410,1,3,"00:01:47,060","00:01:51,910",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,reason which is to assess the actual
cs-410_1_3_27,cs-410,1,3,"00:01:51,910","00:01:55,200",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,Imagine you're building your
cs-410_1_3_28,cs-410,1,3,"00:01:55,200","00:02:01,660",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,it would be interesting knowing how well
cs-410_1_3_29,cs-410,1,3,"00:02:01,660","00:02:02,350",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"So in this case,"
cs-410_1_3_30,cs-410,1,3,"00:02:02,350","00:02:07,850",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,matches must reflect the utility to
cs-410_1_3_31,cs-410,1,3,"00:02:07,850","00:02:11,840",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"And typically, this has to be"
cs-410_1_3_32,cs-410,1,3,"00:02:11,840","00:02:13,960",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,using the real search engine.
cs-410_1_3_33,cs-410,1,3,"00:02:16,340","00:02:18,630",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,"In the second case, or the second reason,"
cs-410_1_3_34,cs-410,1,3,"00:02:19,970","00:02:26,130",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,the measures actually all need to collated
cs-410_1_3_35,cs-410,1,3,"00:02:26,130","00:02:30,120",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,"Thus, they don't have to accurately"
cs-410_1_3_36,cs-410,1,3,"00:02:31,980","00:02:37,680",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,So the measure only needs to be good
cs-410_1_3_37,cs-410,1,3,"00:02:38,860","00:02:41,780",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,And this is usually done
cs-410_1_3_38,cs-410,1,3,"00:02:41,780","00:02:48,110",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,And this is the main idea that we'll
cs-410_1_3_39,cs-410,1,3,"00:02:48,110","00:02:53,520",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,This has been very important for
cs-410_1_3_40,cs-410,1,3,"00:02:53,520","00:02:56,910",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,for improving search
cs-410_1_3_41,cs-410,1,3,"00:02:58,910","00:03:01,880",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,So let's talk about what to measure.
cs-410_1_3_42,cs-410,1,3,"00:03:01,880","00:03:06,750",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,There are many aspects of searching
cs-410_1_3_43,cs-410,1,3,"00:03:06,750","00:03:09,000",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"And here,"
cs-410_1_3_44,cs-410,1,3,"00:03:09,000","00:03:11,190",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,"One, is effectiveness or accuracy."
cs-410_1_3_45,cs-410,1,3,"00:03:11,190","00:03:13,710",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,How accurate are the search results?
cs-410_1_3_46,cs-410,1,3,"00:03:13,710","00:03:18,110",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,"In this case, we're measuring a system's"
cs-410_1_3_47,cs-410,1,3,"00:03:18,110","00:03:20,150",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,on top of non relevant ones.
cs-410_1_3_48,cs-410,1,3,"00:03:20,150","00:03:21,850",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,"The second, is efficiency."
cs-410_1_3_49,cs-410,1,3,"00:03:21,850","00:03:24,470",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,How quickly can you get the results?
cs-410_1_3_50,cs-410,1,3,"00:03:24,470","00:03:27,710",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,How much computing resources
cs-410_1_3_51,cs-410,1,3,"00:03:27,710","00:03:31,150",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,"In this case, we need to measure the space"
cs-410_1_3_52,cs-410,1,3,"00:03:32,540","00:03:34,890",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,The third aspect is usability.
cs-410_1_3_53,cs-410,1,3,"00:03:34,890","00:03:38,950",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"Basically the question is,"
cs-410_1_3_54,cs-410,1,3,"00:03:38,950","00:03:40,840",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"Here, obviously, interfaces and"
cs-410_1_3_55,cs-410,1,3,"00:03:40,840","00:03:45,870",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,many other things also important and
cs-410_1_3_56,cs-410,1,3,"00:03:47,410","00:03:51,670",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"Now in this course, we're going to"
cs-410_1_3_57,cs-410,1,3,"00:03:51,670","00:03:52,710",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,accuracy measures.
cs-410_1_3_58,cs-410,1,3,"00:03:52,710","00:03:55,340",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,Because the efficiency and
cs-410_1_3_59,cs-410,1,3,"00:03:55,340","00:04:00,230",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,usability dimensions are not
cs-410_1_3_60,cs-410,1,3,"00:04:00,230","00:04:08,640",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,And so they are needed for
cs-410_1_3_61,cs-410,1,3,"00:04:08,640","00:04:13,347",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,And there is also good coverage
cs-410_1_3_62,cs-410,1,3,"00:04:13,347","00:04:18,780",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,But how to evaluate search
cs-410_1_3_63,cs-410,1,3,"00:04:18,780","00:04:23,110",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,something unique to text retrieval and
cs-410_1_3_64,cs-410,1,3,"00:04:23,110","00:04:28,428",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,The main idea that people have proposed
cs-410_1_3_65,cs-410,1,3,"00:04:28,428","00:04:33,850",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,the text retrieval algorithm is called
cs-410_1_3_66,cs-410,1,3,"00:04:33,850","00:04:40,145",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,This one actually was developed
cs-410_1_3_67,cs-410,1,3,"00:04:40,145","00:04:44,785",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,It's a methodology for
cs-410_1_3_68,cs-410,1,3,"00:04:45,985","00:04:49,305",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,Its sampling methodology that has
cs-410_1_3_69,cs-410,1,3,"00:04:49,305","00:04:50,880",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,search engine evaluation.
cs-410_1_3_70,cs-410,1,3,"00:04:50,880","00:04:55,930",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,But also for evaluating virtually
cs-410_1_3_71,cs-410,1,3,"00:04:55,930","00:05:01,180",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,for example in natural language processing
cs-410_1_3_72,cs-410,1,3,"00:05:01,180","00:05:05,620",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,"is empirical to find, we typically"
cs-410_1_3_73,cs-410,1,3,"00:05:05,620","00:05:09,450",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,And today with the big data challenging
cs-410_1_3_74,cs-410,1,3,"00:05:09,450","00:05:13,590",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,with the use of machine
cs-410_1_3_75,cs-410,1,3,"00:05:13,590","00:05:17,430",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,"This methodology has been very popular,"
cs-410_1_3_76,cs-410,1,3,"00:05:17,430","00:05:20,100",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,a search engine application in the 1960s.
cs-410_1_3_77,cs-410,1,3,"00:05:20,100","00:05:25,250",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,So the basic idea of this approach is
cs-410_1_3_78,cs-410,1,3,"00:05:25,250","00:05:26,200",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,define measures.
cs-410_1_3_79,cs-410,1,3,"00:05:27,220","00:05:30,350",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,"Once such a test collection is built,"
cs-410_1_3_80,cs-410,1,3,"00:05:30,350","00:05:32,990",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,again to test different algorithms.
cs-410_1_3_81,cs-410,1,3,"00:05:32,990","00:05:36,180",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,And we're going to define measures
cs-410_1_3_82,cs-410,1,3,"00:05:36,180","00:05:39,660",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,performance of a system and algorithm.
cs-410_1_3_83,cs-410,1,3,"00:05:41,070","00:05:42,960",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,So how exactly will this work?
cs-410_1_3_84,cs-410,1,3,"00:05:42,960","00:05:47,090",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,Well we can do have a sample collection of
cs-410_1_3_85,cs-410,1,3,"00:05:47,090","00:05:49,986",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,the real document collection
cs-410_1_3_86,cs-410,1,3,"00:05:49,986","00:05:53,210",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,We're going to also have a sample
cs-410_1_3_87,cs-410,1,3,"00:05:53,210","00:05:55,240",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,This is a little simulator
cs-410_1_3_88,cs-410,1,3,"00:05:56,270","00:05:58,980",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"Then, we'll have to have"
cs-410_1_3_89,cs-410,1,3,"00:05:58,980","00:06:03,930",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,These are judgments of which documents
cs-410_1_3_90,cs-410,1,3,"00:06:03,930","00:06:08,250",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,"Ideally, they have to be made by"
cs-410_1_3_91,cs-410,1,3,"00:06:08,250","00:06:12,930",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,Because those are the people that know
cs-410_1_3_92,cs-410,1,3,"00:06:12,930","00:06:14,690",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,"And finally, we have to have matches for"
cs-410_1_3_93,cs-410,1,3,"00:06:14,690","00:06:19,830",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,quantify how well our system's result
cs-410_1_3_94,cs-410,1,3,"00:06:19,830","00:06:24,560",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,That would be constructed base
cs-410_1_3_95,cs-410,1,3,"00:06:24,560","00:06:30,917",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,So this methodology is very useful for
cs-410_1_3_96,cs-410,1,3,"00:06:30,917","00:06:36,130",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,because the test can be reused many times.
cs-410_1_3_97,cs-410,1,3,"00:06:36,130","00:06:41,340",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,And it will also provide a fair
cs-410_1_3_98,cs-410,1,3,"00:06:41,340","00:06:43,370",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,We have the same criteria or
cs-410_1_3_99,cs-410,1,3,"00:06:43,370","00:06:47,570",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,same dataset to be used to
cs-410_1_3_100,cs-410,1,3,"00:06:47,570","00:06:50,810",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,This allows us to compare
cs-410_1_3_101,cs-410,1,3,"00:06:50,810","00:06:55,660",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,an old algorithm that was divided many
cs-410_1_3_102,cs-410,1,3,"00:06:55,660","00:06:59,580",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,"So this is the illustration of this works,"
cs-410_1_3_103,cs-410,1,3,"00:06:59,580","00:07:03,930",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,we need our queries that are showing here.
cs-410_1_3_104,cs-410,1,3,"00:07:03,930","00:07:05,180",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,"We have Q1, Q2 etc."
cs-410_1_3_105,cs-410,1,3,"00:07:05,180","00:07:08,300",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,We also need the documents and
cs-410_1_3_106,cs-410,1,3,"00:07:08,300","00:07:10,580",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,on the right side you will see
cs-410_1_3_107,cs-410,1,3,"00:07:10,580","00:07:19,150",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,These are basically the binary judgments
cs-410_1_3_108,cs-410,1,3,"00:07:19,150","00:07:23,790",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,"So for example,"
cs-410_1_3_109,cs-410,1,3,"00:07:23,790","00:07:27,920",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,"D2 is judged as being relevant as well,"
cs-410_1_3_110,cs-410,1,3,"00:07:27,920","00:07:28,980",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,And the Q1 etc.
cs-410_1_3_111,cs-410,1,3,"00:07:28,980","00:07:32,490",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,These will be created by users.
cs-410_1_3_112,cs-410,1,3,"00:07:34,190","00:07:38,460",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,"Once we have these, and"
cs-410_1_3_113,cs-410,1,3,"00:07:38,460","00:07:43,560",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"And then if you have two systems,"
cs-410_1_3_114,cs-410,1,3,"00:07:43,560","00:07:47,260",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,then you can just run each
cs-410_1_3_115,cs-410,1,3,"00:07:47,260","00:07:50,580",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,the documents and
cs-410_1_3_116,cs-410,1,3,"00:07:50,580","00:07:56,347",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,Let's say if the queries Q1 and
cs-410_1_3_117,cs-410,1,3,"00:07:56,347","00:08:02,350",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,Here I show R sub A as
cs-410_1_3_118,cs-410,1,3,"00:08:02,350","00:08:05,170",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,"So this is, remember we talked about"
cs-410_1_3_119,cs-410,1,3,"00:08:05,170","00:08:09,750",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,task of computing approximation
cs-410_1_3_120,cs-410,1,3,"00:08:09,750","00:08:13,910",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,R sub A is system A's approximation here.
cs-410_1_3_121,cs-410,1,3,"00:08:14,980","00:08:20,000",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,And R sub B is system B's
cs-410_1_3_122,cs-410,1,3,"00:08:21,100","00:08:22,810",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,"Now, let's take a look at these results."
cs-410_1_3_123,cs-410,1,3,"00:08:22,810","00:08:24,190",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,So which is better?
cs-410_1_3_124,cs-410,1,3,"00:08:24,190","00:08:26,810",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,"Now imagine if a user,"
cs-410_1_3_125,cs-410,1,3,"00:08:26,810","00:08:31,145",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,Now let's take a look at the both results.
cs-410_1_3_126,cs-410,1,3,"00:08:31,145","00:08:33,270",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,And there are some differences and
cs-410_1_3_127,cs-410,1,3,"00:08:33,270","00:08:40,160",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,there are some documents that
cs-410_1_3_128,cs-410,1,3,"00:08:40,160","00:08:44,200",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,"But if you look at the results,"
cs-410_1_3_129,cs-410,1,3,"00:08:44,200","00:08:48,640",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,A is better in the sense that we don't
cs-410_1_3_130,cs-410,1,3,"00:08:48,640","00:08:52,260",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"And among the three documents returned,"
cs-410_1_3_131,cs-410,1,3,"00:08:52,260","00:08:55,430",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,"So that's good, it's precise."
cs-410_1_3_132,cs-410,1,3,"00:08:55,430","00:08:58,770",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,On the other hand one council
cs-410_1_3_133,cs-410,1,3,"00:08:58,770","00:09:01,280",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,because we've got all of
cs-410_1_3_134,cs-410,1,3,"00:09:01,280","00:09:03,500",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,We've got three instead of two.
cs-410_1_3_135,cs-410,1,3,"00:09:03,500","00:09:06,690",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,So which one is better and
cs-410_1_3_136,cs-410,1,3,"00:09:08,820","00:09:12,670",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"Well, obviously this question"
cs-410_1_3_137,cs-410,1,3,"00:09:12,670","00:09:14,820",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,It depends on users as well.
cs-410_1_3_138,cs-410,1,3,"00:09:14,820","00:09:19,950",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,You might even imagine for
cs-410_1_3_139,cs-410,1,3,"00:09:19,950","00:09:23,747",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,If the user is not interested in
cs-410_1_3_140,cs-410,1,3,"00:09:23,747","00:09:28,582",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,"Right, in this case the user doesn't"
cs-410_1_3_141,cs-410,1,3,"00:09:28,582","00:09:31,020",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,see most of the relevant documents.
cs-410_1_3_142,cs-410,1,3,"00:09:31,020","00:09:34,230",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"On the other hand,"
cs-410_1_3_143,cs-410,1,3,"00:09:34,230","00:09:37,130",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,to have as many random
cs-410_1_3_144,cs-410,1,3,"00:09:37,130","00:09:41,617",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,"For example, if you're doing a literature"
cs-410_1_3_145,cs-410,1,3,"00:09:41,617","00:09:43,844",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,and you might find that
cs-410_1_3_146,cs-410,1,3,"00:09:43,844","00:09:48,985",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"So in the case, we will have to also"
cs-410_1_3_147,cs-410,1,3,"00:09:48,985","00:09:53,408",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,And we might need it to define multiple
cs-410_1_3_148,cs-410,1,3,"00:09:53,408","00:09:55,798",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,perspectives of looking at the results.
cs-410_1_3_149,cs-410,1,3,"00:09:58,259","00:10:08,259",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,[MUSIC]
cs-410_1_4_1,cs-410,1,4,"00:00:00,086","00:00:07,516",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_4_2,cs-410,1,4,"00:00:07,516","00:00:10,282",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about
cs-410_1_4_3,cs-410,1,4,"00:00:10,282","00:00:11,805",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,"In this lecture,"
cs-410_1_4_4,cs-410,1,4,"00:00:11,805","00:00:17,806",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,we're going to continue the discussion
cs-410_1_4_5,cs-410,1,4,"00:00:17,806","00:00:22,942",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,We're going to look at another kind of
cs-410_1_4_6,cs-410,1,4,"00:00:22,942","00:00:27,584",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,functions than the Vector Space Model
cs-410_1_4_7,cs-410,1,4,"00:00:32,146","00:00:36,589",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"In probabilistic models,"
cs-410_1_4_8,cs-410,1,4,"00:00:36,589","00:00:41,822",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,based on the probability that this
cs-410_1_4_9,cs-410,1,4,"00:00:41,822","00:00:46,802",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,"In other words, we introduce"
cs-410_1_4_10,cs-410,1,4,"00:00:46,802","00:00:51,400",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,This is the variable R here.
cs-410_1_4_11,cs-410,1,4,"00:00:51,400","00:00:54,520",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,And we also assume that the query and
cs-410_1_4_12,cs-410,1,4,"00:00:54,520","00:00:59,830",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,the documents are all observations
cs-410_1_4_13,cs-410,1,4,"00:01:00,920","00:01:05,810",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"Note that in the vector-based models,"
cs-410_1_4_14,cs-410,1,4,"00:01:05,810","00:01:11,120",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,here we assume they are the data
cs-410_1_4_15,cs-410,1,4,"00:01:11,120","00:01:17,940",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,"And so, the problem of retrieval becomes"
cs-410_1_4_16,cs-410,1,4,"00:01:19,490","00:01:23,060",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,"In this category of models,"
cs-410_1_4_17,cs-410,1,4,"00:01:23,060","00:01:27,130",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,The classic probabilistic model has
cs-410_1_4_18,cs-410,1,4,"00:01:27,130","00:01:30,150",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,which we discussed in in
cs-410_1_4_19,cs-410,1,4,"00:01:30,150","00:01:33,570",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,because its a form is actually
cs-410_1_4_20,cs-410,1,4,"00:01:35,260","00:01:40,180",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,"In this lecture,"
cs-410_1_4_21,cs-410,1,4,"00:01:41,230","00:01:45,550",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,P class called a language
cs-410_1_4_22,cs-410,1,4,"00:01:45,550","00:01:50,150",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,"In particular, we're going to discuss"
cs-410_1_4_23,cs-410,1,4,"00:01:51,370","00:01:55,330",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,which is one of the most effective
cs-410_1_4_24,cs-410,1,4,"00:01:57,050","00:02:01,840",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,There was also another line called
cs-410_1_4_25,cs-410,1,4,"00:02:01,840","00:02:04,970",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"which has led to the PL2 function,"
cs-410_1_4_26,cs-410,1,4,"00:02:06,440","00:02:11,070",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,it's also one of the most effective
cs-410_1_4_27,cs-410,1,4,"00:02:11,070","00:02:16,847",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,"In query likelihood, our assumption"
cs-410_1_4_28,cs-410,1,4,"00:02:16,847","00:02:23,002",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,can be approximated by the probability
cs-410_1_4_29,cs-410,1,4,"00:02:23,002","00:02:29,656",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,"So intuitively, this probability just"
cs-410_1_4_30,cs-410,1,4,"00:02:29,656","00:02:34,808",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,"And that is if a user likes document d,"
cs-410_1_4_31,cs-410,1,4,"00:02:34,808","00:02:40,220",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"the user enter query q ,in"
cs-410_1_4_32,cs-410,1,4,"00:02:40,220","00:02:47,680",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,"So we assume that the user likes d,"
cs-410_1_4_33,cs-410,1,4,"00:02:47,680","00:02:52,610",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,And then we ask the question about how
cs-410_1_4_34,cs-410,1,4,"00:02:52,610","00:02:53,250",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,from this user?
cs-410_1_4_35,cs-410,1,4,"00:02:54,890","00:02:56,508",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,So this is the basic idea.
cs-410_1_4_36,cs-410,1,4,"00:02:56,508","00:03:00,676",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,"Now, to understand this idea,"
cs-410_1_4_37,cs-410,1,4,"00:03:00,676","00:03:03,741",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,the basic idea of
cs-410_1_4_38,cs-410,1,4,"00:03:03,741","00:03:09,150",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,"So here, I listed some imagined"
cs-410_1_4_39,cs-410,1,4,"00:03:09,150","00:03:13,599",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,relevance judgments of queries and
cs-410_1_4_40,cs-410,1,4,"00:03:13,599","00:03:17,576",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,"For example, in this line,"
cs-410_1_4_41,cs-410,1,4,"00:03:17,576","00:03:24,546",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,it shows that q1 is a query
cs-410_1_4_42,cs-410,1,4,"00:03:24,546","00:03:28,043",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,And d1 is a document
cs-410_1_4_43,cs-410,1,4,"00:03:28,043","00:03:33,036",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,And 1 means the user thinks
cs-410_1_4_44,cs-410,1,4,"00:03:33,036","00:03:38,685",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,So this R here can be also approximated
cs-410_1_4_45,cs-410,1,4,"00:03:38,685","00:03:44,810",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,engine can collect by watching how you
cs-410_1_4_46,cs-410,1,4,"00:03:44,810","00:03:47,990",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,"So in this case, let's say"
cs-410_1_4_47,cs-410,1,4,"00:03:47,990","00:03:49,000",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,So there's a 1 here.
cs-410_1_4_48,cs-410,1,4,"00:03:50,080","00:03:56,480",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,"Similarly, the user clicked on d2 also,"
cs-410_1_4_49,cs-410,1,4,"00:03:56,480","00:03:59,630",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"In other words,"
cs-410_1_4_50,cs-410,1,4,"00:04:00,700","00:04:05,080",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,"On the other hand,"
cs-410_1_4_51,cs-410,1,4,"00:04:07,430","00:04:13,485",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,And d4 is non-relevant and then d5 is
cs-410_1_4_52,cs-410,1,4,"00:04:13,485","00:04:17,860",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,"And this part, maybe,"
cs-410_1_4_53,cs-410,1,4,"00:04:17,860","00:04:23,009",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,So this user typed in q1 and then found
cs-410_1_4_54,cs-410,1,4,"00:04:23,009","00:04:26,170",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,so d1 is actually non-relevant.
cs-410_1_4_55,cs-410,1,4,"00:04:26,170","00:04:31,124",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"In contrast, here we see it's relevant."
cs-410_1_4_56,cs-410,1,4,"00:04:31,124","00:04:38,401",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,Or this could be the same query typed
cs-410_1_4_57,cs-410,1,4,"00:04:38,401","00:04:42,660",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,"But d2 is also relevant, etc."
cs-410_1_4_58,cs-410,1,4,"00:04:42,660","00:04:47,050",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,"And then here,"
cs-410_1_4_59,cs-410,1,4,"00:04:48,390","00:04:50,870",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,"Now, we can imagine we"
cs-410_1_4_60,cs-410,1,4,"00:04:52,940","00:04:54,740",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,"Now we can ask the question,"
cs-410_1_4_61,cs-410,1,4,"00:04:54,740","00:04:58,460",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,how can we then estimate
cs-410_1_4_62,cs-410,1,4,"00:05:00,390","00:05:03,690",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,So how can we compute this
cs-410_1_4_63,cs-410,1,4,"00:05:03,690","00:05:06,230",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,"Well, intuitively that just means"
cs-410_1_4_64,cs-410,1,4,"00:05:06,230","00:05:10,770",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,if we look at all the entries
cs-410_1_4_65,cs-410,1,4,"00:05:10,770","00:05:16,010",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"this particular q, how likely we'll"
cs-410_1_4_66,cs-410,1,4,"00:05:16,010","00:05:18,500",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,So basically that just means that
cs-410_1_4_67,cs-410,1,4,"00:05:19,730","00:05:24,536",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,We can first count how many
cs-410_1_4_68,cs-410,1,4,"00:05:24,536","00:05:29,576",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,d as a pair in this table and
cs-410_1_4_69,cs-410,1,4,"00:05:29,576","00:05:34,518",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,we actually have also seen
cs-410_1_4_70,cs-410,1,4,"00:05:34,518","00:05:37,227",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,"And then, we just compute the ratio."
cs-410_1_4_71,cs-410,1,4,"00:05:39,409","00:05:42,347",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,So let's take a look at
cs-410_1_4_72,cs-410,1,4,"00:05:42,347","00:05:48,466",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,Suppose we are trying to compute this
cs-410_1_4_73,cs-410,1,4,"00:05:48,466","00:05:52,240",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,What is the estimated probability?
cs-410_1_4_74,cs-410,1,4,"00:05:52,240","00:05:54,760",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,"Now, think about that."
cs-410_1_4_75,cs-410,1,4,"00:05:54,760","00:05:58,823",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,You can pause the video if needed.
cs-410_1_4_76,cs-410,1,4,"00:05:58,823","00:06:01,560",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,Try to take a look at the table.
cs-410_1_4_77,cs-410,1,4,"00:06:01,560","00:06:04,606",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,And try to give your
cs-410_1_4_78,cs-410,1,4,"00:06:07,069","00:06:11,802",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,"Have you seen that,"
cs-410_1_4_79,cs-410,1,4,"00:06:11,802","00:06:15,050",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,we'll be looking at these two pairs?
cs-410_1_4_80,cs-410,1,4,"00:06:15,050","00:06:18,020",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,"And in both cases, well,"
cs-410_1_4_81,cs-410,1,4,"00:06:18,020","00:06:23,190",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,"actually, in one of the cases, the user"
cs-410_1_4_82,cs-410,1,4,"00:06:23,190","00:06:26,282",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,So R = 1 in only one of the two cases.
cs-410_1_4_83,cs-410,1,4,"00:06:26,282","00:06:28,244",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"In the other case, it's 0."
cs-410_1_4_84,cs-410,1,4,"00:06:28,244","00:06:30,846",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,So that's one out of two.
cs-410_1_4_85,cs-410,1,4,"00:06:30,846","00:06:34,525",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,What about the d1 and the d2?
cs-410_1_4_86,cs-410,1,4,"00:06:34,525","00:06:39,127",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,"Well, they are here, d1 and d2, d1 and d2,"
cs-410_1_4_87,cs-410,1,4,"00:06:39,127","00:06:42,729",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"in both cases, in this case, R = 1."
cs-410_1_4_88,cs-410,1,4,"00:06:42,729","00:06:45,700",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So it's a two out of two and
cs-410_1_4_89,cs-410,1,4,"00:06:45,700","00:06:48,195",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,"So you can see with this approach,"
cs-410_1_4_90,cs-410,1,4,"00:06:48,195","00:06:52,679",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,we can actually score these documents for
cs-410_1_4_91,cs-410,1,4,"00:06:52,679","00:06:56,625",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"We now have a score for d1,"
cs-410_1_4_92,cs-410,1,4,"00:06:56,625","00:07:00,334",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,And we can simply rank them
cs-410_1_4_93,cs-410,1,4,"00:07:00,334","00:07:04,056",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,so that's the basic idea
cs-410_1_4_94,cs-410,1,4,"00:07:04,056","00:07:06,971",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,"And you can see it makes a lot of sense,"
cs-410_1_4_95,cs-410,1,4,"00:07:06,971","00:07:10,036",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,it's going to rank d2 above
cs-410_1_4_96,cs-410,1,4,"00:07:10,036","00:07:15,992",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,"Because in all the cases,"
cs-410_1_4_97,cs-410,1,4,"00:07:15,992","00:07:18,314",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,The user clicked on this document.
cs-410_1_4_98,cs-410,1,4,"00:07:18,314","00:07:23,957",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,So this also should show that
cs-410_1_4_99,cs-410,1,4,"00:07:23,957","00:07:30,830",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,a search engine can learn a lot from
cs-410_1_4_100,cs-410,1,4,"00:07:30,830","00:07:33,580",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,This is a simple example
cs-410_1_4_101,cs-410,1,4,"00:07:33,580","00:07:38,760",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,with small amount of entries here we can
cs-410_1_4_102,cs-410,1,4,"00:07:38,760","00:07:42,160",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,These probabilities would give us
cs-410_1_4_103,cs-410,1,4,"00:07:42,160","00:07:46,160",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,might be more relevant or more useful
cs-410_1_4_104,cs-410,1,4,"00:07:47,170","00:07:51,100",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,"Now, of course, the problems that we"
cs-410_1_4_105,cs-410,1,4,"00:07:51,100","00:07:54,048",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,all the documents and
cs-410_1_4_106,cs-410,1,4,"00:07:55,320","00:07:57,890",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,"There would be a lot of unseen documents,"
cs-410_1_4_107,cs-410,1,4,"00:07:57,890","00:08:02,880",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,we have only collected the data from the
cs-410_1_4_108,cs-410,1,4,"00:08:02,880","00:08:07,370",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,And there are even more unseen queries
cs-410_1_4_109,cs-410,1,4,"00:08:07,370","00:08:10,060",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,queries will be typed in by users.
cs-410_1_4_110,cs-410,1,4,"00:08:10,060","00:08:15,090",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"So obviously,"
cs-410_1_4_111,cs-410,1,4,"00:08:15,090","00:08:17,190",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,it to unseen queries or unseen documents.
cs-410_1_4_112,cs-410,1,4,"00:08:18,635","00:08:22,278",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,"Nevertheless, this shows the basic idea"
cs-410_1_4_113,cs-410,1,4,"00:08:22,278","00:08:23,646",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,it makes sense intuitively.
cs-410_1_4_114,cs-410,1,4,"00:08:23,646","00:08:28,275",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,So what do we do in such a case when
cs-410_1_4_115,cs-410,1,4,"00:08:28,275","00:08:29,508",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,unseen queries?
cs-410_1_4_116,cs-410,1,4,"00:08:29,508","00:08:32,818",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,"Well, the solutions that we have"
cs-410_1_4_117,cs-410,1,4,"00:08:32,818","00:08:37,003",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,So in this particular case called
cs-410_1_4_118,cs-410,1,4,"00:08:37,003","00:08:40,784",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,we just approximate this by
cs-410_1_4_119,cs-410,1,4,"00:08:40,784","00:08:46,682",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,"p(q given d, R=1)."
cs-410_1_4_120,cs-410,1,4,"00:08:46,682","00:08:51,539",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,"So in the condition part, we assume that"
cs-410_1_4_121,cs-410,1,4,"00:08:51,539","00:08:54,640",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,have seen that the user
cs-410_1_4_122,cs-410,1,4,"00:08:56,190","00:08:58,777",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,And this part shows that
cs-410_1_4_123,cs-410,1,4,"00:08:58,777","00:09:01,438",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,likely the user would
cs-410_1_4_124,cs-410,1,4,"00:09:01,438","00:09:04,653",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,How likely we will see this
cs-410_1_4_125,cs-410,1,4,"00:09:04,653","00:09:08,880",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,"So note that here, we have made"
cs-410_1_4_126,cs-410,1,4,"00:09:08,880","00:09:13,900",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"Basically, we're going to do, assume that"
cs-410_1_4_127,cs-410,1,4,"00:09:13,900","00:09:17,970",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,has something to do with whether
cs-410_1_4_128,cs-410,1,4,"00:09:17,970","00:09:20,740",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,"In other words,"
cs-410_1_4_129,cs-410,1,4,"00:09:22,160","00:09:27,671",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,And that is a user formulates a query
cs-410_1_4_130,cs-410,1,4,"00:09:27,671","00:09:30,358",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,Where if you just look at this
cs-410_1_4_131,cs-410,1,4,"00:09:30,358","00:09:32,629",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,it's not obvious we
cs-410_1_4_132,cs-410,1,4,"00:09:32,629","00:09:37,941",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,So what I really meant is that
cs-410_1_4_133,cs-410,1,4,"00:09:37,941","00:09:43,367",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,"probability to help us score,"
cs-410_1_4_134,cs-410,1,4,"00:09:43,367","00:09:48,794",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,probability will have to somehow
cs-410_1_4_135,cs-410,1,4,"00:09:48,794","00:09:54,696",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,conditional probability without
cs-410_1_4_136,cs-410,1,4,"00:09:54,696","00:09:59,306",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,Otherwise we would be having
cs-410_1_4_137,cs-410,1,4,"00:09:59,306","00:10:04,537",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"by making this assumption,"
cs-410_1_4_138,cs-410,1,4,"00:10:04,537","00:10:09,252",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,and try to just model how the user
cs-410_1_4_139,cs-410,1,4,"00:10:09,252","00:10:13,639",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,So this is how you can
cs-410_1_4_140,cs-410,1,4,"00:10:13,639","00:10:18,570",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,that we can derive a specific
cs-410_1_4_141,cs-410,1,4,"00:10:18,570","00:10:22,020",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,So let's look at how this model work for
cs-410_1_4_142,cs-410,1,4,"00:10:22,020","00:10:23,300",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,"And basically,"
cs-410_1_4_143,cs-410,1,4,"00:10:23,300","00:10:27,420",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,what we are going to do in this case
cs-410_1_4_144,cs-410,1,4,"00:10:27,420","00:10:30,760",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,Which of these documents is most
cs-410_1_4_145,cs-410,1,4,"00:10:30,760","00:10:34,300",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,document in the user's mind when
cs-410_1_4_146,cs-410,1,4,"00:10:34,300","00:10:38,488",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,So we ask this question and we quantify
cs-410_1_4_147,cs-410,1,4,"00:10:38,488","00:10:43,443",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,a conditional probability of observing
cs-410_1_4_148,cs-410,1,4,"00:10:43,443","00:10:47,245",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,fact the imaginary relevant
cs-410_1_4_149,cs-410,1,4,"00:10:47,245","00:10:51,885",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,Here you can see we've computed all
cs-410_1_4_150,cs-410,1,4,"00:10:51,885","00:10:55,340",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,The likelihood of queries
cs-410_1_4_151,cs-410,1,4,"00:10:55,340","00:10:56,880",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,"Once we have these values,"
cs-410_1_4_152,cs-410,1,4,"00:10:56,880","00:11:00,370",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,we can then rank these documents
cs-410_1_4_153,cs-410,1,4,"00:11:00,370","00:11:05,420",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,"So to summarize, the general idea"
cs-410_1_4_154,cs-410,1,4,"00:11:05,420","00:11:11,740",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,risk model is to assume the we introduce
cs-410_1_4_155,cs-410,1,4,"00:11:11,740","00:11:12,690",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,"And then,"
cs-410_1_4_156,cs-410,1,4,"00:11:12,690","00:11:16,740",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,let the scoring function be defined
cs-410_1_4_157,cs-410,1,4,"00:11:16,740","00:11:20,980",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,We also talked about approximating
cs-410_1_4_158,cs-410,1,4,"00:11:22,450","00:11:27,065",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=682,And in this case we have a ranking
cs-410_1_4_159,cs-410,1,4,"00:11:27,065","00:11:31,385",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,based on the probability of
cs-410_1_4_160,cs-410,1,4,"00:11:31,385","00:11:36,165",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,And this probability should be interpreted
cs-410_1_4_161,cs-410,1,4,"00:11:36,165","00:11:39,236",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,"likes document d, would pose query q."
cs-410_1_4_162,cs-410,1,4,"00:11:40,265","00:11:44,645",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,"Now, the question of course is, how do"
cs-410_1_4_163,cs-410,1,4,"00:11:44,645","00:11:49,500",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,At this in general has to do with how
cs-410_1_4_164,cs-410,1,4,"00:11:49,500","00:11:51,980",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,because q is a text.
cs-410_1_4_165,cs-410,1,4,"00:11:51,980","00:11:56,560",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,And this has to do with a model
cs-410_1_4_166,cs-410,1,4,"00:11:56,560","00:12:00,580",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=716,And these kind of models
cs-410_1_4_167,cs-410,1,4,"00:12:02,190","00:12:07,440",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,"So more specifically, we will be"
cs-410_1_4_168,cs-410,1,4,"00:12:07,440","00:12:12,050",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,conditional probability
cs-410_1_4_169,cs-410,1,4,"00:12:12,050","00:12:18,463",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,"If the user liked this document,"
cs-410_1_4_170,cs-410,1,4,"00:12:18,463","00:12:21,884",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,"And in the next lecture we're going to do,"
cs-410_1_4_171,cs-410,1,4,"00:12:21,884","00:12:27,016",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,giving introduction to language
cs-410_1_4_172,cs-410,1,4,"00:12:27,016","00:12:32,063",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=747,can model text that was a probable
cs-410_1_4_173,cs-410,1,4,"00:12:32,063","00:12:42,063",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,[MUSIC]
cs-410_1_5_1,cs-410,1,5,"00:00:00,012","00:00:07,436",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_5_2,cs-410,1,5,"00:00:07,436","00:00:10,250",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about the feedback
cs-410_1_5_3,cs-410,1,5,"00:00:12,910","00:00:17,060",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"So in this lecture, we will continue with"
cs-410_1_5_4,cs-410,1,5,"00:00:18,840","00:00:22,103",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"In particular, we're going to talk"
cs-410_1_5_5,cs-410,1,5,"00:00:24,866","00:00:28,380",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,This is a diagram that shows
cs-410_1_5_6,cs-410,1,5,"00:00:30,685","00:00:34,895",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,We can see the user would type in a query.
cs-410_1_5_7,cs-410,1,5,"00:00:37,365","00:00:41,965",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,"And then, the query would be"
cs-410_1_5_8,cs-410,1,5,"00:00:41,965","00:00:46,015",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,"search engine, and"
cs-410_1_5_9,cs-410,1,5,"00:00:46,015","00:00:47,865",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,These results would be issued to the user.
cs-410_1_5_10,cs-410,1,5,"00:00:49,475","00:00:52,760",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,"Now, after the user has"
cs-410_1_5_11,cs-410,1,5,"00:00:52,760","00:00:55,410",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,the user can actually make judgements.
cs-410_1_5_12,cs-410,1,5,"00:00:55,410","00:00:59,009",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,"So for example, the user says,"
cs-410_1_5_13,cs-410,1,5,"00:00:59,009","00:01:03,097",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,this document is not very useful and
cs-410_1_5_14,cs-410,1,5,"00:01:03,097","00:01:07,921",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,"Now, this is called a relevance judgment"
cs-410_1_5_15,cs-410,1,5,"00:01:07,921","00:01:12,510",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,got some feedback information from
cs-410_1_5_16,cs-410,1,5,"00:01:12,510","00:01:14,930",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"And this can be very useful to the system,"
cs-410_1_5_17,cs-410,1,5,"00:01:14,930","00:01:18,320",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,knowing what exactly is
cs-410_1_5_18,cs-410,1,5,"00:01:18,320","00:01:22,790",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,So the feedback module would
cs-410_1_5_19,cs-410,1,5,"00:01:22,790","00:01:26,970",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,also use the document collection
cs-410_1_5_20,cs-410,1,5,"00:01:26,970","00:01:30,720",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,Typically it would involve
cs-410_1_5_21,cs-410,1,5,"00:01:30,720","00:01:34,960",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,the system can now render the results
cs-410_1_5_22,cs-410,1,5,"00:01:34,960","00:01:36,897",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,So this is called relevance feedback.
cs-410_1_5_23,cs-410,1,5,"00:01:36,897","00:01:42,470",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,The feedback is based on relevance
cs-410_1_5_24,cs-410,1,5,"00:01:42,470","00:01:44,660",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"Now, these judgements are reliable but"
cs-410_1_5_25,cs-410,1,5,"00:01:44,660","00:01:50,350",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,the users generally don't want to make
cs-410_1_5_26,cs-410,1,5,"00:01:50,350","00:01:54,980",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,So the down side is that it involves
cs-410_1_5_27,cs-410,1,5,"00:01:57,250","00:02:00,920",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,There's another form of feedback
cs-410_1_5_28,cs-410,1,5,"00:02:00,920","00:02:03,800",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"blind feedback,"
cs-410_1_5_29,cs-410,1,5,"00:02:03,800","00:02:08,380",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,"In this case, we can see once"
cs-410_1_5_30,cs-410,1,5,"00:02:08,380","00:02:11,340",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,in fact we don't have to invoke users.
cs-410_1_5_31,cs-410,1,5,"00:02:11,340","00:02:13,720",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,So you can see there's
cs-410_1_5_32,cs-410,1,5,"00:02:14,730","00:02:19,846",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,And we simply assume that the top
cs-410_1_5_33,cs-410,1,5,"00:02:19,846","00:02:23,940",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,Let's say we have assumed
cs-410_1_5_34,cs-410,1,5,"00:02:25,250","00:02:31,000",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,"And then, we will then use this"
cs-410_1_5_35,cs-410,1,5,"00:02:31,000","00:02:33,110",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,and to improve the query.
cs-410_1_5_36,cs-410,1,5,"00:02:34,110","00:02:35,821",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"Now, you might wonder,"
cs-410_1_5_37,cs-410,1,5,"00:02:35,821","00:02:40,887",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,how could this help if we simply
cs-410_1_5_38,cs-410,1,5,"00:02:40,887","00:02:46,490",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,"Well, you can imagine these top"
cs-410_1_5_39,cs-410,1,5,"00:02:46,490","00:02:52,070",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,similar to relevant documents
cs-410_1_5_40,cs-410,1,5,"00:02:52,070","00:02:53,480",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,They look like relevant documents.
cs-410_1_5_41,cs-410,1,5,"00:02:53,480","00:02:59,350",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,So it's possible to learn some related
cs-410_1_5_42,cs-410,1,5,"00:02:59,350","00:03:03,610",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,"In fact, you may recall that we"
cs-410_1_5_43,cs-410,1,5,"00:03:03,610","00:03:08,180",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,"analyze what association, to learn"
cs-410_1_5_44,cs-410,1,5,"00:03:09,480","00:03:13,040",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,"And there, what we did is we"
cs-410_1_5_45,cs-410,1,5,"00:03:13,040","00:03:15,500",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,all the documents that contain computer.
cs-410_1_5_46,cs-410,1,5,"00:03:15,500","00:03:18,761",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,So imagine now the query
cs-410_1_5_47,cs-410,1,5,"00:03:18,761","00:03:23,870",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"And then, the result will be those"
cs-410_1_5_48,cs-410,1,5,"00:03:23,870","00:03:29,040",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,And what we can do then is
cs-410_1_5_49,cs-410,1,5,"00:03:29,040","00:03:31,860",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,They can match computer very well.
cs-410_1_5_50,cs-410,1,5,"00:03:31,860","00:03:36,890",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,And we're going to count
cs-410_1_5_51,cs-410,1,5,"00:03:36,890","00:03:42,126",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,"And then, we're going to then use"
cs-410_1_5_52,cs-410,1,5,"00:03:42,126","00:03:47,794",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,the terms that are frequent in this set
cs-410_1_5_53,cs-410,1,5,"00:03:47,794","00:03:52,364",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,So if we make a contrast between
cs-410_1_5_54,cs-410,1,5,"00:03:52,364","00:03:57,360",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,is that related to terms
cs-410_1_5_55,cs-410,1,5,"00:03:57,360","00:03:58,528",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,As we have seen before.
cs-410_1_5_56,cs-410,1,5,"00:03:58,528","00:04:04,786",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,And these related words can then be added
cs-410_1_5_57,cs-410,1,5,"00:04:04,786","00:04:08,770",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,And this would help us bring the documents
cs-410_1_5_58,cs-410,1,5,"00:04:08,770","00:04:11,640",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,match other words like program and
cs-410_1_5_59,cs-410,1,5,"00:04:11,640","00:04:16,450",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,So this is very effective for
cs-410_1_5_60,cs-410,1,5,"00:04:18,590","00:04:21,790",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,"But of course, pseudo-relevancy"
cs-410_1_5_61,cs-410,1,5,"00:04:21,790","00:04:24,050",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,We have to arbitrarily set a cut off.
cs-410_1_5_62,cs-410,1,5,"00:04:24,050","00:04:27,010",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,So there's also something in
cs-410_1_5_63,cs-410,1,5,"00:04:27,010","00:04:31,120",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,"In this case,"
cs-410_1_5_64,cs-410,1,5,"00:04:31,120","00:04:33,510",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,we don't have to ask
cs-410_1_5_65,cs-410,1,5,"00:04:33,510","00:04:38,730",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"Instead, we're going to observe how the"
cs-410_1_5_66,cs-410,1,5,"00:04:38,730","00:04:41,760",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,So in this case we'll look
cs-410_1_5_67,cs-410,1,5,"00:04:41,760","00:04:43,930",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,So the user clicked on this one.
cs-410_1_5_68,cs-410,1,5,"00:04:43,930","00:04:45,620",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,And the user viewed this one.
cs-410_1_5_69,cs-410,1,5,"00:04:45,620","00:04:47,480",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,And the user skipped this one.
cs-410_1_5_70,cs-410,1,5,"00:04:47,480","00:04:49,400",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,And the user viewed this one again.
cs-410_1_5_71,cs-410,1,5,"00:04:50,410","00:04:56,880",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,"Now, this also is a clue about whether"
cs-410_1_5_72,cs-410,1,5,"00:04:56,880","00:05:01,540",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,And we can even assume that we're
cs-410_1_5_73,cs-410,1,5,"00:05:01,540","00:05:05,930",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,"here in this document,"
cs-410_1_5_74,cs-410,1,5,"00:05:05,930","00:05:10,810",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,instead of the actual
cs-410_1_5_75,cs-410,1,5,"00:05:10,810","00:05:15,370",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,The link they are saying web search
cs-410_1_5_76,cs-410,1,5,"00:05:15,370","00:05:20,250",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,If the user tries to fetch this
cs-410_1_5_77,cs-410,1,5,"00:05:20,250","00:05:25,400",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,we can assume these displayed
cs-410_1_5_78,cs-410,1,5,"00:05:25,400","00:05:29,310",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,is interesting to you so
cs-410_1_5_79,cs-410,1,5,"00:05:29,310","00:05:31,830",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,And this is called interesting feedback.
cs-410_1_5_80,cs-410,1,5,"00:05:31,830","00:05:35,400",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,"And we can, again,"
cs-410_1_5_81,cs-410,1,5,"00:05:35,400","00:05:39,760",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,This is a very important
cs-410_1_5_82,cs-410,1,5,"00:05:39,760","00:05:42,080",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"Now, think about the Google and Bing and"
cs-410_1_5_83,cs-410,1,5,"00:05:42,080","00:05:46,990",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,they can collect a lot of user
cs-410_1_5_84,cs-410,1,5,"00:05:46,990","00:05:51,320",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,So they would observe what documents
cs-410_1_5_85,cs-410,1,5,"00:05:51,320","00:05:54,040",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,And this information is very valuable.
cs-410_1_5_86,cs-410,1,5,"00:05:54,040","00:05:57,680",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,And they can use this to
cs-410_1_5_87,cs-410,1,5,"00:05:59,040","00:06:03,625",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,"So to summarize, we talked about"
cs-410_1_5_88,cs-410,1,5,"00:06:03,625","00:06:07,280",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,Relevant feedback where the user
cs-410_1_5_89,cs-410,1,5,"00:06:07,280","00:06:11,200",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,"It takes some user effort, but"
cs-410_1_5_90,cs-410,1,5,"00:06:11,200","00:06:15,931",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,We talk about the pseudo feedback where
cs-410_1_5_91,cs-410,1,5,"00:06:15,931","00:06:17,310",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,will be relevant.
cs-410_1_5_92,cs-410,1,5,"00:06:17,310","00:06:20,540",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,We don't have to involve the user
cs-410_1_5_93,cs-410,1,5,"00:06:20,540","00:06:23,590",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,actually before we return
cs-410_1_5_94,cs-410,1,5,"00:06:24,850","00:06:28,014",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,And the third is implicit feedback
cs-410_1_5_95,cs-410,1,5,"00:06:29,685","00:06:31,530",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"Where we involve the users, but"
cs-410_1_5_96,cs-410,1,5,"00:06:31,530","00:06:34,887",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,the user doesn't have to make
cs-410_1_5_97,cs-410,1,5,"00:06:34,887","00:06:36,118",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,Make judgement.
cs-410_1_5_98,cs-410,1,5,"00:06:36,118","00:06:46,118",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,[MUSIC]
cs-410_1_6_1,cs-410,1,6,"00:00:00,000","00:00:02,695",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_1_6_2,cs-410,1,6,"00:00:07,363","00:00:10,900",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about
cs-410_1_6_3,cs-410,1,6,"00:00:10,900","00:00:15,210",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,"In this lecture, we are going to"
cs-410_1_6_4,cs-410,1,6,"00:00:15,210","00:00:17,860",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,In particular we're going to talk
cs-410_1_6_5,cs-410,1,6,"00:00:17,860","00:00:21,339",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,to combine different features
cs-410_1_6_6,cs-410,1,6,"00:00:22,340","00:00:28,500",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,So the question that we address in
cs-410_1_6_7,cs-410,1,6,"00:00:28,500","00:00:36,230",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,many features to generate a single ranking
cs-410_1_6_8,cs-410,1,6,"00:00:36,230","00:00:42,140",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,In the previous lectures we have talked
cs-410_1_6_9,cs-410,1,6,"00:00:42,140","00:00:48,270",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,We have talked about some retrieval
cs-410_1_6_10,cs-410,1,6,"00:00:48,270","00:00:54,760",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,They can generate a based this course for
cs-410_1_6_11,cs-410,1,6,"00:00:54,760","00:00:58,130",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,And we also talked about the link
cs-410_1_6_12,cs-410,1,6,"00:00:59,230","00:01:02,759",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,that can give additional scores
cs-410_1_6_13,cs-410,1,6,"00:01:03,940","00:01:07,313",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,"Now the question now is,"
cs-410_1_6_14,cs-410,1,6,"00:01:07,313","00:01:09,843",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,potentially many other
cs-410_1_6_15,cs-410,1,6,"00:01:09,843","00:01:14,912",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,And this will be very useful for
cs-410_1_6_16,cs-410,1,6,"00:01:14,912","00:01:19,833",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,"accuracy, but also to improve"
cs-410_1_6_17,cs-410,1,6,"00:01:19,833","00:01:24,176",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,So that it's not easy for
cs-410_1_6_18,cs-410,1,6,"00:01:24,176","00:01:26,720",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,a few features to promote a page.
cs-410_1_6_19,cs-410,1,6,"00:01:27,910","00:01:32,000",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,So the general idea of learning
cs-410_1_6_20,cs-410,1,6,"00:01:32,000","00:01:36,030",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,learning to combine this
cs-410_1_6_21,cs-410,1,6,"00:01:36,030","00:01:39,160",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,on different features to generate
cs-410_1_6_22,cs-410,1,6,"00:01:40,610","00:01:44,720",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,So we will assume that the given
cs-410_1_6_23,cs-410,1,6,"00:01:44,720","00:01:49,680",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,we can define a number of features.
cs-410_1_6_24,cs-410,1,6,"00:01:49,680","00:01:55,180",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,And these features can vary from
cs-410_1_6_25,cs-410,1,6,"00:01:55,180","00:01:59,875",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,a score of the document with
cs-410_1_6_26,cs-410,1,6,"00:01:59,875","00:02:05,060",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,a retrieval function such as BM25 or
cs-410_1_6_27,cs-410,1,6,"00:02:05,060","00:02:10,440",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,of punitive commands from a machine or
cs-410_1_6_28,cs-410,1,6,"00:02:10,440","00:02:15,410",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,It can also be a link based score like or
cs-410_1_6_29,cs-410,1,6,"00:02:15,410","00:02:23,470",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,It can be also application of retrieval
cs-410_1_6_30,cs-410,1,6,"00:02:23,470","00:02:28,240",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,Those are the types of descriptions
cs-410_1_6_31,cs-410,1,6,"00:02:29,520","00:02:33,909",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,"So, these can all the clues whether"
cs-410_1_6_32,cs-410,1,6,"00:02:35,070","00:02:41,320",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,We can even include a feature
cs-410_1_6_33,cs-410,1,6,"00:02:41,320","00:02:46,680",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,has a tilde because this might be
cs-410_1_6_34,cs-410,1,6,"00:02:48,170","00:02:52,180",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,So all these features can then be combined
cs-410_1_6_35,cs-410,1,6,"00:02:52,180","00:02:53,610",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"The question is, of course."
cs-410_1_6_36,cs-410,1,6,"00:02:53,610","00:02:55,250",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,How can we combine them?
cs-410_1_6_37,cs-410,1,6,"00:02:55,250","00:03:00,580",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,"In this approach,"
cs-410_1_6_38,cs-410,1,6,"00:03:00,580","00:03:07,730",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,that this document isn't relevant to this
cs-410_1_6_39,cs-410,1,6,"00:03:07,730","00:03:10,329",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,So we can hypothesize this
cs-410_1_6_40,cs-410,1,6,"00:03:11,450","00:03:16,730",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,that the probability of relevance
cs-410_1_6_41,cs-410,1,6,"00:03:16,730","00:03:22,070",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,through a particular form of
cs-410_1_6_42,cs-410,1,6,"00:03:22,070","00:03:25,510",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,These parameters can control
cs-410_1_6_43,cs-410,1,6,"00:03:25,510","00:03:29,410",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,the influence of different
cs-410_1_6_44,cs-410,1,6,"00:03:29,410","00:03:33,820",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,Now this is of course just an assumption.
cs-410_1_6_45,cs-410,1,6,"00:03:33,820","00:03:38,925",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,Whether this assumption really
cs-410_1_6_46,cs-410,1,6,"00:03:38,925","00:03:43,570",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,that's they have to empirically
cs-410_1_6_47,cs-410,1,6,"00:03:45,020","00:03:50,450",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,But by hypothesizing that
cs-410_1_6_48,cs-410,1,6,"00:03:50,450","00:03:55,783",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,"features in the particular way, we can"
cs-410_1_6_49,cs-410,1,6,"00:03:55,783","00:04:00,805",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,the potential more powerful ranking
cs-410_1_6_50,cs-410,1,6,"00:04:00,805","00:04:05,342",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,Naturally the next question is how
cs-410_1_6_51,cs-410,1,6,"00:04:05,342","00:04:08,732",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,How do we know which features
cs-410_1_6_52,cs-410,1,6,"00:04:08,732","00:04:11,922",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,and which features will have lower weight?
cs-410_1_6_53,cs-410,1,6,"00:04:11,922","00:04:15,732",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,So this is the task of training or
cs-410_1_6_54,cs-410,1,6,"00:04:15,732","00:04:20,000",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,in this approach what we will
cs-410_1_6_55,cs-410,1,6,"00:04:20,000","00:04:24,910",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,Those are the data that have
cs-410_1_6_56,cs-410,1,6,"00:04:24,910","00:04:27,370",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,that we already know
cs-410_1_6_57,cs-410,1,6,"00:04:27,370","00:04:31,443",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,We already know which documents should
cs-410_1_6_58,cs-410,1,6,"00:04:31,443","00:04:36,074",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,And this information can be based
cs-410_1_6_59,cs-410,1,6,"00:04:36,074","00:04:41,508",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,this can also be approximated by just
cs-410_1_6_60,cs-410,1,6,"00:04:41,508","00:04:47,477",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,where we can assume the clicked documents
cs-410_1_6_61,cs-410,1,6,"00:04:47,477","00:04:53,500",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,clicked documents are relevant and
cs-410_1_6_62,cs-410,1,6,"00:04:53,500","00:04:58,222",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,So in general with the fit
cs-410_1_6_63,cs-410,1,6,"00:04:58,222","00:05:00,960",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,function to the training data
cs-410_1_6_64,cs-410,1,6,"00:05:00,960","00:05:06,650",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,meaning that we will try to optimize it's
cs-410_1_6_65,cs-410,1,6,"00:05:06,650","00:05:08,920",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,And we can adjust these parameters to see
cs-410_1_6_66,cs-410,1,6,"00:05:09,960","00:05:14,780",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,how we can optimize the performance of
cs-410_1_6_67,cs-410,1,6,"00:05:16,030","00:05:19,180",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,in terms of some measures such as MAP or
cs-410_1_6_68,cs-410,1,6,"00:05:20,600","00:05:25,440",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,So the training date would
cs-410_1_6_69,cs-410,1,6,"00:05:25,440","00:05:32,800",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,"Each tuple has three elements, the query,"
cs-410_1_6_70,cs-410,1,6,"00:05:32,800","00:05:37,469",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,So it looks very much like our
cs-410_1_6_71,cs-410,1,6,"00:05:37,469","00:05:40,933",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,about in the evaluation
cs-410_1_6_72,cs-410,1,6,"00:05:40,933","00:05:50,933",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,[MUSIC]
cs-410_1_7_1,cs-410,1,7,"00:00:00,012","00:00:06,665",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_7_2,cs-410,1,7,"00:00:06,665","00:00:11,700",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,this lecture we give an overview
cs-410_1_7_3,cs-410,1,7,"00:00:13,743","00:00:19,830",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"First, let's define the term text mining,"
cs-410_1_7_4,cs-410,1,7,"00:00:19,830","00:00:24,200",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,The title of this course is
cs-410_1_7_5,cs-410,1,7,"00:00:25,590","00:00:31,250",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"But the two terms text mining, and text"
cs-410_1_7_6,cs-410,1,7,"00:00:32,670","00:00:36,370",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,So we are not really going to
cs-410_1_7_7,cs-410,1,7,"00:00:36,370","00:00:38,230",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,we're going to use them interchangeably.
cs-410_1_7_8,cs-410,1,7,"00:00:38,230","00:00:42,880",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,But the reason that we have chosen to use
cs-410_1_7_9,cs-410,1,7,"00:00:42,880","00:00:47,720",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,both terms in the title is because
cs-410_1_7_10,cs-410,1,7,"00:00:47,720","00:00:51,070",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,if you look at the two phrases literally.
cs-410_1_7_11,cs-410,1,7,"00:00:52,110","00:00:55,640",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,Mining emphasizes more on the process.
cs-410_1_7_12,cs-410,1,7,"00:00:55,640","00:01:01,683",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,So it gives us a error rate
cs-410_1_7_13,cs-410,1,7,"00:01:01,683","00:01:06,359",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"Analytics, on the other hand"
cs-410_1_7_14,cs-410,1,7,"00:01:07,600","00:01:09,900",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,or having a problem in mind.
cs-410_1_7_15,cs-410,1,7,"00:01:09,900","00:01:14,720",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,We are going to look at text
cs-410_1_7_16,cs-410,1,7,"00:01:16,010","00:01:19,940",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,"But again as I said, we can treat"
cs-410_1_7_17,cs-410,1,7,"00:01:21,150","00:01:24,820",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,And I think in the literature
cs-410_1_7_18,cs-410,1,7,"00:01:24,820","00:01:27,820",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,So we're not going to really
cs-410_1_7_19,cs-410,1,7,"00:01:29,850","00:01:35,450",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,Both text mining and
cs-410_1_7_20,cs-410,1,7,"00:01:35,450","00:01:40,250",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,want to turn text data into high quality
cs-410_1_7_21,cs-410,1,7,"00:01:42,570","00:01:44,020",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"So in both cases, we"
cs-410_1_7_22,cs-410,1,7,"00:01:45,830","00:01:50,670",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,have the problem of dealing with
cs-410_1_7_23,cs-410,1,7,"00:01:50,670","00:01:56,090",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,Turn these text data into something more
cs-410_1_7_24,cs-410,1,7,"00:01:57,730","00:02:00,380",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,And here we distinguish
cs-410_1_7_25,cs-410,1,7,"00:02:00,380","00:02:04,530",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"One is high-quality information,"
cs-410_1_7_26,cs-410,1,7,"00:02:05,740","00:02:08,680",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,Sometimes the boundary between
cs-410_1_7_27,cs-410,1,7,"00:02:09,850","00:02:11,690",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,But I also want to say a little bit about
cs-410_1_7_28,cs-410,1,7,"00:02:12,780","00:02:17,590",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,these two different angles of
cs-410_1_7_29,cs-410,1,7,"00:02:19,250","00:02:22,982",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,"In the case of high quality information,"
cs-410_1_7_30,cs-410,1,7,"00:02:22,982","00:02:27,715",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,concise information about the topic.
cs-410_1_7_31,cs-410,1,7,"00:02:28,895","00:02:34,205",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,Which might be much easier for
cs-410_1_7_32,cs-410,1,7,"00:02:34,205","00:02:37,045",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"For example, you might face"
cs-410_1_7_33,cs-410,1,7,"00:02:38,260","00:02:42,700",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,A more concise form of information
cs-410_1_7_34,cs-410,1,7,"00:02:42,700","00:02:46,570",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,of the major opinions about
cs-410_1_7_35,cs-410,1,7,"00:02:46,570","00:02:50,874",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,"Positive about,"
cs-410_1_7_36,cs-410,1,7,"00:02:53,436","00:02:58,260",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,Now this kind of results are very useful
cs-410_1_7_37,cs-410,1,7,"00:02:59,930","00:03:05,030",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,And so this is to minimize a human effort
cs-410_1_7_38,cs-410,1,7,"00:03:06,250","00:03:09,880",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,The other kind of output
cs-410_1_7_39,cs-410,1,7,"00:03:09,880","00:03:15,300",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,Here we emphasize the utility
cs-410_1_7_40,cs-410,1,7,"00:03:15,300","00:03:17,190",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,knowledge we discover from text data.
cs-410_1_7_41,cs-410,1,7,"00:03:18,270","00:03:23,830",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,It's actionable knowledge for some
cs-410_1_7_42,cs-410,1,7,"00:03:24,990","00:03:31,510",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,"For example, we might be able to determine"
cs-410_1_7_43,cs-410,1,7,"00:03:31,510","00:03:36,270",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,or a better choice for
cs-410_1_7_44,cs-410,1,7,"00:03:38,115","00:03:43,190",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"Now, such an outcome could be"
cs-410_1_7_45,cs-410,1,7,"00:03:43,190","00:03:49,550",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,because a consumer can take the knowledge
cs-410_1_7_46,cs-410,1,7,"00:03:49,550","00:03:55,131",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,"So, in this case text mining supplies"
cs-410_1_7_47,cs-410,1,7,"00:03:55,131","00:03:59,424",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,"But again, the two are not so"
cs-410_1_7_48,cs-410,1,7,"00:03:59,424","00:04:03,281",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,we don't necessarily have
cs-410_1_7_49,cs-410,1,7,"00:04:06,253","00:04:09,821",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,Text mining is also
cs-410_1_7_50,cs-410,1,7,"00:04:09,821","00:04:14,380",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,which is a essential component
cs-410_1_7_51,cs-410,1,7,"00:04:15,910","00:04:20,434",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,"Now, text retrieval refers to"
cs-410_1_7_52,cs-410,1,7,"00:04:20,434","00:04:22,380",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,a large amount of text data.
cs-410_1_7_53,cs-410,1,7,"00:04:24,140","00:04:30,210",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,So I've taught another separate MOOC
cs-410_1_7_54,cs-410,1,7,"00:04:31,710","00:04:34,680",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,Where we discussed various techniques for
cs-410_1_7_55,cs-410,1,7,"00:04:36,360","00:04:41,080",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,"If you have taken that MOOC,"
cs-410_1_7_56,cs-410,1,7,"00:04:42,120","00:04:46,700",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,And it will be useful To know
cs-410_1_7_57,cs-410,1,7,"00:04:46,700","00:04:50,010",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,of understanding some of
cs-410_1_7_58,cs-410,1,7,"00:04:51,750","00:04:54,350",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,"But, if you have not taken that MOOC,"
cs-410_1_7_59,cs-410,1,7,"00:04:54,350","00:04:59,440",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,it's also fine because in this MOOC
cs-410_1_7_60,cs-410,1,7,"00:04:59,440","00:05:03,260",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,going to repeat some of the key concepts
cs-410_1_7_61,cs-410,1,7,"00:05:03,260","00:05:06,540",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,But they're at the high level and
cs-410_1_7_62,cs-410,1,7,"00:05:06,540","00:05:10,710",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,they also explain the relation between
cs-410_1_7_63,cs-410,1,7,"00:05:12,320","00:05:18,278",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,Text retrieval is very useful for
cs-410_1_7_64,cs-410,1,7,"00:05:18,278","00:05:23,200",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,"First, text retrieval can be"
cs-410_1_7_65,cs-410,1,7,"00:05:23,200","00:05:27,600",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,Meaning that it can help
cs-410_1_7_66,cs-410,1,7,"00:05:27,600","00:05:32,030",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,a relatively small amount
cs-410_1_7_67,cs-410,1,7,"00:05:32,030","00:05:35,180",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,Which is often what's needed for
cs-410_1_7_68,cs-410,1,7,"00:05:36,580","00:05:41,186",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,"And in this sense, text retrieval"
cs-410_1_7_69,cs-410,1,7,"00:05:43,323","00:05:46,365",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,Text retrieval is also needed for
cs-410_1_7_70,cs-410,1,7,"00:05:46,365","00:05:50,976",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,And this roughly corresponds
cs-410_1_7_71,cs-410,1,7,"00:05:50,976","00:05:56,350",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,mining as turning text data
cs-410_1_7_72,cs-410,1,7,"00:05:56,350","00:05:58,970",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"Once we find the patterns in text data, or"
cs-410_1_7_73,cs-410,1,7,"00:05:58,970","00:06:04,040",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,"actionable knowledge, we generally"
cs-410_1_7_74,cs-410,1,7,"00:06:04,040","00:06:06,580",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,By looking at the original text data.
cs-410_1_7_75,cs-410,1,7,"00:06:06,580","00:06:11,110",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,So the users would have to have some text
cs-410_1_7_76,cs-410,1,7,"00:06:11,110","00:06:16,010",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,text data to interpret the pattern or
cs-410_1_7_77,cs-410,1,7,"00:06:16,010","00:06:19,910",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,to verify whether a pattern
cs-410_1_7_78,cs-410,1,7,"00:06:19,910","00:06:23,830",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,So this is a high level introduction
cs-410_1_7_79,cs-410,1,7,"00:06:23,830","00:06:29,530",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,and the relationship between
cs-410_1_7_80,cs-410,1,7,"00:06:32,110","00:06:36,554",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"Next, let's talk about text"
cs-410_1_7_81,cs-410,1,7,"00:06:39,689","00:06:45,607",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,Now it's interesting to
cs-410_1_7_82,cs-410,1,7,"00:06:45,607","00:06:51,380",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,generated by humans as subjective sensors.
cs-410_1_7_83,cs-410,1,7,"00:06:53,200","00:07:03,420",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,"So, this slide shows an analogy"
cs-410_1_7_84,cs-410,1,7,"00:07:03,420","00:07:07,832",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,And between humans as
cs-410_1_7_85,cs-410,1,7,"00:07:07,832","00:07:13,993",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,"physical sensors,"
cs-410_1_7_86,cs-410,1,7,"00:07:16,292","00:07:21,377",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,So in general a sensor would
cs-410_1_7_87,cs-410,1,7,"00:07:21,377","00:07:26,483",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,It would sense some signal
cs-410_1_7_88,cs-410,1,7,"00:07:26,483","00:07:32,205",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"then would report the signal as data,"
cs-410_1_7_89,cs-410,1,7,"00:07:32,205","00:07:38,346",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,"For example, a thermometer would watch"
cs-410_1_7_90,cs-410,1,7,"00:07:38,346","00:07:43,310",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,then we report the temperature
cs-410_1_7_91,cs-410,1,7,"00:07:44,962","00:07:49,098",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,"Similarly, a geo sensor would sense"
cs-410_1_7_92,cs-410,1,7,"00:07:49,098","00:07:53,740",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"The location specification, for"
cs-410_1_7_93,cs-410,1,7,"00:07:53,740","00:07:57,140",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"example, in the form of longitude"
cs-410_1_7_94,cs-410,1,7,"00:07:57,140","00:08:02,580",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,A network sends over
cs-410_1_7_95,cs-410,1,7,"00:08:02,580","00:08:04,873",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,or activities in the network and
cs-410_1_7_96,cs-410,1,7,"00:08:04,873","00:08:09,477",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,Some digital format of data.
cs-410_1_7_97,cs-410,1,7,"00:08:09,477","00:08:16,460",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,Similarly we can think of
cs-410_1_7_98,cs-410,1,7,"00:08:16,460","00:08:22,050",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,That will observe the real world and
cs-410_1_7_99,cs-410,1,7,"00:08:22,050","00:08:28,440",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,And then humans will express what they
cs-410_1_7_100,cs-410,1,7,"00:08:28,440","00:08:33,330",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,"So, in this sense, human is actually"
cs-410_1_7_101,cs-410,1,7,"00:08:33,330","00:08:36,200",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,sense what's happening in the world and
cs-410_1_7_102,cs-410,1,7,"00:08:36,200","00:08:43,060",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,then express what's observed in the form
cs-410_1_7_103,cs-410,1,7,"00:08:43,060","00:08:47,350",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,"Now, looking at the text data in"
cs-410_1_7_104,cs-410,1,7,"00:08:47,350","00:08:50,240",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,able to integrate all
cs-410_1_7_105,cs-410,1,7,"00:08:50,240","00:08:54,381",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,And that's indeed needed in
cs-410_1_7_106,cs-410,1,7,"00:08:56,123","00:09:01,672",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,So here we are looking at
cs-410_1_7_107,cs-410,1,7,"00:09:02,725","00:09:07,518",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,And in general we would Be
cs-410_1_7_108,cs-410,1,7,"00:09:07,518","00:09:11,982",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,about our world that
cs-410_1_7_109,cs-410,1,7,"00:09:11,982","00:09:17,180",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,And in general it will be dealing with
cs-410_1_7_110,cs-410,1,7,"00:09:17,180","00:09:21,348",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,And of course the non-text data
cs-410_1_7_111,cs-410,1,7,"00:09:21,348","00:09:26,330",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,And those non-text data can
cs-410_1_7_112,cs-410,1,7,"00:09:27,840","00:09:30,830",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"Numerical data, categorical,"
cs-410_1_7_113,cs-410,1,7,"00:09:30,830","00:09:33,800",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,or multi-media data like video or speech.
cs-410_1_7_114,cs-410,1,7,"00:09:36,360","00:09:41,900",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,"So, these non text data are often"
cs-410_1_7_115,cs-410,1,7,"00:09:41,900","00:09:45,590",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,"But text data is also very important,"
cs-410_1_7_116,cs-410,1,7,"00:09:45,590","00:09:50,960",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,mostly because they contain
cs-410_1_7_117,cs-410,1,7,"00:09:50,960","00:09:55,930",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,And they often contain
cs-410_1_7_118,cs-410,1,7,"00:09:55,930","00:09:58,860",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,especially preferences and
cs-410_1_7_119,cs-410,1,7,"00:10:01,360","00:10:07,990",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,"So, but by treating text data as"
cs-410_1_7_120,cs-410,1,7,"00:10:07,990","00:10:14,510",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,we can treat all this data
cs-410_1_7_121,cs-410,1,7,"00:10:14,510","00:10:18,110",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,So the data mining problem is
cs-410_1_7_122,cs-410,1,7,"00:10:18,110","00:10:22,960",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,turn all the data in your actionable
cs-410_1_7_123,cs-410,1,7,"00:10:22,960","00:10:26,260",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,of it to change the real
cs-410_1_7_124,cs-410,1,7,"00:10:26,260","00:10:31,490",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,So this means the data mining problem is
cs-410_1_7_125,cs-410,1,7,"00:10:31,490","00:10:37,450",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,basically taking a lot of data as input
cs-410_1_7_126,cs-410,1,7,"00:10:37,450","00:10:42,260",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=637,"Inside of the data mining module,"
cs-410_1_7_127,cs-410,1,7,"00:10:42,260","00:10:46,510",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,we have a number of different
cs-410_1_7_128,cs-410,1,7,"00:10:46,510","00:10:49,940",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,"And this is because, for"
cs-410_1_7_129,cs-410,1,7,"00:10:49,940","00:10:55,180",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,we generally need different algorithms for
cs-410_1_7_130,cs-410,1,7,"00:10:56,390","00:10:57,100",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,"For example,"
cs-410_1_7_131,cs-410,1,7,"00:10:57,100","00:11:01,870",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,video data might require computer
cs-410_1_7_132,cs-410,1,7,"00:11:01,870","00:11:06,050",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,And that would facilitate
cs-410_1_7_133,cs-410,1,7,"00:11:06,050","00:11:11,110",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,And we also have a lot of general
cs-410_1_7_134,cs-410,1,7,"00:11:11,110","00:11:16,948",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,"to all kinds of data and those algorithms,"
cs-410_1_7_135,cs-410,1,7,"00:11:16,948","00:11:19,692",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,"Although, for a particular kind of data,"
cs-410_1_7_136,cs-410,1,7,"00:11:19,692","00:11:23,287",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,we generally want to also
cs-410_1_7_137,cs-410,1,7,"00:11:23,287","00:11:27,939",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,So this course will cover
cs-410_1_7_138,cs-410,1,7,"00:11:27,939","00:11:31,994",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,are particularly useful for
cs-410_1_7_139,cs-410,1,7,"00:11:31,994","00:11:41,994",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,[MUSIC]
cs-410_1_8_1,cs-410,1,8,"00:00:00,250","00:00:06,380",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND].
cs-410_1_8_2,cs-410,1,8,"00:00:06,380","00:00:13,220",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,This lecture is about the syntagmatic
cs-410_1_8_3,cs-410,1,8,"00:00:13,220","00:00:17,760",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In this lecture, we're going to continue"
cs-410_1_8_4,cs-410,1,8,"00:00:17,760","00:00:22,420",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,"In particular, we're going to talk about"
cs-410_1_8_5,cs-410,1,8,"00:00:22,420","00:00:25,770",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,And we're going to start with
cs-410_1_8_6,cs-410,1,8,"00:00:25,770","00:00:29,860",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,which is the basis for designing some
cs-410_1_8_7,cs-410,1,8,"00:00:32,480","00:00:33,110",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"By definition,"
cs-410_1_8_8,cs-410,1,8,"00:00:33,110","00:00:39,890",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,syntagmatic relations hold between words
cs-410_1_8_9,cs-410,1,8,"00:00:39,890","00:00:44,190",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"That means,"
cs-410_1_8_10,cs-410,1,8,"00:00:44,190","00:00:47,350",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,we tend to see the occurrence
cs-410_1_8_11,cs-410,1,8,"00:00:48,370","00:00:53,560",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"So, take a more specific example, here."
cs-410_1_8_12,cs-410,1,8,"00:00:53,560","00:00:55,470",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"We can ask the question,"
cs-410_1_8_13,cs-410,1,8,"00:00:55,470","00:00:59,750",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,"whenever eats occurs,"
cs-410_1_8_14,cs-410,1,8,"00:01:01,140","00:01:06,000",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"Looking at the sentences on the left,"
cs-410_1_8_15,cs-410,1,8,"00:01:06,000","00:01:11,030",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"together with eats, like cat,"
cs-410_1_8_16,cs-410,1,8,"00:01:11,030","00:01:15,870",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,But if I take them out and
cs-410_1_8_17,cs-410,1,8,"00:01:15,870","00:01:21,550",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,"only show eats and some other words,"
cs-410_1_8_18,cs-410,1,8,"00:01:21,550","00:01:27,050",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,Can you predict what other words
cs-410_1_8_19,cs-410,1,8,"00:01:28,315","00:01:31,040",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,Right so
cs-410_1_8_20,cs-410,1,8,"00:01:31,040","00:01:33,630",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,other words are associated with eats.
cs-410_1_8_21,cs-410,1,8,"00:01:33,630","00:01:37,610",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"If they are associated with eats,"
cs-410_1_8_22,cs-410,1,8,"00:01:38,625","00:01:43,060",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,More specifically our
cs-410_1_8_23,cs-410,1,8,"00:01:43,060","00:01:47,072",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"any text segment which can be a sentence,"
cs-410_1_8_24,cs-410,1,8,"00:01:47,072","00:01:51,340",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"And then ask I the question,"
cs-410_1_8_25,cs-410,1,8,"00:01:51,340","00:01:52,640",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,absent in this segment?
cs-410_1_8_26,cs-410,1,8,"00:01:54,550","00:01:57,400",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,Right here we ask about the word W.
cs-410_1_8_27,cs-410,1,8,"00:01:57,400","00:02:00,160",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,Is W present or absent in this segment?
cs-410_1_8_28,cs-410,1,8,"00:02:02,400","00:02:05,100",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,Now what's interesting is that
cs-410_1_8_29,cs-410,1,8,"00:02:05,100","00:02:08,230",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,some words are actually easier
cs-410_1_8_30,cs-410,1,8,"00:02:10,150","00:02:14,570",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,If you take a look at the three
cs-410_1_8_31,cs-410,1,8,"00:02:14,570","00:02:17,970",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,"unicorn, which one do you"
cs-410_1_8_32,cs-410,1,8,"00:02:20,630","00:02:23,530",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,Now if you think about it for
cs-410_1_8_33,cs-410,1,8,"00:02:24,530","00:02:27,910",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,the is easier to predict because
cs-410_1_8_34,cs-410,1,8,"00:02:27,910","00:02:30,770",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"So I can just say,"
cs-410_1_8_35,cs-410,1,8,"00:02:31,940","00:02:37,946",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,Unicorn is also relatively easy
cs-410_1_8_36,cs-410,1,8,"00:02:37,946","00:02:41,470",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,And I can bet that it doesn't
cs-410_1_8_37,cs-410,1,8,"00:02:42,780","00:02:46,080",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,But meat is somewhere in
cs-410_1_8_38,cs-410,1,8,"00:02:46,080","00:02:50,580",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,And it makes it harder to predict because
cs-410_1_8_39,cs-410,1,8,"00:02:50,580","00:02:52,520",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,"or the segment, more accurately."
cs-410_1_8_40,cs-410,1,8,"00:02:53,842","00:02:58,820",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"But it may also not occur in the sentence,"
cs-410_1_8_41,cs-410,1,8,"00:02:58,820","00:03:01,500",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,now let's study this
cs-410_1_8_42,cs-410,1,8,"00:03:02,680","00:03:06,090",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,So the problem can be formally defined
cs-410_1_8_43,cs-410,1,8,"00:03:06,090","00:03:10,030",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,as predicting the value of
cs-410_1_8_44,cs-410,1,8,"00:03:10,030","00:03:14,080",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,"Here we denote it by X sub w,"
cs-410_1_8_45,cs-410,1,8,"00:03:14,080","00:03:17,340",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,this random variable is associated
cs-410_1_8_46,cs-410,1,8,"00:03:18,380","00:03:23,020",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"When the value of the variable is 1,"
cs-410_1_8_47,cs-410,1,8,"00:03:23,020","00:03:26,110",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,"When it's 0, it means the word is absent."
cs-410_1_8_48,cs-410,1,8,"00:03:26,110","00:03:31,010",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"And naturally, the probabilities for"
cs-410_1_8_49,cs-410,1,8,"00:03:31,010","00:03:34,187",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,because a word is either present or
cs-410_1_8_50,cs-410,1,8,"00:03:35,240","00:03:36,070",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,There's no other choice.
cs-410_1_8_51,cs-410,1,8,"00:03:38,290","00:03:43,610",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,So the intuition with this concept earlier
cs-410_1_8_52,cs-410,1,8,"00:03:43,610","00:03:48,280",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,"The more random this random variable is,"
cs-410_1_8_53,cs-410,1,8,"00:03:49,710","00:03:53,600",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,Now the question is how does one
cs-410_1_8_54,cs-410,1,8,"00:03:53,600","00:03:55,590",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,a random variable like X sub w?
cs-410_1_8_55,cs-410,1,8,"00:03:56,940","00:04:01,850",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"How in general, can we quantify"
cs-410_1_8_56,cs-410,1,8,"00:04:01,850","00:04:04,690",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,that's why we need a measure
cs-410_1_8_57,cs-410,1,8,"00:04:04,690","00:04:10,560",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,this measure introduced in information
cs-410_1_8_58,cs-410,1,8,"00:04:10,560","00:04:13,790",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,There is also some connection
cs-410_1_8_59,cs-410,1,8,"00:04:13,790","00:04:15,620",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,that is beyond the scope of this course.
cs-410_1_8_60,cs-410,1,8,"00:04:17,460","00:04:20,750",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,So for
cs-410_1_8_61,cs-410,1,8,"00:04:20,750","00:04:22,910",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,as a function defined
cs-410_1_8_62,cs-410,1,8,"00:04:22,910","00:04:27,000",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"In this case, it is a binary random"
cs-410_1_8_63,cs-410,1,8,"00:04:27,000","00:04:30,930",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,be easily generalized for
cs-410_1_8_64,cs-410,1,8,"00:04:32,070","00:04:34,950",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,"Now the function form looks like this,"
cs-410_1_8_65,cs-410,1,8,"00:04:34,950","00:04:39,410",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,there's the sum of all the possible
cs-410_1_8_66,cs-410,1,8,"00:04:39,410","00:04:44,030",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,Inside the sum for each value we
cs-410_1_8_67,cs-410,1,8,"00:04:45,210","00:04:52,060",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,that the random variable equals this
cs-410_1_8_68,cs-410,1,8,"00:04:53,380","00:04:55,250",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,And note that there is also
cs-410_1_8_69,cs-410,1,8,"00:04:56,270","00:04:59,900",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,Now entropy in general is non-negative.
cs-410_1_8_70,cs-410,1,8,"00:04:59,900","00:05:01,480",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,And that can be mathematically proved.
cs-410_1_8_71,cs-410,1,8,"00:05:02,620","00:05:10,320",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,"So if we expand this sum, we'll see that"
cs-410_1_8_72,cs-410,1,8,"00:05:10,320","00:05:14,130",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,Where I explicitly plugged
cs-410_1_8_73,cs-410,1,8,"00:05:14,130","00:05:18,370",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,"And sometimes when we have 0 log of 0,"
cs-410_1_8_74,cs-410,1,8,"00:05:18,370","00:05:25,960",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,"we would generally define that as 0,"
cs-410_1_8_75,cs-410,1,8,"00:05:28,480","00:05:30,330",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,So this is the entropy function.
cs-410_1_8_76,cs-410,1,8,"00:05:30,330","00:05:33,020",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,And this function will
cs-410_1_8_77,cs-410,1,8,"00:05:33,020","00:05:35,520",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,different distributions
cs-410_1_8_78,cs-410,1,8,"00:05:37,260","00:05:40,650",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,And it clearly depends on the probability
cs-410_1_8_79,cs-410,1,8,"00:05:40,650","00:05:43,850",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,that the random variable
cs-410_1_8_80,cs-410,1,8,"00:05:43,850","00:05:49,780",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,If we plot this function against
cs-410_1_8_81,cs-410,1,8,"00:05:49,780","00:05:55,114",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,the probability that the random
cs-410_1_8_82,cs-410,1,8,"00:05:56,990","00:05:59,080",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,And then the function looks like this.
cs-410_1_8_83,cs-410,1,8,"00:06:01,310","00:06:06,820",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,"At the two ends,"
cs-410_1_8_84,cs-410,1,8,"00:06:07,950","00:06:13,698",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,"equals 1 is very small or very large,"
cs-410_1_8_85,cs-410,1,8,"00:06:13,698","00:06:18,280",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,When it's 0.5 in the middle
cs-410_1_8_86,cs-410,1,8,"00:06:20,180","00:06:24,150",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,Now if we plot the function
cs-410_1_8_87,cs-410,1,8,"00:06:25,950","00:06:31,090",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,is taking a value of 0 and the function
cs-410_1_8_88,cs-410,1,8,"00:06:31,090","00:06:37,810",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,"would show exactly the same curve here,"
cs-410_1_8_89,cs-410,1,8,"00:06:37,810","00:06:40,620",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,And so that's because
cs-410_1_8_90,cs-410,1,8,"00:06:42,340","00:06:46,730",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,"the two probabilities are symmetric,"
cs-410_1_8_91,cs-410,1,8,"00:06:48,740","00:06:52,850",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,So an interesting question you
cs-410_1_8_92,cs-410,1,8,"00:06:52,850","00:06:59,390",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,what kind of X does entropy
cs-410_1_8_93,cs-410,1,8,"00:06:59,390","00:07:02,960",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,And we can in particular think
cs-410_1_8_94,cs-410,1,8,"00:07:02,960","00:07:07,700",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,"For example, in one case,"
cs-410_1_8_95,cs-410,1,8,"00:07:08,840","00:07:10,600",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,always takes a value of 1.
cs-410_1_8_96,cs-410,1,8,"00:07:10,600","00:07:14,304",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,The probability is 1.
cs-410_1_8_97,cs-410,1,8,"00:07:16,390","00:07:18,650",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,Or there's a random variable that
cs-410_1_8_98,cs-410,1,8,"00:07:19,890","00:07:24,320",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,is equally likely taking a value of one or
cs-410_1_8_99,cs-410,1,8,"00:07:24,320","00:07:28,750",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,So in this case the probability
cs-410_1_8_100,cs-410,1,8,"00:07:30,700","00:07:32,250",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,Now which one has a higher entropy?
cs-410_1_8_101,cs-410,1,8,"00:07:34,650","00:07:38,530",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,It's easier to look at the problem
cs-410_1_8_102,cs-410,1,8,"00:07:40,800","00:07:42,380",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,using coin tossing.
cs-410_1_8_103,cs-410,1,8,"00:07:43,420","00:07:47,660",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,So when we think about random
cs-410_1_8_104,cs-410,1,8,"00:07:48,770","00:07:55,740",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,"it gives us a random variable,"
cs-410_1_8_105,cs-410,1,8,"00:07:55,740","00:07:57,860",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,It can be head or tail.
cs-410_1_8_106,cs-410,1,8,"00:07:57,860","00:08:03,040",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,So we can define a random variable
cs-410_1_8_107,cs-410,1,8,"00:08:03,040","00:08:08,470",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,"when the coin shows up as head,"
cs-410_1_8_108,cs-410,1,8,"00:08:09,800","00:08:15,390",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,So now we can compute the entropy
cs-410_1_8_109,cs-410,1,8,"00:08:15,390","00:08:20,050",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,And this entropy indicates how
cs-410_1_8_110,cs-410,1,8,"00:08:22,050","00:08:22,890",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,of a coin toss.
cs-410_1_8_111,cs-410,1,8,"00:08:25,440","00:08:27,530",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,So we can think about the two cases.
cs-410_1_8_112,cs-410,1,8,"00:08:27,530","00:08:29,590",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,"One is a fair coin, it's completely fair."
cs-410_1_8_113,cs-410,1,8,"00:08:29,590","00:08:34,160",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,The coin shows up as head or
cs-410_1_8_114,cs-410,1,8,"00:08:34,160","00:08:39,160",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,So the two probabilities would be a half.
cs-410_1_8_115,cs-410,1,8,"00:08:39,160","00:08:42,890",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,Right?
cs-410_1_8_116,cs-410,1,8,"00:08:44,680","00:08:47,620",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,Another extreme case is
cs-410_1_8_117,cs-410,1,8,"00:08:47,620","00:08:50,420",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,where the coin always shows up as heads.
cs-410_1_8_118,cs-410,1,8,"00:08:50,420","00:08:52,760",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,So it's a completely biased coin.
cs-410_1_8_119,cs-410,1,8,"00:08:54,670","00:08:57,910",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,Now let's think about
cs-410_1_8_120,cs-410,1,8,"00:08:57,910","00:09:04,850",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,And if you plug in these values you can
cs-410_1_8_121,cs-410,1,8,"00:09:04,850","00:09:09,524",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,For a fair coin we see the entropy
cs-410_1_8_122,cs-410,1,8,"00:09:11,270","00:09:14,460",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,"For the completely biased coin,"
cs-410_1_8_123,cs-410,1,8,"00:09:14,460","00:09:17,360",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,And that intuitively makes a lot of sense.
cs-410_1_8_124,cs-410,1,8,"00:09:17,360","00:09:20,490",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,Because a fair coin is
cs-410_1_8_125,cs-410,1,8,"00:09:22,080","00:09:24,950",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,Whereas a completely biased
cs-410_1_8_126,cs-410,1,8,"00:09:24,950","00:09:26,860",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"We can always say, well, it's a head."
cs-410_1_8_127,cs-410,1,8,"00:09:26,860","00:09:29,190",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,Because it is a head all the time.
cs-410_1_8_128,cs-410,1,8,"00:09:29,190","00:09:34,400",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,So they can be shown on
cs-410_1_8_129,cs-410,1,8,"00:09:34,400","00:09:40,300",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,So the fair coin corresponds to the middle
cs-410_1_8_130,cs-410,1,8,"00:09:40,300","00:09:45,410",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,The completely biased coin
cs-410_1_8_131,cs-410,1,8,"00:09:45,410","00:09:48,058",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,point where we have a probability
cs-410_1_8_132,cs-410,1,8,"00:09:48,058","00:09:54,870",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,"So, now let's see how we can use"
cs-410_1_8_133,cs-410,1,8,"00:09:54,870","00:09:59,670",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,Let's think about our problem is
cs-410_1_8_134,cs-410,1,8,"00:09:59,670","00:10:01,650",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,absent in this segment.
cs-410_1_8_135,cs-410,1,8,"00:10:01,650","00:10:05,300",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,"Again, think about the three words,"
cs-410_1_8_136,cs-410,1,8,"00:10:06,540","00:10:10,130",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,Now we can assume high entropy
cs-410_1_8_137,cs-410,1,8,"00:10:11,910","00:10:18,790",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,And so we now have a quantitative way to
cs-410_1_8_138,cs-410,1,8,"00:10:20,890","00:10:25,810",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,"Now if you look at the three words meat,"
cs-410_1_8_139,cs-410,1,8,"00:10:25,810","00:10:33,310",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,we clearly would expect meat to have
cs-410_1_8_140,cs-410,1,8,"00:10:33,310","00:10:39,180",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,"In fact if you look at the entropy of the,"
cs-410_1_8_141,cs-410,1,8,"00:10:39,180","00:10:41,570",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,Because it occurs everywhere.
cs-410_1_8_142,cs-410,1,8,"00:10:41,570","00:10:43,390",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,So it's like a completely biased coin.
cs-410_1_8_143,cs-410,1,8,"00:10:44,610","00:10:46,380",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,Therefore the entropy is zero.
cs-410_1_8_144,cs-410,1,8,"00:10:48,710","00:10:58,710",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,[MUSIC]
cs-410_1_9_1,cs-410,1,9,"00:00:00,012","00:00:08,224",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_1_9_2,cs-410,1,9,"00:00:08,224","00:00:12,538",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=8,this is indeed a general idea of
cs-410_1_9_3,cs-410,1,9,"00:00:12,538","00:00:13,310",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,Algorithm.
cs-410_1_9_4,cs-410,1,9,"00:00:14,640","00:00:19,210",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,So in all the EM algorithms we
cs-410_1_9_5,cs-410,1,9,"00:00:19,210","00:00:21,970",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,to help us solve the problem more easily.
cs-410_1_9_6,cs-410,1,9,"00:00:21,970","00:00:25,453",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,In our case the hidden variable
cs-410_1_9_7,cs-410,1,9,"00:00:25,453","00:00:27,203",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,each occurrence of a word.
cs-410_1_9_8,cs-410,1,9,"00:00:27,203","00:00:32,020",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,And this binary variable would
cs-410_1_9_9,cs-410,1,9,"00:00:32,020","00:00:35,144",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,been generated from 0 sub d or 0 sub p.
cs-410_1_9_10,cs-410,1,9,"00:00:35,144","00:00:38,420",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,And here we show some possible
cs-410_1_9_11,cs-410,1,9,"00:00:38,420","00:00:43,470",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,"For example, for the it's from background,"
cs-410_1_9_12,cs-410,1,9,"00:00:43,470","00:00:45,105",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,And text on the other hand.
cs-410_1_9_13,cs-410,1,9,"00:00:45,105","00:00:52,040",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,Is from the topic then it's zero for
cs-410_1_9_14,cs-410,1,9,"00:00:53,260","00:00:58,915",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"Now, of course, we don't observe these z"
cs-410_1_9_15,cs-410,1,9,"00:00:58,915","00:01:01,875",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,Values of z attaching to other words.
cs-410_1_9_16,cs-410,1,9,"00:01:02,905","00:01:04,975",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,And that's why we call
cs-410_1_9_17,cs-410,1,9,"00:01:06,135","00:01:08,905",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"Now, the idea that we"
cs-410_1_9_18,cs-410,1,9,"00:01:08,905","00:01:12,930",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,predicting the word distribution that
cs-410_1_9_19,cs-410,1,9,"00:01:12,930","00:01:18,840",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"is it a predictor,"
cs-410_1_9_20,cs-410,1,9,"00:01:18,840","00:01:25,080",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"And, so, the EM algorithm then,"
cs-410_1_9_21,cs-410,1,9,"00:01:25,080","00:01:30,060",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,"First, we'll initialize all"
cs-410_1_9_22,cs-410,1,9,"00:01:30,060","00:01:34,960",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,"In our case,"
cs-410_1_9_23,cs-410,1,9,"00:01:34,960","00:01:37,840",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"of a word, given by theta sub d."
cs-410_1_9_24,cs-410,1,9,"00:01:37,840","00:01:39,680",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,So this is an initial addition stage.
cs-410_1_9_25,cs-410,1,9,"00:01:39,680","00:01:44,150",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,These initialized values would allow
cs-410_1_9_26,cs-410,1,9,"00:01:44,150","00:01:48,510",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,"of these z values, so"
cs-410_1_9_27,cs-410,1,9,"00:01:48,510","00:01:53,580",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,We can't say for sure whether
cs-410_1_9_28,cs-410,1,9,"00:01:53,580","00:01:55,090",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,But we can have our guess.
cs-410_1_9_29,cs-410,1,9,"00:01:55,090","00:01:57,620",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,This is given by this formula.
cs-410_1_9_30,cs-410,1,9,"00:01:57,620","00:01:59,710",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,It's called an E-step.
cs-410_1_9_31,cs-410,1,9,"00:01:59,710","00:02:06,520",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,And so the algorithm would then try to
cs-410_1_9_32,cs-410,1,9,"00:02:06,520","00:02:12,190",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,"After that, it would then invoke"
cs-410_1_9_33,cs-410,1,9,"00:02:12,190","00:02:17,490",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,In this step we simply take advantage
cs-410_1_9_34,cs-410,1,9,"00:02:17,490","00:02:22,825",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,then just group words that are in
cs-410_1_9_35,cs-410,1,9,"00:02:22,825","00:02:26,315",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,from that ground including this as well.
cs-410_1_9_36,cs-410,1,9,"00:02:27,585","00:02:32,865",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,We can then normalize the count
cs-410_1_9_37,cs-410,1,9,"00:02:32,865","00:02:35,479",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,to revise our estimate of the parameters.
cs-410_1_9_38,cs-410,1,9,"00:02:36,590","00:02:42,310",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,So let me also illustrate
cs-410_1_9_39,cs-410,1,9,"00:02:42,310","00:02:46,760",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,that are believed to have
cs-410_1_9_40,cs-410,1,9,"00:02:46,760","00:02:50,010",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,"that's text, mining algorithm,"
cs-410_1_9_41,cs-410,1,9,"00:02:51,760","00:02:55,718",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,And we group them together to help us
cs-410_1_9_42,cs-410,1,9,"00:02:55,718","00:03:01,170",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,re-estimate the parameters
cs-410_1_9_43,cs-410,1,9,"00:03:01,170","00:03:05,120",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,So these will help us
cs-410_1_9_44,cs-410,1,9,"00:03:06,170","00:03:09,970",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,Note that before we just set
cs-410_1_9_45,cs-410,1,9,"00:03:09,970","00:03:15,670",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,"But with this guess, we will have"
cs-410_1_9_46,cs-410,1,9,"00:03:15,670","00:03:18,740",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,"Of course, we don't know exactly"
cs-410_1_9_47,cs-410,1,9,"00:03:18,740","00:03:24,850",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,So we're not going to really
cs-410_1_9_48,cs-410,1,9,"00:03:24,850","00:03:26,800",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,But rather we're going to
cs-410_1_9_49,cs-410,1,9,"00:03:26,800","00:03:27,980",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,And this is what happened here.
cs-410_1_9_50,cs-410,1,9,"00:03:29,150","00:03:34,420",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,So we're going to adjust the count by
cs-410_1_9_51,cs-410,1,9,"00:03:34,420","00:03:38,410",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,this word has been generated
cs-410_1_9_52,cs-410,1,9,"00:03:39,840","00:03:42,580",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"And you can see this,"
cs-410_1_9_53,cs-410,1,9,"00:03:42,580","00:03:46,630",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,"Well, this has come from here, right?"
cs-410_1_9_54,cs-410,1,9,"00:03:46,630","00:03:48,120",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,From the E-step.
cs-410_1_9_55,cs-410,1,9,"00:03:48,120","00:03:52,472",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,So the EM Algorithm would
cs-410_1_9_56,cs-410,1,9,"00:03:52,472","00:03:57,375",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,estimate of parameters by using
cs-410_1_9_57,cs-410,1,9,"00:03:57,375","00:04:02,458",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,The E-step is to augment the data
cs-410_1_9_58,cs-410,1,9,"00:04:02,458","00:04:05,910",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,And the M-step is to take advantage
cs-410_1_9_59,cs-410,1,9,"00:04:05,910","00:04:08,660",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,of the additional information
cs-410_1_9_60,cs-410,1,9,"00:04:08,660","00:04:13,467",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,To split the data accounts and
cs-410_1_9_61,cs-410,1,9,"00:04:13,467","00:04:17,870",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,re-estimate our parameter.
cs-410_1_9_62,cs-410,1,9,"00:04:17,870","00:04:22,400",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,And then once we have a new generation of
cs-410_1_9_63,cs-410,1,9,"00:04:22,400","00:04:25,150",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,We are going the E-step again.
cs-410_1_9_64,cs-410,1,9,"00:04:25,150","00:04:28,520",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,To improve our estimate
cs-410_1_9_65,cs-410,1,9,"00:04:28,520","00:04:33,630",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,And then that would lead to another
cs-410_1_9_66,cs-410,1,9,"00:04:34,770","00:04:37,910",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,For the word distribution
cs-410_1_9_67,cs-410,1,9,"00:04:39,610","00:04:44,670",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"Okay, so, as I said,"
cs-410_1_9_68,cs-410,1,9,"00:04:44,670","00:04:50,380",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,"is really the variable z, hidden variable,"
cs-410_1_9_69,cs-410,1,9,"00:04:50,380","00:04:55,200",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,this water is from the top water
cs-410_1_9_70,cs-410,1,9,"00:04:56,810","00:05:00,780",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"So, this slide has a lot of content and"
cs-410_1_9_71,cs-410,1,9,"00:05:00,780","00:05:03,850",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,Pause the reader to digest it.
cs-410_1_9_72,cs-410,1,9,"00:05:03,850","00:05:07,300",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,But this basically captures
cs-410_1_9_73,cs-410,1,9,"00:05:07,300","00:05:12,500",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,Start with initial values that
cs-410_1_9_74,cs-410,1,9,"00:05:12,500","00:05:18,150",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,And then we invoke E-step followed
cs-410_1_9_75,cs-410,1,9,"00:05:18,150","00:05:19,690",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,setting of parameters.
cs-410_1_9_76,cs-410,1,9,"00:05:19,690","00:05:23,340",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,"And then we repeated this, so"
cs-410_1_9_77,cs-410,1,9,"00:05:23,340","00:05:27,060",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,that would gradually improve
cs-410_1_9_78,cs-410,1,9,"00:05:27,060","00:05:30,050",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,As I will explain later
cs-410_1_9_79,cs-410,1,9,"00:05:30,050","00:05:35,340",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,reaching a local maximum of
cs-410_1_9_80,cs-410,1,9,"00:05:35,340","00:05:40,180",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,So lets take a look at the computation for
cs-410_1_9_81,cs-410,1,9,"00:05:40,180","00:05:41,840",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,these formulas are the EM.
cs-410_1_9_82,cs-410,1,9,"00:05:41,840","00:05:48,220",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,"Formulas that you see before, and"
cs-410_1_9_83,cs-410,1,9,"00:05:48,220","00:05:53,720",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"here, like here, n,"
cs-410_1_9_84,cs-410,1,9,"00:05:53,720","00:05:56,040",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,Like here for example we have n plus one.
cs-410_1_9_85,cs-410,1,9,"00:05:56,040","00:05:59,728",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,That means we have improved.
cs-410_1_9_86,cs-410,1,9,"00:05:59,728","00:06:04,047",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,From here to here we have an improvement.
cs-410_1_9_87,cs-410,1,9,"00:06:04,047","00:06:08,106",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,So in this setting we have assumed the two
cs-410_1_9_88,cs-410,1,9,"00:06:08,106","00:06:09,689",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,the background model is null.
cs-410_1_9_89,cs-410,1,9,"00:06:09,689","00:06:11,872",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,So what are the relevance
cs-410_1_9_90,cs-410,1,9,"00:06:11,872","00:06:13,892",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,Well these are the word counts.
cs-410_1_9_91,cs-410,1,9,"00:06:13,892","00:06:18,290",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,"So assume we have just four words,"
cs-410_1_9_92,cs-410,1,9,"00:06:18,290","00:06:22,680",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,And this is our background model that
cs-410_1_9_93,cs-410,1,9,"00:06:22,680","00:06:23,380",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,words like the.
cs-410_1_9_94,cs-410,1,9,"00:06:25,910","00:06:29,860",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,"And in the first iteration,"
cs-410_1_9_95,cs-410,1,9,"00:06:29,860","00:06:32,280",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,Well first we initialize all the values.
cs-410_1_9_96,cs-410,1,9,"00:06:32,280","00:06:37,360",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"So here, this probability that we're"
cs-410_1_9_97,cs-410,1,9,"00:06:37,360","00:06:38,890",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,distribution of all the words.
cs-410_1_9_98,cs-410,1,9,"00:06:40,330","00:06:45,940",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,And then the E-step would give us a guess
cs-410_1_9_99,cs-410,1,9,"00:06:45,940","00:06:48,470",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,That will generate each word.
cs-410_1_9_100,cs-410,1,9,"00:06:48,470","00:06:51,450",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,We can see we have different
cs-410_1_9_101,cs-410,1,9,"00:06:51,450","00:06:52,430",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,Why?
cs-410_1_9_102,cs-410,1,9,"00:06:52,430","00:06:56,840",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"Well, that's because these words have"
cs-410_1_9_103,cs-410,1,9,"00:06:56,840","00:07:00,020",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,So even though the two
cs-410_1_9_104,cs-410,1,9,"00:07:00,020","00:07:05,320",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,And then our initial audition say uniform
cs-410_1_9_105,cs-410,1,9,"00:07:05,320","00:07:09,270",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"in the background of the distribution,"
cs-410_1_9_106,cs-410,1,9,"00:07:09,270","00:07:14,280",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,So these words are believed to
cs-410_1_9_107,cs-410,1,9,"00:07:15,820","00:07:17,930",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,These on the other hand are less likely.
cs-410_1_9_108,cs-410,1,9,"00:07:17,930","00:07:19,030",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,Probably from background.
cs-410_1_9_109,cs-410,1,9,"00:07:20,620","00:07:23,040",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,"So once we have these z values,"
cs-410_1_9_110,cs-410,1,9,"00:07:23,040","00:07:28,810",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,we know in the M-step these probabilities
cs-410_1_9_111,cs-410,1,9,"00:07:28,810","00:07:33,670",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,So four must be multiplied by this 0.33
cs-410_1_9_112,cs-410,1,9,"00:07:33,670","00:07:38,190",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,in order to get the allocated
cs-410_1_9_113,cs-410,1,9,"00:07:39,550","00:07:43,770",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,And this is done by this multiplication.
cs-410_1_9_114,cs-410,1,9,"00:07:43,770","00:07:49,700",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,Note that if our guess says this
cs-410_1_9_115,cs-410,1,9,"00:07:52,380","00:07:58,010",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,then we just get the full count
cs-410_1_9_116,cs-410,1,9,"00:07:58,010","00:08:01,200",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,In general it's not going
cs-410_1_9_117,cs-410,1,9,"00:08:01,200","00:08:06,760",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,So we're just going to get some percentage
cs-410_1_9_118,cs-410,1,9,"00:08:06,760","00:08:09,550",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,Then we simply normalize these counts
cs-410_1_9_119,cs-410,1,9,"00:08:09,550","00:08:13,170",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,to have a new generation
cs-410_1_9_120,cs-410,1,9,"00:08:13,170","00:08:16,600",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"So you can see, compare this with"
cs-410_1_9_121,cs-410,1,9,"00:08:18,330","00:08:23,060",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,So compare this with this one and
cs-410_1_9_122,cs-410,1,9,"00:08:23,060","00:08:25,930",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,"Not only that, we also see some"
cs-410_1_9_123,cs-410,1,9,"00:08:25,930","00:08:30,110",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,words that are believed to have come from
cs-410_1_9_124,cs-410,1,9,"00:08:30,110","00:08:31,400",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,"Like this one, text."
cs-410_1_9_125,cs-410,1,9,"00:08:32,530","00:08:35,930",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,"And of course, this new generation of"
cs-410_1_9_126,cs-410,1,9,"00:08:35,930","00:08:42,680",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,adjust the inferred latent variable or
cs-410_1_9_127,cs-410,1,9,"00:08:42,680","00:08:45,742",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,"So we have a new generation of values,"
cs-410_1_9_128,cs-410,1,9,"00:08:45,742","00:08:51,115",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,because of the E-step based on
cs-410_1_9_129,cs-410,1,9,"00:08:51,115","00:08:56,343",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,And these new inferred values
cs-410_1_9_130,cs-410,1,9,"00:08:56,343","00:09:03,166",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,another generation of the estimate
cs-410_1_9_131,cs-410,1,9,"00:09:03,166","00:09:07,990",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,And so on and so forth so this is what
cs-410_1_9_132,cs-410,1,9,"00:09:07,990","00:09:11,750",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,these probabilities
cs-410_1_9_133,cs-410,1,9,"00:09:11,750","00:09:16,745",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,As you can see in the last row
cs-410_1_9_134,cs-410,1,9,"00:09:16,745","00:09:20,985",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,and the likelihood is increasing
cs-410_1_9_135,cs-410,1,9,"00:09:20,985","00:09:25,875",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,And note that these log-likelihood is
cs-410_1_9_136,cs-410,1,9,"00:09:25,875","00:09:30,070",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,"between 0 and 1 when you take a logarithm,"
cs-410_1_9_137,cs-410,1,9,"00:09:30,070","00:09:33,180",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,"Now what's also interesting is,"
cs-410_1_9_138,cs-410,1,9,"00:09:33,180","00:09:36,600",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,And these are the inverted word split.
cs-410_1_9_139,cs-410,1,9,"00:09:36,600","00:09:42,150",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,And these are the probabilities
cs-410_1_9_140,cs-410,1,9,"00:09:42,150","00:09:47,980",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,"have come from one distribution, in this"
cs-410_1_9_141,cs-410,1,9,"00:09:47,980","00:09:50,580",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,And you might wonder whether
cs-410_1_9_142,cs-410,1,9,"00:09:50,580","00:09:55,540",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,Because our main goal is to
cs-410_1_9_143,cs-410,1,9,"00:09:55,540","00:09:57,400",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,So this is our primary goal.
cs-410_1_9_144,cs-410,1,9,"00:09:57,400","00:10:00,900",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,We hope to have a more discriminative
cs-410_1_9_145,cs-410,1,9,"00:10:00,900","00:10:04,400",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,But the last column is also bi-product.
cs-410_1_9_146,cs-410,1,9,"00:10:04,400","00:10:07,170",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,This also can actually be very useful.
cs-410_1_9_147,cs-410,1,9,"00:10:07,170","00:10:08,380",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,You can think about that.
cs-410_1_9_148,cs-410,1,9,"00:10:08,380","00:10:10,220",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"We want to use, is to for"
cs-410_1_9_149,cs-410,1,9,"00:10:10,220","00:10:16,080",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,example is to estimate to what extent this
cs-410_1_9_150,cs-410,1,9,"00:10:16,080","00:10:18,165",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,"And this, when we add this up or"
cs-410_1_9_151,cs-410,1,9,"00:10:18,165","00:10:23,304",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,take the average we will kind of know to
cs-410_1_9_152,cs-410,1,9,"00:10:23,304","00:10:27,823",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,versus content was that are not
cs-410_1_9_153,cs-410,1,9,"00:10:27,823","00:10:37,823",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,[MUSIC]
cs-410_2_1_1,cs-410,2,1,"00:00:00,012","00:00:09,434",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_2_1_2,cs-410,2,1,"00:00:09,434","00:00:12,223",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,"In this lecture,"
cs-410_2_1_3,cs-410,2,1,"00:00:14,279","00:00:18,349",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"In the previous lecture, we talked about"
cs-410_2_1_4,cs-410,2,1,"00:00:19,360","00:00:23,970",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,We explained that the state of the are
cs-410_2_1_5,cs-410,2,1,"00:00:23,970","00:00:28,970",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,are still not good enough to process
cs-410_2_1_6,cs-410,2,1,"00:00:28,970","00:00:30,550",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,in a robust manner.
cs-410_2_1_7,cs-410,2,1,"00:00:30,550","00:00:31,360",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"As a result,"
cs-410_2_1_8,cs-410,2,1,"00:00:31,360","00:00:37,250",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,bag of words remains very popular in
cs-410_2_1_9,cs-410,2,1,"00:00:39,140","00:00:44,100",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"In this lecture, we're going to talk"
cs-410_2_1_10,cs-410,2,1,"00:00:44,100","00:00:48,120",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,help users get access to the text data.
cs-410_2_1_11,cs-410,2,1,"00:00:48,120","00:00:55,000",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,This is also important step to convert
cs-410_2_1_12,cs-410,2,1,"00:00:55,000","00:00:57,610",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,That are actually needed
cs-410_2_1_13,cs-410,2,1,"00:00:57,610","00:01:02,510",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,"So the main question we'll address here,"
cs-410_2_1_14,cs-410,2,1,"00:01:02,510","00:01:07,450",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,"can a text information system, help users"
cs-410_2_1_15,cs-410,2,1,"00:01:07,450","00:01:11,550",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,We're going to cover two complimentary
cs-410_2_1_16,cs-410,2,1,"00:01:12,610","00:01:17,700",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,And then we're going to talk about
cs-410_2_1_17,cs-410,2,1,"00:01:17,700","00:01:19,080",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,querying versus browsing.
cs-410_2_1_18,cs-410,2,1,"00:01:20,770","00:01:22,860",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,So first push versus pull.
cs-410_2_1_19,cs-410,2,1,"00:01:24,500","00:01:29,250",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,These are two different ways connect
cs-410_2_1_20,cs-410,2,1,"00:01:29,250","00:01:29,900",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,at the right time.
cs-410_2_1_21,cs-410,2,1,"00:01:31,190","00:01:35,900",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,The difference is which
cs-410_2_1_22,cs-410,2,1,"00:01:37,230","00:01:38,740",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,which party takes the initiative.
cs-410_2_1_23,cs-410,2,1,"00:01:40,290","00:01:41,380",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"In the pull mode,"
cs-410_2_1_24,cs-410,2,1,"00:01:41,380","00:01:46,439",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,the users take the initiative to
cs-410_2_1_25,cs-410,2,1,"00:01:47,700","00:01:53,420",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"And in this case, a user typically would"
cs-410_2_1_26,cs-410,2,1,"00:01:53,420","00:01:56,100",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,"For example,"
cs-410_2_1_27,cs-410,2,1,"00:01:56,100","00:02:01,640",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,then browse the results to
cs-410_2_1_28,cs-410,2,1,"00:02:02,790","00:02:06,280",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,So this is usually appropriate for
cs-410_2_1_29,cs-410,2,1,"00:02:06,280","00:02:09,340",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,satisfying a user's ad
cs-410_2_1_30,cs-410,2,1,"00:02:10,580","00:02:14,280",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,An ad hoc information need is
cs-410_2_1_31,cs-410,2,1,"00:02:14,280","00:02:17,870",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,"For example, you want to buy a product so"
cs-410_2_1_32,cs-410,2,1,"00:02:17,870","00:02:22,550",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,you suddenly have a need to read
cs-410_2_1_33,cs-410,2,1,"00:02:22,550","00:02:26,620",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"But after you have cracked information,"
cs-410_2_1_34,cs-410,2,1,"00:02:26,620","00:02:28,880",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,You generally no longer
cs-410_2_1_35,cs-410,2,1,"00:02:28,880","00:02:30,200",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,it's a temporary information need.
cs-410_2_1_36,cs-410,2,1,"00:02:31,360","00:02:35,230",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,"In such a case, it's very hard for"
cs-410_2_1_37,cs-410,2,1,"00:02:35,230","00:02:39,480",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,it's more proper for
cs-410_2_1_38,cs-410,2,1,"00:02:39,480","00:02:42,260",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,that's why search engines are very useful.
cs-410_2_1_39,cs-410,2,1,"00:02:42,260","00:02:48,370",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,Today because many people have many
cs-410_2_1_40,cs-410,2,1,"00:02:48,370","00:02:52,620",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,So as we're speaking Google is probably
cs-410_2_1_41,cs-410,2,1,"00:02:52,620","00:02:55,720",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"And those are all, or mostly adequate."
cs-410_2_1_42,cs-410,2,1,"00:02:55,720","00:02:56,590",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,Information needs.
cs-410_2_1_43,cs-410,2,1,"00:02:57,950","00:02:59,680",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,So this is a pull mode.
cs-410_2_1_44,cs-410,2,1,"00:02:59,680","00:03:03,570",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,In contrast in the push mode in
cs-410_2_1_45,cs-410,2,1,"00:03:03,570","00:03:07,510",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,to push the information to the user or
cs-410_2_1_46,cs-410,2,1,"00:03:07,510","00:03:11,090",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,So in this case this is usually
cs-410_2_1_47,cs-410,2,1,"00:03:13,100","00:03:15,190",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,Now this would be appropriate if.
cs-410_2_1_48,cs-410,2,1,"00:03:15,190","00:03:16,900",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,The user has a stable information.
cs-410_2_1_49,cs-410,2,1,"00:03:17,900","00:03:22,040",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,For example you may have a research
cs-410_2_1_50,cs-410,2,1,"00:03:22,040","00:03:24,980",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,that interest tends to stay for a while.
cs-410_2_1_51,cs-410,2,1,"00:03:24,980","00:03:26,930",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,"So, it's rather stable."
cs-410_2_1_52,cs-410,2,1,"00:03:26,930","00:03:29,240",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,Your hobby is another example of.
cs-410_2_1_53,cs-410,2,1,"00:03:29,240","00:03:34,100",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,A stable information need is such a case
cs-410_2_1_54,cs-410,2,1,"00:03:34,100","00:03:38,860",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"can learn your interest, and"
cs-410_2_1_55,cs-410,2,1,"00:03:38,860","00:03:43,710",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,If the system hasn't seen any
cs-410_2_1_56,cs-410,2,1,"00:03:43,710","00:03:47,900",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,the system could then take the initiative
cs-410_2_1_57,cs-410,2,1,"00:03:47,900","00:03:49,940",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"So, for example, a news filter or"
cs-410_2_1_58,cs-410,2,1,"00:03:49,940","00:03:53,020",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,news recommended system could
cs-410_2_1_59,cs-410,2,1,"00:03:53,020","00:03:56,870",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,identify interesting news to you and
cs-410_2_1_60,cs-410,2,1,"00:03:59,130","00:04:03,960",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,This mode of information access may be
cs-410_2_1_61,cs-410,2,1,"00:04:03,960","00:04:08,790",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,has good knowledge about the users need
cs-410_2_1_62,cs-410,2,1,"00:04:08,790","00:04:11,850",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,"So for example, when you search for"
cs-410_2_1_63,cs-410,2,1,"00:04:11,850","00:04:16,130",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,a search engine might infer you might be
cs-410_2_1_64,cs-410,2,1,"00:04:16,130","00:04:17,530",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,Formation.
cs-410_2_1_65,cs-410,2,1,"00:04:17,530","00:04:20,950",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,And they would recommend the information
cs-410_2_1_66,cs-410,2,1,"00:04:20,950","00:04:24,780",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,"example, of an advertisement"
cs-410_2_1_67,cs-410,2,1,"00:04:27,790","00:04:34,540",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,So this is about the two high level
cs-410_2_1_68,cs-410,2,1,"00:04:35,720","00:04:38,440",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,Now let's look at the pull
cs-410_2_1_69,cs-410,2,1,"00:04:39,900","00:04:43,740",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"In the pull mode, we can further"
cs-410_2_1_70,cs-410,2,1,"00:04:43,740","00:04:46,010",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,Querying versus browsing.
cs-410_2_1_71,cs-410,2,1,"00:04:46,010","00:04:48,790",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,"In querying,"
cs-410_2_1_72,cs-410,2,1,"00:04:48,790","00:04:50,560",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,"Typical the keyword query, and"
cs-410_2_1_73,cs-410,2,1,"00:04:50,560","00:04:53,430",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,the search engine system would
cs-410_2_1_74,cs-410,2,1,"00:04:54,500","00:05:00,730",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,And this works well when the user knows
cs-410_2_1_75,cs-410,2,1,"00:05:00,730","00:05:02,450",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,So if you know exactly
cs-410_2_1_76,cs-410,2,1,"00:05:02,450","00:05:04,540",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,you tend to know the right keywords.
cs-410_2_1_77,cs-410,2,1,"00:05:04,540","00:05:07,880",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"And then query works very well,"
cs-410_2_1_78,cs-410,2,1,"00:05:09,290","00:05:12,740",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,But we also know that sometimes
cs-410_2_1_79,cs-410,2,1,"00:05:12,740","00:05:16,970",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,When you don't know the right
cs-410_2_1_80,cs-410,2,1,"00:05:16,970","00:05:21,760",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,you want to browse information
cs-410_2_1_81,cs-410,2,1,"00:05:21,760","00:05:24,780",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,You use because browsing
cs-410_2_1_82,cs-410,2,1,"00:05:24,780","00:05:29,890",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,"So in this case, in the case of browsing,"
cs-410_2_1_83,cs-410,2,1,"00:05:29,890","00:05:33,330",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,into the relevant information
cs-410_2_1_84,cs-410,2,1,"00:05:34,740","00:05:39,850",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,supported by the structures of documents.
cs-410_2_1_85,cs-410,2,1,"00:05:39,850","00:05:42,690",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,So the system would maintain
cs-410_2_1_86,cs-410,2,1,"00:05:42,690","00:05:45,190",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,then the user could follow
cs-410_2_1_87,cs-410,2,1,"00:05:47,370","00:05:53,850",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,So this really works well when the user
cs-410_2_1_88,cs-410,2,1,"00:05:53,850","00:05:59,750",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,or the user doesn't know what
cs-410_2_1_89,cs-410,2,1,"00:05:59,750","00:06:05,070",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,Or simply because the user finds it
cs-410_2_1_90,cs-410,2,1,"00:06:05,070","00:06:10,450",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,So even if a user knows what query to
cs-410_2_1_91,cs-410,2,1,"00:06:10,450","00:06:12,370",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,to search for information.
cs-410_2_1_92,cs-410,2,1,"00:06:12,370","00:06:14,760",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,It's still harder to enter the query.
cs-410_2_1_93,cs-410,2,1,"00:06:14,760","00:06:18,840",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"In such a case, again,"
cs-410_2_1_94,cs-410,2,1,"00:06:18,840","00:06:23,060",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,The relationship between browsing and
cs-410_2_1_95,cs-410,2,1,"00:06:23,060","00:06:24,130",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,imagine you're site seeing.
cs-410_2_1_96,cs-410,2,1,"00:06:25,230","00:06:27,080",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,Imagine if you're touring a city.
cs-410_2_1_97,cs-410,2,1,"00:06:27,080","00:06:29,800",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,Now if you know the exact
cs-410_2_1_98,cs-410,2,1,"00:06:31,670","00:06:34,900",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,Taking a taxi there is
cs-410_2_1_99,cs-410,2,1,"00:06:34,900","00:06:36,860",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,You can go directly to the site.
cs-410_2_1_100,cs-410,2,1,"00:06:36,860","00:06:40,440",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"But if you don't know the exact address,"
cs-410_2_1_101,cs-410,2,1,"00:06:40,440","00:06:43,579",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,Or you can take a taxi to a nearby
cs-410_2_1_102,cs-410,2,1,"00:06:44,670","00:06:48,160",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,It turns out that we do exactly
cs-410_2_1_103,cs-410,2,1,"00:06:48,160","00:06:51,480",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,If you know exactly what you
cs-410_2_1_104,cs-410,2,1,"00:06:51,480","00:06:55,360",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,use the right keywords in your query
cs-410_2_1_105,cs-410,2,1,"00:06:55,360","00:06:58,150",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,"That's usually the fastest way to do,"
cs-410_2_1_106,cs-410,2,1,"00:06:59,480","00:07:02,180",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,But what if you don't know
cs-410_2_1_107,cs-410,2,1,"00:07:02,180","00:07:04,369",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,"Well, you clearly probably won't so well."
cs-410_2_1_108,cs-410,2,1,"00:07:04,369","00:07:06,150",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,You will not related pages.
cs-410_2_1_109,cs-410,2,1,"00:07:06,150","00:07:10,160",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,"And then, you need to also walk"
cs-410_2_1_110,cs-410,2,1,"00:07:10,160","00:07:14,110",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,meaning by following the links or
cs-410_2_1_111,cs-410,2,1,"00:07:14,110","00:07:16,430",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,You can then finally get
cs-410_2_1_112,cs-410,2,1,"00:07:17,580","00:07:20,720",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,If you want to learn about again.
cs-410_2_1_113,cs-410,2,1,"00:07:20,720","00:07:24,610",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,You will likely do a lot of browsing so
cs-410_2_1_114,cs-410,2,1,"00:07:24,610","00:07:29,914",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,just like you are looking around in
cs-410_2_1_115,cs-410,2,1,"00:07:29,914","00:07:36,405",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,interesting attractions
cs-410_2_1_116,cs-410,2,1,"00:07:36,405","00:07:39,200",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,[INAUDIBLE].
cs-410_2_1_117,cs-410,2,1,"00:07:39,200","00:07:45,330",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,So this analogy also tells us that
cs-410_2_1_118,cs-410,2,1,"00:07:45,330","00:07:50,600",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"query, but we don't really have"
cs-410_2_1_119,cs-410,2,1,"00:07:50,600","00:07:54,470",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,And this is because in order
cs-410_2_1_120,cs-410,2,1,"00:07:54,470","00:07:57,840",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,"we need a map to guide us,"
cs-410_2_1_121,cs-410,2,1,"00:07:57,840","00:07:58,410",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,"Of Chicago,"
cs-410_2_1_122,cs-410,2,1,"00:07:58,410","00:08:04,060",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,"through the city of Chicago, you need a"
cs-410_2_1_123,cs-410,2,1,"00:08:04,060","00:08:08,190",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,So how to construct such a topical
cs-410_2_1_124,cs-410,2,1,"00:08:08,190","00:08:12,730",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,research question that might bring us
cs-410_2_1_125,cs-410,2,1,"00:08:12,730","00:08:16,950",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,more interesting browsing experience
cs-410_2_1_126,cs-410,2,1,"00:08:19,170","00:08:21,280",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,"So, to summarize this lecture,"
cs-410_2_1_127,cs-410,2,1,"00:08:21,280","00:08:26,550",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,we've talked about the two high level
cs-410_2_1_128,cs-410,2,1,"00:08:26,550","00:08:29,130",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,Push tends to be supported by
cs-410_2_1_129,cs-410,2,1,"00:08:29,130","00:08:31,770",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,Pull tends to be supported
cs-410_2_1_130,cs-410,2,1,"00:08:31,770","00:08:35,710",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"Of course, in the sophisticated"
cs-410_2_1_131,cs-410,2,1,"00:08:35,710","00:08:36,780",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,we should combine the two.
cs-410_2_1_132,cs-410,2,1,"00:08:38,590","00:08:41,830",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,"In the pull mode, we can further this"
cs-410_2_1_133,cs-410,2,1,"00:08:41,830","00:08:47,140",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,Again we generally want to combine
cs-410_2_1_134,cs-410,2,1,"00:08:47,140","00:08:50,080",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,so that you can support
cs-410_2_1_135,cs-410,2,1,"00:08:51,220","00:08:55,420",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,If you want to know more about
cs-410_2_1_136,cs-410,2,1,"00:08:55,420","00:08:58,600",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,"push, you can read this article."
cs-410_2_1_137,cs-410,2,1,"00:08:58,600","00:09:03,560",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,This give excellent discussion of the
cs-410_2_1_138,cs-410,2,1,"00:09:03,560","00:09:05,330",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,information retrieval.
cs-410_2_1_139,cs-410,2,1,"00:09:05,330","00:09:10,271",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,Here informational filtering is similar
cs-410_2_1_140,cs-410,2,1,"00:09:10,271","00:09:12,749",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,the push mode of information access.
cs-410_2_1_141,cs-410,2,1,"00:09:12,749","00:09:22,749",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,[MUSIC]
cs-410_2_10_1,cs-410,2,10,"00:00:00,069","00:00:07,429",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_2_10_2,cs-410,2,10,"00:00:07,429","00:00:11,820",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,is a continuing discussion of Generative
cs-410_2_10_3,cs-410,2,10,"00:00:13,450","00:00:17,620",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In this lecture, we are going to continue"
cs-410_2_10_4,cs-410,2,10,"00:00:17,620","00:00:20,910",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,"particularly, the Generative"
cs-410_2_10_5,cs-410,2,10,"00:00:23,950","00:00:28,320",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,So this is a slide that you have seen
cs-410_2_10_6,cs-410,2,10,"00:00:28,320","00:00:32,735",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,the likelihood function for
cs-410_2_10_7,cs-410,2,10,"00:00:32,735","00:00:38,049",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"distributions, being a two component"
cs-410_2_10_8,cs-410,2,10,"00:00:39,800","00:00:47,360",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"Now in this lecture, we're going to"
cs-410_2_10_9,cs-410,2,10,"00:00:47,360","00:00:51,670",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,Now if you look at the formula and think
cs-410_2_10_10,cs-410,2,10,"00:00:51,670","00:00:56,860",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,you'll realize that all we need is to add
cs-410_2_10_11,cs-410,2,10,"00:00:57,960","00:01:04,020",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,So you can just add more thetas and
cs-410_2_10_12,cs-410,2,10,"00:01:04,020","00:01:08,890",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,thetas and the probabilities of
cs-410_2_10_13,cs-410,2,10,"00:01:08,890","00:01:13,200",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,So this is precisely what we
cs-410_2_10_14,cs-410,2,10,"00:01:13,200","00:01:17,860",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,the general presentation of the mixture
cs-410_2_10_15,cs-410,2,10,"00:01:19,810","00:01:24,820",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,So as more cases would follow these
cs-410_2_10_16,cs-410,2,10,"00:01:24,820","00:01:27,430",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,think about our data.
cs-410_2_10_17,cs-410,2,10,"00:01:27,430","00:01:30,360",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,And so in this case our data
cs-410_2_10_18,cs-410,2,10,"00:01:30,360","00:01:33,740",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,"end documents denoted by d sub i,"
cs-410_2_10_19,cs-410,2,10,"00:01:33,740","00:01:37,310",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"and then we talk about the other models,"
cs-410_2_10_20,cs-410,2,10,"00:01:37,310","00:01:41,410",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"In this case, we design a mixture"
cs-410_2_10_21,cs-410,2,10,"00:01:41,410","00:01:48,280",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,It's a little bit different from the topic
cs-410_2_10_22,cs-410,2,10,"00:01:48,280","00:01:52,396",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,We have a set of theta i's that
cs-410_2_10_23,cs-410,2,10,"00:01:52,396","00:01:55,810",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,corresponding to the k
cs-410_2_10_24,cs-410,2,10,"00:01:55,810","00:02:01,260",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,We have p of each theta i as
cs-410_2_10_25,cs-410,2,10,"00:02:01,260","00:02:05,463",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,each of the k distributions
cs-410_2_10_26,cs-410,2,10,"00:02:05,463","00:02:11,090",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,Now note that although our goal
cs-410_2_10_27,cs-410,2,10,"00:02:11,090","00:02:16,450",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,we actually have used a more general
cs-410_2_10_28,cs-410,2,10,"00:02:16,450","00:02:19,560",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,"cluster and this as you will see later,"
cs-410_2_10_29,cs-410,2,10,"00:02:19,560","00:02:25,610",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,will allow us to assign
cs-410_2_10_30,cs-410,2,10,"00:02:25,610","00:02:29,510",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,that has the highest probability of
cs-410_2_10_31,cs-410,2,10,"00:02:31,070","00:02:35,530",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,"So as a result,"
cs-410_2_10_32,cs-410,2,10,"00:02:36,880","00:02:40,520",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,"properties, as you will see later."
cs-410_2_10_33,cs-410,2,10,"00:02:42,390","00:02:46,010",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,So the model basically would make
cs-410_2_10_34,cs-410,2,10,"00:02:46,010","00:02:47,370",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,the generation of a document.
cs-410_2_10_35,cs-410,2,10,"00:02:47,370","00:02:51,130",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,We first choose a theta i according
cs-410_2_10_36,cs-410,2,10,"00:02:51,130","00:02:55,740",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,then generate all the words in
cs-410_2_10_37,cs-410,2,10,"00:02:55,740","00:02:58,500",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,Note that it's important that we
cs-410_2_10_38,cs-410,2,10,"00:02:58,500","00:03:02,030",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,use this distribution all
cs-410_2_10_39,cs-410,2,10,"00:03:02,030","00:03:04,770",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,This is very different from topic model.
cs-410_2_10_40,cs-410,2,10,"00:03:04,770","00:03:08,100",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,So the likelihood function would
cs-410_2_10_41,cs-410,2,10,"00:03:10,060","00:03:16,620",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,So you can take a look
cs-410_2_10_42,cs-410,2,10,"00:03:16,620","00:03:22,244",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,we have used the different notation
cs-410_2_10_43,cs-410,2,10,"00:03:22,244","00:03:28,810",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,here in the second line of this equation.
cs-410_2_10_44,cs-410,2,10,"00:03:28,810","00:03:33,837",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,You are going to see now
cs-410_2_10_45,cs-410,2,10,"00:03:33,837","00:03:39,102",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,"to use unique word in the vocabulary,"
cs-410_2_10_46,cs-410,2,10,"00:03:39,102","00:03:45,130",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,instead of particular
cs-410_2_10_47,cs-410,2,10,"00:03:45,130","00:03:50,750",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,"So from X subject to W,"
cs-410_2_10_48,cs-410,2,10,"00:03:50,750","00:03:58,580",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,this change allows us to show
cs-410_2_10_49,cs-410,2,10,"00:03:58,580","00:04:03,227",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,And you have seen this change also
cs-410_2_10_50,cs-410,2,10,"00:04:03,227","00:04:08,191",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,it's basically still just a product of
cs-410_2_10_51,cs-410,2,10,"00:04:10,010","00:04:10,900",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,And so
cs-410_2_10_52,cs-410,2,10,"00:04:10,900","00:04:15,100",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,"with the likelihood function, now we can"
cs-410_2_10_53,cs-410,2,10,"00:04:15,100","00:04:19,090",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,Here we can simply use
cs-410_2_10_54,cs-410,2,10,"00:04:19,090","00:04:22,960",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,So that's just a standard
cs-410_2_10_55,cs-410,2,10,"00:04:22,960","00:04:25,880",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,So all should be familiar to you now.
cs-410_2_10_56,cs-410,2,10,"00:04:25,880","00:04:27,890",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,It's just a different model.
cs-410_2_10_57,cs-410,2,10,"00:04:27,890","00:04:30,390",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,"So after we have estimated parameters,"
cs-410_2_10_58,cs-410,2,10,"00:04:30,390","00:04:34,060",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,how can we then allocate
cs-410_2_10_59,cs-410,2,10,"00:04:34,060","00:04:37,740",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,"Well, let's take a look at"
cs-410_2_10_60,cs-410,2,10,"00:04:37,740","00:04:41,850",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,So we just repeated the parameters
cs-410_2_10_61,cs-410,2,10,"00:04:43,030","00:04:47,230",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,Now if you think about what we can
cs-410_2_10_62,cs-410,2,10,"00:04:47,230","00:04:52,640",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,we can actually get more information than
cs-410_2_10_63,cs-410,2,10,"00:04:52,640","00:04:57,008",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,"So theta i for example,"
cs-410_2_10_64,cs-410,2,10,"00:04:57,008","00:05:02,770",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,"this is actually a by-product, it can help"
cs-410_2_10_65,cs-410,2,10,"00:05:02,770","00:05:06,020",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,If you look at the top
cs-410_2_10_66,cs-410,2,10,"00:05:06,020","00:05:09,740",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,in this word distribution and they will
cs-410_2_10_67,cs-410,2,10,"00:05:11,130","00:05:16,010",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,p of theta i can be interpreted as
cs-410_2_10_68,cs-410,2,10,"00:05:16,010","00:05:21,310",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,tells us how likely the cluster would
cs-410_2_10_69,cs-410,2,10,"00:05:21,310","00:05:24,750",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,The more likely a cluster is
cs-410_2_10_70,cs-410,2,10,"00:05:24,750","00:05:28,240",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,we can assume the larger
cs-410_2_10_71,cs-410,2,10,"00:05:30,280","00:05:32,880",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,Note that unlike in PLSA and
cs-410_2_10_72,cs-410,2,10,"00:05:32,880","00:05:36,640",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,this probability of theta
cs-410_2_10_73,cs-410,2,10,"00:05:37,640","00:05:41,520",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,Now you may recall that the topic
cs-410_2_10_74,cs-410,2,10,"00:05:41,520","00:05:42,750",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,actually depends on d.
cs-410_2_10_75,cs-410,2,10,"00:05:42,750","00:05:48,720",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,That means each document can have
cs-410_2_10_76,cs-410,2,10,"00:05:48,720","00:05:54,260",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,but here we have a generic choice
cs-410_2_10_77,cs-410,2,10,"00:05:54,260","00:05:58,950",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,"But of course, even a particular document"
cs-410_2_10_78,cs-410,2,10,"00:05:58,950","00:06:01,840",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,topic is more likely to
cs-410_2_10_79,cs-410,2,10,"00:06:01,840","00:06:02,770",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,"So in that sense,"
cs-410_2_10_80,cs-410,2,10,"00:06:02,770","00:06:08,890",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,we can still have a document
cs-410_2_10_81,cs-410,2,10,"00:06:10,020","00:06:14,890",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,So now let's look at the key problem
cs-410_2_10_82,cs-410,2,10,"00:06:14,890","00:06:16,320",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,assigning clusters to documents.
cs-410_2_10_83,cs-410,2,10,"00:06:17,940","00:06:22,587",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,So that's the computer c sub d here and
cs-410_2_10_84,cs-410,2,10,"00:06:22,587","00:06:27,560",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,the range of one through k to indicate
cs-410_2_10_85,cs-410,2,10,"00:06:28,690","00:06:32,985",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,Now first you might think about
cs-410_2_10_86,cs-410,2,10,"00:06:32,985","00:06:37,939",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,that is to assign d to the cluster
cs-410_2_10_87,cs-410,2,10,"00:06:37,939","00:06:41,090",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,that most likely has
cs-410_2_10_88,cs-410,2,10,"00:06:42,450","00:06:46,530",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So that means we're going to choose
cs-410_2_10_89,cs-410,2,10,"00:06:46,530","00:06:49,500",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,gives d the highest probability.
cs-410_2_10_90,cs-410,2,10,"00:06:49,500","00:06:50,734",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,"In other words,"
cs-410_2_10_91,cs-410,2,10,"00:06:50,734","00:06:56,580",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,we see which distribution has the content
cs-410_2_10_92,cs-410,2,10,"00:06:56,580","00:07:01,870",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,"Intuitively that makes sense,"
cs-410_2_10_93,cs-410,2,10,"00:07:01,870","00:07:06,980",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,"does not consider the size of clusters,"
cs-410_2_10_94,cs-410,2,10,"00:07:06,980","00:07:12,140",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,so a better way is to use
cs-410_2_10_95,cs-410,2,10,"00:07:12,140","00:07:16,038",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,in this case the prior is p of theta i.
cs-410_2_10_96,cs-410,2,10,"00:07:16,038","00:07:20,880",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,"And together, that is, we're going to"
cs-410_2_10_97,cs-410,2,10,"00:07:20,880","00:07:24,230",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,"the posterior probability of theta,"
cs-410_2_10_98,cs-410,2,10,"00:07:25,650","00:07:30,058",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,And if we choose theta .based
cs-410_2_10_99,cs-410,2,10,"00:07:30,058","00:07:36,010",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,we would have the following formula that
cs-410_2_10_100,cs-410,2,10,"00:07:36,010","00:07:42,390",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,"And in this case, we're going to choose"
cs-410_2_10_101,cs-410,2,10,"00:07:42,390","00:07:47,610",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,that means a large cluster and
cs-410_2_10_102,cs-410,2,10,"00:07:47,610","00:07:51,690",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,So we're going to favor
cs-410_2_10_103,cs-410,2,10,"00:07:51,690","00:07:54,982",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,also consistent with the document.
cs-410_2_10_104,cs-410,2,10,"00:07:54,982","00:08:01,090",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,And that intuitively makes
cs-410_2_10_105,cs-410,2,10,"00:08:01,090","00:08:05,720",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,a document being a large cluster is
cs-410_2_10_106,cs-410,2,10,"00:08:07,640","00:08:13,000",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So this means once we can estimate
cs-410_2_10_107,cs-410,2,10,"00:08:13,000","00:08:16,930",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,then we can easily solve
cs-410_2_10_108,cs-410,2,10,"00:08:16,930","00:08:20,850",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"So next, we'll have to discuss how to"
cs-410_2_10_109,cs-410,2,10,"00:08:20,850","00:08:25,512",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,actually compute
cs-410_2_10_110,cs-410,2,10,"00:08:25,512","00:08:35,512",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,[MUSIC]
cs-410_2_11_1,cs-410,2,11,"00:00:07,290","00:00:11,068",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,[SOUND] This lecture is
cs-410_2_11_2,cs-410,2,11,"00:00:11,068","00:00:15,610",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,Discriminative Classifiers for
cs-410_2_11_3,cs-410,2,11,"00:00:15,610","00:00:18,096",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,"So, in this lecture,"
cs-410_2_11_4,cs-410,2,11,"00:00:18,096","00:00:22,450",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,another Discriminative Classifier called
cs-410_2_11_5,cs-410,2,11,"00:00:22,450","00:00:25,050",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,Which is a very popular
cs-410_2_11_6,cs-410,2,11,"00:00:25,050","00:00:28,790",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,it has been also shown to be effective for
cs-410_2_11_7,cs-410,2,11,"00:00:31,350","00:00:34,380",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,"So to introduce this classifier,"
cs-410_2_11_8,cs-410,2,11,"00:00:34,380","00:00:38,060",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,let's also think about the simple
cs-410_2_11_9,cs-410,2,11,"00:00:38,060","00:00:43,300",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,"We have two topic categories,"
cs-410_2_11_10,cs-410,2,11,"00:00:43,300","00:00:47,760",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,And we want to classify documents
cs-410_2_11_11,cs-410,2,11,"00:00:47,760","00:00:51,820",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,we're going to represent again
cs-410_2_11_12,cs-410,2,11,"00:00:53,200","00:00:58,020",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"Now, the idea of this classifier is"
cs-410_2_11_13,cs-410,2,11,"00:00:59,150","00:01:01,360",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,here that you'll see and
cs-410_2_11_14,cs-410,2,11,"00:01:01,360","00:01:05,820",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,it's very similar to what you have
cs-410_2_11_15,cs-410,2,11,"00:01:05,820","00:01:11,240",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,And we're going to do also say
cs-410_2_11_16,cs-410,2,11,"00:01:11,240","00:01:16,690",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,value is positive then we're going to
cs-410_2_11_17,cs-410,2,11,"00:01:16,690","00:01:20,470",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,"Otherwise, we're going to"
cs-410_2_11_18,cs-410,2,11,"00:01:20,470","00:01:27,700",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,So that makes 0 that is the decision
cs-410_2_11_19,cs-410,2,11,"00:01:28,830","00:01:33,990",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"So, in generally hiding"
cs-410_2_11_20,cs-410,2,11,"00:01:33,990","00:01:37,070",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,corresponds to a hyper plain.
cs-410_2_11_21,cs-410,2,11,"00:01:38,210","00:01:43,180",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,Now I've shown you a simple case of two
cs-410_2_11_22,cs-410,2,11,"00:01:43,180","00:01:49,910",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,X2 and this case this corresponds
cs-410_2_11_23,cs-410,2,11,"00:01:51,220","00:01:55,980",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,"So, this is a line defined by"
cs-410_2_11_24,cs-410,2,11,"00:01:55,980","00:02:00,970",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,"just three parameters here,"
cs-410_2_11_25,cs-410,2,11,"00:02:02,390","00:02:07,320",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,"Now, this line is heading"
cs-410_2_11_26,cs-410,2,11,"00:02:07,320","00:02:13,450",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"it shows that as we increase X1,"
cs-410_2_11_27,cs-410,2,11,"00:02:13,450","00:02:17,780",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,So we know that beta one and beta two have
cs-410_2_11_28,cs-410,2,11,"00:02:17,780","00:02:18,920",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,the other is positive.
cs-410_2_11_29,cs-410,2,11,"00:02:20,800","00:02:26,790",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,So let's just assume that beta one is
cs-410_2_11_30,cs-410,2,11,"00:02:28,810","00:02:31,250",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,"Now, it's interesting to examine, then,"
cs-410_2_11_31,cs-410,2,11,"00:02:31,250","00:02:34,800",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,the data instances on
cs-410_2_11_32,cs-410,2,11,"00:02:34,800","00:02:39,690",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"So, here, the data instance are visualized"
cs-410_2_11_33,cs-410,2,11,"00:02:39,690","00:02:41,800",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,diamonds for the other class.
cs-410_2_11_34,cs-410,2,11,"00:02:43,140","00:02:49,090",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,"Now, one question is to take a point"
cs-410_2_11_35,cs-410,2,11,"00:02:49,090","00:02:54,110",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"what's the value of this expression, or"
cs-410_2_11_36,cs-410,2,11,"00:02:55,350","00:02:57,000",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,So what do you think?
cs-410_2_11_37,cs-410,2,11,"00:02:57,000","00:03:00,650",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,"Basically, we're going to evaluate"
cs-410_2_11_38,cs-410,2,11,"00:03:01,740","00:03:06,190",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,"And as we said, if this value's positive"
cs-410_2_11_39,cs-410,2,11,"00:03:06,190","00:03:09,610",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"one, and if it's negative,"
cs-410_2_11_40,cs-410,2,11,"00:03:09,610","00:03:15,343",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,"Intuitively, this line separates these two"
cs-410_2_11_41,cs-410,2,11,"00:03:15,343","00:03:19,870",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,one side would be positive and the points
cs-410_2_11_42,cs-410,2,11,"00:03:19,870","00:03:23,200",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,Our question is under the assumption
cs-410_2_11_43,cs-410,2,11,"00:03:23,200","00:03:25,320",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,let's examine a particular
cs-410_2_11_44,cs-410,2,11,"00:03:27,590","00:03:30,480",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,So what do you think is
cs-410_2_11_45,cs-410,2,11,"00:03:31,610","00:03:37,830",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"Well, to examine the sine we can"
cs-410_2_11_46,cs-410,2,11,"00:03:37,830","00:03:40,950",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"And we can compare this with let's say,"
cs-410_2_11_47,cs-410,2,11,"00:03:42,050","00:03:46,950",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,"value on the line, let's see,"
cs-410_2_11_48,cs-410,2,11,"00:03:48,440","00:03:53,520",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"While they have identical X1, but"
cs-410_2_11_49,cs-410,2,11,"00:03:54,740","00:03:59,790",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"Now, let's look at the sin"
cs-410_2_11_50,cs-410,2,11,"00:03:59,790","00:04:01,610",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"Well, we know this is a positive."
cs-410_2_11_51,cs-410,2,11,"00:04:02,850","00:04:06,260",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,"So, what that means is"
cs-410_2_11_52,cs-410,2,11,"00:04:06,260","00:04:10,400",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,this point should be higher
cs-410_2_11_53,cs-410,2,11,"00:04:10,400","00:04:14,800",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,this point on the line that means
cs-410_2_11_54,cs-410,2,11,"00:04:16,190","00:04:19,900",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,So we know in general of
cs-410_2_11_55,cs-410,2,11,"00:04:20,960","00:04:25,380",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,the function's value will be positive and
cs-410_2_11_56,cs-410,2,11,"00:04:25,380","00:04:29,380",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,you can also verify all the points
cs-410_2_11_57,cs-410,2,11,"00:04:29,380","00:04:31,750",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,And so this is how this kind
cs-410_2_11_58,cs-410,2,11,"00:04:31,750","00:04:35,940",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,linear separator can then separate
cs-410_2_11_59,cs-410,2,11,"00:04:37,810","00:04:42,830",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"So, now the natural question is,"
cs-410_2_11_60,cs-410,2,11,"00:04:42,830","00:04:47,687",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,"Now, I've get you one line here"
cs-410_2_11_61,cs-410,2,11,"00:04:47,687","00:04:53,190",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"And this line, of course, is determined"
cs-410_2_11_62,cs-410,2,11,"00:04:53,190","00:04:55,210",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,Different coefficients will
cs-410_2_11_63,cs-410,2,11,"00:04:55,210","00:04:58,770",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"So, we could imagine there are other"
cs-410_2_11_64,cs-410,2,11,"00:04:58,770","00:05:00,630",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"Gamma, for example,"
cs-410_2_11_65,cs-410,2,11,"00:05:00,630","00:05:04,860",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,could give us another line that counts
cs-410_2_11_66,cs-410,2,11,"00:05:06,010","00:05:09,710",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,"Of course, there are also lines that won't"
cs-410_2_11_67,cs-410,2,11,"00:05:09,710","00:05:12,310",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"But, the question is,"
cs-410_2_11_68,cs-410,2,11,"00:05:12,310","00:05:15,950",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,"separate both clauses,"
cs-410_2_11_69,cs-410,2,11,"00:05:15,950","00:05:21,740",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"In fact, you can imagine, there are many"
cs-410_2_11_70,cs-410,2,11,"00:05:21,740","00:05:27,310",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,"So, the logistical regression classifier"
cs-410_2_11_71,cs-410,2,11,"00:05:27,310","00:05:33,060",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,some criteria to determine where this line
cs-410_2_11_72,cs-410,2,11,"00:05:33,060","00:05:36,610",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,And uses a conditional likelihood
cs-410_2_11_73,cs-410,2,11,"00:05:36,610","00:05:38,310",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,which line is the best.
cs-410_2_11_74,cs-410,2,11,"00:05:38,310","00:05:41,130",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,But in SVM we're going to
cs-410_2_11_75,cs-410,2,11,"00:05:41,130","00:05:43,500",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,determining which line is the best.
cs-410_2_11_76,cs-410,2,11,"00:05:43,500","00:05:44,230",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"And this time,"
cs-410_2_11_77,cs-410,2,11,"00:05:44,230","00:05:48,300",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,the criteria is more tied to
cs-410_2_11_78,cs-410,2,11,"00:05:49,460","00:05:56,120",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,"So, the basic idea is to choose"
cs-410_2_11_79,cs-410,2,11,"00:05:56,120","00:05:57,180",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,So what is a margin?
cs-410_2_11_80,cs-410,2,11,"00:05:57,180","00:06:03,540",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,"So, I choose some dotted"
cs-410_2_11_81,cs-410,2,11,"00:06:03,540","00:06:09,020",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,the boundaries of those
cs-410_2_11_82,cs-410,2,11,"00:06:09,020","00:06:13,890",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,And the margin is simply
cs-410_2_11_83,cs-410,2,11,"00:06:13,890","00:06:17,420",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,"the separator, and"
cs-410_2_11_84,cs-410,2,11,"00:06:18,490","00:06:23,830",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,So you can see the margin of this
cs-410_2_11_85,cs-410,2,11,"00:06:23,830","00:06:25,810",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,you can also define
cs-410_2_11_86,cs-410,2,11,"00:06:27,020","00:06:31,190",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,In order for
cs-410_2_11_87,cs-410,2,11,"00:06:31,190","00:06:35,700",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,it has to be kind of in the middle
cs-410_2_11_88,cs-410,2,11,"00:06:35,700","00:06:40,050",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,you don't want this separator to
cs-410_2_11_89,cs-410,2,11,"00:06:40,050","00:06:42,800",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,that in intuition makes a lot of sense.
cs-410_2_11_90,cs-410,2,11,"00:06:44,460","00:06:47,050",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,So this is basic idea of SVM.
cs-410_2_11_91,cs-410,2,11,"00:06:47,050","00:06:50,020",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,We're going to choose a linear
cs-410_2_11_92,cs-410,2,11,"00:06:52,130","00:06:55,450",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"Now on this slide,"
cs-410_2_11_93,cs-410,2,11,"00:06:55,450","00:06:58,460",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,that I'm not going to use beta
cs-410_2_11_94,cs-410,2,11,"00:06:58,460","00:07:03,740",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"But instead, I'm going to use w although"
cs-410_2_11_95,cs-410,2,11,"00:07:03,740","00:07:05,370",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,don't be confused here.
cs-410_2_11_96,cs-410,2,11,"00:07:05,370","00:07:09,618",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"W here is actually a width,"
cs-410_2_11_97,cs-410,2,11,"00:07:12,734","00:07:19,030",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,So I'm also using lowercase b to
cs-410_2_11_98,cs-410,2,11,"00:07:20,030","00:07:24,100",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,And there are instances do
cs-410_2_11_99,cs-410,2,11,"00:07:24,100","00:07:28,790",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,I also use the vector form
cs-410_2_11_100,cs-410,2,11,"00:07:28,790","00:07:34,110",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,So we see a transpose of w vector
cs-410_2_11_101,cs-410,2,11,"00:07:35,290","00:07:42,080",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,So b is a bias constant and w is a set of
cs-410_2_11_102,cs-410,2,11,"00:07:42,080","00:07:45,260",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,We have m features and
cs-410_2_11_103,cs-410,2,11,"00:07:45,260","00:07:46,420",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,that will represent as a vector.
cs-410_2_11_104,cs-410,2,11,"00:07:47,640","00:07:51,260",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,"And similarly, the data instance here,"
cs-410_2_11_105,cs-410,2,11,"00:07:51,260","00:07:55,940",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,is represented by also a feature
cs-410_2_11_106,cs-410,2,11,"00:07:55,940","00:07:59,100",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,Xi is a feature value.
cs-410_2_11_107,cs-410,2,11,"00:07:59,100","00:08:04,418",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"For example, word count and"
cs-410_2_11_108,cs-410,2,11,"00:08:04,418","00:08:08,960",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,"Multiply these two vectors together,"
cs-410_2_11_109,cs-410,2,11,"00:08:08,960","00:08:14,335",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,we get the same form of the linear
cs-410_2_11_110,cs-410,2,11,"00:08:14,335","00:08:16,713",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,It's just a different way
cs-410_2_11_111,cs-410,2,11,"00:08:16,713","00:08:21,267",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,Now I use this way so that it's
cs-410_2_11_112,cs-410,2,11,"00:08:21,267","00:08:24,750",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,people usually use when
cs-410_2_11_113,cs-410,2,11,"00:08:24,750","00:08:29,470",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,This way you can better connect the slides
cs-410_2_11_114,cs-410,2,11,"00:08:31,190","00:08:39,780",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"Okay, so when we maximize"
cs-410_2_11_115,cs-410,2,11,"00:08:39,780","00:08:44,730",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,it just means the boundary of
cs-410_2_11_116,cs-410,2,11,"00:08:44,730","00:08:49,800",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,"a few data points, and these are the data"
cs-410_2_11_117,cs-410,2,11,"00:08:49,800","00:08:54,600",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,So here illustrated are two support
cs-410_2_11_118,cs-410,2,11,"00:08:54,600","00:08:56,220",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,the other class.
cs-410_2_11_119,cs-410,2,11,"00:08:56,220","00:09:00,900",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,And these quotas define
cs-410_2_11_120,cs-410,2,11,"00:09:00,900","00:09:05,350",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,you can imagine once we know which
cs-410_2_11_121,cs-410,2,11,"00:09:06,430","00:09:09,750",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,center separator line will
cs-410_2_11_122,cs-410,2,11,"00:09:09,750","00:09:16,320",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,So the other data points actually
cs-410_2_11_123,cs-410,2,11,"00:09:16,320","00:09:20,420",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,And you can see if you change the other
cs-410_2_11_124,cs-410,2,11,"00:09:20,420","00:09:22,905",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"the margin, so"
cs-410_2_11_125,cs-410,2,11,"00:09:22,905","00:09:26,514",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,Mainly affected by
cs-410_2_11_126,cs-410,2,11,"00:09:26,514","00:09:29,705",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,"Sorry, it's mainly affected"
cs-410_2_11_127,cs-410,2,11,"00:09:29,705","00:09:32,639",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,that's why it's called
cs-410_2_11_128,cs-410,2,11,"00:09:32,639","00:09:37,968",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,"Okay, so now the next question is,"
cs-410_2_11_129,cs-410,2,11,"00:09:37,968","00:09:42,730",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,how can we set it up to optimize the line?
cs-410_2_11_130,cs-410,2,11,"00:09:42,730","00:09:47,430",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,How can we actually find the line or
cs-410_2_11_131,cs-410,2,11,"00:09:47,430","00:09:51,390",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,Now this is equivalent to
cs-410_2_11_132,cs-410,2,11,"00:09:51,390","00:09:55,779",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,"b, because they will determine"
cs-410_2_11_133,cs-410,2,11,"00:09:58,010","00:10:04,700",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,"So in the simplest case, the linear SVM"
cs-410_2_11_134,cs-410,2,11,"00:10:04,700","00:10:10,230",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,"So again, let's recall that our classifier"
cs-410_2_11_135,cs-410,2,11,"00:10:10,230","00:10:15,980",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,"have weights for all the features, and the"
cs-410_2_11_136,cs-410,2,11,"00:10:15,980","00:10:21,040",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,And the classifier will say X is in
cs-410_2_11_137,cs-410,2,11,"00:10:21,040","00:10:23,950",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,"Otherwise, it's going to say"
cs-410_2_11_138,cs-410,2,11,"00:10:23,950","00:10:27,220",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,"So this is our assumption, our setup."
cs-410_2_11_139,cs-410,2,11,"00:10:27,220","00:10:32,406",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,"So in the linear SVM,"
cs-410_2_11_140,cs-410,2,11,"00:10:32,406","00:10:37,510",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=632,values to optimize the margins and
cs-410_2_11_141,cs-410,2,11,"00:10:38,800","00:10:41,920",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,The training data would be basically
cs-410_2_11_142,cs-410,2,11,"00:10:41,920","00:10:45,940",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,We have a set of training points
cs-410_2_11_143,cs-410,2,11,"00:10:45,940","00:10:50,290",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,"then we also know the corresponding label,"
cs-410_2_11_144,cs-410,2,11,"00:10:50,290","00:10:54,310",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,"And here we define y i as two values, but"
cs-410_2_11_145,cs-410,2,11,"00:10:54,310","00:10:58,358",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,"these values are not 0, 1 as you"
cs-410_2_11_146,cs-410,2,11,"00:10:58,358","00:11:03,990",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,"positive 1, and they're corresponding to"
cs-410_2_11_147,cs-410,2,11,"00:11:03,990","00:11:08,330",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,Now you might wonder why we
cs-410_2_11_148,cs-410,2,11,"00:11:08,330","00:11:11,770",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,"1 instead of having -1, 1."
cs-410_2_11_149,cs-410,2,11,"00:11:11,770","00:11:15,520",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,And this is purely for mathematical
cs-410_2_11_150,cs-410,2,11,"00:11:16,700","00:11:19,450",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,So the goal of optimization first is
cs-410_2_11_151,cs-410,2,11,"00:11:19,450","00:11:23,700",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,to make sure the labeling of
cs-410_2_11_152,cs-410,2,11,"00:11:23,700","00:11:28,240",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,"So that just means if y i,"
cs-410_2_11_153,cs-410,2,11,"00:11:28,240","00:11:33,610",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=688,"is 1, we would like this"
cs-410_2_11_154,cs-410,2,11,"00:11:33,610","00:11:36,740",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,And here we just choose
cs-410_2_11_155,cs-410,2,11,"00:11:36,740","00:11:41,875",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,"But if you use another threshold,"
cs-410_2_11_156,cs-410,2,11,"00:11:41,875","00:11:47,300",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,into the parameter values b and
cs-410_2_11_157,cs-410,2,11,"00:11:48,950","00:11:54,780",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,"Now if, on the other hand, y i is -1,"
cs-410_2_11_158,cs-410,2,11,"00:11:54,780","00:11:58,460",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,then we want this classifier
cs-410_2_11_159,cs-410,2,11,"00:11:58,460","00:12:04,860",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,"in fact a negative value, and we want this"
cs-410_2_11_160,cs-410,2,11,"00:12:04,860","00:12:11,110",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,"Now these are the two different instances,"
cs-410_2_11_161,cs-410,2,11,"00:12:11,110","00:12:13,714",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,How can we combine them together?
cs-410_2_11_162,cs-410,2,11,"00:12:13,714","00:12:18,622",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,Now this is where it's convenient
cs-410_2_11_163,cs-410,2,11,"00:12:18,622","00:12:20,200",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,"the other category,"
cs-410_2_11_164,cs-410,2,11,"00:12:20,200","00:12:25,830",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,because it turns out that we can either
cs-410_2_11_165,cs-410,2,11,"00:12:26,832","00:12:32,085",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,y i multiplied by the classifier value
cs-410_2_11_166,cs-410,2,11,"00:12:33,210","00:12:35,484",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,"And obviously when y i is just 1,"
cs-410_2_11_167,cs-410,2,11,"00:12:35,484","00:12:39,968",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=755,you see this is the same as
cs-410_2_11_168,cs-410,2,11,"00:12:39,968","00:12:48,020",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=759,"But when y i is -1, you also see that this"
cs-410_2_11_169,cs-410,2,11,"00:12:48,020","00:12:53,060",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=768,So this one actually captures both
cs-410_2_11_170,cs-410,2,11,"00:12:53,060","00:12:56,960",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,and that's a convenient way of
cs-410_2_11_171,cs-410,2,11,"00:12:56,960","00:12:58,137",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=776,What's our second goal?
cs-410_2_11_172,cs-410,2,11,"00:12:58,137","00:13:00,414",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,"Well, that's to maximize margin, so"
cs-410_2_11_173,cs-410,2,11,"00:13:00,414","00:13:04,600",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,we want to ensure that separator
cs-410_2_11_174,cs-410,2,11,"00:13:04,600","00:13:08,109",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,"But then, among all the cases"
cs-410_2_11_175,cs-410,2,11,"00:13:08,109","00:13:12,172",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,we also would like to choose the separator
cs-410_2_11_176,cs-410,2,11,"00:13:12,172","00:13:18,758",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,Now the margin can be assumed to be
cs-410_2_11_177,cs-410,2,11,"00:13:18,758","00:13:23,777",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,And so
cs-410_2_11_178,cs-410,2,11,"00:13:23,777","00:13:29,893",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,us basically the sum of
cs-410_2_11_179,cs-410,2,11,"00:13:29,893","00:13:35,691",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=809,So to have a small value for
cs-410_2_11_180,cs-410,2,11,"00:13:35,691","00:13:40,430",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,it means all the w i's must be small.
cs-410_2_11_181,cs-410,2,11,"00:13:42,440","00:13:45,710",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,So we've just assumed that
cs-410_2_11_182,cs-410,2,11,"00:13:46,930","00:13:50,890",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=826,getting the data on the training
cs-410_2_11_183,cs-410,2,11,"00:13:50,890","00:13:57,649",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=830,Now we also have the objective that's
cs-410_2_11_184,cs-410,2,11,"00:13:57,649","00:14:03,013",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,and this is simply to minimize
cs-410_2_11_185,cs-410,2,11,"00:14:03,013","00:14:06,251",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=843,and we often denote this by phi of w.
cs-410_2_11_186,cs-410,2,11,"00:14:06,251","00:14:10,616",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=846,So now you can see this is
cs-410_2_11_187,cs-410,2,11,"00:14:10,616","00:14:15,044",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,"We have some variables to optimize,"
cs-410_2_11_188,cs-410,2,11,"00:14:15,044","00:14:17,540",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,b and we have some constraints.
cs-410_2_11_189,cs-410,2,11,"00:14:17,540","00:14:18,949",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=857,These are linear constraints and
cs-410_2_11_190,cs-410,2,11,"00:14:18,949","00:14:22,380",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=858,the objective function is
cs-410_2_11_191,cs-410,2,11,"00:14:22,380","00:14:25,370",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,So this a quadratic program
cs-410_2_11_192,cs-410,2,11,"00:14:25,370","00:14:30,050",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,and there are standard algorithm that
cs-410_2_11_193,cs-410,2,11,"00:14:30,050","00:14:34,190",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,And once we solve the problem
cs-410_2_11_194,cs-410,2,11,"00:14:34,190","00:14:37,080",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=874,And then this would give us
cs-410_2_11_195,cs-410,2,11,"00:14:37,080","00:14:42,160",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,So we can then use this classifier
cs-410_2_11_196,cs-410,2,11,"00:14:42,160","00:14:47,190",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=882,Now the previous formulation did not
cs-410_2_11_197,cs-410,2,11,"00:14:47,190","00:14:50,448",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=887,but sometimes the data may not
cs-410_2_11_198,cs-410,2,11,"00:14:50,448","00:14:54,690",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,That means that they may not
cs-410_2_11_199,cs-410,2,11,"00:14:54,690","00:14:59,300",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,the previous slide where a line
cs-410_2_11_200,cs-410,2,11,"00:14:59,300","00:15:02,850",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,And what would happen if
cs-410_2_11_201,cs-410,2,11,"00:15:02,850","00:15:04,980",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=902,"Well, the principle can stay."
cs-410_2_11_202,cs-410,2,11,"00:15:04,980","00:15:09,305",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,We want to minimize the training error but
cs-410_2_11_203,cs-410,2,11,"00:15:09,305","00:15:12,270",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=909,"But in this case we have a soft margin,"
cs-410_2_11_204,cs-410,2,11,"00:15:12,270","00:15:16,000",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=912,because the data points may
cs-410_2_11_205,cs-410,2,11,"00:15:17,030","00:15:24,650",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=917,So it turns out that we can easily
cs-410_2_11_206,cs-410,2,11,"00:15:24,650","00:15:28,090",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=924,So what you see here is very similar
cs-410_2_11_207,cs-410,2,11,"00:15:28,090","00:15:31,760",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=928,but we have introduced
cs-410_2_11_208,cs-410,2,11,"00:15:31,760","00:15:35,610",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=931,And we in fact will have one for
cs-410_2_11_209,cs-410,2,11,"00:15:35,610","00:15:40,780",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=935,this is going to model the error
cs-410_2_11_210,cs-410,2,11,"00:15:40,780","00:15:43,245",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=940,But the optimization problem
cs-410_2_11_211,cs-410,2,11,"00:15:43,245","00:15:44,783",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,"So specifically,"
cs-410_2_11_212,cs-410,2,11,"00:15:44,783","00:15:50,170",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=944,you will see we have added something
cs-410_2_11_213,cs-410,2,11,"00:15:50,170","00:15:56,861",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=950,First we have added some
cs-410_2_11_214,cs-410,2,11,"00:15:56,861","00:16:02,119",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=956,that now we allow a Allow the classifier
cs-410_2_11_215,cs-410,2,11,"00:16:02,119","00:16:06,760",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=962,to make some mistakes here.
cs-410_2_11_216,cs-410,2,11,"00:16:06,760","00:16:12,860",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=966,"So, this Xi i is allowed an error."
cs-410_2_11_217,cs-410,2,11,"00:16:12,860","00:16:16,560",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=972,"If we set Xi i to 0, then we go"
cs-410_2_11_218,cs-410,2,11,"00:16:16,560","00:16:20,260",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=976,We want every instance to
cs-410_2_11_219,cs-410,2,11,"00:16:20,260","00:16:26,420",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,"But, if we allow this to be non-zero,"
cs-410_2_11_220,cs-410,2,11,"00:16:26,420","00:16:30,730",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,"In fact, if the length of the Xi i is very"
cs-410_2_11_221,cs-410,2,11,"00:16:30,730","00:16:33,270",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=990,"So naturally,"
cs-410_2_11_222,cs-410,2,11,"00:16:33,270","00:16:37,570",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=993,So we want to then also
cs-410_2_11_223,cs-410,2,11,"00:16:37,570","00:16:41,940",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=997,"So, because Xi i needs to be minimized"
cs-410_2_11_224,cs-410,2,11,"00:16:42,940","00:16:46,020",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1002,"And so, as a result,"
cs-410_2_11_225,cs-410,2,11,"00:16:46,020","00:16:50,910",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1006,"we also add more to the original one,"
cs-410_2_11_226,cs-410,2,11,"00:16:50,910","00:16:55,190",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1010,by basically ensuring that we not
cs-410_2_11_227,cs-410,2,11,"00:16:55,190","00:16:59,130",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1015,"also minimize the errors, as you see here."
cs-410_2_11_228,cs-410,2,11,"00:16:59,130","00:17:02,705",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1019,Here we simply take a sum
cs-410_2_11_229,cs-410,2,11,"00:17:02,705","00:17:07,695",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1022,Each one has a Xi i to model
cs-410_2_11_230,cs-410,2,11,"00:17:07,695","00:17:10,413",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1027,"And when we combine them together,"
cs-410_2_11_231,cs-410,2,11,"00:17:10,413","00:17:14,680",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1030,we basically want to minimize
cs-410_2_11_232,cs-410,2,11,"00:17:16,350","00:17:21,001",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1036,"Now you see there's a parameter C here,"
cs-410_2_11_233,cs-410,2,11,"00:17:21,001","00:17:25,740",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1041,the trade-off between minimizing
cs-410_2_11_234,cs-410,2,11,"00:17:25,740","00:17:27,888",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1045,"If C is set to zero, you can see,"
cs-410_2_11_235,cs-410,2,11,"00:17:27,888","00:17:33,070",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1047,we go back to the original object function
cs-410_2_11_236,cs-410,2,11,"00:17:34,340","00:17:38,368",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1054,We don't really optimize
cs-410_2_11_237,cs-410,2,11,"00:17:38,368","00:17:43,730",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1058,then Xi i can be set to a very large value
cs-410_2_11_238,cs-410,2,11,"00:17:43,730","00:17:46,512",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1063,"That's not very good of course, so"
cs-410_2_11_239,cs-410,2,11,"00:17:46,512","00:17:50,884",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1066,"C should be set to a non-zero value,"
cs-410_2_11_240,cs-410,2,11,"00:17:50,884","00:17:53,412",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1070,"But when C is set to a very,"
cs-410_2_11_241,cs-410,2,11,"00:17:53,412","00:17:58,143",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1073,we'll see the object of the function will
cs-410_2_11_242,cs-410,2,11,"00:17:58,143","00:18:02,420",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1078,and so the optimization of margin
cs-410_2_11_243,cs-410,2,11,"00:18:02,420","00:18:06,350",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1082,"So if that happens, what would happen is"
cs-410_2_11_244,cs-410,2,11,"00:18:07,420","00:18:11,420",244,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1087,then we will try to do our best to
cs-410_2_11_245,cs-410,2,11,"00:18:11,420","00:18:14,730",245,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1091,then we're not going to
cs-410_2_11_246,cs-410,2,11,"00:18:14,730","00:18:19,270",246,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1094,that affects the generalization factors
cs-410_2_11_247,cs-410,2,11,"00:18:19,270","00:18:20,548",247,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1099,So it's also not good.
cs-410_2_11_248,cs-410,2,11,"00:18:20,548","00:18:28,175",248,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1100,"So in particular, this parameter C"
cs-410_2_11_249,cs-410,2,11,"00:18:28,175","00:18:32,045",249,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1108,And this is just like in the case of
cs-410_2_11_250,cs-410,2,11,"00:18:32,045","00:18:34,080",250,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1112,to optimize a number of neighbors.
cs-410_2_11_251,cs-410,2,11,"00:18:34,080","00:18:35,510",251,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1114,Here you need to optimize the C.
cs-410_2_11_252,cs-410,2,11,"00:18:35,510","00:18:40,510",252,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1115,"And this is, in general,"
cs-410_2_11_253,cs-410,2,11,"00:18:40,510","00:18:43,331",253,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1120,"Basically, you look at"
cs-410_2_11_254,cs-410,2,11,"00:18:43,331","00:18:47,610",254,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,see what value C should be set to in
cs-410_2_11_255,cs-410,2,11,"00:18:49,050","00:18:50,390",255,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1129,"Now with this modification,"
cs-410_2_11_256,cs-410,2,11,"00:18:50,390","00:18:54,250",256,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1130,the problem is still quadratic programming
cs-410_2_11_257,cs-410,2,11,"00:18:54,250","00:19:00,003",257,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1134,algorithm can be actually applied to solve
cs-410_2_11_258,cs-410,2,11,"00:19:02,080","00:19:05,780",258,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1142,"Again, once we have obtained"
cs-410_2_11_259,cs-410,2,11,"00:19:05,780","00:19:11,360",259,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1145,then we can have classifier that's
cs-410_2_11_260,cs-410,2,11,"00:19:11,360","00:19:13,566",260,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1151,So that's the basic idea of SVM.
cs-410_2_11_261,cs-410,2,11,"00:19:16,993","00:19:20,402",261,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1156,So to summarize the text
cs-410_2_11_262,cs-410,2,11,"00:19:20,402","00:19:25,170",262,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1160,"where we introduce the many methods,"
cs-410_2_11_263,cs-410,2,11,"00:19:25,170","00:19:27,140",263,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1165,Some are discriminative methods.
cs-410_2_11_264,cs-410,2,11,"00:19:27,140","00:19:32,230",264,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1167,And these tend to perform
cs-410_2_11_265,cs-410,2,11,"00:19:32,230","00:19:37,920",265,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1172,"So there's still no clear winner,"
cs-410_2_11_266,cs-410,2,11,"00:19:37,920","00:19:42,460",266,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1177,And the performance might also
cs-410_2_11_267,cs-410,2,11,"00:19:42,460","00:19:44,320",267,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1182,different problems.
cs-410_2_11_268,cs-410,2,11,"00:19:44,320","00:19:50,610",268,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1184,And one reason is also because the feature
cs-410_2_11_269,cs-410,2,11,"00:19:52,280","00:19:56,470",269,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1192,and these methods all require
cs-410_2_11_270,cs-410,2,11,"00:19:56,470","00:19:59,400",270,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1196,"And to design an effective feature set,"
cs-410_2_11_271,cs-410,2,11,"00:19:59,400","00:20:03,530",271,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1199,we need domain knowledge and humans
cs-410_2_11_272,cs-410,2,11,"00:20:03,530","00:20:05,608",272,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1203,although there are new
cs-410_2_11_273,cs-410,2,11,"00:20:05,608","00:20:10,020",273,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1205,algorithm representation learning
cs-410_2_11_274,cs-410,2,11,"00:20:12,640","00:20:18,169",274,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1212,And another common thing
cs-410_2_11_275,cs-410,2,11,"00:20:18,169","00:20:23,546",275,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1218,"be performing similarly on the data set,"
cs-410_2_11_276,cs-410,2,11,"00:20:23,546","00:20:28,220",276,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1223,but with different mistakes.
cs-410_2_11_277,cs-410,2,11,"00:20:28,220","00:20:30,913",277,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1228,"And so,"
cs-410_2_11_278,cs-410,2,11,"00:20:30,913","00:20:34,070",278,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1230,then the mistakes they
cs-410_2_11_279,cs-410,2,11,"00:20:34,070","00:20:37,630",279,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1234,So that means it's useful to
cs-410_2_11_280,cs-410,2,11,"00:20:37,630","00:20:42,690",280,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1237,a particular problem and
cs-410_2_11_281,cs-410,2,11,"00:20:42,690","00:20:49,092",281,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1242,because this can improve the robustness
cs-410_2_11_282,cs-410,2,11,"00:20:49,092","00:20:54,192",282,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1249,So assemble approaches that
cs-410_2_11_283,cs-410,2,11,"00:20:54,192","00:20:59,990",283,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1254,methods tend to be more robust and
cs-410_2_11_284,cs-410,2,11,"00:20:59,990","00:21:04,530",284,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1259,Most techniques that we introduce
cs-410_2_11_285,cs-410,2,11,"00:21:04,530","00:21:06,990",285,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1264,which is a very general method.
cs-410_2_11_286,cs-410,2,11,"00:21:06,990","00:21:10,975",286,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1266,So that means that these methods can
cs-410_2_11_287,cs-410,2,11,"00:21:10,975","00:21:12,580",287,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1270,categorization problem.
cs-410_2_11_288,cs-410,2,11,"00:21:12,580","00:21:17,554",288,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1272,As long as we have humans to help
cs-410_2_11_289,cs-410,2,11,"00:21:17,554","00:21:23,493",289,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1277,"design features, then supervising machine"
cs-410_2_11_290,cs-410,2,11,"00:21:23,493","00:21:29,255",290,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1283,can be easily applied to those problems
cs-410_2_11_291,cs-410,2,11,"00:21:29,255","00:21:34,431",291,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1289,allow us to characterize content
cs-410_2_11_292,cs-410,2,11,"00:21:34,431","00:21:38,716",292,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1294,Or to predict the sum
cs-410_2_11_293,cs-410,2,11,"00:21:38,716","00:21:43,250",293,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1298,variables that are associated
cs-410_2_11_294,cs-410,2,11,"00:21:43,250","00:21:47,875",294,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1303,"The computers, of course, here are trying"
cs-410_2_11_295,cs-410,2,11,"00:21:47,875","00:21:49,908",295,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1307,the features provided by human.
cs-410_2_11_296,cs-410,2,11,"00:21:49,908","00:21:53,357",296,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1309,"And as I said, there are many"
cs-410_2_11_297,cs-410,2,11,"00:21:53,357","00:21:56,130",297,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1313,they also optimize different object or
cs-410_2_11_298,cs-410,2,11,"00:21:58,180","00:22:02,240",298,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1318,"But in order to achieve good performance,"
cs-410_2_11_299,cs-410,2,11,"00:22:02,240","00:22:03,750",299,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1322,also plenty of training data.
cs-410_2_11_300,cs-410,2,11,"00:22:04,770","00:22:08,870",300,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1324,"So as a general rule, and if you can"
cs-410_2_11_301,cs-410,2,11,"00:22:08,870","00:22:13,860",301,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1328,"and then provide more training data,"
cs-410_2_11_302,cs-410,2,11,"00:22:13,860","00:22:18,390",302,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1333,Performance is often much more
cs-410_2_11_303,cs-410,2,11,"00:22:18,390","00:22:23,030",303,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1338,features than by the choice
cs-410_2_11_304,cs-410,2,11,"00:22:23,030","00:22:26,972",304,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1343,So feature design tends to be more
cs-410_2_11_305,cs-410,2,11,"00:22:26,972","00:22:27,768",305,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1346,classifier.
cs-410_2_11_306,cs-410,2,11,"00:22:30,844","00:22:34,170",306,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1350,"So, how do we design effective features?"
cs-410_2_11_307,cs-410,2,11,"00:22:34,170","00:22:37,360",307,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1354,"Well, unfortunately,"
cs-410_2_11_308,cs-410,2,11,"00:22:37,360","00:22:43,108",308,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1357,So there's no really much
cs-410_2_11_309,cs-410,2,11,"00:22:43,108","00:22:47,672",309,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1363,But we can do some analysis of
cs-410_2_11_310,cs-410,2,11,"00:22:47,672","00:22:54,400",310,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1367,try to understand what kind of features
cs-410_2_11_311,cs-410,2,11,"00:22:54,400","00:22:59,720",311,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1374,"And in general, we can use a lot of domain"
cs-410_2_11_312,cs-410,2,11,"00:23:01,640","00:23:06,180",312,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1381,And another way to figure out
cs-410_2_11_313,cs-410,2,11,"00:23:06,180","00:23:10,230",313,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1386,to do error analysis on
cs-410_2_11_314,cs-410,2,11,"00:23:10,230","00:23:11,080",314,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1390,"You could, for example,"
cs-410_2_11_315,cs-410,2,11,"00:23:11,080","00:23:16,110",315,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1391,look at which category tends to be
cs-410_2_11_316,cs-410,2,11,"00:23:16,110","00:23:20,890",316,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1396,And you can use a confusion matrix
cs-410_2_11_317,cs-410,2,11,"00:23:20,890","00:23:22,340",317,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1400,across categories.
cs-410_2_11_318,cs-410,2,11,"00:23:22,340","00:23:25,320",318,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1402,"And then,"
cs-410_2_11_319,cs-410,2,11,"00:23:25,320","00:23:29,780",319,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1405,see why the mistake has been made and
cs-410_2_11_320,cs-410,2,11,"00:23:29,780","00:23:35,260",320,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1409,And this can allow you to obtain
cs-410_2_11_321,cs-410,2,11,"00:23:35,260","00:23:37,840",321,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1415,So error analysis is very
cs-410_2_11_322,cs-410,2,11,"00:23:37,840","00:23:40,860",322,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1417,that's where you can get the insights
cs-410_2_11_323,cs-410,2,11,"00:23:42,150","00:23:45,220",323,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1422,"And finally, we can leverage this"
cs-410_2_11_324,cs-410,2,11,"00:23:45,220","00:23:48,710",324,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1425,"So, for example, feature selection is"
cs-410_2_11_325,cs-410,2,11,"00:23:48,710","00:23:50,390",325,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1428,"about, but is very important."
cs-410_2_11_326,cs-410,2,11,"00:23:50,390","00:23:54,830",326,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1430,And it has to do with trying to select the
cs-410_2_11_327,cs-410,2,11,"00:23:54,830","00:23:56,276",327,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1434,train a full classifier.
cs-410_2_11_328,cs-410,2,11,"00:23:56,276","00:24:00,900",328,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1436,Sometimes training a classifier will also
cs-410_2_11_329,cs-410,2,11,"00:24:00,900","00:24:01,419",329,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1440,values.
cs-410_2_11_330,cs-410,2,11,"00:24:01,419","00:24:04,658",330,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1441,There are also other ways
cs-410_2_11_331,cs-410,2,11,"00:24:04,658","00:24:07,538",331,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1444,"Of the model,"
cs-410_2_11_332,cs-410,2,11,"00:24:07,538","00:24:12,870",332,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1447,"For example, the SVM actually tries"
cs-410_2_11_333,cs-410,2,11,"00:24:12,870","00:24:16,630",333,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1452,"But you can further force some features,"
cs-410_2_11_334,cs-410,2,11,"00:24:16,630","00:24:19,019",334,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1456,force to use only a small
cs-410_2_11_335,cs-410,2,11,"00:24:21,080","00:24:25,030",335,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1461,There are also techniques for
cs-410_2_11_336,cs-410,2,11,"00:24:25,030","00:24:29,450",336,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1465,And that's to reduce a high dimensional
cs-410_2_11_337,cs-410,2,11,"00:24:29,450","00:24:33,150",337,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1469,space typically by clustering
cs-410_2_11_338,cs-410,2,11,"00:24:33,150","00:24:38,150",338,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1473,So metrics factorization
cs-410_2_11_339,cs-410,2,11,"00:24:38,150","00:24:42,860",339,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1478,"such a job, and this is some of the"
cs-410_2_11_340,cs-410,2,11,"00:24:42,860","00:24:44,820",340,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1482,the talking models that we'll discuss.
cs-410_2_11_341,cs-410,2,11,"00:24:44,820","00:24:48,220",341,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1484,So talking morals like psa or
cs-410_2_11_342,cs-410,2,11,"00:24:48,220","00:24:52,570",342,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1488,lda can actually help us reduce
cs-410_2_11_343,cs-410,2,11,"00:24:52,570","00:24:56,331",343,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1492,Like imagine the words
cs-410_2_11_344,cs-410,2,11,"00:24:56,331","00:25:01,970",344,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1496,But the can be matched to the topic
cs-410_2_11_345,cs-410,2,11,"00:25:01,970","00:25:04,380",345,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1501,So a document can now be represented
cs-410_2_11_346,cs-410,2,11,"00:25:04,380","00:25:08,750",346,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1504,as a vector of just k values
cs-410_2_11_347,cs-410,2,11,"00:25:08,750","00:25:12,380",347,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1508,So we can let each topic define one
cs-410_2_11_348,cs-410,2,11,"00:25:12,380","00:25:17,920",348,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1512,space instead of the original high
cs-410_2_11_349,cs-410,2,11,"00:25:17,920","00:25:21,720",349,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1517,And this is often another way
cs-410_2_11_350,cs-410,2,11,"00:25:21,720","00:25:26,200",350,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1521,"Especially, we could also use the"
cs-410_2_11_351,cs-410,2,11,"00:25:26,200","00:25:28,370",351,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1526,such low dimensional structures.
cs-410_2_11_352,cs-410,2,11,"00:25:29,850","00:25:36,070",352,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1529,"And so, the original worth features"
cs-410_2_11_353,cs-410,2,11,"00:25:36,070","00:25:40,480",353,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1536,amazing dimension features or
cs-410_2_11_354,cs-410,2,11,"00:25:40,480","00:25:44,810",354,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1540,to provide a multi resolution
cs-410_2_11_355,cs-410,2,11,"00:25:44,810","00:25:49,940",355,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1544,Deep learning is a new technique that
cs-410_2_11_356,cs-410,2,11,"00:25:51,190","00:25:54,890",356,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1551,It's particularly useful for
cs-410_2_11_357,cs-410,2,11,"00:25:54,890","00:25:59,840",357,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1554,So deep learning refers to deep neural
cs-410_2_11_358,cs-410,2,11,"00:25:59,840","00:26:07,110",358,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1559,where you can have intermediate
cs-410_2_11_359,cs-410,2,11,"00:26:07,110","00:26:11,570",359,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1567,"That it's highly non-linear transpire, and"
cs-410_2_11_360,cs-410,2,11,"00:26:11,570","00:26:17,220",360,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1571,some recent events that's allowed us to
cs-410_2_11_361,cs-410,2,11,"00:26:17,220","00:26:23,300",361,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1577,And the technique has been shown to be
cs-410_2_11_362,cs-410,2,11,"00:26:23,300","00:26:27,620",362,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1583,"computer reasoning, and"
cs-410_2_11_363,cs-410,2,11,"00:26:27,620","00:26:29,530",363,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1587,It has shown some promise.
cs-410_2_11_364,cs-410,2,11,"00:26:29,530","00:26:33,010",364,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1589,And one important advantage
cs-410_2_11_365,cs-410,2,11,"00:26:34,270","00:26:39,010",365,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1594,"relationship with the featured design,"
cs-410_2_11_366,cs-410,2,11,"00:26:39,010","00:26:43,920",366,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1599,learn intermediate replantations or
cs-410_2_11_367,cs-410,2,11,"00:26:43,920","00:26:49,193",367,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1603,And this is very valuable for
cs-410_2_11_368,cs-410,2,11,"00:26:49,193","00:26:51,660",368,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1609,for text recalibration.
cs-410_2_11_369,cs-410,2,11,"00:26:51,660","00:26:57,390",369,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1611,"Although in text domain, because words are"
cs-410_2_11_370,cs-410,2,11,"00:26:57,390","00:27:01,620",370,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1617,because these are human's imaging for
cs-410_2_11_371,cs-410,2,11,"00:27:01,620","00:27:08,160",371,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1621,And they are generally sufficient for
cs-410_2_11_372,cs-410,2,11,"00:27:08,160","00:27:11,430",372,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1628,If there's a need for
cs-410_2_11_373,cs-410,2,11,"00:27:11,430","00:27:15,250",373,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1631,people would have invented a new word.
cs-410_2_11_374,cs-410,2,11,"00:27:15,250","00:27:18,320",374,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1635,So because of this we think
cs-410_2_11_375,cs-410,2,11,"00:27:18,320","00:27:22,610",375,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1638,text processing tends to be lower than for
cs-410_2_11_376,cs-410,2,11,"00:27:22,610","00:27:26,490",376,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1642,And the speech revenue where
cs-410_2_11_377,cs-410,2,11,"00:27:26,490","00:27:29,920",377,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1646,where the design that worked as features.
cs-410_2_11_378,cs-410,2,11,"00:27:31,160","00:27:35,020",378,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1651,But people only still very promising for
cs-410_2_11_379,cs-410,2,11,"00:27:35,020","00:27:35,857",379,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1655,complicated tasks.
cs-410_2_11_380,cs-410,2,11,"00:27:35,857","00:27:39,850",380,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1655,Like a analysis it has
cs-410_2_11_381,cs-410,2,11,"00:27:41,230","00:27:44,760",381,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1661,because it can provide that
cs-410_2_11_382,cs-410,2,11,"00:27:47,030","00:27:50,240",382,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1667,Now regarding the training examples.
cs-410_2_11_383,cs-410,2,11,"00:27:50,240","00:27:53,940",383,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1670,It's generally hard to get a lot of
cs-410_2_11_384,cs-410,2,11,"00:27:53,940","00:27:54,560",384,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1673,human labor.
cs-410_2_11_385,cs-410,2,11,"00:27:56,310","00:27:58,570",385,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1676,But there are also some
cs-410_2_11_386,cs-410,2,11,"00:27:58,570","00:28:04,830",386,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1678,So one is to assume in some low quality
cs-410_2_11_387,cs-410,2,11,"00:28:04,830","00:28:07,800",387,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1684,"So, those can be called"
cs-410_2_11_388,cs-410,2,11,"00:28:07,800","00:28:13,220",388,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1687,"For example, if you take reviews from the"
cs-410_2_11_389,cs-410,2,11,"00:28:13,220","00:28:21,250",389,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1693,"So, to train a of categorizer,"
cs-410_2_11_390,cs-410,2,11,"00:28:21,250","00:28:24,860",390,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1701,And categorize these reviews
cs-410_2_11_391,cs-410,2,11,"00:28:24,860","00:28:31,570",391,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1704,Then we could assume five star reviews
cs-410_2_11_392,cs-410,2,11,"00:28:31,570","00:28:33,270",392,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1711,One star are negative.
cs-410_2_11_393,cs-410,2,11,"00:28:33,270","00:28:34,190",393,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1713,"But of course,"
cs-410_2_11_394,cs-410,2,11,"00:28:34,190","00:28:38,520",394,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1714,sometimes even five star reviews will also
cs-410_2_11_395,cs-410,2,11,"00:28:38,520","00:28:43,180",395,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1718,"sample is not all of that high quality,"
cs-410_2_11_396,cs-410,2,11,"00:28:45,200","00:28:47,970",396,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1725,Another idea is to exploit
cs-410_2_11_397,cs-410,2,11,"00:28:47,970","00:28:50,830",397,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1727,there are techniques called
cs-410_2_11_398,cs-410,2,11,"00:28:50,830","00:28:55,685",398,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1730,learning techniques that can allow you to
cs-410_2_11_399,cs-410,2,11,"00:28:55,685","00:29:01,070",399,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1735,"So, in other case it's easy to see"
cs-410_2_11_400,cs-410,2,11,"00:29:01,070","00:29:03,760",400,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1741,both text plus read and
cs-410_2_11_401,cs-410,2,11,"00:29:03,760","00:29:09,220",401,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1743,"So you can imagine, if you have a lot of"
cs-410_2_11_402,cs-410,2,11,"00:29:09,220","00:29:15,620",402,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1749,then you can actually do clustering
cs-410_2_11_403,cs-410,2,11,"00:29:15,620","00:29:18,088",403,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1755,And then try to somehow
cs-410_2_11_404,cs-410,2,11,"00:29:18,088","00:29:23,230",404,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1758,With the categories defined
cs-410_2_11_405,cs-410,2,11,"00:29:23,230","00:29:26,390",405,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1763,where we already know which
cs-410_2_11_406,cs-410,2,11,"00:29:26,390","00:29:31,620",406,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1766,So you can in fact use the Algorithm
cs-410_2_11_407,cs-410,2,11,"00:29:31,620","00:29:37,390",407,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1771,That would allow you essentially also
cs-410_2_11_408,cs-410,2,11,"00:29:37,390","00:29:39,320",408,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1777,You can think of this in another way.
cs-410_2_11_409,cs-410,2,11,"00:29:39,320","00:29:43,804",409,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1779,"Basically, we can use let's say a to"
cs-410_2_11_410,cs-410,2,11,"00:29:43,804","00:29:48,480",410,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1783,classify all of the unlabeled text
cs-410_2_11_411,cs-410,2,11,"00:29:48,480","00:29:54,040",411,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1788,assume the high confidence Classification
cs-410_2_11_412,cs-410,2,11,"00:29:54,040","00:29:58,600",412,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1794,Then you suddenly have more training
cs-410_2_11_413,cs-410,2,11,"00:29:58,600","00:30:03,450",413,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1798,"now know some are labeled as category one,"
cs-410_2_11_414,cs-410,2,11,"00:30:03,450","00:30:06,380",414,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1803,All though the label is not
cs-410_2_11_415,cs-410,2,11,"00:30:06,380","00:30:07,830",415,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1806,then they can still be useful.
cs-410_2_11_416,cs-410,2,11,"00:30:07,830","00:30:14,720",416,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1807,So let's assume they are actually training
cs-410_2_11_417,cs-410,2,11,"00:30:14,720","00:30:19,940",417,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1814,with true training examples through
cs-410_2_11_418,cs-410,2,11,"00:30:19,940","00:30:22,110",418,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1819,And so this idea is very powerful.
cs-410_2_11_419,cs-410,2,11,"00:30:23,980","00:30:28,280",419,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1823,When the enabled data and
cs-410_2_11_420,cs-410,2,11,"00:30:28,280","00:30:32,410",420,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1828,we might need to use other advanced
cs-410_2_11_421,cs-410,2,11,"00:30:32,410","00:30:35,150",421,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1832,called domain adaptation or
cs-410_2_11_422,cs-410,2,11,"00:30:35,150","00:30:37,580",422,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1835,This is when we can
cs-410_2_11_423,cs-410,2,11,"00:30:37,580","00:30:42,450",423,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1837,Borrow some training examples from
cs-410_2_11_424,cs-410,2,11,"00:30:42,450","00:30:44,470",424,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1842,"Or, from a categorization password"
cs-410_2_11_425,cs-410,2,11,"00:30:46,780","00:30:52,130",425,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1846,that follow very different distribution
cs-410_2_11_426,cs-410,2,11,"00:30:52,130","00:30:54,190",426,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1852,"But basically,"
cs-410_2_11_427,cs-410,2,11,"00:30:54,190","00:30:57,640",427,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1854,then we need to be careful and
cs-410_2_11_428,cs-410,2,11,"00:30:57,640","00:31:02,300",428,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1857,"But yet, we can still want to use some"
cs-410_2_11_429,cs-410,2,11,"00:31:02,300","00:31:07,270",429,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1862,"So for example,"
cs-410_2_11_430,cs-410,2,11,"00:31:07,270","00:31:12,410",430,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1867,give you Effective plus y for
cs-410_2_11_431,cs-410,2,11,"00:31:12,410","00:31:19,490",431,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1872,But you can still learn something from
cs-410_2_11_432,cs-410,2,11,"00:31:19,490","00:31:25,470",432,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1879,So there are mission learning techniques
cs-410_2_11_433,cs-410,2,11,"00:31:25,470","00:31:30,259",433,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1885,Here's a suggested reading where you
cs-410_2_11_434,cs-410,2,11,"00:31:30,259","00:31:33,271",434,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1890,more of the methods is
cs-410_2_11_435,cs-410,2,11,"00:31:33,271","00:31:43,271",435,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1893,[MUSIC]
cs-410_2_2_1,cs-410,2,2,"00:00:00,000","00:00:05,293",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_2_2_2,cs-410,2,2,"00:00:10,067","00:00:15,310",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,"In this lecture, we continue"
cs-410_2_2_3,cs-410,2,2,"00:00:15,310","00:00:18,810",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,"In particular, we're going to"
cs-410_2_2_4,cs-410,2,2,"00:00:18,810","00:00:20,100",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"In the previous lecture,"
cs-410_2_2_5,cs-410,2,2,"00:00:20,100","00:00:25,880",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,we have derived a TF idea of weighting
cs-410_2_2_6,cs-410,2,2,"00:00:27,100","00:00:31,760",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,And we have assumed that this model
cs-410_2_2_7,cs-410,2,2,"00:00:31,760","00:00:37,302",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,"these examples as shown on this slide,"
cs-410_2_2_8,cs-410,2,2,"00:00:37,302","00:00:41,340",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,"d5, which has received a very high score."
cs-410_2_2_9,cs-410,2,2,"00:00:41,340","00:00:46,510",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,"Indeed, it has received the highest"
cs-410_2_2_10,cs-410,2,2,"00:00:46,510","00:00:51,980",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,But this document is intuitive and
cs-410_2_2_11,cs-410,2,2,"00:00:53,240","00:00:55,390",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"In this lecture,"
cs-410_2_2_12,cs-410,2,2,"00:00:55,390","00:00:58,960",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,how we're going to use TF
cs-410_2_2_13,cs-410,2,2,"00:01:00,410","00:01:04,870",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"Before we discuss the details,"
cs-410_2_2_14,cs-410,2,2,"00:01:04,870","00:01:08,820",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,this simple TF-IDF
cs-410_2_2_15,cs-410,2,2,"00:01:08,820","00:01:13,520",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,And see why this document has
cs-410_2_2_16,cs-410,2,2,"00:01:13,520","00:01:17,510",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"So this is the formula, and"
cs-410_2_2_17,cs-410,2,2,"00:01:17,510","00:01:21,730",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,then you will see it involves a sum
cs-410_2_2_18,cs-410,2,2,"00:01:23,810","00:01:28,140",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"And inside the sum, each matched"
cs-410_2_2_19,cs-410,2,2,"00:01:28,140","00:01:30,259",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,And this weight is TF-IDF weighting.
cs-410_2_2_20,cs-410,2,2,"00:01:31,580","00:01:36,853",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"So it has an idea of component,"
cs-410_2_2_21,cs-410,2,2,"00:01:36,853","00:01:42,586",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,One is the total number of documents
cs-410_2_2_22,cs-410,2,2,"00:01:42,586","00:01:45,890",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,The other is the document of frequency.
cs-410_2_2_23,cs-410,2,2,"00:01:45,890","00:01:48,220",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,This is the number of
cs-410_2_2_24,cs-410,2,2,"00:01:48,220","00:01:49,070",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,This word w.
cs-410_2_2_25,cs-410,2,2,"00:01:49,070","00:01:53,810",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,The other variables
cs-410_2_2_26,cs-410,2,2,"00:01:53,810","00:01:58,350",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,involved in the formula include
cs-410_2_2_27,cs-410,2,2,"00:02:01,440","00:02:06,100",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"W in the query, and"
cs-410_2_2_28,cs-410,2,2,"00:02:07,650","00:02:12,150",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"If you look at this document again,"
cs-410_2_2_29,cs-410,2,2,"00:02:12,150","00:02:16,710",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,the reason why it hasn't
cs-410_2_2_30,cs-410,2,2,"00:02:16,710","00:02:21,035",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,it has a very high count of campaign.
cs-410_2_2_31,cs-410,2,2,"00:02:21,035","00:02:27,170",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,So the count of campaign in this document
cs-410_2_2_32,cs-410,2,2,"00:02:27,170","00:02:31,580",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"the other documents, and has contributed"
cs-410_2_2_33,cs-410,2,2,"00:02:31,580","00:02:35,485",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,So in treating the amount
cs-410_2_2_34,cs-410,2,2,"00:02:35,485","00:02:40,695",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,"this document, we need to somehow"
cs-410_2_2_35,cs-410,2,2,"00:02:40,695","00:02:44,514",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,of the matching of this
cs-410_2_2_36,cs-410,2,2,"00:02:44,514","00:02:49,934",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,And if you think about the matching
cs-410_2_2_37,cs-410,2,2,"00:02:49,934","00:02:52,193",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"you actually would realize,"
cs-410_2_2_38,cs-410,2,2,"00:02:52,193","00:02:57,540",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,we probably shouldn't reward
cs-410_2_2_39,cs-410,2,2,"00:02:57,540","00:03:02,406",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,"And by that I mean,"
cs-410_2_2_40,cs-410,2,2,"00:03:02,406","00:03:06,680",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,says a lot about
cs-410_2_2_41,cs-410,2,2,"00:03:06,680","00:03:11,570",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,because it goes from zero
cs-410_2_2_42,cs-410,2,2,"00:03:11,570","00:03:15,370",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,And that increase means a lot.
cs-410_2_2_43,cs-410,2,2,"00:03:17,160","00:03:19,277",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,"Once we see a word in the document,"
cs-410_2_2_44,cs-410,2,2,"00:03:19,277","00:03:23,219",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,it's very likely that the document
cs-410_2_2_45,cs-410,2,2,"00:03:23,219","00:03:27,934",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,If we see a extra occurrence on
cs-410_2_2_46,cs-410,2,2,"00:03:27,934","00:03:33,493",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,"that is to go from one to two,"
cs-410_2_2_47,cs-410,2,2,"00:03:33,493","00:03:39,844",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,occurrence kind of confirmed that it's
cs-410_2_2_48,cs-410,2,2,"00:03:39,844","00:03:44,220",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,Now we are more sure that this
cs-410_2_2_49,cs-410,2,2,"00:03:44,220","00:03:50,430",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,"But imagine we have seen, let's say,"
cs-410_2_2_50,cs-410,2,2,"00:03:50,430","00:03:56,140",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,"Now, adding one extra occurrence is not"
cs-410_2_2_51,cs-410,2,2,"00:03:56,140","00:03:59,580",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,because we're already sure that
cs-410_2_2_52,cs-410,2,2,"00:04:01,160","00:04:06,656",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,"So if you're thinking this way, it seems"
cs-410_2_2_53,cs-410,2,2,"00:04:06,656","00:04:12,785",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"of a high count of a term, and"
cs-410_2_2_54,cs-410,2,2,"00:04:12,785","00:04:17,965",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,So this transformation function is
cs-410_2_2_55,cs-410,2,2,"00:04:17,965","00:04:22,990",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,word into a term frequency weight for
cs-410_2_2_56,cs-410,2,2,"00:04:22,990","00:04:28,420",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"So here I show in x axis that we'll count,"
cs-410_2_2_57,cs-410,2,2,"00:04:28,420","00:04:31,470",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,y axis I show the term frequency weight.
cs-410_2_2_58,cs-410,2,2,"00:04:33,360","00:04:36,370",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"So in the previous breaking functions,"
cs-410_2_2_59,cs-410,2,2,"00:04:36,370","00:04:41,140",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,we actually have imprison rate
cs-410_2_2_60,cs-410,2,2,"00:04:41,140","00:04:43,480",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,"So for example,"
cs-410_2_2_61,cs-410,2,2,"00:04:44,960","00:04:49,070",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,we actually use such a transformation
cs-410_2_2_62,cs-410,2,2,"00:04:49,070","00:04:53,420",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,"Basically if the count is 0,"
cs-410_2_2_63,cs-410,2,2,"00:04:53,420","00:04:56,790",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,otherwise it would have a weight of 1.
cs-410_2_2_64,cs-410,2,2,"00:04:56,790","00:04:57,940",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,It's flat.
cs-410_2_2_65,cs-410,2,2,"00:04:59,550","00:05:04,870",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,"Now, what about using"
cs-410_2_2_66,cs-410,2,2,"00:05:04,870","00:05:10,515",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"Well, that's a linear function, so it has"
cs-410_2_2_67,cs-410,2,2,"00:05:11,575","00:05:16,775",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,Now we have just seen that
cs-410_2_2_68,cs-410,2,2,"00:05:18,395","00:05:20,695",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,So what we want is something like this.
cs-410_2_2_69,cs-410,2,2,"00:05:20,695","00:05:23,160",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,"So for example,"
cs-410_2_2_70,cs-410,2,2,"00:05:23,160","00:05:26,620",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,we can't have a sublinear
cs-410_2_2_71,cs-410,2,2,"00:05:26,620","00:05:29,850",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,And this will control the influence
cs-410_2_2_72,cs-410,2,2,"00:05:29,850","00:05:32,270",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,because it's going to lower its inference.
cs-410_2_2_73,cs-410,2,2,"00:05:32,270","00:05:35,060",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,"Yet, it will retain"
cs-410_2_2_74,cs-410,2,2,"00:05:36,110","00:05:41,570",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,Or we might want to even bend the curve
cs-410_2_2_75,cs-410,2,2,"00:05:42,730","00:05:45,320",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,Now people have tried all these methods.
cs-410_2_2_76,cs-410,2,2,"00:05:45,320","00:05:48,870",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,And they are indeed working better than
cs-410_2_2_77,cs-410,2,2,"00:05:50,230","00:05:54,820",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,"But so far, what works the best seems"
cs-410_2_2_78,cs-410,2,2,"00:05:54,820","00:05:56,620",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,called a BM25 transformation.
cs-410_2_2_79,cs-410,2,2,"00:05:58,070","00:05:59,480",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,BM stands for best matching.
cs-410_2_2_80,cs-410,2,2,"00:06:01,210","00:06:04,830",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,"Now in this transformation,"
cs-410_2_2_81,cs-410,2,2,"00:06:06,460","00:06:10,910",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,And this k controls the upper
cs-410_2_2_82,cs-410,2,2,"00:06:10,910","00:06:15,165",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,It's easy to see this
cs-410_2_2_83,cs-410,2,2,"00:06:15,165","00:06:21,748",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,because if you look at the x divided by
cs-410_2_2_84,cs-410,2,2,"00:06:21,748","00:06:28,060",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,then the numerator will never be able
cs-410_2_2_85,cs-410,2,2,"00:06:28,060","00:06:29,820",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,So it's upper bounded by k+1.
cs-410_2_2_86,cs-410,2,2,"00:06:29,820","00:06:34,540",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"Now, this is also difference between"
cs-410_2_2_87,cs-410,2,2,"00:06:34,540","00:06:35,660",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,a logarithm transformation.
cs-410_2_2_88,cs-410,2,2,"00:06:37,010","00:06:38,450",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,Which it doesn't have upper bound.
cs-410_2_2_89,cs-410,2,2,"00:06:39,830","00:06:44,490",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"Furthermore, one interesting property"
cs-410_2_2_90,cs-410,2,2,"00:06:45,610","00:06:50,310",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,we can actually simulate different
cs-410_2_2_91,cs-410,2,2,"00:06:50,310","00:06:52,900",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,Including the two extremes
cs-410_2_2_92,cs-410,2,2,"00:06:52,900","00:06:57,480",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"That is, the 0/1 bit transformation and"
cs-410_2_2_93,cs-410,2,2,"00:06:57,480","00:07:01,890",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,"So for example, if we set k to 0,"
cs-410_2_2_94,cs-410,2,2,"00:07:03,630","00:07:06,710",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,the function value will be 1.
cs-410_2_2_95,cs-410,2,2,"00:07:06,710","00:07:13,250",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,So we precisely recover
cs-410_2_2_96,cs-410,2,2,"00:07:15,630","00:07:20,040",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,If you set k to very large
cs-410_2_2_97,cs-410,2,2,"00:07:20,040","00:07:22,919",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,it's going to look more like
cs-410_2_2_98,cs-410,2,2,"00:07:24,980","00:07:29,400",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"So in this sense,"
cs-410_2_2_99,cs-410,2,2,"00:07:29,400","00:07:34,600",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,It allows us to control
cs-410_2_2_100,cs-410,2,2,"00:07:34,600","00:07:36,780",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,It also has a nice property
cs-410_2_2_101,cs-410,2,2,"00:07:38,020","00:07:42,390",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,And this upper bound is useful to control
cs-410_2_2_102,cs-410,2,2,"00:07:43,860","00:07:49,718",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,And so that we can prevent a spammer
cs-410_2_2_103,cs-410,2,2,"00:07:49,718","00:07:54,947",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,of one term to spam all queries
cs-410_2_2_104,cs-410,2,2,"00:07:57,258","00:08:00,824",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,"In other words, this upper bound"
cs-410_2_2_105,cs-410,2,2,"00:08:00,824","00:08:05,330",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,terms would be counted when we aggregate
cs-410_2_2_106,cs-410,2,2,"00:08:06,680","00:08:10,620",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,"As I said, this transformation"
cs-410_2_2_107,cs-410,2,2,"00:08:12,300","00:08:16,890",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,"So to summarize this lecture,"
cs-410_2_2_108,cs-410,2,2,"00:08:16,890","00:08:21,930",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"Sublinear TF Transformation,"
cs-410_2_2_109,cs-410,2,2,"00:08:21,930","00:08:25,550",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,capture the intuition of diminishing
cs-410_2_2_110,cs-410,2,2,"00:08:26,620","00:08:30,980",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,It's also to avoid the dominance by
cs-410_2_2_111,cs-410,2,2,"00:08:30,980","00:08:37,050",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,This BM25 transformation that we
cs-410_2_2_112,cs-410,2,2,"00:08:37,050","00:08:43,130",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,It's so far one of the best-performing
cs-410_2_2_113,cs-410,2,2,"00:08:43,130","00:08:46,520",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,"It has upper bound, and so"
cs-410_2_2_114,cs-410,2,2,"00:08:47,830","00:08:54,080",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,Now if we're plugging this function into
cs-410_2_2_115,cs-410,2,2,"00:08:54,080","00:08:57,730",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,Then we'd end up having
cs-410_2_2_116,cs-410,2,2,"00:08:57,730","00:09:00,720",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,which has a BM25 TF component.
cs-410_2_2_117,cs-410,2,2,"00:09:01,870","00:09:06,833",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,"Now, this is already"
cs-410_2_2_118,cs-410,2,2,"00:09:06,833","00:09:11,537",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,the odd ranking function called BM25.
cs-410_2_2_119,cs-410,2,2,"00:09:11,537","00:09:17,890",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,And we'll discuss how we can further
cs-410_2_2_120,cs-410,2,2,"00:09:17,890","00:09:27,890",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,[MUSIC]
cs-410_2_3_1,cs-410,2,3,"00:00:00,012","00:00:06,908",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_2_3_2,cs-410,2,3,"00:00:06,908","00:00:13,498",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,lecture is about the basic measures for
cs-410_2_3_3,cs-410,2,3,"00:00:13,498","00:00:18,955",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In this lecture,"
cs-410_2_3_4,cs-410,2,3,"00:00:18,955","00:00:24,528",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,measures to quantitatively
cs-410_2_3_5,cs-410,2,3,"00:00:24,528","00:00:29,163",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,This is a slide that you have seen
cs-410_2_3_6,cs-410,2,3,"00:00:29,163","00:00:32,318",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,about the Granville
cs-410_2_3_7,cs-410,2,3,"00:00:32,318","00:00:39,122",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,We can have a test faction that consists
cs-410_2_3_8,cs-410,2,3,"00:00:39,122","00:00:47,930",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,We can then run two systems on these
cs-410_2_3_9,cs-410,2,3,"00:00:47,930","00:00:49,528",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,Their performance.
cs-410_2_3_10,cs-410,2,3,"00:00:49,528","00:00:54,828",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,"And we raise the question,"
cs-410_2_3_11,cs-410,2,3,"00:00:54,828","00:00:57,928",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,Is system A better or is system B better?
cs-410_2_3_12,cs-410,2,3,"00:00:57,928","00:01:02,398",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,So let's now talk about how to
cs-410_2_3_13,cs-410,2,3,"00:01:02,398","00:01:07,841",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,Suppose we have a total of 10 relevant
cs-410_2_3_14,cs-410,2,3,"00:01:07,841","00:01:08,848",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,this query.
cs-410_2_3_15,cs-410,2,3,"00:01:08,848","00:01:15,162",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,"Now, the relevant judgments show on"
cs-410_2_3_16,cs-410,2,3,"00:01:15,162","00:01:19,908",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,"And we have only seen 3 [INAUDIBLE] there,"
cs-410_2_3_17,cs-410,2,3,"00:01:19,908","00:01:26,133",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,"But, we can imagine there are other Random"
cs-410_2_3_18,cs-410,2,3,"00:01:26,133","00:01:30,895",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,"So now, intuitively,"
cs-410_2_3_19,cs-410,2,3,"00:01:30,895","00:01:35,668",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,A is better because it
cs-410_2_3_20,cs-410,2,3,"00:01:35,668","00:01:42,019",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,And in particular we have seen
cs-410_2_3_21,cs-410,2,3,"00:01:42,019","00:01:46,251",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"two of them are relevant but in system B,"
cs-410_2_3_22,cs-410,2,3,"00:01:46,251","00:01:52,248",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,we have five results and
cs-410_2_3_23,cs-410,2,3,"00:01:52,248","00:01:56,418",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,So intuitively it looks like
cs-410_2_3_24,cs-410,2,3,"00:01:56,418","00:02:00,670",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,And this infusion can be captured
cs-410_2_3_25,cs-410,2,3,"00:02:00,670","00:02:05,866",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,where we simply compute to what extent
cs-410_2_3_26,cs-410,2,3,"00:02:05,866","00:02:07,788",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"If you have 100% position,"
cs-410_2_3_27,cs-410,2,3,"00:02:07,788","00:02:11,638",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,that would mean that all
cs-410_2_3_28,cs-410,2,3,"00:02:11,638","00:02:16,476",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,So in this case system A has
cs-410_2_3_29,cs-410,2,3,"00:02:16,476","00:02:20,606",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,three System B has some
cs-410_2_3_30,cs-410,2,3,"00:02:20,606","00:02:25,208",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,this shows that system
cs-410_2_3_31,cs-410,2,3,"00:02:25,208","00:02:30,065",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,But we also talked about System B
cs-410_2_3_32,cs-410,2,3,"00:02:30,065","00:02:35,220",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,would like to retrieve as many
cs-410_2_3_33,cs-410,2,3,"00:02:35,220","00:02:39,839",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,So in that case we'll have to compare
cs-410_2_3_34,cs-410,2,3,"00:02:39,839","00:02:42,940",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,retrieve and
cs-410_2_3_35,cs-410,2,3,"00:02:42,940","00:02:48,000",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,This method uses the completeness
cs-410_2_3_36,cs-410,2,3,"00:02:48,000","00:02:51,090",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,In your retrieval result.
cs-410_2_3_37,cs-410,2,3,"00:02:51,090","00:02:57,500",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,So we just assume that there are ten
cs-410_2_3_38,cs-410,2,3,"00:02:57,500","00:03:01,510",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,"And here we've got two of them,"
cs-410_2_3_39,cs-410,2,3,"00:03:01,510","00:03:04,130",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,So the recall is 2 out of 10.
cs-410_2_3_40,cs-410,2,3,"00:03:04,130","00:03:07,630",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,"Whereas System B has called a 3,"
cs-410_2_3_41,cs-410,2,3,"00:03:07,630","00:03:11,278",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,Now we can see by recall
cs-410_2_3_42,cs-410,2,3,"00:03:11,278","00:03:15,240",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,And these two measures turn out to
cs-410_2_3_43,cs-410,2,3,"00:03:15,240","00:03:16,978",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,evaluating search engine.
cs-410_2_3_44,cs-410,2,3,"00:03:16,978","00:03:21,824",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,And they are very important because
cs-410_2_3_45,cs-410,2,3,"00:03:21,824","00:03:24,298",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,other test evaluation problems.
cs-410_2_3_46,cs-410,2,3,"00:03:24,298","00:03:28,660",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,"For example, if you look at"
cs-410_2_3_47,cs-410,2,3,"00:03:28,660","00:03:34,030",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,you tend to see precision recall numbers
cs-410_2_3_48,cs-410,2,3,"00:03:35,290","00:03:38,520",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"Okay so, now let's define these"
cs-410_2_3_49,cs-410,2,3,"00:03:38,520","00:03:44,410",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,And these measures are to evaluate a set
cs-410_2_3_50,cs-410,2,3,"00:03:44,410","00:03:48,950",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,we are considering that approximation
cs-410_2_3_51,cs-410,2,3,"00:03:50,100","00:03:53,300",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,We can distinguish 4 cases depending
cs-410_2_3_52,cs-410,2,3,"00:03:53,300","00:03:59,720",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,A document can be retrieved or
cs-410_2_3_53,cs-410,2,3,"00:03:59,720","00:04:01,570",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,Because we are talking
cs-410_2_3_54,cs-410,2,3,"00:04:02,710","00:04:05,640",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,A document can be also relevant or
cs-410_2_3_55,cs-410,2,3,"00:04:05,640","00:04:10,310",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,not relevant depending on whether the user
cs-410_2_3_56,cs-410,2,3,"00:04:11,950","00:04:16,890",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,So we can now have counts of documents in.
cs-410_2_3_57,cs-410,2,3,"00:04:16,890","00:04:21,610",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,Each of the four categories again
cs-410_2_3_58,cs-410,2,3,"00:04:21,610","00:04:24,420",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,documents that have been retrieved and
cs-410_2_3_59,cs-410,2,3,"00:04:24,420","00:04:30,530",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,B for documents that are not retrieved but
cs-410_2_3_60,cs-410,2,3,"00:04:31,750","00:04:35,550",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,No with this table then
cs-410_2_3_61,cs-410,2,3,"00:04:36,690","00:04:42,450",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,As the ratio of the relevant
cs-410_2_3_62,cs-410,2,3,"00:04:42,450","00:04:47,440",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,retrieved documents A to the total
cs-410_2_3_63,cs-410,2,3,"00:04:48,450","00:04:53,390",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,"So, this is just A divided"
cs-410_2_3_64,cs-410,2,3,"00:04:53,390","00:04:55,640",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,The sum of this column.
cs-410_2_3_65,cs-410,2,3,"00:04:56,820","00:05:04,360",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,Singularly recall is defined by
cs-410_2_3_66,cs-410,2,3,"00:05:04,360","00:05:07,470",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,So that's again to divide a by.
cs-410_2_3_67,cs-410,2,3,"00:05:07,470","00:05:10,360",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,The sum of the row instead of the column.
cs-410_2_3_68,cs-410,2,3,"00:05:10,360","00:05:15,810",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"All right, so we can see precision and"
cs-410_2_3_69,cs-410,2,3,"00:05:16,930","00:05:20,000",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,that's the number of
cs-410_2_3_70,cs-410,2,3,"00:05:20,000","00:05:22,449",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,But we're going to use
cs-410_2_3_71,cs-410,2,3,"00:05:23,590","00:05:27,300",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,"Okay, so what would be an ideal result."
cs-410_2_3_72,cs-410,2,3,"00:05:27,300","00:05:31,330",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,"Well, you can easily see being"
cs-410_2_3_73,cs-410,2,3,"00:05:31,330","00:05:34,060",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,recall oil to be 1.0.
cs-410_2_3_74,cs-410,2,3,"00:05:34,060","00:05:39,510",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,That means We have got 1% of
cs-410_2_3_75,cs-410,2,3,"00:05:39,510","00:05:44,770",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"in our results, and all of the results"
cs-410_2_3_76,cs-410,2,3,"00:05:44,770","00:05:47,540",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,At least there's no single
cs-410_2_3_77,cs-410,2,3,"00:05:48,680","00:05:53,920",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"In reality, however, high recall tends"
cs-410_2_3_78,cs-410,2,3,"00:05:53,920","00:05:56,210",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,And you can imagine why that's the case.
cs-410_2_3_79,cs-410,2,3,"00:05:56,210","00:06:00,790",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,As you go down the to try to get as
cs-410_2_3_80,cs-410,2,3,"00:06:00,790","00:06:05,890",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,"you tend to encounter a lot of documents,"
cs-410_2_3_81,cs-410,2,3,"00:06:05,890","00:06:11,450",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,Note that this set can also
cs-410_2_3_82,cs-410,2,3,"00:06:11,450","00:06:15,490",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"In the rest of this, that's why although"
cs-410_2_3_83,cs-410,2,3,"00:06:15,490","00:06:20,560",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,"retrieve the documents, they are actually"
cs-410_2_3_84,cs-410,2,3,"00:06:20,560","00:06:24,270",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,They are the fundamental measures in
cs-410_2_3_85,cs-410,2,3,"00:06:24,270","00:06:30,010",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,We often are interested in The precision
cs-410_2_3_86,cs-410,2,3,"00:06:30,010","00:06:33,400",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,This means we look at how many documents
cs-410_2_3_87,cs-410,2,3,"00:06:33,400","00:06:35,870",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,among the top ten results
cs-410_2_3_88,cs-410,2,3,"00:06:35,870","00:06:38,290",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,"Now, this is a very meaningful measure,"
cs-410_2_3_89,cs-410,2,3,"00:06:38,290","00:06:43,780",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,because it tells us how many relevant
cs-410_2_3_90,cs-410,2,3,"00:06:43,780","00:06:47,790",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,On the first page of where they
cs-410_2_3_91,cs-410,2,3,"00:06:50,000","00:06:55,780",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,So precision and recall
cs-410_2_3_92,cs-410,2,3,"00:06:55,780","00:07:02,040",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,use them to further evaluate a search
cs-410_2_3_93,cs-410,2,3,"00:07:03,460","00:07:07,210",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,We just said that there tends to be
cs-410_2_3_94,cs-410,2,3,"00:07:07,210","00:07:10,730",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,so naturally it would be
cs-410_2_3_95,cs-410,2,3,"00:07:10,730","00:07:15,490",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,"And here's one method that's often used,"
cs-410_2_3_96,cs-410,2,3,"00:07:15,490","00:07:21,020",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,it's a [INAUDIBLE] mean of precision and
cs-410_2_3_97,cs-410,2,3,"00:07:22,450","00:07:27,741",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,"So, you can see at first, compute the."
cs-410_2_3_98,cs-410,2,3,"00:07:29,210","00:07:34,360",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,"Inverse of R and P here,"
cs-410_2_3_99,cs-410,2,3,"00:07:34,360","00:07:41,180",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,the 2 by using coefficients
cs-410_2_3_100,cs-410,2,3,"00:07:42,850","00:07:47,029",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,And after some transformation you can
cs-410_2_3_101,cs-410,2,3,"00:07:49,010","00:07:51,790",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,And in any case it just becomes
cs-410_2_3_102,cs-410,2,3,"00:07:51,790","00:07:56,360",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,"recall, and beta is a parameter,"
cs-410_2_3_103,cs-410,2,3,"00:07:56,360","00:08:01,572",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,It can control the emphasis
cs-410_2_3_104,cs-410,2,3,"00:08:01,572","00:08:08,360",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,set beta to 1 We end up having a special
cs-410_2_3_105,cs-410,2,3,"00:08:08,360","00:08:13,460",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,This is a popular measure that's often
cs-410_2_3_106,cs-410,2,3,"00:08:13,460","00:08:14,780",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,And the formula looks very simple.
cs-410_2_3_107,cs-410,2,3,"00:08:16,170","00:08:17,948",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"It's just this, here."
cs-410_2_3_108,cs-410,2,3,"00:08:20,718","00:08:24,668",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,Now it's easy to see that if
cs-410_2_3_109,cs-410,2,3,"00:08:24,668","00:08:28,570",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,larger recall than f
cs-410_2_3_110,cs-410,2,3,"00:08:28,570","00:08:32,940",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,"But, what's interesting is that"
cs-410_2_3_111,cs-410,2,3,"00:08:32,940","00:08:36,260",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,recall is captured
cs-410_2_3_112,cs-410,2,3,"00:08:36,260","00:08:41,000",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"So, in order to understand that, we"
cs-410_2_3_113,cs-410,2,3,"00:08:42,170","00:08:48,270",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,can first look at the natural
cs-410_2_3_114,cs-410,2,3,"00:08:48,270","00:08:53,090",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,using the symbol arithmetically
cs-410_2_3_115,cs-410,2,3,"00:08:53,090","00:09:00,730",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,That would be likely the most natural way
cs-410_2_3_116,cs-410,2,3,"00:09:01,870","00:09:05,940",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,"If you want to think more,"
cs-410_2_3_117,cs-410,2,3,"00:09:07,940","00:09:10,960",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,So why is this not as good as F1?
cs-410_2_3_118,cs-410,2,3,"00:09:13,550","00:09:15,038",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,Or what's the problem with this?
cs-410_2_3_119,cs-410,2,3,"00:09:18,121","00:09:23,270",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,"Now, if you think about"
cs-410_2_3_120,cs-410,2,3,"00:09:23,270","00:09:28,300",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,you can see this is
cs-410_2_3_121,cs-410,2,3,"00:09:28,300","00:09:31,870",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,"In this case,"
cs-410_2_3_122,cs-410,2,3,"00:09:31,870","00:09:36,580",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"In the case of a sum, the total value"
cs-410_2_3_123,cs-410,2,3,"00:09:36,580","00:09:42,850",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,that means if you have a very high P or
cs-410_2_3_124,cs-410,2,3,"00:09:42,850","00:09:47,820",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,don't care about whether the other value
cs-410_2_3_125,cs-410,2,3,"00:09:47,820","00:09:53,920",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,Now this is not desirable because one
cs-410_2_3_126,cs-410,2,3,"00:09:53,920","00:09:57,110",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,We have perfect recall easily.
cs-410_2_3_127,cs-410,2,3,"00:09:57,110","00:09:58,140",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,Can we imagine how?
cs-410_2_3_128,cs-410,2,3,"00:09:59,810","00:10:03,830",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,It's probably very easy to
cs-410_2_3_129,cs-410,2,3,"00:10:03,830","00:10:06,399",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,all the documents in the collection and
cs-410_2_3_130,cs-410,2,3,"00:10:07,420","00:10:11,130",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,And this will give us 0.5 as the average.
cs-410_2_3_131,cs-410,2,3,"00:10:11,130","00:10:15,583",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,But such results are clearly not
cs-410_2_3_132,cs-410,2,3,"00:10:15,583","00:10:20,350",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,though the average using this
cs-410_2_3_133,cs-410,2,3,"00:10:21,750","00:10:25,930",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,In contrast you can see F 1 would
cs-410_2_3_134,cs-410,2,3,"00:10:25,930","00:10:27,750",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,"recall are roughly That seminar, so"
cs-410_2_3_135,cs-410,2,3,"00:10:27,750","00:10:33,360",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,it would a case where you had
cs-410_2_3_136,cs-410,2,3,"00:10:35,320","00:10:38,360",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,So this means f one encodes
cs-410_2_3_137,cs-410,2,3,"00:10:38,360","00:10:43,690",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,Now this example shows
cs-410_2_3_138,cs-410,2,3,"00:10:43,690","00:10:44,230",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,Methodology here.
cs-410_2_3_139,cs-410,2,3,"00:10:44,230","00:10:49,950",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,But when you try to solve a problem you
cs-410_2_3_140,cs-410,2,3,"00:10:49,950","00:10:52,120",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,let's say in this it's
cs-410_2_3_141,cs-410,2,3,"00:10:53,790","00:10:57,160",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,But it's important not to
cs-410_2_3_142,cs-410,2,3,"00:10:57,160","00:11:00,790",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,It's important to think whether you
cs-410_2_3_143,cs-410,2,3,"00:11:02,170","00:11:06,180",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,And once you think about the multiple
cs-410_2_3_144,cs-410,2,3,"00:11:06,180","00:11:10,930",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,"difference, and then think about"
cs-410_2_3_145,cs-410,2,3,"00:11:10,930","00:11:13,280",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,"In this case, if you think more carefully,"
cs-410_2_3_146,cs-410,2,3,"00:11:13,280","00:11:15,920",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,you will think that F1
cs-410_2_3_147,cs-410,2,3,"00:11:15,920","00:11:18,300",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,Than the simple.
cs-410_2_3_148,cs-410,2,3,"00:11:18,300","00:11:21,670",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,Although in other cases there
cs-410_2_3_149,cs-410,2,3,"00:11:21,670","00:11:25,858",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,But in this case the seems not reasonable.
cs-410_2_3_150,cs-410,2,3,"00:11:25,858","00:11:29,260",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,But if you don't pay attention
cs-410_2_3_151,cs-410,2,3,"00:11:29,260","00:11:33,780",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,you might just take a easy way to
cs-410_2_3_152,cs-410,2,3,"00:11:33,780","00:11:37,360",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,"And here later, you will find that,"
cs-410_2_3_153,cs-410,2,3,"00:11:37,360","00:11:38,620",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,All right.
cs-410_2_3_154,cs-410,2,3,"00:11:38,620","00:11:43,760",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,So this methodology is actually very
cs-410_2_3_155,cs-410,2,3,"00:11:43,760","00:11:46,020",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=703,Try to think about the best solution.
cs-410_2_3_156,cs-410,2,3,"00:11:46,020","00:11:50,890",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,"Try to understand the problem very well,"
cs-410_2_3_157,cs-410,2,3,"00:11:50,890","00:11:55,890",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,"know why you needed this measure, and why"
cs-410_2_3_158,cs-410,2,3,"00:11:55,890","00:11:59,530",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,And then use that to guide you in
cs-410_2_3_159,cs-410,2,3,"00:12:03,320","00:12:08,510",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,"To summarize, we talked about"
cs-410_2_3_160,cs-410,2,3,"00:12:08,510","00:12:11,530",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,are there retrievable
cs-410_2_3_161,cs-410,2,3,"00:12:11,530","00:12:13,690",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,We also talk about the Recall.
cs-410_2_3_162,cs-410,2,3,"00:12:13,690","00:12:17,260",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,"Which addresses the question, have all of"
cs-410_2_3_163,cs-410,2,3,"00:12:17,260","00:12:21,250",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,"These two, are the two,"
cs-410_2_3_164,cs-410,2,3,"00:12:21,250","00:12:25,270",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,They are used for
cs-410_2_3_165,cs-410,2,3,"00:12:25,270","00:12:28,670",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,We talk about F measure as a way to
cs-410_2_3_166,cs-410,2,3,"00:12:29,970","00:12:33,600",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,We also talked about the tradeoff
cs-410_2_3_167,cs-410,2,3,"00:12:33,600","00:12:38,140",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,And this turns out to depend
cs-410_2_3_168,cs-410,2,3,"00:12:38,140","00:12:42,133",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,we'll discuss this point
cs-410_2_3_169,cs-410,2,3,"00:12:42,133","00:12:52,133",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=762,[MUSIC]
cs-410_2_4_1,cs-410,2,4,"00:00:07,780","00:00:12,596",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,[SOUND] This lecture is about
cs-410_2_4_2,cs-410,2,4,"00:00:12,596","00:00:13,737",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture,"
cs-410_2_4_3,cs-410,2,4,"00:00:13,737","00:00:18,445",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,we're going to give an introduction
cs-410_2_4_4,cs-410,2,4,"00:00:18,445","00:00:23,305",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,This has to do with how do you model
cs-410_2_4_5,cs-410,2,4,"00:00:23,305","00:00:28,272",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,So it's related to how we model
cs-410_2_4_6,cs-410,2,4,"00:00:31,828","00:00:34,032",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,We're going to talk about
cs-410_2_4_7,cs-410,2,4,"00:00:34,032","00:00:37,688",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,And then we're going to talk about the
cs-410_2_4_8,cs-410,2,4,"00:00:37,688","00:00:42,770",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,"language model, which also happens to be"
cs-410_2_4_9,cs-410,2,4,"00:00:42,770","00:00:45,420",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,"And finally, what this class"
cs-410_2_4_10,cs-410,2,4,"00:00:47,200","00:00:48,750",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,What is a language model?
cs-410_2_4_11,cs-410,2,4,"00:00:48,750","00:00:53,570",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"Well, it's just a probability"
cs-410_2_4_12,cs-410,2,4,"00:00:53,570","00:00:54,540",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"So here, I'll show one."
cs-410_2_4_13,cs-410,2,4,"00:00:55,870","00:01:00,430",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,This model gives the sequence Today
cs-410_2_4_14,cs-410,2,4,"00:01:00,430","00:01:03,830",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"It give Today Wednesday is a very,"
cs-410_2_4_15,cs-410,2,4,"00:01:03,830","00:01:09,705",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,very small probability
cs-410_2_4_16,cs-410,2,4,"00:01:11,796","00:01:15,447",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,You can see the probabilities
cs-410_2_4_17,cs-410,2,4,"00:01:15,447","00:01:19,670",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,sequences of words can vary
cs-410_2_4_18,cs-410,2,4,"00:01:19,670","00:01:23,256",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,"Therefore, it's clearly context dependent."
cs-410_2_4_19,cs-410,2,4,"00:01:23,256","00:01:24,552",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"In ordinary conversation,"
cs-410_2_4_20,cs-410,2,4,"00:01:24,552","00:01:28,510",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,probably Today is Wednesday is most
cs-410_2_4_21,cs-410,2,4,"00:01:28,510","00:01:32,132",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,Imagine in the context of
cs-410_2_4_22,cs-410,2,4,"00:01:32,132","00:01:36,890",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"maybe the eigenvalue is positive,"
cs-410_2_4_23,cs-410,2,4,"00:01:36,890","00:01:41,080",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,This means it can be used to
cs-410_2_4_24,cs-410,2,4,"00:01:42,240","00:01:45,900",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,The model can also be regarded
cs-410_2_4_25,cs-410,2,4,"00:01:45,900","00:01:46,950",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,generating text.
cs-410_2_4_26,cs-410,2,4,"00:01:46,950","00:01:51,660",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,And this is why it's also often
cs-410_2_4_27,cs-410,2,4,"00:01:51,660","00:01:52,910",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,So what does that mean?
cs-410_2_4_28,cs-410,2,4,"00:01:52,910","00:01:58,540",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,We can imagine this is a mechanism that's
cs-410_2_4_29,cs-410,2,4,"00:01:58,540","00:02:05,340",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,visualised here as a stochastic system
cs-410_2_4_30,cs-410,2,4,"00:02:05,340","00:02:08,608",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"So, we can ask for a sequence,"
cs-410_2_4_31,cs-410,2,4,"00:02:08,608","00:02:13,548",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,"a sequence from the device if you want,"
cs-410_2_4_32,cs-410,2,4,"00:02:13,548","00:02:18,420",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,"Today is Wednesday, but it could"
cs-410_2_4_33,cs-410,2,4,"00:02:18,420","00:02:21,940",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,"So for example,"
cs-410_2_4_34,cs-410,2,4,"00:02:24,086","00:02:28,418",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"So in this sense,"
cs-410_2_4_35,cs-410,2,4,"00:02:28,418","00:02:32,656",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,a sample observed from
cs-410_2_4_36,cs-410,2,4,"00:02:32,656","00:02:33,840",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,"So, why is such a model useful?"
cs-410_2_4_37,cs-410,2,4,"00:02:33,840","00:02:39,720",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,"Well, it's mainly because it can quantify"
cs-410_2_4_38,cs-410,2,4,"00:02:39,720","00:02:41,190",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,Where do uncertainties come from?
cs-410_2_4_39,cs-410,2,4,"00:02:41,190","00:02:45,690",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,"Well, one source is simply"
cs-410_2_4_40,cs-410,2,4,"00:02:45,690","00:02:48,870",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,that we discussed earlier in the lecture.
cs-410_2_4_41,cs-410,2,4,"00:02:48,870","00:02:52,240",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,Another source is because we don't
cs-410_2_4_42,cs-410,2,4,"00:02:52,240","00:02:55,300",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,we lack all the knowledge
cs-410_2_4_43,cs-410,2,4,"00:02:55,300","00:02:58,420",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,"In that case,"
cs-410_2_4_44,cs-410,2,4,"00:02:58,420","00:03:01,800",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,So let me show some examples of questions
cs-410_2_4_45,cs-410,2,4,"00:03:01,800","00:03:06,220",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,that would have interesting
cs-410_2_4_46,cs-410,2,4,"00:03:06,220","00:03:11,641",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"Given that we see John and feels,"
cs-410_2_4_47,cs-410,2,4,"00:03:11,641","00:03:16,866",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,as opposed to habit as the next
cs-410_2_4_48,cs-410,2,4,"00:03:16,866","00:03:21,123",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"Now, obviously, this would be very useful"
cs-410_2_4_49,cs-410,2,4,"00:03:21,123","00:03:25,180",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,"habit would have similar acoustic sound,"
cs-410_2_4_50,cs-410,2,4,"00:03:25,180","00:03:28,190",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,"But, if we look at the language model,"
cs-410_2_4_51,cs-410,2,4,"00:03:28,190","00:03:32,690",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,we know that John feels happy would be
cs-410_2_4_52,cs-410,2,4,"00:03:35,810","00:03:39,300",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"Another example, given that we"
cs-410_2_4_53,cs-410,2,4,"00:03:39,300","00:03:43,700",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"game once in a news article,"
cs-410_2_4_54,cs-410,2,4,"00:03:43,700","00:03:47,430",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,This obviously is related to text
cs-410_2_4_55,cs-410,2,4,"00:03:48,720","00:03:52,150",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"Also, given that a user is"
cs-410_2_4_56,cs-410,2,4,"00:03:52,150","00:03:55,570",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,how likely would the user
cs-410_2_4_57,cs-410,2,4,"00:03:55,570","00:03:58,530",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,"Now, this is clearly related"
cs-410_2_4_58,cs-410,2,4,"00:03:58,530","00:04:00,185",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,that we discussed in the previous lecture.
cs-410_2_4_59,cs-410,2,4,"00:04:02,180","00:04:05,710",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,"So now,"
cs-410_2_4_60,cs-410,2,4,"00:04:05,710","00:04:07,910",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,called a unigram language model.
cs-410_2_4_61,cs-410,2,4,"00:04:07,910","00:04:09,690",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,"In such a case,"
cs-410_2_4_62,cs-410,2,4,"00:04:09,690","00:04:13,550",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,we assume that we generate a text by
cs-410_2_4_63,cs-410,2,4,"00:04:14,760","00:04:19,356",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,So this means the probability of
cs-410_2_4_64,cs-410,2,4,"00:04:19,356","00:04:22,601",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,the product of
cs-410_2_4_65,cs-410,2,4,"00:04:22,601","00:04:25,800",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"Now normally,"
cs-410_2_4_66,cs-410,2,4,"00:04:25,800","00:04:30,270",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,So if you have single word in like
cs-410_2_4_67,cs-410,2,4,"00:04:30,270","00:04:35,470",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,likely to observe model than if
cs-410_2_4_68,cs-410,2,4,"00:04:35,470","00:04:37,780",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,So this assumption is not
cs-410_2_4_69,cs-410,2,4,"00:04:37,780","00:04:39,920",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,we make this assumption
cs-410_2_4_70,cs-410,2,4,"00:04:41,210","00:04:47,060",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,So now the model has precisely N
cs-410_2_4_71,cs-410,2,4,"00:04:47,060","00:04:51,380",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"We have one probability for each word, and"
cs-410_2_4_72,cs-410,2,4,"00:04:51,380","00:04:57,450",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,"So strictly speaking,"
cs-410_2_4_73,cs-410,2,4,"00:05:00,270","00:05:04,495",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,"As I said,"
cs-410_2_4_74,cs-410,2,4,"00:05:04,495","00:05:06,245",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,drawn from this word distribution.
cs-410_2_4_75,cs-410,2,4,"00:05:08,080","00:05:11,540",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"So for example,"
cs-410_2_4_76,cs-410,2,4,"00:05:11,540","00:05:18,020",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,the model to stochastically generate
cs-410_2_4_77,cs-410,2,4,"00:05:18,020","00:05:19,988",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,"So instead of giving a whole sequence,"
cs-410_2_4_78,cs-410,2,4,"00:05:19,988","00:05:23,900",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,"like Today is Wednesday,"
cs-410_2_4_79,cs-410,2,4,"00:05:23,900","00:05:26,200",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,And we can get all kinds of words.
cs-410_2_4_80,cs-410,2,4,"00:05:26,200","00:05:28,596",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,And we can assemble these
cs-410_2_4_81,cs-410,2,4,"00:05:28,596","00:05:32,304",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,So that will still allow you
cs-410_2_4_82,cs-410,2,4,"00:05:32,304","00:05:36,410",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,Today is Wednesday as the product
cs-410_2_4_83,cs-410,2,4,"00:05:37,420","00:05:43,380",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"As you can see, even though we have not"
cs-410_2_4_84,cs-410,2,4,"00:05:43,380","00:05:48,630",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,it actually allows us to compute
cs-410_2_4_85,cs-410,2,4,"00:05:48,630","00:05:53,550",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,this model now only needs N
cs-410_2_4_86,cs-410,2,4,"00:05:53,550","00:05:56,370",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,That means if we specify
cs-410_2_4_87,cs-410,2,4,"00:05:56,370","00:06:01,850",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"all the words, then the model's"
cs-410_2_4_88,cs-410,2,4,"00:06:01,850","00:06:06,220",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,"Whereas if we don't make this assumption,"
cs-410_2_4_89,cs-410,2,4,"00:06:06,220","00:06:09,720",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,all kinds of combinations
cs-410_2_4_90,cs-410,2,4,"00:06:11,830","00:06:16,720",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"So by making this assumption, it makes it"
cs-410_2_4_91,cs-410,2,4,"00:06:16,720","00:06:18,590",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,So let's see a specific example here.
cs-410_2_4_92,cs-410,2,4,"00:06:19,810","00:06:25,450",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,Here I show two unigram language
cs-410_2_4_93,cs-410,2,4,"00:06:25,450","00:06:28,050",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,And these are high probability
cs-410_2_4_94,cs-410,2,4,"00:06:29,800","00:06:33,290",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,The first one clearly suggests
cs-410_2_4_95,cs-410,2,4,"00:06:33,290","00:06:37,020",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,because the high probability
cs-410_2_4_96,cs-410,2,4,"00:06:37,020","00:06:38,700",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,The second one is more related to health.
cs-410_2_4_97,cs-410,2,4,"00:06:39,790","00:06:41,290",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"Now we can ask the question,"
cs-410_2_4_98,cs-410,2,4,"00:06:41,290","00:06:46,520",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,how likely were observe a particular
cs-410_2_4_99,cs-410,2,4,"00:06:46,520","00:06:49,920",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,Now suppose we sample
cs-410_2_4_100,cs-410,2,4,"00:06:49,920","00:06:53,150",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,"Let's say we take the first distribution,"
cs-410_2_4_101,cs-410,2,4,"00:06:53,150","00:06:56,140",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,What words do you think would be
cs-410_2_4_102,cs-410,2,4,"00:06:56,140","00:06:58,280",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,maybe mining maybe another word?
cs-410_2_4_103,cs-410,2,4,"00:06:58,280","00:06:58,860",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"Even food,"
cs-410_2_4_104,cs-410,2,4,"00:06:58,860","00:07:02,300",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"which has a very small probability,"
cs-410_2_4_105,cs-410,2,4,"00:07:03,880","00:07:06,890",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,"But in general, high probability"
cs-410_2_4_106,cs-410,2,4,"00:07:08,130","00:07:11,200",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,So we can imagine what general text
cs-410_2_4_107,cs-410,2,4,"00:07:12,230","00:07:14,630",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,"In fact, with small probability,"
cs-410_2_4_108,cs-410,2,4,"00:07:14,630","00:07:19,940",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,you might be able to actually generate
cs-410_2_4_109,cs-410,2,4,"00:07:19,940","00:07:23,660",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,"Now, it will actually be meaningful,"
cs-410_2_4_110,cs-410,2,4,"00:07:23,660","00:07:24,400",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,very small.
cs-410_2_4_111,cs-410,2,4,"00:07:26,100","00:07:30,220",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"In an extreme case, you might"
cs-410_2_4_112,cs-410,2,4,"00:07:30,220","00:07:35,980",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,a text mining paper that would be
cs-410_2_4_113,cs-410,2,4,"00:07:35,980","00:07:39,866",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,"And in that case,"
cs-410_2_4_114,cs-410,2,4,"00:07:39,866","00:07:42,152",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,"But it's a non-zero probability,"
cs-410_2_4_115,cs-410,2,4,"00:07:42,152","00:07:45,850",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,if we assume none of the words
cs-410_2_4_116,cs-410,2,4,"00:07:47,430","00:07:49,380",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,"Similarly from the second topic,"
cs-410_2_4_117,cs-410,2,4,"00:07:49,380","00:07:52,660",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,we can imagine we can generate
cs-410_2_4_118,cs-410,2,4,"00:07:52,660","00:07:58,220",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,That doesn't mean we cannot generate this
cs-410_2_4_119,cs-410,2,4,"00:07:59,650","00:08:05,030",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"We can, but the probability would be very,"
cs-410_2_4_120,cs-410,2,4,"00:08:05,030","00:08:09,300",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,generating a paper that can be accepted
cs-410_2_4_121,cs-410,2,4,"00:08:10,400","00:08:12,470",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,So the point is that
cs-410_2_4_122,cs-410,2,4,"00:08:13,590","00:08:18,410",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,we can talk about the probability of
cs-410_2_4_123,cs-410,2,4,"00:08:18,410","00:08:20,790",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,Some texts will have higher
cs-410_2_4_124,cs-410,2,4,"00:08:21,800","00:08:23,900",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,Now let's look at the problem
cs-410_2_4_125,cs-410,2,4,"00:08:23,900","00:08:28,260",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,Suppose we now have available
cs-410_2_4_126,cs-410,2,4,"00:08:28,260","00:08:31,960",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,"In this case, many of the abstract or"
cs-410_2_4_127,cs-410,2,4,"00:08:31,960","00:08:34,350",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,we see these word counts here.
cs-410_2_4_128,cs-410,2,4,"00:08:34,350","00:08:36,846",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,The total number of words is 100.
cs-410_2_4_129,cs-410,2,4,"00:08:36,846","00:08:39,530",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,Now the question you ask here
cs-410_2_4_130,cs-410,2,4,"00:08:39,530","00:08:42,000",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"We can ask the question which model,"
cs-410_2_4_131,cs-410,2,4,"00:08:42,000","00:08:46,340",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,which one of these distribution has
cs-410_2_4_132,cs-410,2,4,"00:08:46,340","00:08:50,150",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,assuming that the text has been generated
cs-410_2_4_133,cs-410,2,4,"00:08:51,970","00:08:53,100",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,So what would be your guess?
cs-410_2_4_134,cs-410,2,4,"00:08:54,230","00:08:58,220",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,What we have to decide are what
cs-410_2_4_135,cs-410,2,4,"00:08:58,220","00:08:58,860",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,would have.
cs-410_2_4_136,cs-410,2,4,"00:09:01,971","00:09:05,260",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,"Suppose the view for a second, and"
cs-410_2_4_137,cs-410,2,4,"00:09:09,616","00:09:14,109",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,"If you're like a lot of people,"
cs-410_2_4_138,cs-410,2,4,"00:09:14,109","00:09:18,683",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,my best guess is text has a probability
cs-410_2_4_139,cs-410,2,4,"00:09:18,683","00:09:23,310",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,"seen text 10 times, and"
cs-410_2_4_140,cs-410,2,4,"00:09:23,310","00:09:25,990",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,So we simply normalize these counts.
cs-410_2_4_141,cs-410,2,4,"00:09:27,242","00:09:29,550",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"And that's in fact the word justified, and"
cs-410_2_4_142,cs-410,2,4,"00:09:29,550","00:09:33,650",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,your intuition is consistent
cs-410_2_4_143,cs-410,2,4,"00:09:33,650","00:09:36,170",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,And this is called the maximum
cs-410_2_4_144,cs-410,2,4,"00:09:36,170","00:09:40,130",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,"In this estimator,"
cs-410_2_4_145,cs-410,2,4,"00:09:40,130","00:09:44,650",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,of those that would give our observe
cs-410_2_4_146,cs-410,2,4,"00:09:44,650","00:09:49,050",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,That means if we change these
cs-410_2_4_147,cs-410,2,4,"00:09:49,050","00:09:53,319",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,observing the particular text
cs-410_2_4_148,cs-410,2,4,"00:09:55,190","00:09:58,840",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,"So you can see,"
cs-410_2_4_149,cs-410,2,4,"00:09:58,840","00:10:05,030",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,"Basically, we just need to look at"
cs-410_2_4_150,cs-410,2,4,"00:10:05,030","00:10:08,987",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,and then divide it by the total number of
cs-410_2_4_151,cs-410,2,4,"00:10:08,987","00:10:11,670",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,Normalize the frequency.
cs-410_2_4_152,cs-410,2,4,"00:10:11,670","00:10:13,090",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,"A consequence of this is,"
cs-410_2_4_153,cs-410,2,4,"00:10:13,090","00:10:18,200",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,"of course, we're going to assign"
cs-410_2_4_154,cs-410,2,4,"00:10:18,200","00:10:19,730",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,"If we have an observed word,"
cs-410_2_4_155,cs-410,2,4,"00:10:19,730","00:10:25,300",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,there will be no incentive to assign a
cs-410_2_4_156,cs-410,2,4,"00:10:25,300","00:10:26,210",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,Why?
cs-410_2_4_157,cs-410,2,4,"00:10:26,210","00:10:30,840",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,Because that would take away probability
cs-410_2_4_158,cs-410,2,4,"00:10:30,840","00:10:33,516",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,And that obviously wouldn't maximize
cs-410_2_4_159,cs-410,2,4,"00:10:33,516","00:10:37,430",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,the probability of this
cs-410_2_4_160,cs-410,2,4,"00:10:37,430","00:10:42,050",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=637,But one has still question whether
cs-410_2_4_161,cs-410,2,4,"00:10:42,050","00:10:47,820",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,"Well, the answer depends on what kind"
cs-410_2_4_162,cs-410,2,4,"00:10:47,820","00:10:52,320",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,This estimator gives a best model
cs-410_2_4_163,cs-410,2,4,"00:10:52,320","00:10:57,400",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,But if you are interested in a model
cs-410_2_4_164,cs-410,2,4,"00:10:57,400","00:11:01,910",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,"paper for this abstract, then you"
cs-410_2_4_165,cs-410,2,4,"00:11:01,910","00:11:07,330",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,"So for thing,"
cs-410_2_4_166,cs-410,2,4,"00:11:07,330","00:11:11,570",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,"of that article, so"
cs-410_2_4_167,cs-410,2,4,"00:11:11,570","00:11:14,390",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,even though they're not
cs-410_2_4_168,cs-410,2,4,"00:11:14,390","00:11:17,750",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,So we're going to cover this
cs-410_2_4_169,cs-410,2,4,"00:11:17,750","00:11:22,520",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,in this class in the query
cs-410_2_4_170,cs-410,2,4,"00:11:24,350","00:11:29,520",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,So let's take a look at some possible
cs-410_2_4_171,cs-410,2,4,"00:11:29,520","00:11:32,820",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,One use is simply to use
cs-410_2_4_172,cs-410,2,4,"00:11:32,820","00:11:37,140",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,So here I show some general
cs-410_2_4_173,cs-410,2,4,"00:11:37,140","00:11:39,830",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,We can use this text to
cs-410_2_4_174,cs-410,2,4,"00:11:39,830","00:11:41,530",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=699,and the model might look like this.
cs-410_2_4_175,cs-410,2,4,"00:11:42,720","00:11:47,845",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,"Right, so on the top, we have those"
cs-410_2_4_176,cs-410,2,4,"00:11:47,845","00:11:52,610",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,"etc., and then we'll see some"
cs-410_2_4_177,cs-410,2,4,"00:11:52,610","00:11:55,310",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,"then some very,"
cs-410_2_4_178,cs-410,2,4,"00:11:55,310","00:11:57,460",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,This is a background language model.
cs-410_2_4_179,cs-410,2,4,"00:11:57,460","00:12:01,900",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,It represents the frequency of
cs-410_2_4_180,cs-410,2,4,"00:12:01,900","00:12:04,140",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=721,This is the background model.
cs-410_2_4_181,cs-410,2,4,"00:12:04,140","00:12:08,000",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,"Now let's look at another text,"
cs-410_2_4_182,cs-410,2,4,"00:12:08,000","00:12:09,979",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,we'll look at the computer
cs-410_2_4_183,cs-410,2,4,"00:12:11,030","00:12:13,800",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,So we have a collection of
cs-410_2_4_184,cs-410,2,4,"00:12:13,800","00:12:17,454",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,"we do as mentioned again, we can just"
cs-410_2_4_185,cs-410,2,4,"00:12:17,454","00:12:19,640",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,where we simply normalize the frequencies.
cs-410_2_4_186,cs-410,2,4,"00:12:20,690","00:12:24,326",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,"Now in this case, we'll get"
cs-410_2_4_187,cs-410,2,4,"00:12:24,326","00:12:28,141",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,"On the top, it looks similar because"
cs-410_2_4_188,cs-410,2,4,"00:12:28,141","00:12:29,406",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,they are very common.
cs-410_2_4_189,cs-410,2,4,"00:12:29,406","00:12:34,243",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,"But as we go down,"
cs-410_2_4_190,cs-410,2,4,"00:12:34,243","00:12:38,806",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=754,"computer science,"
cs-410_2_4_191,cs-410,2,4,"00:12:38,806","00:12:43,146",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,"And so although here, we might also see"
cs-410_2_4_192,cs-410,2,4,"00:12:43,146","00:12:47,490",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,we can imagine the probability here is
cs-410_2_4_193,cs-410,2,4,"00:12:47,490","00:12:55,776",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=767,And we will see many other words here that
cs-410_2_4_194,cs-410,2,4,"00:12:55,776","00:12:58,737",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,So you can see this distribution
cs-410_2_4_195,cs-410,2,4,"00:12:58,737","00:13:00,830",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,the corresponding text.
cs-410_2_4_196,cs-410,2,4,"00:13:00,830","00:13:02,870",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,We can look at even the smaller text.
cs-410_2_4_197,cs-410,2,4,"00:13:03,970","00:13:06,870",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=783,"So in this case,"
cs-410_2_4_198,cs-410,2,4,"00:13:06,870","00:13:10,047",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,"Now if we do the same,"
cs-410_2_4_199,cs-410,2,4,"00:13:10,047","00:13:12,740",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=790,again the can be expected
cs-410_2_4_200,cs-410,2,4,"00:13:12,740","00:13:16,927",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,"The sooner we see text, mining,"
cs-410_2_4_201,cs-410,2,4,"00:13:16,927","00:13:20,440",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=796,these words have relatively
cs-410_2_4_202,cs-410,2,4,"00:13:20,440","00:13:27,540",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,"In contrast, in this distribution, the"
cs-410_2_4_203,cs-410,2,4,"00:13:27,540","00:13:32,190",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,"So this means, again,"
cs-410_2_4_204,cs-410,2,4,"00:13:32,190","00:13:36,266",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=812,"we can have a different model,"
cs-410_2_4_205,cs-410,2,4,"00:13:36,266","00:13:40,450",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=816,So we call this document
cs-410_2_4_206,cs-410,2,4,"00:13:40,450","00:13:42,530",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=820,we call this collection language model.
cs-410_2_4_207,cs-410,2,4,"00:13:42,530","00:13:46,580",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,"And later, you will see how they're"
cs-410_2_4_208,cs-410,2,4,"00:13:47,650","00:13:50,690",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=827,"But now,"
cs-410_2_4_209,cs-410,2,4,"00:13:50,690","00:13:55,210",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=830,Can we statistically find what words
cs-410_2_4_210,cs-410,2,4,"00:13:56,900","00:13:58,770",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=836,Now how do we find such words?
cs-410_2_4_211,cs-410,2,4,"00:13:58,770","00:14:04,230",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=838,"Well, our first thought is that let's take"
cs-410_2_4_212,cs-410,2,4,"00:14:04,230","00:14:08,860",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,So we can take a look at all the documents
cs-410_2_4_213,cs-410,2,4,"00:14:08,860","00:14:10,930",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=848,Let's build a language model.
cs-410_2_4_214,cs-410,2,4,"00:14:10,930","00:14:13,220",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,We can see what words we see there.
cs-410_2_4_215,cs-410,2,4,"00:14:13,220","00:14:19,430",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=853,"Well, not surprisingly, we see these"
cs-410_2_4_216,cs-410,2,4,"00:14:19,430","00:14:23,490",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,"So in this case, this language model gives"
cs-410_2_4_217,cs-410,2,4,"00:14:23,490","00:14:26,260",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=863,the word in the context of computer.
cs-410_2_4_218,cs-410,2,4,"00:14:26,260","00:14:29,370",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=866,And these common words will
cs-410_2_4_219,cs-410,2,4,"00:14:29,370","00:14:31,750",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=869,But we also see the computer itself and
cs-410_2_4_220,cs-410,2,4,"00:14:31,750","00:14:35,490",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=871,software will have relatively
cs-410_2_4_221,cs-410,2,4,"00:14:35,490","00:14:37,320",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,"But if we just use this model,"
cs-410_2_4_222,cs-410,2,4,"00:14:37,320","00:14:42,037",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,we cannot just say all these words
cs-410_2_4_223,cs-410,2,4,"00:14:43,210","00:14:50,700",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,"So ultimately, what we'd like to"
cs-410_2_4_224,cs-410,2,4,"00:14:50,700","00:14:51,420",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,How can we do that?
cs-410_2_4_225,cs-410,2,4,"00:14:52,760","00:14:55,571",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=892,It turns out that it's possible
cs-410_2_4_226,cs-410,2,4,"00:14:57,610","00:15:00,020",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=897,But I suggest you think about that.
cs-410_2_4_227,cs-410,2,4,"00:15:00,020","00:15:03,510",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=900,So how can we know what
cs-410_2_4_228,cs-410,2,4,"00:15:03,510","00:15:06,030",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=903,so that we want to kind
cs-410_2_4_229,cs-410,2,4,"00:15:07,730","00:15:10,220",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=907,What model will tell us that?
cs-410_2_4_230,cs-410,2,4,"00:15:10,220","00:15:14,180",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=910,"Well, maybe you can think about that."
cs-410_2_4_231,cs-410,2,4,"00:15:14,180","00:15:18,170",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=914,So the background language model
cs-410_2_4_232,cs-410,2,4,"00:15:18,170","00:15:21,240",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=918,It tells us what was
cs-410_2_4_233,cs-410,2,4,"00:15:21,240","00:15:23,510",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=921,"So if we use this background model,"
cs-410_2_4_234,cs-410,2,4,"00:15:23,510","00:15:28,390",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,we would know that these words
cs-410_2_4_235,cs-410,2,4,"00:15:28,390","00:15:31,595",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=928,So it's not surprising to observe
cs-410_2_4_236,cs-410,2,4,"00:15:31,595","00:15:36,380",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=931,Whereas computer has a very
cs-410_2_4_237,cs-410,2,4,"00:15:36,380","00:15:41,200",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=936,it's very surprising that we have seen
cs-410_2_4_238,cs-410,2,4,"00:15:41,200","00:15:42,740",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=941,the same is true for software.
cs-410_2_4_239,cs-410,2,4,"00:15:44,220","00:15:48,750",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=944,So then we can use these two
cs-410_2_4_240,cs-410,2,4,"00:15:48,750","00:15:52,590",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,the words that are related to computer.
cs-410_2_4_241,cs-410,2,4,"00:15:52,590","00:15:57,310",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=952,"For example, we can simply take the ratio"
cs-410_2_4_242,cs-410,2,4,"00:15:57,310","00:16:01,050",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,normalize the topic of language model
cs-410_2_4_243,cs-410,2,4,"00:16:01,050","00:16:02,900",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=961,the background language model.
cs-410_2_4_244,cs-410,2,4,"00:16:02,900","00:16:07,632",244,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=962,"So if we do that, we take the ratio,"
cs-410_2_4_245,cs-410,2,4,"00:16:07,632","00:16:11,371",245,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,"computer is ranked, and"
cs-410_2_4_246,cs-410,2,4,"00:16:11,371","00:16:14,796",246,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,"program, all these words"
cs-410_2_4_247,cs-410,2,4,"00:16:14,796","00:16:19,371",247,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=974,Because they occur very frequently in the
cs-410_2_4_248,cs-410,2,4,"00:16:19,371","00:16:23,960",248,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=979,"the whole collection, whereas these common"
cs-410_2_4_249,cs-410,2,4,"00:16:23,960","00:16:27,850",249,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=983,"In fact,"
cs-410_2_4_250,cs-410,2,4,"00:16:27,850","00:16:30,780",250,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=987,because they are not really
cs-410_2_4_251,cs-410,2,4,"00:16:30,780","00:16:34,920",251,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=990,By taking the sample of text
cs-410_2_4_252,cs-410,2,4,"00:16:34,920","00:16:39,240",252,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=994,we don't really see more occurrences
cs-410_2_4_253,cs-410,2,4,"00:16:40,250","00:16:43,310",253,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,So this shows that even with
cs-410_2_4_254,cs-410,2,4,"00:16:43,310","00:16:46,450",254,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1003,we can do some limited
cs-410_2_4_255,cs-410,2,4,"00:16:48,370","00:16:52,343",255,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1008,"So in this lecture,"
cs-410_2_4_256,cs-410,2,4,"00:16:52,343","00:16:56,776",256,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1012,which is basically a probability
cs-410_2_4_257,cs-410,2,4,"00:16:56,776","00:17:00,067",257,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1016,We talked about the simplest language
cs-410_2_4_258,cs-410,2,4,"00:17:00,067","00:17:02,720",258,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1020,which is also just a word distribution.
cs-410_2_4_259,cs-410,2,4,"00:17:02,720","00:17:05,320",259,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1022,We talked about the two
cs-410_2_4_260,cs-410,2,4,"00:17:05,320","00:17:10,360",260,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1025,One is we represent the topic in a
cs-410_2_4_261,cs-410,2,4,"00:17:10,360","00:17:12,650",261,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1030,The other is we discover
cs-410_2_4_262,cs-410,2,4,"00:17:16,456","00:17:20,089",262,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1036,"In the next lecture, we're going to talk"
cs-410_2_4_263,cs-410,2,4,"00:17:20,089","00:17:21,510",263,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1040,design a retrieval function.
cs-410_2_4_264,cs-410,2,4,"00:17:23,260","00:17:24,960",264,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1043,Here are two additional readings.
cs-410_2_4_265,cs-410,2,4,"00:17:24,960","00:17:28,850",265,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1044,The first is a textbook on statistical
cs-410_2_4_266,cs-410,2,4,"00:17:30,290","00:17:35,249",266,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1050,The second is an article that
cs-410_2_4_267,cs-410,2,4,"00:17:35,249","00:17:40,326",267,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1055,language models with a lot of
cs-410_2_4_268,cs-410,2,4,"00:17:40,326","00:17:50,326",268,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1060,[MUSIC]
cs-410_2_5_1,cs-410,2,5,"00:00:00,012","00:00:07,558",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_2_5_2,cs-410,2,5,"00:00:07,558","00:00:10,370",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about the feedback
cs-410_2_5_3,cs-410,2,5,"00:00:12,910","00:00:18,040",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture, we continue talking"
cs-410_2_5_4,cs-410,2,5,"00:00:18,040","00:00:21,210",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"Particularly, we're going to talk about"
cs-410_2_5_5,cs-410,2,5,"00:00:23,930","00:00:29,210",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,"As we have discussed before,"
cs-410_2_5_6,cs-410,2,5,"00:00:29,210","00:00:34,890",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,of text retrieval system is removed from
cs-410_2_5_7,cs-410,2,5,"00:00:34,890","00:00:37,467",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,We will have positive examples.
cs-410_2_5_8,cs-410,2,5,"00:00:37,467","00:00:40,669",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,Those are the documents that
cs-410_2_5_9,cs-410,2,5,"00:00:40,669","00:00:42,610",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,be charged with being relevant.
cs-410_2_5_10,cs-410,2,5,"00:00:42,610","00:00:45,160",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,All the documents that
cs-410_2_5_11,cs-410,2,5,"00:00:45,160","00:00:46,910",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,We also have negative examples.
cs-410_2_5_12,cs-410,2,5,"00:00:46,910","00:00:49,590",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,Those are documents known
cs-410_2_5_13,cs-410,2,5,"00:00:49,590","00:00:52,960",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,They can also be the documents
cs-410_2_5_14,cs-410,2,5,"00:00:55,350","00:00:58,570",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,The general method in
cs-410_2_5_15,cs-410,2,5,"00:00:58,570","00:01:02,690",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,feedback is to modify our query vector.
cs-410_2_5_16,cs-410,2,5,"00:01:04,010","00:01:08,500",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,We want to place the query vector in
cs-410_2_5_17,cs-410,2,5,"00:01:10,120","00:01:11,520",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,And what does that mean exactly?
cs-410_2_5_18,cs-410,2,5,"00:01:11,520","00:01:14,930",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,"Well, if we think about the query vector"
cs-410_2_5_19,cs-410,2,5,"00:01:14,930","00:01:17,270",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,something to the vector elements.
cs-410_2_5_20,cs-410,2,5,"00:01:17,270","00:01:21,240",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,"And in general,"
cs-410_2_5_21,cs-410,2,5,"00:01:21,240","00:01:27,129",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,Or we might just weight of old terms or
cs-410_2_5_22,cs-410,2,5,"00:01:29,230","00:01:32,780",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,"As a result, in general,"
cs-410_2_5_23,cs-410,2,5,"00:01:32,780","00:01:35,110",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,We often call this query expansion.
cs-410_2_5_24,cs-410,2,5,"00:01:37,960","00:01:40,920",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,The most effective method in
cs-410_2_5_25,cs-410,2,5,"00:01:40,920","00:01:44,900",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"is called the Rocchio Feedback, which was"
cs-410_2_5_26,cs-410,2,5,"00:01:47,490","00:01:49,110",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,So the idea is quite simple.
cs-410_2_5_27,cs-410,2,5,"00:01:49,110","00:01:53,402",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,We illustrate this idea by
cs-410_2_5_28,cs-410,2,5,"00:01:53,402","00:01:58,231",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,of all the documents in the collection and
cs-410_2_5_29,cs-410,2,5,"00:01:58,231","00:02:03,935",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,So now we can see the query
cs-410_2_5_30,cs-410,2,5,"00:02:03,935","00:02:07,428",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,and these are all the documents.
cs-410_2_5_31,cs-410,2,5,"00:02:07,428","00:02:11,230",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,So when we use the query back there and
cs-410_2_5_32,cs-410,2,5,"00:02:11,230","00:02:14,780",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,"the most similar documents,"
cs-410_2_5_33,cs-410,2,5,"00:02:14,780","00:02:18,960",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,that these documents would be
cs-410_2_5_34,cs-410,2,5,"00:02:18,960","00:02:22,512",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,"And these process are relevant documents,"
cs-410_2_5_35,cs-410,2,5,"00:02:22,512","00:02:27,762",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"these are relevant documents,"
cs-410_2_5_36,cs-410,2,5,"00:02:27,762","00:02:32,360",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,And then these minuses are negative
cs-410_2_5_37,cs-410,2,5,"00:02:34,310","00:02:40,150",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,So our goal here is trying to move
cs-410_2_5_38,cs-410,2,5,"00:02:40,150","00:02:42,780",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,to improve the retrieval accuracy.
cs-410_2_5_39,cs-410,2,5,"00:02:42,780","00:02:48,390",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,"By looking at this diagram,"
cs-410_2_5_40,cs-410,2,5,"00:02:48,390","00:02:50,650",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,Where should we move the query vector so
cs-410_2_5_41,cs-410,2,5,"00:02:50,650","00:02:53,930",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,that we can improve
cs-410_2_5_42,cs-410,2,5,"00:02:53,930","00:02:56,990",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"Intuitively, where do you"
cs-410_2_5_43,cs-410,2,5,"00:02:58,050","00:03:01,330",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"If you want to think more,"
cs-410_2_5_44,cs-410,2,5,"00:03:02,980","00:03:10,090",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,"If you think about this picture, you can"
cs-410_2_5_45,cs-410,2,5,"00:03:10,090","00:03:15,520",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,case you want the query vector to be as
cs-410_2_5_46,cs-410,2,5,"00:03:15,520","00:03:20,462",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,"That means ideally, you want to place"
cs-410_2_5_47,cs-410,2,5,"00:03:20,462","00:03:24,640",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,Or we want to move the query
cs-410_2_5_48,cs-410,2,5,"00:03:26,510","00:03:29,100",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,Now so what exactly is this point?
cs-410_2_5_49,cs-410,2,5,"00:03:29,100","00:03:35,710",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"Well, if you want these relevant"
cs-410_2_5_50,cs-410,2,5,"00:03:35,710","00:03:41,340",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,you want this to be in the center of
cs-410_2_5_51,cs-410,2,5,"00:03:41,340","00:03:44,710",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,Because then if you draw
cs-410_2_5_52,cs-410,2,5,"00:03:44,710","00:03:47,240",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,you'll get all these relevant documents.
cs-410_2_5_53,cs-410,2,5,"00:03:47,240","00:03:52,250",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,So that means we can move the query
cs-410_2_5_54,cs-410,2,5,"00:03:52,250","00:03:54,510",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,all the relevant document vectors.
cs-410_2_5_55,cs-410,2,5,"00:03:55,680","00:03:59,106",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,And this is basically the idea of Rocchio.
cs-410_2_5_56,cs-410,2,5,"00:03:59,106","00:04:03,645",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"Of course, you can consider"
cs-410_2_5_57,cs-410,2,5,"00:04:03,645","00:04:07,040",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,we want to move away from
cs-410_2_5_58,cs-410,2,5,"00:04:07,040","00:04:11,971",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,Now your match that we're talking about
cs-410_2_5_59,cs-410,2,5,"00:04:11,971","00:04:14,202",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,away from other vectors.
cs-410_2_5_60,cs-410,2,5,"00:04:14,202","00:04:18,340",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,It just means that we have this formula.
cs-410_2_5_61,cs-410,2,5,"00:04:18,340","00:04:22,891",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,Here you can see this is
cs-410_2_5_62,cs-410,2,5,"00:04:22,891","00:04:29,680",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,this average basically is the centroid
cs-410_2_5_63,cs-410,2,5,"00:04:29,680","00:04:32,250",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,"When we take the average of these vectors,"
cs-410_2_5_64,cs-410,2,5,"00:04:32,250","00:04:35,580",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,then were computing
cs-410_2_5_65,cs-410,2,5,"00:04:35,580","00:04:41,070",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,"Similarly, this is the average of"
cs-410_2_5_66,cs-410,2,5,"00:04:41,070","00:04:46,080",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,So it's essentially of
cs-410_2_5_67,cs-410,2,5,"00:04:46,080","00:04:51,710",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,"And we have these three parameters here,"
cs-410_2_5_68,cs-410,2,5,"00:04:51,710","00:04:55,200",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,They are controlling
cs-410_2_5_69,cs-410,2,5,"00:04:55,200","00:04:57,560",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"When we add these two vectors together,"
cs-410_2_5_70,cs-410,2,5,"00:04:57,560","00:05:02,290",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,we're moving the query vector
cs-410_2_5_71,cs-410,2,5,"00:05:03,620","00:05:05,740",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,This is when we add them together.
cs-410_2_5_72,cs-410,2,5,"00:05:05,740","00:05:08,350",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,"When we subtracted this part,"
cs-410_2_5_73,cs-410,2,5,"00:05:08,350","00:05:14,660",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,we kind of move the query
cs-410_2_5_74,cs-410,2,5,"00:05:14,660","00:05:18,420",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,So this is the main idea
cs-410_2_5_75,cs-410,2,5,"00:05:18,420","00:05:20,720",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,"And after we have done this,"
cs-410_2_5_76,cs-410,2,5,"00:05:20,720","00:05:25,710",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,we will get a new query vector which
cs-410_2_5_77,cs-410,2,5,"00:05:25,710","00:05:31,905",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,"This new query vector,"
cs-410_2_5_78,cs-410,2,5,"00:05:31,905","00:05:38,878",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,original query vector toward this
cs-410_2_5_79,cs-410,2,5,"00:05:38,878","00:05:42,900",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,away from the non-relevant value.
cs-410_2_5_80,cs-410,2,5,"00:05:45,110","00:05:48,200",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"Okay, so let's take a look at the example."
cs-410_2_5_81,cs-410,2,5,"00:05:48,200","00:05:51,360",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,This is the example that
cs-410_2_5_82,cs-410,2,5,"00:05:51,360","00:05:55,600",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,Only that I deemed that display
cs-410_2_5_83,cs-410,2,5,"00:05:55,600","00:05:59,210",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,I only showed the vector
cs-410_2_5_84,cs-410,2,5,"00:05:59,210","00:06:03,240",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,We have five documents here and we have
cs-410_2_5_85,cs-410,2,5,"00:06:04,760","00:06:09,667",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"to read in the documents here, right."
cs-410_2_5_86,cs-410,2,5,"00:06:09,667","00:06:12,650",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,And they're displayed in red.
cs-410_2_5_87,cs-410,2,5,"00:06:12,650","00:06:14,760",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,And these are the term vectors.
cs-410_2_5_88,cs-410,2,5,"00:06:14,760","00:06:18,190",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,Now I have just assumed some of weights.
cs-410_2_5_89,cs-410,2,5,"00:06:18,190","00:06:20,549",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,"A lot of terms,"
cs-410_2_5_90,cs-410,2,5,"00:06:20,549","00:06:22,745",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,Now these are negative arguments.
cs-410_2_5_91,cs-410,2,5,"00:06:22,745","00:06:23,952",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,There are two here.
cs-410_2_5_92,cs-410,2,5,"00:06:23,952","00:06:26,120",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,There is another one here.
cs-410_2_5_93,cs-410,2,5,"00:06:26,120","00:06:32,520",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"Now in this Rocchio method, we first"
cs-410_2_5_94,cs-410,2,5,"00:06:32,520","00:06:37,540",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"And so let's see,"
cs-410_2_5_95,cs-410,2,5,"00:06:37,540","00:06:42,910",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,"the positive documents, we simply just,"
cs-410_2_5_96,cs-410,2,5,"00:06:42,910","00:06:48,490",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,We just add this with this one
cs-410_2_5_97,cs-410,2,5,"00:06:48,490","00:06:51,560",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,And then that's down here and
cs-410_2_5_98,cs-410,2,5,"00:06:51,560","00:06:54,801",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,And then we're going to add
cs-410_2_5_99,cs-410,2,5,"00:06:54,801","00:06:56,580",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,then just take the average.
cs-410_2_5_100,cs-410,2,5,"00:06:56,580","00:06:58,790",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,And so we do this for all this.
cs-410_2_5_101,cs-410,2,5,"00:06:58,790","00:07:02,520",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"In the end, what we have is this one."
cs-410_2_5_102,cs-410,2,5,"00:07:02,520","00:07:08,380",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,"This is the average vector of these two,"
cs-410_2_5_103,cs-410,2,5,"00:07:10,030","00:07:13,770",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,Let's also look at the centroid
cs-410_2_5_104,cs-410,2,5,"00:07:13,770","00:07:15,052",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,This is basically the same.
cs-410_2_5_105,cs-410,2,5,"00:07:15,052","00:07:18,150",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,We're going to take the average
cs-410_2_5_106,cs-410,2,5,"00:07:18,150","00:07:22,420",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,And these are the corresponding
cs-410_2_5_107,cs-410,2,5,"00:07:22,420","00:07:23,020",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,on and so forth.
cs-410_2_5_108,cs-410,2,5,"00:07:23,020","00:07:25,120",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,"So in the end, we have this one."
cs-410_2_5_109,cs-410,2,5,"00:07:26,230","00:07:29,340",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,Now in the Rocchio feedback
cs-410_2_5_110,cs-410,2,5,"00:07:29,340","00:07:32,920",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,these with the original
cs-410_2_5_111,cs-410,2,5,"00:07:32,920","00:07:36,083",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,So now let's see how we
cs-410_2_5_112,cs-410,2,5,"00:07:36,083","00:07:37,420",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,"Well, that's basically this."
cs-410_2_5_113,cs-410,2,5,"00:07:38,830","00:07:42,385",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,So we have a parameter alpha
cs-410_2_5_114,cs-410,2,5,"00:07:42,385","00:07:45,210",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,query times weight that's one.
cs-410_2_5_115,cs-410,2,5,"00:07:45,210","00:07:49,626",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,And now we have beta to control
cs-410_2_5_116,cs-410,2,5,"00:07:49,626","00:07:52,820",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"centroid of the weight, that's 1.5."
cs-410_2_5_117,cs-410,2,5,"00:07:52,820","00:07:54,285",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,That comes from here.
cs-410_2_5_118,cs-410,2,5,"00:07:54,285","00:08:00,400",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,"All right, so this goes here."
cs-410_2_5_119,cs-410,2,5,"00:08:00,400","00:08:07,555",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,And we also have this negative
cs-410_2_5_120,cs-410,2,5,"00:08:07,555","00:08:14,520",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,"And this way, it has come from,"
cs-410_2_5_121,cs-410,2,5,"00:08:14,520","00:08:19,051",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,And we do exactly the same for
cs-410_2_5_122,cs-410,2,5,"00:08:22,244","00:08:23,840",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,And this is our new vector.
cs-410_2_5_123,cs-410,2,5,"00:08:25,700","00:08:31,530",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,And we're going to use this new query
cs-410_2_5_124,cs-410,2,5,"00:08:31,530","00:08:33,840",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"You can imagine what would happen, right?"
cs-410_2_5_125,cs-410,2,5,"00:08:33,840","00:08:38,000",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,Because of the movement that this one
cs-410_2_5_126,cs-410,2,5,"00:08:38,000","00:08:42,520",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,better because we moved
cs-410_2_5_127,cs-410,2,5,"00:08:42,520","00:08:47,290",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,And it's going to penalize these black
cs-410_2_5_128,cs-410,2,5,"00:08:47,290","00:08:49,790",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,So this is precisely what
cs-410_2_5_129,cs-410,2,5,"00:08:50,820","00:08:57,220",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,Now of course if we apply this method in
cs-410_2_5_130,cs-410,2,5,"00:08:58,240","00:09:04,290",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,and that is the original query has
cs-410_2_5_131,cs-410,2,5,"00:09:06,410","00:09:08,480",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,But after we do query explaining and
cs-410_2_5_132,cs-410,2,5,"00:09:08,480","00:09:13,210",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"merging, we'll have many times"
cs-410_2_5_133,cs-410,2,5,"00:09:13,210","00:09:16,580",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,So the calculation will
cs-410_2_5_134,cs-410,2,5,"00:09:18,090","00:09:22,160",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,"In practice,"
cs-410_2_5_135,cs-410,2,5,"00:09:22,160","00:09:25,470",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,only retain the terms
cs-410_2_5_136,cs-410,2,5,"00:09:27,000","00:09:29,440",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,So let's talk about how we
cs-410_2_5_137,cs-410,2,5,"00:09:30,660","00:09:34,220",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,I just mentioned that they're
cs-410_2_5_138,cs-410,2,5,"00:09:34,220","00:09:37,400",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,Consider only a small number of
cs-410_2_5_139,cs-410,2,5,"00:09:37,400","00:09:38,690",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,the centroid vector.
cs-410_2_5_140,cs-410,2,5,"00:09:38,690","00:09:39,900",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,This is for efficiency concern.
cs-410_2_5_141,cs-410,2,5,"00:09:41,390","00:09:45,580",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,"I also said here that negative examples,"
cs-410_2_5_142,cs-410,2,5,"00:09:45,580","00:09:49,430",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,"tend not to be very useful, especially"
cs-410_2_5_143,cs-410,2,5,"00:09:50,860","00:09:52,500",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,Now you can think about why.
cs-410_2_5_144,cs-410,2,5,"00:09:55,320","00:09:59,771",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,One reason is because negative documents
cs-410_2_5_145,cs-410,2,5,"00:09:59,771","00:10:00,645",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,directions.
cs-410_2_5_146,cs-410,2,5,"00:10:00,645","00:10:02,391",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,"So, when you take the average,"
cs-410_2_5_147,cs-410,2,5,"00:10:02,391","00:10:06,860",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,it doesn't really tell you where
cs-410_2_5_148,cs-410,2,5,"00:10:06,860","00:10:10,110",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,Whereas positive documents
cs-410_2_5_149,cs-410,2,5,"00:10:10,110","00:10:14,569",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,And they will point you to
cs-410_2_5_150,cs-410,2,5,"00:10:14,569","00:10:19,090",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,So that also means that sometimes we don't
cs-410_2_5_151,cs-410,2,5,"00:10:19,090","00:10:24,580",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,"But note that in some cases, in difficult"
cs-410_2_5_152,cs-410,2,5,"00:10:24,580","00:10:26,390",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,negative feedback after is very useful.
cs-410_2_5_153,cs-410,2,5,"00:10:27,550","00:10:29,370",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,Another thing is to avoid over-fitting.
cs-410_2_5_154,cs-410,2,5,"00:10:29,370","00:10:34,425",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=629,That means we have to keep relatively
cs-410_2_5_155,cs-410,2,5,"00:10:34,425","00:10:35,724",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,Why?
cs-410_2_5_156,cs-410,2,5,"00:10:35,724","00:10:42,250",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,Because the sample that we see in
cs-410_2_5_157,cs-410,2,5,"00:10:42,250","00:10:45,580",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,We don't want to overly
cs-410_2_5_158,cs-410,2,5,"00:10:45,580","00:10:49,390",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,And the original query terms
cs-410_2_5_159,cs-410,2,5,"00:10:49,390","00:10:51,753",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,Those terms are heightened by the user and
cs-410_2_5_160,cs-410,2,5,"00:10:51,753","00:10:55,850",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,the user has decided that those
cs-410_2_5_161,cs-410,2,5,"00:10:55,850","00:11:02,530",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,So in order to prevent
cs-410_2_5_162,cs-410,2,5,"00:11:02,530","00:11:08,910",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,"drifting, prevent topic drifting due to"
cs-410_2_5_163,cs-410,2,5,"00:11:08,910","00:11:12,740",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,We generally would have to keep a pretty
cs-410_2_5_164,cs-410,2,5,"00:11:12,740","00:11:13,980",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,it was safe to do that.
cs-410_2_5_165,cs-410,2,5,"00:11:15,040","00:11:18,910",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,And this is especially true for
cs-410_2_5_166,cs-410,2,5,"00:11:18,910","00:11:20,910",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,"Now, this method can be used for"
cs-410_2_5_167,cs-410,2,5,"00:11:20,910","00:11:23,790",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,both relevance feedback and
cs-410_2_5_168,cs-410,2,5,"00:11:23,790","00:11:28,780",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,"In the case of pseudo-feedback, the prime"
cs-410_2_5_169,cs-410,2,5,"00:11:28,780","00:11:32,930",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=688,value because the relevant examples
cs-410_2_5_170,cs-410,2,5,"00:11:32,930","00:11:36,780",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,They're not as reliable as
cs-410_2_5_171,cs-410,2,5,"00:11:36,780","00:11:40,830",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,"In the case of relevance feedback,"
cs-410_2_5_172,cs-410,2,5,"00:11:40,830","00:11:43,580",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,"So those parameters,"
cs-410_2_5_173,cs-410,2,5,"00:11:45,020","00:11:48,550",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,And the Rocchio Method is
cs-410_2_5_174,cs-410,2,5,"00:11:48,550","00:11:51,961",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,It's still a very popular method for
cs-410_2_5_175,cs-410,2,5,"00:11:51,961","00:12:01,961",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,[MUSIC]
cs-410_2_6_1,cs-410,2,6,"00:00:00,076","00:00:03,466",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_2_6_2,cs-410,2,6,"00:00:06,698","00:00:14,732",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,So now let's take a look at the specific
cs-410_2_6_3,cs-410,2,6,"00:00:14,732","00:00:17,627",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"Now, this is one of the many"
cs-410_2_6_4,cs-410,2,6,"00:00:17,627","00:00:19,309",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,it's one of the simplest methods.
cs-410_2_6_5,cs-410,2,6,"00:00:19,309","00:00:24,550",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,And I choose this to explain
cs-410_2_6_6,cs-410,2,6,"00:00:26,360","00:00:34,350",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,"So in this approach, we simply assume"
cs-410_2_6_7,cs-410,2,6,"00:00:34,350","00:00:39,760",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,respect to a query is related to a linear
cs-410_2_6_8,cs-410,2,6,"00:00:39,760","00:00:47,400",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,Here I used Xi to denote the feature.
cs-410_2_6_9,cs-410,2,6,"00:00:47,400","00:00:51,770",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,So Xi of Q and D is a feature.
cs-410_2_6_10,cs-410,2,6,"00:00:51,770","00:00:54,720",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,And we can have as many
cs-410_2_6_11,cs-410,2,6,"00:00:55,890","00:01:01,670",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,And we assume that these features
cs-410_2_6_12,cs-410,2,6,"00:01:03,580","00:01:06,150",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,And each feature is controlled
cs-410_2_6_13,cs-410,2,6,"00:01:06,150","00:01:08,880",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,and this beta i is a parameter.
cs-410_2_6_14,cs-410,2,6,"00:01:08,880","00:01:10,790",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,That's a weighting parameter.
cs-410_2_6_15,cs-410,2,6,"00:01:10,790","00:01:15,940",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,A larger value would mean the feature
cs-410_2_6_16,cs-410,2,6,"00:01:15,940","00:01:18,525",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,and it would contribute more
cs-410_2_6_17,cs-410,2,6,"00:01:18,525","00:01:23,069",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,This specific form of the function
cs-410_2_6_18,cs-410,2,6,"00:01:23,069","00:01:27,154",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,a transformation of
cs-410_2_6_19,cs-410,2,6,"00:01:27,154","00:01:30,428",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,So this is the probability of relevance.
cs-410_2_6_20,cs-410,2,6,"00:01:30,428","00:01:35,611",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,And we know that the probability of
cs-410_2_6_21,cs-410,2,6,"00:01:36,870","00:01:41,863",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,And we could have just assumed that
cs-410_2_6_22,cs-410,2,6,"00:01:41,863","00:01:43,856",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,this linear combination.
cs-410_2_6_23,cs-410,2,6,"00:01:43,856","00:01:47,479",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,So we can do a linear regression.
cs-410_2_6_24,cs-410,2,6,"00:01:47,479","00:01:53,940",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"But then, the value of this linear"
cs-410_2_6_25,cs-410,2,6,"00:01:53,940","00:01:59,220",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,So this transformation
cs-410_2_6_26,cs-410,2,6,"00:01:59,220","00:02:05,540",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,to 1 range to the whole
cs-410_2_6_27,cs-410,2,6,"00:02:05,540","00:02:08,330",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,you can verify it by yourself.
cs-410_2_6_28,cs-410,2,6,"00:02:10,350","00:02:16,700",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,So this allows us then to connect
cs-410_2_6_29,cs-410,2,6,"00:02:16,700","00:02:23,010",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,which is between 0 and 1 to a linear
cs-410_2_6_30,cs-410,2,6,"00:02:23,010","00:02:28,133",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,And if we rewrite this into a probability
cs-410_2_6_31,cs-410,2,6,"00:02:28,133","00:02:34,299",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,"So on this equation, now we'll"
cs-410_2_6_32,cs-410,2,6,"00:02:35,690","00:02:39,168",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,"And on the right hand side,"
cs-410_2_6_33,cs-410,2,6,"00:02:39,168","00:02:42,448",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,"Now, this form is clearly nonnegative, and"
cs-410_2_6_34,cs-410,2,6,"00:02:42,448","00:02:46,344",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,it still involves a linear
cs-410_2_6_35,cs-410,2,6,"00:02:46,344","00:02:50,890",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,"And it's also clear that if this value is,"
cs-410_2_6_36,cs-410,2,6,"00:02:50,890","00:02:58,991",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,this is actually negative of the linear
cs-410_2_6_37,cs-410,2,6,"00:02:58,991","00:03:04,415",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"If this value here is large,"
cs-410_2_6_38,cs-410,2,6,"00:03:04,415","00:03:11,879",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,then it would mean this value is small.
cs-410_2_6_39,cs-410,2,6,"00:03:11,879","00:03:17,278",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,"And therefore,"
cs-410_2_6_40,cs-410,2,6,"00:03:17,278","00:03:22,034",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,"And that's we expect, that basically,"
cs-410_2_6_41,cs-410,2,6,"00:03:22,034","00:03:26,496",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"gives us a high value, then"
cs-410_2_6_42,cs-410,2,6,"00:03:26,496","00:03:29,015",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,So this is our hypothesis.
cs-410_2_6_43,cs-410,2,6,"00:03:29,015","00:03:33,955",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"Again, this is not necessarily the best"
cs-410_2_6_44,cs-410,2,6,"00:03:33,955","00:03:39,109",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,way to connect these features with
cs-410_2_6_45,cs-410,2,6,"00:03:40,470","00:03:44,578",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,So now we have this combination function.
cs-410_2_6_46,cs-410,2,6,"00:03:44,578","00:03:48,617",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,The next task is to
cs-410_2_6_47,cs-410,2,6,"00:03:48,617","00:03:52,346",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,that the function cache will be applied.
cs-410_2_6_48,cs-410,2,6,"00:03:52,346","00:03:57,430",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"But without knowing the beta values,"
cs-410_2_6_49,cs-410,2,6,"00:03:58,520","00:04:04,068",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,So let's see how can
cs-410_2_6_50,cs-410,2,6,"00:04:04,068","00:04:07,190",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,"All right,"
cs-410_2_6_51,cs-410,2,6,"00:04:08,780","00:04:11,405",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,"In this example, we have three features."
cs-410_2_6_52,cs-410,2,6,"00:04:11,405","00:04:15,060",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,One is the BM25 score of the document and
cs-410_2_6_53,cs-410,2,6,"00:04:15,060","00:04:19,044",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,"One is the PageRank score of the document,"
cs-410_2_6_54,cs-410,2,6,"00:04:19,044","00:04:21,211",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,might not depend on the query.
cs-410_2_6_55,cs-410,2,6,"00:04:21,211","00:04:25,681",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,"We might have a topic-sensitive PageRank,"
cs-410_2_6_56,cs-410,2,6,"00:04:25,681","00:04:29,946",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,"Otherwise, the general PageRank"
cs-410_2_6_57,cs-410,2,6,"00:04:29,946","00:04:35,221",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,And then we have BM25 score on
cs-410_2_6_58,cs-410,2,6,"00:04:35,221","00:04:40,630",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,"Now, these are then the feature values for"
cs-410_2_6_59,cs-410,2,6,"00:04:41,910","00:04:47,370",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,"And in this case, the document is D1 and"
cs-410_2_6_60,cs-410,2,6,"00:04:48,790","00:04:54,547",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,Here's another training instance and
cs-410_2_6_61,cs-410,2,6,"00:04:54,547","00:04:57,832",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,"but in this case, it's not relevant."
cs-410_2_6_62,cs-410,2,6,"00:04:57,832","00:05:02,806",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,This is an oversimplified case where
cs-410_2_6_63,cs-410,2,6,"00:05:02,806","00:05:06,675",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,it's sufficient to illustrate the point.
cs-410_2_6_64,cs-410,2,6,"00:05:06,675","00:05:09,885",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,So what we can do is we use
cs-410_2_6_65,cs-410,2,6,"00:05:09,885","00:05:11,797",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,actually estimate the parameters.
cs-410_2_6_66,cs-410,2,6,"00:05:13,170","00:05:17,801",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,"Basically, we're going to"
cs-410_2_6_67,cs-410,2,6,"00:05:17,801","00:05:22,040",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,of the document based
cs-410_2_6_68,cs-410,2,6,"00:05:22,040","00:05:25,534",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,"That is, given that we observed"
cs-410_2_6_69,cs-410,2,6,"00:05:28,264","00:05:32,653",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,Can we predict the relevance here?
cs-410_2_6_70,cs-410,2,6,"00:05:32,653","00:05:39,070",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,"Now, of course, the prediction would be"
cs-410_2_6_71,cs-410,2,6,"00:05:39,070","00:05:42,680",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,And we hypothesize that the probability
cs-410_2_6_72,cs-410,2,6,"00:05:42,680","00:05:43,920",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,features in this way.
cs-410_2_6_73,cs-410,2,6,"00:05:43,920","00:05:51,260",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"So we are going to see, for what values of"
cs-410_2_6_74,cs-410,2,6,"00:05:51,260","00:05:58,480",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,What do we mean by predicting
cs-410_2_6_75,cs-410,2,6,"00:05:58,480","00:06:02,037",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,"Well, we just mean, in the first case, for"
cs-410_2_6_76,cs-410,2,6,"00:06:02,037","00:06:06,667",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,D1 this expression right here
cs-410_2_6_77,cs-410,2,6,"00:06:06,667","00:06:10,452",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,"In fact, we'll hope this"
cs-410_2_6_78,cs-410,2,6,"00:06:10,452","00:06:13,470",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,Why?
cs-410_2_6_79,cs-410,2,6,"00:06:14,750","00:06:17,954",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"On the other hand,"
cs-410_2_6_80,cs-410,2,6,"00:06:17,954","00:06:22,310",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,"we hope this value will be small, right."
cs-410_2_6_81,cs-410,2,6,"00:06:22,310","00:06:23,040",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,Why?
cs-410_2_6_82,cs-410,2,6,"00:06:23,040","00:06:26,310",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,Because it's a non-relevant document.
cs-410_2_6_83,cs-410,2,6,"00:06:26,310","00:06:30,250",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,So now let's see how this can
cs-410_2_6_84,cs-410,2,6,"00:06:30,250","00:06:34,954",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,And this is similar to expressing
cs-410_2_6_85,cs-410,2,6,"00:06:34,954","00:06:39,657",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,only that we are not talking about
cs-410_2_6_86,cs-410,2,6,"00:06:39,657","00:06:43,771",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,talking about the probability
cs-410_2_6_87,cs-410,2,6,"00:06:43,771","00:06:48,736",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,So what's the probability
cs-410_2_6_88,cs-410,2,6,"00:06:48,736","00:06:52,880",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,relevant if it has these feature values?
cs-410_2_6_89,cs-410,2,6,"00:06:54,250","00:06:57,890",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,"Well, this is just this expression."
cs-410_2_6_90,cs-410,2,6,"00:06:57,890","00:07:00,970",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,We just need to plug in the Xi's.
cs-410_2_6_91,cs-410,2,6,"00:07:00,970","00:07:03,296",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,So that's what we will get.
cs-410_2_6_92,cs-410,2,6,"00:07:03,296","00:07:08,116",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,"It's exactly like what we have seen above,"
cs-410_2_6_93,cs-410,2,6,"00:07:08,116","00:07:14,772",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,only that we replaced these
cs-410_2_6_94,cs-410,2,6,"00:07:14,772","00:07:21,247",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,"So for example, this 0.7 goes to here and"
cs-410_2_6_95,cs-410,2,6,"00:07:21,247","00:07:25,451",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,this 0.11 goes to here.
cs-410_2_6_96,cs-410,2,6,"00:07:25,451","00:07:27,369",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,"And these are different feature values,"
cs-410_2_6_97,cs-410,2,6,"00:07:27,369","00:07:29,405",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,and we combine them in
cs-410_2_6_98,cs-410,2,6,"00:07:29,405","00:07:31,770",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,The beta values are still unknown.
cs-410_2_6_99,cs-410,2,6,"00:07:31,770","00:07:37,202",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,But this gives us the probability
cs-410_2_6_100,cs-410,2,6,"00:07:37,202","00:07:39,342",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,if we assume such a model.
cs-410_2_6_101,cs-410,2,6,"00:07:39,342","00:07:39,853",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,Okay?
cs-410_2_6_102,cs-410,2,6,"00:07:39,853","00:07:44,553",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,"And we want to maximize this probability,"
cs-410_2_6_103,cs-410,2,6,"00:07:44,553","00:07:47,850",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,What do we do for the second document?
cs-410_2_6_104,cs-410,2,6,"00:07:47,850","00:07:53,309",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,"Well, we want to compute the probability"
cs-410_2_6_105,cs-410,2,6,"00:07:53,309","00:08:00,257",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,So this would mean we have to
cs-410_2_6_106,cs-410,2,6,"00:08:00,257","00:08:07,880",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,since this expression is actually
cs-410_2_6_107,cs-410,2,6,"00:08:07,880","00:08:12,524",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So to compute the non-relevance
cs-410_2_6_108,cs-410,2,6,"00:08:12,524","00:08:17,062",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,we just do 1 minus
cs-410_2_6_109,cs-410,2,6,"00:08:17,062","00:08:18,480",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,Okay?
cs-410_2_6_110,cs-410,2,6,"00:08:18,480","00:08:24,450",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,So this whole expression then
cs-410_2_6_111,cs-410,2,6,"00:08:24,450","00:08:29,220",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,predicting these two relevance values.
cs-410_2_6_112,cs-410,2,6,"00:08:29,220","00:08:32,759",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,"One is 1 here, one is 0."
cs-410_2_6_113,cs-410,2,6,"00:08:32,759","00:08:37,782",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,And this whole equation
cs-410_2_6_114,cs-410,2,6,"00:08:37,782","00:08:42,680",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,observing a 1 here and observing a 0 here.
cs-410_2_6_115,cs-410,2,6,"00:08:44,090","00:08:48,370",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,"Of course, this probability"
cs-410_2_6_116,cs-410,2,6,"00:08:50,130","00:08:55,318",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,So then our goal is to adjust
cs-410_2_6_117,cs-410,2,6,"00:08:55,318","00:09:00,121",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,"thing reach its maximum,"
cs-410_2_6_118,cs-410,2,6,"00:09:00,121","00:09:02,540",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,So that means we're going to compute this.
cs-410_2_6_119,cs-410,2,6,"00:09:02,540","00:09:07,280",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,The beta is just the parameter
cs-410_2_6_120,cs-410,2,6,"00:09:07,280","00:09:11,914",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,maximize this whole likelihood expression.
cs-410_2_6_121,cs-410,2,6,"00:09:11,914","00:09:16,284",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,"And what it means is,"
cs-410_2_6_122,cs-410,2,6,"00:09:16,284","00:09:21,224",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,we're going to choose betas to
cs-410_2_6_123,cs-410,2,6,"00:09:21,224","00:09:26,449",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,"make this also as large as possible,"
cs-410_2_6_124,cs-410,2,6,"00:09:26,449","00:09:29,400",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,make this part as small as possible.
cs-410_2_6_125,cs-410,2,6,"00:09:30,560","00:09:32,360",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,And this is precisely what we want.
cs-410_2_6_126,cs-410,2,6,"00:09:34,530","00:09:38,834",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"So once we do the training,"
cs-410_2_6_127,cs-410,2,6,"00:09:38,834","00:09:43,330",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,So then this function
cs-410_2_6_128,cs-410,2,6,"00:09:43,330","00:09:50,690",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"Once beta values are known, both this and"
cs-410_2_6_129,cs-410,2,6,"00:09:50,690","00:09:53,380",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,"So for any new query and new document,"
cs-410_2_6_130,cs-410,2,6,"00:09:53,380","00:09:56,924",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,we can simply compute the features for
cs-410_2_6_131,cs-410,2,6,"00:09:56,924","00:10:00,941",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,And then we just use this formula
cs-410_2_6_132,cs-410,2,6,"00:10:00,941","00:10:06,700",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,And this scoring function can be used to
cs-410_2_6_133,cs-410,2,6,"00:10:06,700","00:10:11,787",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,So that's the basic idea
cs-410_2_6_134,cs-410,2,6,"00:10:11,787","00:10:21,787",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,[MUSIC]
cs-410_2_7_1,cs-410,2,7,"00:00:00,266","00:00:09,956",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_2_7_2,cs-410,2,7,"00:00:09,956","00:00:14,850",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,looking at the text mining problem more
cs-410_2_7_3,cs-410,2,7,"00:00:14,850","00:00:20,300",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"similar to general data mining, except"
cs-410_2_7_4,cs-410,2,7,"00:00:21,710","00:00:26,400",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,And we're going to have text mining
cs-410_2_7_5,cs-410,2,7,"00:00:26,400","00:00:32,240",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,into actionable knowledge that
cs-410_2_7_6,cs-410,2,7,"00:00:32,240","00:00:34,130",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"especially for decision making, or"
cs-410_2_7_7,cs-410,2,7,"00:00:34,130","00:00:39,350",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,for completing whatever tasks that
cs-410_2_7_8,cs-410,2,7,"00:00:39,350","00:00:45,000",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"Because, in general,"
cs-410_2_7_9,cs-410,2,7,"00:00:45,000","00:00:49,720",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,we also tend to have other kinds
cs-410_2_7_10,cs-410,2,7,"00:00:49,720","00:00:54,950",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,So a more general picture would be
cs-410_2_7_11,cs-410,2,7,"00:00:56,000","00:01:00,875",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,And for this reason we might be
cs-410_2_7_12,cs-410,2,7,"00:01:00,875","00:01:02,060",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,non-text data.
cs-410_2_7_13,cs-410,2,7,"00:01:02,060","00:01:05,860",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,And so in this course we're
cs-410_2_7_14,cs-410,2,7,"00:01:05,860","00:01:10,628",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,but we're also going to also touch how do
cs-410_2_7_15,cs-410,2,7,"00:01:10,628","00:01:12,450",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,non-text data.
cs-410_2_7_16,cs-410,2,7,"00:01:12,450","00:01:16,630",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,With this problem definition we
cs-410_2_7_17,cs-410,2,7,"00:01:16,630","00:01:19,419",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,the topics in text mining and analytics.
cs-410_2_7_18,cs-410,2,7,"00:01:21,010","00:01:25,770",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,Now this slide shows the process of
cs-410_2_7_19,cs-410,2,7,"00:01:27,018","00:01:29,800",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"More specifically, a human sensor or"
cs-410_2_7_20,cs-410,2,7,"00:01:29,800","00:01:33,420",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,human observer would look at
cs-410_2_7_21,cs-410,2,7,"00:01:34,660","00:01:38,820",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,Different people would be looking at
cs-410_2_7_22,cs-410,2,7,"00:01:38,820","00:01:41,210",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,they'll pay attention to different things.
cs-410_2_7_23,cs-410,2,7,"00:01:41,210","00:01:46,090",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,The same person at different times might
cs-410_2_7_24,cs-410,2,7,"00:01:46,090","00:01:50,990",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,of the observed world.
cs-410_2_7_25,cs-410,2,7,"00:01:50,990","00:01:55,450",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,And so the humans are able to perceive
cs-410_2_7_26,cs-410,2,7,"00:01:55,450","00:02:01,480",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,"And that human, the sensor,"
cs-410_2_7_27,cs-410,2,7,"00:02:01,480","00:02:05,150",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,And that can be called the Observed World.
cs-410_2_7_28,cs-410,2,7,"00:02:05,150","00:02:10,040",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"Of course, this would be different from"
cs-410_2_7_29,cs-410,2,7,"00:02:10,040","00:02:14,830",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,that the person has taken
cs-410_2_7_30,cs-410,2,7,"00:02:16,840","00:02:22,642",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,Now the Observed World can be
cs-410_2_7_31,cs-410,2,7,"00:02:22,642","00:02:27,535",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,entity-relation graphs or
cs-410_2_7_32,cs-410,2,7,"00:02:27,535","00:02:31,890",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,using knowledge representation language.
cs-410_2_7_33,cs-410,2,7,"00:02:31,890","00:02:39,190",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,"But in general, this is basically what"
cs-410_2_7_34,cs-410,2,7,"00:02:39,190","00:02:43,800",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,And we don't really know what
cs-410_2_7_35,cs-410,2,7,"00:02:43,800","00:02:48,250",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,But then the human would
cs-410_2_7_36,cs-410,2,7,"00:02:48,250","00:02:52,920",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,"observed using a natural language,"
cs-410_2_7_37,cs-410,2,7,"00:02:52,920","00:02:54,760",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,And the result is text data.
cs-410_2_7_38,cs-410,2,7,"00:02:55,870","00:03:00,610",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,Of course a person could have used
cs-410_2_7_39,cs-410,2,7,"00:03:00,610","00:03:02,660",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,she has observed.
cs-410_2_7_40,cs-410,2,7,"00:03:02,660","00:03:08,220",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,In that case we might have text data of
cs-410_2_7_41,cs-410,2,7,"00:03:10,590","00:03:15,790",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,The main goal of text mining
cs-410_2_7_42,cs-410,2,7,"00:03:15,790","00:03:19,280",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,process of generating text data.
cs-410_2_7_43,cs-410,2,7,"00:03:19,280","00:03:24,480",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,We hope to be able to uncover
cs-410_2_7_44,cs-410,2,7,"00:03:28,340","00:03:34,480",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,"Specifically, we can think about mining,"
cs-410_2_7_45,cs-410,2,7,"00:03:35,560","00:03:40,130",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,And that means by looking at text data
cs-410_2_7_46,cs-410,2,7,"00:03:40,130","00:03:46,310",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,"something about English, some usage"
cs-410_2_7_47,cs-410,2,7,"00:03:47,780","00:03:52,340",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"So this is one type of mining problems,"
cs-410_2_7_48,cs-410,2,7,"00:03:52,340","00:03:57,100",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,some knowledge about language which
cs-410_2_7_49,cs-410,2,7,"00:03:58,920","00:04:00,620",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,"If you look at the picture,"
cs-410_2_7_50,cs-410,2,7,"00:04:00,620","00:04:06,380",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,we can also then mine knowledge
cs-410_2_7_51,cs-410,2,7,"00:04:06,380","00:04:10,030",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,And so this has much to do with
cs-410_2_7_52,cs-410,2,7,"00:04:11,490","00:04:15,640",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,We're going to look at what the text
cs-410_2_7_53,cs-410,2,7,"00:04:15,640","00:04:20,820",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,get the essence of it or
cs-410_2_7_54,cs-410,2,7,"00:04:20,820","00:04:25,600",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,about a particular aspect of
cs-410_2_7_55,cs-410,2,7,"00:04:26,900","00:04:30,890",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"For example, everything that has been"
cs-410_2_7_56,cs-410,2,7,"00:04:30,890","00:04:31,630",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,a particular entity.
cs-410_2_7_57,cs-410,2,7,"00:04:31,630","00:04:36,550",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,And this can be regarded as mining content
cs-410_2_7_58,cs-410,2,7,"00:04:36,550","00:04:43,330",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,to describe the observed world in
cs-410_2_7_59,cs-410,2,7,"00:04:45,020","00:04:50,060",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,"If you look further,"
cs-410_2_7_60,cs-410,2,7,"00:04:50,060","00:04:54,710",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,"we can mine knowledge about this observer,"
cs-410_2_7_61,cs-410,2,7,"00:04:54,710","00:05:00,630",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,So this has also to do with
cs-410_2_7_62,cs-410,2,7,"00:05:00,630","00:05:02,270",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,some properties of this person.
cs-410_2_7_63,cs-410,2,7,"00:05:03,380","00:05:07,410",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,And these properties could
cs-410_2_7_64,cs-410,2,7,"00:05:07,410","00:05:09,010",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,sentiment of the person.
cs-410_2_7_65,cs-410,2,7,"00:05:10,200","00:05:15,250",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,And note that we distinguish
cs-410_2_7_66,cs-410,2,7,"00:05:15,250","00:05:21,040",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,because text data can't describe what the
cs-410_2_7_67,cs-410,2,7,"00:05:21,040","00:05:25,960",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,But the description can be also
cs-410_2_7_68,cs-410,2,7,"00:05:25,960","00:05:30,280",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,"in general, you can imagine the text"
cs-410_2_7_69,cs-410,2,7,"00:05:30,280","00:05:34,770",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,descriptions of the world plus
cs-410_2_7_70,cs-410,2,7,"00:05:34,770","00:05:37,330",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,So that's why it's also possible to
cs-410_2_7_71,cs-410,2,7,"00:05:37,330","00:05:41,970",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,do text mining to mine
cs-410_2_7_72,cs-410,2,7,"00:05:41,970","00:05:45,980",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,"Finally, if you look at the picture"
cs-410_2_7_73,cs-410,2,7,"00:05:45,980","00:05:50,150",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,then you can see we can certainly also
cs-410_2_7_74,cs-410,2,7,"00:05:50,150","00:05:50,880",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,Right?
cs-410_2_7_75,cs-410,2,7,"00:05:50,880","00:05:56,860",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,So indeed we can do text mining to
cs-410_2_7_76,cs-410,2,7,"00:05:56,860","00:05:59,220",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,And this is often called
cs-410_2_7_77,cs-410,2,7,"00:06:00,600","00:06:04,000",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,And we want to predict the value
cs-410_2_7_78,cs-410,2,7,"00:06:04,000","00:06:08,935",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"So, this picture basically covered"
cs-410_2_7_79,cs-410,2,7,"00:06:08,935","00:06:13,440",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,multiple types of knowledge that
cs-410_2_7_80,cs-410,2,7,"00:06:14,550","00:06:19,260",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,When we infer other
cs-410_2_7_81,cs-410,2,7,"00:06:19,260","00:06:24,990",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,could also use some of the results from
cs-410_2_7_82,cs-410,2,7,"00:06:24,990","00:06:30,910",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,mining text data as intermediate
cs-410_2_7_83,cs-410,2,7,"00:06:30,910","00:06:31,940",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,"For example,"
cs-410_2_7_84,cs-410,2,7,"00:06:31,940","00:06:38,050",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,after we mine the content of text data we
cs-410_2_7_85,cs-410,2,7,"00:06:38,050","00:06:41,190",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,And that summary could be then used
cs-410_2_7_86,cs-410,2,7,"00:06:41,190","00:06:45,003",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,to help us predict the variables
cs-410_2_7_87,cs-410,2,7,"00:06:45,003","00:06:51,940",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,Now of course this is still generated
cs-410_2_7_88,cs-410,2,7,"00:06:51,940","00:06:58,410",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,but I want to emphasize here that
cs-410_2_7_89,cs-410,2,7,"00:06:58,410","00:07:03,780",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,to generate some features that can help
cs-410_2_7_90,cs-410,2,7,"00:07:04,960","00:07:10,100",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,And that's why here we show the results of
cs-410_2_7_91,cs-410,2,7,"00:07:10,100","00:07:15,010",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,"some other mining tasks, including"
cs-410_2_7_92,cs-410,2,7,"00:07:15,010","00:07:19,520",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"mining knowledge about the observer,"
cs-410_2_7_93,cs-410,2,7,"00:07:21,380","00:07:26,690",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,"In fact, when we have non-text data,"
cs-410_2_7_94,cs-410,2,7,"00:07:26,690","00:07:31,496",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"data to help prediction, and"
cs-410_2_7_95,cs-410,2,7,"00:07:31,496","00:07:39,260",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"In general, non-text data can be very"
cs-410_2_7_96,cs-410,2,7,"00:07:39,260","00:07:44,530",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,"For example,"
cs-410_2_7_97,cs-410,2,7,"00:07:44,530","00:07:49,870",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,changes of stock prices based on
cs-410_2_7_98,cs-410,2,7,"00:07:49,870","00:07:53,730",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"in social media, then this is an example"
cs-410_2_7_99,cs-410,2,7,"00:07:53,730","00:07:58,520",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,of using text data to predict
cs-410_2_7_100,cs-410,2,7,"00:07:58,520","00:07:59,950",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,"But in this case, obviously,"
cs-410_2_7_101,cs-410,2,7,"00:07:59,950","00:08:04,480",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,the historical stock price data would
cs-410_2_7_102,cs-410,2,7,"00:08:04,480","00:08:09,633",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,And so that's an example of
cs-410_2_7_103,cs-410,2,7,"00:08:09,633","00:08:13,750",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,useful for the prediction.
cs-410_2_7_104,cs-410,2,7,"00:08:13,750","00:08:17,161",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,And we're going to combine both kinds
cs-410_2_7_105,cs-410,2,7,"00:08:17,161","00:08:24,510",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,Now non-text data can be also used for
cs-410_2_7_106,cs-410,2,7,"00:08:25,580","00:08:27,050",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,"When we look at the text data alone,"
cs-410_2_7_107,cs-410,2,7,"00:08:27,050","00:08:31,770",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,we'll be mostly looking at the content
cs-410_2_7_108,cs-410,2,7,"00:08:32,790","00:08:36,149",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,But text data generally also
cs-410_2_7_109,cs-410,2,7,"00:08:37,470","00:08:44,150",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,"For example, the time and the location"
cs-410_2_7_110,cs-410,2,7,"00:08:44,150","00:08:47,500",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,And these are useful context information.
cs-410_2_7_111,cs-410,2,7,"00:08:48,740","00:08:54,020",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,And the context can provide interesting
cs-410_2_7_112,cs-410,2,7,"00:08:54,020","00:08:57,980",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,"For example, we might partition text"
cs-410_2_7_113,cs-410,2,7,"00:08:57,980","00:09:00,680",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,because of the availability of the time.
cs-410_2_7_114,cs-410,2,7,"00:09:00,680","00:09:06,480",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,Now we can analyze text data in each
cs-410_2_7_115,cs-410,2,7,"00:09:06,480","00:09:09,970",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,Similarly we can partition text
cs-410_2_7_116,cs-410,2,7,"00:09:09,970","00:09:15,920",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,any meta data that's associated to
cs-410_2_7_117,cs-410,2,7,"00:09:15,920","00:09:20,980",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,"So, in this sense,"
cs-410_2_7_118,cs-410,2,7,"00:09:20,980","00:09:24,580",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,interesting angles or
cs-410_2_7_119,cs-410,2,7,"00:09:24,580","00:09:29,340",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,And it can help us make context-sensitive
cs-410_2_7_120,cs-410,2,7,"00:09:29,340","00:09:33,680",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,analysis of content or
cs-410_2_7_121,cs-410,2,7,"00:09:36,390","00:09:42,920",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,the opinions about the observer or
cs-410_2_7_122,cs-410,2,7,"00:09:42,920","00:09:46,920",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,We could analyze the sentiment
cs-410_2_7_123,cs-410,2,7,"00:09:46,920","00:09:54,500",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,So this is a fairly general landscape of
cs-410_2_7_124,cs-410,2,7,"00:09:54,500","00:09:59,850",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,In this course we're going to
cs-410_2_7_125,cs-410,2,7,"00:09:59,850","00:10:03,796",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,We actually hope to cover
cs-410_2_7_126,cs-410,2,7,"00:10:06,675","00:10:11,321",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,First we're going to cover
cs-410_2_7_127,cs-410,2,7,"00:10:11,321","00:10:16,053",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,briefly because this has to do
cs-410_2_7_128,cs-410,2,7,"00:10:16,053","00:10:21,580",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,this determines how we can represent
cs-410_2_7_129,cs-410,2,7,"00:10:21,580","00:10:27,870",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,"Second, we're going to talk about how to"
cs-410_2_7_130,cs-410,2,7,"00:10:27,870","00:10:34,340",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,And word associations is a form of use for
cs-410_2_7_131,cs-410,2,7,"00:10:34,340","00:10:38,340",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"Third, we're going to talk about"
cs-410_2_7_132,cs-410,2,7,"00:10:38,340","00:10:43,190",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,And this is only one way to
cs-410_2_7_133,cs-410,2,7,"00:10:43,190","00:10:46,100",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,it's a very useful ways
cs-410_2_7_134,cs-410,2,7,"00:10:46,100","00:10:51,400",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,It's also one of the most useful
cs-410_2_7_135,cs-410,2,7,"00:10:53,750","00:10:59,510",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,Then we're going to talk about
cs-410_2_7_136,cs-410,2,7,"00:10:59,510","00:11:05,250",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,So this can be regarded as one example
cs-410_2_7_137,cs-410,2,7,"00:11:07,140","00:11:11,510",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,And finally we're going to
cs-410_2_7_138,cs-410,2,7,"00:11:11,510","00:11:16,020",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,problems where we try to predict some
cs-410_2_7_139,cs-410,2,7,"00:11:17,400","00:11:24,880",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,So this slide also serves as
cs-410_2_7_140,cs-410,2,7,"00:11:24,880","00:11:27,554",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,And we're going to use
cs-410_2_7_141,cs-410,2,7,"00:11:27,554","00:11:30,962",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,the topics that we'll cover
cs-410_2_7_142,cs-410,2,7,"00:11:30,962","00:11:40,962",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,[MUSIC]
cs-410_2_8_1,cs-410,2,8,"00:00:00,025","00:00:05,819",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND] This lecture is
cs-410_2_8_2,cs-410,2,8,"00:00:05,819","00:00:12,090",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=5,relation discovery and
cs-410_2_8_3,cs-410,2,8,"00:00:12,090","00:00:12,963",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture,"
cs-410_2_8_4,cs-410,2,8,"00:00:12,963","00:00:16,939",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,we're going to continue the discussion
cs-410_2_8_5,cs-410,2,8,"00:00:18,060","00:00:22,930",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,We're going to talk about the conditional
cs-410_2_8_6,cs-410,2,8,"00:00:22,930","00:00:25,700",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,discovering syntagmatic relations.
cs-410_2_8_7,cs-410,2,8,"00:00:25,700","00:00:29,400",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"Earlier, we talked about"
cs-410_2_8_8,cs-410,2,8,"00:00:29,400","00:00:33,030",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,how easy it is to predict the presence or
cs-410_2_8_9,cs-410,2,8,"00:00:34,180","00:00:37,700",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,"Now, we'll address"
cs-410_2_8_10,cs-410,2,8,"00:00:37,700","00:00:41,320",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,we assume that we know something
cs-410_2_8_11,cs-410,2,8,"00:00:41,320","00:00:48,830",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,"So now the question is, suppose we know"
cs-410_2_8_12,cs-410,2,8,"00:00:48,830","00:00:51,150",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,How would that help us
cs-410_2_8_13,cs-410,2,8,"00:00:51,150","00:00:53,990",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"absence of water, like in meat?"
cs-410_2_8_14,cs-410,2,8,"00:00:53,990","00:00:58,060",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"And in particular, we want to"
cs-410_2_8_15,cs-410,2,8,"00:00:58,060","00:01:00,959",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,has helped us predict
cs-410_2_8_16,cs-410,2,8,"00:01:02,020","00:01:05,040",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,"And if we frame this using entrophy,"
cs-410_2_8_17,cs-410,2,8,"00:01:05,040","00:01:10,700",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,that would mean we are interested
cs-410_2_8_18,cs-410,2,8,"00:01:10,700","00:01:15,100",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,the presence of eats could reduce
cs-410_2_8_19,cs-410,2,8,"00:01:15,100","00:01:18,800",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,"Or, reduce the entrophy"
cs-410_2_8_20,cs-410,2,8,"00:01:18,800","00:01:23,430",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,corresponding to the presence or
cs-410_2_8_21,cs-410,2,8,"00:01:23,430","00:01:27,950",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"We can also ask as a question,"
cs-410_2_8_22,cs-410,2,8,"00:01:28,950","00:01:33,010",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,Would that also help us predict
cs-410_2_8_23,cs-410,2,8,"00:01:34,720","00:01:39,415",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,These questions can be
cs-410_2_8_24,cs-410,2,8,"00:01:39,415","00:01:43,120",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,concept called a conditioning entropy.
cs-410_2_8_25,cs-410,2,8,"00:01:43,120","00:01:48,460",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"So to explain this concept, let's first"
cs-410_2_8_26,cs-410,2,8,"00:01:48,460","00:01:51,218",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,when we know nothing about the segment.
cs-410_2_8_27,cs-410,2,8,"00:01:51,218","00:01:56,522",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,So we have these probabilities indicating
cs-410_2_8_28,cs-410,2,8,"00:01:56,522","00:01:58,830",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,or it doesn't occur in the segment.
cs-410_2_8_29,cs-410,2,8,"00:01:58,830","00:02:02,650",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,And we have an entropy function that
cs-410_2_8_30,cs-410,2,8,"00:02:03,810","00:02:07,410",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,"Now suppose we know eats is present, so"
cs-410_2_8_31,cs-410,2,8,"00:02:07,410","00:02:11,330",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,now we know the value of another
cs-410_2_8_32,cs-410,2,8,"00:02:12,730","00:02:15,270",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,"Now, that would change all"
cs-410_2_8_33,cs-410,2,8,"00:02:15,270","00:02:17,550",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,conditional probabilities.
cs-410_2_8_34,cs-410,2,8,"00:02:17,550","00:02:20,580",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,Where we look at the presence or
cs-410_2_8_35,cs-410,2,8,"00:02:21,800","00:02:25,570",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,given that we know eats
cs-410_2_8_36,cs-410,2,8,"00:02:25,570","00:02:27,480",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,"So as a result,"
cs-410_2_8_37,cs-410,2,8,"00:02:27,480","00:02:31,820",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,if we replace these probabilities
cs-410_2_8_38,cs-410,2,8,"00:02:31,820","00:02:36,320",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,"probabilities in the entropy function,"
cs-410_2_8_39,cs-410,2,8,"00:02:37,650","00:02:42,522",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,So this equation now here would be
cs-410_2_8_40,cs-410,2,8,"00:02:42,522","00:02:46,900",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,the conditional entropy.
cs-410_2_8_41,cs-410,2,8,"00:02:46,900","00:02:49,150",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,Conditional on the presence of eats.
cs-410_2_8_42,cs-410,2,8,"00:02:52,180","00:02:57,070",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"So, you can see this is essentially"
cs-410_2_8_43,cs-410,2,8,"00:02:57,070","00:03:01,900",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,"seen before, except that all"
cs-410_2_8_44,cs-410,2,8,"00:03:04,420","00:03:09,550",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,And this then tells us
cs-410_2_8_45,cs-410,2,8,"00:03:09,550","00:03:13,020",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,after we have known eats
cs-410_2_8_46,cs-410,2,8,"00:03:14,380","00:03:17,770",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,"And of course, we can also define"
cs-410_2_8_47,cs-410,2,8,"00:03:17,770","00:03:20,540",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,the scenario where we don't see eats.
cs-410_2_8_48,cs-410,2,8,"00:03:20,540","00:03:25,150",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,So if we know it did not occur in
cs-410_2_8_49,cs-410,2,8,"00:03:25,150","00:03:30,710",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,entropy would capture the instances
cs-410_2_8_50,cs-410,2,8,"00:03:30,710","00:03:34,110",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,"So now,"
cs-410_2_8_51,cs-410,2,8,"00:03:34,110","00:03:37,609",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,we have the completed definition
cs-410_2_8_52,cs-410,2,8,"00:03:39,250","00:03:48,520",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"Basically, we're going to consider both"
cs-410_2_8_53,cs-410,2,8,"00:03:48,520","00:03:54,280",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,and this gives us a probability
cs-410_2_8_54,cs-410,2,8,"00:03:54,280","00:03:58,040",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"Basically, whether eats is present or"
cs-410_2_8_55,cs-410,2,8,"00:03:58,040","00:03:59,150",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,"And this of course,"
cs-410_2_8_56,cs-410,2,8,"00:03:59,150","00:04:04,310",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,is the conditional entropy of
cs-410_2_8_57,cs-410,2,8,"00:04:05,510","00:04:10,110",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"So if you expanded this entropy,"
cs-410_2_8_58,cs-410,2,8,"00:04:10,110","00:04:14,330",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,then you have the following equation.
cs-410_2_8_59,cs-410,2,8,"00:04:15,760","00:04:19,429",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,Where you see the involvement of
cs-410_2_8_60,cs-410,2,8,"00:04:21,530","00:04:26,330",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,"Now in general, for any discrete"
cs-410_2_8_61,cs-410,2,8,"00:04:27,940","00:04:35,240",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,the conditional entropy is no larger
cs-410_2_8_62,cs-410,2,8,"00:04:35,240","00:04:41,950",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,"So basically, this is upper bound for"
cs-410_2_8_63,cs-410,2,8,"00:04:41,950","00:04:46,380",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,That means by knowing more
cs-410_2_8_64,cs-410,2,8,"00:04:46,380","00:04:49,630",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,we want to be able to
cs-410_2_8_65,cs-410,2,8,"00:04:49,630","00:04:51,570",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,We can only reduce uncertainty.
cs-410_2_8_66,cs-410,2,8,"00:04:51,570","00:04:56,180",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,And that intuitively makes sense
cs-410_2_8_67,cs-410,2,8,"00:04:56,180","00:05:00,180",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,it should always help
cs-410_2_8_68,cs-410,2,8,"00:05:00,180","00:05:04,000",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,And cannot hurt
cs-410_2_8_69,cs-410,2,8,"00:05:05,420","00:05:08,880",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,"Now, what's interesting here is also to"
cs-410_2_8_70,cs-410,2,8,"00:05:08,880","00:05:11,770",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,value of this conditional entropy?
cs-410_2_8_71,cs-410,2,8,"00:05:11,770","00:05:16,270",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,"Now, we know that the maximum"
cs-410_2_8_72,cs-410,2,8,"00:05:17,940","00:05:20,313",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,"But what about the minimum,"
cs-410_2_8_73,cs-410,2,8,"00:05:22,883","00:05:28,552",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,I hope you can reach the conclusion that
cs-410_2_8_74,cs-410,2,8,"00:05:28,552","00:05:33,090",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,And it will be interesting to think about
cs-410_2_8_75,cs-410,2,8,"00:05:34,120","00:05:37,860",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,"So, let's see how we can use conditional"
cs-410_2_8_76,cs-410,2,8,"00:05:39,420","00:05:44,250",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"Now of course,"
cs-410_2_8_77,cs-410,2,8,"00:05:44,250","00:05:48,300",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,one way to measure
cs-410_2_8_78,cs-410,2,8,"00:05:48,300","00:05:53,750",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"Because it tells us to what extent,"
cs-410_2_8_79,cs-410,2,8,"00:05:53,750","00:05:58,995",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,word given that we know the presence or
cs-410_2_8_80,cs-410,2,8,"00:05:58,995","00:06:03,900",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,Now before we look at the intuition
cs-410_2_8_81,cs-410,2,8,"00:06:03,900","00:06:09,090",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,"syntagmatic relations, it's useful to"
cs-410_2_8_82,cs-410,2,8,"00:06:09,090","00:06:17,910",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,"That is, the conditional entropy"
cs-410_2_8_83,cs-410,2,8,"00:06:19,000","00:06:22,980",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,"So here,"
cs-410_2_8_84,cs-410,2,8,"00:06:22,980","00:06:28,420",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,we listed this conditional
cs-410_2_8_85,cs-410,2,8,"00:06:28,420","00:06:31,280",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,"So, it's here."
cs-410_2_8_86,cs-410,2,8,"00:06:33,550","00:06:35,100",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,"So, what is the value of this?"
cs-410_2_8_87,cs-410,2,8,"00:06:36,380","00:06:43,370",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"Now, this means we know where"
cs-410_2_8_88,cs-410,2,8,"00:06:43,370","00:06:47,717",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,And we hope to predict whether
cs-410_2_8_89,cs-410,2,8,"00:06:47,717","00:06:52,518",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,"And of course, this is 0 because"
cs-410_2_8_90,cs-410,2,8,"00:06:52,518","00:06:55,862",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,Once we know whether the word
cs-410_2_8_91,cs-410,2,8,"00:06:55,862","00:06:59,132",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,we'll already know the answer
cs-410_2_8_92,cs-410,2,8,"00:06:59,132","00:07:00,410",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,So this is zero.
cs-410_2_8_93,cs-410,2,8,"00:07:00,410","00:07:03,390",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,And that's also when this conditional
cs-410_2_8_94,cs-410,2,8,"00:07:06,280","00:07:08,280",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,"So now, let's look at some other cases."
cs-410_2_8_95,cs-410,2,8,"00:07:09,530","00:07:15,840",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,So this is a case of knowing the and
cs-410_2_8_96,cs-410,2,8,"00:07:15,840","00:07:20,840",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,And this is a case of knowing eats and
cs-410_2_8_97,cs-410,2,8,"00:07:20,840","00:07:22,870",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,Which one do you think is smaller?
cs-410_2_8_98,cs-410,2,8,"00:07:22,870","00:07:27,763",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,No doubt smaller entropy means easier for
cs-410_2_8_99,cs-410,2,8,"00:07:31,511","00:07:33,260",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,Which one do you think is higher?
cs-410_2_8_100,cs-410,2,8,"00:07:33,260","00:07:34,820",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,Which one is not smaller?
cs-410_2_8_101,cs-410,2,8,"00:07:36,800","00:07:41,732",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,"Well, if you at the uncertainty,"
cs-410_2_8_102,cs-410,2,8,"00:07:41,732","00:07:45,730",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,the doesn't really tell
cs-410_2_8_103,cs-410,2,8,"00:07:45,730","00:07:51,520",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,So knowing the occurrence of the doesn't
cs-410_2_8_104,cs-410,2,8,"00:07:51,520","00:07:56,465",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,So it stays fairly close to
cs-410_2_8_105,cs-410,2,8,"00:07:56,465","00:08:01,120",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,"Whereas in the case of eats,"
cs-410_2_8_106,cs-410,2,8,"00:08:01,120","00:08:04,420",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,So knowing presence of eats or
cs-410_2_8_107,cs-410,2,8,"00:08:04,420","00:08:07,780",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,would help us predict whether meat occurs.
cs-410_2_8_108,cs-410,2,8,"00:08:07,780","00:08:14,290",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So it can help us reduce entropy of meat.
cs-410_2_8_109,cs-410,2,8,"00:08:14,290","00:08:20,470",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,"So we should expect the sigma term, namely"
cs-410_2_8_110,cs-410,2,8,"00:08:21,630","00:08:25,870",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,And that means there is a stronger
cs-410_2_8_111,cs-410,2,8,"00:08:29,070","00:08:36,360",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,So we now also know when
cs-410_2_8_112,cs-410,2,8,"00:08:36,360","00:08:41,400",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"meat, then the conditional entropy"
cs-410_2_8_113,cs-410,2,8,"00:08:41,400","00:08:45,300",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,And for what kind of words
cs-410_2_8_114,cs-410,2,8,"00:08:45,300","00:08:49,885",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,"Well, that's when this stuff"
cs-410_2_8_115,cs-410,2,8,"00:08:49,885","00:08:55,339",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,"And like the for example,"
cs-410_2_8_116,cs-410,2,8,"00:08:55,339","00:08:58,480",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,which is the entropy of meat itself.
cs-410_2_8_117,cs-410,2,8,"00:08:59,970","00:09:03,050",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,So this suggests that when you
cs-410_2_8_118,cs-410,2,8,"00:09:03,050","00:09:07,710",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,"mining syntagmatic relations,"
cs-410_2_8_119,cs-410,2,8,"00:09:10,140","00:09:14,780",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,"For each word W1, we're going to"
cs-410_2_8_120,cs-410,2,8,"00:09:14,780","00:09:21,020",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,"And then, we can compute"
cs-410_2_8_121,cs-410,2,8,"00:09:22,170","00:09:26,630",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,We thought all the candidate was in
cs-410_2_8_122,cs-410,2,8,"00:09:26,630","00:09:30,090",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,"because we're out of favor,"
cs-410_2_8_123,cs-410,2,8,"00:09:30,090","00:09:34,637",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,Meaning that it helps us predict
cs-410_2_8_124,cs-410,2,8,"00:09:34,637","00:09:38,378",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"And then, we're going to take the top ring"
cs-410_2_8_125,cs-410,2,8,"00:09:38,378","00:09:40,480",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,potential syntagmatic relations with W1.
cs-410_2_8_126,cs-410,2,8,"00:09:41,910","00:09:47,700",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,Note that we need to use
cs-410_2_8_127,cs-410,2,8,"00:09:47,700","00:09:51,474",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,The stresser can be the number
cs-410_2_8_128,cs-410,2,8,"00:09:51,474","00:09:54,550",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,absolute value for
cs-410_2_8_129,cs-410,2,8,"00:09:55,900","00:10:00,110",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,"Now, this would allow us to mine the most"
cs-410_2_8_130,cs-410,2,8,"00:10:00,110","00:10:03,700",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,strongly correlated words with
cs-410_2_8_131,cs-410,2,8,"00:10:06,380","00:10:10,560",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,"But, this algorithm does not"
cs-410_2_8_132,cs-410,2,8,"00:10:10,560","00:10:14,800",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,that K syntagmatical relations
cs-410_2_8_133,cs-410,2,8,"00:10:14,800","00:10:19,370",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,"Because in order to do that, we have to"
cs-410_2_8_134,cs-410,2,8,"00:10:19,370","00:10:24,010",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,are comparable across different words.
cs-410_2_8_135,cs-410,2,8,"00:10:24,010","00:10:28,470",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,In this case of discovering
cs-410_2_8_136,cs-410,2,8,"00:10:28,470","00:10:33,520",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,"a targeted word like W1, we only need"
cs-410_2_8_137,cs-410,2,8,"00:10:34,980","00:10:38,600",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"for W1, given different words."
cs-410_2_8_138,cs-410,2,8,"00:10:38,600","00:10:40,780",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,"And in this case, they are comparable."
cs-410_2_8_139,cs-410,2,8,"00:10:41,860","00:10:43,690",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,All right.
cs-410_2_8_140,cs-410,2,8,"00:10:43,690","00:10:48,040",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,"So, the conditional entropy of W1, given"
cs-410_2_8_141,cs-410,2,8,"00:10:48,040","00:10:49,770",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,given W3 are comparable.
cs-410_2_8_142,cs-410,2,8,"00:10:51,100","00:10:55,490",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,They all measure how hard
cs-410_2_8_143,cs-410,2,8,"00:10:55,490","00:11:00,070",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,"But, if we think about the two pairs,"
cs-410_2_8_144,cs-410,2,8,"00:11:00,070","00:11:06,370",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,"where we share W2 in the same condition,"
cs-410_2_8_145,cs-410,2,8,"00:11:06,370","00:11:11,296",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,"Then, the conditional entropies"
cs-410_2_8_146,cs-410,2,8,"00:11:11,296","00:11:15,925",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,You can think of about this question.
cs-410_2_8_147,cs-410,2,8,"00:11:15,925","00:11:17,022",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,Why?
cs-410_2_8_148,cs-410,2,8,"00:11:17,022","00:11:19,870",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,So why are they not comfortable?
cs-410_2_8_149,cs-410,2,8,"00:11:19,870","00:11:23,210",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,"Well, that was because they"
cs-410_2_8_150,cs-410,2,8,"00:11:23,210","00:11:25,690",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,Right?
cs-410_2_8_151,cs-410,2,8,"00:11:25,690","00:11:29,230",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,the entropy of W1 and the entropy of W3.
cs-410_2_8_152,cs-410,2,8,"00:11:29,230","00:11:31,150",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,And they have different upper bounds.
cs-410_2_8_153,cs-410,2,8,"00:11:31,150","00:11:35,000",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,So we cannot really
cs-410_2_8_154,cs-410,2,8,"00:11:35,000","00:11:36,420",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=695,So how do we address this problem?
cs-410_2_8_155,cs-410,2,8,"00:11:38,000","00:11:45,219",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,"Well later, we'll discuss, we can use"
cs-410_2_8_156,cs-410,2,8,"00:11:45,219","00:11:55,219",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,[MUSIC]
cs-410_2_9_1,cs-410,2,9,"00:00:07,553","00:00:12,636",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,"So, I just showed you that empirically"
cs-410_2_9_2,cs-410,2,9,"00:00:12,636","00:00:17,041",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,but theoretically it can also
cs-410_2_9_3,cs-410,2,9,"00:00:17,041","00:00:19,295",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,converge to a local maximum.
cs-410_2_9_4,cs-410,2,9,"00:00:19,295","00:00:24,925",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,So here's just an illustration of what
cs-410_2_9_5,cs-410,2,9,"00:00:24,925","00:00:29,613",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"This required more knowledge about that,"
cs-410_2_9_6,cs-410,2,9,"00:00:29,613","00:00:36,910",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,"some of that inequalities,"
cs-410_2_9_7,cs-410,2,9,"00:00:39,380","00:00:45,040",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,So here what you see is on the X
cs-410_2_9_8,cs-410,2,9,"00:00:45,040","00:00:46,799",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,This is a parameter that we have.
cs-410_2_9_9,cs-410,2,9,"00:00:46,799","00:00:49,714",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,On the y axis we see
cs-410_2_9_10,cs-410,2,9,"00:00:49,714","00:00:57,171",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,So this curve is the original
cs-410_2_9_11,cs-410,2,9,"00:00:57,171","00:01:04,110",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,and this is the one that
cs-410_2_9_12,cs-410,2,9,"00:01:04,110","00:01:06,630",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,And we hope to find a c0 value
cs-410_2_9_13,cs-410,2,9,"00:01:06,630","00:01:11,480",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,But in the case of Mitsumoto we can
cs-410_2_9_14,cs-410,2,9,"00:01:11,480","00:01:12,470",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,to the problem.
cs-410_2_9_15,cs-410,2,9,"00:01:12,470","00:01:14,698",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"So, we have to resolve"
cs-410_2_9_16,cs-410,2,9,"00:01:14,698","00:01:16,457",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,the EM algorithm is such an algorithm.
cs-410_2_9_17,cs-410,2,9,"00:01:16,457","00:01:17,850",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,It's a Hill-Climb algorithm.
cs-410_2_9_18,cs-410,2,9,"00:01:17,850","00:01:22,490",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,That would mean you start
cs-410_2_9_19,cs-410,2,9,"00:01:22,490","00:01:26,260",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,"Let's say you start from here,"
cs-410_2_9_20,cs-410,2,9,"00:01:26,260","00:01:32,090",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,And then you try to improve
cs-410_2_9_21,cs-410,2,9,"00:01:32,090","00:01:35,420",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,another point where you can
cs-410_2_9_22,cs-410,2,9,"00:01:35,420","00:01:37,630",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,So that's the ideal hill climbing.
cs-410_2_9_23,cs-410,2,9,"00:01:37,630","00:01:43,030",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"And in the EM algorithm, the way we"
cs-410_2_9_24,cs-410,2,9,"00:01:43,030","00:01:46,940",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"First, we'll fix a lower"
cs-410_2_9_25,cs-410,2,9,"00:01:46,940","00:01:48,628",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,So this is the lower bound.
cs-410_2_9_26,cs-410,2,9,"00:01:48,628","00:01:49,128",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,See here.
cs-410_2_9_27,cs-410,2,9,"00:01:51,010","00:01:57,560",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,"And once we fit the lower bound,"
cs-410_2_9_28,cs-410,2,9,"00:01:57,560","00:01:59,420",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,"And of course, the reason why this works,"
cs-410_2_9_29,cs-410,2,9,"00:01:59,420","00:02:02,850",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,is because the lower bound
cs-410_2_9_30,cs-410,2,9,"00:02:02,850","00:02:05,780",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,So we know our current guess is here.
cs-410_2_9_31,cs-410,2,9,"00:02:05,780","00:02:11,530",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"And by maximizing the lower bound,"
cs-410_2_9_32,cs-410,2,9,"00:02:11,530","00:02:12,030",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,To here.
cs-410_2_9_33,cs-410,2,9,"00:02:13,300","00:02:14,650",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,Right?
cs-410_2_9_34,cs-410,2,9,"00:02:14,650","00:02:20,150",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,And we can then map to the original
cs-410_2_9_35,cs-410,2,9,"00:02:20,150","00:02:25,600",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,"Because it's a lower bound, we are"
cs-410_2_9_36,cs-410,2,9,"00:02:25,600","00:02:30,570",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,Because we improve our lower bound and
cs-410_2_9_37,cs-410,2,9,"00:02:30,570","00:02:35,040",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,curve which is above this lower bound
cs-410_2_9_38,cs-410,2,9,"00:02:36,310","00:02:39,090",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,So we already know it's
cs-410_2_9_39,cs-410,2,9,"00:02:39,090","00:02:42,440",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,So we definitely improve this
cs-410_2_9_40,cs-410,2,9,"00:02:42,440","00:02:47,253",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,which is above this lower bound.
cs-410_2_9_41,cs-410,2,9,"00:02:47,253","00:02:49,770",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,"So, in our example,"
cs-410_2_9_42,cs-410,2,9,"00:02:49,770","00:02:53,520",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,the current guess is parameter value
cs-410_2_9_43,cs-410,2,9,"00:02:53,520","00:02:57,660",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,And then the next guess is
cs-410_2_9_44,cs-410,2,9,"00:02:57,660","00:03:01,110",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,From this illustration you
cs-410_2_9_45,cs-410,2,9,"00:03:01,110","00:03:03,620",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,is always better than the current guess.
cs-410_2_9_46,cs-410,2,9,"00:03:03,620","00:03:06,930",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,"Unless it has reached the maximum,"
cs-410_2_9_47,cs-410,2,9,"00:03:06,930","00:03:08,008",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,So the two would be equal.
cs-410_2_9_48,cs-410,2,9,"00:03:08,008","00:03:12,821",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,"So, the E-step is basically"
cs-410_2_9_49,cs-410,2,9,"00:03:12,821","00:03:17,650",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,to compute this lower bound.
cs-410_2_9_50,cs-410,2,9,"00:03:17,650","00:03:22,061",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,We don't directly just compute
cs-410_2_9_51,cs-410,2,9,"00:03:22,061","00:03:25,452",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,we compute the length of
cs-410_2_9_52,cs-410,2,9,"00:03:25,452","00:03:28,990",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,these are basically a part
cs-410_2_9_53,cs-410,2,9,"00:03:28,990","00:03:31,150",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,This helps determine the lower bound.
cs-410_2_9_54,cs-410,2,9,"00:03:31,150","00:03:34,460",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,The M-step on the other hand is
cs-410_2_9_55,cs-410,2,9,"00:03:34,460","00:03:37,480",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,It allows us to move
cs-410_2_9_56,cs-410,2,9,"00:03:37,480","00:03:41,460",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,And that's why EM algorithm is guaranteed
cs-410_2_9_57,cs-410,2,9,"00:03:42,490","00:03:46,720",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,"Now, as you can imagine,"
cs-410_2_9_58,cs-410,2,9,"00:03:46,720","00:03:50,100",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,we also have to repeat the EM
cs-410_2_9_59,cs-410,2,9,"00:03:50,100","00:03:54,340",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,In order to figure out which one
cs-410_2_9_60,cs-410,2,9,"00:03:54,340","00:03:59,070",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,And this actually in general is a
cs-410_2_9_61,cs-410,2,9,"00:03:59,070","00:04:02,689",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,So here for
cs-410_2_9_62,cs-410,2,9,"00:04:02,689","00:04:06,223",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,then we gradually just
cs-410_2_9_63,cs-410,2,9,"00:04:06,223","00:04:11,227",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"So, that's not optimal, and"
cs-410_2_9_64,cs-410,2,9,"00:04:11,227","00:04:16,575",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,so the only way to climb up to this gear
cs-410_2_9_65,cs-410,2,9,"00:04:16,575","00:04:22,767",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"So, in the EM algorithm, we generally"
cs-410_2_9_66,cs-410,2,9,"00:04:22,767","00:04:27,880",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,or have some other way to determine
cs-410_2_9_67,cs-410,2,9,"00:04:29,840","00:04:34,320",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,To summarize in this lecture we
cs-410_2_9_68,cs-410,2,9,"00:04:34,320","00:04:38,683",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,This is a general algorithm for computing
cs-410_2_9_69,cs-410,2,9,"00:04:38,683","00:04:42,153",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,"kinds of models, so"
cs-410_2_9_70,cs-410,2,9,"00:04:42,153","00:04:46,468",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,"And it's a hill-climbing algorithm, so it"
cs-410_2_9_71,cs-410,2,9,"00:04:46,468","00:04:48,250",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,it will depend on initial points.
cs-410_2_9_72,cs-410,2,9,"00:04:49,770","00:04:55,414",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,The general idea is that we will have
cs-410_2_9_73,cs-410,2,9,"00:04:55,414","00:05:00,270",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,In the E-step we roughly [INAUDIBLE]
cs-410_2_9_74,cs-410,2,9,"00:05:00,270","00:05:05,560",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,of useful hidden variables that we
cs-410_2_9_75,cs-410,2,9,"00:05:05,560","00:05:10,056",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,"In our case, this is the distribution"
cs-410_2_9_76,cs-410,2,9,"00:05:10,056","00:05:15,750",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,In the M-step then we would exploit
cs-410_2_9_77,cs-410,2,9,"00:05:15,750","00:05:20,790",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"it easier to estimate the distribution,"
cs-410_2_9_78,cs-410,2,9,"00:05:20,790","00:05:24,860",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,Here improve is guaranteed in
cs-410_2_9_79,cs-410,2,9,"00:05:24,860","00:05:30,240",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,Note that it's not necessary that we
cs-410_2_9_80,cs-410,2,9,"00:05:30,240","00:05:35,260",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,parameter value even though the likelihood
cs-410_2_9_81,cs-410,2,9,"00:05:35,260","00:05:40,370",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,There are some properties that have to
cs-410_2_9_82,cs-410,2,9,"00:05:40,370","00:05:44,640",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,also to convert into some stable value.
cs-410_2_9_83,cs-410,2,9,"00:05:47,500","00:05:50,790",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,Now here data augmentation
cs-410_2_9_84,cs-410,2,9,"00:05:50,790","00:05:51,360",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,"That means,"
cs-410_2_9_85,cs-410,2,9,"00:05:51,360","00:05:54,830",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,we're not going to just say exactly
cs-410_2_9_86,cs-410,2,9,"00:05:54,830","00:05:59,390",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,But we're going to have a probability
cs-410_2_9_87,cs-410,2,9,"00:05:59,390","00:06:01,140",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,these hidden variables.
cs-410_2_9_88,cs-410,2,9,"00:06:01,140","00:06:05,990",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,So this causes a split of counts
cs-410_2_9_89,cs-410,2,9,"00:06:07,430","00:06:12,783",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,And in our case we'll split the word
cs-410_2_9_90,cs-410,2,9,"00:06:12,783","00:06:22,783",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,[MUSIC]
cs-410_3_1_1,cs-410,3,1,"00:00:00,168","00:00:07,728",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_3_1_2,cs-410,3,1,"00:00:07,728","00:00:10,250",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about
cs-410_3_1_3,cs-410,3,1,"00:00:12,820","00:00:15,710",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,This picture shows our overall plan for
cs-410_3_1_4,cs-410,3,1,"00:00:16,780","00:00:21,780",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"In the last lecture, we talked about"
cs-410_3_1_5,cs-410,3,1,"00:00:21,780","00:00:24,150",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,We talked about push versus pull.
cs-410_3_1_6,cs-410,3,1,"00:00:25,350","00:00:30,720",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,Such engines are the main tools for
cs-410_3_1_7,cs-410,3,1,"00:00:30,720","00:00:32,690",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"Starting from this lecture,"
cs-410_3_1_8,cs-410,3,1,"00:00:32,690","00:00:36,270",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,we're going to talk about the how
cs-410_3_1_9,cs-410,3,1,"00:00:38,110","00:00:40,770",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,So first it's about
cs-410_3_1_10,cs-410,3,1,"00:00:42,660","00:00:46,120",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,We're going to talk about
cs-410_3_1_11,cs-410,3,1,"00:00:46,120","00:00:49,650",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,"First, we define Text Retrieval."
cs-410_3_1_12,cs-410,3,1,"00:00:49,650","00:00:54,200",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,Second we're going to make a comparison
cs-410_3_1_13,cs-410,3,1,"00:00:54,200","00:00:56,280",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,the related task Database Retrieval.
cs-410_3_1_14,cs-410,3,1,"00:00:58,240","00:01:02,190",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,"Finally, we're going to talk about"
cs-410_3_1_15,cs-410,3,1,"00:01:02,190","00:01:06,508",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,Document Ranking as two strategies for
cs-410_3_1_16,cs-410,3,1,"00:01:09,728","00:01:11,120",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,So what is Text Retrieval?
cs-410_3_1_17,cs-410,3,1,"00:01:12,850","00:01:14,840",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,It should be a task that's familiar for
cs-410_3_1_18,cs-410,3,1,"00:01:14,840","00:01:18,740",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,the most of us because we're using
cs-410_3_1_19,cs-410,3,1,"00:01:19,920","00:01:24,190",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,So text retrieval is basically a task
cs-410_3_1_20,cs-410,3,1,"00:01:24,190","00:01:29,900",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,where the system would respond to
cs-410_3_1_21,cs-410,3,1,"00:01:29,900","00:01:31,540",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,"Basically, it's for supporting a query"
cs-410_3_1_22,cs-410,3,1,"00:01:32,730","00:01:37,300",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,as one way to implement the poll
cs-410_3_1_23,cs-410,3,1,"00:01:39,250","00:01:40,940",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,So the situation is the following.
cs-410_3_1_24,cs-410,3,1,"00:01:40,940","00:01:43,590",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,You have a collection of
cs-410_3_1_25,cs-410,3,1,"00:01:43,590","00:01:47,364",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,These documents could be all
cs-410_3_1_26,cs-410,3,1,"00:01:47,364","00:01:50,988",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,all the literature articles
cs-410_3_1_27,cs-410,3,1,"00:01:50,988","00:01:56,528",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,Or maybe all the text
cs-410_3_1_28,cs-410,3,1,"00:01:58,528","00:02:04,340",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,A user will typically give a query to
cs-410_3_1_29,cs-410,3,1,"00:02:04,340","00:02:09,480",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,"And then, the system would return"
cs-410_3_1_30,cs-410,3,1,"00:02:09,480","00:02:14,040",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,Relevant documents refer to those
cs-410_3_1_31,cs-410,3,1,"00:02:14,040","00:02:15,500",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,the user who typed in the query.
cs-410_3_1_32,cs-410,3,1,"00:02:16,910","00:02:19,510",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,All this task is a phone call
cs-410_3_1_33,cs-410,3,1,"00:02:21,170","00:02:25,585",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,But literally information retrieval would
cs-410_3_1_34,cs-410,3,1,"00:02:25,585","00:02:30,660",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,"non-textual information as well,"
cs-410_3_1_35,cs-410,3,1,"00:02:30,660","00:02:35,960",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,It's worth noting that
cs-410_3_1_36,cs-410,3,1,"00:02:35,960","00:02:41,610",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,of information retrieval in
cs-410_3_1_37,cs-410,3,1,"00:02:41,610","00:02:47,010",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,video can be retrieved by
cs-410_3_1_38,cs-410,3,1,"00:02:47,010","00:02:52,270",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,"So for example,"
cs-410_3_1_39,cs-410,3,1,"00:02:52,270","00:02:57,390",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,match a user's query was
cs-410_3_1_40,cs-410,3,1,"00:02:59,850","00:03:03,870",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,This problem is also
cs-410_3_1_41,cs-410,3,1,"00:03:05,550","00:03:08,680",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,And the technology is often called
cs-410_3_1_42,cs-410,3,1,"00:03:11,190","00:03:14,540",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,If you ever take a course in databases it
cs-410_3_1_43,cs-410,3,1,"00:03:14,540","00:03:18,400",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,will be useful to pause
cs-410_3_1_44,cs-410,3,1,"00:03:18,400","00:03:25,200",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,think about the differences between
cs-410_3_1_45,cs-410,3,1,"00:03:25,200","00:03:28,450",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,Now these two tasks
cs-410_3_1_46,cs-410,3,1,"00:03:29,530","00:03:31,928",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"But, there are some important differences."
cs-410_3_1_47,cs-410,3,1,"00:03:33,708","00:03:38,140",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,"So, spend a moment to think about"
cs-410_3_1_48,cs-410,3,1,"00:03:38,140","00:03:43,300",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"Think about the data, and the information"
cs-410_3_1_49,cs-410,3,1,"00:03:43,300","00:03:46,080",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,those that are managed
cs-410_3_1_50,cs-410,3,1,"00:03:47,350","00:03:51,570",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,Think about the different between
cs-410_3_1_51,cs-410,3,1,"00:03:51,570","00:03:57,389",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,database system versus queries that
cs-410_3_1_52,cs-410,3,1,"00:03:59,180","00:04:00,970",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,And then finally think about the answers.
cs-410_3_1_53,cs-410,3,1,"00:04:02,870","00:04:06,980",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,What's the difference between the two?
cs-410_3_1_54,cs-410,3,1,"00:04:06,980","00:04:11,760",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"Okay, so if we think about the information"
cs-410_3_1_55,cs-410,3,1,"00:04:11,760","00:04:14,890",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,we will see that in text retrieval.
cs-410_3_1_56,cs-410,3,1,"00:04:14,890","00:04:18,100",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,"The data is unstructured, it's free text."
cs-410_3_1_57,cs-410,3,1,"00:04:18,100","00:04:24,020",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,"But in databases, they are structured data"
cs-410_3_1_58,cs-410,3,1,"00:04:24,020","00:04:30,430",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,to tell you this column is the names
cs-410_3_1_59,cs-410,3,1,"00:04:31,880","00:04:35,020",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,The unstructured text is not obvious
cs-410_3_1_60,cs-410,3,1,"00:04:35,020","00:04:38,420",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,what are the names of people
cs-410_3_1_61,cs-410,3,1,"00:04:40,440","00:04:45,930",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"Because of this difference, we also see"
cs-410_3_1_62,cs-410,3,1,"00:04:45,930","00:04:52,900",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,ambiguous and we talk about that in the
cs-410_3_1_63,cs-410,3,1,"00:04:52,900","00:04:55,500",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,But they don't tend to have
cs-410_3_1_64,cs-410,3,1,"00:04:58,230","00:05:01,990",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,The results important
cs-410_3_1_65,cs-410,3,1,"00:05:01,990","00:05:05,770",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,this is partly due to the difference
cs-410_3_1_66,cs-410,3,1,"00:05:07,610","00:05:10,960",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,So test queries tend to be ambiguous.
cs-410_3_1_67,cs-410,3,1,"00:05:10,960","00:05:16,290",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"Whereas in their research,"
cs-410_3_1_68,cs-410,3,1,"00:05:16,290","00:05:22,330",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,Think about a SQL query that would clearly
cs-410_3_1_69,cs-410,3,1,"00:05:22,330","00:05:24,690",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,So it has very well-defined semantics.
cs-410_3_1_70,cs-410,3,1,"00:05:27,230","00:05:32,252",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,Keyword queries or electronic queries tend
cs-410_3_1_71,cs-410,3,1,"00:05:32,252","00:05:37,952",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,"to be incomplete,"
cs-410_3_1_72,cs-410,3,1,"00:05:37,952","00:05:43,390",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,specify what documents
cs-410_3_1_73,cs-410,3,1,"00:05:43,390","00:05:46,370",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,Whereas complete specification for
cs-410_3_1_74,cs-410,3,1,"00:05:47,390","00:05:50,900",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,"And because of these differences,"
cs-410_3_1_75,cs-410,3,1,"00:05:50,900","00:05:56,670",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,"Being the case of text retrieval, we're"
cs-410_3_1_76,cs-410,3,1,"00:05:58,110","00:06:02,740",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,"In the database search,"
cs-410_3_1_77,cs-410,3,1,"00:06:02,740","00:06:07,260",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,match records with the sequel
cs-410_3_1_78,cs-410,3,1,"00:06:09,110","00:06:14,550",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,"Now in the case of text retrieval,"
cs-410_3_1_79,cs-410,3,1,"00:06:14,550","00:06:19,950",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"to the query is not very well specified,"
cs-410_3_1_80,cs-410,3,1,"00:06:21,140","00:06:25,830",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,So it's unclear what should be
cs-410_3_1_81,cs-410,3,1,"00:06:25,830","00:06:30,510",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,"And this has very important consequences,"
cs-410_3_1_82,cs-410,3,1,"00:06:30,510","00:06:35,108",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,textual retrieval is
cs-410_3_1_83,cs-410,3,1,"00:06:38,578","00:06:44,100",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,So this is a problem because
cs-410_3_1_84,cs-410,3,1,"00:06:44,100","00:06:51,510",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,then we can not mathematically prove one
cs-410_3_1_85,cs-410,3,1,"00:06:52,620","00:06:56,650",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,That also means we must rely
cs-410_3_1_86,cs-410,3,1,"00:06:56,650","00:07:01,120",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,involving users to know
cs-410_3_1_87,cs-410,3,1,"00:07:02,460","00:07:05,080",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,And that's why we have.
cs-410_3_1_88,cs-410,3,1,"00:07:05,080","00:07:09,420",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,You need more than one lectures
cs-410_3_1_89,cs-410,3,1,"00:07:09,420","00:07:12,820",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,Because this is very important topic for
cs-410_3_1_90,cs-410,3,1,"00:07:13,890","00:07:18,902",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,Without knowing how to evaluate heroism
cs-410_3_1_91,cs-410,3,1,"00:07:18,902","00:07:24,563",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,whether we have got the better or
cs-410_3_1_92,cs-410,3,1,"00:07:28,393","00:07:31,155",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,So now let's look at
cs-410_3_1_93,cs-410,3,1,"00:07:32,240","00:07:36,170",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,"So, this slide shows a formal formulation"
cs-410_3_1_94,cs-410,3,1,"00:07:37,460","00:07:43,460",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,"First, we have our vocabulary set, which"
cs-410_3_1_95,cs-410,3,1,"00:07:44,920","00:07:49,140",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,"Now here,"
cs-410_3_1_96,cs-410,3,1,"00:07:49,140","00:07:53,360",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"in reality, on the web,"
cs-410_3_1_97,cs-410,3,1,"00:07:53,360","00:07:56,180",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,We have texts that are in
cs-410_3_1_98,cs-410,3,1,"00:07:57,530","00:08:01,478",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,"But here for simplicity, we just"
cs-410_3_1_99,cs-410,3,1,"00:08:01,478","00:08:07,088",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,As the techniques used for retrieving
cs-410_3_1_100,cs-410,3,1,"00:08:07,088","00:08:12,783",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,less similar to the techniques used for
cs-410_3_1_101,cs-410,3,1,"00:08:12,783","00:08:18,819",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,"although there is important difference,"
cs-410_3_1_102,cs-410,3,1,"00:08:21,759","00:08:24,725",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,"Next, we have the query,"
cs-410_3_1_103,cs-410,3,1,"00:08:26,015","00:08:28,625",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,"And so here, you can see"
cs-410_3_1_104,cs-410,3,1,"00:08:31,175","00:08:36,482",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,the query is defined as
cs-410_3_1_105,cs-410,3,1,"00:08:36,482","00:08:41,252",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,Each q sub i is a word in the vocabulary.
cs-410_3_1_106,cs-410,3,1,"00:08:42,302","00:08:47,000",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,"A document is defined in the same way,"
cs-410_3_1_107,cs-410,3,1,"00:08:47,000","00:08:51,520",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"And here,"
cs-410_3_1_108,cs-410,3,1,"00:08:52,920","00:08:55,900",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,"Now typically, the documents"
cs-410_3_1_109,cs-410,3,1,"00:08:57,100","00:09:01,460",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,But there are also cases where
cs-410_3_1_110,cs-410,3,1,"00:09:04,370","00:09:08,530",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,So you can think about what
cs-410_3_1_111,cs-410,3,1,"00:09:09,670","00:09:13,570",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,I hope you can think of Twitter search.
cs-410_3_1_112,cs-410,3,1,"00:09:13,570","00:09:14,992",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,Tweets are very short.
cs-410_3_1_113,cs-410,3,1,"00:09:16,557","00:09:20,560",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,"But in general,"
cs-410_3_1_114,cs-410,3,1,"00:09:22,934","00:09:27,389",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,"Now, then we have"
cs-410_3_1_115,cs-410,3,1,"00:09:27,389","00:09:31,240",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,and this collection can be very large.
cs-410_3_1_116,cs-410,3,1,"00:09:31,240","00:09:32,370",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,So think about the web.
cs-410_3_1_117,cs-410,3,1,"00:09:32,370","00:09:33,820",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,It could be very large.
cs-410_3_1_118,cs-410,3,1,"00:09:36,140","00:09:40,300",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,And then the goal of text retrieval
cs-410_3_1_119,cs-410,3,1,"00:09:40,300","00:09:46,358",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,"the documents, which we denote by R'(q),"
cs-410_3_1_120,cs-410,3,1,"00:09:46,358","00:09:50,290",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,"And this in general, a subset of all"
cs-410_3_1_121,cs-410,3,1,"00:09:52,410","00:09:57,862",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=592,"Unfortunately, this set of relevant"
cs-410_3_1_122,cs-410,3,1,"00:09:57,862","00:10:03,000",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,"and user-dependent in the sense that,"
cs-410_3_1_123,cs-410,3,1,"00:10:03,000","00:10:08,110",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,"in by different users, they expect"
cs-410_3_1_124,cs-410,3,1,"00:10:09,330","00:10:13,600",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,The query given to us by
cs-410_3_1_125,cs-410,3,1,"00:10:13,600","00:10:15,660",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,on which document should be in this set.
cs-410_3_1_126,cs-410,3,1,"00:10:17,840","00:10:24,940",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,"And indeed, the user is generally"
cs-410_3_1_127,cs-410,3,1,"00:10:24,940","00:10:28,940",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,"be in this set, especially in the case"
cs-410_3_1_128,cs-410,3,1,"00:10:28,940","00:10:32,540",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,"large, the user doesn't have complete"
cs-410_3_1_129,cs-410,3,1,"00:10:34,000","00:10:39,550",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,So the best search system
cs-410_3_1_130,cs-410,3,1,"00:10:39,550","00:10:45,856",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,an approximation of this
cs-410_3_1_131,cs-410,3,1,"00:10:45,856","00:10:50,168",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,So we denote it by R'(q).
cs-410_3_1_132,cs-410,3,1,"00:10:50,168","00:10:54,835",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,"So formerly,"
cs-410_3_1_133,cs-410,3,1,"00:10:54,835","00:10:59,849",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,R'(q) approximation of
cs-410_3_1_134,cs-410,3,1,"00:10:59,849","00:11:01,902",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,So how can we do that?
cs-410_3_1_135,cs-410,3,1,"00:11:01,902","00:11:07,290",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,Now imagine if you are now asked
cs-410_3_1_136,cs-410,3,1,"00:11:08,980","00:11:10,480",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,What would you do?
cs-410_3_1_137,cs-410,3,1,"00:11:10,480","00:11:12,526",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,Now think for a moment.
cs-410_3_1_138,cs-410,3,1,"00:11:12,526","00:11:14,433",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,"Right, so these are your input."
cs-410_3_1_139,cs-410,3,1,"00:11:14,433","00:11:18,425",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,"The query, the documents."
cs-410_3_1_140,cs-410,3,1,"00:11:20,399","00:11:24,021",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,And then you are to compute
cs-410_3_1_141,cs-410,3,1,"00:11:24,021","00:11:28,060",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,which is a set of documents that
cs-410_3_1_142,cs-410,3,1,"00:11:29,770","00:11:31,926",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,"So, how would you solve the problem?"
cs-410_3_1_143,cs-410,3,1,"00:11:31,926","00:11:37,630",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,"Now in general,"
cs-410_3_1_144,cs-410,3,1,"00:11:39,720","00:11:42,970",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=699,The first strategy is we do a document
cs-410_3_1_145,cs-410,3,1,"00:11:42,970","00:11:47,740",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,going to have a binary classification
cs-410_3_1_146,cs-410,3,1,"00:11:49,350","00:11:52,110",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,That's a function that
cs-410_3_1_147,cs-410,3,1,"00:11:52,110","00:11:55,740",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,"query as input, and then give a zero or"
cs-410_3_1_148,cs-410,3,1,"00:11:55,740","00:12:01,170",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,one as output to indicate whether this
cs-410_3_1_149,cs-410,3,1,"00:12:02,330","00:12:05,310",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,"So in this case, you can see the document."
cs-410_3_1_150,cs-410,3,1,"00:12:08,700","00:12:15,130",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,"The relevant document is set,"
cs-410_3_1_151,cs-410,3,1,"00:12:15,130","00:12:22,340",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=735,"It basically, all the documents that"
cs-410_3_1_152,cs-410,3,1,"00:12:25,410","00:12:26,040",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,"So in this case,"
cs-410_3_1_153,cs-410,3,1,"00:12:26,040","00:12:29,930",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,you can see the system must have decide
cs-410_3_1_154,cs-410,3,1,"00:12:29,930","00:12:33,680",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,"Basically, it has to say"
cs-410_3_1_155,cs-410,3,1,"00:12:33,680","00:12:36,330",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,And this is called absolute relevance.
cs-410_3_1_156,cs-410,3,1,"00:12:36,330","00:12:38,940",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,"Basically, it needs to know"
cs-410_3_1_157,cs-410,3,1,"00:12:38,940","00:12:39,850",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,useful to the user.
cs-410_3_1_158,cs-410,3,1,"00:12:41,940","00:12:44,910",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,"Alternatively, there's another"
cs-410_3_1_159,cs-410,3,1,"00:12:46,160","00:12:47,150",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,"Now in this case,"
cs-410_3_1_160,cs-410,3,1,"00:12:47,150","00:12:52,290",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=767,the system is not going to make a call
cs-410_3_1_161,cs-410,3,1,"00:12:52,290","00:12:57,230",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=772,But rather the system is going to
cs-410_3_1_162,cs-410,3,1,"00:12:58,440","00:13:01,540",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,That would simply give us a value
cs-410_3_1_163,cs-410,3,1,"00:13:01,540","00:13:04,170",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=781,that would indicate which
cs-410_3_1_164,cs-410,3,1,"00:13:05,740","00:13:08,590",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,So it's not going to make a call whether
cs-410_3_1_165,cs-410,3,1,"00:13:08,590","00:13:12,696",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,But rather it would say which
cs-410_3_1_166,cs-410,3,1,"00:13:12,696","00:13:17,669",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,So this function then can be
cs-410_3_1_167,cs-410,3,1,"00:13:17,669","00:13:22,135",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=797,then we're going to let
cs-410_3_1_168,cs-410,3,1,"00:13:22,135","00:13:25,296",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=802,when the user looks at the document.
cs-410_3_1_169,cs-410,3,1,"00:13:25,296","00:13:31,410",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=805,So we have a threshold theta
cs-410_3_1_170,cs-410,3,1,"00:13:31,410","00:13:37,398",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=811,documents should be in
cs-410_3_1_171,cs-410,3,1,"00:13:37,398","00:13:40,802",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,And we're going to assume
cs-410_3_1_172,cs-410,3,1,"00:13:40,802","00:13:45,312",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=820,are ranked above the threshold
cs-410_3_1_173,cs-410,3,1,"00:13:45,312","00:13:49,780",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=825,these are the documents that
cs-410_3_1_174,cs-410,3,1,"00:13:49,780","00:13:54,490",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=829,And theta is a cutoff
cs-410_3_1_175,cs-410,3,1,"00:13:56,980","00:14:00,980",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=836,So here we've got some collaboration
cs-410_3_1_176,cs-410,3,1,"00:14:00,980","00:14:03,330",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,because we don't really make a cutoff.
cs-410_3_1_177,cs-410,3,1,"00:14:03,330","00:14:07,060",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=843,And the user kind of helped
cs-410_3_1_178,cs-410,3,1,"00:14:08,120","00:14:10,950",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=848,"So in this case,"
cs-410_3_1_179,cs-410,3,1,"00:14:10,950","00:14:14,440",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,if one document is more
cs-410_3_1_180,cs-410,3,1,"00:14:14,440","00:14:17,990",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=854,"And that is, it only needs to"
cs-410_3_1_181,cs-410,3,1,"00:14:19,050","00:14:20,770",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,as opposed to absolute relevance.
cs-410_3_1_182,cs-410,3,1,"00:14:22,230","00:14:26,290",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,Now you can probably already sense that
cs-410_3_1_183,cs-410,3,1,"00:14:26,290","00:14:31,560",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=866,relative relevance would be easier to
cs-410_3_1_184,cs-410,3,1,"00:14:31,560","00:14:32,800",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=871,"Because in the first case,"
cs-410_3_1_185,cs-410,3,1,"00:14:32,800","00:14:36,420",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=872,we have to say exactly whether
cs-410_3_1_186,cs-410,3,1,"00:14:37,990","00:14:45,500",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,And it turns out that ranking is indeed
cs-410_3_1_187,cs-410,3,1,"00:14:46,710","00:14:50,240",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,So let's look at these two
cs-410_3_1_188,cs-410,3,1,"00:14:50,240","00:14:53,960",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,So this picture shows how it works.
cs-410_3_1_189,cs-410,3,1,"00:14:53,960","00:14:58,780",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=893,"So on the left side,"
cs-410_3_1_190,cs-410,3,1,"00:14:58,780","00:15:02,710",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=898,we use the pluses to indicate
cs-410_3_1_191,cs-410,3,1,"00:15:02,710","00:15:09,990",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=902,So we can see the true relevant
cs-410_3_1_192,cs-410,3,1,"00:15:09,990","00:15:15,210",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=909,"of true relevant documents, consists"
cs-410_3_1_193,cs-410,3,1,"00:15:17,450","00:15:20,972",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=917,"And with the document selection function,"
cs-410_3_1_194,cs-410,3,1,"00:15:20,972","00:15:25,636",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=920,we're going to basically
cs-410_3_1_195,cs-410,3,1,"00:15:25,636","00:15:30,050",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=925,"relevant documents, and non-relevant ones."
cs-410_3_1_196,cs-410,3,1,"00:15:30,050","00:15:34,700",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=930,"Of course, the classified will not"
cs-410_3_1_197,cs-410,3,1,"00:15:34,700","00:15:39,522",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=934,"So here we can see, in the approximation"
cs-410_3_1_198,cs-410,3,1,"00:15:39,522","00:15:41,660",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,we have got some number in the documents.
cs-410_3_1_199,cs-410,3,1,"00:15:43,090","00:15:44,168",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,"And similarly,"
cs-410_3_1_200,cs-410,3,1,"00:15:44,168","00:15:48,868",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=944,there is a relevant document that's
cs-410_3_1_201,cs-410,3,1,"00:15:48,868","00:15:53,972",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,"In the case of document ranking,"
cs-410_3_1_202,cs-410,3,1,"00:15:53,972","00:15:59,368",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,simply ranks all the documents in
cs-410_3_1_203,cs-410,3,1,"00:15:59,368","00:16:04,580",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=959,"And then, we're going to let the user"
cs-410_3_1_204,cs-410,3,1,"00:16:04,580","00:16:07,510",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=964,If the user wants to
cs-410_3_1_205,cs-410,3,1,"00:16:07,510","00:16:11,630",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,then the user will scroll down some
cs-410_3_1_206,cs-410,3,1,"00:16:11,630","00:16:17,010",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,But if the user only wants to
cs-410_3_1_207,cs-410,3,1,"00:16:17,010","00:16:20,750",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=977,the user might stop at the top position.
cs-410_3_1_208,cs-410,3,1,"00:16:20,750","00:16:24,200",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,"So in this case, the user stops at d4."
cs-410_3_1_209,cs-410,3,1,"00:16:24,200","00:16:30,950",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=984,"So in fact, we have delivered"
cs-410_3_1_210,cs-410,3,1,"00:16:33,940","00:16:40,300",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=993,So as I said ranking is generally
cs-410_3_1_211,cs-410,3,1,"00:16:40,300","00:16:46,410",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,because the classifier in the case of
cs-410_3_1_212,cs-410,3,1,"00:16:46,410","00:16:47,790",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1006,Why?
cs-410_3_1_213,cs-410,3,1,"00:16:47,790","00:16:51,100",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1007,Because the only clue
cs-410_3_1_214,cs-410,3,1,"00:16:51,100","00:16:56,550",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1011,But the query may not be accurate in the
cs-410_3_1_215,cs-410,3,1,"00:16:57,660","00:17:02,860",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1017,"For example, you might expect relevant"
cs-410_3_1_216,cs-410,3,1,"00:17:04,460","00:17:08,050",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1024,topics by using specific vocabulary.
cs-410_3_1_217,cs-410,3,1,"00:17:08,050","00:17:13,550",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1028,"And as a result,"
cs-410_3_1_218,cs-410,3,1,"00:17:13,550","00:17:15,690",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1033,"Because in the collection,"
cs-410_3_1_219,cs-410,3,1,"00:17:15,690","00:17:19,990",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,no others have discussed the topic
cs-410_3_1_220,cs-410,3,1,"00:17:19,990","00:17:24,380",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1039,"So in this case,"
cs-410_3_1_221,cs-410,3,1,"00:17:25,970","00:17:31,980",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1045,no relevant documents to return in
cs-410_3_1_222,cs-410,3,1,"00:17:33,230","00:17:37,892",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1053,"On the other hand,"
cs-410_3_1_223,cs-410,3,1,"00:17:37,892","00:17:40,430",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1057,"for example, if the query"
cs-410_3_1_224,cs-410,3,1,"00:17:40,430","00:17:44,610",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1060,does not have sufficient descriptive
cs-410_3_1_225,cs-410,3,1,"00:17:44,610","00:17:51,100",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1064,You may actually end up having of
cs-410_3_1_226,cs-410,3,1,"00:17:51,100","00:17:55,840",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1071,thought these words my be sufficient
cs-410_3_1_227,cs-410,3,1,"00:17:55,840","00:17:58,590",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1075,"But, it turns out they"
cs-410_3_1_228,cs-410,3,1,"00:17:58,590","00:18:04,280",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1078,"there are many distractions,"
cs-410_3_1_229,cs-410,3,1,"00:18:04,280","00:18:07,450",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1084,"And so, this is a case of over delivery."
cs-410_3_1_230,cs-410,3,1,"00:18:08,570","00:18:13,910",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1088,"Unfortunately, it's very hard to find the"
cs-410_3_1_231,cs-410,3,1,"00:18:15,390","00:18:15,900",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1095,Why?
cs-410_3_1_232,cs-410,3,1,"00:18:15,900","00:18:19,940",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1095,Because whether users looking for
cs-410_3_1_233,cs-410,3,1,"00:18:19,940","00:18:24,520",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1099,not have a good knowledge about
cs-410_3_1_234,cs-410,3,1,"00:18:24,520","00:18:28,800",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1104,"And in that case, the user does not"
cs-410_3_1_235,cs-410,3,1,"00:18:30,240","00:18:33,770",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1110,vocabularies will be used in
cs-410_3_1_236,cs-410,3,1,"00:18:33,770","00:18:36,064",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1113,So it's very hard for
cs-410_3_1_237,cs-410,3,1,"00:18:36,064","00:18:42,061",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1116,a user to pre-specify the right
cs-410_3_1_238,cs-410,3,1,"00:18:44,569","00:18:49,502",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1124,"Even if the classifier is accurate,"
cs-410_3_1_239,cs-410,3,1,"00:18:49,502","00:18:54,880",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1129,"relevant documents, because they"
cs-410_3_1_240,cs-410,3,1,"00:18:56,130","00:18:58,330",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1136,Relevance is often a matter of degree.
cs-410_3_1_241,cs-410,3,1,"00:18:59,560","00:19:05,250",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1139,So we must prioritize these documents for
cs-410_3_1_242,cs-410,3,1,"00:19:06,320","00:19:10,690",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1146,And note that this
cs-410_3_1_243,cs-410,3,1,"00:19:12,300","00:19:15,840",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1152,because a user cannot
cs-410_3_1_244,cs-410,3,1,"00:19:15,840","00:19:20,720",244,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1155,the user generally would have to
cs-410_3_1_245,cs-410,3,1,"00:19:21,750","00:19:29,220",245,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1161,"And therefore, it would make sense to"
cs-410_3_1_246,cs-410,3,1,"00:19:29,220","00:19:32,100",246,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1169,And that's what ranking is doing.
cs-410_3_1_247,cs-410,3,1,"00:19:32,100","00:19:35,240",247,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1172,"So for these reasons,"
cs-410_3_1_248,cs-410,3,1,"00:19:36,330","00:19:39,610",248,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1176,Now this preference also has
cs-410_3_1_249,cs-410,3,1,"00:19:39,610","00:19:42,330",249,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1179,this is given by the probability
cs-410_3_1_250,cs-410,3,1,"00:19:44,210","00:19:47,930",250,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1184,"In the end of this lecture,"
cs-410_3_1_251,cs-410,3,1,"00:19:49,320","00:19:54,260",251,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1189,"This principle says, returning a ranked"
cs-410_3_1_252,cs-410,3,1,"00:19:54,260","00:19:57,590",252,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1194,of probability that a document
cs-410_3_1_253,cs-410,3,1,"00:19:57,590","00:20:01,270",253,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1197,is the optimal strategy under
cs-410_3_1_254,cs-410,3,1,"00:20:02,620","00:20:05,690",254,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1202,"First, the utility of"
cs-410_3_1_255,cs-410,3,1,"00:20:05,690","00:20:09,280",255,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1205,Is independent of the utility
cs-410_3_1_256,cs-410,3,1,"00:20:10,980","00:20:15,300",256,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1210,"Second, a user would be assumed to"
cs-410_3_1_257,cs-410,3,1,"00:20:15,300","00:20:21,775",257,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1215,Now it's easy to understand why these
cs-410_3_1_258,cs-410,3,1,"00:20:21,775","00:20:27,130",258,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1221,Site for the ranking strategy.
cs-410_3_1_259,cs-410,3,1,"00:20:27,130","00:20:30,930",259,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1227,"Because if the documents are independent,"
cs-410_3_1_260,cs-410,3,1,"00:20:30,930","00:20:34,270",260,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1230,then we can evaluate the utility
cs-410_3_1_261,cs-410,3,1,"00:20:36,350","00:20:40,270",261,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1236,And this would allow the computer
cs-410_3_1_262,cs-410,3,1,"00:20:40,270","00:20:43,920",262,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1240,"And then, we are going to rank these"
cs-410_3_1_263,cs-410,3,1,"00:20:45,710","00:20:51,300",263,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1245,The second assumption is to say that the
cs-410_3_1_264,cs-410,3,1,"00:20:51,300","00:20:55,050",264,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1251,If the user is not going to follow
cs-410_3_1_265,cs-410,3,1,"00:20:55,050","00:20:59,440",265,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1255,"the documents sequentially, then obviously"
cs-410_3_1_266,cs-410,3,1,"00:21:00,560","00:21:08,270",266,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1260,"So under these two assumptions, we can"
cs-410_3_1_267,cs-410,3,1,"00:21:08,270","00:21:13,170",267,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1268,"is, in fact, the best that you could do."
cs-410_3_1_268,cs-410,3,1,"00:21:13,170","00:21:14,700",268,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1273,"Now, I've put one question here."
cs-410_3_1_269,cs-410,3,1,"00:21:14,700","00:21:16,330",269,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1274,Do these two assumptions hold?
cs-410_3_1_270,cs-410,3,1,"00:21:18,240","00:21:23,110",270,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1278,"I suggest you to pause the lecture,"
cs-410_3_1_271,cs-410,3,1,"00:21:27,950","00:21:33,065",271,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1287,"Now, can you think of"
cs-410_3_1_272,cs-410,3,1,"00:21:33,065","00:21:39,238",272,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1293,suggest these assumptions
cs-410_3_1_273,cs-410,3,1,"00:21:44,462","00:21:46,953",273,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1304,"Now, if you think for a moment,"
cs-410_3_1_274,cs-410,3,1,"00:21:46,953","00:21:51,490",274,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1306,you may realize none of
cs-410_3_1_275,cs-410,3,1,"00:21:53,230","00:21:57,690",275,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1313,"For example, in the case of"
cs-410_3_1_276,cs-410,3,1,"00:21:57,690","00:22:02,590",276,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1317,have documents that have similar or
cs-410_3_1_277,cs-410,3,1,"00:22:02,590","00:22:06,620",277,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1322,"If we look at each of them alone,"
cs-410_3_1_278,cs-410,3,1,"00:22:07,790","00:22:12,510",278,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1327,But if the user has already seen
cs-410_3_1_279,cs-410,3,1,"00:22:12,510","00:22:17,240",279,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1332,generally not very useful for the user to
cs-410_3_1_280,cs-410,3,1,"00:22:19,030","00:22:22,040",280,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1339,So clearly the utility
cs-410_3_1_281,cs-410,3,1,"00:22:22,040","00:22:25,680",281,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1342,is dependent on other documents
cs-410_3_1_282,cs-410,3,1,"00:22:27,350","00:22:32,510",282,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1347,In some other cases you might see
cs-410_3_1_283,cs-410,3,1,"00:22:32,510","00:22:38,490",283,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1352,"be useful to the user, but when three"
cs-410_3_1_284,cs-410,3,1,"00:22:38,490","00:22:41,070",284,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1358,They provide answers to
cs-410_3_1_285,cs-410,3,1,"00:22:42,140","00:22:46,883",285,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1362,So this is a collective relevance and
cs-410_3_1_286,cs-410,3,1,"00:22:46,883","00:22:51,542",286,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1366,the value of the document might
cs-410_3_1_287,cs-410,3,1,"00:22:53,329","00:22:58,100",287,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1373,Sequential browsing generally would make
cs-410_3_1_288,cs-410,3,1,"00:22:59,220","00:23:04,650",288,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1379,"But even if you have a rank list,"
cs-410_3_1_289,cs-410,3,1,"00:23:04,650","00:23:10,140",289,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1384,users don't always just go strictly
cs-410_3_1_290,cs-410,3,1,"00:23:10,140","00:23:14,910",290,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1390,They sometimes will look at the bottom for
cs-410_3_1_291,cs-410,3,1,"00:23:14,910","00:23:17,810",291,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1394,And if you think about the more
cs-410_3_1_292,cs-410,3,1,"00:23:17,810","00:23:22,100",292,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1397,we could possibly use like
cs-410_3_1_293,cs-410,3,1,"00:23:22,100","00:23:26,820",293,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1402,Where you can put that additional
cs-410_3_1_294,cs-410,3,1,"00:23:26,820","00:23:29,379",294,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1406,sequential browsing is a very
cs-410_3_1_295,cs-410,3,1,"00:23:32,010","00:23:34,300",295,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1412,So the point here is that
cs-410_3_1_296,cs-410,3,1,"00:23:35,740","00:23:39,990",296,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1415,none of these assumptions is
cs-410_3_1_297,cs-410,3,1,"00:23:41,100","00:23:45,290",297,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1421,But probability ranking principle
cs-410_3_1_298,cs-410,3,1,"00:23:46,870","00:23:51,020",298,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1426,ranking as a primary pattern for
cs-410_3_1_299,cs-410,3,1,"00:23:51,020","00:23:53,180",299,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1431,And this has actually been the basis for
cs-410_3_1_300,cs-410,3,1,"00:23:53,180","00:23:57,090",300,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1433,a lot of research work in
cs-410_3_1_301,cs-410,3,1,"00:23:57,090","00:24:00,530",301,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1437,And many hours have been designed
cs-410_3_1_302,cs-410,3,1,"00:24:01,590","00:24:06,410",302,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1441,despite that the assumptions
cs-410_3_1_303,cs-410,3,1,"00:24:06,410","00:24:11,570",303,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1446,And we can address this problem
cs-410_3_1_304,cs-410,3,1,"00:24:11,570","00:24:15,430",304,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1451,"Of a ranked list, for example,"
cs-410_3_1_305,cs-410,3,1,"00:24:20,260","00:24:22,500",305,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1460,"So to summarize this lecture,"
cs-410_3_1_306,cs-410,3,1,"00:24:22,500","00:24:28,000",306,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1462,the main points that you can
cs-410_3_1_307,cs-410,3,1,"00:24:28,000","00:24:31,760",307,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1468,"First, text retrieval is"
cs-410_3_1_308,cs-410,3,1,"00:24:31,760","00:24:37,951",308,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1471,And that means which algorithm is
cs-410_3_1_309,cs-410,3,1,"00:24:37,951","00:24:42,500",309,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1477,"Second, document ranking"
cs-410_3_1_310,cs-410,3,1,"00:24:42,500","00:24:46,180",310,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1482,And this will help users prioritize
cs-410_3_1_311,cs-410,3,1,"00:24:47,410","00:24:52,693",311,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1487,And this is also to bypass the difficulty
cs-410_3_1_312,cs-410,3,1,"00:24:52,693","00:24:58,221",312,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1492,Because we can get some help from users
cs-410_3_1_313,cs-410,3,1,"00:24:58,221","00:24:59,809",313,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1498,it's more flexible.
cs-410_3_1_314,cs-410,3,1,"00:25:01,967","00:25:06,904",314,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1501,"So, this further suggests that the main"
cs-410_3_1_315,cs-410,3,1,"00:25:06,904","00:25:09,950",315,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1506,engine is the design
cs-410_3_1_316,cs-410,3,1,"00:25:10,970","00:25:16,150",316,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1510,"In other words, we need to define"
cs-410_3_1_317,cs-410,3,1,"00:25:16,150","00:25:19,480",317,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1516,on the query and document pair.
cs-410_3_1_318,cs-410,3,1,"00:25:21,360","00:25:26,151",318,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1521,How we design such a function is the main
cs-410_3_1_319,cs-410,3,1,"00:25:29,123","00:25:32,060",319,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1529,There are two suggested
cs-410_3_1_320,cs-410,3,1,"00:25:32,060","00:25:36,180",320,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1532,The first is the classical paper on
cs-410_3_1_321,cs-410,3,1,"00:25:37,470","00:25:42,380",321,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1537,The second one is a must-read for anyone
cs-410_3_1_322,cs-410,3,1,"00:25:42,380","00:25:49,220",322,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1542,"It's a classic IR book, which has"
cs-410_3_1_323,cs-410,3,1,"00:25:49,220","00:25:55,540",323,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1549,and results in early days up to
cs-410_3_1_324,cs-410,3,1,"00:25:55,540","00:25:59,762",324,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1555,Chapter six of this book has
cs-410_3_1_325,cs-410,3,1,"00:25:59,762","00:26:06,211",325,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1559,the Probability Ranking Principle and
cs-410_3_1_326,cs-410,3,1,"00:26:06,211","00:26:16,211",326,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1566,[MUSIC]
cs-410_3_10_1,cs-410,3,10,"00:00:00,192","00:00:03,512",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_10_2,cs-410,3,10,"00:00:06,614","00:00:09,660",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,This lecture is about text categorization.
cs-410_3_10_3,cs-410,3,10,"00:00:11,360","00:00:15,320",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,"In this lecture, we're going to"
cs-410_3_10_4,cs-410,3,10,"00:00:16,390","00:00:21,320",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,This is a very important technique for
cs-410_3_10_5,cs-410,3,10,"00:00:22,470","00:00:27,035",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,It is relevant to discovery
cs-410_3_10_6,cs-410,3,10,"00:00:27,035","00:00:29,134",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,knowledge as shown here.
cs-410_3_10_7,cs-410,3,10,"00:00:29,134","00:00:33,380",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,"First, it's related to topic mining and"
cs-410_3_10_8,cs-410,3,10,"00:00:33,380","00:00:36,060",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"And, that's because it has to do with"
cs-410_3_10_9,cs-410,3,10,"00:00:36,060","00:00:40,970",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,analyzing text to data based
cs-410_3_10_10,cs-410,3,10,"00:00:40,970","00:00:46,239",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,"Secondly, it's also related to"
cs-410_3_10_11,cs-410,3,10,"00:00:46,239","00:00:51,941",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,which has to do with discovery knowledge
cs-410_3_10_12,cs-410,3,10,"00:00:51,941","00:00:56,301",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"Because we can categorize the authors,"
cs-410_3_10_13,cs-410,3,10,"00:00:56,301","00:01:01,813",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,based on the content of the articles
cs-410_3_10_14,cs-410,3,10,"00:01:01,813","00:01:06,611",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"We can, in general,"
cs-410_3_10_15,cs-410,3,10,"00:01:06,611","00:01:10,800",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,based on the content that they produce.
cs-410_3_10_16,cs-410,3,10,"00:01:12,300","00:01:16,720",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"Finally, it's also related"
cs-410_3_10_17,cs-410,3,10,"00:01:16,720","00:01:21,760",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,"Because, we can often use text"
cs-410_3_10_18,cs-410,3,10,"00:01:21,760","00:01:26,180",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,variables in the real world that
cs-410_3_10_19,cs-410,3,10,"00:01:27,230","00:01:32,490",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"And so, this is a very important"
cs-410_3_10_20,cs-410,3,10,"00:01:34,820","00:01:37,860",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,This is the overall plan for
cs-410_3_10_21,cs-410,3,10,"00:01:37,860","00:01:40,750",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"First, we're going to talk about"
cs-410_3_10_22,cs-410,3,10,"00:01:40,750","00:01:44,510",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,why we're interested in
cs-410_3_10_23,cs-410,3,10,"00:01:44,510","00:01:47,920",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,"And now, we're going to talk about"
cs-410_3_10_24,cs-410,3,10,"00:01:47,920","00:01:50,780",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,how to evaluate
cs-410_3_10_25,cs-410,3,10,"00:01:50,780","00:01:56,140",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,"So, the problem of text"
cs-410_3_10_26,cs-410,3,10,"00:01:56,140","00:02:03,461",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,We're given a set of predefined categories
cs-410_3_10_27,cs-410,3,10,"00:02:03,461","00:02:07,462",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,"And often,"
cs-410_3_10_28,cs-410,3,10,"00:02:07,462","00:02:12,519",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,training set of labeled text
cs-410_3_10_29,cs-410,3,10,"00:02:12,519","00:02:17,810",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,objects have already been
cs-410_3_10_30,cs-410,3,10,"00:02:17,810","00:02:23,040",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,"And then, the task is to classify"
cs-410_3_10_31,cs-410,3,10,"00:02:23,040","00:02:26,320",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,more of these predefined categories.
cs-410_3_10_32,cs-410,3,10,"00:02:26,320","00:02:29,139",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,"So, the picture on this"
cs-410_3_10_33,cs-410,3,10,"00:02:30,270","00:02:32,120",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,"When we do text categorization,"
cs-410_3_10_34,cs-410,3,10,"00:02:32,120","00:02:37,630",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,we have a lot of text objects to be
cs-410_3_10_35,cs-410,3,10,"00:02:37,630","00:02:43,820",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,"the system will, in general,"
cs-410_3_10_36,cs-410,3,10,"00:02:43,820","00:02:49,110",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,As shown on the right and
cs-410_3_10_37,cs-410,3,10,"00:02:49,110","00:02:54,280",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,and we often assume the availability
cs-410_3_10_38,cs-410,3,10,"00:02:54,280","00:02:59,060",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,these are the documents that
cs-410_3_10_39,cs-410,3,10,"00:02:59,060","00:03:01,660",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,And these examples are very important for
cs-410_3_10_40,cs-410,3,10,"00:03:01,660","00:03:06,110",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,helping the system to learn
cs-410_3_10_41,cs-410,3,10,"00:03:06,110","00:03:10,180",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"And, this would further help"
cs-410_3_10_42,cs-410,3,10,"00:03:11,280","00:03:16,560",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,the categories of new text
cs-410_3_10_43,cs-410,3,10,"00:03:16,560","00:03:20,950",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"So, here are some specific"
cs-410_3_10_44,cs-410,3,10,"00:03:20,950","00:03:26,140",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,"And in fact, there are many examples,"
cs-410_3_10_45,cs-410,3,10,"00:03:27,230","00:03:33,000",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,"So first, text objects can vary,"
cs-410_3_10_46,cs-410,3,10,"00:03:33,000","00:03:36,730",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,"or a passage, or a sentence,"
cs-410_3_10_47,cs-410,3,10,"00:03:36,730","00:03:41,400",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,"As in the case of clustering, the units"
cs-410_3_10_48,cs-410,3,10,"00:03:41,400","00:03:44,090",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,this creates a lot of possibilities.
cs-410_3_10_49,cs-410,3,10,"00:03:44,090","00:03:46,690",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,"Secondly, categories can also vary."
cs-410_3_10_50,cs-410,3,10,"00:03:46,690","00:03:49,880",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,"Allocate in general,"
cs-410_3_10_51,cs-410,3,10,"00:03:49,880","00:03:51,560",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,One is internal categories.
cs-410_3_10_52,cs-410,3,10,"00:03:51,560","00:03:55,890",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,These are categories that
cs-410_3_10_53,cs-410,3,10,"00:03:55,890","00:04:00,850",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,"For example, topic categories or"
cs-410_3_10_54,cs-410,3,10,"00:04:00,850","00:04:04,810",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,they generally have to do with
cs-410_3_10_55,cs-410,3,10,"00:04:04,810","00:04:06,930",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,throughout the categorization
cs-410_3_10_56,cs-410,3,10,"00:04:08,210","00:04:13,430",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,The other kind is external categories
cs-410_3_10_57,cs-410,3,10,"00:04:13,430","00:04:16,120",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,associated with the text object.
cs-410_3_10_58,cs-410,3,10,"00:04:16,120","00:04:17,630",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"For example,"
cs-410_3_10_59,cs-410,3,10,"00:04:17,630","00:04:22,810",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,authors are entities associated
cs-410_3_10_60,cs-410,3,10,"00:04:22,810","00:04:28,340",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"And so, we can use their content in"
cs-410_3_10_61,cs-410,3,10,"00:04:28,340","00:04:31,670",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,"which part, for example, and"
cs-410_3_10_62,cs-410,3,10,"00:04:33,540","00:04:38,048",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"Or, we can have any"
cs-410_3_10_63,cs-410,3,10,"00:04:38,048","00:04:43,147",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,associate with text data
cs-410_3_10_64,cs-410,3,10,"00:04:43,147","00:04:47,788",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,connection between the entity and
cs-410_3_10_65,cs-410,3,10,"00:04:47,788","00:04:54,025",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"For example, we might collect a lot"
cs-410_3_10_66,cs-410,3,10,"00:04:54,025","00:04:58,073",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,"a lot of reviews about a product,"
cs-410_3_10_67,cs-410,3,10,"00:04:58,073","00:05:04,770",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,this text data can help us infer
cs-410_3_10_68,cs-410,3,10,"00:05:04,770","00:05:07,770",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"In that case, we can treat this"
cs-410_3_10_69,cs-410,3,10,"00:05:07,770","00:05:09,921",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,We can categorize restaurants or
cs-410_3_10_70,cs-410,3,10,"00:05:09,921","00:05:13,924",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,categorize products based on
cs-410_3_10_71,cs-410,3,10,"00:05:13,924","00:05:17,245",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,"So, this is an example for"
cs-410_3_10_72,cs-410,3,10,"00:05:17,245","00:05:20,400",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,Here are some specific
cs-410_3_10_73,cs-410,3,10,"00:05:20,400","00:05:25,110",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,News categorization is very
cs-410_3_10_74,cs-410,3,10,"00:05:25,110","00:05:30,009",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,News agencies would like
cs-410_3_10_75,cs-410,3,10,"00:05:30,009","00:05:35,672",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,categories to categorize
cs-410_3_10_76,cs-410,3,10,"00:05:35,672","00:05:39,824",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,"And, these virtual article"
cs-410_3_10_77,cs-410,3,10,"00:05:39,824","00:05:43,650",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"For example, in the biomedical domain,"
cs-410_3_10_78,cs-410,3,10,"00:05:43,650","00:05:47,930",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"MeSH stands for Medical Subject Heading,"
cs-410_3_10_79,cs-410,3,10,"00:05:49,090","00:05:52,490",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,characterize content of
cs-410_3_10_80,cs-410,3,10,"00:05:54,590","00:05:59,860",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,Another example of application is spam
cs-410_3_10_81,cs-410,3,10,"00:05:59,860","00:06:04,940",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,"So, we often have a spam filter"
cs-410_3_10_82,cs-410,3,10,"00:06:04,940","00:06:10,260",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,to help us distinguish spams
cs-410_3_10_83,cs-410,3,10,"00:06:10,260","00:06:13,000",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,this is clearly a binary
cs-410_3_10_84,cs-410,3,10,"00:06:14,500","00:06:18,460",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,Sentiment categorization of
cs-410_3_10_85,cs-410,3,10,"00:06:18,460","00:06:23,120",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,another kind of applications where we
cs-410_3_10_86,cs-410,3,10,"00:06:23,120","00:06:26,380",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,negative or positive and
cs-410_3_10_87,cs-410,3,10,"00:06:27,460","00:06:32,820",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,"So, you can have send them to categories,"
cs-410_3_10_88,cs-410,3,10,"00:06:35,520","00:06:39,480",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,Another application is automatic
cs-410_3_10_89,cs-410,3,10,"00:06:39,480","00:06:43,750",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,you might want to automatically sort your
cs-410_3_10_90,cs-410,3,10,"00:06:43,750","00:06:47,320",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,one application of text categorization
cs-410_3_10_91,cs-410,3,10,"00:06:48,370","00:06:52,580",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,The results are another important kind
cs-410_3_10_92,cs-410,3,10,"00:06:52,580","00:06:55,910",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"to the right person to handle,"
cs-410_3_10_93,cs-410,3,10,"00:06:55,910","00:07:01,890",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,email messaging is generally routed
cs-410_3_10_94,cs-410,3,10,"00:07:01,890","00:07:05,820",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,Different people tend to handle
cs-410_3_10_95,cs-410,3,10,"00:07:05,820","00:07:11,220",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"And in many cases, a person would manually"
cs-410_3_10_96,cs-410,3,10,"00:07:11,220","00:07:15,231",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,"But, if you can imagine,"
cs-410_3_10_97,cs-410,3,10,"00:07:15,231","00:07:18,794",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,text categorization system
cs-410_3_10_98,cs-410,3,10,"00:07:18,794","00:07:24,969",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,"And, this is a class file, the incoming"
cs-410_3_10_99,cs-410,3,10,"00:07:24,969","00:07:31,265",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,where each category actually corresponds
cs-410_3_10_100,cs-410,3,10,"00:07:31,265","00:07:35,975",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"And finally, author attribution, as I just"
cs-410_3_10_101,cs-410,3,10,"00:07:35,975","00:07:39,759",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,it's another example of using text
cs-410_3_10_102,cs-410,3,10,"00:07:41,480","00:07:42,960",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,some other entities.
cs-410_3_10_103,cs-410,3,10,"00:07:42,960","00:07:46,890",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,"And, there are also many variants"
cs-410_3_10_104,cs-410,3,10,"00:07:46,890","00:07:50,980",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,"And so, first, we have the simplest case,"
cs-410_3_10_105,cs-410,3,10,"00:07:50,980","00:07:52,990",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,where there are only two categories.
cs-410_3_10_106,cs-410,3,10,"00:07:52,990","00:07:57,660",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,"And, there are many examples like that,"
cs-410_3_10_107,cs-410,3,10,"00:07:59,040","00:08:03,600",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,Applications with one distinguishing
cs-410_3_10_108,cs-410,3,10,"00:08:03,600","00:08:04,940",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,documents for a particular query.
cs-410_3_10_109,cs-410,3,10,"00:08:06,040","00:08:12,330",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,Spam filtering just distinguishing spams
cs-410_3_10_110,cs-410,3,10,"00:08:12,330","00:08:16,800",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,"Sometimes, classifications of"
cs-410_3_10_111,cs-410,3,10,"00:08:16,800","00:08:17,800",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,positive and a negative.
cs-410_3_10_112,cs-410,3,10,"00:08:19,120","00:08:22,650",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,A more general case would be K-category
cs-410_3_10_113,cs-410,3,10,"00:08:22,650","00:08:26,755",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,"many applications like that,"
cs-410_3_10_114,cs-410,3,10,"00:08:26,755","00:08:30,155",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,"So, topic categorization is often"
cs-410_3_10_115,cs-410,3,10,"00:08:30,155","00:08:31,935",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,multiple topics.
cs-410_3_10_116,cs-410,3,10,"00:08:31,935","00:08:36,205",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,Email routing would be another example
cs-410_3_10_117,cs-410,3,10,"00:08:36,205","00:08:39,322",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,if you route the email to
cs-410_3_10_118,cs-410,3,10,"00:08:39,322","00:08:44,550",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,then there are multiple
cs-410_3_10_119,cs-410,3,10,"00:08:44,550","00:08:48,212",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,"So, in all these cases, there are more"
cs-410_3_10_120,cs-410,3,10,"00:08:49,272","00:08:52,382",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,Another variation is to have
cs-410_3_10_121,cs-410,3,10,"00:08:52,382","00:08:54,442",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,where categories form a hierarchy.
cs-410_3_10_122,cs-410,3,10,"00:08:54,442","00:08:56,602",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,"Again, topical hierarchy is very common."
cs-410_3_10_123,cs-410,3,10,"00:08:58,232","00:09:00,742",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,Yet another variation is
cs-410_3_10_124,cs-410,3,10,"00:09:00,742","00:09:04,550",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,That's when you have multiple
cs-410_3_10_125,cs-410,3,10,"00:09:04,550","00:09:08,150",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,then you hope to kind of
cs-410_3_10_126,cs-410,3,10,"00:09:08,150","00:09:13,340",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,Further leverage the dependency of
cs-410_3_10_127,cs-410,3,10,"00:09:13,340","00:09:15,250",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,each individual task.
cs-410_3_10_128,cs-410,3,10,"00:09:15,250","00:09:19,870",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,Among all these binary categorizations
cs-410_3_10_129,cs-410,3,10,"00:09:19,870","00:09:25,170",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,part of it also is because it's simple and
cs-410_3_10_130,cs-410,3,10,"00:09:25,170","00:09:31,000",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,it can actually be used to perform
cs-410_3_10_131,cs-410,3,10,"00:09:31,000","00:09:34,839",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"For example, a K-category"
cs-410_3_10_132,cs-410,3,10,"00:09:34,839","00:09:38,665",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,performed by using binary categorization.
cs-410_3_10_133,cs-410,3,10,"00:09:40,075","00:09:43,405",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,"Basically, we can look at"
cs-410_3_10_134,cs-410,3,10,"00:09:43,405","00:09:49,385",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,then the binary categorization problem
cs-410_3_10_135,cs-410,3,10,"00:09:49,385","00:09:52,005",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,"not, meaning in other categories."
cs-410_3_10_136,cs-410,3,10,"00:09:53,485","00:09:59,820",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,"And, the hierarchical categorization"
cs-410_3_10_137,cs-410,3,10,"00:09:59,820","00:10:04,300",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,doing flat categorization at each level.
cs-410_3_10_138,cs-410,3,10,"00:10:04,300","00:10:07,000",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,"So, we have, first, we categorize"
cs-410_3_10_139,cs-410,3,10,"00:10:07,000","00:10:09,140",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,"a small number of high-level categories,"
cs-410_3_10_140,cs-410,3,10,"00:10:09,140","00:10:13,740",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,"and inside each category, we have further"
cs-410_3_10_141,cs-410,3,10,"00:10:15,000","00:10:16,728",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,"So, why is text categorization important?"
cs-410_3_10_142,cs-410,3,10,"00:10:16,728","00:10:21,464",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,"Well, I already showed that you,"
cs-410_3_10_143,cs-410,3,10,"00:10:21,464","00:10:23,244",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,there are several reasons.
cs-410_3_10_144,cs-410,3,10,"00:10:23,244","00:10:28,891",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,One is text categorization helps enrich
cs-410_3_10_145,cs-410,3,10,"00:10:28,891","00:10:34,970",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,more understanding of text data that's
cs-410_3_10_146,cs-410,3,10,"00:10:34,970","00:10:38,738",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"So, now with categorization text can"
cs-410_3_10_147,cs-410,3,10,"00:10:38,738","00:10:47,310",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,The keyword conditions that's often
cs-410_3_10_148,cs-410,3,10,"00:10:47,310","00:10:52,455",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,But we can now also add categories and
cs-410_3_10_149,cs-410,3,10,"00:10:55,485","00:11:00,085",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,Semantic categories assigned can also
cs-410_3_10_150,cs-410,3,10,"00:11:00,085","00:11:01,145",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,application.
cs-410_3_10_151,cs-410,3,10,"00:11:01,145","00:11:07,869",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,"So, for example, semantic categories"
cs-410_3_10_152,cs-410,3,10,"00:11:07,869","00:11:12,248",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,other attribution might
cs-410_3_10_153,cs-410,3,10,"00:11:12,248","00:11:18,118",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,Another example is when semantic
cs-410_3_10_154,cs-410,3,10,"00:11:18,118","00:11:24,660",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,of text content and this is another case
cs-410_3_10_155,cs-410,3,10,"00:11:25,950","00:11:30,940",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,"For example, if we want to know"
cs-410_3_10_156,cs-410,3,10,"00:11:32,010","00:11:37,830",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,could first categorize the opinions
cs-410_3_10_157,cs-410,3,10,"00:11:37,830","00:11:42,730",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,"as positive or negative and then, that"
cs-410_3_10_158,cs-410,3,10,"00:11:42,730","00:11:47,810",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,"the sentiment, and it would tell us about"
cs-410_3_10_159,cs-410,3,10,"00:11:47,810","00:11:52,680",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,the 70% of the views are positive and
cs-410_3_10_160,cs-410,3,10,"00:11:53,810","00:11:56,865",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,"So, without doing categorization,"
cs-410_3_10_161,cs-410,3,10,"00:11:56,865","00:12:02,402",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=716,it will be much harder to aggregate
cs-410_3_10_162,cs-410,3,10,"00:12:02,402","00:12:07,468",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,way of coding text in some sense
cs-410_3_10_163,cs-410,3,10,"00:12:07,468","00:12:13,640",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,"And, sometimes you may see in some"
cs-410_3_10_164,cs-410,3,10,"00:12:13,640","00:12:18,704",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,"called a text coded,"
cs-410_3_10_165,cs-410,3,10,"00:12:18,704","00:12:22,316",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,The second kind of reasons is to use text
cs-410_3_10_166,cs-410,3,10,"00:12:22,316","00:12:27,024",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=742,categorization to infer
cs-410_3_10_167,cs-410,3,10,"00:12:27,024","00:12:31,950",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=747,and text categories allows
cs-410_3_10_168,cs-410,3,10,"00:12:31,950","00:12:36,950",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=751,of such entities that
cs-410_3_10_169,cs-410,3,10,"00:12:36,950","00:12:41,140",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,"So, this means we can"
cs-410_3_10_170,cs-410,3,10,"00:12:41,140","00:12:44,090",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,to discover knowledge about the world.
cs-410_3_10_171,cs-410,3,10,"00:12:44,090","00:12:48,370",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,"In general, as long as we can associate"
cs-410_3_10_172,cs-410,3,10,"00:12:48,370","00:12:53,600",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=768,we can always the text of data to help
cs-410_3_10_173,cs-410,3,10,"00:12:53,600","00:12:54,502",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,"So, it's used for"
cs-410_3_10_174,cs-410,3,10,"00:12:54,502","00:12:59,380",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,single information network that will
cs-410_3_10_175,cs-410,3,10,"00:12:59,380","00:13:03,750",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=779,The obvious entities that can be
cs-410_3_10_176,cs-410,3,10,"00:13:03,750","00:13:08,340",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=783,"But, you can also imagine the author's"
cs-410_3_10_177,cs-410,3,10,"00:13:08,340","00:13:14,090",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,other things can be actually
cs-410_3_10_178,cs-410,3,10,"00:13:14,090","00:13:18,860",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,"Once we have made the connection, then we"
cs-410_3_10_179,cs-410,3,10,"00:13:18,860","00:13:23,520",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,"So, this is a general way to allow"
cs-410_3_10_180,cs-410,3,10,"00:13:23,520","00:13:26,890",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,the text categorization to discover
cs-410_3_10_181,cs-410,3,10,"00:13:26,890","00:13:32,330",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=806,"Very useful, especially in big text"
cs-410_3_10_182,cs-410,3,10,"00:13:32,330","00:13:38,150",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=812,just using text data as extra sets
cs-410_3_10_183,cs-410,3,10,"00:13:38,150","00:13:43,930",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=818,to infer certain decision factors
cs-410_3_10_184,cs-410,3,10,"00:13:43,930","00:13:45,710",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,"Specifically with text, for example,"
cs-410_3_10_185,cs-410,3,10,"00:13:45,710","00:13:49,220",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=825,we can also think of examples of
cs-410_3_10_186,cs-410,3,10,"00:13:49,220","00:13:53,190",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=829,"For example, discovery of"
cs-410_3_10_187,cs-410,3,10,"00:13:53,190","00:13:59,460",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=833,"And, this can be done by categorizing"
cs-410_3_10_188,cs-410,3,10,"00:14:00,680","00:14:05,566",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,Another example is to predict the party
cs-410_3_10_189,cs-410,3,10,"00:14:05,566","00:14:07,314",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=845,on the political speech.
cs-410_3_10_190,cs-410,3,10,"00:14:07,314","00:14:11,967",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=847,"And, this is again an example"
cs-410_3_10_191,cs-410,3,10,"00:14:11,967","00:14:15,146",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,some knowledge about the real world.
cs-410_3_10_192,cs-410,3,10,"00:14:15,146","00:14:19,265",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,"In nature,"
cs-410_3_10_193,cs-410,3,10,"00:14:19,265","00:14:24,980",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,that's as we defined and
cs-410_3_10_194,cs-410,3,10,"00:14:24,980","00:14:34,980",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=864,[MUSIC]
cs-410_3_11_1,cs-410,3,11,"00:00:00,025","00:00:06,573",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND] This lecture is
cs-410_3_11_2,cs-410,3,11,"00:00:06,573","00:00:12,439",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,of evaluation of text categorization.
cs-410_3_11_3,cs-410,3,11,"00:00:12,439","00:00:18,302",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,Earlier we have introduced measures that
cs-410_3_11_4,cs-410,3,11,"00:00:18,302","00:00:19,920",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,recall.
cs-410_3_11_5,cs-410,3,11,"00:00:19,920","00:00:26,090",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,For each category and each document
cs-410_3_11_6,cs-410,3,11,"00:00:27,680","00:00:32,530",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,further examine how to combine the
cs-410_3_11_7,cs-410,3,11,"00:00:32,530","00:00:36,980",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"different documents how to aggregate them,"
cs-410_3_11_8,cs-410,3,11,"00:00:36,980","00:00:41,220",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,You see on the title here I indicated
cs-410_3_11_9,cs-410,3,11,"00:00:41,220","00:00:46,190",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,this is in contrast to micro average
cs-410_3_11_10,cs-410,3,11,"00:00:47,750","00:00:53,710",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,"So, again, for each category we're going"
cs-410_3_11_11,cs-410,3,11,"00:00:53,710","00:00:59,880",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,for example category c1 we have
cs-410_3_11_12,cs-410,3,11,"00:00:59,880","00:01:06,380",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,And similarly we can do that for category
cs-410_3_11_13,cs-410,3,11,"00:01:06,380","00:01:11,050",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,Now once we compute that and
cs-410_3_11_14,cs-410,3,11,"00:01:11,050","00:01:13,840",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,example we can aggregate
cs-410_3_11_15,cs-410,3,11,"00:01:13,840","00:01:17,610",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"For all the categories, for"
cs-410_3_11_16,cs-410,3,11,"00:01:17,610","00:01:24,160",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,And this is often very useful to summarize
cs-410_3_11_17,cs-410,3,11,"00:01:24,160","00:01:26,780",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,And aggregation can be
cs-410_3_11_18,cs-410,3,11,"00:01:26,780","00:01:32,550",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,"Again as I said, in a case when you"
cs-410_3_11_19,cs-410,3,11,"00:01:32,550","00:01:36,630",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,it's always good to think about what's
cs-410_3_11_20,cs-410,3,11,"00:01:36,630","00:01:41,750",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,"For example, we can consider arithmetic"
cs-410_3_11_21,cs-410,3,11,"00:01:41,750","00:01:46,180",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,"you can use geometric mean,"
cs-410_3_11_22,cs-410,3,11,"00:01:46,180","00:01:50,540",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"Depending on the way you aggregate,"
cs-410_3_11_23,cs-410,3,11,"00:01:50,540","00:01:54,370",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,"in terms of which method works better,"
cs-410_3_11_24,cs-410,3,11,"00:01:54,370","00:02:00,860",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,differences and choosing the right one or
cs-410_3_11_25,cs-410,3,11,"00:02:00,860","00:02:03,770",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,So the difference fore example
cs-410_3_11_26,cs-410,3,11,"00:02:03,770","00:02:08,360",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,geometrically is that the arithmetically
cs-410_3_11_27,cs-410,3,11,"00:02:08,360","00:02:12,170",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,values whereas geometrically would
cs-410_3_11_28,cs-410,3,11,"00:02:12,170","00:02:16,940",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,Base and so whether you are want
cs-410_3_11_29,cs-410,3,11,"00:02:16,940","00:02:22,040",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,high values would be a question
cs-410_3_11_30,cs-410,3,11,"00:02:22,040","00:02:24,720",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,similar we can do that for
cs-410_3_11_31,cs-410,3,11,"00:02:24,720","00:02:29,400",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,So that's how we can generate the overall
cs-410_3_11_32,cs-410,3,11,"00:02:31,660","00:02:36,990",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,Now we can do the same for aggregation
cs-410_3_11_33,cs-410,3,11,"00:02:36,990","00:02:40,300",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,So it's exactly the same situation for
cs-410_3_11_34,cs-410,3,11,"00:02:40,300","00:02:42,340",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,"Precision, recall, and F."
cs-410_3_11_35,cs-410,3,11,"00:02:42,340","00:02:47,130",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,And then after we have completed
cs-410_3_11_36,cs-410,3,11,"00:02:47,130","00:02:51,590",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,we're going to aggregate them to generate
cs-410_3_11_37,cs-410,3,11,"00:02:51,590","00:02:52,490",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,overall F score.
cs-410_3_11_38,cs-410,3,11,"00:02:53,510","00:02:57,380",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"These are, again, examining"
cs-410_3_11_39,cs-410,3,11,"00:02:57,380","00:03:00,390",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,Which one's more useful will
cs-410_3_11_40,cs-410,3,11,"00:03:00,390","00:03:06,180",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"In general, it's beneficial to look at"
cs-410_3_11_41,cs-410,3,11,"00:03:06,180","00:03:10,850",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,And especially if you compare different
cs-410_3_11_42,cs-410,3,11,"00:03:10,850","00:03:16,370",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,it might reveal which method
cs-410_3_11_43,cs-410,3,11,"00:03:16,370","00:03:19,830",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,in what situations and
cs-410_3_11_44,cs-410,3,11,"00:03:19,830","00:03:23,070",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,Understanding the strands of a method or
cs-410_3_11_45,cs-410,3,11,"00:03:23,070","00:03:25,100",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,this provides further insight for
cs-410_3_11_46,cs-410,3,11,"00:03:28,260","00:03:32,180",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,"So as I mentioned,"
cs-410_3_11_47,cs-410,3,11,"00:03:32,180","00:03:35,890",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,in contrast to the macro average
cs-410_3_11_48,cs-410,3,11,"00:03:35,890","00:03:41,110",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"In this case, what we do is you"
cs-410_3_11_49,cs-410,3,11,"00:03:41,110","00:03:44,380",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,and then compute the precision and recall.
cs-410_3_11_50,cs-410,3,11,"00:03:45,460","00:03:50,480",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,So we can compute the overall
cs-410_3_11_51,cs-410,3,11,"00:03:50,480","00:03:55,832",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,"how many cases are in true positive,"
cs-410_3_11_52,cs-410,3,11,"00:03:55,832","00:04:01,660",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,"etc, it's computing the values"
cs-410_3_11_53,cs-410,3,11,"00:04:01,660","00:04:04,090",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,and then we can compute the precision and
cs-410_3_11_54,cs-410,3,11,"00:04:06,060","00:04:10,296",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"In contrast, in macro-averaging, we're"
cs-410_3_11_55,cs-410,3,11,"00:04:10,296","00:04:16,070",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,And then aggregate over these categories
cs-410_3_11_56,cs-410,3,11,"00:04:16,070","00:04:19,950",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,then aggregate all the documents but
cs-410_3_11_57,cs-410,3,11,"00:04:21,130","00:04:24,660",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,Now this would be very similar to
cs-410_3_11_58,cs-410,3,11,"00:04:24,660","00:04:26,390",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,"used earlier, and"
cs-410_3_11_59,cs-410,3,11,"00:04:26,390","00:04:31,270",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,one problem here of course to treat all
cs-410_3_11_60,cs-410,3,11,"00:04:32,400","00:04:34,990",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,And this may not be desirable.
cs-410_3_11_61,cs-410,3,11,"00:04:36,310","00:04:39,160",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,But it may be a property for
cs-410_3_11_62,cs-410,3,11,"00:04:39,160","00:04:45,570",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"especially if we associate the, for"
cs-410_3_11_63,cs-410,3,11,"00:04:45,570","00:04:50,090",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,"Then we can actually compute for example,"
cs-410_3_11_64,cs-410,3,11,"00:04:50,090","00:04:55,140",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,Where you associate the different cost or
cs-410_3_11_65,cs-410,3,11,"00:04:56,210","00:04:59,620",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,so there could be variations of these
cs-410_3_11_66,cs-410,3,11,"00:04:59,620","00:05:06,398",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,But in general macro average tends to
cs-410_3_11_67,cs-410,3,11,"00:05:06,398","00:05:13,889",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,just because it might reflect the need for
cs-410_3_11_68,cs-410,3,11,"00:05:14,890","00:05:20,620",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,on each category or performance on each
cs-410_3_11_69,cs-410,3,11,"00:05:20,620","00:05:27,210",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,"But macro averaging and micro averaging,"
cs-410_3_11_70,cs-410,3,11,"00:05:27,210","00:05:32,780",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,and you might see both reported in
cs-410_3_11_71,cs-410,3,11,"00:05:32,780","00:05:36,750",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,Also sometimes categorization
cs-410_3_11_72,cs-410,3,11,"00:05:36,750","00:05:39,290",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,be evaluated from ranking prospective.
cs-410_3_11_73,cs-410,3,11,"00:05:40,400","00:05:43,990",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,And this is because categorization
cs-410_3_11_74,cs-410,3,11,"00:05:43,990","00:05:49,610",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,often indeed passed it to a human for
cs-410_3_11_75,cs-410,3,11,"00:05:49,610","00:05:53,300",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,"For example, it might be passed"
cs-410_3_11_76,cs-410,3,11,"00:05:53,300","00:05:58,810",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,"For example, news articles can be tempted"
cs-410_3_11_77,cs-410,3,11,"00:05:58,810","00:06:01,040",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,then human editors would
cs-410_3_11_78,cs-410,3,11,"00:06:02,680","00:06:07,500",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,And all the email messages might be
cs-410_3_11_79,cs-410,3,11,"00:06:07,500","00:06:09,890",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,handling in the help desk.
cs-410_3_11_80,cs-410,3,11,"00:06:09,890","00:06:14,090",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,And in such a case the categorizations
cs-410_3_11_81,cs-410,3,11,"00:06:14,090","00:06:18,600",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,the task for
cs-410_3_11_82,cs-410,3,11,"00:06:19,690","00:06:25,360",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,"So, in this case the results"
cs-410_3_11_83,cs-410,3,11,"00:06:26,370","00:06:32,450",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,and if the system can't give a score
cs-410_3_11_84,cs-410,3,11,"00:06:32,450","00:06:39,830",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,confidence then we can use the scores
cs-410_3_11_85,cs-410,3,11,"00:06:39,830","00:06:44,660",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"then evaluate the results as a rank list,"
cs-410_3_11_86,cs-410,3,11,"00:06:44,660","00:06:47,990",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,Evaluation where you rank
cs-410_3_11_87,cs-410,3,11,"00:06:49,040","00:06:53,840",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,So for example a discovery of
cs-410_3_11_88,cs-410,3,11,"00:06:55,790","00:07:00,140",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,based on ranking emails for
cs-410_3_11_89,cs-410,3,11,"00:07:00,140","00:07:04,660",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,And this is useful if you want people
cs-410_3_11_90,cs-410,3,11,"00:07:04,660","00:07:05,770",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,"spam, right?"
cs-410_3_11_91,cs-410,3,11,"00:07:05,770","00:07:10,170",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,The person would then take
cs-410_3_11_92,cs-410,3,11,"00:07:10,170","00:07:14,770",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,then verify whether this is indeed a spam.
cs-410_3_11_93,cs-410,3,11,"00:07:14,770","00:07:19,180",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,So to reflect the utility for
cs-410_3_11_94,cs-410,3,11,"00:07:19,180","00:07:23,860",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,better to evaluate Ranking Chris and this
cs-410_3_11_95,cs-410,3,11,"00:07:25,020","00:07:27,650",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,And in such a case often
cs-410_3_11_96,cs-410,3,11,"00:07:27,650","00:07:31,810",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,better formulated as a ranking problem
cs-410_3_11_97,cs-410,3,11,"00:07:31,810","00:07:35,545",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"So for example, ranking documents in"
cs-410_3_11_98,cs-410,3,11,"00:07:35,545","00:07:39,255",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,"as a binary categorization problem,"
cs-410_3_11_99,cs-410,3,11,"00:07:39,255","00:07:43,505",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,are useful to users from those that
cs-410_3_11_100,cs-410,3,11,"00:07:43,505","00:07:47,045",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,"frame this as a ranking problem,"
cs-410_3_11_101,cs-410,3,11,"00:07:47,045","00:07:50,540",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,That's because people tend
cs-410_3_11_102,cs-410,3,11,"00:07:52,160","00:07:56,420",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,ranking evaluation more reflects
cs-410_3_11_103,cs-410,3,11,"00:07:58,180","00:08:02,230",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,"So to summarize categorization evaluation,"
cs-410_3_11_104,cs-410,3,11,"00:08:02,230","00:08:05,220",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,first evaluation is always very
cs-410_3_11_105,cs-410,3,11,"00:08:05,220","00:08:06,090",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,So get it right.
cs-410_3_11_106,cs-410,3,11,"00:08:07,200","00:08:10,120",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,"If you don't get it right,"
cs-410_3_11_107,cs-410,3,11,"00:08:10,120","00:08:14,160",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,And you might be misled to believe
cs-410_3_11_108,cs-410,3,11,"00:08:14,160","00:08:15,810",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,which is in fact not true.
cs-410_3_11_109,cs-410,3,11,"00:08:15,810","00:08:17,460",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,So it's very important to get it right.
cs-410_3_11_110,cs-410,3,11,"00:08:18,880","00:08:22,270",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,Measures must also reflect
cs-410_3_11_111,cs-410,3,11,"00:08:22,270","00:08:24,100",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,a particular application.
cs-410_3_11_112,cs-410,3,11,"00:08:24,100","00:08:25,760",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,"For example, in spam filtering and"
cs-410_3_11_113,cs-410,3,11,"00:08:25,760","00:08:29,670",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,news categorization the results
cs-410_3_11_114,cs-410,3,11,"00:08:30,680","00:08:33,760",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,So then we would need to
cs-410_3_11_115,cs-410,3,11,"00:08:33,760","00:08:35,490",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,design measures appropriately.
cs-410_3_11_116,cs-410,3,11,"00:08:36,650","00:08:41,660",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,We generally need to consider how will the
cs-410_3_11_117,cs-410,3,11,"00:08:41,660","00:08:43,630",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,and think from a user's perspective.
cs-410_3_11_118,cs-410,3,11,"00:08:43,630","00:08:46,220",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,What quality is important?
cs-410_3_11_119,cs-410,3,11,"00:08:46,220","00:08:47,880",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,What aspect of quality is important?
cs-410_3_11_120,cs-410,3,11,"00:08:49,240","00:08:52,440",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,Sometimes there are trade offs between
cs-410_3_11_121,cs-410,3,11,"00:08:52,440","00:08:57,610",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,recall and so we need to know for this
cs-410_3_11_122,cs-410,3,11,"00:08:57,610","00:08:58,860",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,or high precision is more important.
cs-410_3_11_123,cs-410,3,11,"00:08:59,910","00:09:03,570",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,Ideally we associate the different cost
cs-410_3_11_124,cs-410,3,11,"00:09:03,570","00:09:06,810",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,And this of course has to be designed
cs-410_3_11_125,cs-410,3,11,"00:09:08,140","00:09:12,950",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,Some commonly used measures for relative
cs-410_3_11_126,cs-410,3,11,"00:09:12,950","00:09:17,268",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,"Classification accuracy, it's very"
cs-410_3_11_127,cs-410,3,11,"00:09:17,268","00:09:22,230",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,[INAUDIBLE] preceding [INAUDIBLE]
cs-410_3_11_128,cs-410,3,11,"00:09:22,230","00:09:27,266",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,"report characterizing performances,"
cs-410_3_11_129,cs-410,3,11,"00:09:27,266","00:09:32,440",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,[INAUDIBLE] like a [INAUDIBLE] Per
cs-410_3_11_130,cs-410,3,11,"00:09:32,440","00:09:37,790",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,"take a average of all of them, different"
cs-410_3_11_131,cs-410,3,11,"00:09:37,790","00:09:42,910",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,"In general, you want to look at the"
cs-410_3_11_132,cs-410,3,11,"00:09:42,910","00:09:46,970",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,particular applications some perspectives
cs-410_3_11_133,cs-410,3,11,"00:09:46,970","00:09:50,120",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,diagnoses and
cs-410_3_11_134,cs-410,3,11,"00:09:50,120","00:09:54,920",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,It's generally useful to look at
cs-410_3_11_135,cs-410,3,11,"00:09:54,920","00:10:00,220",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,to see subtle differences between methods
cs-410_3_11_136,cs-410,3,11,"00:10:00,220","00:10:03,100",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,from which you can obtain sight for
cs-410_3_11_137,cs-410,3,11,"00:10:04,670","00:10:07,340",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,Finally sometimes ranking
cs-410_3_11_138,cs-410,3,11,"00:10:07,340","00:10:11,590",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,be careful sometimes categorization has
cs-410_3_11_139,cs-410,3,11,"00:10:11,590","00:10:16,390",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,and there're machine running methods for
cs-410_3_11_140,cs-410,3,11,"00:10:17,480","00:10:19,990",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,So here are two suggested readings.
cs-410_3_11_141,cs-410,3,11,"00:10:19,990","00:10:25,120",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,One is some chapters of this book where
cs-410_3_11_142,cs-410,3,11,"00:10:25,120","00:10:27,090",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,evaluation measures.
cs-410_3_11_143,cs-410,3,11,"00:10:27,090","00:10:31,916",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,The second is a paper about
cs-410_3_11_144,cs-410,3,11,"00:10:31,916","00:10:33,759",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,text categorization and
cs-410_3_11_145,cs-410,3,11,"00:10:33,759","00:10:39,738",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,it also has an excellent discussion of
cs-410_3_11_146,cs-410,3,11,"00:10:39,738","00:10:49,738",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,[MUSIC]
cs-410_3_2_1,cs-410,3,2,"00:00:00,012","00:00:03,576",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_2_2,cs-410,3,2,"00:00:08,498","00:00:10,214",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=8,This lecture is about
cs-410_3_2_3,cs-410,3,2,"00:00:10,214","00:00:14,988",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,Document Length Normalization
cs-410_3_2_4,cs-410,3,2,"00:00:14,988","00:00:19,740",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"In this lecture, we will continue"
cs-410_3_2_5,cs-410,3,2,"00:00:19,740","00:00:23,990",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,"In particular, we're going to discuss the"
cs-410_3_2_6,cs-410,3,2,"00:00:25,850","00:00:30,330",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,So far in the lectures about the vector
cs-410_3_2_7,cs-410,3,2,"00:00:30,330","00:00:37,480",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,signals from the document to assess
cs-410_3_2_8,cs-410,3,2,"00:00:37,480","00:00:40,000",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,"In particular,"
cs-410_3_2_9,cs-410,3,2,"00:00:40,000","00:00:42,750",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,The count of a tone in a document.
cs-410_3_2_10,cs-410,3,2,"00:00:42,750","00:00:48,055",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,We have also considered it's
cs-410_3_2_11,cs-410,3,2,"00:00:48,055","00:00:50,795",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"IDF, Inverse Document Frequency."
cs-410_3_2_12,cs-410,3,2,"00:00:50,795","00:00:53,620",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,But we have not considered
cs-410_3_2_13,cs-410,3,2,"00:00:54,855","00:01:00,899",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,"So here I show two example documents,"
cs-410_3_2_14,cs-410,3,2,"00:01:01,910","00:01:05,098",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"D6 on the other hand, has a 5000 words."
cs-410_3_2_15,cs-410,3,2,"00:01:05,098","00:01:08,882",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,If you look at the matching
cs-410_3_2_16,cs-410,3,2,"00:01:08,882","00:01:13,878",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,"we see that in d6, there are more"
cs-410_3_2_17,cs-410,3,2,"00:01:13,878","00:01:18,958",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"But one might reason that,"
cs-410_3_2_18,cs-410,3,2,"00:01:18,958","00:01:23,410",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,these query words in a scattered manner.
cs-410_3_2_19,cs-410,3,2,"00:01:24,450","00:01:30,060",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,"So maybe the topic of d6, is not"
cs-410_3_2_20,cs-410,3,2,"00:01:31,350","00:01:34,980",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"So, the discussion of the campaign"
cs-410_3_2_21,cs-410,3,2,"00:01:34,980","00:01:38,739",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,may have nothing to do with the managing
cs-410_3_2_22,cs-410,3,2,"00:01:40,810","00:01:44,600",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"In general,"
cs-410_3_2_23,cs-410,3,2,"00:01:44,600","00:01:47,370",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,they would have a higher chance for
cs-410_3_2_24,cs-410,3,2,"00:01:47,370","00:01:54,760",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"In fact, if you generate a long document"
cs-410_3_2_25,cs-410,3,2,"00:01:54,760","00:01:59,690",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,"a distribution of words, then eventually"
cs-410_3_2_26,cs-410,3,2,"00:02:00,760","00:02:05,800",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"So in this sense, we should penalize on"
cs-410_3_2_27,cs-410,3,2,"00:02:05,800","00:02:10,400",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"better chance matching to any query, and"
cs-410_3_2_28,cs-410,3,2,"00:02:12,300","00:02:18,600",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,We also need to be careful in avoiding
cs-410_3_2_29,cs-410,3,2,"00:02:19,770","00:02:22,790",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,"On the one hand,"
cs-410_3_2_30,cs-410,3,2,"00:02:22,790","00:02:27,202",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"But on the other hand,"
cs-410_3_2_31,cs-410,3,2,"00:02:27,202","00:02:30,790",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"Now, the reasoning is because"
cs-410_3_2_32,cs-410,3,2,"00:02:30,790","00:02:31,309",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,different reasons.
cs-410_3_2_33,cs-410,3,2,"00:02:32,770","00:02:36,950",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,"In one case, the document may be"
cs-410_3_2_34,cs-410,3,2,"00:02:38,270","00:02:44,460",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,"So for example, think about the vortex"
cs-410_3_2_35,cs-410,3,2,"00:02:44,460","00:02:47,620",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,It would use more words than
cs-410_3_2_36,cs-410,3,2,"00:02:49,560","00:02:53,140",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"So, this is a case where we probably"
cs-410_3_2_37,cs-410,3,2,"00:02:54,980","00:02:57,278",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,long documents such as a full paper.
cs-410_3_2_38,cs-410,3,2,"00:02:57,278","00:03:02,520",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,When we compare the matching
cs-410_3_2_39,cs-410,3,2,"00:03:02,520","00:03:06,410",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,document with matching of
cs-410_3_2_40,cs-410,3,2,"00:03:07,830","00:03:10,700",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,"Then long papers in general,"
cs-410_3_2_41,cs-410,3,2,"00:03:10,700","00:03:15,380",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,have a higher chance of matching clearer
cs-410_3_2_42,cs-410,3,2,"00:03:15,380","00:03:18,550",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,"However, there is another case"
cs-410_3_2_43,cs-410,3,2,"00:03:18,550","00:03:21,750",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,that is when the document
cs-410_3_2_44,cs-410,3,2,"00:03:21,750","00:03:24,040",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,Now consider another
cs-410_3_2_45,cs-410,3,2,"00:03:24,040","00:03:29,450",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,where we simply concatenate a lot
cs-410_3_2_46,cs-410,3,2,"00:03:29,450","00:03:34,190",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"In such a case, obviously, we don't want"
cs-410_3_2_47,cs-410,3,2,"00:03:34,190","00:03:38,270",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"Indeed, we probably don't want to penalize"
cs-410_3_2_48,cs-410,3,2,"00:03:39,700","00:03:46,490",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"So that's why, we need to be careful about"
cs-410_3_2_49,cs-410,3,2,"00:03:48,360","00:03:52,420",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"A method of that has been working well,"
cs-410_3_2_50,cs-410,3,2,"00:03:52,420","00:03:54,890",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,is called a pivoted length normalization.
cs-410_3_2_51,cs-410,3,2,"00:03:54,890","00:03:55,860",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"And in this case,"
cs-410_3_2_52,cs-410,3,2,"00:03:55,860","00:04:01,550",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,the idea is to use the average document
cs-410_3_2_53,cs-410,3,2,"00:04:01,550","00:04:05,820",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,That means we'll assume that for
cs-410_3_2_54,cs-410,3,2,"00:04:05,820","00:04:10,335",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,the score is about right so
cs-410_3_2_55,cs-410,3,2,"00:04:10,335","00:04:13,035",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,But if the document is longer
cs-410_3_2_56,cs-410,3,2,"00:04:14,125","00:04:16,275",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,then there will be some penalization.
cs-410_3_2_57,cs-410,3,2,"00:04:16,275","00:04:20,785",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"Whereas if it's a shorter,"
cs-410_3_2_58,cs-410,3,2,"00:04:20,785","00:04:26,050",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,So this is illustrated at
cs-410_3_2_59,cs-410,3,2,"00:04:26,050","00:04:28,578",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,x-axis you can see the length of document.
cs-410_3_2_60,cs-410,3,2,"00:04:28,578","00:04:33,390",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,"On the y-axis, we show the normalizer."
cs-410_3_2_61,cs-410,3,2,"00:04:33,390","00:04:39,080",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"In this case, the Pivoted Length"
cs-410_3_2_62,cs-410,3,2,"00:04:39,080","00:04:45,850",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,is seeing to be interpolation of 1 and
cs-410_3_2_63,cs-410,3,2,"00:04:45,850","00:04:50,460",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,the normalize the document in length
cs-410_3_2_64,cs-410,3,2,"00:04:53,110","00:04:58,640",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"So you can see here,"
cs-410_3_2_65,cs-410,3,2,"00:04:58,640","00:05:03,470",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"of the document by the average documents,"
cs-410_3_2_66,cs-410,3,2,"00:05:03,470","00:05:07,890",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,sense about how this document is
cs-410_3_2_67,cs-410,3,2,"00:05:07,890","00:05:16,120",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,also gives us a benefit of not
cs-410_3_2_68,cs-410,3,2,"00:05:16,120","00:05:18,990",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,We can measure the length by words or
cs-410_3_2_69,cs-410,3,2,"00:05:20,760","00:05:24,260",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,"Anyway, this normalizer"
cs-410_3_2_70,cs-410,3,2,"00:05:24,260","00:05:29,660",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,"First we see that, if we set the parameter"
cs-410_3_2_71,cs-410,3,2,"00:05:29,660","00:05:33,580",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,"So, there's no lens normalization at all."
cs-410_3_2_72,cs-410,3,2,"00:05:33,580","00:05:37,540",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,"So, b, in this sense,"
cs-410_3_2_73,cs-410,3,2,"00:05:39,450","00:05:44,980",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"Whereas, if we set b to a nonzero value,"
cs-410_3_2_74,cs-410,3,2,"00:05:44,980","00:05:49,010",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,"All right, so"
cs-410_3_2_75,cs-410,3,2,"00:05:49,010","00:05:52,179",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,documents that are longer than
cs-410_3_2_76,cs-410,3,2,"00:05:53,860","00:05:56,580",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,"Whereas, the value of"
cs-410_3_2_77,cs-410,3,2,"00:05:56,580","00:05:59,460",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,would be smaller for shorter documents.
cs-410_3_2_78,cs-410,3,2,"00:05:59,460","00:06:02,720",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,"So in this sense,"
cs-410_3_2_79,cs-410,3,2,"00:06:02,720","00:06:07,230",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,"long documents, and"
cs-410_3_2_80,cs-410,3,2,"00:06:09,040","00:06:11,500",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,The degree of penalization
cs-410_3_2_81,cs-410,3,2,"00:06:11,500","00:06:16,750",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"because if we set b to a larger value,"
cs-410_3_2_82,cs-410,3,2,"00:06:16,750","00:06:20,580",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,There's even more penalization for
cs-410_3_2_83,cs-410,3,2,"00:06:20,580","00:06:22,380",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,the short documents.
cs-410_3_2_84,cs-410,3,2,"00:06:22,380","00:06:25,440",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,"By adjusting b, which varies from 0 to 1,"
cs-410_3_2_85,cs-410,3,2,"00:06:25,440","00:06:29,450",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,we can control the degree
cs-410_3_2_86,cs-410,3,2,"00:06:29,450","00:06:35,050",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"So, if we plug in this length"
cs-410_3_2_87,cs-410,3,2,"00:06:35,050","00:06:40,490",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,"the vector space model, ranking functions"
cs-410_3_2_88,cs-410,3,2,"00:06:41,510","00:06:45,270",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,Then we will end up having
cs-410_3_2_89,cs-410,3,2,"00:06:46,370","00:06:51,569",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,And these are in fact the state of
cs-410_3_2_90,cs-410,3,2,"00:06:51,569","00:06:55,290",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,Let's take a look at each of them.
cs-410_3_2_91,cs-410,3,2,"00:06:55,290","00:07:00,972",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,The first one is called a pivoted length
cs-410_3_2_92,cs-410,3,2,"00:07:00,972","00:07:04,980",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,and a reference in [INAUDIBLE]
cs-410_3_2_93,cs-410,3,2,"00:07:04,980","00:07:11,836",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,"And here we see that, it's basically"
cs-410_3_2_94,cs-410,3,2,"00:07:11,836","00:07:16,830",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,the idea of component should
cs-410_3_2_95,cs-410,3,2,"00:07:18,010","00:07:21,608",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,There is also a query term
cs-410_3_2_96,cs-410,3,2,"00:07:24,628","00:07:30,504",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"And then, in the middle, there is"
cs-410_3_2_97,cs-410,3,2,"00:07:30,504","00:07:35,486",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,we see we use the double logarithm
cs-410_3_2_98,cs-410,3,2,"00:07:35,486","00:07:40,460",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,this is to achieve
cs-410_3_2_99,cs-410,3,2,"00:07:40,460","00:07:45,488",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,But we also put a document
cs-410_3_2_100,cs-410,3,2,"00:07:45,488","00:07:50,596",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"Right, so this would cause"
cs-410_3_2_101,cs-410,3,2,"00:07:50,596","00:07:56,698",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"because the larger the denominator is,"
cs-410_3_2_102,cs-410,3,2,"00:07:56,698","00:07:59,660",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,And this is of course controlled
cs-410_3_2_103,cs-410,3,2,"00:08:01,420","00:08:06,130",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,"And you can see again, if b is set to 0"
cs-410_3_2_104,cs-410,3,2,"00:08:08,760","00:08:16,350",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"Okay, so this is one of the two most"
cs-410_3_2_105,cs-410,3,2,"00:08:16,350","00:08:20,652",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"The next one called a BM25 or Okapi,"
cs-410_3_2_106,cs-410,3,2,"00:08:20,652","00:08:26,971",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,is also similar in that it
cs-410_3_2_107,cs-410,3,2,"00:08:26,971","00:08:30,478",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,and query IDF component here.
cs-410_3_2_108,cs-410,3,2,"00:08:32,958","00:08:36,150",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,"But in the middle,"
cs-410_3_2_109,cs-410,3,2,"00:08:36,150","00:08:41,450",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"As we explained,"
cs-410_3_2_110,cs-410,3,2,"00:08:41,450","00:08:46,460",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,and that does sublinear
cs-410_3_2_111,cs-410,3,2,"00:08:48,340","00:08:53,610",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,In this case we have put the length
cs-410_3_2_112,cs-410,3,2,"00:08:53,610","00:08:58,160",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,We're adjusting k but
cs-410_3_2_113,cs-410,3,2,"00:08:58,160","00:09:02,610",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,because we put a normalizer
cs-410_3_2_114,cs-410,3,2,"00:09:02,610","00:09:08,680",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,"Therefore, again, if a document is longer"
cs-410_3_2_115,cs-410,3,2,"00:09:10,110","00:09:16,070",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,So you can see after we have gone through
cs-410_3_2_116,cs-410,3,2,"00:09:16,070","00:09:24,226",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,and we have in the end reached
cs-410_3_2_117,cs-410,3,2,"00:09:24,226","00:09:28,726",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"So, So far, we have talked about"
cs-410_3_2_118,cs-410,3,2,"00:09:28,726","00:09:33,530",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,mainly how to place the document
cs-410_3_2_119,cs-410,3,2,"00:09:35,010","00:09:39,752",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,"And, this has played an important role"
cs-410_3_2_120,cs-410,3,2,"00:09:39,752","00:09:41,169",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,the simple function.
cs-410_3_2_121,cs-410,3,2,"00:09:41,169","00:09:45,728",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,"But there are also other dimensions,"
cs-410_3_2_122,cs-410,3,2,"00:09:45,728","00:09:50,343",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,"For example, can we further"
cs-410_3_2_123,cs-410,3,2,"00:09:50,343","00:09:53,648",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,the dimension of the Vector Space Model?
cs-410_3_2_124,cs-410,3,2,"00:09:53,648","00:09:57,424",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,"Now, we've just assumed that the bag"
cs-410_3_2_125,cs-410,3,2,"00:09:57,424","00:10:01,240",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,"dimension as a word but obviously,"
cs-410_3_2_126,cs-410,3,2,"00:10:01,240","00:10:07,040",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,"For example, a stemmed word, those"
cs-410_3_2_127,cs-410,3,2,"00:10:07,040","00:10:11,110",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,"into the same root form, so"
cs-410_3_2_128,cs-410,3,2,"00:10:11,110","00:10:16,510",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,that computation and computing were all
cs-410_3_2_129,cs-410,3,2,"00:10:16,510","00:10:18,740",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,We get those stop word removal.
cs-410_3_2_130,cs-410,3,2,"00:10:18,740","00:10:25,270",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,This is to remove some very common words
cs-410_3_2_131,cs-410,3,2,"00:10:26,760","00:10:29,750",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,We get use of phrases
cs-410_3_2_132,cs-410,3,2,"00:10:29,750","00:10:33,630",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=629,We can even use later in
cs-410_3_2_133,cs-410,3,2,"00:10:33,630","00:10:38,540",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,some clusters of words that represent the
cs-410_3_2_134,cs-410,3,2,"00:10:39,700","00:10:44,080",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,"We can also use smaller unit,"
cs-410_3_2_135,cs-410,3,2,"00:10:44,080","00:10:48,820",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,are sequences of and
cs-410_3_2_136,cs-410,3,2,"00:10:50,320","00:10:57,087",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,"However, in practice, people have found"
cs-410_3_2_137,cs-410,3,2,"00:10:57,087","00:11:02,148",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,phrases is still the most effective
cs-410_3_2_138,cs-410,3,2,"00:11:02,148","00:11:08,930",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,"So, this is still so far the most"
cs-410_3_2_139,cs-410,3,2,"00:11:10,120","00:11:12,560",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,And it's used in all major search engines.
cs-410_3_2_140,cs-410,3,2,"00:11:13,960","00:11:18,910",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,"I should also mention, that sometimes"
cs-410_3_2_141,cs-410,3,2,"00:11:18,910","00:11:21,300",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,domain specific tokenization.
cs-410_3_2_142,cs-410,3,2,"00:11:21,300","00:11:27,991",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,"And this is actually very important, as we"
cs-410_3_2_143,cs-410,3,2,"00:11:27,991","00:11:33,545",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,prevent us from matching them with each
cs-410_3_2_144,cs-410,3,2,"00:11:33,545","00:11:39,660",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,"In some languages like Chinese,"
cs-410_3_2_145,cs-410,3,2,"00:11:40,860","00:11:47,290",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,text to obtain word band rates because
cs-410_3_2_146,cs-410,3,2,"00:11:47,290","00:11:51,505",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,A word might correspond to one
cs-410_3_2_147,cs-410,3,2,"00:11:51,505","00:11:53,248",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,even three characters.
cs-410_3_2_148,cs-410,3,2,"00:11:53,248","00:11:58,164",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,"So, it's easier in English when we"
cs-410_3_2_149,cs-410,3,2,"00:11:58,164","00:12:02,590",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,"In some other languages, we may need"
cs-410_3_2_150,cs-410,3,2,"00:12:02,590","00:12:05,098",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,figure a way out of what
cs-410_3_2_151,cs-410,3,2,"00:12:05,098","00:12:10,850",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=725,There is also the possibility to
cs-410_3_2_152,cs-410,3,2,"00:12:10,850","00:12:13,510",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,And so
cs-410_3_2_153,cs-410,3,2,"00:12:13,510","00:12:16,137",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,one can imagine there are other measures.
cs-410_3_2_154,cs-410,3,2,"00:12:16,137","00:12:20,550",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,"For example, we can measure the cosine"
cs-410_3_2_155,cs-410,3,2,"00:12:20,550","00:12:23,740",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,Or we can use Euclidean distance measure.
cs-410_3_2_156,cs-410,3,2,"00:12:24,880","00:12:27,280",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,"And these are all possible, but"
cs-410_3_2_157,cs-410,3,2,"00:12:27,280","00:12:32,680",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=747,dot product seems still the best and
cs-410_3_2_158,cs-410,3,2,"00:12:33,780","00:12:38,143",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,"In fact that it's sufficiently general,"
cs-410_3_2_159,cs-410,3,2,"00:12:38,143","00:12:43,280",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,if you consider the possibilities
cs-410_3_2_160,cs-410,3,2,"00:12:44,280","00:12:45,390",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,"So, for example,"
cs-410_3_2_161,cs-410,3,2,"00:12:45,390","00:12:50,440",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=765,cosine measure can be thought of as the
cs-410_3_2_162,cs-410,3,2,"00:12:50,440","00:12:54,720",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=770,"That means, we first normalize each factor"
cs-410_3_2_163,cs-410,3,2,"00:12:54,720","00:12:57,720",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,That would be critical
cs-410_3_2_164,cs-410,3,2,"00:12:57,720","00:13:03,860",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=777,"I just mentioned that the BM25, seems to"
cs-410_3_2_165,cs-410,3,2,"00:13:04,930","00:13:09,420",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,But there has been also further
cs-410_3_2_166,cs-410,3,2,"00:13:09,420","00:13:15,478",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=789,"Although, none of these words have"
cs-410_3_2_167,cs-410,3,2,"00:13:15,478","00:13:20,090",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=795,"So in one line work,"
cs-410_3_2_168,cs-410,3,2,"00:13:20,090","00:13:26,663",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,"Here, F stands for field, and this is"
cs-410_3_2_169,cs-410,3,2,"00:13:26,663","00:13:30,960",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=806,"So for example, you might consider"
cs-410_3_2_170,cs-410,3,2,"00:13:30,960","00:13:33,240",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,or body of the research article.
cs-410_3_2_171,cs-410,3,2,"00:13:33,240","00:13:39,800",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,"Or even anchor text on the web page,"
cs-410_3_2_172,cs-410,3,2,"00:13:39,800","00:13:44,970",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=819,links to other pages and
cs-410_3_2_173,cs-410,3,2,"00:13:44,970","00:13:50,490",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,a proper way of different fields to help
cs-410_3_2_174,cs-410,3,2,"00:13:50,490","00:13:55,430",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=830,When we use BM25 for such a document and
cs-410_3_2_175,cs-410,3,2,"00:13:55,430","00:14:00,750",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=835,the obvious choice is to apply BM25 for
cs-410_3_2_176,cs-410,3,2,"00:14:00,750","00:14:06,620",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,"Basically, the idea of BM25F is"
cs-410_3_2_177,cs-410,3,2,"00:14:06,620","00:14:11,670",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=846,"counts of terms in all the fields,"
cs-410_3_2_178,cs-410,3,2,"00:14:11,670","00:14:19,430",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,"Now, this has advantage of avoiding over"
cs-410_3_2_179,cs-410,3,2,"00:14:19,430","00:14:22,000",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,Remember in the sublinear
cs-410_3_2_180,cs-410,3,2,"00:14:22,000","00:14:27,800",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,the first occurrence is very important and
cs-410_3_2_181,cs-410,3,2,"00:14:27,800","00:14:29,660",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=867,"And if we do that for all the fields,"
cs-410_3_2_182,cs-410,3,2,"00:14:29,660","00:14:35,110",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=869,then the same term might have gained
cs-410_3_2_183,cs-410,3,2,"00:14:35,110","00:14:38,820",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,But when we combine these
cs-410_3_2_184,cs-410,3,2,"00:14:38,820","00:14:42,110",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=878,we just do the transformation one time.
cs-410_3_2_185,cs-410,3,2,"00:14:42,110","00:14:42,870",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=882,"At that time,"
cs-410_3_2_186,cs-410,3,2,"00:14:42,870","00:14:47,170",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=882,then the extra occurrences will not be
cs-410_3_2_187,cs-410,3,2,"00:14:48,790","00:14:54,039",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=888,And this method has been working very well
cs-410_3_2_188,cs-410,3,2,"00:14:55,810","00:14:59,283",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=895,The other line of extension
cs-410_3_2_189,cs-410,3,2,"00:14:59,283","00:15:03,810",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,"In this line,"
cs-410_3_2_190,cs-410,3,2,"00:15:03,810","00:15:05,980",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=903,over penalization of
cs-410_3_2_191,cs-410,3,2,"00:15:08,880","00:15:13,990",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=908,"So to address this problem,"
cs-410_3_2_192,cs-410,3,2,"00:15:13,990","00:15:18,180",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,We can simply add a small constant
cs-410_3_2_193,cs-410,3,2,"00:15:18,180","00:15:23,400",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=918,"But what's interesting is that,"
cs-410_3_2_194,cs-410,3,2,"00:15:23,400","00:15:28,340",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,"doing such a small modification,"
cs-410_3_2_195,cs-410,3,2,"00:15:28,340","00:15:33,570",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=928,the problem of over penalization of
cs-410_3_2_196,cs-410,3,2,"00:15:33,570","00:15:36,380",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=933,"So the new formula called BM25+,"
cs-410_3_2_197,cs-410,3,2,"00:15:36,380","00:15:40,990",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=936,is empirically and
cs-410_3_2_198,cs-410,3,2,"00:15:42,590","00:15:48,432",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=942,So to summarize all what we have
cs-410_3_2_199,cs-410,3,2,"00:15:48,432","00:15:52,100",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,here are the major take away points.
cs-410_3_2_200,cs-410,3,2,"00:15:52,100","00:15:57,590",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=952,"First, in such a model,"
cs-410_3_2_201,cs-410,3,2,"00:15:57,590","00:16:01,780",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,Assuming that relevance of a document
cs-410_3_2_202,cs-410,3,2,"00:16:02,820","00:16:08,030",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=962,basically proportional to the similarity
cs-410_3_2_203,cs-410,3,2,"00:16:08,030","00:16:10,640",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=968,"So naturally,"
cs-410_3_2_204,cs-410,3,2,"00:16:10,640","00:16:13,830",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=970,document must have been
cs-410_3_2_205,cs-410,3,2,"00:16:13,830","00:16:19,050",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=973,"And in this case, we will present them as"
cs-410_3_2_206,cs-410,3,2,"00:16:19,050","00:16:24,170",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=979,"Where the dimensions are defined by words,"
cs-410_3_2_207,cs-410,3,2,"00:16:25,470","00:16:29,850",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=985,"And we generally, need to use a lot of"
cs-410_3_2_208,cs-410,3,2,"00:16:29,850","00:16:34,560",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=989,"We use some examples, which show"
cs-410_3_2_209,cs-410,3,2,"00:16:34,560","00:16:37,200",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=994,including Tf weighting and transformation.
cs-410_3_2_210,cs-410,3,2,"00:16:38,740","00:16:41,950",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=998,"And IDF weighting, and"
cs-410_3_2_211,cs-410,3,2,"00:16:41,950","00:16:45,890",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1001,These major heuristics are the most
cs-410_3_2_212,cs-410,3,2,"00:16:45,890","00:16:51,544",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,to ensure such a general ranking function
cs-410_3_2_213,cs-410,3,2,"00:16:51,544","00:16:55,640",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1011,"And finally, BM25 and"
cs-410_3_2_214,cs-410,3,2,"00:16:55,640","00:16:59,890",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1015,to be the most effective formulas
cs-410_3_2_215,cs-410,3,2,"00:16:59,890","00:17:05,100",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1019,"Now I have to say that, I put BM25 in"
cs-410_3_2_216,cs-410,3,2,"00:17:05,100","00:17:09,759",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1025,"in fact, the BM25 has been derived"
cs-410_3_2_217,cs-410,3,2,"00:17:11,970","00:17:17,470",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1031,So the reason why I've put it in
cs-410_3_2_218,cs-410,3,2,"00:17:17,470","00:17:22,540",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1037,the ranking function actually has a nice
cs-410_3_2_219,cs-410,3,2,"00:17:22,540","00:17:23,450",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1042,"We can easily see,"
cs-410_3_2_220,cs-410,3,2,"00:17:23,450","00:17:27,390",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1043,it looks very much like a vector space
cs-410_3_2_221,cs-410,3,2,"00:17:28,890","00:17:34,640",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1048,The second reason is because the original
cs-410_3_2_222,cs-410,3,2,"00:17:36,070","00:17:39,420",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1056,And that form of IDF after
cs-410_3_2_223,cs-410,3,2,"00:17:39,420","00:17:44,630",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1059,well as the standard IDF
cs-410_3_2_224,cs-410,3,2,"00:17:44,630","00:17:47,910",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1064,"So as effective retrieval function,"
cs-410_3_2_225,cs-410,3,2,"00:17:47,910","00:17:53,360",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1067,BM25 should probably use a heuristic
cs-410_3_2_226,cs-410,3,2,"00:17:53,360","00:17:55,628",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1073,To make them even more look
cs-410_3_2_227,cs-410,3,2,"00:17:59,218","00:18:01,460",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1079,There are some additional readings.
cs-410_3_2_228,cs-410,3,2,"00:18:01,460","00:18:05,330",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1081,"The first is, a paper about"
cs-410_3_2_229,cs-410,3,2,"00:18:05,330","00:18:09,224",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1085,It's an excellent example
cs-410_3_2_230,cs-410,3,2,"00:18:09,224","00:18:13,650",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1089,analysis to suggest the need for
cs-410_3_2_231,cs-410,3,2,"00:18:13,650","00:18:17,590",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1093,then further derive the length
cs-410_3_2_232,cs-410,3,2,"00:18:17,590","00:18:22,830",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1097,"The second, is the original paper"
cs-410_3_2_233,cs-410,3,2,"00:18:24,180","00:18:28,452",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1104,"The third paper,"
cs-410_3_2_234,cs-410,3,2,"00:18:28,452","00:18:31,660",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1108,"its extensions, particularly BM25 F."
cs-410_3_2_235,cs-410,3,2,"00:18:32,860","00:18:38,305",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1112,"And finally, in the last paper"
cs-410_3_2_236,cs-410,3,2,"00:18:38,305","00:18:43,768",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1118,BM25 to correct the over
cs-410_3_2_237,cs-410,3,2,"00:18:43,768","00:18:53,768",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,[MUSIC]
cs-410_3_3_1,cs-410,3,3,"00:00:00,199","00:00:03,699",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_3_3_2,cs-410,3,3,"00:00:07,099","00:00:11,070",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,"This lecture is about,"
cs-410_3_3_3,cs-410,3,3,"00:00:13,290","00:00:17,810",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In this lecture, we will continue"
cs-410_3_3_4,cs-410,3,3,"00:00:17,810","00:00:18,470",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,"In particular,"
cs-410_3_3_5,cs-410,3,3,"00:00:18,470","00:00:21,799",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"we are going to look at, how we can"
cs-410_3_3_6,cs-410,3,3,"00:00:24,970","00:00:30,410",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"In the previous lecture,"
cs-410_3_3_7,cs-410,3,3,"00:00:30,410","00:00:33,430",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"These are the two basic measures for,"
cs-410_3_3_8,cs-410,3,3,"00:00:33,430","00:00:38,410",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,quantitatively measuring
cs-410_3_3_9,cs-410,3,3,"00:00:40,420","00:00:44,247",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,"But, as we talked about, ranking, before,"
cs-410_3_3_10,cs-410,3,3,"00:00:44,247","00:00:49,420",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,we framed that the text of retrieval
cs-410_3_3_11,cs-410,3,3,"00:00:50,800","00:00:55,820",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,"So, we also need to evaluate the,"
cs-410_3_3_12,cs-410,3,3,"00:00:56,910","00:01:01,097",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,How can we use precision-recall
cs-410_3_3_13,cs-410,3,3,"00:01:01,097","00:01:07,180",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"Well, naturally, we have to look after the"
cs-410_3_3_14,cs-410,3,3,"00:01:07,180","00:01:12,330",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,"Because in the end, the approximation"
cs-410_3_3_15,cs-410,3,3,"00:01:12,330","00:01:17,640",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"given by a ranked list, is determined"
cs-410_3_3_16,cs-410,3,3,"00:01:17,640","00:01:21,470",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,Right?
cs-410_3_3_17,cs-410,3,3,"00:01:21,470","00:01:25,520",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,"the list of results, the user would,"
cs-410_3_3_18,cs-410,3,3,"00:01:25,520","00:01:27,790",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,that point would determine the set.
cs-410_3_3_19,cs-410,3,3,"00:01:27,790","00:01:31,680",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"And then,"
cs-410_3_3_20,cs-410,3,3,"00:01:31,680","00:01:35,400",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"that we have to consider,"
cs-410_3_3_21,cs-410,3,3,"00:01:35,400","00:01:37,990",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,Without knowing where
cs-410_3_3_22,cs-410,3,3,"00:01:37,990","00:01:42,380",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"then we have to consider, all"
cs-410_3_3_23,cs-410,3,3,"00:01:42,380","00:01:44,720",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"So, let's look at these positions."
cs-410_3_3_24,cs-410,3,3,"00:01:44,720","00:01:49,020",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,"Look at this slide, and"
cs-410_3_3_25,cs-410,3,3,"00:01:49,020","00:01:51,718",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"what if the user stops at the,"
cs-410_3_3_26,cs-410,3,3,"00:01:51,718","00:01:55,140",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,What's the precision-recall at this point?
cs-410_3_3_27,cs-410,3,3,"00:01:55,140","00:01:55,800",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,What do you think?
cs-410_3_3_28,cs-410,3,3,"00:01:56,970","00:02:02,920",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,"Well, it's easy to see, that this document"
cs-410_3_3_29,cs-410,3,3,"00:02:02,920","00:02:05,960",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,"We have, got one document,"
cs-410_3_3_30,cs-410,3,3,"00:02:05,960","00:02:07,380",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,What about the recall?
cs-410_3_3_31,cs-410,3,3,"00:02:07,380","00:02:11,990",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"Well, note that, we're assuming that,"
cs-410_3_3_32,cs-410,3,3,"00:02:11,990","00:02:14,980",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,"this query in the collection,"
cs-410_3_3_33,cs-410,3,3,"00:02:16,310","00:02:18,650",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,What if the user stops
cs-410_3_3_34,cs-410,3,3,"00:02:19,820","00:02:20,320",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,Top two.
cs-410_3_3_35,cs-410,3,3,"00:02:21,470","00:02:25,820",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,"Well, the precision is the same,"
cs-410_3_3_36,cs-410,3,3,"00:02:25,820","00:02:27,060",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,"And, the record is two out of ten."
cs-410_3_3_37,cs-410,3,3,"00:02:28,600","00:02:31,630",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,What if the user stops
cs-410_3_3_38,cs-410,3,3,"00:02:31,630","00:02:35,980",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,"Well, this is interesting,"
cs-410_3_3_39,cs-410,3,3,"00:02:35,980","00:02:40,030",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,"additional relevant document,"
cs-410_3_3_40,cs-410,3,3,"00:02:41,170","00:02:45,600",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,"But the precision is lower,"
cs-410_3_3_41,cs-410,3,3,"00:02:45,600","00:02:46,680",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,what's exactly the precision?
cs-410_3_3_42,cs-410,3,3,"00:02:49,110","00:02:52,020",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"Well, it's two out of three, right?"
cs-410_3_3_43,cs-410,3,3,"00:02:52,020","00:02:54,920",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"And, recall is the same, two out of ten."
cs-410_3_3_44,cs-410,3,3,"00:02:54,920","00:02:58,930",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,"So, when would see another point,"
cs-410_3_3_45,cs-410,3,3,"00:02:58,930","00:03:02,473",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"Now, if you look down the list,"
cs-410_3_3_46,cs-410,3,3,"00:03:02,473","00:03:06,110",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,"we have, seeing another relevant document."
cs-410_3_3_47,cs-410,3,3,"00:03:06,110","00:03:10,800",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"In this case D5, at that point, the,"
cs-410_3_3_48,cs-410,3,3,"00:03:10,800","00:03:13,840",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,"three out of ten, and,"
cs-410_3_3_49,cs-410,3,3,"00:03:15,150","00:03:20,200",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,"So, you can see, if we keep doing this,"
cs-410_3_3_50,cs-410,3,3,"00:03:20,200","00:03:23,780",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,"And then, we will have"
cs-410_3_3_51,cs-410,3,3,"00:03:23,780","00:03:26,150",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,"because there are eight documents,"
cs-410_3_3_52,cs-410,3,3,"00:03:26,150","00:03:28,200",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"And, the recall is a four out of ten."
cs-410_3_3_53,cs-410,3,3,"00:03:29,540","00:03:33,500",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"Now, when can we get,"
cs-410_3_3_54,cs-410,3,3,"00:03:33,500","00:03:39,740",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,"Well, in this list, we don't have it,"
cs-410_3_3_55,cs-410,3,3,"00:03:39,740","00:03:40,560",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"We don't know, where it is?"
cs-410_3_3_56,cs-410,3,3,"00:03:40,560","00:03:45,700",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,"But, as convenience, we often assume that,"
cs-410_3_3_57,cs-410,3,3,"00:03:47,230","00:03:51,890",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"at all the, the othe,"
cs-410_3_3_58,cs-410,3,3,"00:03:51,890","00:03:56,550",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"all the other levels of recall,"
cs-410_3_3_59,cs-410,3,3,"00:03:56,550","00:03:59,140",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"So, of course,"
cs-410_3_3_60,cs-410,3,3,"00:03:59,140","00:04:04,040",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"the actual position would be higher,"
cs-410_3_3_61,cs-410,3,3,"00:04:05,230","00:04:09,390",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"in order to, have an easy way to,"
cs-410_3_3_62,cs-410,3,3,"00:04:09,390","00:04:12,980",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,compute another measure called Average
cs-410_3_3_63,cs-410,3,3,"00:04:14,300","00:04:16,560",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,"Now, I should also say, now, here you see,"
cs-410_3_3_64,cs-410,3,3,"00:04:16,560","00:04:21,230",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,we make these assumptions that
cs-410_3_3_65,cs-410,3,3,"00:04:22,270","00:04:28,950",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"But, this is okay, for"
cs-410_3_3_66,cs-410,3,3,"00:04:28,950","00:04:34,870",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,"And, this is for the relative comparison,"
cs-410_3_3_67,cs-410,3,3,"00:04:34,870","00:04:39,560",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,"or actual, actual number deviates"
cs-410_3_3_68,cs-410,3,3,"00:04:39,560","00:04:41,970",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"As long as the deviation,"
cs-410_3_3_69,cs-410,3,3,"00:04:41,970","00:04:46,810",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,is not biased toward any particular
cs-410_3_3_70,cs-410,3,3,"00:04:46,810","00:04:50,560",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,"We can still,"
cs-410_3_3_71,cs-410,3,3,"00:04:50,560","00:04:53,360",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,"And, this is important point,"
cs-410_3_3_72,cs-410,3,3,"00:04:53,360","00:04:55,550",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"When you compare different algorithms,"
cs-410_3_3_73,cs-410,3,3,"00:04:55,550","00:04:58,810",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,the key's to avoid any
cs-410_3_3_74,cs-410,3,3,"00:04:58,810","00:05:02,130",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"And, as long as, you can avoid that."
cs-410_3_3_75,cs-410,3,3,"00:05:02,130","00:05:06,580",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,"It's okay, for you to do transformation"
cs-410_3_3_76,cs-410,3,3,"00:05:06,580","00:05:07,640",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,you can preserve the order.
cs-410_3_3_77,cs-410,3,3,"00:05:09,380","00:05:11,170",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"Okay, so, we'll just talk about,"
cs-410_3_3_78,cs-410,3,3,"00:05:11,170","00:05:16,030",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,we can get a lot of precision-recall
cs-410_3_3_79,cs-410,3,3,"00:05:16,030","00:05:19,000",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,"So, now, you can imagine,"
cs-410_3_3_80,cs-410,3,3,"00:05:19,000","00:05:22,389",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,"And, this just shows on the,"
cs-410_3_3_81,cs-410,3,3,"00:05:23,610","00:05:30,110",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,"And, on the y-axis, we show the precision."
cs-410_3_3_82,cs-410,3,3,"00:05:30,110","00:05:35,336",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,"So, the precision line was marked as .1,"
cs-410_3_3_83,cs-410,3,3,"00:05:35,336","00:05:35,998",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,Right?
cs-410_3_3_84,cs-410,3,3,"00:05:35,998","00:05:38,618",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,"this is, the different, levels of recall."
cs-410_3_3_85,cs-410,3,3,"00:05:38,618","00:05:44,390",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,"And,, the y-axis also has,"
cs-410_3_3_86,cs-410,3,3,"00:05:45,450","00:05:49,150",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"So, we plot the, these, precision-recall"
cs-410_3_3_87,cs-410,3,3,"00:05:49,150","00:05:51,360",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,as points on this picture.
cs-410_3_3_88,cs-410,3,3,"00:05:51,360","00:05:56,410",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,"Now, we can further, and"
cs-410_3_3_89,cs-410,3,3,"00:05:56,410","00:05:57,290",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"As you'll see,"
cs-410_3_3_90,cs-410,3,3,"00:05:57,290","00:06:02,040",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,"we assumed all the other, precision"
cs-410_3_3_91,cs-410,3,3,"00:06:02,040","00:06:08,232",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,"And, that's why, they are down here,"
cs-410_3_3_92,cs-410,3,3,"00:06:08,232","00:06:14,980",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,"And this, the actual curve probably will"
cs-410_3_3_93,cs-410,3,3,"00:06:14,980","00:06:19,410",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"discussed, it, it doesn't matter that"
cs-410_3_3_94,cs-410,3,3,"00:06:20,430","00:06:24,600",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,"because this would be,"
cs-410_3_3_95,cs-410,3,3,"00:06:25,950","00:06:31,016",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,"Okay, so, now that we,"
cs-410_3_3_96,cs-410,3,3,"00:06:31,016","00:06:34,290",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,how can we compare ranked to back list?
cs-410_3_3_97,cs-410,3,3,"00:06:34,290","00:06:37,049",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,"All right, so, that means,"
cs-410_3_3_98,cs-410,3,3,"00:06:38,430","00:06:40,880",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,"And here, we show, two cases."
cs-410_3_3_99,cs-410,3,3,"00:06:40,880","00:06:47,260",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,"Where system A is showing red,"
cs-410_3_3_100,cs-410,3,3,"00:06:48,610","00:06:50,820",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"All right, so, which one is better?"
cs-410_3_3_101,cs-410,3,3,"00:06:50,820","00:06:54,080",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,"I hope you can see,"
cs-410_3_3_102,cs-410,3,3,"00:06:54,080","00:06:56,900",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,Why?
cs-410_3_3_103,cs-410,3,3,"00:06:58,340","00:07:01,500",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"see same level of recall here,"
cs-410_3_3_104,cs-410,3,3,"00:07:01,500","00:07:06,800",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,"the precision point by system A is better,"
cs-410_3_3_105,cs-410,3,3,"00:07:06,800","00:07:08,260",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,"So, there's no question."
cs-410_3_3_106,cs-410,3,3,"00:07:08,260","00:07:13,360",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,"In here, you can imagine, what does the"
cs-410_3_3_107,cs-410,3,3,"00:07:13,360","00:07:17,470",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,"Well, it has to have perfect,"
cs-410_3_3_108,cs-410,3,3,"00:07:17,470","00:07:18,450",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,it has to be this line.
cs-410_3_3_109,cs-410,3,3,"00:07:18,450","00:07:21,300",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,That would be the ideal system.
cs-410_3_3_110,cs-410,3,3,"00:07:21,300","00:07:24,230",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,"In general, the higher the curve is,"
cs-410_3_3_111,cs-410,3,3,"00:07:24,230","00:07:27,160",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"The problem is that,"
cs-410_3_3_112,cs-410,3,3,"00:07:27,160","00:07:29,110",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,This actually happens often.
cs-410_3_3_113,cs-410,3,3,"00:07:29,110","00:07:30,790",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,"Like, the two curves cross each other."
cs-410_3_3_114,cs-410,3,3,"00:07:32,430","00:07:34,240",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,"Now, in this case, which one is better?"
cs-410_3_3_115,cs-410,3,3,"00:07:35,300","00:07:35,800",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,What do you think?
cs-410_3_3_116,cs-410,3,3,"00:07:38,240","00:07:41,730",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"Now, this is a real problem,"
cs-410_3_3_117,cs-410,3,3,"00:07:41,730","00:07:47,150",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"Suppose, you build a search engine,"
cs-410_3_3_118,cs-410,3,3,"00:07:47,150","00:07:50,990",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,"that's shown here in blue, or system B."
cs-410_3_3_119,cs-410,3,3,"00:07:50,990","00:07:53,580",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"And, you have come up with a new idea."
cs-410_3_3_120,cs-410,3,3,"00:07:53,580","00:07:54,500",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"And, you test it."
cs-410_3_3_121,cs-410,3,3,"00:07:54,500","00:07:58,490",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,"And, the results are shown in red,"
cs-410_3_3_122,cs-410,3,3,"00:07:59,990","00:08:04,550",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"Now, your question is, is your new"
cs-410_3_3_123,cs-410,3,3,"00:08:05,630","00:08:10,510",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,"Or more, practically,"
cs-410_3_3_124,cs-410,3,3,"00:08:10,510","00:08:15,410",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"you're already using, your, in your search"
cs-410_3_3_125,cs-410,3,3,"00:08:15,410","00:08:20,760",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,"So, should we use system,"
cs-410_3_3_126,cs-410,3,3,"00:08:20,760","00:08:23,250",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,"This is going to be a real decision,"
cs-410_3_3_127,cs-410,3,3,"00:08:23,250","00:08:29,430",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,"If you make the replacement, the search"
cs-410_3_3_128,cs-410,3,3,"00:08:29,430","00:08:34,170",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,"whereas, if you don't do that,"
cs-410_3_3_129,cs-410,3,3,"00:08:34,170","00:08:34,770",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,"So, what do you do?"
cs-410_3_3_130,cs-410,3,3,"00:08:36,210","00:08:40,580",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"Now, if you want to spend more time"
cs-410_3_3_131,cs-410,3,3,"00:08:40,580","00:08:42,840",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,"And, it's actually very"
cs-410_3_3_132,cs-410,3,3,"00:08:42,840","00:08:46,350",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,"As I said, it's a real decision that you"
cs-410_3_3_133,cs-410,3,3,"00:08:46,350","00:08:51,329",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,"search engine, or if you're working, for"
cs-410_3_3_134,cs-410,3,3,"00:08:52,330","00:08:54,630",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,"Now, if you have thought about this for"
cs-410_3_3_135,cs-410,3,3,"00:08:54,630","00:08:59,630",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,"a moment, you might realize that,"
cs-410_3_3_136,cs-410,3,3,"00:08:59,630","00:09:04,615",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,"Now, some users might like a system A,"
cs-410_3_3_137,cs-410,3,3,"00:09:04,615","00:09:05,895",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,"So, what's the difference here?"
cs-410_3_3_138,cs-410,3,3,"00:09:05,895","00:09:08,545",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,"Well, the difference is just that,"
cs-410_3_3_139,cs-410,3,3,"00:09:08,545","00:09:14,145",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"in the, low level of recall,"
cs-410_3_3_140,cs-410,3,3,"00:09:14,145","00:09:15,845",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,There's a higher precision.
cs-410_3_3_141,cs-410,3,3,"00:09:15,845","00:09:19,520",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,"But in high recall region,"
cs-410_3_3_142,cs-410,3,3,"00:09:20,910","00:09:24,040",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"Now, so, that also means,"
cs-410_3_3_143,cs-410,3,3,"00:09:24,040","00:09:28,630",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"cares about the high recall, or"
cs-410_3_3_144,cs-410,3,3,"00:09:28,630","00:09:32,200",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,"You can imagine, if someone is just going"
cs-410_3_3_145,cs-410,3,3,"00:09:32,200","00:09:33,489",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,want to find out something
cs-410_3_3_146,cs-410,3,3,"00:09:34,750","00:09:36,530",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"Well, which one is better?"
cs-410_3_3_147,cs-410,3,3,"00:09:36,530","00:09:37,060",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,What do you think?
cs-410_3_3_148,cs-410,3,3,"00:09:38,110","00:09:41,510",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,"In this case, clearly, system B is better,"
cs-410_3_3_149,cs-410,3,3,"00:09:41,510","00:09:44,920",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,because the user is unlikely
cs-410_3_3_150,cs-410,3,3,"00:09:44,920","00:09:46,500",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,The user doesn't care about high recall.
cs-410_3_3_151,cs-410,3,3,"00:09:47,780","00:09:50,673",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,"On the other hand,"
cs-410_3_3_152,cs-410,3,3,"00:09:50,673","00:09:54,800",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,"where a user is doing you are,"
cs-410_3_3_153,cs-410,3,3,"00:09:54,800","00:10:00,320",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,"You want to find, whether your idea ha,"
cs-410_3_3_154,cs-410,3,3,"00:10:00,320","00:10:03,130",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,"In that case, you emphasize high recall."
cs-410_3_3_155,cs-410,3,3,"00:10:03,130","00:10:06,630",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,"So, you want to see,"
cs-410_3_3_156,cs-410,3,3,"00:10:06,630","00:10:09,570",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,"Therefore, you might, favor, system A."
cs-410_3_3_157,cs-410,3,3,"00:10:09,570","00:10:12,090",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,"So, that means, which one is better?"
cs-410_3_3_158,cs-410,3,3,"00:10:12,090","00:10:18,420",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,"That actually depends on users,"
cs-410_3_3_159,cs-410,3,3,"00:10:19,520","00:10:24,040",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,"So, this means, you may not necessarily"
cs-410_3_3_160,cs-410,3,3,"00:10:25,290","00:10:28,810",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,that would accurately
cs-410_3_3_161,cs-410,3,3,"00:10:29,860","00:10:31,750",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=629,You have to look at the overall picture.
cs-410_3_3_162,cs-410,3,3,"00:10:31,750","00:10:35,620",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,"Yet, as I said, when you have"
cs-410_3_3_163,cs-410,3,3,"00:10:35,620","00:10:38,210",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,"whether you replace ours with another,"
cs-410_3_3_164,cs-410,3,3,"00:10:38,210","00:10:44,320",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,then you may have to actually come up with
cs-410_3_3_165,cs-410,3,3,"00:10:44,320","00:10:49,800",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,"Or, when we compare many different"
cs-410_3_3_166,cs-410,3,3,"00:10:49,800","00:10:54,590",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,"one number to compare, them with, so, that"
cs-410_3_3_167,cs-410,3,3,"00:10:54,590","00:11:00,258",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,"So, for all these reasons, it is desirable"
cs-410_3_3_168,cs-410,3,3,"00:11:00,258","00:11:01,510",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,"So, how do we do that?"
cs-410_3_3_169,cs-410,3,3,"00:11:01,510","00:11:05,860",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,"And, that,"
cs-410_3_3_170,cs-410,3,3,"00:11:05,860","00:11:09,560",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,"So, here again it's"
cs-410_3_3_171,cs-410,3,3,"00:11:09,560","00:11:13,570",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=669,"And, one way to summarize"
cs-410_3_3_172,cs-410,3,3,"00:11:13,570","00:11:18,090",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,"this whole curve,"
cs-410_3_3_173,cs-410,3,3,"00:11:19,330","00:11:21,820",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,Right?
cs-410_3_3_174,cs-410,3,3,"00:11:21,820","00:11:25,209",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,"There are other ways to measure that,"
cs-410_3_3_175,cs-410,3,3,"00:11:26,430","00:11:31,110",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=686,this particular way of matching
cs-410_3_3_176,cs-410,3,3,"00:11:31,110","00:11:36,260",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,"has been used, since a long time ago for"
cs-410_3_3_177,cs-410,3,3,"00:11:36,260","00:11:41,140",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,"basically, in this way, and"
cs-410_3_3_178,cs-410,3,3,"00:11:41,140","00:11:46,260",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,"Basically, we're going to take a, a look"
cs-410_3_3_179,cs-410,3,3,"00:11:47,600","00:11:49,540",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,"And then, look out for the precision."
cs-410_3_3_180,cs-410,3,3,"00:11:49,540","00:11:51,930",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,"So, we know, you know,"
cs-410_3_3_181,cs-410,3,3,"00:11:51,930","00:11:56,590",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,"And, this is another,"
cs-410_3_3_182,cs-410,3,3,"00:11:56,590","00:11:59,362",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=716,"Now, this, we don't count to this one,"
cs-410_3_3_183,cs-410,3,3,"00:11:59,362","00:12:04,511",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=719,"because the recall level is the same,"
cs-410_3_3_184,cs-410,3,3,"00:12:04,511","00:12:10,120",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,"this number, and that's precision at"
cs-410_3_3_185,cs-410,3,3,"00:12:10,120","00:12:13,580",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,"So, we have all these, you know, added up."
cs-410_3_3_186,cs-410,3,3,"00:12:13,580","00:12:16,180",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,These are the precisions
cs-410_3_3_187,cs-410,3,3,"00:12:16,180","00:12:21,130",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,corresponding to retrieving the first
cs-410_3_3_188,cs-410,3,3,"00:12:21,130","00:12:25,260",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,"then, the third, that follows, et cetera."
cs-410_3_3_189,cs-410,3,3,"00:12:25,260","00:12:29,265",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,"Now, we missed the many relevant"
cs-410_3_3_190,cs-410,3,3,"00:12:29,265","00:12:32,380",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,"we just, assume,"
cs-410_3_3_191,cs-410,3,3,"00:12:33,540","00:12:35,740",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,"And then, finally, we take the average."
cs-410_3_3_192,cs-410,3,3,"00:12:35,740","00:12:37,900",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=755,"So, we divide it by ten, and"
cs-410_3_3_193,cs-410,3,3,"00:12:37,900","00:12:40,510",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,which is the total number of relevant
cs-410_3_3_194,cs-410,3,3,"00:12:41,670","00:12:46,610",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,"Note that here,"
cs-410_3_3_195,cs-410,3,3,"00:12:46,610","00:12:49,440",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,Which is a number retrieved
cs-410_3_3_196,cs-410,3,3,"00:12:49,440","00:12:52,980",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,"Now, imagine, if I divide by four,"
cs-410_3_3_197,cs-410,3,3,"00:12:54,370","00:12:55,820",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,"Now, think about this, for a moment."
cs-410_3_3_198,cs-410,3,3,"00:12:57,050","00:13:01,300",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=777,"It's a common mistake that people,"
cs-410_3_3_199,cs-410,3,3,"00:13:02,720","00:13:08,208",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=782,"Right, so, if we, we divide this by four,"
cs-410_3_3_200,cs-410,3,3,"00:13:08,208","00:13:13,180",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,"In fact, that you are favoring a system,"
cs-410_3_3_201,cs-410,3,3,"00:13:13,180","00:13:18,785",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=793,"documents, as in that case,"
cs-410_3_3_202,cs-410,3,3,"00:13:18,785","00:13:22,115",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,"So, this would be, not a good matching."
cs-410_3_3_203,cs-410,3,3,"00:13:22,115","00:13:25,862",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=802,"So, note that this denomina,"
cs-410_3_3_204,cs-410,3,3,"00:13:25,862","00:13:29,170",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=805,the total number of relevant documents.
cs-410_3_3_205,cs-410,3,3,"00:13:29,170","00:13:33,620",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=809,"And, this will basically ,compute"
cs-410_3_3_206,cs-410,3,3,"00:13:33,620","00:13:40,080",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,"And, this is the standard method,"
cs-410_3_3_207,cs-410,3,3,"00:13:41,210","00:13:44,860",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=821,"Note that, it actually combines"
cs-410_3_3_208,cs-410,3,3,"00:13:44,860","00:13:49,230",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,"But first, you know, we have"
cs-410_3_3_209,cs-410,3,3,"00:13:49,230","00:13:53,240",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=829,"we also consider recall, because if missed"
cs-410_3_3_210,cs-410,3,3,"00:13:53,240","00:13:57,470",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=833,"All right, so,"
cs-410_3_3_211,cs-410,3,3,"00:13:57,470","00:14:02,190",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,"And furthermore, you can see this"
cs-410_3_3_212,cs-410,3,3,"00:14:02,190","00:14:04,770",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=842,of a position of a relevant document.
cs-410_3_3_213,cs-410,3,3,"00:14:04,770","00:14:09,520",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,"Let's say, if I move this relevant"
cs-410_3_3_214,cs-410,3,3,"00:14:09,520","00:14:12,670",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=849,"it would increase this means,"
cs-410_3_3_215,cs-410,3,3,"00:14:12,670","00:14:17,630",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=852,"Whereas, if I move any relevant document,"
cs-410_3_3_216,cs-410,3,3,"00:14:17,630","00:14:23,720",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=857,"document down, then it would decrease,"
cs-410_3_3_217,cs-410,3,3,"00:14:23,720","00:14:25,266",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=863,"So, this is a very good,"
cs-410_3_3_218,cs-410,3,3,"00:14:25,266","00:14:30,570",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,because it's a very sensitive to
cs-410_3_3_219,cs-410,3,3,"00:14:30,570","00:14:34,740",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,"It can tell, small differences"
cs-410_3_3_220,cs-410,3,3,"00:14:34,740","00:14:35,880",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=874,"And, that is what we want,"
cs-410_3_3_221,cs-410,3,3,"00:14:35,880","00:14:40,430",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,sometimes one algorithm only works
cs-410_3_3_222,cs-410,3,3,"00:14:40,430","00:14:42,440",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=880,"And, we want to see this difference."
cs-410_3_3_223,cs-410,3,3,"00:14:42,440","00:14:46,110",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=882,"In contrast, if we look at"
cs-410_3_3_224,cs-410,3,3,"00:14:46,110","00:14:49,520",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,"If we look at this, this whole set, well,"
cs-410_3_3_225,cs-410,3,3,"00:14:49,520","00:14:52,000",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=889,"what, what's the precision,"
cs-410_3_3_226,cs-410,3,3,"00:14:52,000","00:14:54,328",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=892,"Well, it's easy to see,"
cs-410_3_3_227,cs-410,3,3,"00:14:54,328","00:15:02,200",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,"So, that precision is very meaningful,"
cs-410_3_3_228,cs-410,3,3,"00:15:02,200","00:15:04,580",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=902,"So, that's pretty useful, right?"
cs-410_3_3_229,cs-410,3,3,"00:15:04,580","00:15:07,850",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,"So, it's a meaningful measure,"
cs-410_3_3_230,cs-410,3,3,"00:15:07,850","00:15:11,770",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=907,"But, if we use this measure to"
cs-410_3_3_231,cs-410,3,3,"00:15:11,770","00:15:16,480",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=911,because it wouldn't be sensitive to where
cs-410_3_3_232,cs-410,3,3,"00:15:16,480","00:15:21,910",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=916,If I move them around the precision
cs-410_3_3_233,cs-410,3,3,"00:15:21,910","00:15:22,570",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=921,Right.
cs-410_3_3_234,cs-410,3,3,"00:15:22,570","00:15:25,590",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=922,this is not a good measure for
cs-410_3_3_235,cs-410,3,3,"00:15:25,590","00:15:29,990",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=925,"In contrast, the average precision"
cs-410_3_3_236,cs-410,3,3,"00:15:29,990","00:15:34,511",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=929,"It can tell the difference of, different,"
cs-410_3_3_237,cs-410,3,3,"00:15:34,511","00:15:39,269",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=934,"a difference in ranked list in,"
cs-410_3_3_238,cs-410,3,3,"00:15:39,269","00:15:49,269",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,[MUSIC]
cs-410_3_4_1,cs-410,3,4,"00:00:00,012","00:00:03,532",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_4_2,cs-410,3,4,"00:00:07,767","00:00:10,058",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,"This lecture is about query likelihood,"
cs-410_3_4_3,cs-410,3,4,"00:00:10,058","00:00:11,960",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,probabilistic retrieval model.
cs-410_3_4_4,cs-410,3,4,"00:00:14,040","00:00:15,310",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"In this lecture,"
cs-410_3_4_5,cs-410,3,4,"00:00:15,310","00:00:19,190",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,we continue the discussion of
cs-410_3_4_6,cs-410,3,4,"00:00:19,190","00:00:22,830",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,"In particular, we're going to talk about"
cs-410_3_4_7,cs-410,3,4,"00:00:25,870","00:00:31,073",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"In the query light holder retrieval model,"
cs-410_3_4_8,cs-410,3,4,"00:00:31,073","00:00:35,420",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,How like their user who likes a document
cs-410_3_4_9,cs-410,3,4,"00:00:36,990","00:00:41,462",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,"So in this case,"
cs-410_3_4_10,cs-410,3,4,"00:00:41,462","00:00:46,663",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,particular document about
cs-410_3_4_11,cs-410,3,4,"00:00:46,663","00:00:50,410",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,"Now we assume,"
cs-410_3_4_12,cs-410,3,4,"00:00:50,410","00:00:54,780",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,a basis to impose a query to try and
cs-410_3_4_13,cs-410,3,4,"00:00:57,340","00:01:03,840",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,"So again, imagine use a process"
cs-410_3_4_14,cs-410,3,4,"00:01:03,840","00:01:06,940",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,Where we assume that
cs-410_3_4_15,cs-410,3,4,"00:01:06,940","00:01:08,640",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,assembling words from the document.
cs-410_3_4_16,cs-410,3,4,"00:01:10,560","00:01:15,880",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"So for example, a user might"
cs-410_3_4_17,cs-410,3,4,"00:01:15,880","00:01:19,390",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,from this document and
cs-410_3_4_18,cs-410,3,4,"00:01:20,600","00:01:24,590",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,And then the user would pick
cs-410_3_4_19,cs-410,3,4,"00:01:24,590","00:01:25,910",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,that would be the second query word.
cs-410_3_4_20,cs-410,3,4,"00:01:27,420","00:01:32,400",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,Now this of course is an assumption
cs-410_3_4_21,cs-410,3,4,"00:01:32,400","00:01:35,008",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,how a user would pose a query.
cs-410_3_4_22,cs-410,3,4,"00:01:35,008","00:01:39,788",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,Whether a user actually followed this
cs-410_3_4_23,cs-410,3,4,"00:01:39,788","00:01:45,230",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,this assumption has allowed us to formerly
cs-410_3_4_24,cs-410,3,4,"00:01:46,390","00:01:50,930",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,And this allows us to also not rely on
cs-410_3_4_25,cs-410,3,4,"00:01:52,580","00:01:55,750",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,to use empirical data to
cs-410_3_4_26,cs-410,3,4,"00:01:56,870","00:02:00,820",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,And this is why we can use this
cs-410_3_4_27,cs-410,3,4,"00:02:00,820","00:02:03,569",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,retrieval function that we can
cs-410_3_4_28,cs-410,3,4,"00:02:04,900","00:02:08,991",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,So as you see the assumption
cs-410_3_4_29,cs-410,3,4,"00:02:08,991","00:02:11,558",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,word is independent of the sample.
cs-410_3_4_30,cs-410,3,4,"00:02:11,558","00:02:17,880",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,And also each word is basically
cs-410_3_4_31,cs-410,3,4,"00:02:20,910","00:02:24,540",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,So now let's see how this works exactly.
cs-410_3_4_32,cs-410,3,4,"00:02:24,540","00:02:28,550",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"Well, since we are completing"
cs-410_3_4_33,cs-410,3,4,"00:02:29,730","00:02:34,444",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,then the probability here is just
cs-410_3_4_34,cs-410,3,4,"00:02:34,444","00:02:37,210",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,which is a sequence of words.
cs-410_3_4_35,cs-410,3,4,"00:02:37,210","00:02:42,140",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,And we make the assumption that each
cs-410_3_4_36,cs-410,3,4,"00:02:42,140","00:02:46,670",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,"So as a result, the probability"
cs-410_3_4_37,cs-410,3,4,"00:02:46,670","00:02:48,920",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,of the probability of each query word.
cs-410_3_4_38,cs-410,3,4,"00:02:50,100","00:02:52,660",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,Now how do we compute
cs-410_3_4_39,cs-410,3,4,"00:02:52,660","00:02:56,740",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"Well, based on the assumption that a word"
cs-410_3_4_40,cs-410,3,4,"00:02:56,740","00:03:01,360",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,is picked from the document
cs-410_3_4_41,cs-410,3,4,"00:03:01,360","00:03:05,680",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,Now we know the probability of each word
cs-410_3_4_42,cs-410,3,4,"00:03:05,680","00:03:08,120",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,word in the document.
cs-410_3_4_43,cs-410,3,4,"00:03:08,120","00:03:13,780",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,"So for example, the probability of"
cs-410_3_4_44,cs-410,3,4,"00:03:13,780","00:03:17,520",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,Would be just the count
cs-410_3_4_45,cs-410,3,4,"00:03:17,520","00:03:23,060",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,divided by the total number of words
cs-410_3_4_46,cs-410,3,4,"00:03:23,060","00:03:28,940",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,So with these assumptions we now have
cs-410_3_4_47,cs-410,3,4,"00:03:28,940","00:03:30,970",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,We can use this to rank our documents.
cs-410_3_4_48,cs-410,3,4,"00:03:32,650","00:03:34,200",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,So does this model work?
cs-410_3_4_49,cs-410,3,4,"00:03:34,200","00:03:35,260",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,Let's take a look.
cs-410_3_4_50,cs-410,3,4,"00:03:35,260","00:03:38,670",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,Here are some example documents
cs-410_3_4_51,cs-410,3,4,"00:03:38,670","00:03:42,210",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,Suppose now the query is
cs-410_3_4_52,cs-410,3,4,"00:03:42,210","00:03:44,880",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,we see the formula here on the top.
cs-410_3_4_53,cs-410,3,4,"00:03:45,900","00:03:47,490",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,So how do we score this document?
cs-410_3_4_54,cs-410,3,4,"00:03:47,490","00:03:48,790",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"Well, it's very simple."
cs-410_3_4_55,cs-410,3,4,"00:03:48,790","00:03:51,370",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,We just count how many times do
cs-410_3_4_56,cs-410,3,4,"00:03:51,370","00:03:54,380",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"how many times do we have seen campaigns,"
cs-410_3_4_57,cs-410,3,4,"00:03:54,380","00:03:57,458",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"And we see here 44, and"
cs-410_3_4_58,cs-410,3,4,"00:03:57,458","00:04:02,297",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,So that's 2 over the length of
cs-410_3_4_59,cs-410,3,4,"00:04:02,297","00:04:07,710",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,the length of document 4 for
cs-410_3_4_60,cs-410,3,4,"00:04:07,710","00:04:11,146",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,"And similarly, we can get probabilities"
cs-410_3_4_61,cs-410,3,4,"00:04:13,189","00:04:17,505",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,Now if you look at these numbers or
cs-410_3_4_62,cs-410,3,4,"00:04:17,505","00:04:22,030",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,"scoring all these documents,"
cs-410_3_4_63,cs-410,3,4,"00:04:22,030","00:04:28,436",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,Because if we assume d3 and
cs-410_3_4_64,cs-410,3,4,"00:04:28,436","00:04:35,628",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,then looks like a nominal rank d4
cs-410_3_4_65,cs-410,3,4,"00:04:35,628","00:04:40,819",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,"And as we would expect,"
cs-410_3_4_66,cs-410,3,4,"00:04:40,819","00:04:45,916",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"a TF query state, and so"
cs-410_3_4_67,cs-410,3,4,"00:04:45,916","00:04:50,096",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,"However, if we try a different"
cs-410_3_4_68,cs-410,3,4,"00:04:50,096","00:04:54,854",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,presidential campaign update
cs-410_3_4_69,cs-410,3,4,"00:04:54,854","00:04:56,608",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,Well what problem?
cs-410_3_4_70,cs-410,3,4,"00:04:56,608","00:04:58,930",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,Well think about the update.
cs-410_3_4_71,cs-410,3,4,"00:04:58,930","00:05:02,500",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,Now none of these documents
cs-410_3_4_72,cs-410,3,4,"00:05:02,500","00:05:08,420",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,So according to our assumption that a user
cs-410_3_4_73,cs-410,3,4,"00:05:08,420","00:05:15,003",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"generate a query, then the probability of"
cs-410_3_4_74,cs-410,3,4,"00:05:15,003","00:05:16,070",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,Would be 0.
cs-410_3_4_75,cs-410,3,4,"00:05:17,230","00:05:21,380",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,"So that causes a problem,"
cs-410_3_4_76,cs-410,3,4,"00:05:21,380","00:05:23,710",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,to have zero probability
cs-410_3_4_77,cs-410,3,4,"00:05:25,330","00:05:31,127",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,Now why it's fine to have zero probability
cs-410_3_4_78,cs-410,3,4,"00:05:31,127","00:05:33,902",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,It's not okay to have 0 for d3 and
cs-410_3_4_79,cs-410,3,4,"00:05:33,902","00:05:38,600",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,d4 because now we no longer
cs-410_3_4_80,cs-410,3,4,"00:05:38,600","00:05:39,135",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,What's worse?
cs-410_3_4_81,cs-410,3,4,"00:05:39,135","00:05:41,735",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,We can't even distinguish them from d2.
cs-410_3_4_82,cs-410,3,4,"00:05:41,735","00:05:45,700",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,So that's obviously not desirable.
cs-410_3_4_83,cs-410,3,4,"00:05:45,700","00:05:48,630",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"Now when a [INAUDIBLE] has such result,"
cs-410_3_4_84,cs-410,3,4,"00:05:48,630","00:05:50,960",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,we should think about what
cs-410_3_4_85,cs-410,3,4,"00:05:52,530","00:05:56,773",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,So we have to examine what
cs-410_3_4_86,cs-410,3,4,"00:05:56,773","00:05:59,644",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,as we derive this ranking function.
cs-410_3_4_87,cs-410,3,4,"00:05:59,644","00:06:03,285",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,Now is you examine those assumptions
cs-410_3_4_88,cs-410,3,4,"00:06:03,285","00:06:04,983",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,what has caused this problem?
cs-410_3_4_89,cs-410,3,4,"00:06:04,983","00:06:09,080",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,So take a moment to think about it.
cs-410_3_4_90,cs-410,3,4,"00:06:09,080","00:06:17,179",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,What do you think is the reason why update
cs-410_3_4_91,cs-410,3,4,"00:06:17,179","00:06:22,092",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,So if you think about this from the moment
cs-410_3_4_92,cs-410,3,4,"00:06:22,092","00:06:25,317",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,have made an assumption
cs-410_3_4_93,cs-410,3,4,"00:06:25,317","00:06:29,220",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,be drawn from the document
cs-410_3_4_94,cs-410,3,4,"00:06:29,220","00:06:33,982",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"So in order to fix this, we have to"
cs-410_3_4_95,cs-410,3,4,"00:06:33,982","00:06:36,912",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,a word not necessarily from the document.
cs-410_3_4_96,cs-410,3,4,"00:06:36,912","00:06:38,930",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,So that's the improved model.
cs-410_3_4_97,cs-410,3,4,"00:06:38,930","00:06:40,912",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,"An improvement here is to say that,"
cs-410_3_4_98,cs-410,3,4,"00:06:40,912","00:06:43,687",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,well instead of drawing
cs-410_3_4_99,cs-410,3,4,"00:06:43,687","00:06:48,064",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,let's imagine that the user would actually
cs-410_3_4_100,cs-410,3,4,"00:06:48,064","00:06:50,107",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,And so I show a model here.
cs-410_3_4_101,cs-410,3,4,"00:06:50,107","00:06:54,479",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,And we assume that this document is
cs-410_3_4_102,cs-410,3,4,"00:06:54,479","00:06:55,920",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,model.
cs-410_3_4_103,cs-410,3,4,"00:06:55,920","00:07:01,297",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,"Now, this model doesn't necessarily assign"
cs-410_3_4_104,cs-410,3,4,"00:07:01,297","00:07:05,853",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,we can assume this model does not
cs-410_3_4_105,cs-410,3,4,"00:07:05,853","00:07:09,621",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,Now if we're thinking this way then
cs-410_3_4_106,cs-410,3,4,"00:07:09,621","00:07:10,700",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,different.
cs-410_3_4_107,cs-410,3,4,"00:07:10,700","00:07:14,940",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,Now the user has this model in mind
cs-410_3_4_108,cs-410,3,4,"00:07:14,940","00:07:18,669",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,Although the model has to be
cs-410_3_4_109,cs-410,3,4,"00:07:18,669","00:07:22,960",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,So the user can again generate
cs-410_3_4_110,cs-410,3,4,"00:07:22,960","00:07:27,680",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,"Namely, pick a word for example,"
cs-410_3_4_111,cs-410,3,4,"00:07:29,020","00:07:32,390",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,Now the difference is that this time
cs-410_3_4_112,cs-410,3,4,"00:07:32,390","00:07:34,930",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,even though update doesn't
cs-410_3_4_113,cs-410,3,4,"00:07:34,930","00:07:38,050",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,to potentially generate
cs-410_3_4_114,cs-410,3,4,"00:07:38,050","00:07:43,840",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,So that a query was updated
cs-410_3_4_115,cs-410,3,4,"00:07:43,840","00:07:45,720",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,So this would fix our problem.
cs-410_3_4_116,cs-410,3,4,"00:07:45,720","00:07:50,140",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,And it's also reasonable because when our
cs-410_3_4_117,cs-410,3,4,"00:07:50,140","00:07:55,160",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"in a more general way, that is unique"
cs-410_3_4_118,cs-410,3,4,"00:07:55,160","00:07:57,830",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,So how do we compute
cs-410_3_4_119,cs-410,3,4,"00:07:57,830","00:08:01,000",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,If we make this sum wide
cs-410_3_4_120,cs-410,3,4,"00:08:01,000","00:08:07,390",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,"The first one is compute this model, and"
cs-410_3_4_121,cs-410,3,4,"00:08:07,390","00:08:15,070",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,"For example, I've shown two pulse models"
cs-410_3_4_122,cs-410,3,4,"00:08:15,070","00:08:19,803",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,And then given a query like a data mining
cs-410_3_4_123,cs-410,3,4,"00:08:19,803","00:08:22,467",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,just compute the likelihood of this query.
cs-410_3_4_124,cs-410,3,4,"00:08:22,467","00:08:26,574",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,And by making independence
cs-410_3_4_125,cs-410,3,4,"00:08:26,574","00:08:30,766",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,probability as a product of
cs-410_3_4_126,cs-410,3,4,"00:08:30,766","00:08:34,798",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,"We do this for both documents, and"
cs-410_3_4_127,cs-410,3,4,"00:08:34,798","00:08:35,700",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,then rank them.
cs-410_3_4_128,cs-410,3,4,"00:08:37,160","00:08:41,310",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,So that's the basic idea of this
cs-410_3_4_129,cs-410,3,4,"00:08:41,310","00:08:47,890",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,So more generally this ranking function
cs-410_3_4_130,cs-410,3,4,"00:08:47,890","00:08:51,800",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"Here we assume that the query has n words,"
cs-410_3_4_131,cs-410,3,4,"00:08:51,800","00:08:56,540",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,"w1 through wn, and"
cs-410_3_4_132,cs-410,3,4,"00:08:56,540","00:09:01,500",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,The ranking function is the probability
cs-410_3_4_133,cs-410,3,4,"00:09:01,500","00:09:06,080",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,given that the user is
cs-410_3_4_134,cs-410,3,4,"00:09:06,080","00:09:11,970",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,And this is assume it will be product of
cs-410_3_4_135,cs-410,3,4,"00:09:11,970","00:09:15,360",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,This is based on independent assumption.
cs-410_3_4_136,cs-410,3,4,"00:09:15,360","00:09:20,256",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,Now we actually often score
cs-410_3_4_137,cs-410,3,4,"00:09:20,256","00:09:25,250",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,using log of the query likelihood
cs-410_3_4_138,cs-410,3,4,"00:09:26,710","00:09:30,220",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,Now we do this to avoid
cs-410_3_4_139,cs-410,3,4,"00:09:30,220","00:09:35,830",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,"having a lot of small probabilities,"
cs-410_3_4_140,cs-410,3,4,"00:09:35,830","00:09:41,060",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,And this could cause under flow and we
cs-410_3_4_141,cs-410,3,4,"00:09:41,060","00:09:44,100",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,the value in our algorithm function.
cs-410_3_4_142,cs-410,3,4,"00:09:44,100","00:09:51,079",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,We maintain the order of these documents
cs-410_3_4_143,cs-410,3,4,"00:09:51,079","00:09:54,935",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,And so if we take longer than
cs-410_3_4_144,cs-410,3,4,"00:09:54,935","00:09:59,920",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,the product would become a sum
cs-410_3_4_145,cs-410,3,4,"00:09:59,920","00:10:03,620",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,So the sum of all the query
cs-410_3_4_146,cs-410,3,4,"00:10:03,620","00:10:07,670",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,that is one of the probability of
cs-410_3_4_147,cs-410,3,4,"00:10:09,360","00:10:13,020",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,And then we can further rewrite
cs-410_3_4_148,cs-410,3,4,"00:10:14,310","00:10:19,960",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,"So in the first sum here, in this sum,"
cs-410_3_4_149,cs-410,3,4,"00:10:21,910","00:10:28,800",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,we have it over all the query words and
cs-410_3_4_150,cs-410,3,4,"00:10:28,800","00:10:33,030",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,And in this sum we have a sum
cs-410_3_4_151,cs-410,3,4,"00:10:33,030","00:10:37,050",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,But we put a counter here
cs-410_3_4_152,cs-410,3,4,"00:10:37,050","00:10:39,780",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=637,Essentially we are only considering
cs-410_3_4_153,cs-410,3,4,"00:10:39,780","00:10:43,570",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,"because if a word is not in the query,"
cs-410_3_4_154,cs-410,3,4,"00:10:43,570","00:10:46,820",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,So we're still considering
cs-410_3_4_155,cs-410,3,4,"00:10:46,820","00:10:49,760",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,But we're using a different form as
cs-410_3_4_156,cs-410,3,4,"00:10:49,760","00:10:51,570",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,all the words in the vocabulary.
cs-410_3_4_157,cs-410,3,4,"00:10:52,960","00:10:56,435",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,"And of course, a word might occur"
cs-410_3_4_158,cs-410,3,4,"00:10:56,435","00:10:58,631",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,That's why we have a count here.
cs-410_3_4_159,cs-410,3,4,"00:11:00,407","00:11:04,168",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,And then this part is log of
cs-410_3_4_160,cs-410,3,4,"00:11:04,168","00:11:06,815",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=664,given by the document language model.
cs-410_3_4_161,cs-410,3,4,"00:11:08,647","00:11:11,497",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,"So you can see in this retrieval function,"
cs-410_3_4_162,cs-410,3,4,"00:11:11,497","00:11:13,547",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,we actually know the count
cs-410_3_4_163,cs-410,3,4,"00:11:13,547","00:11:16,507",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,So the only thing that we don't know
cs-410_3_4_164,cs-410,3,4,"00:11:17,817","00:11:21,310",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,"Therefore, we have converted"
cs-410_3_4_165,cs-410,3,4,"00:11:21,310","00:11:24,510",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,include the problem of estimating
cs-410_3_4_166,cs-410,3,4,"00:11:25,920","00:11:30,370",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,So that we can compute the probability of
cs-410_3_4_167,cs-410,3,4,"00:11:32,260","00:11:36,630",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,And different estimation methods would
cs-410_3_4_168,cs-410,3,4,"00:11:36,630","00:11:40,980",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,This is just like a different way to
cs-410_3_4_169,cs-410,3,4,"00:11:40,980","00:11:45,485",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,which leads to a different ranking
cs-410_3_4_170,cs-410,3,4,"00:11:45,485","00:11:49,353",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,Here different ways to
cs-410_3_4_171,cs-410,3,4,"00:11:49,353","00:11:54,065",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,a different ranking function for
cs-410_3_4_172,cs-410,3,4,"00:11:54,065","00:12:04,065",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,[MUSIC]
cs-410_3_5_1,cs-410,3,5,"00:00:00,000","00:00:07,194",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_5_2,cs-410,3,5,"00:00:07,194","00:00:10,660",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about the feedback in
cs-410_3_5_3,cs-410,3,5,"00:00:12,540","00:00:17,520",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture, we will continue the"
cs-410_3_5_4,cs-410,3,5,"00:00:17,520","00:00:18,089",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,"In particular,"
cs-410_3_5_5,cs-410,3,5,"00:00:18,089","00:00:20,659",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,we're going to talk about the feedback
cs-410_3_5_6,cs-410,3,5,"00:00:23,450","00:00:29,280",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,So we derive the query likelihood ranking
cs-410_3_5_7,cs-410,3,5,"00:00:30,410","00:00:35,860",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"As a basic retrieval function,"
cs-410_3_5_8,cs-410,3,5,"00:00:35,860","00:00:39,920",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,But if we think about the feedback
cs-410_3_5_9,cs-410,3,5,"00:00:39,920","00:00:44,730",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"use query likelihood to perform feedback,"
cs-410_3_5_10,cs-410,3,5,"00:00:44,730","00:00:49,620",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,a lot of times the feedback information is
cs-410_3_5_11,cs-410,3,5,"00:00:49,620","00:00:53,260",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,But we assume the query has
cs-410_3_5_12,cs-410,3,5,"00:00:53,260","00:00:56,850",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,from a language model in
cs-410_3_5_13,cs-410,3,5,"00:00:56,850","00:01:03,170",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,It's kind of unnatural to sample
cs-410_3_5_14,cs-410,3,5,"00:01:03,170","00:01:10,330",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,"As a result, researchers proposed a way"
cs-410_3_5_15,cs-410,3,5,"00:01:10,330","00:01:14,070",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,and it's called Kullback-Leibler
cs-410_3_5_16,cs-410,3,5,"00:01:15,450","00:01:20,422",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,And this model is actually going
cs-410_3_5_17,cs-410,3,5,"00:01:20,422","00:01:25,780",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,retrieval function much
cs-410_3_5_18,cs-410,3,5,"00:01:25,780","00:01:32,380",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,Yet this form of the language model
cs-410_3_5_19,cs-410,3,5,"00:01:32,380","00:01:36,560",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"query likelihood, in the sense that it can"
cs-410_3_5_20,cs-410,3,5,"00:01:38,180","00:01:39,300",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,"And in this case,"
cs-410_3_5_21,cs-410,3,5,"00:01:39,300","00:01:44,140",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,then feedback can be achieved through
cs-410_3_5_22,cs-410,3,5,"00:01:44,140","00:01:48,130",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,"This is very similar to Rocchio,"
cs-410_3_5_23,cs-410,3,5,"00:01:50,000","00:01:55,720",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,So let's see what is this
cs-410_3_5_24,cs-410,3,5,"00:01:55,720","00:02:02,306",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,"So on the top, what you see is a query"
cs-410_3_5_25,cs-410,3,5,"00:02:05,072","00:02:11,465",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"And then KL-divergence, or"
cs-410_3_5_26,cs-410,3,5,"00:02:11,465","00:02:16,292",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,retrieval model is basically to generalize
cs-410_3_5_27,cs-410,3,5,"00:02:16,292","00:02:21,600",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,the frequency part here
cs-410_3_5_28,cs-410,3,5,"00:02:21,600","00:02:26,910",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,So basically it's the difference given
cs-410_3_5_29,cs-410,3,5,"00:02:26,910","00:02:32,260",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,by the probabilistic model here to
cs-410_3_5_30,cs-410,3,5,"00:02:32,260","00:02:34,640",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,versus the count of query words there.
cs-410_3_5_31,cs-410,3,5,"00:02:35,820","00:02:42,610",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,And this difference allows us to plug in
cs-410_3_5_32,cs-410,3,5,"00:02:42,610","00:02:45,690",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,So this can be estimated
cs-410_3_5_33,cs-410,3,5,"00:02:45,690","00:02:48,260",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,including using feedback information.
cs-410_3_5_34,cs-410,3,5,"00:02:48,260","00:02:51,370",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,"But this is called a KL-divergence,"
cs-410_3_5_35,cs-410,3,5,"00:02:51,370","00:02:56,232",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,this can be interpreted as matching
cs-410_3_5_36,cs-410,3,5,"00:02:56,232","00:03:02,770",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,"One is the query model,"
cs-410_3_5_37,cs-410,3,5,"00:03:02,770","00:03:06,317",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,One is the document
cs-410_3_5_38,cs-410,3,5,"00:03:06,317","00:03:11,255",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,smooth them with a collection
cs-410_3_5_39,cs-410,3,5,"00:03:11,255","00:03:15,377",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,And we are not going to talk
cs-410_3_5_40,cs-410,3,5,"00:03:15,377","00:03:18,107",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,you'll find it in some references.
cs-410_3_5_41,cs-410,3,5,"00:03:18,107","00:03:22,023",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"It's also called cross entropy because,"
cs-410_3_5_42,cs-410,3,5,"00:03:22,023","00:03:26,207",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,we ignore some terms in
cs-410_3_5_43,cs-410,3,5,"00:03:26,207","00:03:29,690",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,we will end up having
cs-410_3_5_44,cs-410,3,5,"00:03:29,690","00:03:32,109",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,And both are terms of information theory.
cs-410_3_5_45,cs-410,3,5,"00:03:34,390","00:03:38,650",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"But anyway, for our purposes here,"
cs-410_3_5_46,cs-410,3,5,"00:03:38,650","00:03:42,820",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,you can just see the two
cs-410_3_5_47,cs-410,3,5,"00:03:42,820","00:03:48,330",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,except that here we have a probability of
cs-410_3_5_48,cs-410,3,5,"00:03:52,140","00:03:57,730",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,And here the sum is over all the words
cs-410_3_5_49,cs-410,3,5,"00:03:57,730","00:04:02,340",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,also with the nonzero probability for
cs-410_3_5_50,cs-410,3,5,"00:04:02,340","00:04:07,510",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,"So it's kind of, again, a generalization"
cs-410_3_5_51,cs-410,3,5,"00:04:09,930","00:04:15,980",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,Now you can also easily see we can recover
cs-410_3_5_52,cs-410,3,5,"00:04:15,980","00:04:22,130",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,by simply setting this query model to the
cs-410_3_5_53,cs-410,3,5,"00:04:23,450","00:04:26,510",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,This is very easy to
cs-410_3_5_54,cs-410,3,5,"00:04:26,510","00:04:30,005",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,into here you can eliminate this
cs-410_3_5_55,cs-410,3,5,"00:04:30,005","00:04:33,486",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,And then you will get exactly like that.
cs-410_3_5_56,cs-410,3,5,"00:04:33,486","00:04:35,879",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,So you can see the equivalence.
cs-410_3_5_57,cs-410,3,5,"00:04:35,879","00:04:41,581",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,And that's also why this KL-divergence
cs-410_3_5_58,cs-410,3,5,"00:04:41,581","00:04:47,085",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,"of query likelihood, because we can cover"
cs-410_3_5_59,cs-410,3,5,"00:04:47,085","00:04:49,730",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,But it would also allow us
cs-410_3_5_60,cs-410,3,5,"00:04:50,770","00:04:56,104",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,So this is how we can use the
cs-410_3_5_61,cs-410,3,5,"00:04:56,104","00:05:00,183",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,The picture shows that we first
cs-410_3_5_62,cs-410,3,5,"00:05:00,183","00:05:04,836",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,"then we estimate a query language model,"
cs-410_3_5_63,cs-410,3,5,"00:05:04,836","00:05:07,040",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,This is often denoted by a D here.
cs-410_3_5_64,cs-410,3,5,"00:05:09,560","00:05:14,690",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,But this basically means this is
cs-410_3_5_65,cs-410,3,5,"00:05:14,690","00:05:19,010",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,because we compute a vector for the
cs-410_3_5_66,cs-410,3,5,"00:05:19,010","00:05:22,450",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,"the query, and"
cs-410_3_5_67,cs-410,3,5,"00:05:22,450","00:05:26,580",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,Only that these vectors are of special
cs-410_3_5_68,cs-410,3,5,"00:05:27,910","00:05:31,680",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,And then we get the results and
cs-410_3_5_69,cs-410,3,5,"00:05:31,680","00:05:37,420",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,Let's assume they are mostly
cs-410_3_5_70,cs-410,3,5,"00:05:37,420","00:05:40,400",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,although we could also consider
cs-410_3_5_71,cs-410,3,5,"00:05:40,400","00:05:44,974",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"So what we could do is, like in Rocchio,"
cs-410_3_5_72,cs-410,3,5,"00:05:44,974","00:05:48,570",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,model called the feedback
cs-410_3_5_73,cs-410,3,5,"00:05:48,570","00:05:52,568",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"Again, this is going to be another vector"
cs-410_3_5_74,cs-410,3,5,"00:05:52,568","00:05:53,227",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,in Rocchio.
cs-410_3_5_75,cs-410,3,5,"00:05:53,227","00:05:58,060",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,And then this model can be combined
cs-410_3_5_76,cs-410,3,5,"00:05:58,060","00:06:02,800",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,"a linear interpolation, and"
cs-410_3_5_77,cs-410,3,5,"00:06:02,800","00:06:06,260",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,"just like, again, in Rocchio."
cs-410_3_5_78,cs-410,3,5,"00:06:06,260","00:06:10,270",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,So here we can see the parameter alpha
cs-410_3_5_79,cs-410,3,5,"00:06:10,270","00:06:14,170",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,"If it's set to zero,"
cs-410_3_5_80,cs-410,3,5,"00:06:14,170","00:06:19,050",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,"If it's set to one, we get full feedback"
cs-410_3_5_81,cs-410,3,5,"00:06:19,050","00:06:21,820",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,"And this is generally not desirable,"
cs-410_3_5_82,cs-410,3,5,"00:06:21,820","00:06:26,370",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,So unless you are absolutely sure you
cs-410_3_5_83,cs-410,3,5,"00:06:26,370","00:06:29,250",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,then the query terms are not important.
cs-410_3_5_84,cs-410,3,5,"00:06:31,180","00:06:34,870",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,"So of course, the main question here is,"
cs-410_3_5_85,cs-410,3,5,"00:06:34,870","00:06:39,340",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,"This is the big question here, and"
cs-410_3_5_86,cs-410,3,5,"00:06:39,340","00:06:41,760",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,So here we will talk about
cs-410_3_5_87,cs-410,3,5,"00:06:41,760","00:06:43,260",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,"there are many approaches, of course."
cs-410_3_5_88,cs-410,3,5,"00:06:43,260","00:06:45,891",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,This approach is based
cs-410_3_5_89,cs-410,3,5,"00:06:45,891","00:06:47,823",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,I'm going to show you how it works.
cs-410_3_5_90,cs-410,3,5,"00:06:47,823","00:06:50,560",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,This will use a generative mixture model.
cs-410_3_5_91,cs-410,3,5,"00:06:50,560","00:06:55,030",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,So this picture shows that
cs-410_3_5_92,cs-410,3,5,"00:06:55,030","00:06:57,060",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,the feedback model that
cs-410_3_5_93,cs-410,3,5,"00:06:58,080","00:07:00,490",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,And the basis is the feedback documents.
cs-410_3_5_94,cs-410,3,5,"00:07:00,490","00:07:04,110",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,Let's say we are observing
cs-410_3_5_95,cs-410,3,5,"00:07:04,110","00:07:09,012",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,These are the clicked documents by users
cs-410_3_5_96,cs-410,3,5,"00:07:09,012","00:07:12,679",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,or are simply top ranked documents
cs-410_3_5_97,cs-410,3,5,"00:07:14,710","00:07:17,834",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,Now imagine how we can
cs-410_3_5_98,cs-410,3,5,"00:07:17,834","00:07:20,630",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,these documents by using language model.
cs-410_3_5_99,cs-410,3,5,"00:07:20,630","00:07:23,330",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,One approach is simply to assume
cs-410_3_5_100,cs-410,3,5,"00:07:23,330","00:07:26,820",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,these documents are generated
cs-410_3_5_101,cs-410,3,5,"00:07:26,820","00:07:31,287",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"As we did before, what we could do"
cs-410_3_5_102,cs-410,3,5,"00:07:31,287","00:07:34,940",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,here to here and
cs-410_3_5_103,cs-410,3,5,"00:07:36,210","00:07:41,260",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,Now the question is whether this
cs-410_3_5_104,cs-410,3,5,"00:07:41,260","00:07:45,430",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"Well, you can imagine the top"
cs-410_3_5_105,cs-410,3,5,"00:07:45,430","00:07:46,190",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,What do you think?
cs-410_3_5_106,cs-410,3,5,"00:07:48,280","00:07:51,560",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,"Well, those words would be common words."
cs-410_3_5_107,cs-410,3,5,"00:07:51,560","00:07:53,770",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,"As we always see in a language model,"
cs-410_3_5_108,cs-410,3,5,"00:07:53,770","00:07:57,850",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,the top ranked words are actually
cs-410_3_5_109,cs-410,3,5,"00:07:57,850","00:08:02,570",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,"So it's not very good for feedback,"
cs-410_3_5_110,cs-410,3,5,"00:08:02,570","00:08:07,330",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,words to our query when we interpolate
cs-410_3_5_111,cs-410,3,5,"00:08:08,880","00:08:13,100",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"So this was not good, so"
cs-410_3_5_112,cs-410,3,5,"00:08:13,100","00:08:17,059",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"In particular, we are trying to"
cs-410_3_5_113,cs-410,3,5,"00:08:17,059","00:08:21,855",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,And we have seen actually one way
cs-410_3_5_114,cs-410,3,5,"00:08:21,855","00:08:27,020",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,language model in the case of
cs-410_3_5_115,cs-410,3,5,"00:08:27,020","00:08:30,830",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,the words that are related
cs-410_3_5_116,cs-410,3,5,"00:08:30,830","00:08:34,590",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,We could do that and that would be
cs-410_3_5_117,cs-410,3,5,"00:08:34,590","00:08:39,160",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,are going to talk about another approach
cs-410_3_5_118,cs-410,3,5,"00:08:39,160","00:08:43,990",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"In this case, we're going to say well,"
cs-410_3_5_119,cs-410,3,5,"00:08:43,990","00:08:48,818",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,in these documents that should not
cs-410_3_5_120,cs-410,3,5,"00:08:50,310","00:08:53,527",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,"So now what we can do is to assume that,"
cs-410_3_5_121,cs-410,3,5,"00:08:53,527","00:08:58,019",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,those words are generated from
cs-410_3_5_122,cs-410,3,5,"00:08:58,019","00:09:02,020",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,"they will generate those words like the,"
cs-410_3_5_123,cs-410,3,5,"00:09:02,020","00:09:05,302",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,"And if we use maximum likelihood estimate,"
cs-410_3_5_124,cs-410,3,5,"00:09:05,302","00:09:10,182",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,note that if all the words here
cs-410_3_5_125,cs-410,3,5,"00:09:10,182","00:09:15,681",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,then this model is forced to assign
cs-410_3_5_126,cs-410,3,5,"00:09:15,681","00:09:19,620",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,because it occurs so frequently here.
cs-410_3_5_127,cs-410,3,5,"00:09:19,620","00:09:25,100",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,Note that in order to reduce its
cs-410_3_5_128,cs-410,3,5,"00:09:25,100","00:09:31,280",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,"another model, which is this one,"
cs-410_3_5_129,cs-410,3,5,"00:09:31,280","00:09:32,218",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"And in this case,"
cs-410_3_5_130,cs-410,3,5,"00:09:32,218","00:09:37,200",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,it's not appropriate to use the background
cs-410_3_5_131,cs-410,3,5,"00:09:37,200","00:09:42,320",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,goal because this model would assign high
cs-410_3_5_132,cs-410,3,5,"00:09:43,370","00:09:46,000",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"So in this approach, then,"
cs-410_3_5_133,cs-410,3,5,"00:09:46,000","00:09:50,810",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,we assume this machine that was generating
cs-410_3_5_134,cs-410,3,5,"00:09:50,810","00:09:53,630",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,We have a source control up here.
cs-410_3_5_135,cs-410,3,5,"00:09:53,630","00:09:59,110",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,Imagine we flip a coin here to
cs-410_3_5_136,cs-410,3,5,"00:09:59,110","00:10:03,238",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"With probability of lambda,"
cs-410_3_5_137,cs-410,3,5,"00:10:03,238","00:10:05,400",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,we're going to use
cs-410_3_5_138,cs-410,3,5,"00:10:05,400","00:10:08,540",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,And we're going to do that in
cs-410_3_5_139,cs-410,3,5,"00:10:08,540","00:10:12,570",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"With probability of 1 minus lambda,"
cs-410_3_5_140,cs-410,3,5,"00:10:12,570","00:10:17,460",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,"to use a known topic model, here,"
cs-410_3_5_141,cs-410,3,5,"00:10:17,460","00:10:20,100",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,And we're going to then
cs-410_3_5_142,cs-410,3,5,"00:10:20,100","00:10:25,450",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,If we make this assumption and this whole
cs-410_3_5_143,cs-410,3,5,"00:10:25,450","00:10:30,420",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,this a mixture model because there are two
cs-410_3_5_144,cs-410,3,5,"00:10:30,420","00:10:33,940",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,And we actually don't know when
cs-410_3_5_145,cs-410,3,5,"00:10:35,770","00:10:40,320",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,"So again,"
cs-410_3_5_146,cs-410,3,5,"00:10:42,270","00:10:47,920",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,and we can still ask for words and it will
cs-410_3_5_147,cs-410,3,5,"00:10:47,920","00:10:51,920",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,"And of course, which word will show up"
cs-410_3_5_148,cs-410,3,5,"00:10:51,920","00:10:53,003",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,that distribution.
cs-410_3_5_149,cs-410,3,5,"00:10:53,003","00:10:55,780",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,"In addition,"
cs-410_3_5_150,cs-410,3,5,"00:10:55,780","00:10:58,751",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,because if you say lambda is very high and
cs-410_3_5_151,cs-410,3,5,"00:10:58,751","00:11:02,769",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,"always use the background distribution,"
cs-410_3_5_152,cs-410,3,5,"00:11:02,769","00:11:07,260",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,"Then if you say, well, lambda is"
cs-410_3_5_153,cs-410,3,5,"00:11:07,260","00:11:12,353",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,So all of these
cs-410_3_5_154,cs-410,3,5,"00:11:12,353","00:11:15,108",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,"And then if you're thinking this way,"
cs-410_3_5_155,cs-410,3,5,"00:11:15,108","00:11:19,206",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,basically we can do exactly
cs-410_3_5_156,cs-410,3,5,"00:11:19,206","00:11:23,445",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,We're going to use maximum likelihood
cs-410_3_5_157,cs-410,3,5,"00:11:23,445","00:11:25,760",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,to estimate the parameters.
cs-410_3_5_158,cs-410,3,5,"00:11:25,760","00:11:30,201",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,Basically we're going to
cs-410_3_5_159,cs-410,3,5,"00:11:30,201","00:11:33,512",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,that we can best explain all the data.
cs-410_3_5_160,cs-410,3,5,"00:11:33,512","00:11:41,200",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,The difference now is that we are not
cs-410_3_5_161,cs-410,3,5,"00:11:41,200","00:11:46,633",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,But rather we are going to ask this whole
cs-410_3_5_162,cs-410,3,5,"00:11:46,633","00:11:50,049",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,Because it has got some help
cs-410_3_5_163,cs-410,3,5,"00:11:50,049","00:11:54,080",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,it doesn't have to assign high
cs-410_3_5_164,cs-410,3,5,"00:11:54,080","00:11:58,890",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,"As a result, it will then assign higher"
cs-410_3_5_165,cs-410,3,5,"00:11:58,890","00:12:04,950",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,are common here but
cs-410_3_5_166,cs-410,3,5,"00:12:04,950","00:12:06,877",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,So those would be common here.
cs-410_3_5_167,cs-410,3,5,"00:12:11,321","00:12:14,907",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,"And if they're common, they would"
cs-410_3_5_168,cs-410,3,5,"00:12:14,907","00:12:17,661",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,according to a maximum
cs-410_3_5_169,cs-410,3,5,"00:12:17,661","00:12:23,692",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,"And if they are rare here,"
cs-410_3_5_170,cs-410,3,5,"00:12:23,692","00:12:29,620",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,much help from this background model.
cs-410_3_5_171,cs-410,3,5,"00:12:29,620","00:12:33,940",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,"As a result, this topic model"
cs-410_3_5_172,cs-410,3,5,"00:12:33,940","00:12:37,410",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,"So the high probability words,"
cs-410_3_5_173,cs-410,3,5,"00:12:37,410","00:12:41,630",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,would be those that are common here but
cs-410_3_5_174,cs-410,3,5,"00:12:43,960","00:12:48,897",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,So this is basically a little bit
cs-410_3_5_175,cs-410,3,5,"00:12:48,897","00:12:53,664",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=768,But this would allow us to achieve the
cs-410_3_5_176,cs-410,3,5,"00:12:53,664","00:12:55,770",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,are meaningless in the feedback.
cs-410_3_5_177,cs-410,3,5,"00:12:56,780","00:13:01,200",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=776,"So mathematically, what we have is"
cs-410_3_5_178,cs-410,3,5,"00:13:01,200","00:13:04,794",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=781,"local likelihood,"
cs-410_3_5_179,cs-410,3,5,"00:13:06,200","00:13:08,860",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,And note that we also have another
cs-410_3_5_180,cs-410,3,5,"00:13:08,860","00:13:13,150",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,we assume that the lambda denotes
cs-410_3_5_181,cs-410,3,5,"00:13:13,150","00:13:16,010",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=793,"So we are going to,"
cs-410_3_5_182,cs-410,3,5,"00:13:16,010","00:13:21,800",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=796,Let's say 50% of the words are noise or
cs-410_3_5_183,cs-410,3,5,"00:13:21,800","00:13:24,295",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=801,And this can then be
cs-410_3_5_184,cs-410,3,5,"00:13:24,295","00:13:30,896",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=804,"If we assume this is fixed, then we only"
cs-410_3_5_185,cs-410,3,5,"00:13:30,896","00:13:35,090",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,just like in the simple
cs-410_3_5_186,cs-410,3,5,"00:13:35,090","00:13:39,090",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,"We have n parameters,"
cs-410_3_5_187,cs-410,3,5,"00:13:39,090","00:13:41,289",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=819,And then the likelihood
cs-410_3_5_188,cs-410,3,5,"00:13:42,760","00:13:47,643",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,It's very similar to the global
cs-410_3_5_189,cs-410,3,5,"00:13:47,643","00:13:51,537",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=827,except that inside the logarithm
cs-410_3_5_190,cs-410,3,5,"00:13:51,537","00:13:57,070",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=831,And this sum is because we
cs-410_3_5_191,cs-410,3,5,"00:13:57,070","00:14:01,300",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,And which one is used would depend on
cs-410_3_5_192,cs-410,3,5,"00:14:02,460","00:14:08,790",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=842,"But mathematically, this is the function"
cs-410_3_5_193,cs-410,3,5,"00:14:08,790","00:14:10,510",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=848,So this is just a function.
cs-410_3_5_194,cs-410,3,5,"00:14:10,510","00:14:13,620",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,All the other values are known except for
cs-410_3_5_195,cs-410,3,5,"00:14:15,010","00:14:19,834",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,So we can then choose this
cs-410_3_5_196,cs-410,3,5,"00:14:19,834","00:14:21,531",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,"this log likelihood,"
cs-410_3_5_197,cs-410,3,5,"00:14:21,531","00:14:27,357",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=861,the same idea as the maximum likelihood
cs-410_3_5_198,cs-410,3,5,"00:14:27,357","00:14:30,060",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=867,We just have to solve this
cs-410_3_5_199,cs-410,3,5,"00:14:30,060","00:14:34,460",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,We essentially would try all
cs-410_3_5_200,cs-410,3,5,"00:14:34,460","00:14:37,670",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=874,that gives this whole thing
cs-410_3_5_201,cs-410,3,5,"00:14:37,670","00:14:39,210",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,So it's a well-defined math problem.
cs-410_3_5_202,cs-410,3,5,"00:14:40,900","00:14:45,720",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=880,"Once we have done that, we obtain this"
cs-410_3_5_203,cs-410,3,5,"00:14:45,720","00:14:47,812",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=885,original query model to the feedback.
cs-410_3_5_204,cs-410,3,5,"00:14:50,980","00:14:55,963",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,So here are some examples of
cs-410_3_5_205,cs-410,3,5,"00:14:55,963","00:14:57,817",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=895,document collection.
cs-410_3_5_206,cs-410,3,5,"00:14:57,817","00:15:01,673",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=897,And we do pseudo-feedback we just
cs-410_3_5_207,cs-410,3,5,"00:15:01,673","00:15:03,750",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=901,we use this mixture model.
cs-410_3_5_208,cs-410,3,5,"00:15:03,750","00:15:06,090",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=903,So the query is airport security.
cs-410_3_5_209,cs-410,3,5,"00:15:06,090","00:15:11,480",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=906,What we do is we first retrieve ten
cs-410_3_5_210,cs-410,3,5,"00:15:11,480","00:15:14,520",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=911,this is of course pseudo-feedback.
cs-410_3_5_211,cs-410,3,5,"00:15:14,520","00:15:20,000",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=914,And then we're going to feed that
cs-410_3_5_212,cs-410,3,5,"00:15:21,130","00:15:25,770",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=921,And these are the words
cs-410_3_5_213,cs-410,3,5,"00:15:25,770","00:15:30,220",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=925,This is the probability of a word given
cs-410_3_5_214,cs-410,3,5,"00:15:31,600","00:15:34,350",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=931,So in both cases you can see the highest
cs-410_3_5_215,cs-410,3,5,"00:15:34,350","00:15:38,480",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=934,probability words include the very
cs-410_3_5_216,cs-410,3,5,"00:15:38,480","00:15:40,208",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=938,"So airport security, for example,"
cs-410_3_5_217,cs-410,3,5,"00:15:40,208","00:15:45,450",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=940,these query words still show up as high
cs-410_3_5_218,cs-410,3,5,"00:15:45,450","00:15:48,850",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=945,because they occur frequently
cs-410_3_5_219,cs-410,3,5,"00:15:48,850","00:15:53,830",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,"But we also see beverage,"
cs-410_3_5_220,cs-410,3,5,"00:15:53,830","00:15:59,436",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,"So these are relevant to this topic,"
cs-410_3_5_221,cs-410,3,5,"00:15:59,436","00:16:05,280",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=959,"if combined with original query, can help"
cs-410_3_5_222,cs-410,3,5,"00:16:05,280","00:16:11,200",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=965,And also they can help us bring up
cs-410_3_5_223,cs-410,3,5,"00:16:11,200","00:16:16,980",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,"these other words, maybe, for example,"
cs-410_3_5_224,cs-410,3,5,"00:16:18,070","00:16:20,680",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=978,So this is how pseudo-feedback works.
cs-410_3_5_225,cs-410,3,5,"00:16:20,680","00:16:26,790",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,It shows that this model really works and
cs-410_3_5_226,cs-410,3,5,"00:16:26,790","00:16:31,546",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,What's also interesting is that if
cs-410_3_5_227,cs-410,3,5,"00:16:31,546","00:16:35,154",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=991,"you compare them,"
cs-410_3_5_228,cs-410,3,5,"00:16:35,154","00:16:40,415",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=995,"when lambda is set to a small value,"
cs-410_3_5_229,cs-410,3,5,"00:16:40,415","00:16:45,473",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,"And that means, well,"
cs-410_3_5_230,cs-410,3,5,"00:16:45,473","00:16:48,575",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,"Remember, lambda confuses the probability"
cs-410_3_5_231,cs-410,3,5,"00:16:48,575","00:16:50,925",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1008,to generate the text.
cs-410_3_5_232,cs-410,3,5,"00:16:50,925","00:16:53,245",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1010,"If we don't rely much on background model,"
cs-410_3_5_233,cs-410,3,5,"00:16:53,245","00:16:58,100",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,we still have to use this topic model
cs-410_3_5_234,cs-410,3,5,"00:16:58,100","00:17:01,340",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1018,Whereas if we set lambda
cs-410_3_5_235,cs-410,3,5,"00:17:01,340","00:17:05,550",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1021,we will use the background model
cs-410_3_5_236,cs-410,3,5,"00:17:05,550","00:17:08,930",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1025,Then there's no burden on
cs-410_3_5_237,cs-410,3,5,"00:17:08,930","00:17:11,790",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1028,in the feedback documents
cs-410_3_5_238,cs-410,3,5,"00:17:11,790","00:17:17,430",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1031,"So as a result, the topic model"
cs-410_3_5_239,cs-410,3,5,"00:17:17,430","00:17:20,060",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1037,It contains all the relevant
cs-410_3_5_240,cs-410,3,5,"00:17:21,260","00:17:26,100",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1041,So this can be added to the original
cs-410_3_5_241,cs-410,3,5,"00:17:28,140","00:17:29,900",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1048,"So to summarize,"
cs-410_3_5_242,cs-410,3,5,"00:17:29,900","00:17:34,470",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1049,in this lecture we have talked about
cs-410_3_5_243,cs-410,3,5,"00:17:34,470","00:17:38,290",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1054,"In general,"
cs-410_3_5_244,cs-410,3,5,"00:17:38,290","00:17:43,610",244,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1058,"These examples can be assumed examples,"
cs-410_3_5_245,cs-410,3,5,"00:17:43,610","00:17:48,770",245,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1063,like assume the top ten documents
cs-410_3_5_246,cs-410,3,5,"00:17:48,770","00:17:51,419",246,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1068,"They could be based on user interactions,"
cs-410_3_5_247,cs-410,3,5,"00:17:51,419","00:17:55,260",247,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1071,like feedback based on clickthroughs or
cs-410_3_5_248,cs-410,3,5,"00:17:55,260","00:17:59,308",248,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1075,We talked about the three major
cs-410_3_5_249,cs-410,3,5,"00:17:59,308","00:18:01,657",249,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1079,"pseudo feedback, and implicit feedback."
cs-410_3_5_250,cs-410,3,5,"00:18:01,657","00:18:08,108",250,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1081,We talked about how to use Rocchio to
cs-410_3_5_251,cs-410,3,5,"00:18:08,108","00:18:14,047",251,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1088,how to use query model estimation for
cs-410_3_5_252,cs-410,3,5,"00:18:14,047","00:18:18,350",252,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1094,And we briefly talked about
cs-410_3_5_253,cs-410,3,5,"00:18:19,790","00:18:21,650",253,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1099,There are many other methods.
cs-410_3_5_254,cs-410,3,5,"00:18:21,650","00:18:22,170",254,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1101,"For example,"
cs-410_3_5_255,cs-410,3,5,"00:18:22,170","00:18:26,990",255,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1102,the relevance model is a very effective
cs-410_3_5_256,cs-410,3,5,"00:18:26,990","00:18:31,130",256,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1106,So you can read more about these
cs-410_3_5_257,cs-410,3,5,"00:18:32,170","00:18:36,200",257,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1112,are listed at the end of this lecture.
cs-410_3_5_258,cs-410,3,5,"00:18:36,200","00:18:38,420",258,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1116,So there are two additional readings here.
cs-410_3_5_259,cs-410,3,5,"00:18:38,420","00:18:42,047",259,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1118,The first one is a book that
cs-410_3_5_260,cs-410,3,5,"00:18:42,047","00:18:46,170",260,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1122,discussion of language models for
cs-410_3_5_261,cs-410,3,5,"00:18:46,170","00:18:49,745",261,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1126,And the second one is a important research
cs-410_3_5_262,cs-410,3,5,"00:18:49,745","00:18:54,549",262,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1129,paper that's about relevance
cs-410_3_5_263,cs-410,3,5,"00:18:54,549","00:18:59,471",263,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1134,and it's a very effective way
cs-410_3_5_264,cs-410,3,5,"00:18:59,471","00:19:09,471",264,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1139,[MUSIC]
cs-410_3_6_1,cs-410,3,6,"00:00:00,008","00:00:07,386",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_6_2,cs-410,3,6,"00:00:07,386","00:00:11,551",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,more of the Munster learning algorithms
cs-410_3_6_3,cs-410,3,6,"00:00:11,551","00:00:15,009",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,they generally attempt to direct
cs-410_3_6_4,cs-410,3,6,"00:00:16,690","00:00:18,010",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,Like a MAP or nDCG.
cs-410_3_6_5,cs-410,3,6,"00:00:19,020","00:00:24,640",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,Note that the optimization object or
cs-410_3_6_6,cs-410,3,6,"00:00:24,640","00:00:29,610",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,on the previous slide is not directly
cs-410_3_6_7,cs-410,3,6,"00:00:31,390","00:00:33,870",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,By maximizing the prediction of one or
cs-410_3_6_8,cs-410,3,6,"00:00:33,870","00:00:39,430",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"zero, we don't necessarily optimize"
cs-410_3_6_9,cs-410,3,6,"00:00:39,430","00:00:44,106",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,One can imagine that our
cs-410_3_6_10,cs-410,3,6,"00:00:44,106","00:00:46,626",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,And let's say both are around 0.5.
cs-410_3_6_11,cs-410,3,6,"00:00:46,626","00:00:51,230",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,So it's kind of in the middle of zero and
cs-410_3_6_12,cs-410,3,6,"00:00:51,230","00:00:58,250",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"But the ranking can be wrong, so we might"
cs-410_3_6_13,cs-410,3,6,"00:01:00,750","00:01:04,235",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,So that won't be good from
cs-410_3_6_14,cs-410,3,6,"00:01:04,235","00:01:07,420",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,"even though function, it's not bad."
cs-410_3_6_15,cs-410,3,6,"00:01:07,420","00:01:11,773",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,"In contrast, we might have another"
cs-410_3_6_16,cs-410,3,6,"00:01:11,773","00:01:14,000",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,"around the 0.9, it said."
cs-410_3_6_17,cs-410,3,6,"00:01:14,000","00:01:17,580",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,"And by the objective function,"
cs-410_3_6_18,cs-410,3,6,"00:01:17,580","00:01:20,500",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,But if we didn't get the order
cs-410_3_6_19,cs-410,3,6,"00:01:20,500","00:01:22,970",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,that's actually a better result.
cs-410_3_6_20,cs-410,3,6,"00:01:22,970","00:01:28,070",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,"So these new, more advanced approaches"
cs-410_3_6_21,cs-410,3,6,"00:01:28,070","00:01:32,120",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"Of course, then the challenge is"
cs-410_3_6_22,cs-410,3,6,"00:01:32,120","00:01:33,700",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,be harder to solve.
cs-410_3_6_23,cs-410,3,6,"00:01:33,700","00:01:39,143",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"And then, researchers have posed"
cs-410_3_6_24,cs-410,3,6,"00:01:39,143","00:01:46,153",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,and you can read more of the references at
cs-410_3_6_25,cs-410,3,6,"00:01:46,153","00:01:50,540",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"Now, these learning ranked"
cs-410_3_6_26,cs-410,3,6,"00:01:50,540","00:01:53,530",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,So there accounts would be be applied
cs-410_3_6_27,cs-410,3,6,"00:01:53,530","00:01:55,350",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,not just the retrieval problem.
cs-410_3_6_28,cs-410,3,6,"00:01:55,350","00:01:58,993",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,So some people will go
cs-410_3_6_29,cs-410,3,6,"00:01:58,993","00:02:02,810",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,"computational advertising,"
cs-410_3_6_30,cs-410,3,6,"00:02:02,810","00:02:08,636",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,there are many others that you can
cs-410_3_6_31,cs-410,3,6,"00:02:11,157","00:02:15,884",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,To summarize this lecture we
cs-410_3_6_32,cs-410,3,6,"00:02:15,884","00:02:21,270",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,learning to combine much more
cs-410_3_6_33,cs-410,3,6,"00:02:22,780","00:02:24,690",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,Actually the use of machine learning
cs-410_3_6_34,cs-410,3,6,"00:02:25,810","00:02:29,840",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,in information retrieval has
cs-410_3_6_35,cs-410,3,6,"00:02:29,840","00:02:35,212",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,"So for example, the Rocchio feedback"
cs-410_3_6_36,cs-410,3,6,"00:02:35,212","00:02:40,700",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,was a machine learning approach
cs-410_3_6_37,cs-410,3,6,"00:02:40,700","00:02:46,750",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,But the most recent use of machine
cs-410_3_6_38,cs-410,3,6,"00:02:46,750","00:02:51,000",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,changes in the environment of
cs-410_3_6_39,cs-410,3,6,"00:02:52,550","00:02:58,650",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"First, it's mostly freedom of"
cs-410_3_6_40,cs-410,3,6,"00:02:58,650","00:03:04,250",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"in the form of critical, such as"
cs-410_3_6_41,cs-410,3,6,"00:03:04,250","00:03:11,106",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,So the data can provide a lot of
cs-410_3_6_42,cs-410,3,6,"00:03:11,106","00:03:17,487",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,machine learning methods can be
cs-410_3_6_43,cs-410,3,6,"00:03:17,487","00:03:21,744",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,"Secondly, it's also freedom by"
cs-410_3_6_44,cs-410,3,6,"00:03:21,744","00:03:24,464",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,and this is not only just
cs-410_3_6_45,cs-410,3,6,"00:03:24,464","00:03:29,840",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,features available on the web that can
cs-410_3_6_46,cs-410,3,6,"00:03:29,840","00:03:36,208",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"It's also because by combining them,"
cs-410_3_6_47,cs-410,3,6,"00:03:36,208","00:03:41,168",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,"of ranking, so this is desired for"
cs-410_3_6_48,cs-410,3,6,"00:03:41,168","00:03:45,887",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,Modern search engines all use some
cs-410_3_6_49,cs-410,3,6,"00:03:45,887","00:03:48,855",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,combine many features
cs-410_3_6_50,cs-410,3,6,"00:03:48,855","00:03:53,590",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,this is a major feature of these
cs-410_3_6_51,cs-410,3,6,"00:03:56,190","00:04:02,368",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,The topic of learning to rank is still
cs-410_3_6_52,cs-410,3,6,"00:04:02,368","00:04:08,265",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,and so we can expect to see new results
cs-410_3_6_53,cs-410,3,6,"00:04:08,265","00:04:09,119",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,perhaps.
cs-410_3_6_54,cs-410,3,6,"00:04:12,753","00:04:17,686",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,Here are some additional readings
cs-410_3_6_55,cs-410,3,6,"00:04:17,686","00:04:22,544",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,about how learning to rank at works and
cs-410_3_6_56,cs-410,3,6,"00:04:25,281","00:04:35,281",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,[MUSIC]
cs-410_3_7_1,cs-410,3,7,"00:00:00,300","00:00:03,380",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_7_2,cs-410,3,7,"00:00:09,170","00:00:13,893",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,This lecture is about natural language
cs-410_3_7_3,cs-410,3,7,"00:00:13,893","00:00:16,330",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,content analysis.
cs-410_3_7_4,cs-410,3,7,"00:00:16,330","00:00:21,510",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,Natural language content analysis
cs-410_3_7_5,cs-410,3,7,"00:00:21,510","00:00:23,780",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,So we're going to first talk about this.
cs-410_3_7_6,cs-410,3,7,"00:00:24,980","00:00:26,330",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"And in particular,"
cs-410_3_7_7,cs-410,3,7,"00:00:26,330","00:00:31,540",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,natural language processing with
cs-410_3_7_8,cs-410,3,7,"00:00:33,210","00:00:38,230",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,And this determines what algorithms can
cs-410_3_7_9,cs-410,3,7,"00:00:40,820","00:00:44,991",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,We're going to take a look at the basic
cs-410_3_7_10,cs-410,3,7,"00:00:46,330","00:00:48,970",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,And I'm going to explain these concepts
cs-410_3_7_11,cs-410,3,7,"00:00:48,970","00:00:52,600",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,using a similar example
cs-410_3_7_12,cs-410,3,7,"00:00:52,600","00:00:55,650",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,A dog is chasing a boy on the playground.
cs-410_3_7_13,cs-410,3,7,"00:00:55,650","00:00:58,310",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,Now this is a very simple sentence.
cs-410_3_7_14,cs-410,3,7,"00:00:58,310","00:01:01,160",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,When we read such a sentence
cs-410_3_7_15,cs-410,3,7,"00:01:01,160","00:01:05,200",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,about it to get the meaning of it.
cs-410_3_7_16,cs-410,3,7,"00:01:05,200","00:01:09,460",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,But when a computer has to
cs-410_3_7_17,cs-410,3,7,"00:01:09,460","00:01:12,340",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,the computer has to go
cs-410_3_7_18,cs-410,3,7,"00:01:13,430","00:01:16,532",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"First, the computer needs"
cs-410_3_7_19,cs-410,3,7,"00:01:16,532","00:01:18,630",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,how to segment the words in English.
cs-410_3_7_20,cs-410,3,7,"00:01:18,630","00:01:22,010",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"And this is very easy,"
cs-410_3_7_21,cs-410,3,7,"00:01:22,010","00:01:26,136",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,And then the computer will need
cs-410_3_7_22,cs-410,3,7,"00:01:26,136","00:01:27,870",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,syntactical categories.
cs-410_3_7_23,cs-410,3,7,"00:01:27,870","00:01:34,510",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"So for example, dog is a noun,"
cs-410_3_7_24,cs-410,3,7,"00:01:34,510","00:01:37,350",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,And this is called a Lexical analysis.
cs-410_3_7_25,cs-410,3,7,"00:01:37,350","00:01:41,590",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"In particular, tagging these words"
cs-410_3_7_26,cs-410,3,7,"00:01:41,590","00:01:43,270",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,is called a part-of-speech tagging.
cs-410_3_7_27,cs-410,3,7,"00:01:45,030","00:01:48,383",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,After that the computer also needs to
cs-410_3_7_28,cs-410,3,7,"00:01:48,383","00:01:49,040",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,these words.
cs-410_3_7_29,cs-410,3,7,"00:01:49,040","00:01:53,300",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,So a and dog would form a noun phrase.
cs-410_3_7_30,cs-410,3,7,"00:01:53,300","00:01:57,590",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,On the playground would be
cs-410_3_7_31,cs-410,3,7,"00:01:57,590","00:02:01,378",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,And there is certain way for
cs-410_3_7_32,cs-410,3,7,"00:02:01,378","00:02:03,620",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,them to create meaning.
cs-410_3_7_33,cs-410,3,7,"00:02:03,620","00:02:06,469",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,Some other combinations
cs-410_3_7_34,cs-410,3,7,"00:02:07,720","00:02:12,620",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"And this is called syntactical parsing, or"
cs-410_3_7_35,cs-410,3,7,"00:02:12,620","00:02:17,090",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,"syntactical analysis,"
cs-410_3_7_36,cs-410,3,7,"00:02:17,090","00:02:21,180",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,The outcome is a parse tree
cs-410_3_7_37,cs-410,3,7,"00:02:21,180","00:02:24,050",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,That tells us the structure
cs-410_3_7_38,cs-410,3,7,"00:02:24,050","00:02:27,430",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,that we know how we can
cs-410_3_7_39,cs-410,3,7,"00:02:27,430","00:02:29,740",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,But this is not semantics yet.
cs-410_3_7_40,cs-410,3,7,"00:02:29,740","00:02:34,530",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,So in order to get the meaning we
cs-410_3_7_41,cs-410,3,7,"00:02:34,530","00:02:39,860",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,these structures into some real world
cs-410_3_7_42,cs-410,3,7,"00:02:39,860","00:02:45,500",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,"So dog is a concept that we know,"
cs-410_3_7_43,cs-410,3,7,"00:02:45,500","00:02:50,870",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,So connecting these phrases
cs-410_3_7_44,cs-410,3,7,"00:02:52,160","00:02:58,788",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"Now for a computer, would have to formally"
cs-410_3_7_45,cs-410,3,7,"00:02:58,788","00:03:03,630",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"So dog, d1 means d1 is a dog."
cs-410_3_7_46,cs-410,3,7,"00:03:04,690","00:03:09,420",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,"Boy, b1 means b1 refers to a boy etc."
cs-410_3_7_47,cs-410,3,7,"00:03:09,420","00:03:13,430",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,And also represents the chasing
cs-410_3_7_48,cs-410,3,7,"00:03:13,430","00:03:18,334",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,"So, chasing is a predicate here with"
cs-410_3_7_49,cs-410,3,7,"00:03:18,334","00:03:23,720",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"three arguments, d1, b1, and p1."
cs-410_3_7_50,cs-410,3,7,"00:03:23,720","00:03:25,920",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,Which is playground.
cs-410_3_7_51,cs-410,3,7,"00:03:25,920","00:03:31,320",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,So this formal rendition of
cs-410_3_7_52,cs-410,3,7,"00:03:31,320","00:03:35,950",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"Once we reach that level of understanding,"
cs-410_3_7_53,cs-410,3,7,"00:03:35,950","00:03:42,050",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"For example, if we assume there's a rule"
cs-410_3_7_54,cs-410,3,7,"00:03:42,050","00:03:48,420",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,"the person can get scared, then we"
cs-410_3_7_55,cs-410,3,7,"00:03:48,420","00:03:52,800",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"This is the inferred meaning,"
cs-410_3_7_56,cs-410,3,7,"00:03:52,800","00:03:58,485",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"And finally, we might even further infer"
cs-410_3_7_57,cs-410,3,7,"00:03:58,485","00:04:06,170",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,"what this sentence is requesting,"
cs-410_3_7_58,cs-410,3,7,"00:04:06,170","00:04:12,920",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,or why the person who say it in
cs-410_3_7_59,cs-410,3,7,"00:04:12,920","00:04:18,310",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,"And so, this has to do with"
cs-410_3_7_60,cs-410,3,7,"00:04:18,310","00:04:24,550",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,This is called speech act analysis or
cs-410_3_7_61,cs-410,3,7,"00:04:24,550","00:04:27,920",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,Which first to the use of language.
cs-410_3_7_62,cs-410,3,7,"00:04:27,920","00:04:32,704",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,"So, in this case a person saying this"
cs-410_3_7_63,cs-410,3,7,"00:04:32,704","00:04:34,070",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,bring back the dog.
cs-410_3_7_64,cs-410,3,7,"00:04:35,240","00:04:42,320",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,"So this means when saying a sentence,"
cs-410_3_7_65,cs-410,3,7,"00:04:42,320","00:04:44,769",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,So the action here is to make a request.
cs-410_3_7_66,cs-410,3,7,"00:04:46,770","00:04:51,408",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,"Now, this slide clearly shows that"
cs-410_3_7_67,cs-410,3,7,"00:04:51,408","00:04:55,720",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,a sentence there are a lot of
cs-410_3_7_68,cs-410,3,7,"00:04:55,720","00:05:00,337",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"Now, in general it's very hard for"
cs-410_3_7_69,cs-410,3,7,"00:05:00,337","00:05:04,910",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,especially if you would want
cs-410_3_7_70,cs-410,3,7,"00:05:04,910","00:05:06,450",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,This is very difficult.
cs-410_3_7_71,cs-410,3,7,"00:05:08,190","00:05:11,094",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"Now, the main reason why natural"
cs-410_3_7_72,cs-410,3,7,"00:05:11,094","00:05:14,820",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,it's because it's designed it will
cs-410_3_7_73,cs-410,3,7,"00:05:15,990","00:05:20,040",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"As a result, for example,"
cs-410_3_7_74,cs-410,3,7,"00:05:21,250","00:05:25,150",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,Because we assume all of
cs-410_3_7_75,cs-410,3,7,"00:05:25,150","00:05:28,170",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,there's no need to encode this knowledge.
cs-410_3_7_76,cs-410,3,7,"00:05:29,780","00:05:31,360",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,That makes communication efficient.
cs-410_3_7_77,cs-410,3,7,"00:05:32,480","00:05:37,460",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,"We also keep a lot of ambiguities,"
cs-410_3_7_78,cs-410,3,7,"00:05:39,090","00:05:45,130",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,"And this is again, because we assume we"
cs-410_3_7_79,cs-410,3,7,"00:05:45,130","00:05:48,800",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"So, there's no problem with"
cs-410_3_7_80,cs-410,3,7,"00:05:48,800","00:05:50,869",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,possibly different things
cs-410_3_7_81,cs-410,3,7,"00:05:52,610","00:05:55,880",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,Yet for
cs-410_3_7_82,cs-410,3,7,"00:05:55,880","00:06:00,250",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,because a computer does not have
cs-410_3_7_83,cs-410,3,7,"00:06:00,250","00:06:03,620",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,So the computer will be confused indeed.
cs-410_3_7_84,cs-410,3,7,"00:06:03,620","00:06:06,980",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,And this makes it hard for
cs-410_3_7_85,cs-410,3,7,"00:06:06,980","00:06:09,440",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,"Indeed, it makes it very hard for"
cs-410_3_7_86,cs-410,3,7,"00:06:09,440","00:06:15,140",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,every step in the slide
cs-410_3_7_87,cs-410,3,7,"00:06:16,550","00:06:19,380",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,Ambiguity is a main killer.
cs-410_3_7_88,cs-410,3,7,"00:06:19,380","00:06:22,820",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,Meaning that in every step
cs-410_3_7_89,cs-410,3,7,"00:06:22,820","00:06:26,790",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,and the computer would have to
cs-410_3_7_90,cs-410,3,7,"00:06:26,790","00:06:30,550",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,that decision can be very difficult
cs-410_3_7_91,cs-410,3,7,"00:06:31,690","00:06:32,300",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,"And in general,"
cs-410_3_7_92,cs-410,3,7,"00:06:32,300","00:06:37,530",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,we need common sense reasoning in order
cs-410_3_7_93,cs-410,3,7,"00:06:37,530","00:06:40,595",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,And computers today don't yet have that.
cs-410_3_7_94,cs-410,3,7,"00:06:40,595","00:06:42,820",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,That's why it's very hard for
cs-410_3_7_95,cs-410,3,7,"00:06:42,820","00:06:47,310",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,computers to precisely understand
cs-410_3_7_96,cs-410,3,7,"00:06:48,310","00:06:51,280",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,So here are some specific
cs-410_3_7_97,cs-410,3,7,"00:06:51,280","00:06:53,390",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,Think about the world-level ambiguity.
cs-410_3_7_98,cs-410,3,7,"00:06:53,390","00:06:56,940",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,A word like design can be a noun or
cs-410_3_7_99,cs-410,3,7,"00:06:56,940","00:06:59,200",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,we've got ambiguous part of speech tag.
cs-410_3_7_100,cs-410,3,7,"00:07:00,980","00:07:06,190",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"Root also has multiple meanings,"
cs-410_3_7_101,cs-410,3,7,"00:07:06,190","00:07:10,670",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,"like in the square of, or"
cs-410_3_7_102,cs-410,3,7,"00:07:12,310","00:07:17,310",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,Syntactic ambiguity refers
cs-410_3_7_103,cs-410,3,7,"00:07:19,440","00:07:21,670",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,of a sentence in terms structures.
cs-410_3_7_104,cs-410,3,7,"00:07:21,670","00:07:23,010",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,"So for example,"
cs-410_3_7_105,cs-410,3,7,"00:07:23,010","00:07:26,219",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,natural language processing can
cs-410_3_7_106,cs-410,3,7,"00:07:28,240","00:07:33,410",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,So one is the ordinary meaning that we
cs-410_3_7_107,cs-410,3,7,"00:07:33,410","00:07:38,690",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,will be getting as we're
cs-410_3_7_108,cs-410,3,7,"00:07:38,690","00:07:41,670",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"So, it's processing of natural language."
cs-410_3_7_109,cs-410,3,7,"00:07:41,670","00:07:44,600",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,But there's is also another
cs-410_3_7_110,cs-410,3,7,"00:07:44,600","00:07:47,190",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,which is to say language
cs-410_3_7_111,cs-410,3,7,"00:07:48,950","00:07:53,500",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,"Now we don't generally have this problem,"
cs-410_3_7_112,cs-410,3,7,"00:07:53,500","00:07:56,960",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"the structure, the computer would have"
cs-410_3_7_113,cs-410,3,7,"00:07:59,040","00:08:03,530",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,Another classic example is a man
cs-410_3_7_114,cs-410,3,7,"00:08:03,530","00:08:10,230",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,And this ambiguity lies in
cs-410_3_7_115,cs-410,3,7,"00:08:10,230","00:08:13,630",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,This is called a prepositional
cs-410_3_7_116,cs-410,3,7,"00:08:14,960","00:08:20,440",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,Meaning where to attach this
cs-410_3_7_117,cs-410,3,7,"00:08:20,440","00:08:22,670",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,Should it modify the boy?
cs-410_3_7_118,cs-410,3,7,"00:08:22,670","00:08:28,330",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,"Or should it be modifying, saw, the verb."
cs-410_3_7_119,cs-410,3,7,"00:08:28,330","00:08:31,330",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,Another problem is anaphora resolution.
cs-410_3_7_120,cs-410,3,7,"00:08:31,330","00:08:35,740",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,In John persuaded Bill to buy a TV for
cs-410_3_7_121,cs-410,3,7,"00:08:35,740","00:08:37,960",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,Does himself refer to John or Bill?
cs-410_3_7_122,cs-410,3,7,"00:08:39,380","00:08:41,790",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,Presupposition is another difficulty.
cs-410_3_7_123,cs-410,3,7,"00:08:41,790","00:08:45,459",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,He has quit smoking implies
cs-410_3_7_124,cs-410,3,7,"00:08:45,459","00:08:50,180",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,we need to have such a knowledge in
cs-410_3_7_125,cs-410,3,7,"00:08:52,630","00:08:57,614",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,"Because of these problems, the state"
cs-410_3_7_126,cs-410,3,7,"00:08:57,614","00:09:01,410",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,techniques can not do anything perfectly.
cs-410_3_7_127,cs-410,3,7,"00:09:01,410","00:09:04,560",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,Even for
cs-410_3_7_128,cs-410,3,7,"00:09:04,560","00:09:07,700",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,we still can not solve the whole problem.
cs-410_3_7_129,cs-410,3,7,"00:09:07,700","00:09:12,930",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,"The accuracy that are listed here,"
cs-410_3_7_130,cs-410,3,7,"00:09:12,930","00:09:16,100",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,was just taken from some studies earlier.
cs-410_3_7_131,cs-410,3,7,"00:09:17,330","00:09:22,840",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,And these studies obviously have to
cs-410_3_7_132,cs-410,3,7,"00:09:22,840","00:09:27,640",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,the numbers here are not
cs-410_3_7_133,cs-410,3,7,"00:09:27,640","00:09:33,210",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,take it out of the context of the data
cs-410_3_7_134,cs-410,3,7,"00:09:33,210","00:09:39,350",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,But I show these numbers mainly to give
cs-410_3_7_135,cs-410,3,7,"00:09:39,350","00:09:42,080",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,or how well we can do things like this.
cs-410_3_7_136,cs-410,3,7,"00:09:42,080","00:09:47,670",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,It doesn't mean any data set
cs-410_3_7_137,cs-410,3,7,"00:09:47,670","00:09:52,780",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,"But, in general, we can do parsing speech"
cs-410_3_7_138,cs-410,3,7,"00:09:53,980","00:09:59,030",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,"Parsing would be more difficult, but for"
cs-410_3_7_139,cs-410,3,7,"00:09:59,030","00:10:04,870",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"phrases correct, we can probably"
cs-410_3_7_140,cs-410,3,7,"00:10:06,920","00:10:12,330",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,But to get the complete parse tree
cs-410_3_7_141,cs-410,3,7,"00:10:13,610","00:10:18,210",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,"For semantic analysis, we can also do"
cs-410_3_7_142,cs-410,3,7,"00:10:18,210","00:10:22,570",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,"particularly, extraction of entities and"
cs-410_3_7_143,cs-410,3,7,"00:10:22,570","00:10:27,910",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,"For example, recognizing this is"
cs-410_3_7_144,cs-410,3,7,"00:10:27,910","00:10:33,380",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,this person and
cs-410_3_7_145,cs-410,3,7,"00:10:33,380","00:10:36,470",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,We can also do word sense to some extent.
cs-410_3_7_146,cs-410,3,7,"00:10:38,000","00:10:45,360",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,The occurrence of root in this sentence
cs-410_3_7_147,cs-410,3,7,"00:10:45,360","00:10:49,330",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,Sentiment analysis is another aspect
cs-410_3_7_148,cs-410,3,7,"00:10:50,480","00:10:55,840",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,That means we can tag the senses
cs-410_3_7_149,cs-410,3,7,"00:10:55,840","00:11:00,670",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,it's talking about the product or
cs-410_3_7_150,cs-410,3,7,"00:11:02,790","00:11:08,600",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,"Inference, however, is very hard,"
cs-410_3_7_151,cs-410,3,7,"00:11:08,600","00:11:14,040",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,any big domain and if it's only
cs-410_3_7_152,cs-410,3,7,"00:11:14,040","00:11:18,800",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,And that's a generally difficult
cs-410_3_7_153,cs-410,3,7,"00:11:18,800","00:11:21,961",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,Speech act analysis is
cs-410_3_7_154,cs-410,3,7,"00:11:21,961","00:11:26,480",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,we can only do this probably for
cs-410_3_7_155,cs-410,3,7,"00:11:26,480","00:11:32,090",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=686,And with a lot of help from humans
cs-410_3_7_156,cs-410,3,7,"00:11:32,090","00:11:34,180",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,the computers to learn from.
cs-410_3_7_157,cs-410,3,7,"00:11:36,380","00:11:38,890",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,So the slide also shows that
cs-410_3_7_158,cs-410,3,7,"00:11:38,890","00:11:44,300",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,computers are far from being able to
cs-410_3_7_159,cs-410,3,7,"00:11:44,300","00:11:50,320",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,And that also explains why the text
cs-410_3_7_160,cs-410,3,7,"00:11:50,320","00:11:54,390",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,Because we cannot rely on
cs-410_3_7_161,cs-410,3,7,"00:11:54,390","00:11:58,940",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,computational methods to
cs-410_3_7_162,cs-410,3,7,"00:11:58,940","00:12:04,770",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,"Therefore, we have to use"
cs-410_3_7_163,cs-410,3,7,"00:12:04,770","00:12:10,090",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,A particular statistical machine learning
cs-410_3_7_164,cs-410,3,7,"00:12:10,090","00:12:16,092",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,to try to get as much meaning
cs-410_3_7_165,cs-410,3,7,"00:12:16,092","00:12:19,320",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,"And, later you will see"
cs-410_3_7_166,cs-410,3,7,"00:12:20,360","00:12:25,450",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,many such algorithms
cs-410_3_7_167,cs-410,3,7,"00:12:25,450","00:12:30,790",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,interesting model from text even though
cs-410_3_7_168,cs-410,3,7,"00:12:30,790","00:12:36,010",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=750,Meaning of all the natural
cs-410_3_7_169,cs-410,3,7,"00:12:36,010","00:12:46,010",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,[MUSIC]
cs-410_3_8_1,cs-410,3,8,"00:00:00,025","00:00:07,457",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND].
cs-410_3_8_2,cs-410,3,8,"00:00:07,457","00:00:11,800",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about the syntagmatic
cs-410_3_8_3,cs-410,3,8,"00:00:13,400","00:00:18,196",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,In this lecture we are going to continue
cs-410_3_8_4,cs-410,3,8,"00:00:18,196","00:00:20,850",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"In particular,"
cs-410_3_8_5,cs-410,3,8,"00:00:20,850","00:00:24,880",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,"the concept in the information series,"
cs-410_3_8_6,cs-410,3,8,"00:00:24,880","00:00:28,760",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,how it can be used to discover
cs-410_3_8_7,cs-410,3,8,"00:00:28,760","00:00:32,880",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,Before we talked about the problem
cs-410_3_8_8,cs-410,3,8,"00:00:32,880","00:00:38,014",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,that is the conditional entropy
cs-410_3_8_9,cs-410,3,8,"00:00:38,014","00:00:42,600",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,"It is not really comparable, so"
cs-410_3_8_10,cs-410,3,8,"00:00:42,600","00:00:48,360",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,strong synagmatic relations
cs-410_3_8_11,cs-410,3,8,"00:00:48,360","00:00:53,050",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,So now we are going to introduce mutual
cs-410_3_8_12,cs-410,3,8,"00:00:53,050","00:00:57,370",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,in the information series
cs-410_3_8_13,cs-410,3,8,"00:00:57,370","00:01:03,460",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,normalize the conditional entropy to make
cs-410_3_8_14,cs-410,3,8,"00:01:04,930","00:01:10,090",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,"In particular, mutual information"
cs-410_3_8_15,cs-410,3,8,"00:01:10,090","00:01:17,380",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,matches the entropy reduction
cs-410_3_8_16,cs-410,3,8,"00:01:17,380","00:01:22,270",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,More specifically the question we
cs-410_3_8_17,cs-410,3,8,"00:01:22,270","00:01:25,463",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,of an entropy of X can
cs-410_3_8_18,cs-410,3,8,"00:01:27,220","00:01:31,940",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,So mathematically it can be
cs-410_3_8_19,cs-410,3,8,"00:01:31,940","00:01:36,670",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"the original entropy of X, and"
cs-410_3_8_20,cs-410,3,8,"00:01:37,970","00:01:42,730",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"And you might see,"
cs-410_3_8_21,cs-410,3,8,"00:01:42,730","00:01:47,790",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,as reduction of entropy of
cs-410_3_8_22,cs-410,3,8,"00:01:48,930","00:01:54,070",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,Now normally the two conditional
cs-410_3_8_23,cs-410,3,8,"00:01:54,070","00:01:58,240",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,"the entropy of Y given X are not equal,"
cs-410_3_8_24,cs-410,3,8,"00:01:58,240","00:02:05,476",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,the reduction of entropy by knowing
cs-410_3_8_25,cs-410,3,8,"00:02:05,476","00:02:12,805",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"So, this quantity is called a Mutual"
cs-410_3_8_26,cs-410,3,8,"00:02:12,805","00:02:17,085",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,And this function has some interesting
cs-410_3_8_27,cs-410,3,8,"00:02:17,085","00:02:21,415",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,This is easy to understand because
cs-410_3_8_28,cs-410,3,8,"00:02:22,782","00:02:29,132",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,not going to be lower than the possibility
cs-410_3_8_29,cs-410,3,8,"00:02:29,132","00:02:33,512",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,"In other words, the conditional entropy"
cs-410_3_8_30,cs-410,3,8,"00:02:33,512","00:02:37,784",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,Knowing some information can
cs-410_3_8_31,cs-410,3,8,"00:02:37,784","00:02:40,282",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,will not hurt us in predicting x.
cs-410_3_8_32,cs-410,3,8,"00:02:41,510","00:02:46,375",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,The signal property is that it
cs-410_3_8_33,cs-410,3,8,"00:02:46,375","00:02:51,142",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,"entropy is not symmetrical,"
cs-410_3_8_34,cs-410,3,8,"00:02:51,142","00:02:56,394",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,the third property is that It
cs-410_3_8_35,cs-410,3,8,"00:02:56,394","00:03:01,580",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,only if the two random variables
cs-410_3_8_36,cs-410,3,8,"00:03:01,580","00:03:07,949",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,That means knowing one of them does not
cs-410_3_8_37,cs-410,3,8,"00:03:07,949","00:03:14,626",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,this last property can be verified by
cs-410_3_8_38,cs-410,3,8,"00:03:14,626","00:03:19,144",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,it reaches 0 if and
cs-410_3_8_39,cs-410,3,8,"00:03:19,144","00:03:24,102",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,[INAUDIBLE] Y is exactly the same
cs-410_3_8_40,cs-410,3,8,"00:03:24,102","00:03:28,344",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,So that means knowing why it did not
cs-410_3_8_41,cs-410,3,8,"00:03:28,344","00:03:30,520",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,a Y are completely independent.
cs-410_3_8_42,cs-410,3,8,"00:03:32,120","00:03:37,880",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,Now when we fix X to rank different
cs-410_3_8_43,cs-410,3,8,"00:03:37,880","00:03:44,180",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,would give the same order as
cs-410_3_8_44,cs-410,3,8,"00:03:44,180","00:03:49,940",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,"because in the function here,"
cs-410_3_8_45,cs-410,3,8,"00:03:49,940","00:03:53,820",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,So ranking based on mutual entropy is
cs-410_3_8_46,cs-410,3,8,"00:03:53,820","00:03:57,600",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,"the conditional entropy of X given Y, but"
cs-410_3_8_47,cs-410,3,8,"00:03:57,600","00:04:03,058",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,the mutual information allows us to
cs-410_3_8_48,cs-410,3,8,"00:04:03,058","00:04:07,990",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,"So, that is why mutual information is"
cs-410_3_8_49,cs-410,3,8,"00:04:10,688","00:04:14,420",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,"So, let us examine the intuition"
cs-410_3_8_50,cs-410,3,8,"00:04:14,420","00:04:15,880",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,Syntagmatical Relation Mining.
cs-410_3_8_51,cs-410,3,8,"00:04:17,150","00:04:20,430",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,"Now, the question we ask forcing"
cs-410_3_8_52,cs-410,3,8,"00:04:20,430","00:04:24,300",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,"whenever ""eats"" occurs,"
cs-410_3_8_53,cs-410,3,8,"00:04:25,610","00:04:30,710",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,So this question can be framed as
cs-410_3_8_54,cs-410,3,8,"00:04:30,710","00:04:33,055",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,which words have high mutual
cs-410_3_8_55,cs-410,3,8,"00:04:33,055","00:04:37,700",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,so computer the missing information
cs-410_3_8_56,cs-410,3,8,"00:04:39,050","00:04:44,520",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"And if we do that, and it is basically"
cs-410_3_8_57,cs-410,3,8,"00:04:44,520","00:04:48,990",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,we will see that words that
cs-410_3_8_58,cs-410,3,8,"00:04:48,990","00:04:50,960",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,will have a high point.
cs-410_3_8_59,cs-410,3,8,"00:04:50,960","00:04:55,200",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,Whereas words that are not related
cs-410_3_8_60,cs-410,3,8,"00:04:55,200","00:04:58,530",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"For this, I will give some example here."
cs-410_3_8_61,cs-410,3,8,"00:04:58,530","00:05:01,220",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"The mutual information between ""eats"" and"
cs-410_3_8_62,cs-410,3,8,"00:05:01,220","00:05:05,650",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,"which is the same as between ""meats"" and"
cs-410_3_8_63,cs-410,3,8,"00:05:05,650","00:05:10,960",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,symmetrical is expected to be higher than
cs-410_3_8_64,cs-410,3,8,"00:05:10,960","00:05:14,638",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"the, because knowing the does not"
cs-410_3_8_65,cs-410,3,8,"00:05:14,638","00:05:17,998",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,"It is similar, and"
cs-410_3_8_66,cs-410,3,8,"00:05:17,998","00:05:22,280",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,the as well.
cs-410_3_8_67,cs-410,3,8,"00:05:22,280","00:05:26,970",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,And you also can easily
cs-410_3_8_68,cs-410,3,8,"00:05:26,970","00:05:32,030",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,information between a word and
cs-410_3_8_69,cs-410,3,8,"00:05:32,030","00:05:37,890",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,which is equal to
cs-410_3_8_70,cs-410,3,8,"00:05:37,890","00:05:42,740",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"so, because in this case the reduction is"
cs-410_3_8_71,cs-410,3,8,"00:05:42,740","00:05:48,530",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,maximum because knowing one allows
cs-410_3_8_72,cs-410,3,8,"00:05:48,530","00:05:50,570",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"So the conditional entropy is zero,"
cs-410_3_8_73,cs-410,3,8,"00:05:50,570","00:05:54,472",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,therefore the mutual information
cs-410_3_8_74,cs-410,3,8,"00:05:54,472","00:06:02,520",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,"It is going to be larger, then are equal"
cs-410_3_8_75,cs-410,3,8,"00:06:02,520","00:06:05,420",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,In other words picking any other word and
cs-410_3_8_76,cs-410,3,8,"00:06:05,420","00:06:08,588",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,the computer picking between eats and
cs-410_3_8_77,cs-410,3,8,"00:06:08,588","00:06:13,511",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,You will not get any information larger
cs-410_3_8_78,cs-410,3,8,"00:06:16,386","00:06:21,390",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,So now let us look at how to
cs-410_3_8_79,cs-410,3,8,"00:06:21,390","00:06:23,490",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,"Now in order to do that, we often"
cs-410_3_8_80,cs-410,3,8,"00:06:25,110","00:06:29,100",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,use a different form of mutual
cs-410_3_8_81,cs-410,3,8,"00:06:29,100","00:06:34,190",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,rewrite the mutual information
cs-410_3_8_82,cs-410,3,8,"00:06:34,190","00:06:38,655",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,Where we essentially see
cs-410_3_8_83,cs-410,3,8,"00:06:38,655","00:06:43,075",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,called a KL-divergence or divergence.
cs-410_3_8_84,cs-410,3,8,"00:06:43,075","00:06:45,615",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,This is another term
cs-410_3_8_85,cs-410,3,8,"00:06:45,615","00:06:48,865",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,It measures the divergence
cs-410_3_8_86,cs-410,3,8,"00:06:50,615","00:06:54,645",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,"Now, if you look at the formula,"
cs-410_3_8_87,cs-410,3,8,"00:06:54,645","00:06:58,190",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,different values of the two random
cs-410_3_8_88,cs-410,3,8,"00:06:58,190","00:07:04,110",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,mainly we are doing a comparison
cs-410_3_8_89,cs-410,3,8,"00:07:04,110","00:07:06,690",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,"The numerator has the joint,"
cs-410_3_8_90,cs-410,3,8,"00:07:06,690","00:07:11,110",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,actual observed the joint distribution
cs-410_3_8_91,cs-410,3,8,"00:07:12,690","00:07:15,720",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,The bottom part or the denominator can be
cs-410_3_8_92,cs-410,3,8,"00:07:15,720","00:07:20,695",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,interpreted as the expected joint
cs-410_3_8_93,cs-410,3,8,"00:07:20,695","00:07:26,782",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,if they were independent because when
cs-410_3_8_94,cs-410,3,8,"00:07:26,782","00:07:32,810",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,they are joined distribution is equal to
cs-410_3_8_95,cs-410,3,8,"00:07:35,300","00:07:39,800",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,So this comparison will tell us whether
cs-410_3_8_96,cs-410,3,8,"00:07:39,800","00:07:43,170",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,If they are indeed independent then we
cs-410_3_8_97,cs-410,3,8,"00:07:44,390","00:07:49,470",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,but if the numerator is different
cs-410_3_8_98,cs-410,3,8,"00:07:49,470","00:07:54,530",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,the two variables are not independent and
cs-410_3_8_99,cs-410,3,8,"00:07:56,120","00:08:00,110",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,The sum is simply to take into
cs-410_3_8_100,cs-410,3,8,"00:08:00,110","00:08:04,180",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,of the values of these
cs-410_3_8_101,cs-410,3,8,"00:08:04,180","00:08:08,750",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,"In our case, each random variable"
cs-410_3_8_102,cs-410,3,8,"00:08:08,750","00:08:13,950",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"zero or one, so"
cs-410_3_8_103,cs-410,3,8,"00:08:13,950","00:08:17,330",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,If we look at this form of mutual
cs-410_3_8_104,cs-410,3,8,"00:08:17,330","00:08:21,230",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,information matches the divergence
cs-410_3_8_105,cs-410,3,8,"00:08:21,230","00:08:25,800",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,from the expected distribution
cs-410_3_8_106,cs-410,3,8,"00:08:25,800","00:08:30,144",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,"The larger this divergence is, the higher"
cs-410_3_8_107,cs-410,3,8,"00:08:33,507","00:08:37,091",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,So now let us further look at what
cs-410_3_8_108,cs-410,3,8,"00:08:37,091","00:08:39,840",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,involved in this formula
cs-410_3_8_109,cs-410,3,8,"00:08:41,300","00:08:45,080",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,"And here, this is all the probabilities"
cs-410_3_8_110,cs-410,3,8,"00:08:45,080","00:08:46,500",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,you to verify that.
cs-410_3_8_111,cs-410,3,8,"00:08:46,500","00:08:51,610",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,"Basically, we have first to"
cs-410_3_8_112,cs-410,3,8,"00:08:51,610","00:08:56,380",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,corresponding to the presence or
cs-410_3_8_113,cs-410,3,8,"00:08:56,380","00:08:59,610",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,"So, for w1,"
cs-410_3_8_114,cs-410,3,8,"00:09:02,600","00:09:07,995",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,"They should sum to one, because a word"
cs-410_3_8_115,cs-410,3,8,"00:09:07,995","00:09:13,260",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,"In the segment, and similarly for"
cs-410_3_8_116,cs-410,3,8,"00:09:13,260","00:09:18,230",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,"the second word, we also have two"
cs-410_3_8_117,cs-410,3,8,"00:09:18,230","00:09:20,920",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,"absences of this word, and"
cs-410_3_8_118,cs-410,3,8,"00:09:21,920","00:09:26,162",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,"And finally, we have a lot of"
cs-410_3_8_119,cs-410,3,8,"00:09:26,162","00:09:31,100",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,the scenarios of co-occurrences of
cs-410_3_8_120,cs-410,3,8,"00:09:34,513","00:09:39,107",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,And they sum to one because the two
cs-410_3_8_121,cs-410,3,8,"00:09:39,107","00:09:41,420",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,possible scenarios.
cs-410_3_8_122,cs-410,3,8,"00:09:41,420","00:09:43,730",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,"Either they both occur, so"
cs-410_3_8_123,cs-410,3,8,"00:09:43,730","00:09:49,500",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,in that case both variables will have
cs-410_3_8_124,cs-410,3,8,"00:09:49,500","00:09:50,579",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,There are two scenarios.
cs-410_3_8_125,cs-410,3,8,"00:09:51,660","00:09:55,910",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,In these two cases one of the random
cs-410_3_8_126,cs-410,3,8,"00:09:55,910","00:10:03,560",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,the other will be zero and finally we have
cs-410_3_8_127,cs-410,3,8,"00:10:03,560","00:10:06,420",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,This is when the two variables
cs-410_3_8_128,cs-410,3,8,"00:10:07,620","00:10:12,855",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,So these are the probabilities involved
cs-410_3_8_129,cs-410,3,8,"00:10:12,855","00:10:13,600",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,over here.
cs-410_3_8_130,cs-410,3,8,"00:10:16,007","00:10:18,416",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,Once we know how to calculate
cs-410_3_8_131,cs-410,3,8,"00:10:18,416","00:10:20,670",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,we can easily calculate
cs-410_3_8_132,cs-410,3,8,"00:10:24,063","00:10:28,231",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,It is also interesting to know that
cs-410_3_8_133,cs-410,3,8,"00:10:28,231","00:10:32,960",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,"constraint among these probabilities,"
cs-410_3_8_134,cs-410,3,8,"00:10:32,960","00:10:36,400",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=632,"So in the previous slide,"
cs-410_3_8_135,cs-410,3,8,"00:10:36,400","00:10:41,830",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=636,that you have seen that
cs-410_3_8_136,cs-410,3,8,"00:10:41,830","00:10:46,114",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,words sum to one and
cs-410_3_8_137,cs-410,3,8,"00:10:46,114","00:10:53,190",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,that says the two words have these
cs-410_3_8_138,cs-410,3,8,"00:10:53,190","00:10:57,370",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,but we also have some additional
cs-410_3_8_139,cs-410,3,8,"00:10:58,600","00:11:03,670",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,"For example, this one means if we add up"
cs-410_3_8_140,cs-410,3,8,"00:11:03,670","00:11:07,890",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,the probabilities that we observe
cs-410_3_8_141,cs-410,3,8,"00:11:07,890","00:11:12,500",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,the probabilities when the first word
cs-410_3_8_142,cs-410,3,8,"00:11:12,500","00:11:16,860",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,We get exactly the probability
cs-410_3_8_143,cs-410,3,8,"00:11:16,860","00:11:20,040",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,"In other words, when the word is observed."
cs-410_3_8_144,cs-410,3,8,"00:11:20,040","00:11:22,210",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,"When the first word is observed, and"
cs-410_3_8_145,cs-410,3,8,"00:11:22,210","00:11:27,640",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=682,"there are only two scenarios, depending on"
cs-410_3_8_146,cs-410,3,8,"00:11:27,640","00:11:31,750",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,"So, this probability captures the first"
cs-410_3_8_147,cs-410,3,8,"00:11:31,750","00:11:33,860",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,"actually is also observed, and"
cs-410_3_8_148,cs-410,3,8,"00:11:33,860","00:11:38,130",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,this captures the second scenario
cs-410_3_8_149,cs-410,3,8,"00:11:38,130","00:11:40,145",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,"So, we only see the first word, and"
cs-410_3_8_150,cs-410,3,8,"00:11:40,145","00:11:45,410",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,it is easy to see the other equations
cs-410_3_8_151,cs-410,3,8,"00:11:46,980","00:11:50,980",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,Now these equations allow us to
cs-410_3_8_152,cs-410,3,8,"00:11:50,980","00:11:54,610",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,"other probabilities, and"
cs-410_3_8_153,cs-410,3,8,"00:11:55,750","00:12:01,010",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,"So more specifically,"
cs-410_3_8_154,cs-410,3,8,"00:12:01,010","00:12:06,490",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=721,"a word is present, like in this case,"
cs-410_3_8_155,cs-410,3,8,"00:12:06,490","00:12:12,630",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=726,if we know the probability of
cs-410_3_8_156,cs-410,3,8,"00:12:12,630","00:12:17,002",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,then we can easily compute
cs-410_3_8_157,cs-410,3,8,"00:12:17,002","00:12:22,770",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,It is very easy to use this
cs-410_3_8_158,cs-410,3,8,"00:12:22,770","00:12:27,820",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=742,we take care of the computation of
cs-410_3_8_159,cs-410,3,8,"00:12:27,820","00:12:29,950",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=747,absence of each word.
cs-410_3_8_160,cs-410,3,8,"00:12:29,950","00:12:33,146",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,Now let's look at
cs-410_3_8_161,cs-410,3,8,"00:12:33,146","00:12:36,460",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,Let us assume that we also have available
cs-410_3_8_162,cs-410,3,8,"00:12:36,460","00:12:39,548",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,the probability that
cs-410_3_8_163,cs-410,3,8,"00:12:39,548","00:12:44,220",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=759,Now it is easy to see that we can
cs-410_3_8_164,cs-410,3,8,"00:12:44,220","00:12:45,829",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,probabilities based on these.
cs-410_3_8_165,cs-410,3,8,"00:12:46,870","00:12:51,170",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,Specifically for
cs-410_3_8_166,cs-410,3,8,"00:12:51,170","00:12:56,260",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=771,the probability that the first word
cs-410_3_8_167,cs-410,3,8,"00:12:56,260","00:13:02,020",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=776,because we know these probabilities in
cs-410_3_8_168,cs-410,3,8,"00:13:02,020","00:13:05,364",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=782,equation we can compute the probability
cs-410_3_8_169,cs-410,3,8,"00:13:05,364","00:13:06,000",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,Word.
cs-410_3_8_170,cs-410,3,8,"00:13:06,000","00:13:10,421",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,"And then finally,"
cs-410_3_8_171,cs-410,3,8,"00:13:10,421","00:13:14,745",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=790,by using this equation because
cs-410_3_8_172,cs-410,3,8,"00:13:14,745","00:13:19,282",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,"this is also known, and"
cs-410_3_8_173,cs-410,3,8,"00:13:19,282","00:13:23,120",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,So this can be easier to calculate.
cs-410_3_8_174,cs-410,3,8,"00:13:23,120","00:13:24,430",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,So now this can be calculated.
cs-410_3_8_175,cs-410,3,8,"00:13:26,080","00:13:30,989",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=806,So this slide shows that we only
cs-410_3_8_176,cs-410,3,8,"00:13:30,989","00:13:35,800",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,these three probabilities
cs-410_3_8_177,cs-410,3,8,"00:13:35,800","00:13:43,092",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,naming the presence of each word and the
cs-410_3_8_178,cs-410,3,8,"00:13:43,092","00:13:53,092",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,[MUSIC]
cs-410_3_9_1,cs-410,3,9,"00:00:00,012","00:00:07,295",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_3_9_2,cs-410,3,9,"00:00:07,295","00:00:11,390",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about probabilistic and
cs-410_3_9_3,cs-410,3,9,"00:00:12,710","00:00:18,000",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,In this lecture we're going to introduce
cs-410_3_9_4,cs-410,3,9,"00:00:18,000","00:00:18,770",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,often called PLSA.
cs-410_3_9_5,cs-410,3,9,"00:00:18,770","00:00:26,060",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"This is the most basic topic model,"
cs-410_3_9_6,cs-410,3,9,"00:00:26,060","00:00:30,890",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,Now this kind of models
cs-410_3_9_7,cs-410,3,9,"00:00:30,890","00:00:34,560",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,mine multiple topics from text documents.
cs-410_3_9_8,cs-410,3,9,"00:00:34,560","00:00:39,410",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,And PRSA is one of the most basic
cs-410_3_9_9,cs-410,3,9,"00:00:39,410","00:00:43,800",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,So let's first examine this power
cs-410_3_9_10,cs-410,3,9,"00:00:43,800","00:00:47,710",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,Here I show a sample article which is
cs-410_3_9_11,cs-410,3,9,"00:00:48,830","00:00:51,100",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,And I show some simple topics.
cs-410_3_9_12,cs-410,3,9,"00:00:51,100","00:00:55,870",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"For example government response,"
cs-410_3_9_13,cs-410,3,9,"00:00:55,870","00:00:57,420",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,Donation and the background.
cs-410_3_9_14,cs-410,3,9,"00:00:59,260","00:01:04,070",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,You can see in the article we use
cs-410_3_9_15,cs-410,3,9,"00:01:05,150","00:01:09,540",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,So we first for example see there's
cs-410_3_9_16,cs-410,3,9,"00:01:09,540","00:01:14,740",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,this is followed by discussion of flooding
cs-410_3_9_17,cs-410,3,9,"00:01:14,740","00:01:17,440",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,We also see background
cs-410_3_9_18,cs-410,3,9,"00:01:18,840","00:01:23,740",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,So the overall of topic analysis here
cs-410_3_9_19,cs-410,3,9,"00:01:23,740","00:01:28,250",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"the text, to segment the topics,"
cs-410_3_9_20,cs-410,3,9,"00:01:28,250","00:01:33,820",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"distribution and to figure out first,"
cs-410_3_9_21,cs-410,3,9,"00:01:33,820","00:01:36,420",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,How do we know there's a topic
cs-410_3_9_22,cs-410,3,9,"00:01:36,420","00:01:39,020",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,There's a topic about a flood in the city.
cs-410_3_9_23,cs-410,3,9,"00:01:39,020","00:01:41,850",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,So these are the tasks
cs-410_3_9_24,cs-410,3,9,"00:01:42,870","00:01:46,110",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,If we had discovered these
cs-410_3_9_25,cs-410,3,9,"00:01:46,110","00:01:50,030",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"as you see here,"
cs-410_3_9_26,cs-410,3,9,"00:01:50,030","00:01:54,390",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,"Then you can do a lot of things,"
cs-410_3_9_27,cs-410,3,9,"00:01:54,390","00:01:59,800",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,"of the topics,"
cs-410_3_9_28,cs-410,3,9,"00:01:59,800","00:02:04,220",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,So the formal definition of problem of
cs-410_3_9_29,cs-410,3,9,"00:02:04,220","00:02:04,870",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,shown here.
cs-410_3_9_30,cs-410,3,9,"00:02:04,870","00:02:09,270",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,And this is after a slide that you
cs-410_3_9_31,cs-410,3,9,"00:02:09,270","00:02:14,100",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,"So the input is a collection, the number"
cs-410_3_9_32,cs-410,3,9,"00:02:14,100","00:02:15,060",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,of course the text data.
cs-410_3_9_33,cs-410,3,9,"00:02:16,300","00:02:18,760",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,And then the output is of two kinds.
cs-410_3_9_34,cs-410,3,9,"00:02:18,760","00:02:21,720",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,"One is the topic category,"
cs-410_3_9_35,cs-410,3,9,"00:02:21,720","00:02:22,520",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,Theta i's.
cs-410_3_9_36,cs-410,3,9,"00:02:22,520","00:02:24,790",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,Each theta i is a word distribution.
cs-410_3_9_37,cs-410,3,9,"00:02:24,790","00:02:28,160",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"And second, it's the topic coverage for"
cs-410_3_9_38,cs-410,3,9,"00:02:28,160","00:02:30,130",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,These are pi sub i j's.
cs-410_3_9_39,cs-410,3,9,"00:02:30,130","00:02:33,490",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,And they tell us which document it covers.
cs-410_3_9_40,cs-410,3,9,"00:02:33,490","00:02:35,440",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,Which topic to what extent.
cs-410_3_9_41,cs-410,3,9,"00:02:35,440","00:02:37,960",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,So we hope to generate these as output.
cs-410_3_9_42,cs-410,3,9,"00:02:37,960","00:02:41,350",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,Because there are many useful
cs-410_3_9_43,cs-410,3,9,"00:02:42,880","00:02:47,100",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,So the idea of PLSA is
cs-410_3_9_44,cs-410,3,9,"00:02:47,100","00:02:50,660",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,the two component mixture model
cs-410_3_9_45,cs-410,3,9,"00:02:50,660","00:02:54,760",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,The only difference is that we
cs-410_3_9_46,cs-410,3,9,"00:02:54,760","00:02:57,960",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,"Otherwise, it is essentially the same."
cs-410_3_9_47,cs-410,3,9,"00:02:57,960","00:03:03,730",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,So here I illustrate how we can generate
cs-410_3_9_48,cs-410,3,9,"00:03:03,730","00:03:06,490",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,naturally in all cases
cs-410_3_9_49,cs-410,3,9,"00:03:06,490","00:03:11,310",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,of Probabilistic modelling would want
cs-410_3_9_50,cs-410,3,9,"00:03:11,310","00:03:13,400",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,"So we would also ask the question,"
cs-410_3_9_51,cs-410,3,9,"00:03:13,400","00:03:18,200",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,what's the probability of observing
cs-410_3_9_52,cs-410,3,9,"00:03:18,200","00:03:19,470",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,Now if you look at this picture and
cs-410_3_9_53,cs-410,3,9,"00:03:19,470","00:03:21,840",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,compare this with the picture
cs-410_3_9_54,cs-410,3,9,"00:03:21,840","00:03:25,580",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,you will see the only difference is
cs-410_3_9_55,cs-410,3,9,"00:03:26,940","00:03:32,900",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"So, before we have just one topic,"
cs-410_3_9_56,cs-410,3,9,"00:03:32,900","00:03:35,990",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,But now we have more topics.
cs-410_3_9_57,cs-410,3,9,"00:03:35,990","00:03:38,260",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"Specifically, we have k topics now."
cs-410_3_9_58,cs-410,3,9,"00:03:38,260","00:03:43,930",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,All these are topics that we assume
cs-410_3_9_59,cs-410,3,9,"00:03:43,930","00:03:49,450",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,So the consequence is that our switch for
cs-410_3_9_60,cs-410,3,9,"00:03:49,450","00:03:51,210",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,Before it's just a two way switch.
cs-410_3_9_61,cs-410,3,9,"00:03:51,210","00:03:53,420",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,We can think of it as flipping a coin.
cs-410_3_9_62,cs-410,3,9,"00:03:53,420","00:03:55,110",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,But now we have multiple ways.
cs-410_3_9_63,cs-410,3,9,"00:03:55,110","00:03:59,660",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,First we can flip a coin to decide
cs-410_3_9_64,cs-410,3,9,"00:03:59,660","00:04:06,913",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,So it's the background lambda
cs-410_3_9_65,cs-410,3,9,"00:04:06,913","00:04:11,490",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,1 minus lambda sub B gives
cs-410_3_9_66,cs-410,3,9,"00:04:11,490","00:04:16,300",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,actually choosing a non-background topic.
cs-410_3_9_67,cs-410,3,9,"00:04:16,300","00:04:17,860",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"After we have made this decision,"
cs-410_3_9_68,cs-410,3,9,"00:04:17,860","00:04:24,750",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,we have to make another decision to
cs-410_3_9_69,cs-410,3,9,"00:04:24,750","00:04:26,480",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,So there are K way switch here.
cs-410_3_9_70,cs-410,3,9,"00:04:26,480","00:04:30,120",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"And this is characterized by pi,"
cs-410_3_9_71,cs-410,3,9,"00:04:31,450","00:04:33,775",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,This is just the difference of designs.
cs-410_3_9_72,cs-410,3,9,"00:04:33,775","00:04:36,745",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,Which is a little bit more complicated.
cs-410_3_9_73,cs-410,3,9,"00:04:36,745","00:04:40,655",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,But once we decide which distribution to
cs-410_3_9_74,cs-410,3,9,"00:04:40,655","00:04:45,145",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,just generate a word by using one of
cs-410_3_9_75,cs-410,3,9,"00:04:46,885","00:04:50,920",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,So now lets look at the question
cs-410_3_9_76,cs-410,3,9,"00:04:50,920","00:04:55,780",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,So what's the probability of observing
cs-410_3_9_77,cs-410,3,9,"00:04:55,780","00:04:57,250",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,What do you think?
cs-410_3_9_78,cs-410,3,9,"00:04:57,250","00:05:01,150",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,Now we've seen this
cs-410_3_9_79,cs-410,3,9,"00:05:01,150","00:05:05,210",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,"if you can recall, it's generally a sum."
cs-410_3_9_80,cs-410,3,9,"00:05:05,210","00:05:08,540",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,Of all the different possibilities
cs-410_3_9_81,cs-410,3,9,"00:05:08,540","00:05:14,260",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,So let's first look at how the word can
cs-410_3_9_82,cs-410,3,9,"00:05:14,260","00:05:18,340",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,"Well, the probability that the word is"
cs-410_3_9_83,cs-410,3,9,"00:05:18,340","00:05:22,700",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,is lambda multiplied by the probability
cs-410_3_9_84,cs-410,3,9,"00:05:22,700","00:05:24,200",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,"Model, right."
cs-410_3_9_85,cs-410,3,9,"00:05:24,200","00:05:25,150",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,Two things must happen.
cs-410_3_9_86,cs-410,3,9,"00:05:25,150","00:05:28,270",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,"First, we have to have"
cs-410_3_9_87,cs-410,3,9,"00:05:28,270","00:05:31,730",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,"and that's the probability of lambda,"
cs-410_3_9_88,cs-410,3,9,"00:05:31,730","00:05:36,330",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,"Then second, we must have actually"
cs-410_3_9_89,cs-410,3,9,"00:05:36,330","00:05:39,161",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,and that's probability
cs-410_3_9_90,cs-410,3,9,"00:05:40,220","00:05:41,790",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"Okay, so similarly,"
cs-410_3_9_91,cs-410,3,9,"00:05:41,790","00:05:46,020",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,we can figure out the probability of
cs-410_3_9_92,cs-410,3,9,"00:05:46,020","00:05:48,530",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,Like the topic theta sub k.
cs-410_3_9_93,cs-410,3,9,"00:05:48,530","00:05:51,890",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,Now notice that here's
cs-410_3_9_94,cs-410,3,9,"00:05:51,890","00:05:57,023",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,And that's because of the choice
cs-410_3_9_95,cs-410,3,9,"00:05:57,023","00:06:00,630",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,only happens if two things happen.
cs-410_3_9_96,cs-410,3,9,"00:06:00,630","00:06:04,020",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,One is we decide not to
cs-410_3_9_97,cs-410,3,9,"00:06:04,020","00:06:07,630",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"So, that's a probability"
cs-410_3_9_98,cs-410,3,9,"00:06:07,630","00:06:13,290",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,"Second, we also have to actually choose"
cs-410_3_9_99,cs-410,3,9,"00:06:13,290","00:06:16,000",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,"So that's probability of theta sub K,"
cs-410_3_9_100,cs-410,3,9,"00:06:17,900","00:06:21,460",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,"And similarly, the probability of"
cs-410_3_9_101,cs-410,3,9,"00:06:21,460","00:06:26,480",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,The topic and the first topic
cs-410_3_9_102,cs-410,3,9,"00:06:26,480","00:06:27,250",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,And so
cs-410_3_9_103,cs-410,3,9,"00:06:27,250","00:06:32,480",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,in the end the probability of observing
cs-410_3_9_104,cs-410,3,9,"00:06:32,480","00:06:38,080",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,And I have to stress again this is a very
cs-410_3_9_105,cs-410,3,9,"00:06:38,080","00:06:44,150",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,really key to understanding all the topic
cs-410_3_9_106,cs-410,3,9,"00:06:44,150","00:06:47,410",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,So make sure that you really
cs-410_3_9_107,cs-410,3,9,"00:06:49,410","00:06:53,390",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,of w is indeed the sum of these terms.
cs-410_3_9_108,cs-410,3,9,"00:06:56,540","00:07:00,620",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,"So, next,"
cs-410_3_9_109,cs-410,3,9,"00:07:00,620","00:07:05,250",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,we would be interested in
cs-410_3_9_110,cs-410,3,9,"00:07:05,250","00:07:07,250",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"All right, so to estimate the parameters."
cs-410_3_9_111,cs-410,3,9,"00:07:07,250","00:07:07,760",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,"But firstly,"
cs-410_3_9_112,cs-410,3,9,"00:07:07,760","00:07:13,510",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,let's put all these together to have the
cs-410_3_9_113,cs-410,3,9,"00:07:13,510","00:07:19,010",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,The first line shows the probability of a
cs-410_3_9_114,cs-410,3,9,"00:07:19,010","00:07:20,980",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,And this is an important
cs-410_3_9_115,cs-410,3,9,"00:07:22,560","00:07:24,250",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,So let's take a closer look at this.
cs-410_3_9_116,cs-410,3,9,"00:07:24,250","00:07:27,430",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,This actually commands all
cs-410_3_9_117,cs-410,3,9,"00:07:27,430","00:07:29,280",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,So first of all we see lambda sub b here.
cs-410_3_9_118,cs-410,3,9,"00:07:29,280","00:07:31,539",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,This represents a percentage
cs-410_3_9_119,cs-410,3,9,"00:07:32,610","00:07:35,560",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,that we believe exist in the text data.
cs-410_3_9_120,cs-410,3,9,"00:07:35,560","00:07:39,220",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,And this can be a known value
cs-410_3_9_121,cs-410,3,9,"00:07:41,180","00:07:43,380",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"Second, we see the background"
cs-410_3_9_122,cs-410,3,9,"00:07:43,380","00:07:45,210",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,typically we also assume this is known.
cs-410_3_9_123,cs-410,3,9,"00:07:45,210","00:07:48,000",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"We can use a large collection of text, or"
cs-410_3_9_124,cs-410,3,9,"00:07:48,000","00:07:51,780",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,use all the text that we have available
cs-410_3_9_125,cs-410,3,9,"00:07:52,890","00:07:55,008",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,Now next in the next stop this formula.
cs-410_3_9_126,cs-410,3,9,"00:07:55,008","00:07:57,960",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,[COUGH] Excuse me.
cs-410_3_9_127,cs-410,3,9,"00:07:57,960","00:08:00,160",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,You see two interesting
cs-410_3_9_128,cs-410,3,9,"00:08:00,160","00:08:01,886",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,those are the most important parameters.
cs-410_3_9_129,cs-410,3,9,"00:08:01,886","00:08:04,690",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,That we are.
cs-410_3_9_130,cs-410,3,9,"00:08:04,690","00:08:06,190",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,So one is pi's.
cs-410_3_9_131,cs-410,3,9,"00:08:06,190","00:08:10,060",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,And these are the coverage
cs-410_3_9_132,cs-410,3,9,"00:08:11,280","00:08:15,310",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,And the other is word distributions
cs-410_3_9_133,cs-410,3,9,"00:08:18,530","00:08:23,780",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,"So the next line,"
cs-410_3_9_134,cs-410,3,9,"00:08:23,780","00:08:26,280",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,in to calculate
cs-410_3_9_135,cs-410,3,9,"00:08:26,280","00:08:29,720",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,"This is, again, of the familiar"
cs-410_3_9_136,cs-410,3,9,"00:08:29,720","00:08:32,050",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,you have a count of
cs-410_3_9_137,cs-410,3,9,"00:08:32,050","00:08:35,100",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,And then log of a probability.
cs-410_3_9_138,cs-410,3,9,"00:08:35,100","00:08:39,040",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,Now it's a little bit more
cs-410_3_9_139,cs-410,3,9,"00:08:39,040","00:08:43,890",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"Because now we have more components,"
cs-410_3_9_140,cs-410,3,9,"00:08:43,890","00:08:47,750",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,And then this line is just
cs-410_3_9_141,cs-410,3,9,"00:08:47,750","00:08:51,130",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"And it's very similar, just accounting for"
cs-410_3_9_142,cs-410,3,9,"00:08:52,470","00:08:54,060",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,So what are the unknown parameters?
cs-410_3_9_143,cs-410,3,9,"00:08:54,060","00:08:55,960",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,I already said that there are two kinds.
cs-410_3_9_144,cs-410,3,9,"00:08:55,960","00:08:59,150",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,"One is coverage,"
cs-410_3_9_145,cs-410,3,9,"00:08:59,150","00:09:02,350",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,"Again, it's a useful exercise for"
cs-410_3_9_146,cs-410,3,9,"00:09:02,350","00:09:04,730",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,Exactly how many
cs-410_3_9_147,cs-410,3,9,"00:09:05,750","00:09:07,940",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,How many unknown parameters are there?
cs-410_3_9_148,cs-410,3,9,"00:09:07,940","00:09:08,680",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,"Now, try and"
cs-410_3_9_149,cs-410,3,9,"00:09:08,680","00:09:13,090",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,think out that question will help you
cs-410_3_9_150,cs-410,3,9,"00:09:13,090","00:09:17,760",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,And will also allow you to understand
cs-410_3_9_151,cs-410,3,9,"00:09:17,760","00:09:20,430",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,when use PLSA to analyze text data?
cs-410_3_9_152,cs-410,3,9,"00:09:20,430","00:09:22,480",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,And these are precisely
cs-410_3_9_153,cs-410,3,9,"00:09:24,480","00:09:28,200",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,So after we have obtained
cs-410_3_9_154,cs-410,3,9,"00:09:28,200","00:09:30,820",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,the next is to worry about
cs-410_3_9_155,cs-410,3,9,"00:09:32,050","00:09:34,770",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,"And we can do the usual think,"
cs-410_3_9_156,cs-410,3,9,"00:09:34,770","00:09:40,190",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"So again, it's a constrained optimization"
cs-410_3_9_157,cs-410,3,9,"00:09:40,190","00:09:44,350",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,Only that we have a collection of text and
cs-410_3_9_158,cs-410,3,9,"00:09:44,350","00:09:48,655",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,"And we still have two constraints,"
cs-410_3_9_159,cs-410,3,9,"00:09:48,655","00:09:50,145",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,One is the word distributions.
cs-410_3_9_160,cs-410,3,9,"00:09:51,245","00:09:56,525",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,All the words must have probabilities
cs-410_3_9_161,cs-410,3,9,"00:09:56,525","00:09:59,975",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,The other is the topic
cs-410_3_9_162,cs-410,3,9,"00:09:59,975","00:10:05,200",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,a document will have to cover
cs-410_3_9_163,cs-410,3,9,"00:10:05,200","00:10:08,820",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,the probability of covering each
cs-410_3_9_164,cs-410,3,9,"00:10:08,820","00:10:13,190",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,So at this point though it's basically
cs-410_3_9_165,cs-410,3,9,"00:10:13,190","00:10:16,370",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,you just need to figure out
cs-410_3_9_166,cs-410,3,9,"00:10:16,370","00:10:18,670",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,There's a function with many variables.
cs-410_3_9_167,cs-410,3,9,"00:10:18,670","00:10:22,481",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,and we need to just figure
cs-410_3_9_168,cs-410,3,9,"00:10:22,481","00:10:26,397",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,variables to make the function
cs-410_3_9_169,cs-410,3,9,"00:10:26,397","00:10:36,397",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,>> [MUSIC]
cs-410_4_1_1,cs-410,4,1,"00:00:00,000","00:00:07,569",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_4_1_2,cs-410,4,1,"00:00:07,569","00:00:10,320",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is a overview of
cs-410_4_1_3,cs-410,4,1,"00:00:13,630","00:00:17,820",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In the previous lecture, we introduced"
cs-410_4_1_4,cs-410,4,1,"00:00:17,820","00:00:20,330",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,We explained that the main problem
cs-410_4_1_5,cs-410,4,1,"00:00:20,330","00:00:24,780",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,is the design of ranking function
cs-410_4_1_6,cs-410,4,1,"00:00:24,780","00:00:25,510",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"In this lecture,"
cs-410_4_1_7,cs-410,4,1,"00:00:25,510","00:00:31,040",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,we will give an overview of different
cs-410_4_1_8,cs-410,4,1,"00:00:33,840","00:00:35,750",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,So the problem is the following.
cs-410_4_1_9,cs-410,4,1,"00:00:35,750","00:00:39,310",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,We have a query that has
cs-410_4_1_10,cs-410,4,1,"00:00:39,310","00:00:42,710",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,the document that's also
cs-410_4_1_11,cs-410,4,1,"00:00:42,710","00:00:44,509",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,And we hope to define a function f
cs-410_4_1_12,cs-410,4,1,"00:00:45,770","00:00:49,596",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,that can compute a score based
cs-410_4_1_13,cs-410,4,1,"00:00:49,596","00:00:54,870",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,So the main challenge you hear is with
cs-410_4_1_14,cs-410,4,1,"00:00:54,870","00:01:00,275",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,can rank all the relevant documents
cs-410_4_1_15,cs-410,4,1,"00:01:00,275","00:01:05,544",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"Clearly, this means our function"
cs-410_4_1_16,cs-410,4,1,"00:01:05,544","00:01:10,824",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,the likelihood that a document
cs-410_4_1_17,cs-410,4,1,"00:01:10,824","00:01:16,490",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,That also means we have to have
cs-410_4_1_18,cs-410,4,1,"00:01:16,490","00:01:19,621",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,"In particular, in order to"
cs-410_4_1_19,cs-410,4,1,"00:01:19,621","00:01:23,380",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,we have to have a computational
cs-410_4_1_20,cs-410,4,1,"00:01:23,380","00:01:27,232",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,And we achieve this goal by
cs-410_4_1_21,cs-410,4,1,"00:01:27,232","00:01:30,370",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,which gives us
cs-410_4_1_22,cs-410,4,1,"00:01:32,650","00:01:34,110",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"Now, over many decades,"
cs-410_4_1_23,cs-410,4,1,"00:01:34,110","00:01:38,420",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,researchers have designed many
cs-410_4_1_24,cs-410,4,1,"00:01:38,420","00:01:40,680",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,And they fall into different categories.
cs-410_4_1_25,cs-410,4,1,"00:01:42,290","00:01:48,830",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"First, one family of the models"
cs-410_4_1_26,cs-410,4,1,"00:01:50,090","00:01:54,170",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,"Basically, we assume that if"
cs-410_4_1_27,cs-410,4,1,"00:01:54,170","00:01:57,970",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,"the query than another document is,"
cs-410_4_1_28,cs-410,4,1,"00:01:57,970","00:02:02,310",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,then we will say the first document
cs-410_4_1_29,cs-410,4,1,"00:02:02,310","00:02:05,330",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,"So in this case,"
cs-410_4_1_30,cs-410,4,1,"00:02:05,330","00:02:08,572",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,the similarity between the query and
cs-410_4_1_31,cs-410,4,1,"00:02:08,572","00:02:13,760",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,One well known example in this
cs-410_4_1_32,cs-410,4,1,"00:02:13,760","00:02:17,160",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,which we will cover more in
cs-410_4_1_33,cs-410,4,1,"00:02:20,370","00:02:24,010",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,A second kind of models
cs-410_4_1_34,cs-410,4,1,"00:02:24,010","00:02:30,600",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"In this family of models, we follow a very"
cs-410_4_1_35,cs-410,4,1,"00:02:30,600","00:02:35,200",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,queries and documents are all
cs-410_4_1_36,cs-410,4,1,"00:02:36,420","00:02:41,220",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,And we assume there is a binary
cs-410_4_1_37,cs-410,4,1,"00:02:42,370","00:02:45,420",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,to indicate whether a document
cs-410_4_1_38,cs-410,4,1,"00:02:46,530","00:02:53,090",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,We then define the score of document with
cs-410_4_1_39,cs-410,4,1,"00:02:53,090","00:02:59,780",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"this random variable R is equal to 1,"
cs-410_4_1_40,cs-410,4,1,"00:02:59,780","00:03:04,363",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,There are different cases
cs-410_4_1_41,cs-410,4,1,"00:03:04,363","00:03:08,003",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,"One is classic probabilistic model,"
cs-410_4_1_42,cs-410,4,1,"00:03:08,003","00:03:10,800",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,yet another is divergence
cs-410_4_1_43,cs-410,4,1,"00:03:12,580","00:03:17,865",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,"In a later lecture, we will talk more"
cs-410_4_1_44,cs-410,4,1,"00:03:17,865","00:03:21,740",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,A third kind of model are based
cs-410_4_1_45,cs-410,4,1,"00:03:21,740","00:03:27,440",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,So here the idea is to associate
cs-410_4_1_46,cs-410,4,1,"00:03:27,440","00:03:31,230",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,and we can then quantify
cs-410_4_1_47,cs-410,4,1,"00:03:31,230","00:03:34,790",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,show that the query
cs-410_4_1_48,cs-410,4,1,"00:03:37,100","00:03:41,940",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"Finally, there is also a family of models"
cs-410_4_1_49,cs-410,4,1,"00:03:41,940","00:03:46,237",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,that are using axiomatic thinking.
cs-410_4_1_50,cs-410,4,1,"00:03:46,237","00:03:50,849",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,"Here, an idea is to define"
cs-410_4_1_51,cs-410,4,1,"00:03:50,849","00:03:54,650",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,hope a good retrieval function to satisfy.
cs-410_4_1_52,cs-410,4,1,"00:03:55,760","00:04:00,572",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,"So in this case, the problem is"
cs-410_4_1_53,cs-410,4,1,"00:04:00,572","00:04:04,288",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,that can satisfy all
cs-410_4_1_54,cs-410,4,1,"00:04:05,867","00:04:12,326",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"Interestingly, although these different"
cs-410_4_1_55,cs-410,4,1,"00:04:12,326","00:04:17,930",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,"in the end, the retrieval function"
cs-410_4_1_56,cs-410,4,1,"00:04:17,930","00:04:22,020",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,And these functions tend to
cs-410_4_1_57,cs-410,4,1,"00:04:22,020","00:04:28,010",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,So now let's take a look at the common
cs-410_4_1_58,cs-410,4,1,"00:04:28,010","00:04:32,760",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,and to examine some of the common
cs-410_4_1_59,cs-410,4,1,"00:04:33,900","00:04:38,810",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"First, these models are all"
cs-410_4_1_60,cs-410,4,1,"00:04:38,810","00:04:43,060",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,"of using bag of words to represent text,"
cs-410_4_1_61,cs-410,4,1,"00:04:43,060","00:04:47,500",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,we explained this in the natural
cs-410_4_1_62,cs-410,4,1,"00:04:47,500","00:04:51,450",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,Bag of words representation remains
cs-410_4_1_63,cs-410,4,1,"00:04:51,450","00:04:52,320",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,the search engines.
cs-410_4_1_64,cs-410,4,1,"00:04:53,620","00:04:57,690",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"So with this assumption,"
cs-410_4_1_65,cs-410,4,1,"00:04:57,690","00:05:03,300",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,like a presidential campaign news
cs-410_4_1_66,cs-410,4,1,"00:05:03,300","00:05:08,140",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,would be based on scores computed
cs-410_4_1_67,cs-410,4,1,"00:05:09,560","00:05:15,710",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,And that means the score would
cs-410_4_1_68,cs-410,4,1,"00:05:15,710","00:05:19,510",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"such as presidential, campaign, and news."
cs-410_4_1_69,cs-410,4,1,"00:05:19,510","00:05:23,749",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,"Here, we can see there"
cs-410_4_1_70,cs-410,4,1,"00:05:23,749","00:05:29,501",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,each corresponding to how well the
cs-410_4_1_71,cs-410,4,1,"00:05:31,475","00:05:36,729",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,"Inside of these functions,"
cs-410_4_1_72,cs-410,4,1,"00:05:38,760","00:05:43,770",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,"So for example, one factor that"
cs-410_4_1_73,cs-410,4,1,"00:05:43,770","00:05:48,570",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,is how many times does the word
cs-410_4_1_74,cs-410,4,1,"00:05:48,570","00:05:50,250",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"This is called a term frequency, or TF."
cs-410_4_1_75,cs-410,4,1,"00:05:51,710","00:05:56,823",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,We might also denote as
cs-410_4_1_76,cs-410,4,1,"00:05:56,823","00:06:03,533",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"In general, if the word occurs"
cs-410_4_1_77,cs-410,4,1,"00:06:03,533","00:06:08,550",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,then the value of this
cs-410_4_1_78,cs-410,4,1,"00:06:08,550","00:06:13,610",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,"Another factor is,"
cs-410_4_1_79,cs-410,4,1,"00:06:13,610","00:06:18,141",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,And this is to use the document length for
cs-410_4_1_80,cs-410,4,1,"00:06:18,141","00:06:22,910",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,"In general, if a term occurs in a long"
cs-410_4_1_81,cs-410,4,1,"00:06:22,910","00:06:28,430",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,"document many times,"
cs-410_4_1_82,cs-410,4,1,"00:06:28,430","00:06:32,678",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,if it occurred the same number
cs-410_4_1_83,cs-410,4,1,"00:06:32,678","00:06:37,130",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"Because in a long document, any term"
cs-410_4_1_84,cs-410,4,1,"00:06:38,980","00:06:42,840",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,"Finally, there is this factor"
cs-410_4_1_85,cs-410,4,1,"00:06:42,840","00:06:48,020",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,"That is, we also want to look at how"
cs-410_4_1_86,cs-410,4,1,"00:06:48,020","00:06:55,240",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"collection, and we call this document"
cs-410_4_1_87,cs-410,4,1,"00:06:55,240","00:07:01,200",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,"And in some other models,"
cs-410_4_1_88,cs-410,4,1,"00:07:01,200","00:07:04,620",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,to characterize this information.
cs-410_4_1_89,cs-410,4,1,"00:07:05,860","00:07:09,770",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"So here, I show the probability of"
cs-410_4_1_90,cs-410,4,1,"00:07:10,830","00:07:14,564",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,So all these are trying to characterize
cs-410_4_1_91,cs-410,4,1,"00:07:14,564","00:07:15,555",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,the collection.
cs-410_4_1_92,cs-410,4,1,"00:07:15,555","00:07:20,418",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"In general, matching a rare term in"
cs-410_4_1_93,cs-410,4,1,"00:07:20,418","00:07:23,860",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,to the overall score than
cs-410_4_1_94,cs-410,4,1,"00:07:25,720","00:07:30,564",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,So this captures some of the main ideas
cs-410_4_1_95,cs-410,4,1,"00:07:30,564","00:07:32,349",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,the art original models.
cs-410_4_1_96,cs-410,4,1,"00:07:34,000","00:07:38,422",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,"So now, a natural question is,"
cs-410_4_1_97,cs-410,4,1,"00:07:39,834","00:07:45,080",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,Now it turns out that many
cs-410_4_1_98,cs-410,4,1,"00:07:45,080","00:07:47,700",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,So here are a list of
cs-410_4_1_99,cs-410,4,1,"00:07:47,700","00:07:52,463",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,that are generally regarded as
cs-410_4_1_100,cs-410,4,1,"00:07:52,463","00:07:57,920",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,"pivoted length normalization,"
cs-410_4_1_101,cs-410,4,1,"00:07:57,920","00:08:02,110",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,"When optimized,"
cs-410_4_1_102,cs-410,4,1,"00:08:02,110","00:08:08,508",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,And this was discussed in detail in this
cs-410_4_1_103,cs-410,4,1,"00:08:08,508","00:08:13,130",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"Among all these,"
cs-410_4_1_104,cs-410,4,1,"00:08:13,130","00:08:17,750",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,It's most likely that this has been used
cs-410_4_1_105,cs-410,4,1,"00:08:17,750","00:08:21,730",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,and you will also often see this
cs-410_4_1_106,cs-410,4,1,"00:08:22,800","00:08:27,090",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,And we'll talk more about this
cs-410_4_1_107,cs-410,4,1,"00:08:30,430","00:08:36,770",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,"So, to summarize, the main points made"
cs-410_4_1_108,cs-410,4,1,"00:08:36,770","00:08:41,540",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,of a good ranking function pre-requires a
cs-410_4_1_109,cs-410,4,1,"00:08:41,540","00:08:45,310",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,we achieve this goal by designing
cs-410_4_1_110,cs-410,4,1,"00:08:47,170","00:08:52,260",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"Second, many models are equally effective,"
cs-410_4_1_111,cs-410,4,1,"00:08:52,260","00:08:55,760",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,Researchers are still active and
cs-410_4_1_112,cs-410,4,1,"00:08:55,760","00:08:58,636",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,trying to find a truly
cs-410_4_1_113,cs-410,4,1,"00:09:00,865","00:09:03,926",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,"Finally, the state of the art"
cs-410_4_1_114,cs-410,4,1,"00:09:03,926","00:09:05,920",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,to rely on the following ideas.
cs-410_4_1_115,cs-410,4,1,"00:09:05,920","00:09:08,970",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,"First, bag of words representation."
cs-410_4_1_116,cs-410,4,1,"00:09:08,970","00:09:14,740",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"Second, TF and"
cs-410_4_1_117,cs-410,4,1,"00:09:14,740","00:09:19,787",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,Such information is used in
cs-410_4_1_118,cs-410,4,1,"00:09:19,787","00:09:25,028",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,the overall contribution of matching
cs-410_4_1_119,cs-410,4,1,"00:09:25,028","00:09:29,692",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,These are often combined in interesting
cs-410_4_1_120,cs-410,4,1,"00:09:29,692","00:09:34,210",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,exactly they are combined to rank
cs-410_4_1_121,cs-410,4,1,"00:09:36,390","00:09:40,560",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,There are two suggested additional
cs-410_4_1_122,cs-410,4,1,"00:09:41,760","00:09:45,150",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,The first is a paper where you can
cs-410_4_1_123,cs-410,4,1,"00:09:45,150","00:09:48,420",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,comparison of multiple
cs-410_4_1_124,cs-410,4,1,"00:09:49,840","00:09:54,674",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,The second is a book with
cs-410_4_1_125,cs-410,4,1,"00:09:54,674","00:09:58,507",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,review of different retrieval models.
cs-410_4_1_126,cs-410,4,1,"00:09:58,507","00:10:08,507",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,[MUSIC]
cs-410_4_10_1,cs-410,4,10,"00:00:00,012","00:00:08,521",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_4_10_2,cs-410,4,10,"00:00:08,521","00:00:13,230",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=8,about how to use generative probabilistic
cs-410_4_10_3,cs-410,4,10,"00:00:14,450","00:00:19,470",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,There are in general about two kinds
cs-410_4_10_4,cs-410,4,10,"00:00:19,470","00:00:20,580",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,by using machine learning.
cs-410_4_10_5,cs-410,4,10,"00:00:20,580","00:00:22,970",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,One is by generating probabilistic models.
cs-410_4_10_6,cs-410,4,10,"00:00:22,970","00:00:25,750",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,The other is discriminative approaches.
cs-410_4_10_7,cs-410,4,10,"00:00:25,750","00:00:29,740",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"In this lecture, we're going to"
cs-410_4_10_8,cs-410,4,10,"00:00:29,740","00:00:34,460",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,"In the next lecture, we're going to"
cs-410_4_10_9,cs-410,4,10,"00:00:34,460","00:00:36,990",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,So the problem of text categorization
cs-410_4_10_10,cs-410,4,10,"00:00:36,990","00:00:39,400",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,is actually a very similar
cs-410_4_10_11,cs-410,4,10,"00:00:39,400","00:00:44,820",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"In that, we'll assume that each document"
cs-410_4_10_12,cs-410,4,10,"00:00:44,820","00:00:48,500",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,The main difference is that in
cs-410_4_10_13,cs-410,4,10,"00:00:48,500","00:00:51,810",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"what are the predefined categories are,"
cs-410_4_10_14,cs-410,4,10,"00:00:51,810","00:00:54,660",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"In fact,"
cs-410_4_10_15,cs-410,4,10,"00:00:55,780","00:00:58,260",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,We want to find such clusters in the data.
cs-410_4_10_16,cs-410,4,10,"00:00:59,280","00:01:02,310",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,"But in the case of categorization,"
cs-410_4_10_17,cs-410,4,10,"00:01:02,310","00:01:07,470",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,So we kind of have
cs-410_4_10_18,cs-410,4,10,"00:01:07,470","00:01:11,810",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,then based on these categories and
cs-410_4_10_19,cs-410,4,10,"00:01:11,810","00:01:18,710",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,a document to one of these categories or
cs-410_4_10_20,cs-410,4,10,"00:01:18,710","00:01:21,160",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,But because of the similarity
cs-410_4_10_21,cs-410,4,10,"00:01:21,160","00:01:26,930",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,we can actually get the document
cs-410_4_10_22,cs-410,4,10,"00:01:26,930","00:01:30,630",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,And we understand how we can
cs-410_4_10_23,cs-410,4,10,"00:01:30,630","00:01:35,930",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,text categorization from
cs-410_4_10_24,cs-410,4,10,"00:01:35,930","00:01:41,620",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,"And so, this is a slide that we've talked"
cs-410_4_10_25,cs-410,4,10,"00:01:41,620","00:01:47,550",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,where we assume there are multiple topics
cs-410_4_10_26,cs-410,4,10,"00:01:47,550","00:01:49,620",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,Each topic is one cluster.
cs-410_4_10_27,cs-410,4,10,"00:01:49,620","00:01:52,080",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"So once we estimated such a model,"
cs-410_4_10_28,cs-410,4,10,"00:01:52,080","00:01:58,380",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,we faced a problem of deciding which
cs-410_4_10_29,cs-410,4,10,"00:01:58,380","00:02:04,260",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,And this question boils down to decide
cs-410_4_10_30,cs-410,4,10,"00:02:06,440","00:02:14,470",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,"Now, suppose d has L words"
cs-410_4_10_31,cs-410,4,10,"00:02:14,470","00:02:21,220",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,"Now, how can you compute"
cs-410_4_10_32,cs-410,4,10,"00:02:21,220","00:02:25,960",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,topic word distribution zeta i has
cs-410_4_10_33,cs-410,4,10,"00:02:27,050","00:02:32,980",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"Well, in general, we use base"
cs-410_4_10_34,cs-410,4,10,"00:02:32,980","00:02:40,167",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,you can see this prior information here
cs-410_4_10_35,cs-410,4,10,"00:02:40,167","00:02:44,846",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,cluster has a higher prior
cs-410_4_10_36,cs-410,4,10,"00:02:44,846","00:02:49,920",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,that the document has
cs-410_4_10_37,cs-410,4,10,"00:02:49,920","00:02:52,470",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"And so, we should favor such a cluster."
cs-410_4_10_38,cs-410,4,10,"00:02:52,470","00:02:54,960",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,"The other is a likelihood part,"
cs-410_4_10_39,cs-410,4,10,"00:02:56,080","00:02:58,880",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,And this has to do with whether
cs-410_4_10_40,cs-410,4,10,"00:02:58,880","00:03:02,370",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,can explain the content
cs-410_4_10_41,cs-410,4,10,"00:03:02,370","00:03:07,730",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,And we want to pick a topic
cs-410_4_10_42,cs-410,4,10,"00:03:07,730","00:03:10,810",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,"So more specifically,"
cs-410_4_10_43,cs-410,4,10,"00:03:10,810","00:03:14,650",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,then choose which topic
cs-410_4_10_44,cs-410,4,10,"00:03:14,650","00:03:18,740",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,"So more rigorously,"
cs-410_4_10_45,cs-410,4,10,"00:03:18,740","00:03:22,630",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,So we're going to choose
cs-410_4_10_46,cs-410,4,10,"00:03:22,630","00:03:27,660",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,This posterior probability at the top
cs-410_4_10_47,cs-410,4,10,"00:03:27,660","00:03:33,020",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,"because this one,"
cs-410_4_10_48,cs-410,4,10,"00:03:33,020","00:03:36,810",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,That's our belief about
cs-410_4_10_49,cs-410,4,10,"00:03:36,810","00:03:39,470",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,before we observe any document.
cs-410_4_10_50,cs-410,4,10,"00:03:39,470","00:03:43,320",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,But this conditional probability here is
cs-410_4_10_51,cs-410,4,10,"00:03:43,320","00:03:47,850",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,the posterior probability of the topic
cs-410_4_10_52,cs-410,4,10,"00:03:49,758","00:03:54,130",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,And base wall allows us to update this
cs-410_4_10_53,cs-410,4,10,"00:03:54,130","00:03:59,040",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"I have shown the details,"
cs-410_4_10_54,cs-410,4,10,"00:03:59,040","00:04:04,792",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,the prior here is related to
cs-410_4_10_55,cs-410,4,10,"00:04:05,960","00:04:10,870",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,And this is related to how
cs-410_4_10_56,cs-410,4,10,"00:04:10,870","00:04:16,200",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,"explains the document here, and"
cs-410_4_10_57,cs-410,4,10,"00:04:16,200","00:04:21,470",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,So to find the topic that has the higher
cs-410_4_10_58,cs-410,4,10,"00:04:21,470","00:04:26,930",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,posterior probability here it's
cs-410_4_10_59,cs-410,4,10,"00:04:26,930","00:04:30,450",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"as we have seen also,"
cs-410_4_10_60,cs-410,4,10,"00:04:32,300","00:04:37,159",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,And we can then change the probability
cs-410_4_10_61,cs-410,4,10,"00:04:37,159","00:04:42,019",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"the probability of each word, and"
cs-410_4_10_62,cs-410,4,10,"00:04:42,019","00:04:46,382",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,an assumption about independence
cs-410_4_10_63,cs-410,4,10,"00:04:46,382","00:04:49,640",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,So this is just something that you
cs-410_4_10_64,cs-410,4,10,"00:04:50,680","00:04:56,760",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,And we now can see clearly how we
cs-410_4_10_65,cs-410,4,10,"00:04:56,760","00:05:02,270",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,based on the information
cs-410_4_10_66,cs-410,4,10,"00:05:02,270","00:05:05,740",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,these categories and
cs-410_4_10_67,cs-410,4,10,"00:05:05,740","00:05:10,690",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,So this idea can be directly
cs-410_4_10_68,cs-410,4,10,"00:05:10,690","00:05:16,360",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,And this is precisely what
cs-410_4_10_69,cs-410,4,10,"00:05:16,360","00:05:19,290",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,So here it's most really
cs-410_4_10_70,cs-410,4,10,"00:05:19,290","00:05:21,910",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,that we're looking at
cs-410_4_10_71,cs-410,4,10,"00:05:21,910","00:05:26,620",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,So we assume that if theta i
cs-410_4_10_72,cs-410,4,10,"00:05:26,620","00:05:31,770",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,"represents category i accurately,"
cs-410_4_10_73,cs-410,4,10,"00:05:31,770","00:05:37,120",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,characterizes the content of
cs-410_4_10_74,cs-410,4,10,"00:05:37,120","00:05:40,802",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"Then, what we can do is precisely"
cs-410_4_10_75,cs-410,4,10,"00:05:40,802","00:05:45,580",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,Namely we're going to assign
cs-410_4_10_76,cs-410,4,10,"00:05:45,580","00:05:50,310",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,that has the highest probability
cs-410_4_10_77,cs-410,4,10,"00:05:50,310","00:05:54,990",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,"In other words, we're going to maximize"
cs-410_4_10_78,cs-410,4,10,"00:05:56,620","00:05:59,910",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,And this is related to the prior and
cs-410_4_10_79,cs-410,4,10,"00:05:59,910","00:06:04,290",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,the [INAUDIBLE] as you have
cs-410_4_10_80,cs-410,4,10,"00:06:04,290","00:06:07,400",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"And so, naturally we can decompose"
cs-410_4_10_81,cs-410,4,10,"00:06:07,400","00:06:11,840",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,this [INAUDIBLE] into
cs-410_4_10_82,cs-410,4,10,"00:06:11,840","00:06:16,770",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"Now, here, I change the notation so"
cs-410_4_10_83,cs-410,4,10,"00:06:16,770","00:06:20,970",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,product of all the words
cs-410_4_10_84,cs-410,4,10,"00:06:20,970","00:06:25,730",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,even though the document
cs-410_4_10_85,cs-410,4,10,"00:06:25,730","00:06:31,810",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,And the product is still accurately
cs-410_4_10_86,cs-410,4,10,"00:06:31,810","00:06:35,580",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,of all the words in the document
cs-410_4_10_87,cs-410,4,10,"00:06:37,150","00:06:39,120",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,"When a word,"
cs-410_4_10_88,cs-410,4,10,"00:06:39,120","00:06:42,840",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"The count would be 0, so"
cs-410_4_10_89,cs-410,4,10,"00:06:42,840","00:06:48,380",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So if actively we'll just have the product
cs-410_4_10_90,cs-410,4,10,"00:06:48,380","00:06:50,940",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"So basically, with Naive Bayes Classifier,"
cs-410_4_10_91,cs-410,4,10,"00:06:50,940","00:06:55,680",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,we're going to score each category for
cs-410_4_10_92,cs-410,4,10,"00:06:56,850","00:07:02,390",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,"Now, you may notice that here it involves"
cs-410_4_10_93,cs-410,4,10,"00:07:02,390","00:07:06,480",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,And this can cause and the four problem.
cs-410_4_10_94,cs-410,4,10,"00:07:06,480","00:07:10,269",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,So one way to solve the problem is
cs-410_4_10_95,cs-410,4,10,"00:07:10,269","00:07:13,530",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,which it doesn't changes all
cs-410_4_10_96,cs-410,4,10,"00:07:13,530","00:07:15,732",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,But will helps us preserve precision.
cs-410_4_10_97,cs-410,4,10,"00:07:15,732","00:07:20,519",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"And so, this is often the function"
cs-410_4_10_98,cs-410,4,10,"00:07:20,519","00:07:24,611",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,score each category and
cs-410_4_10_99,cs-410,4,10,"00:07:24,611","00:07:30,300",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,the category that has the highest
cs-410_4_10_100,cs-410,4,10,"00:07:30,300","00:07:34,870",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,So this is called an Naive Bayes
cs-410_4_10_101,cs-410,4,10,"00:07:34,870","00:07:39,535",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,understandable because we are applying
cs-410_4_10_102,cs-410,4,10,"00:07:39,535","00:07:46,380",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,the posterior probability of the topic to
cs-410_4_10_103,cs-410,4,10,"00:07:47,560","00:07:52,337",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,"Now, it's also called a naive because"
cs-410_4_10_104,cs-410,4,10,"00:07:52,337","00:07:56,553",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,in the document is generated
cs-410_4_10_105,cs-410,4,10,"00:07:56,553","00:08:00,932",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,assumption because in reality they're
cs-410_4_10_106,cs-410,4,10,"00:08:00,932","00:08:05,241",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,"Once you see some word,"
cs-410_4_10_107,cs-410,4,10,"00:08:05,241","00:08:08,235",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,"For example,"
cs-410_4_10_108,cs-410,4,10,"00:08:08,235","00:08:09,598",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"Than that mixed category,"
cs-410_4_10_109,cs-410,4,10,"00:08:09,598","00:08:13,640",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,they see more clustering more likely to
cs-410_4_10_110,cs-410,4,10,"00:08:15,490","00:08:18,740",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,But this assumption allows
cs-410_4_10_111,cs-410,4,10,"00:08:18,740","00:08:22,854",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,And it's actually quite effective for
cs-410_4_10_112,cs-410,4,10,"00:08:22,854","00:08:26,370",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,But you should know that
cs-410_4_10_113,cs-410,4,10,"00:08:26,370","00:08:29,000",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,have to make this assumption.
cs-410_4_10_114,cs-410,4,10,"00:08:29,000","00:08:33,760",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,"We could for example, assume that"
cs-410_4_10_115,cs-410,4,10,"00:08:33,760","00:08:38,411",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,So that would make it a bigram analogy
cs-410_4_10_116,cs-410,4,10,"00:08:38,411","00:08:43,019",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,And of course you can even use a mixture
cs-410_4_10_117,cs-410,4,10,"00:08:43,019","00:08:45,120",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,like in each category.
cs-410_4_10_118,cs-410,4,10,"00:08:45,120","00:08:49,530",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,"So in nature, they will be all using"
cs-410_4_10_119,cs-410,4,10,"00:08:49,530","00:08:54,760",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,But the actual generating model for
cs-410_4_10_120,cs-410,4,10,"00:08:54,760","00:08:59,670",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,"And here, we just talk about very"
cs-410_4_10_121,cs-410,4,10,"00:09:00,980","00:09:05,220",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,"So now the question is,"
cs-410_4_10_122,cs-410,4,10,"00:09:05,220","00:09:09,520",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,actually represents category i accurately?
cs-410_4_10_123,cs-410,4,10,"00:09:09,520","00:09:13,700",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,"Now in clustering,"
cs-410_4_10_124,cs-410,4,10,"00:09:13,700","00:09:17,310",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,what are the distributions for
cs-410_4_10_125,cs-410,4,10,"00:09:17,310","00:09:20,820",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,"But in our case,"
cs-410_4_10_126,cs-410,4,10,"00:09:20,820","00:09:24,940",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,this theta i represents indeed category i.
cs-410_4_10_127,cs-410,4,10,"00:09:25,960","00:09:27,510",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,"Well if you think about the question, and"
cs-410_4_10_128,cs-410,4,10,"00:09:27,510","00:09:33,150",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,you likely come up with the idea
cs-410_4_10_129,cs-410,4,10,"00:09:34,800","00:09:36,050",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"Indeed in the textbook,"
cs-410_4_10_130,cs-410,4,10,"00:09:36,050","00:09:40,140",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,we typically assume that there
cs-410_4_10_131,cs-410,4,10,"00:09:40,140","00:09:47,810",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,those are the documents that unknown
cs-410_4_10_132,cs-410,4,10,"00:09:47,810","00:09:51,680",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,"In other words, these are the documents"
cs-410_4_10_133,cs-410,4,10,"00:09:51,680","00:09:54,450",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,of course human experts must do that.
cs-410_4_10_134,cs-410,4,10,"00:09:54,450","00:09:58,960",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,"In here, you see that T1"
cs-410_4_10_135,cs-410,4,10,"00:09:58,960","00:10:03,020",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,that are known to have
cs-410_4_10_136,cs-410,4,10,"00:10:03,020","00:10:07,187",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,And T2 represents the documents
cs-410_4_10_137,cs-410,4,10,"00:10:07,187","00:10:09,699",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,"generated from category 2, etc."
cs-410_4_10_138,cs-410,4,10,"00:10:09,699","00:10:14,475",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,"Now if you look at this picture,"
cs-410_4_10_139,cs-410,4,10,"00:10:14,475","00:10:18,872",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,here is really a simplified
cs-410_4_10_140,cs-410,4,10,"00:10:18,872","00:10:20,452",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,"It's no longer mixed modal, why?"
cs-410_4_10_141,cs-410,4,10,"00:10:20,452","00:10:25,350",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,Because we already know which distribution
cs-410_4_10_142,cs-410,4,10,"00:10:25,350","00:10:29,820",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,"There's no uncertainty here, there's"
cs-410_4_10_143,cs-410,4,10,"00:10:30,980","00:10:35,110",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,So the estimation problem of
cs-410_4_10_144,cs-410,4,10,"00:10:35,110","00:10:38,720",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,"But in general,"
cs-410_4_10_145,cs-410,4,10,"00:10:38,720","00:10:42,380",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,estimate these probabilities
cs-410_4_10_146,cs-410,4,10,"00:10:42,380","00:10:46,799",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,And what other probability is that we have
cs-410_4_10_147,cs-410,4,10,"00:10:46,799","00:10:48,553",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,Well there are two kinds.
cs-410_4_10_148,cs-410,4,10,"00:10:48,553","00:10:53,114",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,"So one is the prior,"
cs-410_4_10_149,cs-410,4,10,"00:10:53,114","00:10:57,349",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,this indicates how popular
cs-410_4_10_150,cs-410,4,10,"00:10:57,349","00:11:03,224",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,how likely will it have observed
cs-410_4_10_151,cs-410,4,10,"00:11:03,224","00:11:05,990",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,The other kind is
cs-410_4_10_152,cs-410,4,10,"00:11:05,990","00:11:10,300",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,we want to know what words have high
cs-410_4_10_153,cs-410,4,10,"00:11:11,690","00:11:15,570",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,So the idea then is to just
cs-410_4_10_154,cs-410,4,10,"00:11:15,570","00:11:17,810",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,to estimate these two probabilities.
cs-410_4_10_155,cs-410,4,10,"00:11:18,830","00:11:23,261",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,"And in general, we can do this"
cs-410_4_10_156,cs-410,4,10,"00:11:23,261","00:11:27,650",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,That's just because these documents
cs-410_4_10_157,cs-410,4,10,"00:11:27,650","00:11:29,565",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,from a specific category.
cs-410_4_10_158,cs-410,4,10,"00:11:29,565","00:11:30,825",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,"So once we know that,"
cs-410_4_10_159,cs-410,4,10,"00:11:30,825","00:11:35,660",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,it's in some sense irrelevant of what
cs-410_4_10_160,cs-410,4,10,"00:11:37,470","00:11:41,737",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,So now this is a statistical
cs-410_4_10_161,cs-410,4,10,"00:11:41,737","00:11:44,209",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,We have observed some
cs-410_4_10_162,cs-410,4,10,"00:11:44,209","00:11:47,220",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,we want to guess
cs-410_4_10_163,cs-410,4,10,"00:11:47,220","00:11:49,580",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,We want to take our best
cs-410_4_10_164,cs-410,4,10,"00:11:51,060","00:11:56,073",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,And this is a problem that we have seen
cs-410_4_10_165,cs-410,4,10,"00:11:56,073","00:11:58,728",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=716,"Now, if you haven't thought"
cs-410_4_10_166,cs-410,4,10,"00:11:58,728","00:12:00,775",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,haven't seen life based classifier.
cs-410_4_10_167,cs-410,4,10,"00:12:00,775","00:12:04,649",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=720,It would be very useful for
cs-410_4_10_168,cs-410,4,10,"00:12:04,649","00:12:07,690",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,to think about how to solve this problem.
cs-410_4_10_169,cs-410,4,10,"00:12:07,690","00:12:10,680",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,So let me state the problem again.
cs-410_4_10_170,cs-410,4,10,"00:12:10,680","00:12:13,310",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,"So let's just think about with category 1,"
cs-410_4_10_171,cs-410,4,10,"00:12:13,310","00:12:18,750",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,we know there is one word of distribution
cs-410_4_10_172,cs-410,4,10,"00:12:18,750","00:12:23,110",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,And we generate each word in the document
cs-410_4_10_173,cs-410,4,10,"00:12:23,110","00:12:29,350",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,observed a set of n sub 1
cs-410_4_10_174,cs-410,4,10,"00:12:29,350","00:12:32,980",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,These documents have been all
cs-410_4_10_175,cs-410,4,10,"00:12:32,980","00:12:37,500",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,Namely have been all generated
cs-410_4_10_176,cs-410,4,10,"00:12:37,500","00:12:40,420",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,"Now the question is,"
cs-410_4_10_177,cs-410,4,10,"00:12:40,420","00:12:44,369",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=760,estimate of the probability of
cs-410_4_10_178,cs-410,4,10,"00:12:44,369","00:12:49,710",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,And what would be your guess of
cs-410_4_10_179,cs-410,4,10,"00:12:49,710","00:12:52,200",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,"Of course,"
cs-410_4_10_180,cs-410,4,10,"00:12:52,200","00:12:54,840",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=772,how likely are you to see
cs-410_4_10_181,cs-410,4,10,"00:12:55,990","00:13:00,970",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,"So think for a moment, how do you"
cs-410_4_10_182,cs-410,4,10,"00:13:00,970","00:13:06,430",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,all these documents that are known
cs-410_4_10_183,cs-410,4,10,"00:13:06,430","00:13:08,950",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,to estimate all these parameters?
cs-410_4_10_184,cs-410,4,10,"00:13:08,950","00:13:11,280",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,"Now, if you spend some time"
cs-410_4_10_185,cs-410,4,10,"00:13:11,280","00:13:15,770",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=791,it would help you understand
cs-410_4_10_186,cs-410,4,10,"00:13:15,770","00:13:20,390",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=795,So do spend some time to make sure that
cs-410_4_10_187,cs-410,4,10,"00:13:20,390","00:13:23,310",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,do you best to solve the problem yourself.
cs-410_4_10_188,cs-410,4,10,"00:13:23,310","00:13:28,270",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,Now if you have thought about and
cs-410_4_10_189,cs-410,4,10,"00:13:29,400","00:13:35,880",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=809,"First, what's the bases for estimating the"
cs-410_4_10_190,cs-410,4,10,"00:13:35,880","00:13:40,080",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,Well this has to do with whether you
cs-410_4_10_191,cs-410,4,10,"00:13:40,080","00:13:41,625",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=820,form that category.
cs-410_4_10_192,cs-410,4,10,"00:13:41,625","00:13:44,870",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=821,"Intuitively, you have seen a lot"
cs-410_4_10_193,cs-410,4,10,"00:13:44,870","00:13:46,770",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,very few in medical science.
cs-410_4_10_194,cs-410,4,10,"00:13:46,770","00:13:51,870",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=826,Then you guess is that the probability
cs-410_4_10_195,cs-410,4,10,"00:13:51,870","00:13:55,800",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=831,your prior on the category
cs-410_4_10_196,cs-410,4,10,"00:13:57,130","00:14:01,570",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,And what about the basis for estimating
cs-410_4_10_197,cs-410,4,10,"00:14:01,570","00:14:05,929",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=841,"Well the same, and you'll be just"
cs-410_4_10_198,cs-410,4,10,"00:14:05,929","00:14:10,645",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=845,frequently in the documents that are known
cs-410_4_10_199,cs-410,4,10,"00:14:10,645","00:14:12,799",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,likely have a higher probability.
cs-410_4_10_200,cs-410,4,10,"00:14:12,799","00:14:15,493",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=852,And that's just a maximum
cs-410_4_10_201,cs-410,4,10,"00:14:15,493","00:14:20,707",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,"Indeed, that's what we can do, so this"
cs-410_4_10_202,cs-410,4,10,"00:14:20,707","00:14:24,990",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=860,"to answer the question,"
cs-410_4_10_203,cs-410,4,10,"00:14:24,990","00:14:31,210",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=864,"Then we can simply normalize,"
cs-410_4_10_204,cs-410,4,10,"00:14:31,210","00:14:36,470",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=871,So here you see N sub i denotes
cs-410_4_10_205,cs-410,4,10,"00:14:37,950","00:14:41,313",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,And we simply just normalize these
cs-410_4_10_206,cs-410,4,10,"00:14:41,313","00:14:46,929",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=881,"In other words, we make this"
cs-410_4_10_207,cs-410,4,10,"00:14:46,929","00:14:52,960",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,of training intercept in each category
cs-410_4_10_208,cs-410,4,10,"00:14:55,260","00:14:58,010",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=895,Now what about the word distribution?
cs-410_4_10_209,cs-410,4,10,"00:14:58,010","00:14:59,460",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=898,"Well, we do the same."
cs-410_4_10_210,cs-410,4,10,"00:14:59,460","00:15:03,940",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,Again this time we can do this for
cs-410_4_10_211,cs-410,4,10,"00:15:03,940","00:15:08,640",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=903,"So let's say,"
cs-410_4_10_212,cs-410,4,10,"00:15:08,640","00:15:12,480",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=908,So which word has a higher probability?
cs-410_4_10_213,cs-410,4,10,"00:15:12,480","00:15:15,850",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=912,"Well, we simply count the word occurrences"
cs-410_4_10_214,cs-410,4,10,"00:15:15,850","00:15:19,140",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=915,in the documents that are known
cs-410_4_10_215,cs-410,4,10,"00:15:20,290","00:15:25,516",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=920,And then we put together all
cs-410_4_10_216,cs-410,4,10,"00:15:25,516","00:15:30,565",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=925,And then we just normalize these
cs-410_4_10_217,cs-410,4,10,"00:15:30,565","00:15:35,660",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=930,of all the words make all
cs-410_4_10_218,cs-410,4,10,"00:15:35,660","00:15:39,000",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=935,"So in this case, you're going to see this"
cs-410_4_10_219,cs-410,4,10,"00:15:39,000","00:15:43,870",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,the word in the collection of
cs-410_4_10_220,cs-410,4,10,"00:15:43,870","00:15:48,380",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,that's denoted by c of w and T sub i.
cs-410_4_10_221,cs-410,4,10,"00:15:49,710","00:15:55,110",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=949,"Now, you may notice that we"
cs-410_4_10_222,cs-410,4,10,"00:15:55,110","00:16:01,715",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=955,estimate in the form of being
cs-410_4_10_223,cs-410,4,10,"00:16:01,715","00:16:03,529",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=961,"And this is often sufficient,"
cs-410_4_10_224,cs-410,4,10,"00:16:03,529","00:16:07,033",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=963,because we have some constraints
cs-410_4_10_225,cs-410,4,10,"00:16:07,033","00:16:11,281",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,So the normalizer is
cs-410_4_10_226,cs-410,4,10,"00:16:11,281","00:16:15,191",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,"So in this case, it will be useful for"
cs-410_4_10_227,cs-410,4,10,"00:16:15,191","00:16:19,711",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=975,what are the constraints on these
cs-410_4_10_228,cs-410,4,10,"00:16:19,711","00:16:22,753",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=979,So once you figure out
cs-410_4_10_229,cs-410,4,10,"00:16:22,753","00:16:25,410",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=982,you will know how to
cs-410_4_10_230,cs-410,4,10,"00:16:25,410","00:16:32,940",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=985,And so this is a good exercise to
cs-410_4_10_231,cs-410,4,10,"00:16:32,940","00:16:35,940",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=992,There is another issue in
cs-410_4_10_232,cs-410,4,10,"00:16:35,940","00:16:41,720",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=995,In fact the smoothing is a general problem
cs-410_4_10_233,cs-410,4,10,"00:16:41,720","00:16:43,420",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1001,"And this has to do with,"
cs-410_4_10_234,cs-410,4,10,"00:16:43,420","00:16:47,540",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1003,what would happen if you have
cs-410_4_10_235,cs-410,4,10,"00:16:47,540","00:16:51,590",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1007,So smoothing is an important technique
cs-410_4_10_236,cs-410,4,10,"00:16:51,590","00:16:56,620",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1011,"In our case, the training data can be"
cs-410_4_10_237,cs-410,4,10,"00:16:56,620","00:17:01,140",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1016,we use maximum likely estimator we often
cs-410_4_10_238,cs-410,4,10,"00:17:01,140","00:17:03,770",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1021,That means if an event is not observed
cs-410_4_10_239,cs-410,4,10,"00:17:03,770","00:17:06,440",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1023,then the estimated
cs-410_4_10_240,cs-410,4,10,"00:17:06,440","00:17:11,070",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1026,"In this case, if we have not seen"
cs-410_4_10_241,cs-410,4,10,"00:17:11,070","00:17:13,590",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1031,"let's say, category i."
cs-410_4_10_242,cs-410,4,10,"00:17:13,590","00:17:18,770",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1033,Then our estimator would be zero for the
cs-410_4_10_243,cs-410,4,10,"00:17:18,770","00:17:20,800",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1038,and this is generally not accurate.
cs-410_4_10_244,cs-410,4,10,"00:17:20,800","00:17:25,380",244,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1040,So we have to do smoothing to make
cs-410_4_10_245,cs-410,4,10,"00:17:25,380","00:17:30,990",245,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1045,The other reason for smoothing is that
cs-410_4_10_246,cs-410,4,10,"00:17:30,990","00:17:35,330",246,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1050,and this is also generally true for
cs-410_4_10_247,cs-410,4,10,"00:17:35,330","00:17:37,346",247,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1055,"When the data set is small,"
cs-410_4_10_248,cs-410,4,10,"00:17:37,346","00:17:41,827",248,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1057,we tend to rely on some prior
cs-410_4_10_249,cs-410,4,10,"00:17:41,827","00:17:46,242",249,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1061,So in this case our [INAUDIBLE] says that
cs-410_4_10_250,cs-410,4,10,"00:17:46,242","00:17:51,035",250,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1066,So smoothing allows us to inject
cs-410_4_10_251,cs-410,4,10,"00:17:51,035","00:17:53,810",251,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1071,order has a real zero probability.
cs-410_4_10_252,cs-410,4,10,"00:17:54,970","00:17:59,230",252,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1074,There is also a third reason which
cs-410_4_10_253,cs-410,4,10,"00:17:59,230","00:18:00,850",253,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1079,we explain that in a moment.
cs-410_4_10_254,cs-410,4,10,"00:18:00,850","00:18:05,480",254,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1080,And that is to help achieve
cs-410_4_10_255,cs-410,4,10,"00:18:05,480","00:18:08,160",255,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1085,"And this is also called IDF weighting,"
cs-410_4_10_256,cs-410,4,10,"00:18:08,160","00:18:13,640",256,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1088,inverse document frequency weighting that
cs-410_4_10_257,cs-410,4,10,"00:18:14,740","00:18:15,850",257,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1094,So how do we do smoothing?
cs-410_4_10_258,cs-410,4,10,"00:18:15,850","00:18:19,220",258,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1095,Well in general we add pseudo
cs-410_4_10_259,cs-410,4,10,"00:18:19,220","00:18:21,607",259,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1099,we'll make sure that no event has 0 count.
cs-410_4_10_260,cs-410,4,10,"00:18:22,790","00:18:27,676",260,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1102,So one possible way of smoothing
cs-410_4_10_261,cs-410,4,10,"00:18:27,676","00:18:32,483",261,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1107,is to simply add a small non active
cs-410_4_10_262,cs-410,4,10,"00:18:32,483","00:18:37,413",262,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1112,Let's pretend that every category
cs-410_4_10_263,cs-410,4,10,"00:18:37,413","00:18:39,880",263,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1117,documents represented by delta.
cs-410_4_10_264,cs-410,4,10,"00:18:40,990","00:18:45,660",264,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1120,And in the denominator we also add
cs-410_4_10_265,cs-410,4,10,"00:18:45,660","00:18:48,730",265,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1125,we want the probability to some to 1.
cs-410_4_10_266,cs-410,4,10,"00:18:48,730","00:18:54,427",266,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1128,So in total we've added delta k times
cs-410_4_10_267,cs-410,4,10,"00:18:54,427","00:18:59,242",267,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1134,"Therefore in this sum,"
cs-410_4_10_268,cs-410,4,10,"00:18:59,242","00:19:04,490",268,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1139,delta as a total pseudocount
cs-410_4_10_269,cs-410,4,10,"00:19:06,420","00:19:09,568",269,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1146,"Now, it's interesting to think"
cs-410_4_10_270,cs-410,4,10,"00:19:09,568","00:19:11,678",270,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1149,obvious data is a smoothing
cs-410_4_10_271,cs-410,4,10,"00:19:11,678","00:19:16,285",271,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1151,Meaning that the larger data is and
cs-410_4_10_272,cs-410,4,10,"00:19:16,285","00:19:19,505",272,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1156,that means we'll more
cs-410_4_10_273,cs-410,4,10,"00:19:19,505","00:19:24,238",273,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1159,And we might indeed ignore the actual
cs-410_4_10_274,cs-410,4,10,"00:19:24,238","00:19:25,587",274,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1164,set to infinity.
cs-410_4_10_275,cs-410,4,10,"00:19:25,587","00:19:30,172",275,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1165,Imagine what would happen if there
cs-410_4_10_276,cs-410,4,10,"00:19:30,172","00:19:39,270",276,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1170,"Well, we are going to say every category"
cs-410_4_10_277,cs-410,4,10,"00:19:39,270","00:19:43,300",277,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1179,And then there's no distinction to them so
cs-410_4_10_278,cs-410,4,10,"00:19:44,850","00:19:46,200",278,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1184,What if delta is 0?
cs-410_4_10_279,cs-410,4,10,"00:19:46,200","00:19:51,050",279,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1186,"Well, we just go back to the original"
cs-410_4_10_280,cs-410,4,10,"00:19:51,050","00:19:54,880",280,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1191,data to estimate to estimate
cs-410_4_10_281,cs-410,4,10,"00:19:54,880","00:19:57,610",281,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1194,Now we can do the same for
cs-410_4_10_282,cs-410,4,10,"00:19:57,610","00:20:01,670",282,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1197,"But in this case,"
cs-410_4_10_283,cs-410,4,10,"00:20:01,670","00:20:05,750",283,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1201,to use a nonuniform seudocount for
cs-410_4_10_284,cs-410,4,10,"00:20:05,750","00:20:09,372",284,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1205,So here you'll see we'll add
cs-410_4_10_285,cs-410,4,10,"00:20:09,372","00:20:12,143",285,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1209,that's mule multiplied
cs-410_4_10_286,cs-410,4,10,"00:20:12,143","00:20:15,781",286,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1212,the word given by a background
cs-410_4_10_287,cs-410,4,10,"00:20:15,781","00:20:19,772",287,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1215,Now that background model in
cs-410_4_10_288,cs-410,4,10,"00:20:19,772","00:20:22,070",288,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1219,a logic collection of tests.
cs-410_4_10_289,cs-410,4,10,"00:20:22,070","00:20:26,588",289,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1222,Or in this case we will use the whole
cs-410_4_10_290,cs-410,4,10,"00:20:26,588","00:20:29,693",290,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1226,estimate this background language model.
cs-410_4_10_291,cs-410,4,10,"00:20:29,693","00:20:31,494",291,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1229,"But we don't have to use this one,"
cs-410_4_10_292,cs-410,4,10,"00:20:31,494","00:20:35,050",292,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1231,we can use larger test data that
cs-410_4_10_293,cs-410,4,10,"00:20:36,170","00:20:40,110",293,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1236,Now if we use such a background
cs-410_4_10_294,cs-410,4,10,"00:20:40,110","00:20:43,605",294,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1240,we'll find that some words will
cs-410_4_10_295,cs-410,4,10,"00:20:43,605","00:20:44,848",295,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1243,So what are those words?
cs-410_4_10_296,cs-410,4,10,"00:20:44,848","00:20:48,580",296,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1244,Well those are the common words
cs-410_4_10_297,cs-410,4,10,"00:20:48,580","00:20:50,328",297,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1248,the background average model.
cs-410_4_10_298,cs-410,4,10,"00:20:50,328","00:20:53,314",298,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1250,So the pseudocounts added for
cs-410_4_10_299,cs-410,4,10,"00:20:53,314","00:20:59,126",299,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1253,Real words on the other hand
cs-410_4_10_300,cs-410,4,10,"00:20:59,126","00:21:03,443",300,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1259,Now this addition of background
cs-410_4_10_301,cs-410,4,10,"00:21:03,443","00:21:06,382",301,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1263,smoothing of these word distributions.
cs-410_4_10_302,cs-410,4,10,"00:21:06,382","00:21:10,630",302,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1266,We're going to bring the probability of
cs-410_4_10_303,cs-410,4,10,"00:21:10,630","00:21:12,880",303,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1270,because of the background model.
cs-410_4_10_304,cs-410,4,10,"00:21:12,880","00:21:17,769",304,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1272,Now this helps make the difference
cs-410_4_10_305,cs-410,4,10,"00:21:17,769","00:21:21,312",305,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1277,such words smaller across categories.
cs-410_4_10_306,cs-410,4,10,"00:21:21,312","00:21:26,005",306,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1281,Because every category has some help
cs-410_4_10_307,cs-410,4,10,"00:21:26,005","00:21:29,050",307,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1286,"I get the, a,"
cs-410_4_10_308,cs-410,4,10,"00:21:29,050","00:21:33,890",308,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1289,"Therefore, it's not always so"
cs-410_4_10_309,cs-410,4,10,"00:21:33,890","00:21:38,320",309,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1293,has documents that contain a lot
cs-410_4_10_310,cs-410,4,10,"00:21:38,320","00:21:41,600",310,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1298,the estimate is more influenced
cs-410_4_10_311,cs-410,4,10,"00:21:41,600","00:21:44,360",311,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1301,And the consequence is that
cs-410_4_10_312,cs-410,4,10,"00:21:44,360","00:21:48,420",312,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1304,such words tend not to influence
cs-410_4_10_313,cs-410,4,10,"00:21:48,420","00:21:53,590",313,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1308,as words that have small probabilities
cs-410_4_10_314,cs-410,4,10,"00:21:53,590","00:21:57,070",314,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1313,Those words don't get some help
cs-410_4_10_315,cs-410,4,10,"00:21:57,070","00:22:02,080",315,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1317,So the difference would be primary because
cs-410_4_10_316,cs-410,4,10,"00:22:02,080","00:22:04,089",316,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1322,in the training documents
cs-410_4_10_317,cs-410,4,10,"00:22:05,400","00:22:09,970",317,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1325,We also see another smoothing parameter
cs-410_4_10_318,cs-410,4,10,"00:22:09,970","00:22:13,610",318,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1329,smoothing and just like a delta does for
cs-410_4_10_319,cs-410,4,10,"00:22:14,920","00:22:19,353",319,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1334,And you can easily understand why we
cs-410_4_10_320,cs-410,4,10,"00:22:19,353","00:22:23,587",320,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1339,represents the sum of all the pseudocounts
cs-410_4_10_321,cs-410,4,10,"00:22:25,689","00:22:29,051",321,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1345,So view is also a non
cs-410_4_10_322,cs-410,4,10,"00:22:29,051","00:22:32,611",322,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1349,it's [INAUDIBLE] set to control smoothing.
cs-410_4_10_323,cs-410,4,10,"00:22:32,611","00:22:35,409",323,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1352,Now there are some interesting
cs-410_4_10_324,cs-410,4,10,"00:22:35,409","00:22:39,280",324,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1355,"First, let's think about when mu"
cs-410_4_10_325,cs-410,4,10,"00:22:39,280","00:22:41,319",325,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1359,Well in this case
cs-410_4_10_326,cs-410,4,10,"00:22:43,170","00:22:47,640",326,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1363,to the background language model we'll
cs-410_4_10_327,cs-410,4,10,"00:22:47,640","00:22:52,365",327,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1367,So we will bring every word distribution
cs-410_4_10_328,cs-410,4,10,"00:22:52,365","00:22:56,352",328,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1372,that essentially remove the difference
cs-410_4_10_329,cs-410,4,10,"00:22:56,352","00:22:57,943",329,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1376,"Obviously, we don't want to do that."
cs-410_4_10_330,cs-410,4,10,"00:22:57,943","00:23:02,683",330,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1377,The other special case is the thing
cs-410_4_10_331,cs-410,4,10,"00:23:02,683","00:23:06,919",331,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1382,"suppose, we actually set"
cs-410_4_10_332,cs-410,4,10,"00:23:06,919","00:23:10,218",332,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1386,"And let's say,"
cs-410_4_10_333,cs-410,4,10,"00:23:10,218","00:23:16,121",333,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1390,"So each one has the same probability,"
cs-410_4_10_334,cs-410,4,10,"00:23:16,121","00:23:22,030",334,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1396,going to be very similar to the one
cs-410_4_10_335,cs-410,4,10,"00:23:22,030","00:23:25,025",335,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1402,It's because we're going to add
cs-410_4_10_336,cs-410,4,10,"00:23:29,268","00:23:30,441",336,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1409,"So in general,"
cs-410_4_10_337,cs-410,4,10,"00:23:30,441","00:23:35,240",337,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1410,in Naive Bayes categorization we
cs-410_4_10_338,cs-410,4,10,"00:23:35,240","00:23:38,970",338,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1415,"And then once we have these probabilities,"
cs-410_4_10_339,cs-410,4,10,"00:23:38,970","00:23:42,960",339,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1418,then we can compute the score for
cs-410_4_10_340,cs-410,4,10,"00:23:42,960","00:23:44,030",340,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1422,For a document and
cs-410_4_10_341,cs-410,4,10,"00:23:44,030","00:23:47,240",341,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1424,then choose the category where it was
cs-410_4_10_342,cs-410,4,10,"00:23:49,250","00:23:53,266",342,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1429,"Now, it's useful to"
cs-410_4_10_343,cs-410,4,10,"00:23:53,266","00:23:57,863",343,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1433,the Naive Bayes scoring
cs-410_4_10_344,cs-410,4,10,"00:23:57,863","00:24:03,755",344,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1437,"So to understand that, and also to"
cs-410_4_10_345,cs-410,4,10,"00:24:03,755","00:24:10,029",345,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1443,will actually achieve the effect of IDF
cs-410_4_10_346,cs-410,4,10,"00:24:10,029","00:24:13,669",346,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1450,So suppose we have just two categories and
cs-410_4_10_347,cs-410,4,10,"00:24:13,669","00:24:19,395",347,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1453,we're going to score based on
cs-410_4_10_348,cs-410,4,10,"00:24:19,395","00:24:21,647",348,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1459,So this is the.
cs-410_4_10_349,cs-410,4,10,"00:24:24,563","00:24:29,723",349,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1464,Lets say this is our scoring function for
cs-410_4_10_350,cs-410,4,10,"00:24:29,723","00:24:33,072",350,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1469,"two categories, right?"
cs-410_4_10_351,cs-410,4,10,"00:24:33,072","00:24:40,100",351,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1473,"So, this is a score of a document for"
cs-410_4_10_352,cs-410,4,10,"00:24:40,100","00:24:44,409",352,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1480,And we're going to score based
cs-410_4_10_353,cs-410,4,10,"00:24:44,409","00:24:47,196",353,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1484,"So if the ratio is larger,"
cs-410_4_10_354,cs-410,4,10,"00:24:47,196","00:24:52,907",354,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1487,then it means it's more
cs-410_4_10_355,cs-410,4,10,"00:24:52,907","00:24:57,779",355,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1492,So the larger the score is the more likely
cs-410_4_10_356,cs-410,4,10,"00:24:57,779","00:25:01,800",356,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1497,the document is in category one.
cs-410_4_10_357,cs-410,4,10,"00:25:01,800","00:25:03,810",357,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1501,"So by using Bayes' rule,"
cs-410_4_10_358,cs-410,4,10,"00:25:03,810","00:25:08,150",358,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1503,"we can write down this ratio as follows,"
cs-410_4_10_359,cs-410,4,10,"00:25:09,190","00:25:15,920",359,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1509,"Now, we generally take logarithm of this"
cs-410_4_10_360,cs-410,4,10,"00:25:15,920","00:25:21,450",360,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1515,And this would then give us this
cs-410_4_10_361,cs-410,4,10,"00:25:21,450","00:25:23,520",361,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1521,And here we see something
cs-410_4_10_362,cs-410,4,10,"00:25:23,520","00:25:28,300",362,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1523,because this is our scoring function for
cs-410_4_10_363,cs-410,4,10,"00:25:30,280","00:25:34,718",363,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1530,"And if you look at this function,"
cs-410_4_10_364,cs-410,4,10,"00:25:34,718","00:25:38,860",364,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1534,The first part here is actually
cs-410_4_10_365,cs-410,4,10,"00:25:38,860","00:25:40,409",365,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1538,And so this is a category bias.
cs-410_4_10_366,cs-410,4,10,"00:25:41,870","00:25:43,740",366,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1541,It doesn't really depend on the document.
cs-410_4_10_367,cs-410,4,10,"00:25:43,740","00:25:48,780",367,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1543,It just says which category is more
cs-410_4_10_368,cs-410,4,10,"00:25:48,780","00:25:53,240",368,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1548,"this category slightly, right?"
cs-410_4_10_369,cs-410,4,10,"00:25:53,240","00:25:58,114",369,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1553,"So, the second part has a sum"
cs-410_4_10_370,cs-410,4,10,"00:25:58,114","00:26:03,590",370,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1558,"So, these are the words that"
cs-410_4_10_371,cs-410,4,10,"00:26:03,590","00:26:06,510",371,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1563,in general we can consider all
cs-410_4_10_372,cs-410,4,10,"00:26:06,510","00:26:08,960",372,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1566,So here we're going to
cs-410_4_10_373,cs-410,4,10,"00:26:08,960","00:26:12,280",373,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1568,"about which category is more likely,"
cs-410_4_10_374,cs-410,4,10,"00:26:12,280","00:26:16,920",374,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1572,So inside of the sum you can see
cs-410_4_10_375,cs-410,4,10,"00:26:16,920","00:26:19,056",375,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1576,"The first, is a count of the word."
cs-410_4_10_376,cs-410,4,10,"00:26:19,056","00:26:25,660",376,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1579,And this count of the word serves as
cs-410_4_10_377,cs-410,4,10,"00:26:27,080","00:26:30,390",377,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1587,And this is what we can
cs-410_4_10_378,cs-410,4,10,"00:26:30,390","00:26:33,645",378,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1590,The second part is
cs-410_4_10_379,cs-410,4,10,"00:26:33,645","00:26:37,236",379,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1593,"here it's the weight on which word, right?"
cs-410_4_10_380,cs-410,4,10,"00:26:37,236","00:26:42,500",380,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1597,This weight tells us to
cs-410_4_10_381,cs-410,4,10,"00:26:42,500","00:26:47,487",381,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1602,this word helps contribute in our decision
cs-410_4_10_382,cs-410,4,10,"00:26:47,487","00:26:51,930",382,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1607,to put this document in category one.
cs-410_4_10_383,cs-410,4,10,"00:26:51,930","00:26:54,495",383,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1611,"Now remember,"
cs-410_4_10_384,cs-410,4,10,"00:26:54,495","00:26:56,426",384,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1614,the more likely it's in category one.
cs-410_4_10_385,cs-410,4,10,"00:26:56,426","00:27:01,493",385,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1616,"Now if you look at this ratio, basically,"
cs-410_4_10_386,cs-410,4,10,"00:27:01,493","00:27:06,025",386,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1621,on the ratio of the probability of the
cs-410_4_10_387,cs-410,4,10,"00:27:06,025","00:27:09,492",387,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1626,Essentially we're comparing
cs-410_4_10_388,cs-410,4,10,"00:27:09,492","00:27:11,080",388,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1629,distributions.
cs-410_4_10_389,cs-410,4,10,"00:27:11,080","00:27:14,780",389,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1631,"And if it's a higher according to theta 1,"
cs-410_4_10_390,cs-410,4,10,"00:27:14,780","00:27:20,289",390,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1634,"then according to theta 2,"
cs-410_4_10_391,cs-410,4,10,"00:27:20,289","00:27:23,860",391,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1640,And therefore it means when
cs-410_4_10_392,cs-410,4,10,"00:27:23,860","00:27:27,940",392,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1643,we will say that it's more
cs-410_4_10_393,cs-410,4,10,"00:27:27,940","00:27:30,237",393,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1647,"And the more we observe such a word,"
cs-410_4_10_394,cs-410,4,10,"00:27:30,237","00:27:34,155",394,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1650,the more likely the document
cs-410_4_10_395,cs-410,4,10,"00:27:35,210","00:27:38,940",395,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1655,"If, on the other hand,"
cs-410_4_10_396,cs-410,4,10,"00:27:38,940","00:27:42,720",396,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1658,theta 1 is smaller than the probability
cs-410_4_10_397,cs-410,4,10,"00:27:42,720","00:27:45,220",397,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1662,then you can see that
cs-410_4_10_398,cs-410,4,10,"00:27:45,220","00:27:51,390",398,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1665,"Therefore, this is negative evidence for"
cs-410_4_10_399,cs-410,4,10,"00:27:51,390","00:27:54,010",399,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1671,That means the more we
cs-410_4_10_400,cs-410,4,10,"00:27:54,010","00:27:56,587",400,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1674,the more likely the document
cs-410_4_10_401,cs-410,4,10,"00:27:58,270","00:28:01,350",401,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1678,"So this formula now makes a little sense,"
cs-410_4_10_402,cs-410,4,10,"00:28:01,350","00:28:05,050",402,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1681,So we're going to aggregate all
cs-410_4_10_403,cs-410,4,10,"00:28:05,050","00:28:07,010",403,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1685,we take a sum of all the words.
cs-410_4_10_404,cs-410,4,10,"00:28:07,010","00:28:09,820",404,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1687,We can call this the features
cs-410_4_10_405,cs-410,4,10,"00:28:09,820","00:28:13,530",405,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1689,that we collected from the document
cs-410_4_10_406,cs-410,4,10,"00:28:13,530","00:28:18,550",406,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1693,And then each feature has
cs-410_4_10_407,cs-410,4,10,"00:28:19,660","00:28:24,815",407,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1699,does this feature support category one or
cs-410_4_10_408,cs-410,4,10,"00:28:24,815","00:28:30,465",408,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1704,And this is estimated as the log of
cs-410_4_10_409,cs-410,4,10,"00:28:32,315","00:28:35,565",409,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1712,And then finally we have
cs-410_4_10_410,cs-410,4,10,"00:28:35,565","00:28:39,555",410,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1715,So that formula actually
cs-410_4_10_411,cs-410,4,10,"00:28:39,555","00:28:43,862",411,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1719,be generalized to accommodate
cs-410_4_10_412,cs-410,4,10,"00:28:43,862","00:28:48,704",412,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1723,that's why I have introduce
cs-410_4_10_413,cs-410,4,10,"00:28:48,704","00:28:54,546",413,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1728,To introduce beta 0 to denote the Bayes
cs-410_4_10_414,cs-410,4,10,"00:28:54,546","00:28:58,269",414,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1734,beta sub i to denote
cs-410_4_10_415,cs-410,4,10,"00:28:58,269","00:29:03,154",415,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1738,"Now we do this generalisation,"
cs-410_4_10_416,cs-410,4,10,"00:29:03,154","00:29:08,815",416,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1743,general we can represent
cs-410_4_10_417,cs-410,4,10,"00:29:08,815","00:29:13,960",417,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1748,here of course in this case
cs-410_4_10_418,cs-410,4,10,"00:29:13,960","00:29:17,700",418,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1753,"But in general, we can put any features"
cs-410_4_10_419,cs-410,4,10,"00:29:17,700","00:29:18,760",419,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1757,categorization.
cs-410_4_10_420,cs-410,4,10,"00:29:18,760","00:29:20,650",420,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1758,"For example, document length or"
cs-410_4_10_421,cs-410,4,10,"00:29:20,650","00:29:25,720",421,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1760,font size or
cs-410_4_10_422,cs-410,4,10,"00:29:27,310","00:29:35,120",422,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1767,And then our scoring function can be
cs-410_4_10_423,cs-410,4,10,"00:29:35,120","00:29:40,430",423,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1775,the sum of the feature
cs-410_4_10_424,cs-410,4,10,"00:29:42,156","00:29:46,890",424,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1782,So if each f sub i is a feature
cs-410_4_10_425,cs-410,4,10,"00:29:46,890","00:29:51,140",425,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1786,"by the corresponding weight,"
cs-410_4_10_426,cs-410,4,10,"00:29:51,140","00:29:54,860",426,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1791,And this is the aggregate of all evidence
cs-410_4_10_427,cs-410,4,10,"00:29:54,860","00:29:56,360",427,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1794,features.
cs-410_4_10_428,cs-410,4,10,"00:29:56,360","00:29:57,960",428,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1796,And of course there are parameters here.
cs-410_4_10_429,cs-410,4,10,"00:29:57,960","00:29:59,060",429,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1797,So what are the parameters?
cs-410_4_10_430,cs-410,4,10,"00:29:59,060","00:30:00,510",430,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1799,"Well, these are the betas."
cs-410_4_10_431,cs-410,4,10,"00:30:00,510","00:30:02,690",431,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1800,These betas are weights.
cs-410_4_10_432,cs-410,4,10,"00:30:02,690","00:30:07,040",432,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1802,"And with a proper setting of the weights,"
cs-410_4_10_433,cs-410,4,10,"00:30:07,040","00:30:13,470",433,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1807,"to work well to classify documents,"
cs-410_4_10_434,cs-410,4,10,"00:30:13,470","00:30:16,770",434,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1813,We can clearly see naive Bayes
cs-410_4_10_435,cs-410,4,10,"00:30:16,770","00:30:18,760",435,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1816,this general classifier.
cs-410_4_10_436,cs-410,4,10,"00:30:18,760","00:30:23,940",436,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1818,"Actually, this general form is very close"
cs-410_4_10_437,cs-410,4,10,"00:30:23,940","00:30:28,800",437,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1823,"regression, and this is actually one"
cs-410_4_10_438,cs-410,4,10,"00:30:28,800","00:30:31,210",438,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1828,discriminative approaches
cs-410_4_10_439,cs-410,4,10,"00:30:32,340","00:30:36,980",439,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1832,And we're going to talk more
cs-410_4_10_440,cs-410,4,10,"00:30:36,980","00:30:40,600",440,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1836,here I want you to note that
cs-410_4_10_441,cs-410,4,10,"00:30:40,600","00:30:44,350",441,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1840,a close connection between
cs-410_4_10_442,cs-410,4,10,"00:30:44,350","00:30:48,930",442,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1844,And this slide shows how naive Bayes
cs-410_4_10_443,cs-410,4,10,"00:30:48,930","00:30:50,663",443,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1848,a logistic regression.
cs-410_4_10_444,cs-410,4,10,"00:30:50,663","00:30:55,692",444,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1850,And you can also see that in
cs-410_4_10_445,cs-410,4,10,"00:30:55,692","00:31:00,079",445,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1855,that tend to use more
cs-410_4_10_446,cs-410,4,10,"00:31:00,079","00:31:05,116",446,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1860,we can accommodate more
cs-410_4_10_447,cs-410,4,10,"00:31:05,116","00:31:15,116",447,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1865,[MUSIC]
cs-410_4_11_1,cs-410,4,11,"00:00:00,025","00:00:05,824",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[NOISE] This lecture is about the ordinal
cs-410_4_11_2,cs-410,4,11,"00:00:05,824","00:00:12,859",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=5,logistic regression for
cs-410_4_11_3,cs-410,4,11,"00:00:12,859","00:00:18,730",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"So, this is our problem set up for a"
cs-410_4_11_4,cs-410,4,11,"00:00:18,730","00:00:21,460",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,Or more specifically a rating prediction.
cs-410_4_11_5,cs-410,4,11,"00:00:21,460","00:00:27,430",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,We have an opinionated text document d as
cs-410_4_11_6,cs-410,4,11,"00:00:27,430","00:00:30,770",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,a rating in the range of 1 through k so
cs-410_4_11_7,cs-410,4,11,"00:00:30,770","00:00:37,110",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"it's a discrete rating, and"
cs-410_4_11_8,cs-410,4,11,"00:00:37,110","00:00:38,960",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,We have k categories here.
cs-410_4_11_9,cs-410,4,11,"00:00:38,960","00:00:40,330",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,Now we could use a regular text for
cs-410_4_11_10,cs-410,4,11,"00:00:40,330","00:00:42,950",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,categorization technique
cs-410_4_11_11,cs-410,4,11,"00:00:42,950","00:00:48,890",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,But such a solution would not consider the
cs-410_4_11_12,cs-410,4,11,"00:00:48,890","00:00:53,400",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"Intuitively, the features that can"
cs-410_4_11_13,cs-410,4,11,"00:00:53,400","00:00:58,380",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"or rather rating 2 from 1,"
cs-410_4_11_14,cs-410,4,11,"00:00:58,380","00:01:02,749",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,those that can distinguish k from k-1.
cs-410_4_11_15,cs-410,4,11,"00:01:02,749","00:01:08,610",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,"For example, positive words"
cs-410_4_11_16,cs-410,4,11,"00:01:08,610","00:01:11,950",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,When we train categorization
cs-410_4_11_17,cs-410,4,11,"00:01:11,950","00:01:16,670",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,problem by treating these categories as
cs-410_4_11_18,cs-410,4,11,"00:01:17,700","00:01:18,760",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,So what's the solution?
cs-410_4_11_19,cs-410,4,11,"00:01:18,760","00:01:23,830",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,Well in general we can order to classify
cs-410_4_11_20,cs-410,4,11,"00:01:23,830","00:01:26,330",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,And here we're going to
cs-410_4_11_21,cs-410,4,11,"00:01:26,330","00:01:29,030",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,called ordinal logistic regression.
cs-410_4_11_22,cs-410,4,11,"00:01:29,030","00:01:33,218",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,"Now, let's first think about how"
cs-410_4_11_23,cs-410,4,11,"00:01:33,218","00:01:34,410",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,a binary sentiment.
cs-410_4_11_24,cs-410,4,11,"00:01:34,410","00:01:36,460",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,A categorization problem.
cs-410_4_11_25,cs-410,4,11,"00:01:36,460","00:01:40,700",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,So suppose we just wanted to distinguish
cs-410_4_11_26,cs-410,4,11,"00:01:40,700","00:01:44,070",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,that is just a two category
cs-410_4_11_27,cs-410,4,11,"00:01:44,070","00:01:47,660",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,So the predictors are represented as X and
cs-410_4_11_28,cs-410,4,11,"00:01:47,660","00:01:50,080",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,And there are M features all together.
cs-410_4_11_29,cs-410,4,11,"00:01:50,080","00:01:52,390",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,The feature value is a real number.
cs-410_4_11_30,cs-410,4,11,"00:01:52,390","00:01:55,360",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,And this can be representation
cs-410_4_11_31,cs-410,4,11,"00:01:56,790","00:02:02,150",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,"And why it has two values,"
cs-410_4_11_32,cs-410,4,11,"00:02:02,150","00:02:04,940",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,"1 means X is positive,"
cs-410_4_11_33,cs-410,4,11,"00:02:04,940","00:02:09,940",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,And then of course this is a standard
cs-410_4_11_34,cs-410,4,11,"00:02:09,940","00:02:11,990",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,We can apply logistical regression.
cs-410_4_11_35,cs-410,4,11,"00:02:11,990","00:02:17,620",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,You may recall that in logistical
cs-410_4_11_36,cs-410,4,11,"00:02:17,620","00:02:22,820",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,"of probability that the Y is equal to one,"
cs-410_4_11_37,cs-410,4,11,"00:02:22,820","00:02:28,490",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,assumed to be a linear function
cs-410_4_11_38,cs-410,4,11,"00:02:28,490","00:02:35,000",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,So this would allow us to also write
cs-410_4_11_39,cs-410,4,11,"00:02:36,030","00:02:41,820",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,in this equation that you
cs-410_4_11_40,cs-410,4,11,"00:02:43,020","00:02:47,306",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,So that's a logistical function and
cs-410_4_11_41,cs-410,4,11,"00:02:47,306","00:02:52,421",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,you can see it relates
cs-410_4_11_42,cs-410,4,11,"00:02:52,421","00:02:57,970",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,probability that y=1
cs-410_4_11_43,cs-410,4,11,"00:02:57,970","00:03:02,960",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,And of course beta i's
cs-410_4_11_44,cs-410,4,11,"00:03:02,960","00:03:07,400",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,just a direct application of logistical
cs-410_4_11_45,cs-410,4,11,"00:03:08,790","00:03:11,730",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,"What if we have multiple categories,"
cs-410_4_11_46,cs-410,4,11,"00:03:11,730","00:03:16,600",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,Well we have to use such a binary
cs-410_4_11_47,cs-410,4,11,"00:03:16,600","00:03:20,000",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,to solve this multi
cs-410_4_11_48,cs-410,4,11,"00:03:21,170","00:03:26,215",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,And the idea is we can introduce
cs-410_4_11_49,cs-410,4,11,"00:03:26,215","00:03:29,790",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,In each case we asked
cs-410_4_11_50,cs-410,4,11,"00:03:29,790","00:03:35,210",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"whether the rating is j or above,"
cs-410_4_11_51,cs-410,4,11,"00:03:35,210","00:03:41,550",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"So when Yj is equal to 1,"
cs-410_4_11_52,cs-410,4,11,"00:03:41,550","00:03:44,080",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,"When it's 0,"
cs-410_4_11_53,cs-410,4,11,"00:03:45,360","00:03:51,520",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,So basically if we want to predict
cs-410_4_11_54,cs-410,4,11,"00:03:51,520","00:03:57,070",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,we first have one classifier to
cs-410_4_11_55,cs-410,4,11,"00:03:57,070","00:03:59,220",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,And that's our classifier one.
cs-410_4_11_56,cs-410,4,11,"00:03:59,220","00:04:02,275",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,And then we're going to have another
cs-410_4_11_57,cs-410,4,11,"00:04:02,275","00:04:05,850",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,At k-1 from the rest.
cs-410_4_11_58,cs-410,4,11,"00:04:05,850","00:04:06,700",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,That's Classifier 2.
cs-410_4_11_59,cs-410,4,11,"00:04:06,700","00:04:11,989",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"And in the end, we need a Classifier"
cs-410_4_11_60,cs-410,4,11,"00:04:11,989","00:04:16,281",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,So altogether we'll have k-1 classifiers.
cs-410_4_11_61,cs-410,4,11,"00:04:17,830","00:04:23,580",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,Now if we do that of course then
cs-410_4_11_62,cs-410,4,11,"00:04:23,580","00:04:27,750",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,and the logistical regression program
cs-410_4_11_63,cs-410,4,11,"00:04:27,750","00:04:30,910",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,as you have just seen
cs-410_4_11_64,cs-410,4,11,"00:04:30,910","00:04:33,760",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,Only that here we have more parameters.
cs-410_4_11_65,cs-410,4,11,"00:04:33,760","00:04:37,566",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"Because for each classifier,"
cs-410_4_11_66,cs-410,4,11,"00:04:37,566","00:04:41,889",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,So now the logistical regression
cs-410_4_11_67,cs-410,4,11,"00:04:41,889","00:04:44,780",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,which corresponds to a rating level.
cs-410_4_11_68,cs-410,4,11,"00:04:46,190","00:04:51,910",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,And I have also used of
cs-410_4_11_69,cs-410,4,11,"00:04:51,910","00:04:54,390",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,And this is to.
cs-410_4_11_70,cs-410,4,11,"00:04:54,390","00:04:57,451",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,"Make the notation more consistent,"
cs-410_4_11_71,cs-410,4,11,"00:04:57,451","00:05:02,800",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,than was what we can show in
cs-410_4_11_72,cs-410,4,11,"00:05:02,800","00:05:09,000",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,So here we now have basically k minus one
cs-410_4_11_73,cs-410,4,11,"00:05:09,000","00:05:12,380",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,Each has it's own set of parameters.
cs-410_4_11_74,cs-410,4,11,"00:05:12,380","00:05:18,349",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,"So now with this approach,"
cs-410_4_11_75,cs-410,4,11,"00:05:19,350","00:05:23,760",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,After we have trained these k-1
cs-410_4_11_76,cs-410,4,11,"00:05:23,760","00:05:30,160",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,"separately of course,"
cs-410_4_11_77,cs-410,4,11,"00:05:30,160","00:05:37,085",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,then invoke a classifier
cs-410_4_11_78,cs-410,4,11,"00:05:37,085","00:05:43,955",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,So first let look at the classifier
cs-410_4_11_79,cs-410,4,11,"00:05:43,955","00:05:49,010",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,So this classifier will tell
cs-410_4_11_80,cs-410,4,11,"00:05:49,010","00:05:54,230",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,have a rating of K or about.
cs-410_4_11_81,cs-410,4,11,"00:05:54,230","00:05:58,360",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,If probability according to this
cs-410_4_11_82,cs-410,4,11,"00:05:58,360","00:06:00,650",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,"larger than point five,"
cs-410_4_11_83,cs-410,4,11,"00:06:00,650","00:06:01,310",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,The rating is K.
cs-410_4_11_84,cs-410,4,11,"00:06:02,540","00:06:06,750",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,"Now, what if it's not as"
cs-410_4_11_85,cs-410,4,11,"00:06:06,750","00:06:10,020",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,"Well, that means the rating's below K,"
cs-410_4_11_86,cs-410,4,11,"00:06:11,050","00:06:13,750",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"So now,"
cs-410_4_11_87,cs-410,4,11,"00:06:13,750","00:06:17,530",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,which tells us whether
cs-410_4_11_88,cs-410,4,11,"00:06:18,690","00:06:20,660",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,It's at least K minus one.
cs-410_4_11_89,cs-410,4,11,"00:06:20,660","00:06:23,140",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,And if the probability is
cs-410_4_11_90,cs-410,4,11,"00:06:23,140","00:06:26,400",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,"then we'll say, well, then it's k-1."
cs-410_4_11_91,cs-410,4,11,"00:06:26,400","00:06:27,960",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,What if it says no?
cs-410_4_11_92,cs-410,4,11,"00:06:27,960","00:06:30,280",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,"Well, that means the rating"
cs-410_4_11_93,cs-410,4,11,"00:06:30,280","00:06:34,990",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,And so we're going to just keep
cs-410_4_11_94,cs-410,4,11,"00:06:34,990","00:06:41,340",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,And here we hit the end when we need
cs-410_4_11_95,cs-410,4,11,"00:06:41,340","00:06:43,510",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,So this would help us solve the problem.
cs-410_4_11_96,cs-410,4,11,"00:06:43,510","00:06:44,350",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,Right?
cs-410_4_11_97,cs-410,4,11,"00:06:44,350","00:06:49,320",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,So we can have a classifier that would
cs-410_4_11_98,cs-410,4,11,"00:06:49,320","00:06:51,120",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,in the range of 1 through k.
cs-410_4_11_99,cs-410,4,11,"00:06:51,120","00:06:55,510",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,Now unfortunately such a strategy is not
cs-410_4_11_100,cs-410,4,11,"00:06:55,510","00:07:01,661",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,And specifically there are two
cs-410_4_11_101,cs-410,4,11,"00:07:01,661","00:07:03,850",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,So these equations are the same as.
cs-410_4_11_102,cs-410,4,11,"00:07:03,850","00:07:05,110",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,You have seen before.
cs-410_4_11_103,cs-410,4,11,"00:07:06,250","00:07:10,100",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,Now the first problem is that there
cs-410_4_11_104,cs-410,4,11,"00:07:10,100","00:07:11,630",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,There are many parameters.
cs-410_4_11_105,cs-410,4,11,"00:07:11,630","00:07:15,540",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,"Now, can you count how many"
cs-410_4_11_106,cs-410,4,11,"00:07:15,540","00:07:18,680",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,Now this may be a interesting exercise.
cs-410_4_11_107,cs-410,4,11,"00:07:18,680","00:07:19,440",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,To do.
cs-410_4_11_108,cs-410,4,11,"00:07:19,440","00:07:24,250",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,you might want to just pause the video and
cs-410_4_11_109,cs-410,4,11,"00:07:24,250","00:07:27,060",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,How many parameters do I have for
cs-410_4_11_110,cs-410,4,11,"00:07:28,580","00:07:30,280",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,And how many classifiers do we have?
cs-410_4_11_111,cs-410,4,11,"00:07:31,840","00:07:37,310",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"Well you can see the, and so"
cs-410_4_11_112,cs-410,4,11,"00:07:37,310","00:07:42,680",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,"n plus one parameters, and we have k"
cs-410_4_11_113,cs-410,4,11,"00:07:42,680","00:07:49,030",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,so the total number of parameters is
cs-410_4_11_114,cs-410,4,11,"00:07:49,030","00:07:49,820",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,That's a lot.
cs-410_4_11_115,cs-410,4,11,"00:07:49,820","00:07:54,096",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"A lot of parameters, so when"
cs-410_4_11_116,cs-410,4,11,"00:07:54,096","00:07:58,530",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,we would in general need a lot of data
cs-410_4_11_117,cs-410,4,11,"00:07:58,530","00:08:03,220",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,to help us decide the optimal
cs-410_4_11_118,cs-410,4,11,"00:08:04,450","00:08:05,785",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,So that's not ideal.
cs-410_4_11_119,cs-410,4,11,"00:08:07,225","00:08:10,751",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,Now the second problems
cs-410_4_11_120,cs-410,4,11,"00:08:10,751","00:08:15,595",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"these k minus 1 plus fives,"
cs-410_4_11_121,cs-410,4,11,"00:08:15,595","00:08:17,305",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,These problems are actually dependent.
cs-410_4_11_122,cs-410,4,11,"00:08:18,372","00:08:23,172",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,"In general, words that are positive"
cs-410_4_11_123,cs-410,4,11,"00:08:25,042","00:08:27,082",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,for any of these classifiers.
cs-410_4_11_124,cs-410,4,11,"00:08:27,082","00:08:28,752",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,For all these classifiers.
cs-410_4_11_125,cs-410,4,11,"00:08:28,752","00:08:31,896",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,So we should be able to take
cs-410_4_11_126,cs-410,4,11,"00:08:33,016","00:08:37,846",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,Now the idea of ordinal logistical
cs-410_4_11_127,cs-410,4,11,"00:08:37,846","00:08:42,007",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,The key idea is just
cs-410_4_11_128,cs-410,4,11,"00:08:42,007","00:08:46,390",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,independent logistical
cs-410_4_11_129,cs-410,4,11,"00:08:46,390","00:08:51,590",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,And that idea is to tie
cs-410_4_11_130,cs-410,4,11,"00:08:51,590","00:08:59,070",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,And that means we are going to
cs-410_4_11_131,cs-410,4,11,"00:08:59,070","00:09:05,290",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,These are the parameters that indicated
cs-410_4_11_132,cs-410,4,11,"00:09:05,290","00:09:09,490",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,And we're going to assume these
cs-410_4_11_133,cs-410,4,11,"00:09:09,490","00:09:10,920",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,all the K- 1 parameters.
cs-410_4_11_134,cs-410,4,11,"00:09:10,920","00:09:13,678",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,"And this just encodes our intuition that,"
cs-410_4_11_135,cs-410,4,11,"00:09:13,678","00:09:17,980",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,positive words in general would
cs-410_4_11_136,cs-410,4,11,"00:09:19,550","00:09:25,220",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,"So this is intuitively assumptions,"
cs-410_4_11_137,cs-410,4,11,"00:09:25,220","00:09:27,410",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,And we have this order
cs-410_4_11_138,cs-410,4,11,"00:09:28,630","00:09:34,370",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,"Now in fact, this would allow us"
cs-410_4_11_139,cs-410,4,11,"00:09:34,370","00:09:37,450",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,One is it's going to reduce
cs-410_4_11_140,cs-410,4,11,"00:09:38,750","00:09:42,880",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,And the other is to allow us
cs-410_4_11_141,cs-410,4,11,"00:09:42,880","00:09:45,860",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,Because all these parameters
cs-410_4_11_142,cs-410,4,11,"00:09:45,860","00:09:51,200",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,"So these training data, for"
cs-410_4_11_143,cs-410,4,11,"00:09:51,200","00:09:55,230",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,shared to help us set
cs-410_4_11_144,cs-410,4,11,"00:09:56,280","00:10:00,010",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,So we have more data to help
cs-410_4_11_145,cs-410,4,11,"00:10:01,790","00:10:02,840",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,"So what's the consequence,"
cs-410_4_11_146,cs-410,4,11,"00:10:02,840","00:10:08,010",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,well the formula would look very similar
cs-410_4_11_147,cs-410,4,11,"00:10:08,010","00:10:13,440",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,now the beta parameter has just one
cs-410_4_11_148,cs-410,4,11,"00:10:13,440","00:10:17,820",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,It no longer has the other index that
cs-410_4_11_149,cs-410,4,11,"00:10:19,260","00:10:21,340",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,So that means we tie them together.
cs-410_4_11_150,cs-410,4,11,"00:10:21,340","00:10:26,340",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,And there's only one set of better
cs-410_4_11_151,cs-410,4,11,"00:10:26,340","00:10:31,180",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,"However, each classifier still"
cs-410_4_11_152,cs-410,4,11,"00:10:31,180","00:10:33,060",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,The R for parameter.
cs-410_4_11_153,cs-410,4,11,"00:10:33,060","00:10:35,290",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,Except it's different.
cs-410_4_11_154,cs-410,4,11,"00:10:35,290","00:10:39,950",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,And this is of course needed to predict
cs-410_4_11_155,cs-410,4,11,"00:10:39,950","00:10:43,840",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,So R for sub j is different it
cs-410_4_11_156,cs-410,4,11,"00:10:43,840","00:10:46,020",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,has a different R value.
cs-410_4_11_157,cs-410,4,11,"00:10:46,020","00:10:48,890",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,"But the rest of the parameters,"
cs-410_4_11_158,cs-410,4,11,"00:10:48,890","00:10:53,940",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,"So now you can also ask the question,"
cs-410_4_11_159,cs-410,4,11,"00:10:53,940","00:10:57,140",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,"Again, that's an interesting"
cs-410_4_11_160,cs-410,4,11,"00:10:57,140","00:11:00,910",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,"So if you think about it for a moment, and"
cs-410_4_11_161,cs-410,4,11,"00:11:00,910","00:11:05,415",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,"you will see now, the param,"
cs-410_4_11_162,cs-410,4,11,"00:11:05,415","00:11:08,320",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,Specifically we have M plus K minus one.
cs-410_4_11_163,cs-410,4,11,"00:11:08,320","00:11:13,720",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,"Because we have M, beta values, and"
cs-410_4_11_164,cs-410,4,11,"00:11:15,550","00:11:17,575",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,"So let's just look basically,"
cs-410_4_11_165,cs-410,4,11,"00:11:17,575","00:11:21,931",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,that's basically the main idea of
cs-410_4_11_166,cs-410,4,11,"00:11:24,695","00:11:31,290",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,"So, now, let's see how we can use such"
cs-410_4_11_167,cs-410,4,11,"00:11:31,290","00:11:39,730",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,"It turns out that with this, this idea of"
cs-410_4_11_168,cs-410,4,11,"00:11:39,730","00:11:44,710",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=699,We also end up by having
cs-410_4_11_169,cs-410,4,11,"00:11:44,710","00:11:50,220",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,"And more specifically now, the criteria"
cs-410_4_11_170,cs-410,4,11,"00:11:50,220","00:11:55,440",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,"are at least 0.5 above,"
cs-410_4_11_171,cs-410,4,11,"00:11:55,440","00:12:00,810",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,whether the score of
cs-410_4_11_172,cs-410,4,11,"00:12:00,810","00:12:06,390",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=720,"equal to negative authors of j,"
cs-410_4_11_173,cs-410,4,11,"00:12:06,390","00:12:11,130",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=726,"Now, the scoring function is just"
cs-410_4_11_174,cs-410,4,11,"00:12:11,130","00:12:14,380",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,all the features with
cs-410_4_11_175,cs-410,4,11,"00:12:15,790","00:12:21,820",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=735,"So, this means now we can simply make"
cs-410_4_11_176,cs-410,4,11,"00:12:21,820","00:12:27,900",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,"the value of this scoring function,"
cs-410_4_11_177,cs-410,4,11,"00:12:27,900","00:12:33,121",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=747,Now you can see the general
cs-410_4_11_178,cs-410,4,11,"00:12:33,121","00:12:39,584",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,when the score is in the particular
cs-410_4_11_179,cs-410,4,11,"00:12:39,584","00:12:46,569",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=759,then we will assign the corresponding
cs-410_4_11_180,cs-410,4,11,"00:12:49,960","00:12:53,760",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,"So in this approach,"
cs-410_4_11_181,cs-410,4,11,"00:12:55,140","00:12:59,090",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,by using the features and
cs-410_4_11_182,cs-410,4,11,"00:13:00,150","00:13:04,490",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,This score will then be
cs-410_4_11_183,cs-410,4,11,"00:13:04,490","00:13:09,020",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,alpha values to see which
cs-410_4_11_184,cs-410,4,11,"00:13:09,020","00:13:09,540",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=789,"And then,"
cs-410_4_11_185,cs-410,4,11,"00:13:09,540","00:13:14,220",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=789,"using the range, we can then decide which"
cs-410_4_11_186,cs-410,4,11,"00:13:14,220","00:13:19,750",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,"Because, these ranges of alpha"
cs-410_4_11_187,cs-410,4,11,"00:13:19,750","00:13:24,840",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,"levels of ratings, and that's from"
cs-410_4_11_188,cs-410,4,11,"00:13:24,840","00:13:30,909",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=804,Each is tied to some level of rating.
cs-410_4_11_189,cs-410,4,11,"00:13:30,909","00:13:40,909",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,[MUSIC]
cs-410_4_2_1,cs-410,4,2,"00:00:00,248","00:00:06,368",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_4_2_2,cs-410,4,2,"00:00:06,368","00:00:10,308",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,This lecture is about the implementation
cs-410_4_2_3,cs-410,4,2,"00:00:12,878","00:00:17,327",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,In this lecture we will discuss
cs-410_4_2_4,cs-410,4,2,"00:00:17,327","00:00:20,768",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,retrieval method to build a search engine.
cs-410_4_2_5,cs-410,4,2,"00:00:20,768","00:00:24,753",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,The main challenge is to
cs-410_4_2_6,cs-410,4,2,"00:00:24,753","00:00:30,698",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,to enable a query to be answered very
cs-410_4_2_7,cs-410,4,2,"00:00:30,698","00:00:34,858",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,This is a typical text
cs-410_4_2_8,cs-410,4,2,"00:00:34,858","00:00:39,805",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,We can see the documents are first
cs-410_4_2_9,cs-410,4,2,"00:00:39,805","00:00:43,498",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"get tokenized units, for example, words."
cs-410_4_2_10,cs-410,4,2,"00:00:43,498","00:00:48,058",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,"And then, these words, or"
cs-410_4_2_11,cs-410,4,2,"00:00:48,058","00:00:53,188",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"a indexer that will create a index,"
cs-410_4_2_12,cs-410,4,2,"00:00:53,188","00:00:57,280",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,the search engine to use
cs-410_4_2_13,cs-410,4,2,"00:00:57,280","00:01:01,830",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,And the query would be going
cs-410_4_2_14,cs-410,4,2,"00:01:01,830","00:01:05,761",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,So the Tokenizer would be
cs-410_4_2_15,cs-410,4,2,"00:01:05,761","00:01:09,200",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,so that the text can be
cs-410_4_2_16,cs-410,4,2,"00:01:09,200","00:01:12,960",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,The same units would be
cs-410_4_2_17,cs-410,4,2,"00:01:12,960","00:01:17,604",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,The query's representation would
cs-410_4_2_18,cs-410,4,2,"00:01:17,604","00:01:22,506",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,which would use the index to quickly
cs-410_4_2_19,cs-410,4,2,"00:01:22,506","00:01:25,268",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,the documents and then ranking them.
cs-410_4_2_20,cs-410,4,2,"00:01:25,268","00:01:27,628",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,The results will be given to the user.
cs-410_4_2_21,cs-410,4,2,"00:01:27,628","00:01:32,033",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,And then the user can look at the results
cs-410_4_2_22,cs-410,4,2,"00:01:32,033","00:01:35,126",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,explicit judgements of both
cs-410_4_2_23,cs-410,4,2,"00:01:35,126","00:01:36,603",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,which documents are bad.
cs-410_4_2_24,cs-410,4,2,"00:01:36,603","00:01:43,353",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,Or implicit feedback such as so that
cs-410_4_2_25,cs-410,4,2,"00:01:43,353","00:01:46,187",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"End user will just look at the results,"
cs-410_4_2_26,cs-410,4,2,"00:01:46,187","00:01:49,193",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"skip some, and"
cs-410_4_2_27,cs-410,4,2,"00:01:49,193","00:01:55,353",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,So these interacting signals can be used
cs-410_4_2_28,cs-410,4,2,"00:01:55,353","00:02:01,718",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,accuracy by assuming that viewed documents
cs-410_4_2_29,cs-410,4,2,"00:02:01,718","00:02:05,678",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,So a search engine system then
cs-410_4_2_30,cs-410,4,2,"00:02:05,678","00:02:10,738",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"The first part is the indexer, and"
cs-410_4_2_31,cs-410,4,2,"00:02:10,738","00:02:16,458",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,"responds to the users query, and"
cs-410_4_2_32,cs-410,4,2,"00:02:16,458","00:02:21,072",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,"Now typically, the Indexer is"
cs-410_4_2_33,cs-410,4,2,"00:02:21,072","00:02:24,179",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,you can pre-process the correct data and
cs-410_4_2_34,cs-410,4,2,"00:02:24,179","00:02:29,168",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"to build the inventory index,"
cs-410_4_2_35,cs-410,4,2,"00:02:29,168","00:02:34,819",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,And this data structure can then be used
cs-410_4_2_36,cs-410,4,2,"00:02:34,819","00:02:40,668",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,to process a user's query dynamically and
cs-410_4_2_37,cs-410,4,2,"00:02:40,668","00:02:45,368",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,The feedback mechanism can be done online
cs-410_4_2_38,cs-410,4,2,"00:02:45,368","00:02:50,367",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,The implementation of the indexer and
cs-410_4_2_39,cs-410,4,2,"00:02:50,367","00:02:55,378",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,and this is the main topic of this
cs-410_4_2_40,cs-410,4,2,"00:02:55,378","00:02:59,843",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,"The feedback mechanism,"
cs-410_4_2_41,cs-410,4,2,"00:02:59,843","00:03:02,378",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,it depends on which method is used.
cs-410_4_2_42,cs-410,4,2,"00:03:02,378","00:03:08,818",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,So that is usually done in
cs-410_4_2_43,cs-410,4,2,"00:03:08,818","00:03:11,538",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,Let's first talk about the tokenizer.
cs-410_4_2_44,cs-410,4,2,"00:03:11,538","00:03:16,578",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,Tokernization is a normalized lexical
cs-410_4_2_45,cs-410,4,2,"00:03:16,578","00:03:21,368",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,so that semantically similar words
cs-410_4_2_46,cs-410,4,2,"00:03:21,368","00:03:25,133",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,"Now, in the language like English,"
cs-410_4_2_47,cs-410,4,2,"00:03:25,133","00:03:29,548",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,this will map all the inflectional
cs-410_4_2_48,cs-410,4,2,"00:03:29,548","00:03:33,047",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"So for example, computer, computation, and"
cs-410_4_2_49,cs-410,4,2,"00:03:33,047","00:03:37,078",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,computing can all be matched
cs-410_4_2_50,cs-410,4,2,"00:03:37,078","00:03:43,628",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,This way all these different forms of
cs-410_4_2_51,cs-410,4,2,"00:03:43,628","00:03:46,433",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,"Now normally, this is a good idea,"
cs-410_4_2_52,cs-410,4,2,"00:03:46,433","00:03:52,337",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,to increase the coverage of documents
cs-410_4_2_53,cs-410,4,2,"00:03:52,337","00:03:55,553",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"But it's also not always beneficial,"
cs-410_4_2_54,cs-410,4,2,"00:03:55,553","00:04:00,914",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,because sometimes the subtlest
cs-410_4_2_55,cs-410,4,2,"00:04:00,914","00:04:07,558",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,computation might still suggest the
cs-410_4_2_56,cs-410,4,2,"00:04:07,558","00:04:13,398",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,"But in most cases,"
cs-410_4_2_57,cs-410,4,2,"00:04:13,398","00:04:19,363",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,When we tokenize the text in some other
cs-410_4_2_58,cs-410,4,2,"00:04:19,363","00:04:25,338",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,face some special challenges in segmenting
cs-410_4_2_59,cs-410,4,2,"00:04:25,338","00:04:29,697",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,Because it's not obvious
cs-410_4_2_60,cs-410,4,2,"00:04:29,697","00:04:32,928",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,there's no space to separate them.
cs-410_4_2_61,cs-410,4,2,"00:04:32,928","00:04:41,638",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,"So here of course, we have to use some"
cs-410_4_2_62,cs-410,4,2,"00:04:41,638","00:04:47,144",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,"Once we do tokenization, then we would"
cs-410_4_2_63,cs-410,4,2,"00:04:47,144","00:04:52,748",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,convert the documents and do some data
cs-410_4_2_64,cs-410,4,2,"00:04:52,748","00:04:58,298",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,The basic idea is to precompute
cs-410_4_2_65,cs-410,4,2,"00:04:58,298","00:05:02,848",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,So the most commonly used index
cs-410_4_2_66,cs-410,4,2,"00:05:02,848","00:05:06,555",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,And this has been used
cs-410_4_2_67,cs-410,4,2,"00:05:06,555","00:05:09,768",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,to support basic search algorithms.
cs-410_4_2_68,cs-410,4,2,"00:05:09,768","00:05:13,426",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"Sometimes the other indices, for example,"
cs-410_4_2_69,cs-410,4,2,"00:05:13,426","00:05:19,498",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,document index might be needed in order
cs-410_4_2_70,cs-410,4,2,"00:05:19,498","00:05:24,106",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,And these kind of techniques
cs-410_4_2_71,cs-410,4,2,"00:05:24,106","00:05:28,828",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,that they vary a lot according
cs-410_4_2_72,cs-410,4,2,"00:05:28,828","00:05:34,549",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,To understand why we want to use
cs-410_4_2_73,cs-410,4,2,"00:05:34,549","00:05:40,698",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,you to think about how you would
cs-410_4_2_74,cs-410,4,2,"00:05:40,698","00:05:44,938",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,So if you want to use more time to
cs-410_4_2_75,cs-410,4,2,"00:05:44,938","00:05:49,584",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,So think about how you can
cs-410_4_2_76,cs-410,4,2,"00:05:49,584","00:05:54,768",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,that you can quickly respond
cs-410_4_2_77,cs-410,4,2,"00:05:54,768","00:05:58,466",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,Where if you have thought
cs-410_4_2_78,cs-410,4,2,"00:05:58,466","00:06:02,811",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,you might realize that where
cs-410_4_2_79,cs-410,4,2,"00:06:02,811","00:06:07,718",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,the list of documents that match
cs-410_4_2_80,cs-410,4,2,"00:06:07,718","00:06:11,788",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,"In this way, you can basically"
cs-410_4_2_81,cs-410,4,2,"00:06:11,788","00:06:17,503",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,So when you see a term you can simply just
cs-410_4_2_82,cs-410,4,2,"00:06:17,503","00:06:20,508",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,that term and return the list to the user.
cs-410_4_2_83,cs-410,4,2,"00:06:20,508","00:06:24,928",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,So that's the fastest way to
cs-410_4_2_84,cs-410,4,2,"00:06:24,928","00:06:30,468",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,Now the idea of the invert index
cs-410_4_2_85,cs-410,4,2,"00:06:30,468","00:06:36,017",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,We're going to do pre-constructed
cs-410_4_2_86,cs-410,4,2,"00:06:36,017","00:06:41,388",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,us to quickly find all the documents
cs-410_4_2_87,cs-410,4,2,"00:06:41,388","00:06:43,878",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,So let's take a look at this example.
cs-410_4_2_88,cs-410,4,2,"00:06:43,878","00:06:45,439",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,"We have three documents here,"
cs-410_4_2_89,cs-410,4,2,"00:06:45,439","00:06:49,168",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,and these are the documents that you
cs-410_4_2_90,cs-410,4,2,"00:06:49,168","00:06:52,916",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,Suppose that we want to create
cs-410_4_2_91,cs-410,4,2,"00:06:52,916","00:06:57,502",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"Then we want to maintain a dictionary, in"
cs-410_4_2_92,cs-410,4,2,"00:06:57,502","00:07:01,628",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,each term and we're going to store
cs-410_4_2_93,cs-410,4,2,"00:07:01,628","00:07:05,960",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,"For example, the number of"
cs-410_4_2_94,cs-410,4,2,"00:07:05,960","00:07:09,458",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,the total number of code or
cs-410_4_2_95,cs-410,4,2,"00:07:09,458","00:07:14,148",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,which means we would kind of duplicate
cs-410_4_2_96,cs-410,4,2,"00:07:14,148","00:07:17,415",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,"And so, for example, news,"
cs-410_4_2_97,cs-410,4,2,"00:07:17,415","00:07:22,253",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,this term occur in all
cs-410_4_2_98,cs-410,4,2,"00:07:22,253","00:07:26,198",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,so the count of documents is three.
cs-410_4_2_99,cs-410,4,2,"00:07:26,198","00:07:32,820",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,And you might also realize we needed this
cs-410_4_2_100,cs-410,4,2,"00:07:32,820","00:07:38,002",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,for computing some statistics to
cs-410_4_2_101,cs-410,4,2,"00:07:38,002","00:07:42,422",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,Can you think of that?
cs-410_4_2_102,cs-410,4,2,"00:07:42,422","00:07:49,862",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,So what weighting heuristic
cs-410_4_2_103,cs-410,4,2,"00:07:49,862","00:07:53,622",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"Well, that's the idea, right,"
cs-410_4_2_104,cs-410,4,2,"00:07:53,622","00:07:58,300",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"So, IDF is the property of a term,"
cs-410_4_2_105,cs-410,4,2,"00:07:58,300","00:08:03,291",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,"So, with the document that count here,"
cs-410_4_2_106,cs-410,4,2,"00:08:03,291","00:08:06,556",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,"either at this time, or"
cs-410_4_2_107,cs-410,4,2,"00:08:06,556","00:08:10,134",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,At random time when we see a query.
cs-410_4_2_108,cs-410,4,2,"00:08:10,134","00:08:13,641",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"Now in addition to these basic statistics,"
cs-410_4_2_109,cs-410,4,2,"00:08:13,641","00:08:18,380",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,we'll also store all the documents
cs-410_4_2_110,cs-410,4,2,"00:08:18,380","00:08:23,049",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,and these entries are stored
cs-410_4_2_111,cs-410,4,2,"00:08:24,150","00:08:27,595",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,So in this case it matched
cs-410_4_2_112,cs-410,4,2,"00:08:27,595","00:08:31,680",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,we store information about
cs-410_4_2_113,cs-410,4,2,"00:08:31,680","00:08:38,160",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"This is the document id,"
cs-410_4_2_114,cs-410,4,2,"00:08:38,160","00:08:45,240",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,"The tf is one for news, in the second"
cs-410_4_2_115,cs-410,4,2,"00:08:45,240","00:08:50,864",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,"So from this list, we can get all"
cs-410_4_2_116,cs-410,4,2,"00:08:50,864","00:08:55,320",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,we can also know the frequency
cs-410_4_2_117,cs-410,4,2,"00:08:55,320","00:08:58,214",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,"So, if the query has just one word,"
cs-410_4_2_118,cs-410,4,2,"00:08:58,214","00:09:01,628",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,we have easily look up to this
cs-410_4_2_119,cs-410,4,2,"00:09:01,628","00:09:06,780",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,go quicker into the postings to fetch
cs-410_4_2_120,cs-410,4,2,"00:09:06,780","00:09:08,180",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,"So, let's take a look at another term."
cs-410_4_2_121,cs-410,4,2,"00:09:09,280","00:09:12,600",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,"This time, let's take a look"
cs-410_4_2_122,cs-410,4,2,"00:09:14,130","00:09:17,950",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,"This would occur in only one document,"
cs-410_4_2_123,cs-410,4,2,"00:09:17,950","00:09:23,490",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,So the document frequency is 1 but
cs-410_4_2_124,cs-410,4,2,"00:09:23,490","00:09:29,210",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,"So the frequency count is two, and"
cs-410_4_2_125,cs-410,4,2,"00:09:29,210","00:09:33,250",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,some other reachable method where
cs-410_4_2_126,cs-410,4,2,"00:09:34,490","00:09:38,770",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,assess the popularity of
cs-410_4_2_127,cs-410,4,2,"00:09:38,770","00:09:42,930",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,Similarly we'll have a pointer
cs-410_4_2_128,cs-410,4,2,"00:09:42,930","00:09:47,490",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,"and in this case,"
cs-410_4_2_129,cs-410,4,2,"00:09:48,900","00:09:53,570",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,the term occurred in just one document and
cs-410_4_2_130,cs-410,4,2,"00:09:53,570","00:09:57,320",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,The document id is 3 and
cs-410_4_2_131,cs-410,4,2,"00:09:59,610","00:10:02,550",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,So this is the basic
cs-410_4_2_132,cs-410,4,2,"00:10:02,550","00:10:04,340",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,"It's actually pretty simple, right?"
cs-410_4_2_133,cs-410,4,2,"00:10:06,580","00:10:12,370",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,With this structure we can easily fetch
cs-410_4_2_134,cs-410,4,2,"00:10:12,370","00:10:15,760",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,And this will be the basis for
cs-410_4_2_135,cs-410,4,2,"00:10:15,760","00:10:23,770",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,Now sometimes we also want to store
cs-410_4_2_136,cs-410,4,2,"00:10:25,220","00:10:31,960",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,So in many of these cases the term
cs-410_4_2_137,cs-410,4,2,"00:10:31,960","00:10:34,320",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,So there's only one position for
cs-410_4_2_138,cs-410,4,2,"00:10:35,810","00:10:40,990",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,"But in this case, the term occurred"
cs-410_4_2_139,cs-410,4,2,"00:10:40,990","00:10:44,690",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,Now the position information is very
cs-410_4_2_140,cs-410,4,2,"00:10:44,690","00:10:48,400",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,the matching of query terms is
cs-410_4_2_141,cs-410,4,2,"00:10:48,400","00:10:51,360",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,"let's say, five words or ten words."
cs-410_4_2_142,cs-410,4,2,"00:10:52,410","00:11:00,700",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,"Or, whether the matching of the two query"
cs-410_4_2_143,cs-410,4,2,"00:11:00,700","00:11:04,540",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,That this can all be checked quickly
cs-410_4_2_144,cs-410,4,2,"00:11:05,920","00:11:10,160",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,"So, why is inverted index good for"
cs-410_4_2_145,cs-410,4,2,"00:11:10,160","00:11:16,349",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,"Well, we just talked about the possibility"
cs-410_4_2_146,cs-410,4,2,"00:11:16,349","00:11:17,990",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,And that's very easy.
cs-410_4_2_147,cs-410,4,2,"00:11:17,990","00:11:19,910",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,What about the multiple term queries?
cs-410_4_2_148,cs-410,4,2,"00:11:19,910","00:11:23,800",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,Well let's first look at the some
cs-410_4_2_149,cs-410,4,2,"00:11:23,800","00:11:27,740",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,A Boolean query is basically
cs-410_4_2_150,cs-410,4,2,"00:11:27,740","00:11:36,290",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,So I want the value in the document
cs-410_4_2_151,cs-410,4,2,"00:11:36,290","00:11:38,770",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,So that's one conjunctive query.
cs-410_4_2_152,cs-410,4,2,"00:11:38,770","00:11:45,440",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,Or I want the web documents
cs-410_4_2_153,cs-410,4,2,"00:11:45,440","00:11:46,540",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,That's a disjunctive query.
cs-410_4_2_154,cs-410,4,2,"00:11:46,540","00:11:51,070",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,But how can we answer such
cs-410_4_2_155,cs-410,4,2,"00:11:52,090","00:11:53,860",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,"Well if you think a bit about it,"
cs-410_4_2_156,cs-410,4,2,"00:11:53,860","00:11:58,130",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,it would be obvious because
cs-410_4_2_157,cs-410,4,2,"00:11:58,130","00:12:03,170",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,the documents that match term A and also
cs-410_4_2_158,cs-410,4,2,"00:12:03,170","00:12:08,160",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,And then just take the intersection
cs-410_4_2_159,cs-410,4,2,"00:12:08,160","00:12:13,050",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,Or to take the union to
cs-410_4_2_160,cs-410,4,2,"00:12:13,050","00:12:16,020",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,So this is all very easy to answer.
cs-410_4_2_161,cs-410,4,2,"00:12:16,020","00:12:17,780",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,It's going to be very quick.
cs-410_4_2_162,cs-410,4,2,"00:12:17,780","00:12:20,850",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,Now what about the multi-term
cs-410_4_2_163,cs-410,4,2,"00:12:20,850","00:12:24,390",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,We talked about the vector space model for
cs-410_4_2_164,cs-410,4,2,"00:12:24,390","00:12:28,940",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,we will do a match such query with
cs-410_4_2_165,cs-410,4,2,"00:12:28,940","00:12:32,330",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,And the score is based on
cs-410_4_2_166,cs-410,4,2,"00:12:32,330","00:12:35,670",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,So in this case it's not
cs-410_4_2_167,cs-410,4,2,"00:12:35,670","00:12:38,770",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=755,the scoring can be actually
cs-410_4_2_168,cs-410,4,2,"00:12:38,770","00:12:42,430",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,Basically it's similar to
cs-410_4_2_169,cs-410,4,2,"00:12:42,430","00:12:45,140",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=762,"Basically, it's like A or B."
cs-410_4_2_170,cs-410,4,2,"00:12:45,140","00:12:50,680",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=765,We take the union of all the documents
cs-410_4_2_171,cs-410,4,2,"00:12:50,680","00:12:53,320",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=770,then we would aggregate the term weights.
cs-410_4_2_172,cs-410,4,2,"00:12:53,320","00:13:01,420",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,So this is a basic idea of using inverted
cs-410_4_2_173,cs-410,4,2,"00:13:01,420","00:13:05,210",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=781,And we're going to talk about
cs-410_4_2_174,cs-410,4,2,"00:13:05,210","00:13:06,000",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,"But for now,"
cs-410_4_2_175,cs-410,4,2,"00:13:06,000","00:13:12,210",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,let's just look at the question
cs-410_4_2_176,cs-410,4,2,"00:13:12,210","00:13:17,470",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,Basically why is more efficient than
cs-410_4_2_177,cs-410,4,2,"00:13:17,470","00:13:20,770",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=797,This is the obvious approach.
cs-410_4_2_178,cs-410,4,2,"00:13:20,770","00:13:27,518",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,You can just compute a score for each
cs-410_4_2_179,cs-410,4,2,"00:13:27,518","00:13:29,936",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,And this is a straightforward method but
cs-410_4_2_180,cs-410,4,2,"00:13:29,936","00:13:34,496",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=809,this is going to be very slow imagine
cs-410_4_2_181,cs-410,4,2,"00:13:34,496","00:13:39,620",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=814,If you do this then it will take
cs-410_4_2_182,cs-410,4,2,"00:13:39,620","00:13:44,975",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=819,So the question now is why would
cs-410_4_2_183,cs-410,4,2,"00:13:44,975","00:13:48,780",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,Well it has to do is the word
cs-410_4_2_184,cs-410,4,2,"00:13:48,780","00:13:54,010",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,"So, here's some common phenomena"
cs-410_4_2_185,cs-410,4,2,"00:13:54,010","00:13:58,720",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=834,There are some languages independent
cs-410_4_2_186,cs-410,4,2,"00:14:00,300","00:14:07,690",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,And these patterns are basically
cs-410_4_2_187,cs-410,4,2,"00:14:07,690","00:14:10,830",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=847,A few words like the common
cs-410_4_2_188,cs-410,4,2,"00:14:10,830","00:14:14,780",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,"we occur very, very frequently in text."
cs-410_4_2_189,cs-410,4,2,"00:14:14,780","00:14:18,210",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=854,So they account for
cs-410_4_2_190,cs-410,4,2,"00:14:19,405","00:14:22,885",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,But most words would occur just rarely.
cs-410_4_2_191,cs-410,4,2,"00:14:22,885","00:14:25,615",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=862,"There are many words that occur just once,"
cs-410_4_2_192,cs-410,4,2,"00:14:25,615","00:14:29,790",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,"let's say, in a document or"
cs-410_4_2_193,cs-410,4,2,"00:14:29,790","00:14:33,306",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=869,And there are many such.
cs-410_4_2_194,cs-410,4,2,"00:14:33,306","00:14:37,977",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,It's also true that the most
cs-410_4_2_195,cs-410,4,2,"00:14:37,977","00:14:40,462",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,they have to be rare in another.
cs-410_4_2_196,cs-410,4,2,"00:14:40,462","00:14:45,800",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=880,That means although the general
cs-410_4_2_197,cs-410,4,2,"00:14:45,800","00:14:51,060",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=885,was observed in many cases that
cs-410_4_2_198,cs-410,4,2,"00:14:51,060","00:14:54,770",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=891,may vary from context to context.
cs-410_4_2_199,cs-410,4,2,"00:14:54,770","00:14:59,450",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,So this phenomena is characterized
cs-410_4_2_200,cs-410,4,2,"00:14:59,450","00:15:02,210",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,This law says that the rank of a word
cs-410_4_2_201,cs-410,4,2,"00:15:02,210","00:15:06,370",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=902,multiplied by the frequency of
cs-410_4_2_202,cs-410,4,2,"00:15:07,450","00:15:13,045",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=907,So formally if we use F(w)
cs-410_4_2_203,cs-410,4,2,"00:15:13,045","00:15:16,310",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,r(w) to denote the rank of a word.
cs-410_4_2_204,cs-410,4,2,"00:15:16,310","00:15:17,390",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=916,Then this is the formula.
cs-410_4_2_205,cs-410,4,2,"00:15:17,390","00:15:21,300",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=917,"It basically says the same thing,"
cs-410_4_2_206,cs-410,4,2,"00:15:21,300","00:15:28,510",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=921,Where C is basically a constant and
cs-410_4_2_207,cs-410,4,2,"00:15:28,510","00:15:34,180",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=928,"alpha, that might be adjusted to"
cs-410_4_2_208,cs-410,4,2,"00:15:34,180","00:15:38,128",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=934,So if I plot the word
cs-410_4_2_209,cs-410,4,2,"00:15:38,128","00:15:40,980",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=938,then you can see this more easily.
cs-410_4_2_210,cs-410,4,2,"00:15:40,980","00:15:43,660",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=940,The x axis is basically the word rank.
cs-410_4_2_211,cs-410,4,2,"00:15:43,660","00:15:50,393",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,This is r(w) and
cs-410_4_2_212,cs-410,4,2,"00:15:50,393","00:15:57,448",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=950,Now this curve shows that the product
cs-410_4_2_213,cs-410,4,2,"00:15:57,448","00:16:02,524",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,"Now if you look at these words, we can see"
cs-410_4_2_214,cs-410,4,2,"00:16:02,524","00:16:06,870",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=962,"In the middle,"
cs-410_4_2_215,cs-410,4,2,"00:16:06,870","00:16:11,440",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=966,These words tend to occur
cs-410_4_2_216,cs-410,4,2,"00:16:11,440","00:16:14,890",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,they are not like those
cs-410_4_2_217,cs-410,4,2,"00:16:14,890","00:16:17,070",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=974,And they are also not very rare.
cs-410_4_2_218,cs-410,4,2,"00:16:18,190","00:16:21,620",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=978,So they tend to be often used in
cs-410_4_2_219,cs-410,4,2,"00:16:22,700","00:16:28,240",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=982,queries and they also tend
cs-410_4_2_220,cs-410,4,2,"00:16:28,240","00:16:31,290",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=988,These intermediate frequency words.
cs-410_4_2_221,cs-410,4,2,"00:16:31,290","00:16:34,480",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=991,But if you look at the left
cs-410_4_2_222,cs-410,4,2,"00:16:35,820","00:16:38,330",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=995,these are the highest frequency words.
cs-410_4_2_223,cs-410,4,2,"00:16:38,330","00:16:39,620",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=998,They are covered very frequently.
cs-410_4_2_224,cs-410,4,2,"00:16:39,620","00:16:45,540",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=999,"They are usually words,"
cs-410_4_2_225,cs-410,4,2,"00:16:45,540","00:16:49,440",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,"Those words are very, very frequent and"
cs-410_4_2_226,cs-410,4,2,"00:16:49,440","00:16:54,226",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1009,"discriminated, and they are generally"
cs-410_4_2_227,cs-410,4,2,"00:16:54,226","00:17:01,900",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1014,So they are often removed and
cs-410_4_2_228,cs-410,4,2,"00:17:01,900","00:17:06,960",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1021,So you can use pretty much just the kind
cs-410_4_2_229,cs-410,4,2,"00:17:06,960","00:17:09,620",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1026,infer what words might be stop words.
cs-410_4_2_230,cs-410,4,2,"00:17:09,620","00:17:12,690",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1029,Those are basically
cs-410_4_2_231,cs-410,4,2,"00:17:13,780","00:17:18,500",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1033,And they also occupy a lot of
cs-410_4_2_232,cs-410,4,2,"00:17:18,500","00:17:23,048",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1038,You can imagine the posting entries for
cs-410_4_2_233,cs-410,4,2,"00:17:23,048","00:17:24,370",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1043,"And then therefore,"
cs-410_4_2_234,cs-410,4,2,"00:17:24,370","00:17:28,299",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1044,if you can remove such words you can save
cs-410_4_2_235,cs-410,4,2,"00:17:29,890","00:17:35,100",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1049,"We also show the tail part,"
cs-410_4_2_236,cs-410,4,2,"00:17:35,100","00:17:38,470",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1055,"Those words don't occur very frequently,"
cs-410_4_2_237,cs-410,4,2,"00:17:39,680","00:17:41,330",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1059,Those words are actually very useful for
cs-410_4_2_238,cs-410,4,2,"00:17:41,330","00:17:45,630",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1061,"search also, if a user happens to"
cs-410_4_2_239,cs-410,4,2,"00:17:45,630","00:17:49,730",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1065,"But because they're rare,"
cs-410_4_2_240,cs-410,4,2,"00:17:49,730","00:17:54,030",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1069,aren't necessarily
cs-410_4_2_241,cs-410,4,2,"00:17:54,030","00:17:58,970",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1074,But retain them would allow us to
cs-410_4_2_242,cs-410,4,2,"00:17:58,970","00:18:00,610",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1078,They generally have very high IDF.
cs-410_4_2_243,cs-410,4,2,"00:18:05,559","00:18:10,840",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1085,So what kind of data structures should
cs-410_4_2_244,cs-410,4,2,"00:18:10,840","00:18:11,970",244,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1090,"Well, it has two parts, right."
cs-410_4_2_245,cs-410,4,2,"00:18:11,970","00:18:16,720",245,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1091,"If you recall, we have a dictionary and"
cs-410_4_2_246,cs-410,4,2,"00:18:16,720","00:18:21,810",246,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1096,"The dictionary has modest size, although"
cs-410_4_2_247,cs-410,4,2,"00:18:21,810","00:18:24,810",247,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1101,large but compare it with
cs-410_4_2_248,cs-410,4,2,"00:18:26,220","00:18:29,710",248,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1106,And we also need to have fast
cs-410_4_2_249,cs-410,4,2,"00:18:29,710","00:18:32,940",249,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1109,because we're going to look up
cs-410_4_2_250,cs-410,4,2,"00:18:32,940","00:18:39,200",250,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1112,"So therefore, we'd prefer to keep such"
cs-410_4_2_251,cs-410,4,2,"00:18:39,200","00:18:43,333",251,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1119,"If the collection is not very large,"
cs-410_4_2_252,cs-410,4,2,"00:18:43,333","00:18:47,810",252,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1123,if the collection is very large
cs-410_4_2_253,cs-410,4,2,"00:18:47,810","00:18:52,100",253,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1127,"If the vocabulary size is very large,"
cs-410_4_2_254,cs-410,4,2,"00:18:52,100","00:18:55,800",254,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1132,"So, in general that's how it goes."
cs-410_4_2_255,cs-410,4,2,"00:18:55,800","00:18:58,578",255,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1135,So the data structures
cs-410_4_2_256,cs-410,4,2,"00:18:58,578","00:19:01,390",256,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1138,"storing dictionary,"
cs-410_4_2_257,cs-410,4,2,"00:19:01,390","00:19:04,375",257,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1141,"There are structures like hash table, or"
cs-410_4_2_258,cs-410,4,2,"00:19:04,375","00:19:09,090",258,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1144,b-tree if we can't store
cs-410_4_2_259,cs-410,4,2,"00:19:09,090","00:19:12,760",259,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1149,And then try to build a structure that
cs-410_4_2_260,cs-410,4,2,"00:19:14,530","00:19:16,705",260,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1154,For postings they are huge.
cs-410_4_2_261,cs-410,4,2,"00:19:18,045","00:19:24,815",261,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1158,"And in general, we don't have to have"
cs-410_4_2_262,cs-410,4,2,"00:19:24,815","00:19:29,145",262,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1164,We generally would just look up
cs-410_4_2_263,cs-410,4,2,"00:19:29,145","00:19:32,850",263,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1169,frequencies for all the documents
cs-410_4_2_264,cs-410,4,2,"00:19:33,930","00:19:36,570",264,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1173,So would read those entries sequentially.
cs-410_4_2_265,cs-410,4,2,"00:19:37,670","00:19:43,704",265,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1177,And therefore because it's large and
cs-410_4_2_266,cs-410,4,2,"00:19:43,704","00:19:49,826",266,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1183,they have to stay on disc and they would
cs-410_4_2_267,cs-410,4,2,"00:19:49,826","00:19:53,392",267,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1189,term frequency or
cs-410_4_2_268,cs-410,4,2,"00:19:53,392","00:19:58,300",268,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1193,"Now because they are very large,"
cs-410_4_2_269,cs-410,4,2,"00:19:59,360","00:20:04,390",269,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1199,"Now this is not only to save disc space,"
cs-410_4_2_270,cs-410,4,2,"00:20:04,390","00:20:09,080",270,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1204,"one benefit of compression, it It's"
cs-410_4_2_271,cs-410,4,2,"00:20:09,080","00:20:11,750",271,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1209,But it's also to help improving speed.
cs-410_4_2_272,cs-410,4,2,"00:20:13,110","00:20:15,980",272,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1213,Can you see why?
cs-410_4_2_273,cs-410,4,2,"00:20:15,980","00:20:23,470",273,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1215,"Well, we know that input and"
cs-410_4_2_274,cs-410,4,2,"00:20:23,470","00:20:28,320",274,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1223,In comparison with the time taken by CPU.
cs-410_4_2_275,cs-410,4,2,"00:20:28,320","00:20:33,410",275,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1228,"So, CPU is much faster but"
cs-410_4_2_276,cs-410,4,2,"00:20:33,410","00:20:39,335",276,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1233,"so by compressing the inverter index,"
cs-410_4_2_277,cs-410,4,2,"00:20:39,335","00:20:45,115",277,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1239,"the entries, that we have the readings,"
cs-410_4_2_278,cs-410,4,2,"00:20:45,115","00:20:50,150",278,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1245,"would be smaller, and"
cs-410_4_2_279,cs-410,4,2,"00:20:50,150","00:20:55,080",279,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1250,the amount of tracking IO and
cs-410_4_2_280,cs-410,4,2,"00:20:55,080","00:21:00,270",280,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1255,"Of course, we have to then do more"
cs-410_4_2_281,cs-410,4,2,"00:21:00,270","00:21:03,630",281,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1260,uncompress the data in the memory.
cs-410_4_2_282,cs-410,4,2,"00:21:03,630","00:21:05,550",282,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1263,But as I said CPU is fast.
cs-410_4_2_283,cs-410,4,2,"00:21:05,550","00:21:07,019",283,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1265,So over all we can still save time.
cs-410_4_2_284,cs-410,4,2,"00:21:08,360","00:21:11,301",284,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1268,So compression here is both
cs-410_4_2_285,cs-410,4,2,"00:21:11,301","00:21:14,035",285,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1271,to speed up the loading of the index.
cs-410_4_2_286,cs-410,4,2,"00:21:14,035","00:21:24,035",286,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1274,[MUSIC]
cs-410_4_3_1,cs-410,4,3,"00:00:00,012","00:00:03,467",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_4_3_2,cs-410,4,3,"00:00:11,647","00:00:13,963",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,So average precision is computer for
cs-410_4_3_3,cs-410,4,3,"00:00:13,963","00:00:14,690",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,just one.
cs-410_4_3_4,cs-410,4,3,"00:00:14,690","00:00:18,290",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,one query.
cs-410_4_3_5,cs-410,4,3,"00:00:18,290","00:00:24,570",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,different queries and this is to
cs-410_4_3_6,cs-410,4,3,"00:00:24,570","00:00:29,530",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,Depending on the queries you use you
cs-410_4_3_7,cs-410,4,3,"00:00:29,530","00:00:31,870",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,"Right, so"
cs-410_4_3_8,cs-410,4,3,"00:00:33,610","00:00:36,580",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"If you use more queries then,"
cs-410_4_3_9,cs-410,4,3,"00:00:36,580","00:00:39,850",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,take the average of the average
cs-410_4_3_10,cs-410,4,3,"00:00:41,600","00:00:42,470",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,So how can we do that?
cs-410_4_3_11,cs-410,4,3,"00:00:43,560","00:00:46,160",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,"Well, you can naturally."
cs-410_4_3_12,cs-410,4,3,"00:00:46,160","00:00:49,260",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,Think of just doing arithmetic mean as we
cs-410_4_3_13,cs-410,4,3,"00:00:50,670","00:00:56,000",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,"always tend to, to think in, in this way."
cs-410_4_3_14,cs-410,4,3,"00:00:56,000","00:01:02,000",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,"So, this would give us what's called"
cs-410_4_3_15,cs-410,4,3,"00:01:02,000","00:01:02,540",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,"In this case,"
cs-410_4_3_16,cs-410,4,3,"00:01:02,540","00:01:08,370",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,we take arithmetic mean of all the average
cs-410_4_3_17,cs-410,4,3,"00:01:09,930","00:01:13,840",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,But as I just mentioned in
cs-410_4_3_18,cs-410,4,3,"00:01:15,370","00:01:16,580",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,We call that.
cs-410_4_3_19,cs-410,4,3,"00:01:16,580","00:01:21,190",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,We talked about the different ways
cs-410_4_3_20,cs-410,4,3,"00:01:21,190","00:01:27,340",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,And we conclude that the arithmetic
cs-410_4_3_21,cs-410,4,3,"00:01:27,340","00:01:28,420",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,But here it's the same.
cs-410_4_3_22,cs-410,4,3,"00:01:28,420","00:01:32,120",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,We can also think about the alternative
cs-410_4_3_23,cs-410,4,3,"00:01:32,120","00:01:34,850",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"Don't just automatically assume that,"
cs-410_4_3_24,cs-410,4,3,"00:01:34,850","00:01:37,785",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,Let's just also take the arithmetic
cs-410_4_3_25,cs-410,4,3,"00:01:37,785","00:01:38,590",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,these queries.
cs-410_4_3_26,cs-410,4,3,"00:01:38,590","00:01:42,910",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,Let's think about what's
cs-410_4_3_27,cs-410,4,3,"00:01:42,910","00:01:46,660",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,"If you think about the different ways,"
cs-410_4_3_28,cs-410,4,3,"00:01:46,660","00:01:49,880",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,probably be able to think about
cs-410_4_3_29,cs-410,4,3,"00:01:51,230","00:01:53,680",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,And we call this kind of average a gMAP.
cs-410_4_3_30,cs-410,4,3,"00:01:55,650","00:01:56,860",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,This is another way.
cs-410_4_3_31,cs-410,4,3,"00:01:56,860","00:01:59,520",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,"So now, once you think about"
cs-410_4_3_32,cs-410,4,3,"00:01:59,520","00:02:00,820",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,Of doing the same thing.
cs-410_4_3_33,cs-410,4,3,"00:02:00,820","00:02:03,400",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"The natural question to ask is,"
cs-410_4_3_34,cs-410,4,3,"00:02:03,400","00:02:03,900",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,So.
cs-410_4_3_35,cs-410,4,3,"00:02:05,230","00:02:08,060",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"So, do you use MAP or gMAP?"
cs-410_4_3_36,cs-410,4,3,"00:02:09,830","00:02:11,200",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,"Again, that's important question."
cs-410_4_3_37,cs-410,4,3,"00:02:11,200","00:02:14,490",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,Imagine you are again
cs-410_4_3_38,cs-410,4,3,"00:02:14,490","00:02:17,109",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,by comparing the ways your old
cs-410_4_3_39,cs-410,4,3,"00:02:18,390","00:02:22,080",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,Now you tested multiple topics.
cs-410_4_3_40,cs-410,4,3,"00:02:22,080","00:02:25,150",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,Now you've got the average precision for
cs-410_4_3_41,cs-410,4,3,"00:02:25,150","00:02:28,470",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,Now you are thinking of looking
cs-410_4_3_42,cs-410,4,3,"00:02:28,470","00:02:29,470",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,You have to take the average.
cs-410_4_3_43,cs-410,4,3,"00:02:30,950","00:02:32,840",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,"But which, which strategy would you use?"
cs-410_4_3_44,cs-410,4,3,"00:02:34,040","00:02:38,360",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"Now first, you should also think about the"
cs-410_4_3_45,cs-410,4,3,"00:02:38,360","00:02:43,100",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,Can you think of scenarios where using
cs-410_4_3_46,cs-410,4,3,"00:02:43,100","00:02:45,920",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,That is they would give different
cs-410_4_3_47,cs-410,4,3,"00:02:45,920","00:02:52,440",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,And that also means depending on
cs-410_4_3_48,cs-410,4,3,"00:02:52,440","00:02:54,450",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,Average of these average positions.
cs-410_4_3_49,cs-410,4,3,"00:02:55,600","00:02:57,460",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,You will get different conclusions.
cs-410_4_3_50,cs-410,4,3,"00:02:57,460","00:03:00,379",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,This makes the question
cs-410_4_3_51,cs-410,4,3,"00:03:01,620","00:03:03,480",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,Right?
cs-410_4_3_52,cs-410,4,3,"00:03:05,350","00:03:08,320",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,"Well again, if you look at"
cs-410_4_3_53,cs-410,4,3,"00:03:08,320","00:03:12,750",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,Different ways of aggregating
cs-410_4_3_54,cs-410,4,3,"00:03:12,750","00:03:18,510",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,"You'll realize in arithmetic mean,"
cs-410_4_3_55,cs-410,4,3,"00:03:18,510","00:03:20,250",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,So what does large value here mean?
cs-410_4_3_56,cs-410,4,3,"00:03:20,250","00:03:22,260",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,It means the query is relatively easy.
cs-410_4_3_57,cs-410,4,3,"00:03:22,260","00:03:24,750",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"You can have a high pres,"
cs-410_4_3_58,cs-410,4,3,"00:03:25,950","00:03:29,707",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,Whereas gMAP tends to be
cs-410_4_3_59,cs-410,4,3,"00:03:30,870","00:03:34,790",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,And those are the queries that
cs-410_4_3_60,cs-410,4,3,"00:03:34,790","00:03:36,370",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,The average precision is low.
cs-410_4_3_61,cs-410,4,3,"00:03:37,410","00:03:41,260",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"So if you think about the,"
cs-410_4_3_62,cs-410,4,3,"00:03:41,260","00:03:45,840",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,"those difficult queries,"
cs-410_4_3_63,cs-410,4,3,"00:03:47,480","00:03:50,000",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"On the other hand, if you just want to."
cs-410_4_3_64,cs-410,4,3,"00:03:50,000","00:03:50,960",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,Have improved a lot.
cs-410_4_3_65,cs-410,4,3,"00:03:52,060","00:03:55,750",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,Over all the kinds of queries or
cs-410_4_3_66,cs-410,4,3,"00:03:55,750","00:04:00,890",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,easy and you want to make the perfect and
cs-410_4_3_67,cs-410,4,3,"00:04:00,890","00:04:05,150",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,"So again, the answer depends on"
cs-410_4_3_68,cs-410,4,3,"00:04:05,150","00:04:06,710",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"their pref, their preferences."
cs-410_4_3_69,cs-410,4,3,"00:04:08,020","00:04:13,720",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,So the point that here is to think
cs-410_4_3_70,cs-410,4,3,"00:04:13,720","00:04:18,750",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,"the same problem, and then compare them,"
cs-410_4_3_71,cs-410,4,3,"00:04:18,750","00:04:20,610",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,And which one makes more sense.
cs-410_4_3_72,cs-410,4,3,"00:04:20,610","00:04:24,970",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,"Often, when one of them might"
cs-410_4_3_73,cs-410,4,3,"00:04:24,970","00:04:27,640",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,another might make more sense
cs-410_4_3_74,cs-410,4,3,"00:04:27,640","00:04:31,620",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,So it's important to pick out under
cs-410_4_3_75,cs-410,4,3,"00:04:35,209","00:04:38,967",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,As a special case of the mean average
cs-410_4_3_76,cs-410,4,3,"00:04:38,967","00:04:43,100",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,the case where there was precisely
cs-410_4_3_77,cs-410,4,3,"00:04:43,100","00:04:47,210",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,"And this happens often, for example,"
cs-410_4_3_78,cs-410,4,3,"00:04:47,210","00:04:52,670",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"Where you know a target page, let's"
cs-410_4_3_79,cs-410,4,3,"00:04:52,670","00:04:56,140",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,"You have one relevant document there,"
cs-410_4_3_80,cs-410,4,3,"00:04:56,140","00:04:58,250",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"That's call a ""known item search""."
cs-410_4_3_81,cs-410,4,3,"00:04:58,250","00:05:01,330",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"In that case,"
cs-410_4_3_82,cs-410,4,3,"00:05:01,330","00:05:03,470",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,"Or in another application,"
cs-410_4_3_83,cs-410,4,3,"00:05:03,470","00:05:04,640",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,maybe there's only one answer.
cs-410_4_3_84,cs-410,4,3,"00:05:04,640","00:05:05,250",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,Are there.
cs-410_4_3_85,cs-410,4,3,"00:05:05,250","00:05:07,110",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,"So if you rank the answers,"
cs-410_4_3_86,cs-410,4,3,"00:05:07,110","00:05:12,100",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,then your goal is to rank that one
cs-410_4_3_87,cs-410,4,3,"00:05:12,100","00:05:16,480",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,"So in this case, you can easily"
cs-410_4_3_88,cs-410,4,3,"00:05:16,480","00:05:21,710",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,will basically boil down
cs-410_4_3_89,cs-410,4,3,"00:05:21,710","00:05:28,220",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,"That is, 1 over r where r is the rank"
cs-410_4_3_90,cs-410,4,3,"00:05:28,220","00:05:32,210",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,So if that document is ranked
cs-410_4_3_91,cs-410,4,3,"00:05:32,210","00:05:35,930",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,then it's 1 for reciprocal rank.
cs-410_4_3_92,cs-410,4,3,"00:05:35,930","00:05:39,515",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,"If it's ranked at the,"
cs-410_4_3_93,cs-410,4,3,"00:05:39,515","00:05:40,015",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,Et cetera.
cs-410_4_3_94,cs-410,4,3,"00:05:41,145","00:05:45,335",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,"And then we can also take a, a average"
cs-410_4_3_95,cs-410,4,3,"00:05:45,335","00:05:48,025",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"reciprocal rank over a set of topics, and"
cs-410_4_3_96,cs-410,4,3,"00:05:48,025","00:05:52,555",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,that would give us something
cs-410_4_3_97,cs-410,4,3,"00:05:52,555","00:05:54,830",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,It's a very popular measure.
cs-410_4_3_98,cs-410,4,3,"00:05:54,830","00:05:57,273",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,"For no item search or, you know,"
cs-410_4_3_99,cs-410,4,3,"00:05:57,273","00:06:01,690",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,an problem where you have
cs-410_4_3_100,cs-410,4,3,"00:06:03,070","00:06:09,170",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,"Now again here, you can see this"
cs-410_4_3_101,cs-410,4,3,"00:06:09,170","00:06:13,570",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,And this r is basically
cs-410_4_3_102,cs-410,4,3,"00:06:13,570","00:06:18,700",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,a user would have to make in order
cs-410_4_3_103,cs-410,4,3,"00:06:18,700","00:06:23,780",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,If it's ranked on the top it's low effort
cs-410_4_3_104,cs-410,4,3,"00:06:23,780","00:06:26,860",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,But if it's ranked at 100
cs-410_4_3_105,cs-410,4,3,"00:06:27,940","00:06:32,260",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,read presumably 100 documents
cs-410_4_3_106,cs-410,4,3,"00:06:32,260","00:06:37,380",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"So, in this sense r is also a meaningful"
cs-410_4_3_107,cs-410,4,3,"00:06:37,380","00:06:41,520",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,"take the reciprocal of r,"
cs-410_4_3_108,cs-410,4,3,"00:06:42,750","00:06:45,895",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So my natural question here
cs-410_4_3_109,cs-410,4,3,"00:06:45,895","00:06:50,550",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,I imagine if you were to design
cs-410_4_3_110,cs-410,4,3,"00:06:50,550","00:06:54,680",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,"of a random system,"
cs-410_4_3_111,cs-410,4,3,"00:06:55,760","00:07:00,070",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,You might have thought about
cs-410_4_3_112,cs-410,4,3,"00:07:00,070","00:07:02,906",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"After all,"
cs-410_4_3_113,cs-410,4,3,"00:07:02,906","00:07:10,959",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,"But, think about if you take a average"
cs-410_4_3_114,cs-410,4,3,"00:07:12,200","00:07:13,730",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,Again it would make a difference.
cs-410_4_3_115,cs-410,4,3,"00:07:13,730","00:07:16,330",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,"Right, for one single topic, using r or"
cs-410_4_3_116,cs-410,4,3,"00:07:16,330","00:07:19,140",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,using 1 over r wouldn't
cs-410_4_3_117,cs-410,4,3,"00:07:19,140","00:07:21,640",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,It's the same.
cs-410_4_3_118,cs-410,4,3,"00:07:21,640","00:07:24,790",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,Larger r with corresponds
cs-410_4_3_119,cs-410,4,3,"00:07:26,400","00:07:32,700",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"But the difference would only show when,"
cs-410_4_3_120,cs-410,4,3,"00:07:32,700","00:07:39,290",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,"So again, think about the average of Mean"
cs-410_4_3_121,cs-410,4,3,"00:07:39,290","00:07:39,930",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,What's the difference?
cs-410_4_3_122,cs-410,4,3,"00:07:39,930","00:07:41,730",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,Do you see any difference?
cs-410_4_3_123,cs-410,4,3,"00:07:41,730","00:07:46,050",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"And would, would this difference"
cs-410_4_3_124,cs-410,4,3,"00:07:46,050","00:07:46,760",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,In our conclusion.
cs-410_4_3_125,cs-410,4,3,"00:07:49,050","00:07:53,380",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"And this, it turns out that,"
cs-410_4_3_126,cs-410,4,3,"00:07:53,380","00:07:57,210",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"if you think about it, if you want to"
cs-410_4_3_127,cs-410,4,3,"00:07:57,210","00:07:58,230",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,then pause the video.
cs-410_4_3_128,cs-410,4,3,"00:07:59,410","00:08:04,350",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"Basically, the difference is,"
cs-410_4_3_129,cs-410,4,3,"00:08:04,350","00:08:07,810",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,Again it will be dominated
cs-410_4_3_130,cs-410,4,3,"00:08:07,810","00:08:08,840",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So what are those values?
cs-410_4_3_131,cs-410,4,3,"00:08:08,840","00:08:15,240",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,Those are basically large values that
cs-410_4_3_132,cs-410,4,3,"00:08:15,240","00:08:20,530",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,That means the relevant items
cs-410_4_3_133,cs-410,4,3,"00:08:20,530","00:08:25,160",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,And the sum that's also the average
cs-410_4_3_134,cs-410,4,3,"00:08:25,160","00:08:28,328",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,Where those relevant documents
cs-410_4_3_135,cs-410,4,3,"00:08:28,328","00:08:30,774",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,in the lower portion of the ranked.
cs-410_4_3_136,cs-410,4,3,"00:08:30,774","00:08:35,850",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,But from a users perspective we care
cs-410_4_3_137,cs-410,4,3,"00:08:35,850","00:08:39,529",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,So by taking this transformation
cs-410_4_3_138,cs-410,4,3,"00:08:40,650","00:08:43,920",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,Here we emphasize more on
cs-410_4_3_139,cs-410,4,3,"00:08:43,920","00:08:48,121",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,"You know, think about"
cs-410_4_3_140,cs-410,4,3,"00:08:48,121","00:08:52,390",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"it would make a big difference, in 1 over"
cs-410_4_3_141,cs-410,4,3,"00:08:52,390","00:08:57,030",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,where and when won't make much
cs-410_4_3_142,cs-410,4,3,"00:08:57,030","00:09:01,370",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,But if you use this there will
cs-410_4_3_143,cs-410,4,3,"00:09:01,370","00:09:03,468",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,"let's say 1,000, right."
cs-410_4_3_144,cs-410,4,3,"00:09:03,468","00:09:05,000",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,So this is not the desirable.
cs-410_4_3_145,cs-410,4,3,"00:09:06,260","00:09:09,320",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,"On the other hand, a 1 and"
cs-410_4_3_146,cs-410,4,3,"00:09:09,320","00:09:13,150",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,So this is yet another case where there
cs-410_4_3_147,cs-410,4,3,"00:09:13,150","00:09:15,840",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,thing and then you need to figure
cs-410_4_3_148,cs-410,4,3,"00:09:17,470","00:09:22,360",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,"So to summarize,"
cs-410_4_3_149,cs-410,4,3,"00:09:22,360","00:09:25,738",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,Can characterize the overall
cs-410_4_3_150,cs-410,4,3,"00:09:25,738","00:09:30,650",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,And we emphasized that the actual
cs-410_4_3_151,cs-410,4,3,"00:09:30,650","00:09:34,570",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,on how many top ranked results
cs-410_4_3_152,cs-410,4,3,"00:09:34,570","00:09:37,000",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,Some users will examine more.
cs-410_4_3_153,cs-410,4,3,"00:09:37,000","00:09:38,390",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,Than others.
cs-410_4_3_154,cs-410,4,3,"00:09:38,390","00:09:42,100",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,An average person uses a standard measure
cs-410_4_3_155,cs-410,4,3,"00:09:42,100","00:09:44,837",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,It combines precision and recall and
cs-410_4_3_156,cs-410,4,3,"00:09:44,837","00:09:48,904",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,it's sensitive to the rank
cs-410_4_3_157,cs-410,4,3,"00:09:48,904","00:09:58,904",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,[MUSIC]
cs-410_4_4_1,cs-410,4,4,"00:00:00,012","00:00:07,304",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_4_4_2,cs-410,4,4,"00:00:07,304","00:00:10,420",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about smoothing
cs-410_4_4_3,cs-410,4,4,"00:00:11,700","00:00:12,390",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,"In this lecture,"
cs-410_4_4_4,cs-410,4,4,"00:00:12,390","00:00:16,110",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,we're going to continue talking about
cs-410_4_4_5,cs-410,4,4,"00:00:16,110","00:00:19,630",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"In particular,"
cs-410_4_4_6,cs-410,4,4,"00:00:19,630","00:00:22,390",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,language model in the query
cs-410_4_4_7,cs-410,4,4,"00:00:23,820","00:00:27,248",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,So you have seen this slide
cs-410_4_4_8,cs-410,4,4,"00:00:27,248","00:00:30,470",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,This is the ranking function
cs-410_4_4_9,cs-410,4,4,"00:00:32,540","00:00:39,906",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"Here, we assume that the independence of"
cs-410_4_4_10,cs-410,4,4,"00:00:39,906","00:00:45,367",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,would look like the following where
cs-410_4_4_11,cs-410,4,4,"00:00:45,367","00:00:49,878",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,And inside the sum there is a log
cs-410_4_4_12,cs-410,4,4,"00:00:49,878","00:00:52,700",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,the document or document image model.
cs-410_4_4_13,cs-410,4,4,"00:00:52,700","00:00:57,750",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,So the main task now is to estimate this
cs-410_4_4_14,cs-410,4,4,"00:00:57,750","00:01:02,100",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,document language model as we
cs-410_4_4_15,cs-410,4,4,"00:01:02,100","00:01:06,530",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,estimating this model would lead
cs-410_4_4_16,cs-410,4,4,"00:01:06,530","00:01:10,810",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"So in this lecture, we're going to"
cs-410_4_4_17,cs-410,4,4,"00:01:10,810","00:01:13,110",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,So how do we estimate this language model?
cs-410_4_4_18,cs-410,4,4,"00:01:13,110","00:01:16,350",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,Well the obvious choice would be
cs-410_4_4_19,cs-410,4,4,"00:01:16,350","00:01:17,990",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,that we have seen before.
cs-410_4_4_20,cs-410,4,4,"00:01:17,990","00:01:22,200",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,And that is we're going to normalize
cs-410_4_4_21,cs-410,4,4,"00:01:24,110","00:01:26,913",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,And estimate the probability
cs-410_4_4_22,cs-410,4,4,"00:01:30,234","00:01:33,194",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,This is a step function here.
cs-410_4_4_23,cs-410,4,4,"00:01:35,934","00:01:38,543",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,Which means all of the words that have
cs-410_4_4_24,cs-410,4,4,"00:01:38,543","00:01:43,016",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,the same frequency count will
cs-410_4_4_25,cs-410,4,4,"00:01:43,016","00:01:48,570",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"This is another freedom to count,"
cs-410_4_4_26,cs-410,4,4,"00:01:48,570","00:01:51,770",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,Note that for words that have not
cs-410_4_4_27,cs-410,4,4,"00:01:52,850","00:01:55,130",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,they will have 0 probability.
cs-410_4_4_28,cs-410,4,4,"00:01:55,130","00:02:00,880",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,So we know this is just like the model
cs-410_4_4_29,cs-410,4,4,"00:02:00,880","00:02:06,730",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,Where we assume that the use of
cs-410_4_4_30,cs-410,4,4,"00:02:06,730","00:02:07,670",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,a formula to clear it.
cs-410_4_4_31,cs-410,4,4,"00:02:09,200","00:02:13,510",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,And there's no chance of assembling any
cs-410_4_4_32,cs-410,4,4,"00:02:13,510","00:02:14,360",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,we know that's not good.
cs-410_4_4_33,cs-410,4,4,"00:02:15,420","00:02:17,240",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,So how do we improve this?
cs-410_4_4_34,cs-410,4,4,"00:02:17,240","00:02:23,170",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,Well in order to assign
cs-410_4_4_35,cs-410,4,4,"00:02:23,170","00:02:28,710",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,to words that have not been observed in
cs-410_4_4_36,cs-410,4,4,"00:02:28,710","00:02:35,200",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,some probability mass from the words
cs-410_4_4_37,cs-410,4,4,"00:02:35,200","00:02:39,894",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,"So for example here, we have to take away"
cs-410_4_4_38,cs-410,4,4,"00:02:39,894","00:02:45,103",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,need some extra probability mass for
cs-410_4_4_39,cs-410,4,4,"00:02:45,103","00:02:47,870",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,So all these probabilities must sum to 1.
cs-410_4_4_40,cs-410,4,4,"00:02:47,870","00:02:53,224",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,So to make this transformation and to
cs-410_4_4_41,cs-410,4,4,"00:02:53,224","00:03:00,420",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,by assigning non zero probabilities to
cs-410_4_4_42,cs-410,4,4,"00:03:01,970","00:03:06,630",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,We have to do smoothing and
cs-410_4_4_43,cs-410,4,4,"00:03:06,630","00:03:11,140",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,the estimate by considering
cs-410_4_4_44,cs-410,4,4,"00:03:13,970","00:03:17,800",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,had been asking to write more words for
cs-410_4_4_45,cs-410,4,4,"00:03:17,800","00:03:22,910",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,"the document,"
cs-410_4_4_46,cs-410,4,4,"00:03:22,910","00:03:27,050",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,If you think about this factor
cs-410_4_4_47,cs-410,4,4,"00:03:27,050","00:03:30,830",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,would be a more accurate than
cs-410_4_4_48,cs-410,4,4,"00:03:30,830","00:03:35,270",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,Imagine you have seen an abstract
cs-410_4_4_49,cs-410,4,4,"00:03:35,270","00:03:37,230",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,Let's say this document is abstract.
cs-410_4_4_50,cs-410,4,4,"00:03:39,250","00:03:47,844",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,If we assume and see words in this
cs-410_4_4_51,cs-410,4,4,"00:03:47,844","00:03:51,900",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,That would mean there's
cs-410_4_4_52,cs-410,4,4,"00:03:51,900","00:03:57,170",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,a word outside the abstract
cs-410_4_4_53,cs-410,4,4,"00:03:57,170","00:04:02,193",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,But imagine a user who is interested
cs-410_4_4_54,cs-410,4,4,"00:04:02,193","00:04:06,475",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,The user might actually
cs-410_4_4_55,cs-410,4,4,"00:04:06,475","00:04:08,973",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,that chapter to use as query.
cs-410_4_4_56,cs-410,4,4,"00:04:08,973","00:04:13,916",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,"So obviously,"
cs-410_4_4_57,cs-410,4,4,"00:04:13,916","00:04:18,760",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,author would have written
cs-410_4_4_58,cs-410,4,4,"00:04:18,760","00:04:23,627",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,So smoothing of the language
cs-410_4_4_59,cs-410,4,4,"00:04:23,627","00:04:27,642",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,to recover the model for
cs-410_4_4_60,cs-410,4,4,"00:04:27,642","00:04:32,346",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,"And then of course,"
cs-410_4_4_61,cs-410,4,4,"00:04:32,346","00:04:36,310",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,words that are not
cs-410_4_4_62,cs-410,4,4,"00:04:36,310","00:04:39,250",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,So that's why smoothing is
cs-410_4_4_63,cs-410,4,4,"00:04:39,250","00:04:43,670",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,So let's talk a little more about
cs-410_4_4_64,cs-410,4,4,"00:04:43,670","00:04:48,500",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,"The key question here is, what probability"
cs-410_4_4_65,cs-410,4,4,"00:04:50,480","00:04:52,200",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,And there are many different
cs-410_4_4_66,cs-410,4,4,"00:04:53,290","00:04:59,500",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"One idea here, that's very useful for"
cs-410_4_4_67,cs-410,4,4,"00:04:59,500","00:05:03,790",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,word be proportional to its probability
cs-410_4_4_68,cs-410,4,4,"00:05:03,790","00:05:07,785",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,That means if you don't observe
cs-410_4_4_69,cs-410,4,4,"00:05:07,785","00:05:11,583",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,We're going to assume that its
cs-410_4_4_70,cs-410,4,4,"00:05:11,583","00:05:16,310",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,by another reference language
cs-410_4_4_71,cs-410,4,4,"00:05:16,310","00:05:20,500",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,It will tell us which unseen words
cs-410_4_4_72,cs-410,4,4,"00:05:22,440","00:05:26,060",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,"In the case of retrieval,"
cs-410_4_4_73,cs-410,4,4,"00:05:26,060","00:05:30,080",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,take the collection language model
cs-410_4_4_74,cs-410,4,4,"00:05:30,080","00:05:33,390",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,"That is to say, if you don't"
cs-410_4_4_75,cs-410,4,4,"00:05:33,390","00:05:37,440",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,we're going to assume that
cs-410_4_4_76,cs-410,4,4,"00:05:37,440","00:05:40,658",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,would be proportional to the probability
cs-410_4_4_77,cs-410,4,4,"00:05:40,658","00:05:42,990",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"So more formally,"
cs-410_4_4_78,cs-410,4,4,"00:05:42,990","00:05:46,790",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,we'll be estimating the probability
cs-410_4_4_79,cs-410,4,4,"00:05:48,220","00:05:54,479",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,If the word is seen in
cs-410_4_4_80,cs-410,4,4,"00:05:54,479","00:06:02,251",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,would be this counted the maximum
cs-410_4_4_81,cs-410,4,4,"00:06:02,251","00:06:07,142",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,"Otherwise, if the word is not seen in the"
cs-410_4_4_82,cs-410,4,4,"00:06:07,142","00:06:12,220",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,be proportional to the probability
cs-410_4_4_83,cs-410,4,4,"00:06:12,220","00:06:17,060",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,And here the coefficient that offer is to
cs-410_4_4_84,cs-410,4,4,"00:06:17,060","00:06:21,360",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,control the amount of probability
cs-410_4_4_85,cs-410,4,4,"00:06:22,450","00:06:25,390",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,"Obviously, all these"
cs-410_4_4_86,cs-410,4,4,"00:06:25,390","00:06:28,300",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,alpha sub d is constrained in some way.
cs-410_4_4_87,cs-410,4,4,"00:06:29,390","00:06:33,370",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,So what if we plug in this
cs-410_4_4_88,cs-410,4,4,"00:06:33,370","00:06:35,150",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,query likelihood ranking function?
cs-410_4_4_89,cs-410,4,4,"00:06:35,150","00:06:36,290",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,This is what we will get.
cs-410_4_4_90,cs-410,4,4,"00:06:37,790","00:06:43,930",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,"In this formula, we have this"
cs-410_4_4_91,cs-410,4,4,"00:06:43,930","00:06:48,900",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,as a sum over all the query words and
cs-410_4_4_92,cs-410,4,4,"00:06:48,900","00:06:54,000",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,those that we have written here as the sum
cs-410_4_4_93,cs-410,4,4,"00:06:54,000","00:06:56,780",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,This is the sum of all
cs-410_4_4_94,cs-410,4,4,"00:06:56,780","00:07:00,310",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,but not that we have a count
cs-410_4_4_95,cs-410,4,4,"00:07:00,310","00:07:04,476",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"So in fact, we are just taking"
cs-410_4_4_96,cs-410,4,4,"00:07:04,476","00:07:11,820",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,This is now a common
cs-410_4_4_97,cs-410,4,4,"00:07:11,820","00:07:16,170",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,because of its convenience
cs-410_4_4_98,cs-410,4,4,"00:07:18,710","00:07:21,949",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,"So this is as I said,"
cs-410_4_4_99,cs-410,4,4,"00:07:23,130","00:07:26,950",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,"In our smoothing method,"
cs-410_4_4_100,cs-410,4,4,"00:07:26,950","00:07:31,310",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,are not observed in the method would have
cs-410_4_4_101,cs-410,4,4,"00:07:31,310","00:07:33,663",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"Name it's four, this foru."
cs-410_4_4_102,cs-410,4,4,"00:07:33,663","00:07:37,090",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,"So we're going to do then,"
cs-410_4_4_103,cs-410,4,4,"00:07:38,620","00:07:44,422",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,One sum is over all the query words
cs-410_4_4_104,cs-410,4,4,"00:07:44,422","00:07:49,287",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,"That means that in this sum, all the words"
cs-410_4_4_105,cs-410,4,4,"00:07:49,287","00:07:54,580",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,have a non zero probability
cs-410_4_4_106,cs-410,4,4,"00:07:54,580","00:07:59,740",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,"Sorry, it's the non zero count"
cs-410_4_4_107,cs-410,4,4,"00:07:59,740","00:08:01,220",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,They all occur in the document.
cs-410_4_4_108,cs-410,4,4,"00:08:02,230","00:08:07,800",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,And they also have to of course
cs-410_4_4_109,cs-410,4,4,"00:08:07,800","00:08:13,894",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,So these are the query words
cs-410_4_4_110,cs-410,4,4,"00:08:13,894","00:08:19,153",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"On the other hand, in this sum we"
cs-410_4_4_111,cs-410,4,4,"00:08:19,153","00:08:23,630",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,that are not all query was
cs-410_4_4_112,cs-410,4,4,"00:08:25,840","00:08:31,250",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,So they occur in the query
cs-410_4_4_113,cs-410,4,4,"00:08:31,250","00:08:33,200",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,they don't occur in the document.
cs-410_4_4_114,cs-410,4,4,"00:08:33,200","00:08:33,920",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,"In this case,"
cs-410_4_4_115,cs-410,4,4,"00:08:33,920","00:08:39,346",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,these words have this probability because
cs-410_4_4_116,cs-410,4,4,"00:08:39,346","00:08:44,880",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"That here, these seen words"
cs-410_4_4_117,cs-410,4,4,"00:08:47,490","00:08:51,460",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"Now, we can go further by"
cs-410_4_4_118,cs-410,4,4,"00:08:52,570","00:08:54,790",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,as a difference of two other sums.
cs-410_4_4_119,cs-410,4,4,"00:08:54,790","00:08:58,760",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,"Basically, the first sum is"
cs-410_4_4_120,cs-410,4,4,"00:09:00,060","00:09:05,190",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,"Now, we know that the original sum"
cs-410_4_4_121,cs-410,4,4,"00:09:05,190","00:09:10,760",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,This is over all the query words that
cs-410_4_4_122,cs-410,4,4,"00:09:12,400","00:09:19,740",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,So here we pretend that they
cs-410_4_4_123,cs-410,4,4,"00:09:19,740","00:09:21,920",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,So we take a sum over all the query words.
cs-410_4_4_124,cs-410,4,4,"00:09:21,920","00:09:28,750",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,"Obviously, this sum has extra"
cs-410_4_4_125,cs-410,4,4,"00:09:30,770","00:09:33,710",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,"Because, here we're taking"
cs-410_4_4_126,cs-410,4,4,"00:09:33,710","00:09:37,880",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,"There, it's not matched in the document."
cs-410_4_4_127,cs-410,4,4,"00:09:37,880","00:09:44,370",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,"So in order to make them equal, we will"
cs-410_4_4_128,cs-410,4,4,"00:09:44,370","00:09:48,758",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,And this is the sum over all the query
cs-410_4_4_129,cs-410,4,4,"00:09:51,069","00:09:55,411",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,"And this makes sense, because here"
cs-410_4_4_130,cs-410,4,4,"00:09:55,411","00:09:59,410",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,And then we subtract the query
cs-410_4_4_131,cs-410,4,4,"00:09:59,410","00:10:04,020",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,That would give us the query that
cs-410_4_4_132,cs-410,4,4,"00:10:05,880","00:10:11,100",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,And this is almost a reverse
cs-410_4_4_133,cs-410,4,4,"00:10:12,770","00:10:14,758",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,And you might wonder why
cs-410_4_4_134,cs-410,4,4,"00:10:14,758","00:10:19,510",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,"Well, that's because if we do this,"
cs-410_4_4_135,cs-410,4,4,"00:10:19,510","00:10:25,360",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,then we have different forms
cs-410_4_4_136,cs-410,4,4,"00:10:25,360","00:10:31,370",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,"So now, you can see in this sum"
cs-410_4_4_137,cs-410,4,4,"00:10:31,370","00:10:35,440",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,the query was matching the document
cs-410_4_4_138,cs-410,4,4,"00:10:36,760","00:10:45,750",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=636,Here we have another sum over the same set
cs-410_4_4_139,cs-410,4,4,"00:10:45,750","00:10:47,870",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,"But inside the sum, it's different."
cs-410_4_4_140,cs-410,4,4,"00:10:49,180","00:10:52,640",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,But these two sums can clearly be merged.
cs-410_4_4_141,cs-410,4,4,"00:10:54,300","00:10:57,530",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,"So if we do that, we'll get another form"
cs-410_4_4_142,cs-410,4,4,"00:10:57,530","00:11:02,140",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,of the formula that looks like
cs-410_4_4_143,cs-410,4,4,"00:11:04,360","00:11:06,966",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=664,And note that this is
cs-410_4_4_144,cs-410,4,4,"00:11:06,966","00:11:10,796",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,Because here we combine
cs-410_4_4_145,cs-410,4,4,"00:11:10,796","00:11:16,710",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,some of the query words matching in
cs-410_4_4_146,cs-410,4,4,"00:11:19,040","00:11:24,469",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,And the other sum now is
cs-410_4_4_147,cs-410,4,4,"00:11:24,469","00:11:26,988",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,And these two parts
cs-410_4_4_148,cs-410,4,4,"00:11:26,988","00:11:30,130",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=686,because these are the probabilities
cs-410_4_4_149,cs-410,4,4,"00:11:31,630","00:11:36,419",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,This formula is very interesting
cs-410_4_4_150,cs-410,4,4,"00:11:37,450","00:11:39,970",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,the match the query terms.
cs-410_4_4_151,cs-410,4,4,"00:11:41,340","00:11:44,210",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,"And just like in the vector space model,"
cs-410_4_4_152,cs-410,4,4,"00:11:46,030","00:11:49,930",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,of terms that are in the intersection of
cs-410_4_4_153,cs-410,4,4,"00:11:51,320","00:11:55,620",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,So it already looks a little bit
cs-410_4_4_154,cs-410,4,4,"00:11:55,620","00:12:02,573",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,"In fact, there's even more similarity"
cs-410_4_4_155,cs-410,4,4,"00:12:02,573","00:12:12,573",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,[MUSIC]
cs-410_4_5_1,cs-410,4,5,"00:00:07,440","00:00:09,410",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about Web Search.
cs-410_4_5_2,cs-410,4,5,"00:00:11,950","00:00:14,750",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,"In this lecture,"
cs-410_4_5_3,cs-410,4,5,"00:00:14,750","00:00:19,150",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,of the most important applications of
cs-410_4_5_4,cs-410,4,5,"00:00:19,150","00:00:21,520",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,So let's first look at some
cs-410_4_5_5,cs-410,4,5,"00:00:21,520","00:00:23,380",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,opportunities in web search.
cs-410_4_5_6,cs-410,4,5,"00:00:23,380","00:00:26,010",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,"Now, many informational"
cs-410_4_5_7,cs-410,4,5,"00:00:26,010","00:00:29,010",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,had been developed
cs-410_4_5_8,cs-410,4,5,"00:00:29,010","00:00:33,890",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,"So when the web was born,"
cs-410_4_5_9,cs-410,4,5,"00:00:33,890","00:00:39,890",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,those algorithms to major application
cs-410_4_5_10,cs-410,4,5,"00:00:39,890","00:00:45,780",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"So naturally, there have to be some"
cs-410_4_5_11,cs-410,4,5,"00:00:45,780","00:00:53,460",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,search algorithms to address new
cs-410_4_5_12,cs-410,4,5,"00:00:53,460","00:00:56,210",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,So here are some general challenges.
cs-410_4_5_13,cs-410,4,5,"00:00:56,210","00:00:58,510",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,"First, this is a scalability challenge."
cs-410_4_5_14,cs-410,4,5,"00:00:58,510","00:01:00,200",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,How to handle the size of the web and
cs-410_4_5_15,cs-410,4,5,"00:01:00,200","00:01:02,750",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,ensure completeness of
cs-410_4_5_16,cs-410,4,5,"00:01:03,870","00:01:07,820",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,How to serve many users quickly and
cs-410_4_5_17,cs-410,4,5,"00:01:07,820","00:01:10,801",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,And so that's one major challenge and
cs-410_4_5_18,cs-410,4,5,"00:01:10,801","00:01:16,480",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,before the web was born the scale
cs-410_4_5_19,cs-410,4,5,"00:01:16,480","00:01:20,190",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,The second problem is that there's
cs-410_4_5_20,cs-410,4,5,"00:01:20,190","00:01:21,960",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,there are often spams.
cs-410_4_5_21,cs-410,4,5,"00:01:21,960","00:01:24,334",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,The third challenge is
cs-410_4_5_22,cs-410,4,5,"00:01:24,334","00:01:31,879",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,The new pages are constantly create and
cs-410_4_5_23,cs-410,4,5,"00:01:31,879","00:01:36,281",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,so it makes it harder to
cs-410_4_5_24,cs-410,4,5,"00:01:36,281","00:01:40,391",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,So these are some of the challenges
cs-410_4_5_25,cs-410,4,5,"00:01:40,391","00:01:42,880",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,deal with high quality web searching.
cs-410_4_5_26,cs-410,4,5,"00:01:44,090","00:01:47,492",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,On the other hand there are also some
cs-410_4_5_27,cs-410,4,5,"00:01:47,492","00:01:49,930",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,leverage to include the search results.
cs-410_4_5_28,cs-410,4,5,"00:01:49,930","00:01:53,330",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"There are many additional heuristics,"
cs-410_4_5_29,cs-410,4,5,"00:01:55,070","00:02:00,020",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,using links that we can
cs-410_4_5_30,cs-410,4,5,"00:02:00,020","00:02:03,510",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,Now everything that we talked about
cs-410_4_5_31,cs-410,4,5,"00:02:03,510","00:02:04,459",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,are general algorithms.
cs-410_4_5_32,cs-410,4,5,"00:02:05,630","00:02:11,050",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,They can be applied to any search
cs-410_4_5_33,cs-410,4,5,"00:02:11,050","00:02:15,890",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,"On the other hand, they also don't take"
cs-410_4_5_34,cs-410,4,5,"00:02:15,890","00:02:21,375",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,of pages or documents in the specific
cs-410_4_5_35,cs-410,4,5,"00:02:21,375","00:02:23,855",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,"Web pages are linked with each other,"
cs-410_4_5_36,cs-410,4,5,"00:02:23,855","00:02:28,645",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,the linking is something
cs-410_4_5_37,cs-410,4,5,"00:02:28,645","00:02:33,610",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,"So, because of these challenges and"
cs-410_4_5_38,cs-410,4,5,"00:02:33,610","00:02:39,110",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,that have been developed for
cs-410_4_5_39,cs-410,4,5,"00:02:39,110","00:02:41,390",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,One is parallel indexing and searching and
cs-410_4_5_40,cs-410,4,5,"00:02:41,390","00:02:44,410",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,this is to address
cs-410_4_5_41,cs-410,4,5,"00:02:44,410","00:02:49,930",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,"In particular, Google's imaging of"
cs-410_4_5_42,cs-410,4,5,"00:02:49,930","00:02:53,590",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,has been very helpful in that aspect.
cs-410_4_5_43,cs-410,4,5,"00:02:53,590","00:02:56,680",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"Second, there are techniques"
cs-410_4_5_44,cs-410,4,5,"00:02:56,680","00:03:00,460",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,"addressing the problem of spams,"
cs-410_4_5_45,cs-410,4,5,"00:03:00,460","00:03:03,570",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,We'll have to prevent those spam
cs-410_4_5_46,cs-410,4,5,"00:03:04,680","00:03:07,338",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,And there are also techniques
cs-410_4_5_47,cs-410,4,5,"00:03:07,338","00:03:10,520",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,And we're going to use a lot
cs-410_4_5_48,cs-410,4,5,"00:03:10,520","00:03:15,410",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,that it's not easy to spam the search
cs-410_4_5_49,cs-410,4,5,"00:03:15,410","00:03:19,810",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,And the third line of techniques is link
cs-410_4_5_50,cs-410,4,5,"00:03:19,810","00:03:24,730",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,analysis and these are techniques that can
cs-410_4_5_51,cs-410,4,5,"00:03:24,730","00:03:30,780",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,allow us to improve such results
cs-410_4_5_52,cs-410,4,5,"00:03:30,780","00:03:35,230",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,"And in general in web searching,"
cs-410_4_5_53,cs-410,4,5,"00:03:35,230","00:03:37,690",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,ranking not just for link analysis.
cs-410_4_5_54,cs-410,4,5,"00:03:37,690","00:03:43,300",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,But also exploring all kinds
cs-410_4_5_55,cs-410,4,5,"00:03:43,300","00:03:47,730",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,anchor text that describes
cs-410_4_5_56,cs-410,4,5,"00:03:47,730","00:03:51,310",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"So, here's a picture showing"
cs-410_4_5_57,cs-410,4,5,"00:03:51,310","00:03:55,820",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"Basically, this is the web on the left and"
cs-410_4_5_58,cs-410,4,5,"00:03:55,820","00:04:00,550",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,we're going to help this user to get
cs-410_4_5_59,cs-410,4,5,"00:04:00,550","00:04:05,410",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,And the first component is a Crawler that
cs-410_4_5_60,cs-410,4,5,"00:04:05,410","00:04:09,810",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,component is Indexer that would take
cs-410_4_5_61,cs-410,4,5,"00:04:10,920","00:04:15,720",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,The third component there is a Retriever
cs-410_4_5_62,cs-410,4,5,"00:04:15,720","00:04:19,840",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,answer user's query by talking
cs-410_4_5_63,cs-410,4,5,"00:04:19,840","00:04:24,790",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,And then the search results will be given
cs-410_4_5_64,cs-410,4,5,"00:04:24,790","00:04:29,090",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,"show those results, it allows"
cs-410_4_5_65,cs-410,4,5,"00:04:29,090","00:04:32,417",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,"So, we're going to talk about"
cs-410_4_5_66,cs-410,4,5,"00:04:32,417","00:04:37,552",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,"First of all, we're going to talk about"
cs-410_4_5_67,cs-410,4,5,"00:04:37,552","00:04:42,459",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,software robot that would do something
cs-410_4_5_68,cs-410,4,5,"00:04:42,459","00:04:44,954",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,"To build a toy crawler is relatively easy,"
cs-410_4_5_69,cs-410,4,5,"00:04:44,954","00:04:47,875",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,because you just need to start
cs-410_4_5_70,cs-410,4,5,"00:04:47,875","00:04:51,912",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,And then fetch pages from the web and
cs-410_4_5_71,cs-410,4,5,"00:04:51,912","00:04:53,517",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,figure out new links.
cs-410_4_5_72,cs-410,4,5,"00:04:53,517","00:05:00,994",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,And then add them to the priority que and
cs-410_4_5_73,cs-410,4,5,"00:05:00,994","00:05:04,764",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,But to be able to real crawler
cs-410_4_5_74,cs-410,4,5,"00:05:04,764","00:05:09,249",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,there are some complicated issues
cs-410_4_5_75,cs-410,4,5,"00:05:09,249","00:05:13,736",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"For example robustness,"
cs-410_4_5_76,cs-410,4,5,"00:05:13,736","00:05:18,722",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,what if there's a trap that generates
cs-410_4_5_77,cs-410,4,5,"00:05:18,722","00:05:23,456",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,that might attract your crawler to
cs-410_4_5_78,cs-410,4,5,"00:05:23,456","00:05:26,700",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,to fetch dynamic generated pages?
cs-410_4_5_79,cs-410,4,5,"00:05:26,700","00:05:30,093",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,The results of this issue
cs-410_4_5_80,cs-410,4,5,"00:05:30,093","00:05:35,668",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,you don't want to overload one particular
cs-410_4_5_81,cs-410,4,5,"00:05:35,668","00:05:39,158",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,you have to respect the robot
cs-410_4_5_82,cs-410,4,5,"00:05:39,158","00:05:43,340",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,You also need to handle different
cs-410_4_5_83,cs-410,4,5,"00:05:43,340","00:05:46,019",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"PDF files,"
cs-410_4_5_84,cs-410,4,5,"00:05:46,019","00:05:50,189",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,And you have to also
cs-410_4_5_85,cs-410,4,5,"00:05:50,189","00:05:56,237",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,sometimes those are CGI scripts and
cs-410_4_5_86,cs-410,4,5,"00:05:56,237","00:06:01,139",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"etc, and sometimes you have"
cs-410_4_5_87,cs-410,4,5,"00:06:01,139","00:06:03,866",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,they also create challenges.
cs-410_4_5_88,cs-410,4,5,"00:06:03,866","00:06:08,795",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,And you ideally should also recognize
cs-410_4_5_89,cs-410,4,5,"00:06:08,795","00:06:11,475",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,to duplicate those pages.
cs-410_4_5_90,cs-410,4,5,"00:06:11,475","00:06:15,398",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"And finally, you may be interested"
cs-410_4_5_91,cs-410,4,5,"00:06:15,398","00:06:19,935",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,Those are URLs that may not be linked
cs-410_4_5_92,cs-410,4,5,"00:06:19,935","00:06:24,884",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,"the URL to a shorter path, you might"
cs-410_4_5_93,cs-410,4,5,"00:06:27,008","00:06:29,298",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,So what are the Major Crawling Strategies?
cs-410_4_5_94,cs-410,4,5,"00:06:29,298","00:06:30,040",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"In general,"
cs-410_4_5_95,cs-410,4,5,"00:06:30,040","00:06:36,560",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,Breadth-First is most common because
cs-410_4_5_96,cs-410,4,5,"00:06:36,560","00:06:41,405",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,You would not keep probing a particular
cs-410_4_5_97,cs-410,4,5,"00:06:42,635","00:06:47,009",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,Also parallel crawling is very
cs-410_4_5_98,cs-410,4,5,"00:06:47,009","00:06:48,554",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,easy to parallelize.
cs-410_4_5_99,cs-410,4,5,"00:06:48,554","00:06:50,887",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,And there is some variations
cs-410_4_5_100,cs-410,4,5,"00:06:50,887","00:06:54,560",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,and one interesting variation
cs-410_4_5_101,cs-410,4,5,"00:06:54,560","00:06:59,850",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,"In this case, we're going to crawl just"
cs-410_4_5_102,cs-410,4,5,"00:06:59,850","00:07:04,316",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,"For example,"
cs-410_4_5_103,cs-410,4,5,"00:07:04,316","00:07:07,885",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,And this is typically going to
cs-410_4_5_104,cs-410,4,5,"00:07:07,885","00:07:12,953",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,then you can use the query to get some
cs-410_4_5_105,cs-410,4,5,"00:07:12,953","00:07:17,052",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,And then you can start it with those
cs-410_4_5_106,cs-410,4,5,"00:07:17,052","00:07:19,544",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,"The one channel in crawling,"
cs-410_4_5_107,cs-410,4,5,"00:07:19,544","00:07:24,230",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,is you will find the new
cs-410_4_5_108,cs-410,4,5,"00:07:24,230","00:07:28,732",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,people probably are creating
cs-410_4_5_109,cs-410,4,5,"00:07:28,732","00:07:33,502",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,And this is very challenging if
cs-410_4_5_110,cs-410,4,5,"00:07:33,502","00:07:35,930",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,linked to any old pages.
cs-410_4_5_111,cs-410,4,5,"00:07:35,930","00:07:41,655",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,"If they are, then you can probably find"
cs-410_4_5_112,cs-410,4,5,"00:07:41,655","00:07:46,946",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,so these are also some interesting
cs-410_4_5_113,cs-410,4,5,"00:07:46,946","00:07:51,257",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,"And finally, we might face the scenario"
cs-410_4_5_114,cs-410,4,5,"00:07:51,257","00:07:53,157",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,"repeated crawling, right."
cs-410_4_5_115,cs-410,4,5,"00:07:53,157","00:07:56,528",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"Let's say,"
cs-410_4_5_116,cs-410,4,5,"00:07:56,528","00:07:59,448",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,and you first crawl a lot
cs-410_4_5_117,cs-410,4,5,"00:07:59,448","00:08:03,816",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"But then,"
cs-410_4_5_118,cs-410,4,5,"00:08:03,816","00:08:08,968",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,in the future you just need
cs-410_4_5_119,cs-410,4,5,"00:08:08,968","00:08:13,277",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"In general, you don't have to"
cs-410_4_5_120,cs-410,4,5,"00:08:13,277","00:08:14,960",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,It's not necessary.
cs-410_4_5_121,cs-410,4,5,"00:08:16,650","00:08:21,563",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"So in this case, your goal is to"
cs-410_4_5_122,cs-410,4,5,"00:08:21,563","00:08:26,400",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,by using minimum resources
cs-410_4_5_123,cs-410,4,5,"00:08:27,490","00:08:33,986",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,"So, this is actually a very"
cs-410_4_5_124,cs-410,4,5,"00:08:33,986","00:08:40,250",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,"and this is a open research question,"
cs-410_4_5_125,cs-410,4,5,"00:08:40,250","00:08:46,060",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,standard algorithms established yet
cs-410_4_5_126,cs-410,4,5,"00:08:47,300","00:08:51,300",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"But in general, you can imagine,"
cs-410_4_5_127,cs-410,4,5,"00:08:53,640","00:08:57,040",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,So the two major factors that
cs-410_4_5_128,cs-410,4,5,"00:08:57,040","00:09:00,760",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,first will this page
cs-410_4_5_129,cs-410,4,5,"00:09:00,760","00:09:03,411",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,And do I have to quote this page again?
cs-410_4_5_130,cs-410,4,5,"00:09:03,411","00:09:07,726",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,If the page is a static page and
cs-410_4_5_131,cs-410,4,5,"00:09:07,726","00:09:12,703",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,you probably don't have to re-crawl it
cs-410_4_5_132,cs-410,4,5,"00:09:12,703","00:09:14,401",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,will changed frequently.
cs-410_4_5_133,cs-410,4,5,"00:09:14,401","00:09:20,152",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,"On the other hand, if it's a sports score"
cs-410_4_5_134,cs-410,4,5,"00:09:20,152","00:09:25,840",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,you may need to re-crawl it and
cs-410_4_5_135,cs-410,4,5,"00:09:25,840","00:09:30,956",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,"The other factor to consider is,"
cs-410_4_5_136,cs-410,4,5,"00:09:30,956","00:09:35,485",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,"If it is, then it means that"
cs-410_4_5_137,cs-410,4,5,"00:09:35,485","00:09:40,809",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,then thus it's more important to
cs-410_4_5_138,cs-410,4,5,"00:09:40,809","00:09:45,439",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,Compared with another page that has
cs-410_4_5_139,cs-410,4,5,"00:09:45,439","00:09:49,609",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,"a year, then even though that"
cs-410_4_5_140,cs-410,4,5,"00:09:49,609","00:09:55,164",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,It's probably not that necessary to
cs-410_4_5_141,cs-410,4,5,"00:09:55,164","00:10:01,697",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,not as urgent as to maintain the freshness
cs-410_4_5_142,cs-410,4,5,"00:10:01,697","00:10:05,275",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,"So to summarize, web search is one of"
cs-410_4_5_143,cs-410,4,5,"00:10:05,275","00:10:08,689",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,retrieval and there are some new
cs-410_4_5_144,cs-410,4,5,"00:10:08,689","00:10:10,463",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"efficiency, quality information."
cs-410_4_5_145,cs-410,4,5,"00:10:10,463","00:10:15,671",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,There are also new opportunities
cs-410_4_5_146,cs-410,4,5,"00:10:15,671","00:10:16,765",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,"layout, etc."
cs-410_4_5_147,cs-410,4,5,"00:10:17,890","00:10:22,500",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,A crawler is an essential component
cs-410_4_5_148,cs-410,4,5,"00:10:22,500","00:10:24,360",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,"in general, you can find two scenarios."
cs-410_4_5_149,cs-410,4,5,"00:10:24,360","00:10:28,730",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,One is initial crawling and
cs-410_4_5_150,cs-410,4,5,"00:10:30,100","00:10:32,970",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,of the web if you are doing
cs-410_4_5_151,cs-410,4,5,"00:10:32,970","00:10:37,560",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=632,focused crawling if you want to just
cs-410_4_5_152,cs-410,4,5,"00:10:38,610","00:10:43,262",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,"And then, there is another scenario that's"
cs-410_4_5_153,cs-410,4,5,"00:10:43,262","00:10:44,611",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,incremental crawling.
cs-410_4_5_154,cs-410,4,5,"00:10:44,611","00:10:48,692",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,"In this case,"
cs-410_4_5_155,cs-410,4,5,"00:10:48,692","00:10:52,588",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,try to use minimum resource
cs-410_4_5_156,cs-410,4,5,"00:10:54,486","00:11:04,486",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,[MUSIC]
cs-410_4_6_1,cs-410,4,6,"00:00:00,012","00:00:04,047",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND].
cs-410_4_6_2,cs-410,4,6,"00:00:07,043","00:00:10,080",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about
cs-410_4_6_3,cs-410,4,6,"00:00:12,660","00:00:17,560",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture, we're going to talk"
cs-410_4_6_4,cs-410,4,6,"00:00:17,560","00:00:22,489",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,of web search and intelligent information
cs-410_4_6_5,cs-410,4,6,"00:00:24,370","00:00:28,561",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,In order to further improve
cs-410_4_6_6,cs-410,4,6,"00:00:28,561","00:00:33,752",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,it's important that to consider
cs-410_4_6_7,cs-410,4,6,"00:00:33,752","00:00:39,056",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,So one particular trend could be to
cs-410_4_6_8,cs-410,4,6,"00:00:39,056","00:00:44,630",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"customized search engines, and they"
cs-410_4_6_9,cs-410,4,6,"00:00:46,260","00:00:50,180",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,These vertical search engines can be
cs-410_4_6_10,cs-410,4,6,"00:00:50,180","00:00:55,940",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,the current general search engines
cs-410_4_6_11,cs-410,4,6,"00:00:55,940","00:01:02,070",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,users are a special group of users that
cs-410_4_6_12,cs-410,4,6,"00:01:02,070","00:01:06,420",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,and then the search engine can be
cs-410_4_6_13,cs-410,4,6,"00:01:07,970","00:01:12,150",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,"And because of the customization,"
cs-410_4_6_14,cs-410,4,6,"00:01:12,150","00:01:14,360",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"So the search can be personalized,"
cs-410_4_6_15,cs-410,4,6,"00:01:15,430","00:01:18,190",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,because we have a better
cs-410_4_6_16,cs-410,4,6,"00:01:20,330","00:01:25,550",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,"Because of the restrictions with domain,"
cs-410_4_6_17,cs-410,4,6,"00:01:25,550","00:01:29,590",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,"in handling the documents, because we can"
cs-410_4_6_18,cs-410,4,6,"00:01:29,590","00:01:33,880",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,"For example, particular words may"
cs-410_4_6_19,cs-410,4,6,"00:01:33,880","00:01:36,750",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,So we can bypass the problem of ambiguity.
cs-410_4_6_20,cs-410,4,6,"00:01:38,390","00:01:41,460",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,"Another trend we can expect to see,"
cs-410_4_6_21,cs-410,4,6,"00:01:41,460","00:01:45,600",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,is the search engine will
cs-410_4_6_22,cs-410,4,6,"00:01:45,600","00:01:52,430",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,It's like a lifetime learning or
cs-410_4_6_23,cs-410,4,6,"00:01:52,430","00:01:57,800",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,very attractive because that means the
cs-410_4_6_24,cs-410,4,6,"00:01:57,800","00:02:01,780",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,"As more people are using it, the search"
cs-410_4_6_25,cs-410,4,6,"00:02:01,780","00:02:03,260",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"this is already happening,"
cs-410_4_6_26,cs-410,4,6,"00:02:03,260","00:02:06,980",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,because the search engines can learn
cs-410_4_6_27,cs-410,4,6,"00:02:06,980","00:02:10,800",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,"More users use it, and the quality"
cs-410_4_6_28,cs-410,4,6,"00:02:10,800","00:02:15,840",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,the popular queries that are typed in by
cs-410_4_6_29,cs-410,4,6,"00:02:15,840","00:02:19,110",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,so this is sort of another
cs-410_4_6_30,cs-410,4,6,"00:02:21,260","00:02:24,600",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,The third trend might be
cs-410_4_6_31,cs-410,4,6,"00:02:24,600","00:02:27,190",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,bottles of information access.
cs-410_4_6_32,cs-410,4,6,"00:02:27,190","00:02:32,050",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"So search, navigation, and"
cs-410_4_6_33,cs-410,4,6,"00:02:32,050","00:02:37,480",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,combined to form a full-fledged
cs-410_4_6_34,cs-410,4,6,"00:02:37,480","00:02:42,470",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,"And in the beginning of this course,"
cs-410_4_6_35,cs-410,4,6,"00:02:42,470","00:02:47,100",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,These are different modes of information
cs-410_4_6_36,cs-410,4,6,"00:02:48,170","00:02:53,660",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,"And similarly, in the pull mode, querying"
cs-410_4_6_37,cs-410,4,6,"00:02:53,660","00:02:58,390",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"And in fact we're doing that basically,"
cs-410_4_6_38,cs-410,4,6,"00:02:58,390","00:03:02,000",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"We are querying, sometimes browsing,"
cs-410_4_6_39,cs-410,4,6,"00:03:02,000","00:03:05,120",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,Sometimes we've got some
cs-410_4_6_40,cs-410,4,6,"00:03:05,120","00:03:11,466",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,Although most of the cases the information
cs-410_4_6_41,cs-410,4,6,"00:03:11,466","00:03:16,305",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,"But in the future, you can imagine"
cs-410_4_6_42,cs-410,4,6,"00:03:16,305","00:03:21,380",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"multi-mode for information access, and"
cs-410_4_6_43,cs-410,4,6,"00:03:23,160","00:03:27,160",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,Another trend is that we might see systems
cs-410_4_6_44,cs-410,4,6,"00:03:27,160","00:03:30,970",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,that try to go beyond the searches
cs-410_4_6_45,cs-410,4,6,"00:03:30,970","00:03:36,730",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,"After all, the reason why people want"
cs-410_4_6_46,cs-410,4,6,"00:03:36,730","00:03:39,690",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,to make a decision or perform a task.
cs-410_4_6_47,cs-410,4,6,"00:03:39,690","00:03:42,380",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,For example consumers might search for
cs-410_4_6_48,cs-410,4,6,"00:03:42,380","00:03:45,385",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,opinions about products in
cs-410_4_6_49,cs-410,4,6,"00:03:45,385","00:03:50,160",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,"choose a good product by, so"
cs-410_4_6_50,cs-410,4,6,"00:03:50,160","00:03:55,330",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,support the whole workflow of purchasing
cs-410_4_6_51,cs-410,4,6,"00:03:56,732","00:04:00,300",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"In this era, after the common search"
cs-410_4_6_52,cs-410,4,6,"00:04:00,300","00:04:04,190",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,"For example, you can sometimes look at the"
cs-410_4_6_53,cs-410,4,6,"00:04:04,190","00:04:09,040",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,you can just click on the button to go the
cs-410_4_6_54,cs-410,4,6,"00:04:09,040","00:04:12,840",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,"But it does not provide a,"
cs-410_4_6_55,cs-410,4,6,"00:04:12,840","00:04:14,720",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,"For example, for researchers,"
cs-410_4_6_56,cs-410,4,6,"00:04:14,720","00:04:18,800",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,you might want to find the realm in
cs-410_4_6_57,cs-410,4,6,"00:04:18,800","00:04:26,550",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,"And then, there's no, not much support for"
cs-410_4_6_58,cs-410,4,6,"00:04:26,550","00:04:31,130",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"So, in general, I think,"
cs-410_4_6_59,cs-410,4,6,"00:04:31,130","00:04:34,980",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,"So in the following few slides, I'll"
cs-410_4_6_60,cs-410,4,6,"00:04:34,980","00:04:39,900",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,"specific ideas or thoughts that hopefully,"
cs-410_4_6_61,cs-410,4,6,"00:04:39,900","00:04:43,720",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,can help you in imagining new
cs-410_4_6_62,cs-410,4,6,"00:04:43,720","00:04:51,330",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,Some of them might be already relevant
cs-410_4_6_63,cs-410,4,6,"00:04:51,330","00:04:55,370",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,"In general, we can think about any"
cs-410_4_6_64,cs-410,4,6,"00:04:55,370","00:05:02,390",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"information system, as we specified"
cs-410_4_6_65,cs-410,4,6,"00:05:02,390","00:05:05,680",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,And so
cs-410_4_6_66,cs-410,4,6,"00:05:05,680","00:05:09,250",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,then we'll able to specify
cs-410_4_6_67,cs-410,4,6,"00:05:09,250","00:05:12,480",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,And I call this
cs-410_4_6_68,cs-410,4,6,"00:05:12,480","00:05:18,110",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,So basically the three questions you
cs-410_4_6_69,cs-410,4,6,"00:05:18,110","00:05:23,470",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,what kind of data are you are managing and
cs-410_4_6_70,cs-410,4,6,"00:05:24,580","00:05:29,360",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,"Right there, this would help us"
cs-410_4_6_71,cs-410,4,6,"00:05:30,650","00:05:33,400",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,And there are many different ways
cs-410_4_6_72,cs-410,4,6,"00:05:33,400","00:05:36,040",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,"how you connect them,"
cs-410_4_6_73,cs-410,4,6,"00:05:36,040","00:05:37,650",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,So let me give you some examples.
cs-410_4_6_74,cs-410,4,6,"00:05:37,650","00:05:40,470",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"On the top,"
cs-410_4_6_75,cs-410,4,6,"00:05:40,470","00:05:45,250",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"On the left side, you can see different"
cs-410_4_6_76,cs-410,4,6,"00:05:45,250","00:05:48,740",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,"on the bottom,"
cs-410_4_6_77,cs-410,4,6,"00:05:48,740","00:05:51,760",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,Now imagine you can connect
cs-410_4_6_78,cs-410,4,6,"00:05:51,760","00:05:55,990",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,"So, for example, you can connect"
cs-410_4_6_79,cs-410,4,6,"00:05:55,990","00:05:59,140",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,the support search and
cs-410_4_6_80,cs-410,4,6,"00:05:59,140","00:06:01,050",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,"Well, that's web search, right?"
cs-410_4_6_81,cs-410,4,6,"00:06:02,440","00:06:07,680",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,What if we connect UIUC employees with
cs-410_4_6_82,cs-410,4,6,"00:06:07,680","00:06:12,720",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,documents to support the search and
cs-410_4_6_83,cs-410,4,6,"00:06:12,720","00:06:17,110",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,If you connect the scientist
cs-410_4_6_84,cs-410,4,6,"00:06:17,110","00:06:22,050",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,"to provide all kinds of service,"
cs-410_4_6_85,cs-410,4,6,"00:06:22,050","00:06:28,310",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,alert of new random documents or
cs-410_4_6_86,cs-410,4,6,"00:06:28,310","00:06:31,490",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,or provide the task with support or
cs-410_4_6_87,cs-410,4,6,"00:06:31,490","00:06:36,600",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,"For example, we might be,"
cs-410_4_6_88,cs-410,4,6,"00:06:36,600","00:06:40,140",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,automatically generating
cs-410_4_6_89,cs-410,4,6,"00:06:40,140","00:06:44,440",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,"a research paper, and"
cs-410_4_6_90,cs-410,4,6,"00:06:44,440","00:06:45,270",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,Right?
cs-410_4_6_91,cs-410,4,6,"00:06:45,270","00:06:48,010",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,we can imagine this would
cs-410_4_6_92,cs-410,4,6,"00:06:48,010","00:06:52,800",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,If we connect the online shoppers
cs-410_4_6_93,cs-410,4,6,"00:06:53,890","00:06:59,825",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,then we can help these people
cs-410_4_6_94,cs-410,4,6,"00:06:59,825","00:07:05,465",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,"So we can provide, for example data mining"
cs-410_4_6_95,cs-410,4,6,"00:07:05,465","00:07:11,950",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"to compare products, compare sentiment of"
cs-410_4_6_96,cs-410,4,6,"00:07:11,950","00:07:15,950",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,decision support to have them
cs-410_4_6_97,cs-410,4,6,"00:07:15,950","00:07:21,510",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,Or we can connect customer service
cs-410_4_6_98,cs-410,4,6,"00:07:22,630","00:07:27,660",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,"and, and we can imagine a system"
cs-410_4_6_99,cs-410,4,6,"00:07:27,660","00:07:31,460",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,of these emails to find that the major
cs-410_4_6_100,cs-410,4,6,"00:07:31,460","00:07:35,150",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,We can imagine a system we
cs-410_4_6_101,cs-410,4,6,"00:07:35,150","00:07:39,630",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,by automatically generating
cs-410_4_6_102,cs-410,4,6,"00:07:39,630","00:07:45,720",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,Maybe intelligently attach
cs-410_4_6_103,cs-410,4,6,"00:07:45,720","00:07:49,830",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"if appropriate, if they detect that that's"
cs-410_4_6_104,cs-410,4,6,"00:07:49,830","00:07:55,290",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,then you might take this opportunity
cs-410_4_6_105,cs-410,4,6,"00:07:55,290","00:07:57,770",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,"Whereas if it's a complaint,"
cs-410_4_6_106,cs-410,4,6,"00:07:59,510","00:08:03,810",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,automatically generate some
cs-410_4_6_107,cs-410,4,6,"00:08:03,810","00:08:08,790",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,tell the customer that he or she can
cs-410_4_6_108,cs-410,4,6,"00:08:08,790","00:08:14,210",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,All of these are trying to help
cs-410_4_6_109,cs-410,4,6,"00:08:15,570","00:08:19,850",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,So this shows that
cs-410_4_6_110,cs-410,4,6,"00:08:19,850","00:08:22,090",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,It's just only restricted
cs-410_4_6_111,cs-410,4,6,"00:08:22,090","00:08:27,400",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,So this picture shows the trend
cs-410_4_6_112,cs-410,4,6,"00:08:27,400","00:08:33,770",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,"it characterizes the, intelligent"
cs-410_4_6_113,cs-410,4,6,"00:08:33,770","00:08:39,065",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,"You can see in the center, there's"
cs-410_4_6_114,cs-410,4,6,"00:08:39,065","00:08:41,225",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,to search a bag of words representation.
cs-410_4_6_115,cs-410,4,6,"00:08:41,225","00:08:46,721",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,That means the current search engines
cs-410_4_6_116,cs-410,4,6,"00:08:46,721","00:08:54,085",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,to users and mostly model
cs-410_4_6_117,cs-410,4,6,"00:08:54,085","00:08:59,105",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,and sees the data through
cs-410_4_6_118,cs-410,4,6,"00:08:59,105","00:09:06,190",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,So it's a very simple approximation of
cs-410_4_6_119,cs-410,4,6,"00:09:06,190","00:09:08,500",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,But that's what the current system does.
cs-410_4_6_120,cs-410,4,6,"00:09:08,500","00:09:12,150",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,It connects these three nodes
cs-410_4_6_121,cs-410,4,6,"00:09:12,150","00:09:17,655",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,it only provides a basic search function
cs-410_4_6_122,cs-410,4,6,"00:09:17,655","00:09:24,405",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,and it doesn't really understand that
cs-410_4_6_123,cs-410,4,6,"00:09:24,405","00:09:31,862",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"Now, I showed some trends to push each"
cs-410_4_6_124,cs-410,4,6,"00:09:31,862","00:09:35,332",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"So think about the user node here, right?"
cs-410_4_6_125,cs-410,4,6,"00:09:35,332","00:09:39,432",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,"So we can go beyond the keyword queries,"
cs-410_4_6_126,cs-410,4,6,"00:09:39,432","00:09:43,882",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,and then further model the user
cs-410_4_6_127,cs-410,4,6,"00:09:43,882","00:09:49,622",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"the user's task environment,"
cs-410_4_6_128,cs-410,4,6,"00:09:49,622","00:09:55,120",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,"Okay, so this is pushing for"
cs-410_4_6_129,cs-410,4,6,"00:09:55,120","00:09:58,630",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,And this is a major
cs-410_4_6_130,cs-410,4,6,"00:09:58,630","00:10:01,810",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,in order to build intelligent
cs-410_4_6_131,cs-410,4,6,"00:10:01,810","00:10:05,810",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,"On the document side,"
cs-410_4_6_132,cs-410,4,6,"00:10:05,810","00:10:10,640",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,go beyond bag of words implementation
cs-410_4_6_133,cs-410,4,6,"00:10:10,640","00:10:16,040",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,"This means we'll recognize people's names,"
cs-410_4_6_134,cs-410,4,6,"00:10:16,040","00:10:20,430",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,And this is already feasible with
cs-410_4_6_135,cs-410,4,6,"00:10:20,430","00:10:24,130",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,And Google is the reason
cs-410_4_6_136,cs-410,4,6,"00:10:24,130","00:10:28,310",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,"If you haven't heard of it,"
cs-410_4_6_137,cs-410,4,6,"00:10:28,310","00:10:33,820",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,And once we can get to that level without
cs-410_4_6_138,cs-410,4,6,"00:10:33,820","00:10:38,170",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,it can enable the search engine
cs-410_4_6_139,cs-410,4,6,"00:10:38,170","00:10:41,470",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,In the future we would like to have
cs-410_4_6_140,cs-410,4,6,"00:10:41,470","00:10:45,450",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,knowledge representation where we
cs-410_4_6_141,cs-410,4,6,"00:10:45,450","00:10:47,750",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,then the search engine would
cs-410_4_6_142,cs-410,4,6,"00:10:49,390","00:10:53,490",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,So this calls for
cs-410_4_6_143,cs-410,4,6,"00:10:53,490","00:10:57,150",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,perhaps this is more feasible for
cs-410_4_6_144,cs-410,4,6,"00:10:57,150","00:10:59,800",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,It's easier to make progress
cs-410_4_6_145,cs-410,4,6,"00:10:59,800","00:11:01,240",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,"Now on the service side,"
cs-410_4_6_146,cs-410,4,6,"00:11:01,240","00:11:05,920",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=661,we see we need to go beyond the search of
cs-410_4_6_147,cs-410,4,6,"00:11:07,510","00:11:13,702",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,So search is only one way to get access
cs-410_4_6_148,cs-410,4,6,"00:11:13,702","00:11:19,980",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,systems and push and pull so different
cs-410_4_6_149,cs-410,4,6,"00:11:19,980","00:11:21,630",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,"But going beyond access,"
cs-410_4_6_150,cs-410,4,6,"00:11:21,630","00:11:25,820",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,we also need to help people digest the
cs-410_4_6_151,cs-410,4,6,"00:11:25,820","00:11:30,560",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,and this step has to do with analysis
cs-410_4_6_152,cs-410,4,6,"00:11:30,560","00:11:35,540",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,We have to find patterns or
cs-410_4_6_153,cs-410,4,6,"00:11:35,540","00:11:38,865",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=695,real knowledge that can
cs-410_4_6_154,cs-410,4,6,"00:11:38,865","00:11:43,055",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,actionable knowledge that can be used for
cs-410_4_6_155,cs-410,4,6,"00:11:43,055","00:11:47,165",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=703,And furthermore the knowledge
cs-410_4_6_156,cs-410,4,6,"00:11:47,165","00:11:52,580",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,"improve productivity in finishing a task,"
cs-410_4_6_157,cs-410,4,6,"00:11:52,580","00:11:54,000",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,"Right, so this is a trend."
cs-410_4_6_158,cs-410,4,6,"00:11:54,000","00:11:59,370",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,"And, and, and so basically,"
cs-410_4_6_159,cs-410,4,6,"00:11:59,370","00:12:04,210",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=719,in the future intelligent information
cs-410_4_6_160,cs-410,4,6,"00:12:04,210","00:12:06,940",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,interactive task support.
cs-410_4_6_161,cs-410,4,6,"00:12:06,940","00:12:11,200",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=726,Now I should also emphasize interactive
cs-410_4_6_162,cs-410,4,6,"00:12:11,200","00:12:16,790",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,the combined intelligence of the users and
cs-410_4_6_163,cs-410,4,6,"00:12:16,790","00:12:22,140",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,"So we, we can get some help"
cs-410_4_6_164,cs-410,4,6,"00:12:22,140","00:12:26,970",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=742,And we don't have to assume the system
cs-410_4_6_165,cs-410,4,6,"00:12:26,970","00:12:32,290",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,"user, and the machine can collaborate in"
cs-410_4_6_166,cs-410,4,6,"00:12:32,290","00:12:37,270",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,then the combined intelligence
cs-410_4_6_167,cs-410,4,6,"00:12:37,270","00:12:41,010",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,we can minimize the user's overall
cs-410_4_6_168,cs-410,4,6,"00:12:42,700","00:12:47,947",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=762,So this is the big picture of future
cs-410_4_6_169,cs-410,4,6,"00:12:47,947","00:12:52,582",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=767,and this hopefully can provide
cs-410_4_6_170,cs-410,4,6,"00:12:52,582","00:12:57,313",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=772,how to make further innovations
cs-410_4_6_171,cs-410,4,6,"00:12:57,313","00:13:07,313",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=777,[MUSIC]
cs-410_4_7_1,cs-410,4,7,"00:00:00,266","00:00:05,440",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_4_7_2,cs-410,4,7,"00:00:10,218","00:00:13,988",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,So here are some specific examples of what
cs-410_4_7_3,cs-410,4,7,"00:00:13,988","00:00:15,926",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,we can't do today and
cs-410_4_7_4,cs-410,4,7,"00:00:15,926","00:00:21,970",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,part of speech tagging is still
cs-410_4_7_5,cs-410,4,7,"00:00:21,970","00:00:27,938",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,"So in the example, he turned off the"
cs-410_4_7_6,cs-410,4,7,"00:00:27,938","00:00:33,342",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,the two offs actually have somewhat
cs-410_4_7_7,cs-410,4,7,"00:00:33,342","00:00:39,633",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,categories and also its very difficult
cs-410_4_7_8,cs-410,4,7,"00:00:39,633","00:00:44,450",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"Again, the example, a man saw a boy"
cs-410_4_7_9,cs-410,4,7,"00:00:44,450","00:00:48,400",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,be very difficult to parse
cs-410_4_7_10,cs-410,4,7,"00:00:48,400","00:00:52,278",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,Precise deep semantic
cs-410_4_7_11,cs-410,4,7,"00:00:52,278","00:00:55,648",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,"For example, to define the meaning of own,"
cs-410_4_7_12,cs-410,4,7,"00:00:55,648","00:01:01,493",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,precisely is very difficult in
cs-410_4_7_13,cs-410,4,7,"00:01:01,493","00:01:04,737",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,So the state of the off can
cs-410_4_7_14,cs-410,4,7,"00:01:04,737","00:01:05,506",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,Robust and
cs-410_4_7_15,cs-410,4,7,"00:01:05,506","00:01:11,070",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,general NLP tends to be shallow while
cs-410_4_7_16,cs-410,4,7,"00:01:12,430","00:01:18,565",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"For this reason in this course,"
cs-410_4_7_17,cs-410,4,7,"00:01:18,565","00:01:23,610",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"general, shallow techniques for"
cs-410_4_7_18,cs-410,4,7,"00:01:23,610","00:01:29,630",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,mining text data and they are generally
cs-410_4_7_19,cs-410,4,7,"00:01:29,630","00:01:34,510",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,So there are robust and
cs-410_4_7_20,cs-410,4,7,"00:01:36,540","00:01:39,550",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,the in category of shallow analysis.
cs-410_4_7_21,cs-410,4,7,"00:01:39,550","00:01:44,320",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,So such techniques have
cs-410_4_7_22,cs-410,4,7,"00:01:44,320","00:01:49,099",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,applied to any text data in
cs-410_4_7_23,cs-410,4,7,"00:01:49,099","00:01:55,425",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"But the downside is that, they don't"
cs-410_4_7_24,cs-410,4,7,"00:01:55,425","00:01:59,159",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,"For that, we have to rely on"
cs-410_4_7_25,cs-410,4,7,"00:02:00,960","00:02:05,930",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,That typically would require
cs-410_4_7_26,cs-410,4,7,"00:02:05,930","00:02:10,940",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,a lot of examples of analysis that would
cs-410_4_7_27,cs-410,4,7,"00:02:10,940","00:02:16,120",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,machine learning techniques and learn from
cs-410_4_7_28,cs-410,4,7,"00:02:16,120","00:02:21,880",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,"So in practical applications, we generally"
cs-410_4_7_29,cs-410,4,7,"00:02:21,880","00:02:29,150",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,with the general statistical and
cs-410_4_7_30,cs-410,4,7,"00:02:29,150","00:02:32,010",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,These can be applied to any text data.
cs-410_4_7_31,cs-410,4,7,"00:02:32,010","00:02:37,060",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,"And on top of that, we're going to use"
cs-410_4_7_32,cs-410,4,7,"00:02:37,060","00:02:42,770",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,to use supervised machine learning
cs-410_4_7_33,cs-410,4,7,"00:02:42,770","00:02:48,640",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,especially for those important
cs-410_4_7_34,cs-410,4,7,"00:02:48,640","00:02:55,170",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,to analyze text data more precisely.
cs-410_4_7_35,cs-410,4,7,"00:02:55,170","00:03:00,036",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,But this course will cover
cs-410_4_7_36,cs-410,4,7,"00:03:00,036","00:03:04,177",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"that generally,"
cs-410_4_7_37,cs-410,4,7,"00:03:04,177","00:03:09,386",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,"So they're practically,"
cs-410_4_7_38,cs-410,4,7,"00:03:09,386","00:03:16,409",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,analysis techniques that require a lot of
cs-410_4_7_39,cs-410,4,7,"00:03:16,409","00:03:21,302",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"So to summarize,"
cs-410_4_7_40,cs-410,4,7,"00:03:21,302","00:03:24,580",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,is the foundation for text mining.
cs-410_4_7_41,cs-410,4,7,"00:03:24,580","00:03:27,465",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,"So obviously, the better we"
cs-410_4_7_42,cs-410,4,7,"00:03:27,465","00:03:29,090",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,the better we can do text mining.
cs-410_4_7_43,cs-410,4,7,"00:03:30,420","00:03:34,930",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,Computers today are far from being able
cs-410_4_7_44,cs-410,4,7,"00:03:34,930","00:03:38,030",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,Deep NLP requires common sense
cs-410_4_7_45,cs-410,4,7,"00:03:38,030","00:03:42,803",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"Thus, only working for"
cs-410_4_7_46,cs-410,4,7,"00:03:42,803","00:03:44,833",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,large scale text mining.
cs-410_4_7_47,cs-410,4,7,"00:03:44,833","00:03:50,003",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,Shallow NLP based on statistical
cs-410_4_7_48,cs-410,4,7,"00:03:50,003","00:03:52,543",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,is the main topic of this course and
cs-410_4_7_49,cs-410,4,7,"00:03:52,543","00:03:56,763",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,they are generally applicable
cs-410_4_7_50,cs-410,4,7,"00:03:56,763","00:04:02,081",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"They are in some sense also,"
cs-410_4_7_51,cs-410,4,7,"00:04:02,081","00:04:06,834",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,"In practice,"
cs-410_4_7_52,cs-410,4,7,"00:04:06,834","00:04:11,810",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,we'll have humans for
cs-410_4_7_53,cs-410,4,7,"00:04:11,810","00:04:21,810",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,[MUSIC]
cs-410_4_8_1,cs-410,4,8,"00:00:00,000","00:00:04,714",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_4_8_2,cs-410,4,8,"00:00:06,455","00:00:09,677",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,"In general, we can use the empirical count"
cs-410_4_8_3,cs-410,4,8,"00:00:09,677","00:00:15,340",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,of events in the observed data
cs-410_4_8_4,cs-410,4,8,"00:00:15,340","00:00:19,080",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,And a commonly used technique is
cs-410_4_8_5,cs-410,4,8,"00:00:19,080","00:00:22,600",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,where we simply normalize
cs-410_4_8_6,cs-410,4,8,"00:00:22,600","00:00:30,330",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,"So if we do that, we can see, we can"
cs-410_4_8_7,cs-410,4,8,"00:00:30,330","00:00:36,811",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,For estimating the probability that
cs-410_4_8_8,cs-410,4,8,"00:00:36,811","00:00:42,773",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,we simply normalize the count of
cs-410_4_8_9,cs-410,4,8,"00:00:42,773","00:00:47,278",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,So let's first take
cs-410_4_8_10,cs-410,4,8,"00:00:47,278","00:00:52,970",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,"On the right side, you see a list of some,"
cs-410_4_8_11,cs-410,4,8,"00:00:52,970","00:00:55,010",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,These are segments.
cs-410_4_8_12,cs-410,4,8,"00:00:55,010","00:00:59,860",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,And in some segments you see both words
cs-410_4_8_13,cs-410,4,8,"00:00:59,860","00:01:01,630",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,both columns.
cs-410_4_8_14,cs-410,4,8,"00:01:01,630","00:01:05,830",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"In some other cases only one will occur,"
cs-410_4_8_15,cs-410,4,8,"00:01:05,830","00:01:07,590",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,the other column has zero.
cs-410_4_8_16,cs-410,4,8,"00:01:07,590","00:01:11,130",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,"And in all, of course, in some other"
cs-410_4_8_17,cs-410,4,8,"00:01:11,130","00:01:13,930",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,so they are both zeros.
cs-410_4_8_18,cs-410,4,8,"00:01:13,930","00:01:19,310",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"And for estimating these probabilities, we"
cs-410_4_8_19,cs-410,4,8,"00:01:20,340","00:01:23,560",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,"So the three counts are first,"
cs-410_4_8_20,cs-410,4,8,"00:01:23,560","00:01:27,337",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,And that's the total number of
cs-410_4_8_21,cs-410,4,8,"00:01:27,337","00:01:30,950",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,It's just as the ones in the column of W1.
cs-410_4_8_22,cs-410,4,8,"00:01:30,950","00:01:34,470",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,We can count how many
cs-410_4_8_23,cs-410,4,8,"00:01:34,470","00:01:40,460",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"The segment count is for word 2, and we"
cs-410_4_8_24,cs-410,4,8,"00:01:40,460","00:01:45,425",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,And these will give us the total
cs-410_4_8_25,cs-410,4,8,"00:01:45,425","00:01:49,650",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,The third count is when both words occur.
cs-410_4_8_26,cs-410,4,8,"00:01:49,650","00:01:55,370",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"So this time, we're going to count"
cs-410_4_8_27,cs-410,4,8,"00:01:56,580","00:02:00,060",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,"And then, so this would give us"
cs-410_4_8_28,cs-410,4,8,"00:02:00,060","00:02:03,510",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,where we have seen both W1 and W2.
cs-410_4_8_29,cs-410,4,8,"00:02:03,510","00:02:08,112",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,"Once we have these counts,"
cs-410_4_8_30,cs-410,4,8,"00:02:08,112","00:02:11,019",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,"which is the total number of segments, and"
cs-410_4_8_31,cs-410,4,8,"00:02:11,019","00:02:16,706",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,this will give us the probabilities that
cs-410_4_8_32,cs-410,4,8,"00:02:16,706","00:02:22,301",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,"Now, there is a small problem,"
cs-410_4_8_33,cs-410,4,8,"00:02:22,301","00:02:27,458",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"And in this case, we don't want a zero"
cs-410_4_8_34,cs-410,4,8,"00:02:27,458","00:02:33,365",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"a small sample and in general, we would"
cs-410_4_8_35,cs-410,4,8,"00:02:33,365","00:02:35,806",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,a [INAUDIBLE] to avoid any context.
cs-410_4_8_36,cs-410,4,8,"00:02:35,806","00:02:39,630",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,"So, to address this problem,"
cs-410_4_8_37,cs-410,4,8,"00:02:39,630","00:02:43,780",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,And that's basically to add some
cs-410_4_8_38,cs-410,4,8,"00:02:43,780","00:02:48,410",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,and so that we don't get
cs-410_4_8_39,cs-410,4,8,"00:02:48,410","00:02:54,250",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,"Now, the best way to understand smoothing"
cs-410_4_8_40,cs-410,4,8,"00:02:54,250","00:03:00,310",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,"data than we actually have, because we'll"
cs-410_4_8_41,cs-410,4,8,"00:03:00,310","00:03:04,650",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"I illustrated on the top,"
cs-410_4_8_42,cs-410,4,8,"00:03:04,650","00:03:10,095",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,And these pseudo-segments would
cs-410_4_8_43,cs-410,4,8,"00:03:10,095","00:03:15,047",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,of these words so
cs-410_4_8_44,cs-410,4,8,"00:03:15,047","00:03:18,169",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,"Now, in particular we introduce"
cs-410_4_8_45,cs-410,4,8,"00:03:18,169","00:03:20,990",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,Each is weighted at one quarter.
cs-410_4_8_46,cs-410,4,8,"00:03:20,990","00:03:25,930",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,And these represent the four different
cs-410_4_8_47,cs-410,4,8,"00:03:25,930","00:03:30,490",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,"So now each event,"
cs-410_4_8_48,cs-410,4,8,"00:03:30,490","00:03:35,390",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,at least one count or at least a non-zero
cs-410_4_8_49,cs-410,4,8,"00:03:35,390","00:03:39,380",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,"So, in the actual segments"
cs-410_4_8_50,cs-410,4,8,"00:03:39,380","00:03:44,231",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,it's okay if we haven't observed
cs-410_4_8_51,cs-410,4,8,"00:03:44,231","00:03:49,671",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,"So more specifically, you can see"
cs-410_4_8_52,cs-410,4,8,"00:03:49,671","00:03:55,560",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,"ones in the two pseudo-segments,"
cs-410_4_8_53,cs-410,4,8,"00:03:55,560","00:03:59,315",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,"We add them up, we get 0.5."
cs-410_4_8_54,cs-410,4,8,"00:03:59,315","00:04:03,319",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"And similar to this,"
cs-410_4_8_55,cs-410,4,8,"00:04:03,319","00:04:08,240",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,pseudo-segment that indicates
cs-410_4_8_56,cs-410,4,8,"00:04:09,450","00:04:14,000",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,And of course in the denominator we add
cs-410_4_8_57,cs-410,4,8,"00:04:14,000","00:04:17,520",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,"we add, in this case,"
cs-410_4_8_58,cs-410,4,8,"00:04:17,520","00:04:21,780",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,Each is weighed at one quarter so
cs-410_4_8_59,cs-410,4,8,"00:04:21,780","00:04:24,110",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,"So, that's why in the denominator"
cs-410_4_8_60,cs-410,4,8,"00:04:25,990","00:04:31,460",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,"So, this basically concludes"
cs-410_4_8_61,cs-410,4,8,"00:04:31,460","00:04:33,920",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,four syntagmatic relation discoveries.
cs-410_4_8_62,cs-410,4,8,"00:04:36,090","00:04:42,050",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,"Now, so to summarize,"
cs-410_4_8_63,cs-410,4,8,"00:04:42,050","00:04:46,240",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,be discovered by measuring correlations
cs-410_4_8_64,cs-410,4,8,"00:04:46,240","00:04:49,580",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,We've introduced the three
cs-410_4_8_65,cs-410,4,8,"00:04:49,580","00:04:53,230",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,"Entropy, which measures the uncertainty"
cs-410_4_8_66,cs-410,4,8,"00:04:53,230","00:04:59,060",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"Conditional entropy, which measures"
cs-410_4_8_67,cs-410,4,8,"00:04:59,060","00:05:04,530",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,"And mutual information of X and Y,"
cs-410_4_8_68,cs-410,4,8,"00:05:04,530","00:05:11,240",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"due to knowing Y, or"
cs-410_4_8_69,cs-410,4,8,"00:05:11,240","00:05:12,660",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,They are the same.
cs-410_4_8_70,cs-410,4,8,"00:05:12,660","00:05:17,111",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,So these three concepts are actually very
cs-410_4_8_71,cs-410,4,8,"00:05:17,111","00:05:20,340",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,That's why we spent some time
cs-410_4_8_72,cs-410,4,8,"00:05:20,340","00:05:23,150",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,"But in particular,"
cs-410_4_8_73,cs-410,4,8,"00:05:23,150","00:05:25,960",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,discovering syntagmatic relations.
cs-410_4_8_74,cs-410,4,8,"00:05:25,960","00:05:30,142",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,"In particular,"
cs-410_4_8_75,cs-410,4,8,"00:05:30,142","00:05:32,370",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,discovering such a relation.
cs-410_4_8_76,cs-410,4,8,"00:05:32,370","00:05:37,241",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,It allows us to have values
cs-410_4_8_77,cs-410,4,8,"00:05:37,241","00:05:42,211",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,words that are comparable and
cs-410_4_8_78,cs-410,4,8,"00:05:42,211","00:05:48,208",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,discover the strongest syntagmatic
cs-410_4_8_79,cs-410,4,8,"00:05:48,208","00:05:53,700",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"Now, note that there is some relation"
cs-410_4_8_80,cs-410,4,8,"00:05:53,700","00:05:55,910",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,[INAUDIBLE] relation discovery.
cs-410_4_8_81,cs-410,4,8,"00:05:55,910","00:06:01,835",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,So we already discussed the possibility
cs-410_4_8_82,cs-410,4,8,"00:06:01,835","00:06:06,683",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,terms in the context to potentially
cs-410_4_8_83,cs-410,4,8,"00:06:06,683","00:06:11,187",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,that have syntagmatic relations
cs-410_4_8_84,cs-410,4,8,"00:06:11,187","00:06:17,958",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"But here, once we use mutual information"
cs-410_4_8_85,cs-410,4,8,"00:06:17,958","00:06:24,436",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,we can also represent the context with
cs-410_4_8_86,cs-410,4,8,"00:06:24,436","00:06:29,567",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,So this would give us
cs-410_4_8_87,cs-410,4,8,"00:06:29,567","00:06:33,490",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"the context of a word, like a cat."
cs-410_4_8_88,cs-410,4,8,"00:06:33,490","00:06:37,394",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,"And if we do the same for all the words,"
cs-410_4_8_89,cs-410,4,8,"00:06:37,394","00:06:42,320",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,compare the similarity between these
cs-410_4_8_90,cs-410,4,8,"00:06:42,320","00:06:45,850",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,So this provides yet
cs-410_4_8_91,cs-410,4,8,"00:06:45,850","00:06:48,800",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,paradigmatic relation discovery.
cs-410_4_8_92,cs-410,4,8,"00:06:48,800","00:06:55,770",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,And so to summarize this whole part
cs-410_4_8_93,cs-410,4,8,"00:06:55,770","00:06:59,190",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,"We introduce two basic associations,"
cs-410_4_8_94,cs-410,4,8,"00:06:59,190","00:07:01,000",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,a syntagmatic relations.
cs-410_4_8_95,cs-410,4,8,"00:07:01,000","00:07:05,710",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,"These are fairly general, they apply"
cs-410_4_8_96,cs-410,4,8,"00:07:05,710","00:07:10,009",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"the units don't have to be words,"
cs-410_4_8_97,cs-410,4,8,"00:07:11,120","00:07:16,235",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,We introduced multiple statistical
cs-410_4_8_98,cs-410,4,8,"00:07:16,235","00:07:20,762",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,mainly showing that pure
cs-410_4_8_99,cs-410,4,8,"00:07:20,762","00:07:24,840",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,are variable for
cs-410_4_8_100,cs-410,4,8,"00:07:24,840","00:07:28,800",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,And they can be combined to
cs-410_4_8_101,cs-410,4,8,"00:07:28,800","00:07:35,040",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,These approaches can be applied
cs-410_4_8_102,cs-410,4,8,"00:07:35,040","00:07:39,940",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,mostly because they are based
cs-410_4_8_103,cs-410,4,8,"00:07:39,940","00:07:42,690",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,they can actually discover
cs-410_4_8_104,cs-410,4,8,"00:07:44,360","00:07:47,880",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,We can also use different ways with
cs-410_4_8_105,cs-410,4,8,"00:07:47,880","00:07:51,360",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,this would lead us to some interesting
cs-410_4_8_106,cs-410,4,8,"00:07:51,360","00:07:56,190",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,"For example, the context can be very"
cs-410_4_8_107,cs-410,4,8,"00:07:56,190","00:08:00,760",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,"a sentence, or maybe paragraphs,"
cs-410_4_8_108,cs-410,4,8,"00:08:00,760","00:08:05,330",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,allows to discover different flavors
cs-410_4_8_109,cs-410,4,8,"00:08:05,330","00:08:09,362",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,"And similarly,"
cs-410_4_8_110,cs-410,4,8,"00:08:09,362","00:08:13,380",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,visual information to discover
cs-410_4_8_111,cs-410,4,8,"00:08:13,380","00:08:19,110",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"We also have to define the segment, and"
cs-410_4_8_112,cs-410,4,8,"00:08:19,110","00:08:22,560",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,text window or a longer text article.
cs-410_4_8_113,cs-410,4,8,"00:08:22,560","00:08:26,508",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,And this would give us different
cs-410_4_8_114,cs-410,4,8,"00:08:26,508","00:08:32,677",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,These discovery associations can
cs-410_4_8_115,cs-410,4,8,"00:08:32,677","00:08:37,701",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,in both information retrieval and
cs-410_4_8_116,cs-410,4,8,"00:08:37,701","00:08:44,100",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,"So here are some recommended readings,"
cs-410_4_8_117,cs-410,4,8,"00:08:44,100","00:08:46,880",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,The first is a book with
cs-410_4_8_118,cs-410,4,8,"00:08:46,880","00:08:50,810",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,which is quite relevant to
cs-410_4_8_119,cs-410,4,8,"00:08:50,810","00:08:55,120",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,The second is an article
cs-410_4_8_120,cs-410,4,8,"00:08:55,120","00:08:58,160",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,statistical measures to
cs-410_4_8_121,cs-410,4,8,"00:08:58,160","00:09:03,764",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,Those are phrases that
cs-410_4_8_122,cs-410,4,8,"00:09:03,764","00:09:07,560",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,"For example,"
cs-410_4_8_123,cs-410,4,8,"00:09:08,610","00:09:11,550",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,blue chip is not a chip that's blue.
cs-410_4_8_124,cs-410,4,8,"00:09:11,550","00:09:16,180",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,And the paper has a discussion about some
cs-410_4_8_125,cs-410,4,8,"00:09:17,400","00:09:23,227",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,The third one is a new paper on a unified
cs-410_4_8_126,cs-410,4,8,"00:09:23,227","00:09:29,441",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,"relations and a syntagmatical relations,"
cs-410_4_8_127,cs-410,4,8,"00:09:29,441","00:09:39,441",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,[SOUND]
cs-410_5_1_1,cs-410,5,1,"00:00:00,008","00:00:07,957",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_5_1_2,cs-410,5,1,"00:00:07,957","00:00:11,940",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about the
cs-410_5_1_3,cs-410,5,1,"00:00:11,940","00:00:14,410",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,We're going to give
cs-410_5_1_4,cs-410,5,1,"00:00:18,800","00:00:23,730",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"In the last lecture, we talked about"
cs-410_5_1_5,cs-410,5,1,"00:00:23,730","00:00:29,000",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,"a retrieval model, which would give"
cs-410_5_1_6,cs-410,5,1,"00:00:30,270","00:00:33,600",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"In this lecture, we're going to"
cs-410_5_1_7,cs-410,5,1,"00:00:33,600","00:00:36,640",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,designing a ramping function called
cs-410_5_1_8,cs-410,5,1,"00:00:37,760","00:00:41,500",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,And we're going to give a brief
cs-410_5_1_9,cs-410,5,1,"00:00:44,330","00:00:47,320",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,Vector space model is a special case of
cs-410_5_1_10,cs-410,5,1,"00:00:47,320","00:00:50,800",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,similarity based models
cs-410_5_1_11,cs-410,5,1,"00:00:50,800","00:00:56,049",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,Which means we assume relevance
cs-410_5_1_12,cs-410,5,1,"00:00:56,049","00:00:59,450",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,between the document and the query.
cs-410_5_1_13,cs-410,5,1,"00:01:02,140","00:01:06,280",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,Now whether is this assumption
cs-410_5_1_14,cs-410,5,1,"00:01:06,280","00:01:09,965",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"But in order to solve the search problem,"
cs-410_5_1_15,cs-410,5,1,"00:01:09,965","00:01:15,860",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,we have to convert the vague notion
cs-410_5_1_16,cs-410,5,1,"00:01:15,860","00:01:21,459",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,definition that can be implemented
cs-410_5_1_17,cs-410,5,1,"00:01:21,459","00:01:26,430",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,"So in this process,"
cs-410_5_1_18,cs-410,5,1,"00:01:26,430","00:01:31,510",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,This is the first assumption
cs-410_5_1_19,cs-410,5,1,"00:01:31,510","00:01:36,091",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"Basically, we assume that if a document"
cs-410_5_1_20,cs-410,5,1,"00:01:36,091","00:01:37,419",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,another document.
cs-410_5_1_21,cs-410,5,1,"00:01:37,419","00:01:42,070",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,Then the first document will be assumed it
cs-410_5_1_22,cs-410,5,1,"00:01:42,070","00:01:45,310",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,And this is the basis for
cs-410_5_1_23,cs-410,5,1,"00:01:46,800","00:01:51,970",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"Again, it's questionable whether this is"
cs-410_5_1_24,cs-410,5,1,"00:01:51,970","00:01:55,750",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,As we will see later there
cs-410_5_1_25,cs-410,5,1,"00:01:58,300","00:01:59,790",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,The basic idea of vectors for
cs-410_5_1_26,cs-410,5,1,"00:01:59,790","00:02:03,070",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,base retrieval model is actually
cs-410_5_1_27,cs-410,5,1,"00:02:03,070","00:02:10,300",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,Imagine a high dimensional space where
cs-410_5_1_28,cs-410,5,1,"00:02:11,660","00:02:17,088",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,So here I issue a three dimensional
cs-410_5_1_29,cs-410,5,1,"00:02:17,088","00:02:21,120",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,"programming, library and presidential."
cs-410_5_1_30,cs-410,5,1,"00:02:21,120","00:02:23,260",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,So each term here defines one dimension.
cs-410_5_1_31,cs-410,5,1,"00:02:24,370","00:02:28,966",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"Now we can consider vectors in this,"
cs-410_5_1_32,cs-410,5,1,"00:02:28,966","00:02:32,275",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,And we're going to assume
cs-410_5_1_33,cs-410,5,1,"00:02:32,275","00:02:36,340",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,the query will be placed
cs-410_5_1_34,cs-410,5,1,"00:02:36,340","00:02:43,526",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,"So for example, on document might"
cs-410_5_1_35,cs-410,5,1,"00:02:43,526","00:02:48,710",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,Now this means this document
cs-410_5_1_36,cs-410,5,1,"00:02:48,710","00:02:54,657",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,"presidential, but"
cs-410_5_1_37,cs-410,5,1,"00:02:54,657","00:03:00,270",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,What does this mean in terms
cs-410_5_1_38,cs-410,5,1,"00:03:00,270","00:03:04,370",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,That just means we're going to look at
cs-410_5_1_39,cs-410,5,1,"00:03:04,370","00:03:05,710",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,this vector.
cs-410_5_1_40,cs-410,5,1,"00:03:05,710","00:03:07,910",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,We're going to ignore everything else.
cs-410_5_1_41,cs-410,5,1,"00:03:07,910","00:03:12,950",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,"Basically, what we see here is only"
cs-410_5_1_42,cs-410,5,1,"00:03:14,470","00:03:16,380",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,"Of course,"
cs-410_5_1_43,cs-410,5,1,"00:03:16,380","00:03:20,223",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"For example, the orders of"
cs-410_5_1_44,cs-410,5,1,"00:03:20,223","00:03:25,038",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,that's because we assume that
cs-410_5_1_45,cs-410,5,1,"00:03:25,038","00:03:29,310",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,So with this presentation
cs-410_5_1_46,cs-410,5,1,"00:03:29,310","00:03:33,472",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,d1 simply suggests a [INAUDIBLE] library.
cs-410_5_1_47,cs-410,5,1,"00:03:33,472","00:03:37,949",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,Now this is different from another
cs-410_5_1_48,cs-410,5,1,"00:03:37,949","00:03:39,906",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"a different vector, d2 here."
cs-410_5_1_49,cs-410,5,1,"00:03:39,906","00:03:44,319",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"Now in this case, the document that"
cs-410_5_1_50,cs-410,5,1,"00:03:44,319","00:03:46,679",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,it doesn't talk about presidential.
cs-410_5_1_51,cs-410,5,1,"00:03:46,679","00:03:48,830",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,So what does this remind you?
cs-410_5_1_52,cs-410,5,1,"00:03:48,830","00:03:54,540",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,Well you can probably guess the topic
cs-410_5_1_53,cs-410,5,1,"00:03:54,540","00:03:56,840",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,the library is software lab library.
cs-410_5_1_54,cs-410,5,1,"00:03:58,366","00:04:04,110",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,So this shows that by using
cs-410_5_1_55,cs-410,5,1,"00:04:04,110","00:04:08,190",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,we can actually capture the differences
cs-410_5_1_56,cs-410,5,1,"00:04:09,610","00:04:12,190",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,Now you can also imagine
cs-410_5_1_57,cs-410,5,1,"00:04:12,190","00:04:15,296",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,"For example,"
cs-410_5_1_58,cs-410,5,1,"00:04:15,296","00:04:17,632",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,that might be a presidential program.
cs-410_5_1_59,cs-410,5,1,"00:04:17,632","00:04:22,649",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,And in fact we can place all
cs-410_5_1_60,cs-410,5,1,"00:04:22,649","00:04:26,700",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,And they will be pointing
cs-410_5_1_61,cs-410,5,1,"00:04:26,700","00:04:27,340",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"And similarly,"
cs-410_5_1_62,cs-410,5,1,"00:04:27,340","00:04:31,570",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,we're going to place our query also
cs-410_5_1_63,cs-410,5,1,"00:04:32,630","00:04:37,226",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,And then we're going to measure the
cs-410_5_1_64,cs-410,5,1,"00:04:37,226","00:04:39,510",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,every document vector.
cs-410_5_1_65,cs-410,5,1,"00:04:39,510","00:04:40,740",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"So in this case for example,"
cs-410_5_1_66,cs-410,5,1,"00:04:40,740","00:04:47,200",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,we can easily see d2 seems to be
cs-410_5_1_67,cs-410,5,1,"00:04:47,200","00:04:50,040",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"And therefore,"
cs-410_5_1_68,cs-410,5,1,"00:04:51,900","00:04:56,530",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,So this is basically the main
cs-410_5_1_69,cs-410,5,1,"00:04:58,320","00:05:02,455",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"So to be more precise,"
cs-410_5_1_70,cs-410,5,1,"00:05:02,455","00:05:09,000",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,vector space model is a framework.
cs-410_5_1_71,cs-410,5,1,"00:05:09,000","00:05:12,510",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"In this framework,"
cs-410_5_1_72,cs-410,5,1,"00:05:12,510","00:05:17,300",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,"First, we represent a document and"
cs-410_5_1_73,cs-410,5,1,"00:05:18,680","00:05:21,670",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,So here a term can be any basic concept.
cs-410_5_1_74,cs-410,5,1,"00:05:21,670","00:05:28,920",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,"For example, a word or a phrase or"
cs-410_5_1_75,cs-410,5,1,"00:05:28,920","00:05:32,880",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,Those are just sequence of
cs-410_5_1_76,cs-410,5,1,"00:05:34,460","00:05:37,400",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,Each term is assumed that will
cs-410_5_1_77,cs-410,5,1,"00:05:37,400","00:05:42,170",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"Therefore n terms in our vocabulary,"
cs-410_5_1_78,cs-410,5,1,"00:05:44,060","00:05:48,550",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,A query vector would consist
cs-410_5_1_79,cs-410,5,1,"00:05:49,610","00:05:53,680",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,corresponding to the weights
cs-410_5_1_80,cs-410,5,1,"00:05:56,250","00:05:59,540",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,Each document vector is also similar.
cs-410_5_1_81,cs-410,5,1,"00:05:59,540","00:06:04,518",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,It has a number of elements and
cs-410_5_1_82,cs-410,5,1,"00:06:04,518","00:06:08,900",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,indicating the weight of
cs-410_5_1_83,cs-410,5,1,"00:06:08,900","00:06:12,397",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,"Here, you can see,"
cs-410_5_1_84,cs-410,5,1,"00:06:12,397","00:06:14,445",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,"Therefore, they are N elements"
cs-410_5_1_85,cs-410,5,1,"00:06:15,525","00:06:18,715",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,each corresponding to the weight
cs-410_5_1_86,cs-410,5,1,"00:06:21,385","00:06:23,860",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,So the relevance in this case
cs-410_5_1_87,cs-410,5,1,"00:06:23,860","00:06:28,030",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,will be assumed to be the similarity
cs-410_5_1_88,cs-410,5,1,"00:06:29,420","00:06:33,500",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,"Therefore, our ranking function"
cs-410_5_1_89,cs-410,5,1,"00:06:33,500","00:06:35,720",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,between the query vector and
cs-410_5_1_90,cs-410,5,1,"00:06:37,570","00:06:41,780",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,Now if I ask you to write a program
cs-410_5_1_91,cs-410,5,1,"00:06:41,780","00:06:42,370",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,in a search engine.
cs-410_5_1_92,cs-410,5,1,"00:06:44,042","00:06:48,248",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,You would realize that
cs-410_5_1_93,cs-410,5,1,"00:06:48,248","00:06:50,750",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"We haven't said a lot of things in detail,"
cs-410_5_1_94,cs-410,5,1,"00:06:50,750","00:06:56,080",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,therefore it's impossible to actually
cs-410_5_1_95,cs-410,5,1,"00:06:56,080","00:06:58,110",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,"That's why I said, this is a framework."
cs-410_5_1_96,cs-410,5,1,"00:06:59,370","00:07:03,310",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,And this has to be refined
cs-410_5_1_97,cs-410,5,1,"00:07:04,350","00:07:08,630",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,suggest a particular ranking function
cs-410_5_1_98,cs-410,5,1,"00:07:10,890","00:07:13,810",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,So what does this framework not say?
cs-410_5_1_99,cs-410,5,1,"00:07:13,810","00:07:17,800",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,"Well, it actually hasn't said many things"
cs-410_5_1_100,cs-410,5,1,"00:07:17,800","00:07:22,520",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,that would be required in order
cs-410_5_1_101,cs-410,5,1,"00:07:24,420","00:07:30,340",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"First, it did not say how we should define"
cs-410_5_1_102,cs-410,5,1,"00:07:32,580","00:07:36,190",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,We clearly assume
cs-410_5_1_103,cs-410,5,1,"00:07:36,190","00:07:38,660",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,"Otherwise, there will be redundancy."
cs-410_5_1_104,cs-410,5,1,"00:07:38,660","00:07:45,309",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"For example, if two synonyms or somehow"
cs-410_5_1_105,cs-410,5,1,"00:07:45,309","00:07:50,382",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,Then they would be defining
cs-410_5_1_106,cs-410,5,1,"00:07:50,382","00:07:54,299",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,that would clearly cause redundancy here.
cs-410_5_1_107,cs-410,5,1,"00:07:54,299","00:07:59,036",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,Or all the emphasizing of
cs-410_5_1_108,cs-410,5,1,"00:07:59,036","00:08:03,997",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,because it would be as if
cs-410_5_1_109,cs-410,5,1,"00:08:03,997","00:08:08,760",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,when you actually matched
cs-410_5_1_110,cs-410,5,1,"00:08:11,420","00:08:16,020",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,"Secondly, it did not say how we"
cs-410_5_1_111,cs-410,5,1,"00:08:16,020","00:08:18,200",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,the query in this space.
cs-410_5_1_112,cs-410,5,1,"00:08:18,200","00:08:22,970",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,Basically that show you some examples
cs-410_5_1_113,cs-410,5,1,"00:08:22,970","00:08:27,190",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,But where exactly should the vector for
cs-410_5_1_114,cs-410,5,1,"00:08:29,050","00:08:33,930",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,So this is equivalent to how
cs-410_5_1_115,cs-410,5,1,"00:08:33,930","00:08:39,237",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,How do you compute the lose
cs-410_5_1_116,cs-410,5,1,"00:08:39,237","00:08:41,808",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"This is a very important question,"
cs-410_5_1_117,cs-410,5,1,"00:08:41,808","00:08:47,220",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,because term weight in the query vector
cs-410_5_1_118,cs-410,5,1,"00:08:48,820","00:08:51,460",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"So depending on how you assign the weight,"
cs-410_5_1_119,cs-410,5,1,"00:08:51,460","00:08:55,620",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,you might prefer some terms
cs-410_5_1_120,cs-410,5,1,"00:08:56,630","00:08:59,472",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,"Similarly, the total word in"
cs-410_5_1_121,cs-410,5,1,"00:08:59,472","00:09:03,559",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,It indicates how well the term
cs-410_5_1_122,cs-410,5,1,"00:09:03,559","00:09:08,620",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,If you got it wrong then you clearly
cs-410_5_1_123,cs-410,5,1,"00:09:10,150","00:09:15,343",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,"Finally, how to define the similarity"
cs-410_5_1_124,cs-410,5,1,"00:09:15,343","00:09:20,018",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,So these questions must be addressed
cs-410_5_1_125,cs-410,5,1,"00:09:20,018","00:09:24,620",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,function that we can actually
cs-410_5_1_126,cs-410,5,1,"00:09:25,920","00:09:31,767",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,So how do we solve these problems
cs-410_5_1_127,cs-410,5,1,"00:09:31,767","00:09:38,702",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,is the main topic of the next lecture.
cs-410_5_1_128,cs-410,5,1,"00:09:38,702","00:09:44,589",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,[MUSIC]
cs-410_5_2_1,cs-410,5,2,"00:00:00,012","00:00:03,586",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_5_2_2,cs-410,5,2,"00:00:07,325","00:00:10,038",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about the inverted index
cs-410_5_2_3,cs-410,5,2,"00:00:10,038","00:00:11,160",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,construction.
cs-410_5_2_4,cs-410,5,2,"00:00:13,840","00:00:18,520",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In this lecture, we will continue"
cs-410_5_2_5,cs-410,5,2,"00:00:18,520","00:00:22,019",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,"In particular, we're going to discuss"
cs-410_5_2_6,cs-410,5,2,"00:00:25,096","00:00:29,259",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,The construction of the inverted index
cs-410_5_2_7,cs-410,5,2,"00:00:29,259","00:00:30,450",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,very small.
cs-410_5_2_8,cs-410,5,2,"00:00:30,450","00:00:35,430",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,It's very easy to construct a dictionary
cs-410_5_2_9,cs-410,5,2,"00:00:36,600","00:00:42,280",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,The problem is that when our data
cs-410_5_2_10,cs-410,5,2,"00:00:42,280","00:00:45,180",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,then we have to use some
cs-410_5_2_11,cs-410,5,2,"00:00:46,500","00:00:51,900",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,"And unfortunately, in most retrieval"
cs-410_5_2_12,cs-410,5,2,"00:00:51,900","00:00:55,700",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,And they generally cannot be
cs-410_5_2_13,cs-410,5,2,"00:00:56,790","00:01:01,843",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,And there are many approaches to
cs-410_5_2_14,cs-410,5,2,"00:01:01,843","00:01:06,710",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,method is quite common and
cs-410_5_2_15,cs-410,5,2,"00:01:06,710","00:01:11,480",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"First, you collect the local termID,"
cs-410_5_2_16,cs-410,5,2,"00:01:11,480","00:01:16,946",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,Basically you will locate the terms
cs-410_5_2_17,cs-410,5,2,"00:01:16,946","00:01:24,117",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,And then once you collect those accounts
cs-410_5_2_18,cs-410,5,2,"00:01:24,117","00:01:29,104",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,So that you will be able to local
cs-410_5_2_19,cs-410,5,2,"00:01:29,104","00:01:31,310",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,these are called rounds.
cs-410_5_2_20,cs-410,5,2,"00:01:31,310","00:01:36,930",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,And then you write them into
cs-410_5_2_21,cs-410,5,2,"00:01:36,930","00:01:38,940",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,then you merge in step 3.
cs-410_5_2_22,cs-410,5,2,"00:01:38,940","00:01:44,104",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,"Do pairwise merging of these runs, until"
cs-410_5_2_23,cs-410,5,2,"00:01:44,104","00:01:46,460",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,generate a single inverted index.
cs-410_5_2_24,cs-410,5,2,"00:01:47,700","00:01:50,823",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,So this is an illustration of this method.
cs-410_5_2_25,cs-410,5,2,"00:01:50,823","00:01:54,265",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,On the left you see some documents and
cs-410_5_2_26,cs-410,5,2,"00:01:54,265","00:01:59,942",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,on the right we have a term lexicon and
cs-410_5_2_27,cs-410,5,2,"00:01:59,942","00:02:08,070",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,These lexicons are to map string-based
cs-410_5_2_28,cs-410,5,2,"00:02:08,070","00:02:12,261",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,terms into integer representations or
cs-410_5_2_29,cs-410,5,2,"00:02:12,261","00:02:18,112",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,map back from integers to
cs-410_5_2_30,cs-410,5,2,"00:02:18,112","00:02:23,010",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,The reason why we want our interest
cs-410_5_2_31,cs-410,5,2,"00:02:23,010","00:02:26,930",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,IDs is because integers
cs-410_5_2_32,cs-410,5,2,"00:02:26,930","00:02:29,770",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,"For example,"
cs-410_5_2_33,cs-410,5,2,"00:02:29,770","00:02:33,240",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,"array, and they are also easy to compress."
cs-410_5_2_34,cs-410,5,2,"00:02:34,390","00:02:40,530",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,So this is one reason why we tend
cs-410_5_2_35,cs-410,5,2,"00:02:42,180","00:02:46,710",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,so that we don't have to
cs-410_5_2_36,cs-410,5,2,"00:02:46,710","00:02:48,070",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,So how does this approach work?
cs-410_5_2_37,cs-410,5,2,"00:02:48,070","00:02:49,822",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,"Well, it's very simple."
cs-410_5_2_38,cs-410,5,2,"00:02:49,822","00:02:53,260",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,We're going to scan these
cs-410_5_2_39,cs-410,5,2,"00:02:53,260","00:02:58,260",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,then parse the documents and
cs-410_5_2_40,cs-410,5,2,"00:02:58,260","00:03:03,306",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,And in this stage we generally sort
cs-410_5_2_41,cs-410,5,2,"00:03:03,306","00:03:06,961",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,because we process each
cs-410_5_2_42,cs-410,5,2,"00:03:06,961","00:03:11,310",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,So we'll first encounter all
cs-410_5_2_43,cs-410,5,2,"00:03:11,310","00:03:18,786",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,Therefore the document IDs
cs-410_5_2_44,cs-410,5,2,"00:03:18,786","00:03:25,503",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,And this will be followed by document IDs
cs-410_5_2_45,cs-410,5,2,"00:03:25,503","00:03:31,280",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,only just because we process
cs-410_5_2_46,cs-410,5,2,"00:03:31,280","00:03:34,890",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"At some point,"
cs-410_5_2_47,cs-410,5,2,"00:03:34,890","00:03:39,080",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,that would have to write
cs-410_5_2_48,cs-410,5,2,"00:03:39,080","00:03:45,830",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,Before we do that we 're going to sort
cs-410_5_2_49,cs-410,5,2,"00:03:45,830","00:03:51,948",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,We can sort them and then this time
cs-410_5_2_50,cs-410,5,2,"00:03:51,948","00:03:59,459",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"Note that here,"
cs-410_5_2_51,cs-410,5,2,"00:03:59,459","00:04:03,827",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,So all the entries that share the same
cs-410_5_2_52,cs-410,5,2,"00:04:03,827","00:04:08,557",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,"In this case,"
cs-410_5_2_53,cs-410,5,2,"00:04:08,557","00:04:14,090",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,that match term 1 would
cs-410_5_2_54,cs-410,5,2,"00:04:14,090","00:04:18,850",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,And we're going to write this into
cs-410_5_2_55,cs-410,5,2,"00:04:18,850","00:04:22,800",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,And would that allows you to
cs-410_5_2_56,cs-410,5,2,"00:04:22,800","00:04:24,030",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,makes a batch of documents.
cs-410_5_2_57,cs-410,5,2,"00:04:24,030","00:04:26,670",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,And we're going to do that for
cs-410_5_2_58,cs-410,5,2,"00:04:26,670","00:04:32,546",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,So we're going to write a lot of
cs-410_5_2_59,cs-410,5,2,"00:04:32,546","00:04:35,400",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,And then the next stage is
cs-410_5_2_60,cs-410,5,2,"00:04:35,400","00:04:38,360",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,We're going to merge them and
cs-410_5_2_61,cs-410,5,2,"00:04:38,360","00:04:41,729",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,"Eventually, we will get"
cs-410_5_2_62,cs-410,5,2,"00:04:41,729","00:04:45,310",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,where the entries are sorted
cs-410_5_2_63,cs-410,5,2,"00:04:46,960","00:04:50,870",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,"And on the top, we're going to see"
cs-410_5_2_64,cs-410,5,2,"00:04:50,870","00:04:53,620",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,the documents that match term ID 1.
cs-410_5_2_65,cs-410,5,2,"00:04:53,620","00:05:00,300",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"So this is basically, how we can do"
cs-410_5_2_66,cs-410,5,2,"00:05:00,300","00:05:06,445",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,Even though the data cannot be
cs-410_5_2_67,cs-410,5,2,"00:05:06,445","00:05:12,562",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,"Now, we mention earlier that"
cs-410_5_2_68,cs-410,5,2,"00:05:12,562","00:05:15,848",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,it's desirable to compress them.
cs-410_5_2_69,cs-410,5,2,"00:05:15,848","00:05:20,481",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,So let's now take a little bit
cs-410_5_2_70,cs-410,5,2,"00:05:20,481","00:05:24,084",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,"Well the idea of compression in general,"
cs-410_5_2_71,cs-410,5,2,"00:05:24,084","00:05:28,310",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,leverage skewed distributions of values.
cs-410_5_2_72,cs-410,5,2,"00:05:28,310","00:05:31,090",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,And we generally have to use
cs-410_5_2_73,cs-410,5,2,"00:05:31,090","00:05:36,830",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,instead of the fixed-length
cs-410_5_2_74,cs-410,5,2,"00:05:36,830","00:05:41,080",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,a program manager like C++.
cs-410_5_2_75,cs-410,5,2,"00:05:41,080","00:05:45,840",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,And so how can we leverage
cs-410_5_2_76,cs-410,5,2,"00:05:45,840","00:05:48,180",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,to compress these values?
cs-410_5_2_77,cs-410,5,2,"00:05:48,180","00:05:53,827",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"Well in general, we will use few"
cs-410_5_2_78,cs-410,5,2,"00:05:53,827","00:06:00,650",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,words at the cost of using longer
cs-410_5_2_79,cs-410,5,2,"00:06:00,650","00:06:04,560",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,"So in our case, let's think about how"
cs-410_5_2_80,cs-410,5,2,"00:06:05,640","00:06:09,640",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,"Now, if you can picture what"
cs-410_5_2_81,cs-410,5,2,"00:06:09,640","00:06:13,807",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,"you will see in post things,"
cs-410_5_2_82,cs-410,5,2,"00:06:13,807","00:06:19,089",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,Those are the frequencies of
cs-410_5_2_83,cs-410,5,2,"00:06:19,089","00:06:25,650",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,"Now, if you think about it, what kind"
cs-410_5_2_84,cs-410,5,2,"00:06:25,650","00:06:29,980",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,You probably will be able to guess
cs-410_5_2_85,cs-410,5,2,"00:06:29,980","00:06:32,540",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,far more frequently than large numbers.
cs-410_5_2_86,cs-410,5,2,"00:06:32,540","00:06:33,990",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,Why?
cs-410_5_2_87,cs-410,5,2,"00:06:33,990","00:06:39,954",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,"Well, think about the distribution of"
cs-410_5_2_88,cs-410,5,2,"00:06:39,954","00:06:44,810",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,and many words occur just rarely so
cs-410_5_2_89,cs-410,5,2,"00:06:44,810","00:06:48,419",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,"Therefore, we can use fewer bits for"
cs-410_5_2_90,cs-410,5,2,"00:06:48,419","00:06:53,855",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,highly frequent integers and
cs-410_5_2_91,cs-410,5,2,"00:06:53,855","00:06:57,095",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,that's cost of using more bits for
cs-410_5_2_92,cs-410,5,2,"00:06:58,445","00:07:00,005",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,This is a trade off of course.
cs-410_5_2_93,cs-410,5,2,"00:07:00,005","00:07:05,712",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"If the values are distributed to uniform,"
cs-410_5_2_94,cs-410,5,2,"00:07:05,712","00:07:10,824",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,but because we tend to see many small
cs-410_5_2_95,cs-410,5,2,"00:07:10,824","00:07:15,769",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,We can save on average even though
cs-410_5_2_96,cs-410,5,2,"00:07:15,769","00:07:17,740",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,we have to use a lot of bits.
cs-410_5_2_97,cs-410,5,2,"00:07:19,750","00:07:23,700",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,What about the document IDs
cs-410_5_2_98,cs-410,5,2,"00:07:23,700","00:07:27,240",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,Well they are not distributed
cs-410_5_2_99,cs-410,5,2,"00:07:27,240","00:07:31,840",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,So how can we deal with that?
cs-410_5_2_100,cs-410,5,2,"00:07:31,840","00:07:35,488",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,Well it turns out that we can
cs-410_5_2_101,cs-410,5,2,"00:07:35,488","00:07:38,686",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,that is to store the difference
cs-410_5_2_102,cs-410,5,2,"00:07:38,686","00:07:43,495",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,And we can imagine if a term has
cs-410_5_2_103,cs-410,5,2,"00:07:43,495","00:07:46,640",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,there will be longest of document IDs.
cs-410_5_2_104,cs-410,5,2,"00:07:46,640","00:07:52,030",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,"So when we take the gap, and we take the"
cs-410_5_2_105,cs-410,5,2,"00:07:52,030","00:07:54,340",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,those gaps will be small.
cs-410_5_2_106,cs-410,5,2,"00:07:54,340","00:07:57,594",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,"So again, see a lot of small numbers."
cs-410_5_2_107,cs-410,5,2,"00:07:57,594","00:08:00,217",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,Whereas if a term occurred
cs-410_5_2_108,cs-410,5,2,"00:08:00,217","00:08:04,300",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,"then the gap would be large,"
cs-410_5_2_109,cs-410,5,2,"00:08:04,300","00:08:06,610",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,"So this creates some skewed distribution,"
cs-410_5_2_110,cs-410,5,2,"00:08:06,610","00:08:10,669",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,that would allow us to
cs-410_5_2_111,cs-410,5,2,"00:08:11,850","00:08:15,621",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,This is also possible because
cs-410_5_2_112,cs-410,5,2,"00:08:15,621","00:08:21,249",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,"uncompress these document IDs,"
cs-410_5_2_113,cs-410,5,2,"00:08:21,249","00:08:25,484",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,Because we stored the difference and
cs-410_5_2_114,cs-410,5,2,"00:08:25,484","00:08:29,574",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,document ID we have to first
cs-410_5_2_115,cs-410,5,2,"00:08:29,574","00:08:34,536",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,And then we can add the difference to
cs-410_5_2_116,cs-410,5,2,"00:08:34,536","00:08:36,365",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,the current document ID.
cs-410_5_2_117,cs-410,5,2,"00:08:36,365","00:08:40,834",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,Now this was possible because we only
cs-410_5_2_118,cs-410,5,2,"00:08:40,834","00:08:42,900",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,those document IDs.
cs-410_5_2_119,cs-410,5,2,"00:08:42,900","00:08:46,920",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,"Once we look up the term, we look up all"
cs-410_5_2_120,cs-410,5,2,"00:08:46,920","00:08:48,670",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,then we sequentially process them.
cs-410_5_2_121,cs-410,5,2,"00:08:48,670","00:08:52,070",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"So it's very natural,"
cs-410_5_2_122,cs-410,5,2,"00:08:53,600","00:08:55,760",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,And there are many different methods for
cs-410_5_2_123,cs-410,5,2,"00:08:55,760","00:09:02,116",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,So binary code is a commonly used
cs-410_5_2_124,cs-410,5,2,"00:09:02,116","00:09:05,994",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,We use basically fixed glance in coding.
cs-410_5_2_125,cs-410,5,2,"00:09:05,994","00:09:09,276",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,"Unary code, gamma code, and"
cs-410_5_2_126,cs-410,5,2,"00:09:09,276","00:09:11,240",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,there are many other possibilities.
cs-410_5_2_127,cs-410,5,2,"00:09:11,240","00:09:14,130",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,So let's look at some
cs-410_5_2_128,cs-410,5,2,"00:09:14,130","00:09:16,900",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,Binary coding is really
cs-410_5_2_129,cs-410,5,2,"00:09:16,900","00:09:20,930",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,that's a property for
cs-410_5_2_130,cs-410,5,2,"00:09:20,930","00:09:24,700",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,The unary coding is a variable
cs-410_5_2_131,cs-410,5,2,"00:09:24,700","00:09:28,891",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"In this case, integer this 1 will be"
cs-410_5_2_132,cs-410,5,2,"00:09:28,891","00:09:33,630",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,"encoded as x -1, 1 bit followed by 0."
cs-410_5_2_133,cs-410,5,2,"00:09:33,630","00:09:39,329",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,"So for example, 3 will be encoded as 2,"
cs-410_5_2_134,cs-410,5,2,"00:09:39,329","00:09:45,042",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,"whereas 5 will be encoded as 4,"
cs-410_5_2_135,cs-410,5,2,"00:09:45,042","00:09:51,599",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,So now you can imagine how many bits do we
cs-410_5_2_136,cs-410,5,2,"00:09:51,599","00:09:57,450",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,So how many bits do you have to
cs-410_5_2_137,cs-410,5,2,"00:09:57,450","00:10:02,549",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,"Well exactly, we have to use 100 bits."
cs-410_5_2_138,cs-410,5,2,"00:10:02,549","00:10:07,150",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,So it's the same number of bits
cs-410_5_2_139,cs-410,5,2,"00:10:07,150","00:10:12,360",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,So this is very inefficient if you
cs-410_5_2_140,cs-410,5,2,"00:10:12,360","00:10:17,620",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,Imagine if you occasionally see a number
cs-410_5_2_141,cs-410,5,2,"00:10:17,620","00:10:22,894",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,So this only works well if you
cs-410_5_2_142,cs-410,5,2,"00:10:22,894","00:10:28,082",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,"no large numbers, mostly very"
cs-410_5_2_143,cs-410,5,2,"00:10:28,082","00:10:30,184",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,"Now, how do you decode this code?"
cs-410_5_2_144,cs-410,5,2,"00:10:30,184","00:10:33,662",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,Now since these are variable
cs-410_5_2_145,cs-410,5,2,"00:10:33,662","00:10:37,070",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,you can't just count how many bits and
cs-410_5_2_146,cs-410,5,2,"00:10:38,500","00:10:44,800",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,"You can't say 8-bits or 32-bits,"
cs-410_5_2_147,cs-410,5,2,"00:10:44,800","00:10:50,860",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,"They are variable length, so"
cs-410_5_2_148,cs-410,5,2,"00:10:50,860","00:10:55,192",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,"In this case for unary, you can see"
cs-410_5_2_149,cs-410,5,2,"00:10:55,192","00:10:59,161",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,Now you can easily see 0 would
cs-410_5_2_150,cs-410,5,2,"00:10:59,161","00:11:03,120",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,So you just count up how many 1s you
cs-410_5_2_151,cs-410,5,2,"00:11:03,120","00:11:06,560",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,"You have finished one number,"
cs-410_5_2_152,cs-410,5,2,"00:11:07,960","00:11:11,266",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,Now we just saw that unary
cs-410_5_2_153,cs-410,5,2,"00:11:11,266","00:11:13,987",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,"In rewarding small numbers, and"
cs-410_5_2_154,cs-410,5,2,"00:11:13,987","00:11:20,430",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,if you occasionally can see a very
cs-410_5_2_155,cs-410,5,2,"00:11:20,430","00:11:24,900",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,So what about some other
cs-410_5_2_156,cs-410,5,2,"00:11:24,900","00:11:29,200",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,Well gamma coding's one of them and
cs-410_5_2_157,cs-410,5,2,"00:11:29,200","00:11:34,072",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,in this method we can use unary coding for
cs-410_5_2_158,cs-410,5,2,"00:11:34,072","00:11:37,239",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=694,a transform form of that.
cs-410_5_2_159,cs-410,5,2,"00:11:37,239","00:11:41,210",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,So it's 1 plus the floor of log of x.
cs-410_5_2_160,cs-410,5,2,"00:11:41,210","00:11:47,781",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,So the magnitude of this value is
cs-410_5_2_161,cs-410,5,2,"00:11:47,781","00:11:52,703",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,So that's why we can afford
cs-410_5_2_162,cs-410,5,2,"00:11:52,703","00:11:58,728",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,And so first I have the unary code for
cs-410_5_2_163,cs-410,5,2,"00:11:58,728","00:12:02,327",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,And this would be followed by
cs-410_5_2_164,cs-410,5,2,"00:12:02,327","00:12:08,058",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,"And this basically the same uniform code,"
cs-410_5_2_165,cs-410,5,2,"00:12:08,058","00:12:15,956",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,And we're going to use this coder to code
cs-410_5_2_166,cs-410,5,2,"00:12:15,956","00:12:22,178",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=735,And this is basically precisely
cs-410_5_2_167,cs-410,5,2,"00:12:25,000","00:12:30,376",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,So the unary code are basically
cs-410_5_2_168,cs-410,5,2,"00:12:30,376","00:12:33,029",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=750,well add one there and here.
cs-410_5_2_169,cs-410,5,2,"00:12:33,029","00:12:38,428",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,But the remaining part
cs-410_5_2_170,cs-410,5,2,"00:12:38,428","00:12:43,413",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,code through actually code the difference
cs-410_5_2_171,cs-410,5,2,"00:12:43,413","00:12:47,990",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,between the x and this 2 to the log of x.
cs-410_5_2_172,cs-410,5,2,"00:12:49,530","00:12:53,280",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,And it's easy to show that for this
cs-410_5_2_173,cs-410,5,2,"00:12:55,790","00:13:00,297",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,difference we only need to use up
cs-410_5_2_174,cs-410,5,2,"00:13:00,297","00:13:05,390",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,to this many bits and
cs-410_5_2_175,cs-410,5,2,"00:13:06,530","00:13:08,410",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,"And this is easy to understand,"
cs-410_5_2_176,cs-410,5,2,"00:13:08,410","00:13:12,910",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,"if the difference is too large, then we"
cs-410_5_2_177,cs-410,5,2,"00:13:14,330","00:13:19,000",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,So here are some examples for
cs-410_5_2_178,cs-410,5,2,"00:13:19,000","00:13:22,575",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,The first two digits are the unary code.
cs-410_5_2_179,cs-410,5,2,"00:13:22,575","00:13:26,706",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=802,"So this isn't for the value 2,"
cs-410_5_2_180,cs-410,5,2,"00:13:26,706","00:13:30,990",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=806,10 encodes 2 in unary coding.
cs-410_5_2_181,cs-410,5,2,"00:13:32,490","00:13:37,300",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=812,And so that means the floor of
cs-410_5_2_182,cs-410,5,2,"00:13:37,300","00:13:42,398",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,"log of x is 1,"
cs-410_5_2_183,cs-410,5,2,"00:13:42,398","00:13:45,620",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,"In code 1 plus the flow of log of x,"
cs-410_5_2_184,cs-410,5,2,"00:13:45,620","00:13:50,145",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=825,since this is two then we know that
cs-410_5_2_185,cs-410,5,2,"00:13:52,000","00:13:55,720",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=832,So that 3 is still larger than 2 to the 1.
cs-410_5_2_186,cs-410,5,2,"00:13:55,720","00:14:00,040",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=835,"So the difference is 1, and"
cs-410_5_2_187,cs-410,5,2,"00:14:01,460","00:14:04,690",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=841,So that's why we have 101 for 3.
cs-410_5_2_188,cs-410,5,2,"00:14:04,690","00:14:11,554",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,"Now similarly 5 is encoded as 110,"
cs-410_5_2_189,cs-410,5,2,"00:14:12,970","00:14:17,981",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=852,And in this case the unary code in code 3.
cs-410_5_2_190,cs-410,5,2,"00:14:17,981","00:14:25,445",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=857,And so this is a unary code 110 and
cs-410_5_2_191,cs-410,5,2,"00:14:25,445","00:14:30,362",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,And that means we're going to
cs-410_5_2_192,cs-410,5,2,"00:14:30,362","00:14:32,784",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,the 2 to the 2 and that's 1.
cs-410_5_2_193,cs-410,5,2,"00:14:32,784","00:14:35,803",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=872,And so we now have again 1 at the end.
cs-410_5_2_194,cs-410,5,2,"00:14:35,803","00:14:39,226",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,"But this time we're going to use 2 bits,"
cs-410_5_2_195,cs-410,5,2,"00:14:39,226","00:14:43,570",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=879,because with this level
cs-410_5_2_196,cs-410,5,2,"00:14:43,570","00:14:51,040",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,"We could have more numbers a 5, 6, 7 they"
cs-410_5_2_197,cs-410,5,2,"00:14:51,040","00:14:53,210",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=891,"So in order to differentiate them,"
cs-410_5_2_198,cs-410,5,2,"00:14:53,210","00:14:57,690",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=893,we have to use 2 bits in
cs-410_5_2_199,cs-410,5,2,"00:14:57,690","00:15:03,670",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=897,So you can imagine 6 would be 10 here
cs-410_5_2_200,cs-410,5,2,"00:15:04,710","00:15:10,615",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=904,It's also true that the form of
cs-410_5_2_201,cs-410,5,2,"00:15:10,615","00:15:15,155",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=910,"odd number of bits, and"
cs-410_5_2_202,cs-410,5,2,"00:15:15,155","00:15:17,305",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=915,That's the end of the unary code.
cs-410_5_2_203,cs-410,5,2,"00:15:18,335","00:15:24,385",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=918,And before that or on the left side
cs-410_5_2_204,cs-410,5,2,"00:15:24,385","00:15:30,265",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=924,"And on the right side of this 0,"
cs-410_5_2_205,cs-410,5,2,"00:15:32,550","00:15:36,540",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=932,So how can you decode such code?
cs-410_5_2_206,cs-410,5,2,"00:15:36,540","00:15:39,866",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=936,Well you again first do unary coding.
cs-410_5_2_207,cs-410,5,2,"00:15:39,866","00:15:45,371",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,"Once you hit 0, you have got the unary"
cs-410_5_2_208,cs-410,5,2,"00:15:45,371","00:15:50,408",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=945,many bits you have to read further
cs-410_5_2_209,cs-410,5,2,"00:15:50,408","00:15:53,693",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=950,So this is how you can
cs-410_5_2_210,cs-410,5,2,"00:15:53,693","00:15:57,998",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,There is also a delta code that's
cs-410_5_2_211,cs-410,5,2,"00:15:57,998","00:16:01,340",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,that you replace the unary
cs-410_5_2_212,cs-410,5,2,"00:16:01,340","00:16:04,980",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=961,So that's even less
cs-410_5_2_213,cs-410,5,2,"00:16:04,980","00:16:08,910",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=964,in terms of wording the small integers.
cs-410_5_2_214,cs-410,5,2,"00:16:08,910","00:16:12,150",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=968,"So that means, it's okay if you"
cs-410_5_2_215,cs-410,5,2,"00:16:14,100","00:16:15,190",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=974,It's okay with delta code.
cs-410_5_2_216,cs-410,5,2,"00:16:16,810","00:16:23,210",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=976,"It's also fine with the gamma code,"
cs-410_5_2_217,cs-410,5,2,"00:16:23,210","00:16:26,710",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=983,"And they are all operating of course,"
cs-410_5_2_218,cs-410,5,2,"00:16:26,710","00:16:32,360",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,at different degrees of favoring short or
cs-410_5_2_219,cs-410,5,2,"00:16:32,360","00:16:38,560",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=992,And that also means they would be
cs-410_5_2_220,cs-410,5,2,"00:16:38,560","00:16:41,720",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=998,But none of them is perfect for
cs-410_5_2_221,cs-410,5,2,"00:16:41,720","00:16:45,990",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1001,And which method works the best would
cs-410_5_2_222,cs-410,5,2,"00:16:45,990","00:16:47,610",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,in your dataset.
cs-410_5_2_223,cs-410,5,2,"00:16:47,610","00:16:49,660",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1007,"For inverted index compression,"
cs-410_5_2_224,cs-410,5,2,"00:16:49,660","00:16:52,990",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1009,people have found that gamma
cs-410_5_2_225,cs-410,5,2,"00:16:55,114","00:16:58,340",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1015,So how to uncompress inverted index?
cs-410_5_2_226,cs-410,5,2,"00:16:58,340","00:16:59,900",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1018,I will just talk about this.
cs-410_5_2_227,cs-410,5,2,"00:16:59,900","00:17:02,920",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1019,"Firstly, you decode"
cs-410_5_2_228,cs-410,5,2,"00:17:02,920","00:17:10,877",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1022,And we just I think discussed the how we
cs-410_5_2_229,cs-410,5,2,"00:17:10,877","00:17:15,720",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1030,What about the document IDs that
cs-410_5_2_230,cs-410,5,2,"00:17:15,720","00:17:19,320",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,"Well, we're going to do"
cs-410_5_2_231,cs-410,5,2,"00:17:19,320","00:17:23,800",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1039,"supposed the encoded I list is x1,"
cs-410_5_2_232,cs-410,5,2,"00:17:23,800","00:17:28,384",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1043,We first decode x1 to obtain
cs-410_5_2_233,cs-410,5,2,"00:17:28,384","00:17:29,845",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1048,"Then we can decode x2,"
cs-410_5_2_234,cs-410,5,2,"00:17:29,845","00:17:34,610",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1049,which is actually the difference between
cs-410_5_2_235,cs-410,5,2,"00:17:34,610","00:17:40,240",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1054,So we have to add the decoder
cs-410_5_2_236,cs-410,5,2,"00:17:40,240","00:17:45,630",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1060,the value of the ID at
cs-410_5_2_237,cs-410,5,2,"00:17:46,690","00:17:50,420",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1066,So this is where you can
cs-410_5_2_238,cs-410,5,2,"00:17:50,420","00:17:52,870",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1070,converting document IDs to integers.
cs-410_5_2_239,cs-410,5,2,"00:17:52,870","00:17:55,730",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1072,And that allows us to do
cs-410_5_2_240,cs-410,5,2,"00:17:55,730","00:17:59,590",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1075,And we just repeat until we
cs-410_5_2_241,cs-410,5,2,"00:17:59,590","00:18:04,004",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1079,Every time we use the document ID in
cs-410_5_2_242,cs-410,5,2,"00:18:04,004","00:18:06,257",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1084,the document ID in the next position.
cs-410_5_2_243,cs-410,5,2,"00:18:08,871","00:18:18,871",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1088,[MUSIC]
cs-410_5_3_1,cs-410,5,3,"00:00:00,883","00:00:05,127",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_5_3_2,cs-410,5,3,"00:00:07,433","00:00:12,376",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about how to evaluate
cs-410_5_3_3,cs-410,5,3,"00:00:12,376","00:00:15,560",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,multiple levels of judgements.
cs-410_5_3_4,cs-410,5,3,"00:00:15,560","00:00:19,994",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,"In this lecture, we will continue"
cs-410_5_3_5,cs-410,5,3,"00:00:19,994","00:00:23,410",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,We're going to look at how to
cs-410_5_3_6,cs-410,5,3,"00:00:23,410","00:00:26,150",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,when we have multiple
cs-410_5_3_7,cs-410,5,3,"00:00:27,760","00:00:31,180",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,So far we have talked about
cs-410_5_3_8,cs-410,5,3,"00:00:31,180","00:00:34,169",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,that means a document is judged as
cs-410_5_3_9,cs-410,5,3,"00:00:35,270","00:00:40,310",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,"But earlier, we also talk about"
cs-410_5_3_10,cs-410,5,3,"00:00:40,310","00:00:45,580",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,So we often can distinguish
cs-410_5_3_11,cs-410,5,3,"00:00:45,580","00:00:50,230",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,"those are very useful documents,"
cs-410_5_3_12,cs-410,5,3,"00:00:50,230","00:00:53,000",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,"They are okay, they are useful perhaps."
cs-410_5_3_13,cs-410,5,3,"00:00:53,000","00:00:56,170",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"And further from now, we're adding"
cs-410_5_3_14,cs-410,5,3,"00:00:57,450","00:01:01,490",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,So imagine you can have ratings for
cs-410_5_3_15,cs-410,5,3,"00:01:01,490","00:01:05,390",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,"Then, you would have"
cs-410_5_3_16,cs-410,5,3,"00:01:05,390","00:01:10,803",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,"For example, here I show example of three"
cs-410_5_3_17,cs-410,5,3,"00:01:10,803","00:01:15,780",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"very relevant, 2 for marginally relevant,"
cs-410_5_3_18,cs-410,5,3,"00:01:15,780","00:01:18,990",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,"Now, how do we evaluate the search"
cs-410_5_3_19,cs-410,5,3,"00:01:18,990","00:01:23,330",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"Obvious that the map doesn't work, average"
cs-410_5_3_20,cs-410,5,3,"00:01:23,330","00:01:28,190",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"recall doesn't work,"
cs-410_5_3_21,cs-410,5,3,"00:01:28,190","00:01:33,510",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,So let's look at some top ranked
cs-410_5_3_22,cs-410,5,3,"00:01:33,510","00:01:38,518",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,Imagine the user would be mostly
cs-410_5_3_23,cs-410,5,3,"00:01:43,122","00:01:48,165",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"And we marked the rating levels,"
cs-410_5_3_24,cs-410,5,3,"00:01:48,165","00:01:54,620",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,"for these documents as shown here,"
cs-410_5_3_25,cs-410,5,3,"00:01:54,620","00:01:57,122",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,And we call these gain.
cs-410_5_3_26,cs-410,5,3,"00:01:57,122","00:02:02,345",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,And the reason why we call it
cs-410_5_3_27,cs-410,5,3,"00:02:02,345","00:02:08,860",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,that we are infusing is called the NDCG
cs-410_5_3_28,cs-410,5,3,"00:02:10,090","00:02:14,900",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,"So this gain, basically,"
cs-410_5_3_29,cs-410,5,3,"00:02:14,900","00:02:19,790",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,information a user can obtain by
cs-410_5_3_30,cs-410,5,3,"00:02:19,790","00:02:24,120",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,"So looking at the first document,"
cs-410_5_3_31,cs-410,5,3,"00:02:24,120","00:02:28,110",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,Looking at the non-relevant document
cs-410_5_3_32,cs-410,5,3,"00:02:29,510","00:02:32,703",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,Looking at the moderator or
cs-410_5_3_33,cs-410,5,3,"00:02:32,703","00:02:35,910",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,"document the user would get 2 points,"
cs-410_5_3_34,cs-410,5,3,"00:02:35,910","00:02:40,560",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,"So, this gain to each of the measures is"
cs-410_5_3_35,cs-410,5,3,"00:02:40,560","00:02:41,890",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,perspective.
cs-410_5_3_36,cs-410,5,3,"00:02:41,890","00:02:46,140",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,"Of course, if we assume the user"
cs-410_5_3_37,cs-410,5,3,"00:02:46,140","00:02:51,060",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,"we're looking at the cutoff at 10,"
cs-410_5_3_38,cs-410,5,3,"00:02:51,060","00:02:51,774",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,And what's that?
cs-410_5_3_39,cs-410,5,3,"00:02:51,774","00:02:55,825",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,"Well, that's simply the sum of these,"
cs-410_5_3_40,cs-410,5,3,"00:02:55,825","00:02:59,275",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,"So if the user stops after the position 1,"
cs-410_5_3_41,cs-410,5,3,"00:02:59,275","00:03:03,163",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,"If the user looks at another document,"
cs-410_5_3_42,cs-410,5,3,"00:03:03,163","00:03:08,221",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,"If the user looks at the more documents,"
cs-410_5_3_43,cs-410,5,3,"00:03:08,221","00:03:13,200",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,Of course this is at the cost of
cs-410_5_3_44,cs-410,5,3,"00:03:13,200","00:03:16,390",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,So cumulative gain gives
cs-410_5_3_45,cs-410,5,3,"00:03:16,390","00:03:21,368",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,much total gain the user would have if
cs-410_5_3_46,cs-410,5,3,"00:03:21,368","00:03:28,140",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,"Now, in NDCG, we also have another letter"
cs-410_5_3_47,cs-410,5,3,"00:03:29,170","00:03:32,060",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"So, why do we want to do discounting?"
cs-410_5_3_48,cs-410,5,3,"00:03:32,060","00:03:35,685",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,"Well, if you look at this cumulative gain,"
cs-410_5_3_49,cs-410,5,3,"00:03:35,685","00:03:41,975",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,which is it did not consider the rank
cs-410_5_3_50,cs-410,5,3,"00:03:41,975","00:03:46,115",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,"So for example, looking at this sum here,"
cs-410_5_3_51,cs-410,5,3,"00:03:46,115","00:03:51,485",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,and we only know there is 1
cs-410_5_3_52,cs-410,5,3,"00:03:51,485","00:03:54,945",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"1 marginally relevant document,"
cs-410_5_3_53,cs-410,5,3,"00:03:54,945","00:03:57,300",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,We don't really care
cs-410_5_3_54,cs-410,5,3,"00:03:57,300","00:04:02,110",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,"Ideally, we want these two to be ranked"
cs-410_5_3_55,cs-410,5,3,"00:04:03,120","00:04:06,420",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,But how can we capture that intuition?
cs-410_5_3_56,cs-410,5,3,"00:04:06,420","00:04:13,209",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"Well we have to say, well this is 3 here"
cs-410_5_3_57,cs-410,5,3,"00:04:13,209","00:04:18,114",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,And that means the contribution
cs-410_5_3_58,cs-410,5,3,"00:04:18,114","00:04:22,750",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,positions has to be
cs-410_5_3_59,cs-410,5,3,"00:04:22,750","00:04:24,666",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"And this is the idea of discounting,"
cs-410_5_3_60,cs-410,5,3,"00:04:24,666","00:04:29,530",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,"So we're going to to say, well, the first"
cs-410_5_3_61,cs-410,5,3,"00:04:29,530","00:04:33,910",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,because the user can be assumed
cs-410_5_3_62,cs-410,5,3,"00:04:33,910","00:04:38,030",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,"But the second one,"
cs-410_5_3_63,cs-410,5,3,"00:04:38,030","00:04:42,370",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,because there's a small possibility
cs-410_5_3_64,cs-410,5,3,"00:04:42,370","00:04:48,690",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,So we divide this gain by
cs-410_5_3_65,cs-410,5,3,"00:04:48,690","00:04:52,640",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,"So log of 2,"
cs-410_5_3_66,cs-410,5,3,"00:04:52,640","00:04:57,080",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,"And when we go to the third position,"
cs-410_5_3_67,cs-410,5,3,"00:04:57,080","00:05:01,270",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,"because the normalizer is log of 3,"
cs-410_5_3_68,cs-410,5,3,"00:05:01,270","00:05:06,690",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,So when we take such a sum that a lower
cs-410_5_3_69,cs-410,5,3,"00:05:06,690","00:05:10,000",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,that much as a highly ranked document.
cs-410_5_3_70,cs-410,5,3,"00:05:10,000","00:05:15,120",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"So that means if you, for example,"
cs-410_5_3_71,cs-410,5,3,"00:05:15,120","00:05:20,726",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"this position, and this one, and then"
cs-410_5_3_72,cs-410,5,3,"00:05:20,726","00:05:27,050",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,for example very relevant
cs-410_5_3_73,cs-410,5,3,"00:05:27,050","00:05:31,290",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,"Imagine if you put the 3 here,"
cs-410_5_3_74,cs-410,5,3,"00:05:31,290","00:05:34,635",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,So it's not as good as if
cs-410_5_3_75,cs-410,5,3,"00:05:34,635","00:05:36,650",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,So this is the idea of discounting.
cs-410_5_3_76,cs-410,5,3,"00:05:37,900","00:05:43,210",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"Okay, so now at this point that we have"
cs-410_5_3_77,cs-410,5,3,"00:05:43,210","00:05:50,000",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,measuring the utility of this ranked
cs-410_5_3_78,cs-410,5,3,"00:05:51,480","00:05:53,125",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,So are we happy with this?
cs-410_5_3_79,cs-410,5,3,"00:05:53,125","00:05:55,680",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,"Well, we can use this to rank systems."
cs-410_5_3_80,cs-410,5,3,"00:05:55,680","00:05:58,510",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,"Now, we still need to do a little bit more"
cs-410_5_3_81,cs-410,5,3,"00:05:58,510","00:06:03,272",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,in order to make this measure
cs-410_5_3_82,cs-410,5,3,"00:06:03,272","00:06:10,580",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,"And this is the last step, and by the way,"
cs-410_5_3_83,cs-410,5,3,"00:06:10,580","00:06:16,820",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,"so this is the total sum of DCG,"
cs-410_5_3_84,cs-410,5,3,"00:06:16,820","00:06:20,880",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,"So the last step is called N,"
cs-410_5_3_85,cs-410,5,3,"00:06:20,880","00:06:25,240",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,"And if we do that,"
cs-410_5_3_86,cs-410,5,3,"00:06:25,240","00:06:26,463",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,So how do we do that?
cs-410_5_3_87,cs-410,5,3,"00:06:26,463","00:06:31,241",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"Well, the idea here is we're"
cs-410_5_3_88,cs-410,5,3,"00:06:31,241","00:06:35,280",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,the ideal DCG at the same cutoff.
cs-410_5_3_89,cs-410,5,3,"00:06:35,280","00:06:37,130",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,What is the ideal DCG?
cs-410_5_3_90,cs-410,5,3,"00:06:37,130","00:06:40,830",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,"Well, this is the DCG of an ideal ranking."
cs-410_5_3_91,cs-410,5,3,"00:06:40,830","00:06:47,690",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,So imagine if we have 9 documents in
cs-410_5_3_92,cs-410,5,3,"00:06:47,690","00:06:52,840",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,And that means in total we
cs-410_5_3_93,cs-410,5,3,"00:06:53,840","00:07:00,640",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,Then our ideal rank lister would have put
cs-410_5_3_94,cs-410,5,3,"00:07:00,640","00:07:05,730",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,So all these would have to be 3 and
cs-410_5_3_95,cs-410,5,3,"00:07:05,730","00:07:10,040",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,Because that's the best we could
cs-410_5_3_96,cs-410,5,3,"00:07:10,040","00:07:11,800",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,But all these positions would be 3.
cs-410_5_3_97,cs-410,5,3,"00:07:11,800","00:07:13,938",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,Right?
cs-410_5_3_98,cs-410,5,3,"00:07:13,938","00:07:16,090",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,So this would our ideal ranked list.
cs-410_5_3_99,cs-410,5,3,"00:07:18,070","00:07:21,700",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,And then we had computed the DCG for
cs-410_5_3_100,cs-410,5,3,"00:07:23,250","00:07:27,062",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,So this would be given by this
cs-410_5_3_101,cs-410,5,3,"00:07:27,062","00:07:35,723",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,And so this ideal DCG would then
cs-410_5_3_102,cs-410,5,3,"00:07:35,723","00:07:36,845",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,So here.
cs-410_5_3_103,cs-410,5,3,"00:07:36,845","00:07:40,040",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,And this idea of DCG would
cs-410_5_3_104,cs-410,5,3,"00:07:40,040","00:07:43,726",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,"So you can imagine now,"
cs-410_5_3_105,cs-410,5,3,"00:07:43,726","00:07:49,590",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,compare the actual DCG with the best DCG
cs-410_5_3_106,cs-410,5,3,"00:07:49,590","00:07:51,146",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,Now why do we want to do this?
cs-410_5_3_107,cs-410,5,3,"00:07:51,146","00:07:56,590",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,"Well, by doing this we'll map the DCG"
cs-410_5_3_108,cs-410,5,3,"00:07:57,900","00:08:01,650",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,"So the best value, or the highest value,"
cs-410_5_3_109,cs-410,5,3,"00:08:01,650","00:08:07,500",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,"That's when your rank list is,"
cs-410_5_3_110,cs-410,5,3,"00:08:07,500","00:08:12,260",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,"otherwise, in general,"
cs-410_5_3_111,cs-410,5,3,"00:08:13,405","00:08:14,954",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"Now, what if we don't do that?"
cs-410_5_3_112,cs-410,5,3,"00:08:14,954","00:08:19,737",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,"Well, you can see, this transformation,"
cs-410_5_3_113,cs-410,5,3,"00:08:19,737","00:08:24,108",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,doesn't really affect the relative
cs-410_5_3_114,cs-410,5,3,"00:08:24,108","00:08:29,053",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,"just one topic, because this ideal"
cs-410_5_3_115,cs-410,5,3,"00:08:29,053","00:08:33,834",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,so the ranking of systems based on
cs-410_5_3_116,cs-410,5,3,"00:08:33,834","00:08:36,986",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,if you rank them based
cs-410_5_3_117,cs-410,5,3,"00:08:36,986","00:08:40,760",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,The difference however is
cs-410_5_3_118,cs-410,5,3,"00:08:40,760","00:08:42,894",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,"Because if we don't do normalization,"
cs-410_5_3_119,cs-410,5,3,"00:08:42,894","00:08:45,730",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,different topics will have
cs-410_5_3_120,cs-410,5,3,"00:08:46,740","00:08:51,951",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,"For a topic like this one,"
cs-410_5_3_121,cs-410,5,3,"00:08:51,951","00:08:56,593",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,"the DCG can get really high,"
cs-410_5_3_122,cs-410,5,3,"00:08:56,593","00:09:02,794",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,there are only two very relevant documents
cs-410_5_3_123,cs-410,5,3,"00:09:02,794","00:09:06,124",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,Then the highest DCG that
cs-410_5_3_124,cs-410,5,3,"00:09:06,124","00:09:09,210",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,such a topic would not be very high.
cs-410_5_3_125,cs-410,5,3,"00:09:09,210","00:09:15,555",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,"So again, we face the problem of"
cs-410_5_3_126,cs-410,5,3,"00:09:15,555","00:09:17,028",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,"When we take an average,"
cs-410_5_3_127,cs-410,5,3,"00:09:17,028","00:09:20,826",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,we don't want the average to be
cs-410_5_3_128,cs-410,5,3,"00:09:20,826","00:09:23,360",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"Those are, again, easy queries."
cs-410_5_3_129,cs-410,5,3,"00:09:23,360","00:09:27,220",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,"So, by doing the normalization,"
cs-410_5_3_130,cs-410,5,3,"00:09:27,220","00:09:31,690",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,making all the queries contribute
cs-410_5_3_131,cs-410,5,3,"00:09:31,690","00:09:34,882",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"So, this is a idea of NDCG, it's used for"
cs-410_5_3_132,cs-410,5,3,"00:09:34,882","00:09:40,470",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,measuring a rank list based on multiple
cs-410_5_3_133,cs-410,5,3,"00:09:42,830","00:09:47,951",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,In a more general way this
cs-410_5_3_134,cs-410,5,3,"00:09:47,951","00:09:55,900",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,that can be applied to any ranked task
cs-410_5_3_135,cs-410,5,3,"00:09:55,900","00:10:01,111",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,And the scale of the judgements
cs-410_5_3_136,cs-410,5,3,"00:10:01,111","00:10:07,094",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,binary not only more than binary they
cs-410_5_3_137,cs-410,5,3,"00:10:07,094","00:10:11,365",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,"0, 5 or"
cs-410_5_3_138,cs-410,5,3,"00:10:11,365","00:10:15,631",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,"And the main idea of this measure,"
cs-410_5_3_139,cs-410,5,3,"00:10:15,631","00:10:19,920",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,is to measure the total utility
cs-410_5_3_140,cs-410,5,3,"00:10:19,920","00:10:24,120",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,So you always choose a cutoff and
cs-410_5_3_141,cs-410,5,3,"00:10:24,120","00:10:28,700",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,And it would discount the contribution
cs-410_5_3_142,cs-410,5,3,"00:10:28,700","00:10:31,811",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,"And then finally,"
cs-410_5_3_143,cs-410,5,3,"00:10:31,811","00:10:37,645",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,it would do normalization to ensure
cs-410_5_3_144,cs-410,5,3,"00:10:37,645","00:10:43,093",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=637,comparability across queries.
cs-410_5_3_145,cs-410,5,3,"00:10:43,093","00:10:48,319",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,[MUSIC]
cs-410_5_4_1,cs-410,5,4,"00:00:00,005","00:00:03,962",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_5_4_2,cs-410,5,4,"00:00:12,779","00:00:15,641",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,So I showed you how we rewrite the query
cs-410_5_4_3,cs-410,5,4,"00:00:15,641","00:00:20,830",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,like holder which is a function into
cs-410_5_4_4,cs-410,5,4,"00:00:20,830","00:00:25,840",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,of this slide after if we make
cs-410_5_4_5,cs-410,5,4,"00:00:25,840","00:00:30,426",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,the language model based on
cs-410_5_4_6,cs-410,5,4,"00:00:30,426","00:00:36,160",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"Now if you look at this rewriting,"
cs-410_5_4_7,cs-410,5,4,"00:00:36,160","00:00:42,470",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,The first benefit is it helps us better
cs-410_5_4_8,cs-410,5,4,"00:00:42,470","00:00:47,050",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,"In particular, we're going to show that"
cs-410_5_4_9,cs-410,5,4,"00:00:47,050","00:00:51,340",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,with the collection language model would
cs-410_5_4_10,cs-410,5,4,"00:00:51,340","00:00:52,412",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,and length normalization.
cs-410_5_4_11,cs-410,5,4,"00:00:52,412","00:00:57,645",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,The second benefit is that
cs-410_5_4_12,cs-410,5,4,"00:00:57,645","00:01:02,940",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,the query like holder more efficiently.
cs-410_5_4_13,cs-410,5,4,"00:01:02,940","00:01:06,020",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,In particular we see that
cs-410_5_4_14,cs-410,5,4,"00:01:06,020","00:01:07,860",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,is a sum over the match
cs-410_5_4_15,cs-410,5,4,"00:01:09,670","00:01:14,910",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,So this is much better than if we
cs-410_5_4_16,cs-410,5,4,"00:01:14,910","00:01:20,257",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,After we smooth the document the damage
cs-410_5_4_17,cs-410,5,4,"00:01:20,257","00:01:21,400",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,for all the words.
cs-410_5_4_18,cs-410,5,4,"00:01:21,400","00:01:25,760",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,So this new form of the formula is
cs-410_5_4_19,cs-410,5,4,"00:01:27,580","00:01:29,850",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,It's also interesting to note that
cs-410_5_4_20,cs-410,5,4,"00:01:29,850","00:01:34,420",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,the last term here is actually
cs-410_5_4_21,cs-410,5,4,"00:01:34,420","00:01:36,610",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,Since our goal is to
cs-410_5_4_22,cs-410,5,4,"00:01:36,610","00:01:40,610",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,the same query we can ignore this term for
cs-410_5_4_23,cs-410,5,4,"00:01:40,610","00:01:43,650",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,Because it's going to be the same for
cs-410_5_4_24,cs-410,5,4,"00:01:43,650","00:01:46,630",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,Ignoring it wouldn't affect
cs-410_5_4_25,cs-410,5,4,"00:01:49,070","00:01:51,890",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"Inside the sum, we"
cs-410_5_4_26,cs-410,5,4,"00:01:52,940","00:01:57,060",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,also see that each matched query
cs-410_5_4_27,cs-410,5,4,"00:01:58,510","00:02:01,990",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,And this weight actually
cs-410_5_4_28,cs-410,5,4,"00:02:01,990","00:02:07,070",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,is very interesting because it
cs-410_5_4_29,cs-410,5,4,"00:02:07,070","00:02:11,830",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,First we can already see it has
cs-410_5_4_30,cs-410,5,4,"00:02:11,830","00:02:14,250",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,just like in the vector space model.
cs-410_5_4_31,cs-410,5,4,"00:02:14,250","00:02:16,240",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,"When we take a thought product,"
cs-410_5_4_32,cs-410,5,4,"00:02:16,240","00:02:20,940",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,we see the word frequency in
cs-410_5_4_33,cs-410,5,4,"00:02:22,250","00:02:27,670",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,And so naturally this part would
cs-410_5_4_34,cs-410,5,4,"00:02:27,670","00:02:31,510",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,element from the documented vector.
cs-410_5_4_35,cs-410,5,4,"00:02:31,510","00:02:34,170",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,And here indeed we can see it actually
cs-410_5_4_36,cs-410,5,4,"00:02:35,430","00:02:39,950",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,encodes a weight that has similar
cs-410_5_4_37,cs-410,5,4,"00:02:41,160","00:02:43,660",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,"I'll let you examine it, can you see it?"
cs-410_5_4_38,cs-410,5,4,"00:02:43,660","00:02:46,110",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,Can you see which part is capturing TF?
cs-410_5_4_39,cs-410,5,4,"00:02:46,110","00:02:49,870",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,And which part is
cs-410_5_4_40,cs-410,5,4,"00:02:51,680","00:02:54,630",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,So if want you can pause
cs-410_5_4_41,cs-410,5,4,"00:02:55,830","00:03:02,640",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,So have you noticed that this P sub
cs-410_5_4_42,cs-410,5,4,"00:03:02,640","00:03:08,240",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,in the sense that if a word occurs
cs-410_5_4_43,cs-410,5,4,"00:03:08,240","00:03:11,980",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,then the s made through probability
cs-410_5_4_44,cs-410,5,4,"00:03:11,980","00:03:17,694",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,So this means this term is really
cs-410_5_4_45,cs-410,5,4,"00:03:17,694","00:03:22,324",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,Now have you also noticed that
cs-410_5_4_46,cs-410,5,4,"00:03:22,324","00:03:26,090",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,is actually achieving the factor of IDF?
cs-410_5_4_47,cs-410,5,4,"00:03:26,090","00:03:29,870",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"Why, because this is the popularity"
cs-410_5_4_48,cs-410,5,4,"00:03:31,750","00:03:37,110",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"But it's in the denominator, so if the"
cs-410_5_4_49,cs-410,5,4,"00:03:37,110","00:03:39,700",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,then the weight is actually smaller.
cs-410_5_4_50,cs-410,5,4,"00:03:39,700","00:03:41,790",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,And this means a popular term.
cs-410_5_4_51,cs-410,5,4,"00:03:41,790","00:03:45,990",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,We actually have a smaller weight and this
cs-410_5_4_52,cs-410,5,4,"00:03:47,040","00:03:50,330",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,Only that we now have
cs-410_5_4_53,cs-410,5,4,"00:03:51,550","00:03:55,920",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,Remember IDF has a logarithm
cs-410_5_4_54,cs-410,5,4,"00:03:55,920","00:03:57,290",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,But here we have something different.
cs-410_5_4_55,cs-410,5,4,"00:03:58,300","00:04:02,460",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,But intuitively it
cs-410_5_4_56,cs-410,5,4,"00:04:02,460","00:04:06,550",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,"Interestingly, we also have something"
cs-410_5_4_57,cs-410,5,4,"00:04:07,820","00:04:13,470",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,"Again, can you see which factor is related"
cs-410_5_4_58,cs-410,5,4,"00:04:14,790","00:04:18,350",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,What I just say is that this term
cs-410_5_4_59,cs-410,5,4,"00:04:19,560","00:04:24,700",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,"This collection probability,"
cs-410_5_4_60,cs-410,5,4,"00:04:24,700","00:04:29,360",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,this term here is actually related
cs-410_5_4_61,cs-410,5,4,"00:04:29,360","00:04:35,110",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,"In particular, F of sub d might"
cs-410_5_4_62,cs-410,5,4,"00:04:35,110","00:04:40,480",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,So it encodes how much probability
cs-410_5_4_63,cs-410,5,4,"00:04:41,740","00:04:43,700",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,How much smoothing do we want to do?
cs-410_5_4_64,cs-410,5,4,"00:04:43,700","00:04:46,470",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,"Intuitively, if a document is long,"
cs-410_5_4_65,cs-410,5,4,"00:04:46,470","00:04:50,980",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,then we need to do less smoothing because
cs-410_5_4_66,cs-410,5,4,"00:04:50,980","00:04:55,720",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,We probably have observed all the words
cs-410_5_4_67,cs-410,5,4,"00:04:55,720","00:05:00,900",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,But if the document is short then r of
cs-410_5_4_68,cs-410,5,4,"00:05:00,900","00:05:02,432",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,We need to do more smoothing.
cs-410_5_4_69,cs-410,5,4,"00:05:02,432","00:05:06,110",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,It's likey there are words that have
cs-410_5_4_70,cs-410,5,4,"00:05:06,110","00:05:12,250",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,So this term appears to paralyze
cs-410_5_4_71,cs-410,5,4,"00:05:12,250","00:05:19,100",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,other sub D would tend to be longer
cs-410_5_4_72,cs-410,5,4,"00:05:19,100","00:05:23,065",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,But note that alpha sub d
cs-410_5_4_73,cs-410,5,4,"00:05:23,065","00:05:28,570",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,this may not actually be necessary
cs-410_5_4_74,cs-410,5,4,"00:05:28,570","00:05:30,600",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,The effect is not so clear yet.
cs-410_5_4_75,cs-410,5,4,"00:05:31,930","00:05:36,570",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,"But as we will see later, when we"
cs-410_5_4_76,cs-410,5,4,"00:05:36,570","00:05:40,080",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,it turns out that they do
cs-410_5_4_77,cs-410,5,4,"00:05:40,080","00:05:42,730",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,Just like in TF-IDF weighting and
cs-410_5_4_78,cs-410,5,4,"00:05:42,730","00:05:45,880",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,document length normalization
cs-410_5_4_79,cs-410,5,4,"00:05:47,490","00:05:50,670",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,"So, that's a very interesting"
cs-410_5_4_80,cs-410,5,4,"00:05:50,670","00:05:54,880",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,we don't even have to think about
cs-410_5_4_81,cs-410,5,4,"00:05:54,880","00:05:59,910",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,We just need to assume that if we smooth
cs-410_5_4_82,cs-410,5,4,"00:05:59,910","00:06:05,480",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,then we would have a formula that
cs-410_5_4_83,cs-410,5,4,"00:06:05,480","00:06:06,710",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,documents length violation.
cs-410_5_4_84,cs-410,5,4,"00:06:08,210","00:06:13,150",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,What's also interesting that we have
cs-410_5_4_85,cs-410,5,4,"00:06:14,180","00:06:17,890",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,And see we have not heuristically
cs-410_5_4_86,cs-410,5,4,"00:06:19,310","00:06:23,790",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,"In fact, you can think about why"
cs-410_5_4_87,cs-410,5,4,"00:06:23,790","00:06:28,651",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,You look at the assumptions that
cs-410_5_4_88,cs-410,5,4,"00:06:28,651","00:06:33,720",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,it's because we have used a logarithm
cs-410_5_4_89,cs-410,5,4,"00:06:33,720","00:06:38,090",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,And we turned the product into a sum
cs-410_5_4_90,cs-410,5,4,"00:06:38,090","00:06:39,239",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,that's why we have this logarithm.
cs-410_5_4_91,cs-410,5,4,"00:06:40,470","00:06:44,740",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,Note that if only want to heuristically
cs-410_5_4_92,cs-410,5,4,"00:06:44,740","00:06:48,830",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,"IDF weighting, we don't necessary"
cs-410_5_4_93,cs-410,5,4,"00:06:48,830","00:06:53,700",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"Imagine if we drop this logarithm,"
cs-410_5_4_94,cs-410,5,4,"00:06:55,010","00:06:59,740",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,But what's nice with problem risk modeling
cs-410_5_4_95,cs-410,5,4,"00:06:59,740","00:07:01,950",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,the logarithm function here.
cs-410_5_4_96,cs-410,5,4,"00:07:01,950","00:07:07,510",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,And that's basically a fixed form
cs-410_5_4_97,cs-410,5,4,"00:07:07,510","00:07:13,010",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,"really have to heuristically design,"
cs-410_5_4_98,cs-410,5,4,"00:07:13,010","00:07:18,110",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,the logarithm the model probably won't
cs-410_5_4_99,cs-410,5,4,"00:07:19,400","00:07:24,300",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,So a nice property of problem risk
cs-410_5_4_100,cs-410,5,4,"00:07:24,300","00:07:28,260",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,assumptions and the probability rules
cs-410_5_4_101,cs-410,5,4,"00:07:28,260","00:07:33,390",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,And the formula would have
cs-410_5_4_102,cs-410,5,4,"00:07:34,600","00:07:38,540",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,And if we heuristically design
cs-410_5_4_103,cs-410,5,4,"00:07:38,540","00:07:40,530",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,end up having such a specific formula.
cs-410_5_4_104,cs-410,5,4,"00:07:41,700","00:07:46,940",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"So to summarize, we talked about the need"
cs-410_5_4_105,cs-410,5,4,"00:07:46,940","00:07:52,470",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,Otherwise it would give zero probability
cs-410_5_4_106,cs-410,5,4,"00:07:52,470","00:07:57,450",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,that's not good for
cs-410_5_4_107,cs-410,5,4,"00:07:59,370","00:08:03,720",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"It's also necessary, in general,"
cs-410_5_4_108,cs-410,5,4,"00:08:03,720","00:08:08,730",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,the model represent
cs-410_5_4_109,cs-410,5,4,"00:08:08,730","00:08:16,210",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,The general idea of smoothing in retrieval
cs-410_5_4_110,cs-410,5,4,"00:08:17,800","00:08:22,760",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,to give us some clue about which unseen
cs-410_5_4_111,cs-410,5,4,"00:08:22,760","00:08:26,840",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,"That is, the probability of an unseen"
cs-410_5_4_112,cs-410,5,4,"00:08:26,840","00:08:28,340",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,to its probability in the collection.
cs-410_5_4_113,cs-410,5,4,"00:08:29,610","00:08:34,330",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,"With this assumption, we've shown that we"
cs-410_5_4_114,cs-410,5,4,"00:08:34,330","00:08:38,280",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,query likelihood that has
cs-410_5_4_115,cs-410,5,4,"00:08:38,280","00:08:39,970",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,document length normalization.
cs-410_5_4_116,cs-410,5,4,"00:08:39,970","00:08:42,210",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"We also see that, through some rewriting,"
cs-410_5_4_117,cs-410,5,4,"00:08:42,210","00:08:47,080",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,the scoring of such a ranking function
cs-410_5_4_118,cs-410,5,4,"00:08:47,080","00:08:50,530",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"matched query terms,"
cs-410_5_4_119,cs-410,5,4,"00:08:50,530","00:08:54,500",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,"But, the actual ranking"
cs-410_5_4_120,cs-410,5,4,"00:08:54,500","00:08:59,010",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,automatically by the probability rules and
cs-410_5_4_121,cs-410,5,4,"00:08:59,010","00:09:02,210",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,And like in the vector space model
cs-410_5_4_122,cs-410,5,4,"00:09:02,210","00:09:04,580",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,think about the form of the function.
cs-410_5_4_123,cs-410,5,4,"00:09:04,580","00:09:09,234",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,"However, we still need to address"
cs-410_5_4_124,cs-410,5,4,"00:09:09,234","00:09:11,652",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,smooth the document and the model.
cs-410_5_4_125,cs-410,5,4,"00:09:11,652","00:09:14,859",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,How exactly we should
cs-410_5_4_126,cs-410,5,4,"00:09:14,859","00:09:19,223",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,model based on the connection
cs-410_5_4_127,cs-410,5,4,"00:09:19,223","00:09:24,226",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,the maximum micro is made of and
cs-410_5_4_128,cs-410,5,4,"00:09:24,226","00:09:34,226",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,[MUSIC]
cs-410_5_5_1,cs-410,5,5,"00:00:00,000","00:00:03,894",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_5_5_2,cs-410,5,5,"00:00:07,481","00:00:09,920",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about the Web Indexing.
cs-410_5_5_3,cs-410,5,5,"00:00:11,980","00:00:16,740",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,"In this lecture, we will continue"
cs-410_5_5_4,cs-410,5,5,"00:00:16,740","00:00:20,741",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,we're going to talk about how
cs-410_5_5_5,cs-410,5,5,"00:00:24,457","00:00:29,720",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"So once we crawl the web,"
cs-410_5_5_6,cs-410,5,5,"00:00:29,720","00:00:33,489",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,The next step is to use the indexer
cs-410_5_5_7,cs-410,5,5,"00:00:36,540","00:00:41,150",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,"In general, we can use the same"
cs-410_5_5_8,cs-410,5,5,"00:00:41,150","00:00:45,060",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,creating an index and that is what we
cs-410_5_5_9,cs-410,5,5,"00:00:45,060","00:00:48,718",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,but there are there are new
cs-410_5_5_10,cs-410,5,5,"00:00:48,718","00:00:55,100",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"For web scale indexing, and the two main"
cs-410_5_5_11,cs-410,5,5,"00:00:55,100","00:00:57,450",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,"The index would be so large,"
cs-410_5_5_12,cs-410,5,5,"00:00:57,450","00:01:03,220",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,that it cannot actually fit into
cs-410_5_5_13,cs-410,5,5,"00:01:03,220","00:01:05,879",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,So we have to store the data
cs-410_5_5_14,cs-410,5,5,"00:01:06,910","00:01:10,900",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"Also, because the data is so"
cs-410_5_5_15,cs-410,5,5,"00:01:10,900","00:01:15,700",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"process the data in parallel, so"
cs-410_5_5_16,cs-410,5,5,"00:01:15,700","00:01:20,410",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,"Now to address these challenges,"
cs-410_5_5_17,cs-410,5,5,"00:01:20,410","00:01:25,430",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,One is the Google File System that's
cs-410_5_5_18,cs-410,5,5,"00:01:25,430","00:01:30,900",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,programmers manage files stored
cs-410_5_5_19,cs-410,5,5,"00:01:32,000","00:01:33,159",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,The second is MapReduce.
cs-410_5_5_20,cs-410,5,5,"00:01:33,159","00:01:37,140",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,This is a general software framework for
cs-410_5_5_21,cs-410,5,5,"00:01:38,960","00:01:44,830",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,Hadoop is the most well known open
cs-410_5_5_22,cs-410,5,5,"00:01:44,830","00:01:47,790",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,Now used in many applications.
cs-410_5_5_23,cs-410,5,5,"00:01:50,000","00:01:52,510",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,"So, this is the architecture"
cs-410_5_5_24,cs-410,5,5,"00:01:53,790","00:01:56,930",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,It uses a very simple centralized
cs-410_5_5_25,cs-410,5,5,"00:01:56,930","00:02:01,210",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,management mechanism to manage
cs-410_5_5_26,cs-410,5,5,"00:02:01,210","00:02:05,590",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"Files, so"
cs-410_5_5_27,cs-410,5,5,"00:02:05,590","00:02:09,790",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,look up a table to know where
cs-410_5_5_28,cs-410,5,5,"00:02:11,040","00:02:16,250",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,The application client will then
cs-410_5_5_29,cs-410,5,5,"00:02:16,250","00:02:21,420",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,that obtains specific locations of
cs-410_5_5_30,cs-410,5,5,"00:02:22,890","00:02:31,450",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,And once the GFS file kind obtained
cs-410_5_5_31,cs-410,5,5,"00:02:31,450","00:02:37,880",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,then the application client can talk
cs-410_5_5_32,cs-410,5,5,"00:02:37,880","00:02:43,230",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,"data actually sits directly, so"
cs-410_5_5_33,cs-410,5,5,"00:02:43,230","00:02:43,970",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,In the network.
cs-410_5_5_34,cs-410,5,5,"00:02:46,020","00:02:53,290",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,So when this file system stores
cs-410_5_5_35,cs-410,5,5,"00:02:53,290","00:02:59,650",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"with great fixed sizes of chunks, so"
cs-410_5_5_36,cs-410,5,5,"00:03:00,720","00:03:01,460",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,Many chunks.
cs-410_5_5_37,cs-410,5,5,"00:03:01,460","00:03:05,120",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,"Each chunk is 64 MB, so it's pretty big."
cs-410_5_5_38,cs-410,5,5,"00:03:05,120","00:03:09,080",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,And that's appropriate for
cs-410_5_5_39,cs-410,5,5,"00:03:09,080","00:03:12,510",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,These chunks are replicated
cs-410_5_5_40,cs-410,5,5,"00:03:12,510","00:03:17,210",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,So this is something that the programmer
cs-410_5_5_41,cs-410,5,5,"00:03:17,210","00:03:22,210",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,and it's all taken care
cs-410_5_5_42,cs-410,5,5,"00:03:22,210","00:03:24,110",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"So from the application perspective,"
cs-410_5_5_43,cs-410,5,5,"00:03:24,110","00:03:28,250",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,the programmer would see this
cs-410_5_5_44,cs-410,5,5,"00:03:28,250","00:03:32,510",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,And the programmer doesn't have to
cs-410_5_5_45,cs-410,5,5,"00:03:32,510","00:03:35,535",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,can just invoke high level.
cs-410_5_5_46,cs-410,5,5,"00:03:35,535","00:03:38,275",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,Operators to process the file.
cs-410_5_5_47,cs-410,5,5,"00:03:39,975","00:03:44,915",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,And another feature is that the data
cs-410_5_5_48,cs-410,5,5,"00:03:44,915","00:03:45,865",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,and chunk servers.
cs-410_5_5_49,cs-410,5,5,"00:03:45,865","00:03:48,735",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,So it's efficient in this sense.
cs-410_5_5_50,cs-410,5,5,"00:03:51,190","00:03:54,590",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"On top of the Google file system, Google"
cs-410_5_5_51,cs-410,5,5,"00:03:54,590","00:03:59,220",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,also proposed MapReduce as a general
cs-410_5_5_52,cs-410,5,5,"00:03:59,220","00:04:05,660",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"Now, this is very useful to support"
cs-410_5_5_53,cs-410,5,5,"00:04:06,670","00:04:10,618",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"And so, this framework is,"
cs-410_5_5_54,cs-410,5,5,"00:04:12,116","00:04:16,170",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,Hiding a lot of low-level
cs-410_5_5_55,cs-410,5,5,"00:04:16,170","00:04:21,950",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"As a result, the programmer can make"
cs-410_5_5_56,cs-410,5,5,"00:04:21,950","00:04:26,580",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,that can be run a large
cs-410_5_5_57,cs-410,5,5,"00:04:28,990","00:04:33,930",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,So some of the low level details
cs-410_5_5_58,cs-410,5,5,"00:04:33,930","00:04:39,410",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,the specific and network communications or
cs-410_5_5_59,cs-410,5,5,"00:04:39,410","00:04:44,080",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,where the task are executed.
cs-410_5_5_60,cs-410,5,5,"00:04:44,080","00:04:46,600",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,All these details are hidden
cs-410_5_5_61,cs-410,5,5,"00:04:47,880","00:04:52,560",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,There is also a nice feature which
cs-410_5_5_62,cs-410,5,5,"00:04:52,560","00:04:56,490",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,"If one server is broken,"
cs-410_5_5_63,cs-410,5,5,"00:04:56,490","00:05:01,140",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"the server is down, and"
cs-410_5_5_64,cs-410,5,5,"00:05:01,140","00:05:05,300",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,Then the MapReduce mapper will know
cs-410_5_5_65,cs-410,5,5,"00:05:05,300","00:05:11,600",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,So it automatically dispatches a task
cs-410_5_5_66,cs-410,5,5,"00:05:11,600","00:05:15,400",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,"And therefore, again the program"
cs-410_5_5_67,cs-410,5,5,"00:05:15,400","00:05:17,570",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,here's how MapReduce works.
cs-410_5_5_68,cs-410,5,5,"00:05:17,570","00:05:23,330",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,The input data would be separated
cs-410_5_5_69,cs-410,5,5,"00:05:23,330","00:05:26,460",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,Now what exactly is in the value
cs-410_5_5_70,cs-410,5,5,"00:05:26,460","00:05:31,520",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,it's actually a fairly general framework
cs-410_5_5_71,cs-410,5,5,"00:05:31,520","00:05:35,450",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,into different parts and each part
cs-410_5_5_72,cs-410,5,5,"00:05:37,100","00:05:40,984",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,Each key value pair would be and
cs-410_5_5_73,cs-410,5,5,"00:05:40,984","00:05:44,750",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"The program was right the map function,"
cs-410_5_5_74,cs-410,5,5,"00:05:45,890","00:05:50,043",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,And then the map function will
cs-410_5_5_75,cs-410,5,5,"00:05:50,043","00:05:53,870",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,then generate a number of
cs-410_5_5_76,cs-410,5,5,"00:05:53,870","00:05:58,370",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,"Of course, the new key is usually"
cs-410_5_5_77,cs-410,5,5,"00:05:58,370","00:06:02,070",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,that's given to the map as input.
cs-410_5_5_78,cs-410,5,5,"00:06:02,070","00:06:06,000",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,And these key value pairs
cs-410_5_5_79,cs-410,5,5,"00:06:06,000","00:06:10,420",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,all the outputs of all the map
cs-410_5_5_80,cs-410,5,5,"00:06:12,540","00:06:16,670",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,and then there will be for
cs-410_5_5_81,cs-410,5,5,"00:06:16,670","00:06:20,600",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,"And the result is that,"
cs-410_5_5_82,cs-410,5,5,"00:06:20,600","00:06:24,260",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,with the same key will be
cs-410_5_5_83,cs-410,5,5,"00:06:24,260","00:06:30,480",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,So now we've got a pair of of a key and
cs-410_5_5_84,cs-410,5,5,"00:06:31,630","00:06:34,960",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,So this would then be sent
cs-410_5_5_85,cs-410,5,5,"00:06:36,330","00:06:41,330",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"Now, of course, each reduce function"
cs-410_5_5_86,cs-410,5,5,"00:06:41,330","00:06:45,990",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,so we will send these output values to
cs-410_5_5_87,cs-410,5,5,"00:06:45,990","00:06:50,580",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,multiple reduce functions
cs-410_5_5_88,cs-410,5,5,"00:06:52,220","00:06:57,980",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,A reduce function would then
cs-410_5_5_89,cs-410,5,5,"00:06:57,980","00:07:04,920",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,a key in a set of values to produce
cs-410_5_5_90,cs-410,5,5,"00:07:04,920","00:07:08,670",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,So these output values would
cs-410_5_5_91,cs-410,5,5,"00:07:08,670","00:07:11,220",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,to form the final output.
cs-410_5_5_92,cs-410,5,5,"00:07:12,420","00:07:17,210",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,"And so, this is the general"
cs-410_5_5_93,cs-410,5,5,"00:07:17,210","00:07:23,290",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,Now the programmer only needs to write
cs-410_5_5_94,cs-410,5,5,"00:07:23,290","00:07:28,090",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,Everything else is actually taken
cs-410_5_5_95,cs-410,5,5,"00:07:28,090","00:07:32,920",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,So you can see the program really
cs-410_5_5_96,cs-410,5,5,"00:07:32,920","00:07:38,570",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,"And with such a framework, the input data"
cs-410_5_5_97,cs-410,5,5,"00:07:38,570","00:07:42,780",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"which is processing parallel first by map,"
cs-410_5_5_98,cs-410,5,5,"00:07:42,780","00:07:50,130",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,then being the process after
cs-410_5_5_99,cs-410,5,5,"00:07:50,130","00:07:54,340",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,The much more reduced if I'm
cs-410_5_5_100,cs-410,5,5,"00:07:55,720","00:08:00,390",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,the different keys and
cs-410_5_5_101,cs-410,5,5,"00:08:00,390","00:08:02,980",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,"So it achieves some,"
cs-410_5_5_102,cs-410,5,5,"00:08:05,410","00:08:10,510",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,it achieves the purpose of parallel
cs-410_5_5_103,cs-410,5,5,"00:08:10,510","00:08:13,620",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,So let's take a look at a simple example.
cs-410_5_5_104,cs-410,5,5,"00:08:13,620","00:08:15,040",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,And that's Word Counting.
cs-410_5_5_105,cs-410,5,5,"00:08:16,620","00:08:21,570",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"The input is containing words,"
cs-410_5_5_106,cs-410,5,5,"00:08:21,570","00:08:25,990",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,and the output that we want to generate is
cs-410_5_5_107,cs-410,5,5,"00:08:25,990","00:08:27,080",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,So it's the Word Count.
cs-410_5_5_108,cs-410,5,5,"00:08:28,270","00:08:32,940",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,We know this kind of counting
cs-410_5_5_109,cs-410,5,5,"00:08:32,940","00:08:38,290",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,assess the popularity of a word in
cs-410_5_5_110,cs-410,5,5,"00:08:38,290","00:08:41,880",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,achieving a factor of IDF wading for
cs-410_5_5_111,cs-410,5,5,"00:08:42,880","00:08:44,200",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,So how can we solve this problem?
cs-410_5_5_112,cs-410,5,5,"00:08:44,200","00:08:49,200",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,"Well, one natural thought is that,"
cs-410_5_5_113,cs-410,5,5,"00:08:49,200","00:08:53,860",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,done in parallel by simply counting
cs-410_5_5_114,cs-410,5,5,"00:08:53,860","00:08:57,000",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,and then in the end we just
cs-410_5_5_115,cs-410,5,5,"00:08:57,000","00:09:01,800",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,And that's precisely the idea of
cs-410_5_5_116,cs-410,5,5,"00:09:02,900","00:09:06,440",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,We can parallelize on
cs-410_5_5_117,cs-410,5,5,"00:09:07,670","00:09:13,100",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,"So more specifically, we can assume"
cs-410_5_5_118,cs-410,5,5,"00:09:14,120","00:09:20,450",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,a key value pair that represents the line
cs-410_5_5_119,cs-410,5,5,"00:09:20,450","00:09:25,760",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"So the first line, for"
cs-410_5_5_120,cs-410,5,5,"00:09:25,760","00:09:32,240",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,that is another word by word and
cs-410_5_5_121,cs-410,5,5,"00:09:32,240","00:09:36,250",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,So this key value pair would
cs-410_5_5_122,cs-410,5,5,"00:09:36,250","00:09:40,670",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,The Map Function then would just
cs-410_5_5_123,cs-410,5,5,"00:09:41,700","00:09:43,880",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,"And in this case,"
cs-410_5_5_124,cs-410,5,5,"00:09:43,880","00:09:46,360",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,Each world gets a count of one and
cs-410_5_5_125,cs-410,5,5,"00:09:46,360","00:09:52,770",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,these are the output that you see here
cs-410_5_5_126,cs-410,5,5,"00:09:52,770","00:09:56,270",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=592,So the map function is really
cs-410_5_5_127,cs-410,5,5,"00:09:56,270","00:10:00,450",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,what the pseudocode looks
cs-410_5_5_128,cs-410,5,5,"00:10:00,450","00:10:05,370",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,you see it simply needs to iterate
cs-410_5_5_129,cs-410,5,5,"00:10:05,370","00:10:08,330",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,And then just collect the function
cs-410_5_5_130,cs-410,5,5,"00:10:09,390","00:10:14,080",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,which means it would then send the word
cs-410_5_5_131,cs-410,5,5,"00:10:14,080","00:10:18,686",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,The collector would then try to
cs-410_5_5_132,cs-410,5,5,"00:10:18,686","00:10:21,205",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,"different Map Functions, right?"
cs-410_5_5_133,cs-410,5,5,"00:10:21,205","00:10:25,937",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,So the function is very simple and
cs-410_5_5_134,cs-410,5,5,"00:10:25,937","00:10:30,300",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,this function as a way to
cs-410_5_5_135,cs-410,5,5,"00:10:31,620","00:10:34,780",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,"Of course, the second line will be"
cs-410_5_5_136,cs-410,5,5,"00:10:34,780","00:10:36,990",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,which we will produce a single output.
cs-410_5_5_137,cs-410,5,5,"00:10:36,990","00:10:40,800",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=636,"Okay, now the output from the map"
cs-410_5_5_138,cs-410,5,5,"00:10:40,800","00:10:45,550",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,send it to a collector and the collector
cs-410_5_5_139,cs-410,5,5,"00:10:45,550","00:10:50,220",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=645,"So at this stage, you can see,"
cs-410_5_5_140,cs-410,5,5,"00:10:50,220","00:10:53,850",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,Each pair is a word and
cs-410_5_5_141,cs-410,5,5,"00:10:53,850","00:10:58,960",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=653,"So, once we see all these pairs."
cs-410_5_5_142,cs-410,5,5,"00:10:58,960","00:11:03,570",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,"Then we can sort them based on the key,"
cs-410_5_5_143,cs-410,5,5,"00:11:03,570","00:11:08,570",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,So we will collect all the counts
cs-410_5_5_144,cs-410,5,5,"00:11:09,610","00:11:11,790",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=669,"And similarly, we do that for other words."
cs-410_5_5_145,cs-410,5,5,"00:11:11,790","00:11:13,620",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,"Like Hadoop, Hello, etc."
cs-410_5_5_146,cs-410,5,5,"00:11:13,620","00:11:19,040",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,So each word now is attached to
cs-410_5_5_147,cs-410,5,5,"00:11:20,700","00:11:27,860",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,And these counts represent the occurrences
cs-410_5_5_148,cs-410,5,5,"00:11:27,860","00:11:33,330",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,So now we have got a new pair of a key and
cs-410_5_5_149,cs-410,5,5,"00:11:33,330","00:11:38,610",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,this pair will then be fed into reduce
cs-410_5_5_150,cs-410,5,5,"00:11:38,610","00:11:44,450",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,would have to finish the job of counting
cs-410_5_5_151,cs-410,5,5,"00:11:44,450","00:11:47,020",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,"Now, it has all ready got all"
cs-410_5_5_152,cs-410,5,5,"00:11:47,020","00:11:50,370",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,all it needs to do is
cs-410_5_5_153,cs-410,5,5,"00:11:50,370","00:11:53,810",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,So the reduce function here
cs-410_5_5_154,cs-410,5,5,"00:11:53,810","00:11:57,130",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,"You have a counter, and"
cs-410_5_5_155,cs-410,5,5,"00:11:57,130","00:11:59,260",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,That you'll see in this array.
cs-410_5_5_156,cs-410,5,5,"00:11:59,260","00:12:02,884",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=719,"And that,"
cs-410_5_5_157,cs-410,5,5,"00:12:02,884","00:12:07,203",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,"And then finally, you output the P and"
cs-410_5_5_158,cs-410,5,5,"00:12:07,203","00:12:11,140",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,And that's precisely what we want as
cs-410_5_5_159,cs-410,5,5,"00:12:12,220","00:12:14,830",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,"So you can see,"
cs-410_5_5_160,cs-410,5,5,"00:12:14,830","00:12:16,842",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,To building an Invert index.
cs-410_5_5_161,cs-410,5,5,"00:12:16,842","00:12:21,050",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,"And if you think about it,"
cs-410_5_5_162,cs-410,5,5,"00:12:21,050","00:12:24,410",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,"And we have already got a dictionary,"
cs-410_5_5_163,cs-410,5,5,"00:12:24,410","00:12:26,440",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,We have got the count.
cs-410_5_5_164,cs-410,5,5,"00:12:26,440","00:12:32,776",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,But what's missing is
cs-410_5_5_165,cs-410,5,5,"00:12:32,776","00:12:38,240",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,frequency counts of words
cs-410_5_5_166,cs-410,5,5,"00:12:38,240","00:12:43,420",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,So we can modify this slightly to
cs-410_5_5_167,cs-410,5,5,"00:12:43,420","00:12:45,800",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,here's one way to do that.
cs-410_5_5_168,cs-410,5,5,"00:12:45,800","00:12:51,490",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=765,"So in this case, we can assume the input"
cs-410_5_5_169,cs-410,5,5,"00:12:51,490","00:12:56,510",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=771,"which denotes the document ID,"
cs-410_5_5_170,cs-410,5,5,"00:12:56,510","00:13:02,420",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=776,"denoting the screen for that document,"
cs-410_5_5_171,cs-410,5,5,"00:13:02,420","00:13:05,740",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=782,"And so, the map function would do"
cs-410_5_5_172,cs-410,5,5,"00:13:05,740","00:13:07,910",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,seen in the word campaign example.
cs-410_5_5_173,cs-410,5,5,"00:13:07,910","00:13:14,640",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=787,It simply groups all the counts of
cs-410_5_5_174,cs-410,5,5,"00:13:14,640","00:13:18,010",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,And it would then generate
cs-410_5_5_175,cs-410,5,5,"00:13:18,010","00:13:21,140",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,"Each key is a word, and"
cs-410_5_5_176,cs-410,5,5,"00:13:21,140","00:13:27,650",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=801,the value is the count of this word in
cs-410_5_5_177,cs-410,5,5,"00:13:27,650","00:13:32,640",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,"Now, you can easily see why we need to"
cs-410_5_5_178,cs-410,5,5,"00:13:32,640","00:13:36,690",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=812,"in inverted index, we would like to"
cs-410_5_5_179,cs-410,5,5,"00:13:36,690","00:13:41,290",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=816,"should keep track of it, and this can then"
cs-410_5_5_180,cs-410,5,5,"00:13:41,290","00:13:46,710",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=821,Now similarly another document D2
cs-410_5_5_181,cs-410,5,5,"00:13:46,710","00:13:50,890",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=826,"So in the end, again, there is a sorting"
cs-410_5_5_182,cs-410,5,5,"00:13:50,890","00:13:55,690",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=830,"And then we will have just a key,"
cs-410_5_5_183,cs-410,5,5,"00:13:55,690","00:14:00,340",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=835,associated with all the documents
cs-410_5_5_184,cs-410,5,5,"00:14:00,340","00:14:02,870",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,Or all the documents where java occurred.
cs-410_5_5_185,cs-410,5,5,"00:14:04,500","00:14:09,520",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,"And the counts, so"
cs-410_5_5_186,cs-410,5,5,"00:14:09,520","00:14:11,880",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=849,And this will be collected together.
cs-410_5_5_187,cs-410,5,5,"00:14:11,880","00:14:15,840",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,"And this will be, so"
cs-410_5_5_188,cs-410,5,5,"00:14:15,840","00:14:20,010",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,So now you can see the reduce function
cs-410_5_5_189,cs-410,5,5,"00:14:20,010","00:14:21,800",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=860,an inverted index entry.
cs-410_5_5_190,cs-410,5,5,"00:14:21,800","00:14:27,280",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=861,So it's just the word and all
cs-410_5_5_191,cs-410,5,5,"00:14:27,280","00:14:30,900",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=867,the frequencies of the word
cs-410_5_5_192,cs-410,5,5,"00:14:30,900","00:14:35,240",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,So all you need to do is
cs-410_5_5_193,cs-410,5,5,"00:14:37,670","00:14:40,380",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,into a continuous chunk of data.
cs-410_5_5_194,cs-410,5,5,"00:14:40,380","00:14:43,650",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=880,And this can be done
cs-410_5_5_195,cs-410,5,5,"00:14:43,650","00:14:47,520",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,So basically the reduce function
cs-410_5_5_196,cs-410,5,5,"00:14:47,520","00:14:48,020",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=887,Work.
cs-410_5_5_197,cs-410,5,5,"00:14:49,450","00:14:53,647",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=889,"And so, this is a pseudo-code for"
cs-410_5_5_198,cs-410,5,5,"00:14:53,647","00:14:58,010",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=893,[INAUDIBLE] that's construction.
cs-410_5_5_199,cs-410,5,5,"00:14:58,010","00:15:05,290",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=898,"Here we see two functions,"
cs-410_5_5_200,cs-410,5,5,"00:15:05,290","00:15:13,440",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=905,And a programmer would specify these two
cs-410_5_5_201,cs-410,5,5,"00:15:13,440","00:15:18,990",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=913,And you can see basically they
cs-410_5_5_202,cs-410,5,5,"00:15:18,990","00:15:22,870",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=918,"In the case of map, it's going to count"
cs-410_5_5_203,cs-410,5,5,"00:15:22,870","00:15:27,040",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=922,the occurrences of a word
cs-410_5_5_204,cs-410,5,5,"00:15:27,040","00:15:34,232",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=927,And it would output all the counts
cs-410_5_5_205,cs-410,5,5,"00:15:34,232","00:15:40,350",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=934,"So, this is the reduce function,"
cs-410_5_5_206,cs-410,5,5,"00:15:40,350","00:15:47,380",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=940,simply concatenates all the input
cs-410_5_5_207,cs-410,5,5,"00:15:47,380","00:15:53,580",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=947,and then put them together as
cs-410_5_5_208,cs-410,5,5,"00:15:53,580","00:15:58,250",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,So this is a very simple
cs-410_5_5_209,cs-410,5,5,"00:15:58,250","00:16:03,360",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=958,it would allow us to construct an inverted
cs-410_5_5_210,cs-410,5,5,"00:16:03,360","00:16:06,950",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=963,the data can be processed
cs-410_5_5_211,cs-410,5,5,"00:16:06,950","00:16:11,000",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=966,And program doesn't have to
cs-410_5_5_212,cs-410,5,5,"00:16:12,080","00:16:18,930",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=972,So this is how we can do parallel
cs-410_5_5_213,cs-410,5,5,"00:16:20,040","00:16:21,960",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=980,"So to summarize,"
cs-410_5_5_214,cs-410,5,5,"00:16:21,960","00:16:26,040",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=981,web scale indexing requires some
cs-410_5_5_215,cs-410,5,5,"00:16:26,040","00:16:29,230",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,Standard traditional indexing techniques.
cs-410_5_5_216,cs-410,5,5,"00:16:29,230","00:16:32,800",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=989,"Mainly, we have to store"
cs-410_5_5_217,cs-410,5,5,"00:16:32,800","00:16:37,990",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=992,And this is usually done by using a filing
cs-410_5_5_218,cs-410,5,5,"00:16:37,990","00:16:40,240",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=997,But this should be through a file system.
cs-410_5_5_219,cs-410,5,5,"00:16:40,240","00:16:45,320",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,"And secondly, it requires creating"
cs-410_5_5_220,cs-410,5,5,"00:16:45,320","00:16:50,340",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,large and takes long time to create
cs-410_5_5_221,cs-410,5,5,"00:16:50,340","00:16:53,790",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1010,"So if we can do it in parallel,"
cs-410_5_5_222,cs-410,5,5,"00:16:53,790","00:16:56,690",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,this is done by using
cs-410_5_5_223,cs-410,5,5,"00:16:57,850","00:17:02,182",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1017,Note that both the GFS and
cs-410_5_5_224,cs-410,5,5,"00:17:02,182","00:17:05,251",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1022,they can also support
cs-410_5_5_225,cs-410,5,5,"00:17:07,795","00:17:17,795",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1027,[MUSIC]
cs-410_5_6_1,cs-410,5,6,"00:00:00,000","00:00:07,248",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_5_6_2,cs-410,5,6,"00:00:07,248","00:00:09,548",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about
cs-410_5_6_3,cs-410,5,6,"00:00:12,888","00:00:18,400",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,So far we have talked about a lot
cs-410_5_6_4,cs-410,5,6,"00:00:19,680","00:00:24,675",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,We have talked about the problem
cs-410_5_6_5,cs-410,5,6,"00:00:24,675","00:00:30,134",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"different methods for ranking,"
cs-410_5_6_6,cs-410,5,6,"00:00:30,134","00:00:33,198",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"how to evaluate a search engine, etc."
cs-410_5_6_7,cs-410,5,6,"00:00:36,028","00:00:40,719",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,This is important because we know
cs-410_5_6_8,cs-410,5,6,"00:00:40,719","00:00:44,980",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,the most important applications
cs-410_5_6_9,cs-410,5,6,"00:00:44,980","00:00:49,820",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,And they are the most useful tools
cs-410_5_6_10,cs-410,5,6,"00:00:49,820","00:00:53,889",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,data into a small set
cs-410_5_6_11,cs-410,5,6,"00:00:56,330","00:01:00,959",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,Another reason why we spend so
cs-410_5_6_12,cs-410,5,6,"00:01:00,959","00:01:06,961",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,is because many techniques used in search
cs-410_5_6_13,cs-410,5,6,"00:01:06,961","00:01:11,266",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"Recommender Systems,"
cs-410_5_6_14,cs-410,5,6,"00:01:11,266","00:01:16,840",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,"And so, overall, the two systems"
cs-410_5_6_15,cs-410,5,6,"00:01:16,840","00:01:19,110",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,And there are many techniques
cs-410_5_6_16,cs-410,5,6,"00:01:22,690","00:01:24,860",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,So this is a slide that
cs-410_5_6_17,cs-410,5,6,"00:01:24,860","00:01:29,020",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,when we talked about the two
cs-410_5_6_18,cs-410,5,6,"00:01:29,020","00:01:30,230",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,Pull and the Push.
cs-410_5_6_19,cs-410,5,6,"00:01:31,240","00:01:36,362",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,And we mentioned that recommender
cs-410_5_6_20,cs-410,5,6,"00:01:36,362","00:01:42,079",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,"users in the Push Mode, where the systems"
cs-410_5_6_21,cs-410,5,6,"00:01:42,079","00:01:47,228",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,the information to the user or
cs-410_5_6_22,cs-410,5,6,"00:01:47,228","00:01:51,429",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,And this often works
cs-410_5_6_23,cs-410,5,6,"00:01:51,429","00:01:56,341",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,stable information need
cs-410_5_6_24,cs-410,5,6,"00:01:56,341","00:02:01,649",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,So a Recommender System is sometimes
cs-410_5_6_25,cs-410,5,6,"00:02:01,649","00:02:07,431",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,it's because recommending useful
cs-410_5_6_26,cs-410,5,6,"00:02:07,431","00:02:10,749",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"filtering out the the useless articles,"
cs-410_5_6_27,cs-410,5,6,"00:02:10,749","00:02:14,370",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,and so
cs-410_5_6_28,cs-410,5,6,"00:02:16,070","00:02:20,412",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,And in all the cases the system
cs-410_5_6_29,cs-410,5,6,"00:02:20,412","00:02:24,840",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,usually there's a dynamic source
cs-410_5_6_30,cs-410,5,6,"00:02:24,840","00:02:29,028",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,that you have some knowledge
cs-410_5_6_31,cs-410,5,6,"00:02:29,028","00:02:31,788",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,And then the system would make a decision
cs-410_5_6_32,cs-410,5,6,"00:02:31,788","00:02:34,950",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,about whether this item is
cs-410_5_6_33,cs-410,5,6,"00:02:34,950","00:02:39,678",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,then if it's interesting then the system
cs-410_5_6_34,cs-410,5,6,"00:02:43,008","00:02:49,520",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,So the basic filtering question here is
cs-410_5_6_35,cs-410,5,6,"00:02:49,520","00:02:52,426",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,Will U like item X?
cs-410_5_6_36,cs-410,5,6,"00:02:52,426","00:02:55,640",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,And there are two ways to answer this
cs-410_5_6_37,cs-410,5,6,"00:02:56,738","00:03:00,040",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,And one is look at what items U likes and
cs-410_5_6_38,cs-410,5,6,"00:03:00,040","00:03:03,655",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,then we can see if X is
cs-410_5_6_39,cs-410,5,6,"00:03:05,610","00:03:10,460",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,"The other is to look at who likes X,"
cs-410_5_6_40,cs-410,5,6,"00:03:10,460","00:03:16,000",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,"user looks like a one of those users,"
cs-410_5_6_41,cs-410,5,6,"00:03:16,000","00:03:18,640",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,And these strategies can be combined.
cs-410_5_6_42,cs-410,5,6,"00:03:18,640","00:03:20,800",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,If we follow the first strategy and
cs-410_5_6_43,cs-410,5,6,"00:03:20,800","00:03:26,170",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,look at item similarity in the case
cs-410_5_6_44,cs-410,5,6,"00:03:26,170","00:03:31,460",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,then we're talking about a content-based
cs-410_5_6_45,cs-410,5,6,"00:03:31,460","00:03:38,195",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"If we look at the second strategy, then,"
cs-410_5_6_46,cs-410,5,6,"00:03:38,195","00:03:43,110",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,we're user similarity and the technique
cs-410_5_6_47,cs-410,5,6,"00:03:46,010","00:03:49,190",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,"So, let's first look at"
cs-410_5_6_48,cs-410,5,6,"00:03:49,190","00:03:51,530",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,This is what the system would look like.
cs-410_5_6_49,cs-410,5,6,"00:03:52,600","00:03:56,420",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"Inside the system, there will be"
cs-410_5_6_50,cs-410,5,6,"00:03:56,420","00:04:00,860",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"knowledge about the user's interests, and"
cs-410_5_6_51,cs-410,5,6,"00:04:02,210","00:04:06,815",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,It maintains this profile to keep
cs-410_5_6_52,cs-410,5,6,"00:04:06,815","00:04:10,865",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,then there is a utility function
cs-410_5_6_53,cs-410,5,6,"00:04:10,865","00:04:13,955",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,a nice plan utility
cs-410_5_6_54,cs-410,5,6,"00:04:13,955","00:04:17,977",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,It helps the system decide
cs-410_5_6_55,cs-410,5,6,"00:04:17,977","00:04:21,307",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,And then the accepted documents will
cs-410_5_6_56,cs-410,5,6,"00:04:21,307","00:04:23,457",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,according to the classified.
cs-410_5_6_57,cs-410,5,6,"00:04:23,457","00:04:28,327",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,There should be also an initialization
cs-410_5_6_58,cs-410,5,6,"00:04:28,327","00:04:34,167",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,maybe from a user's specified keywords or
cs-410_5_6_59,cs-410,5,6,"00:04:34,167","00:04:38,519",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,"etc., and this would be to feed into"
cs-410_5_6_60,cs-410,5,6,"00:04:39,900","00:04:43,210",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,There is also typically a learning
cs-410_5_6_61,cs-410,5,6,"00:04:43,210","00:04:45,310",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,users' feedback over time.
cs-410_5_6_62,cs-410,5,6,"00:04:45,310","00:04:49,310",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,Now note that in this case typical
cs-410_5_6_63,cs-410,5,6,"00:04:49,310","00:04:53,420",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,the system would have a lot more
cs-410_5_6_64,cs-410,5,6,"00:04:53,420","00:04:58,590",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"If the user has taken a recommended item,"
cs-410_5_6_65,cs-410,5,6,"00:04:58,590","00:05:04,020",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,this a signal to indicate that
cs-410_5_6_66,cs-410,5,6,"00:05:04,020","00:05:07,010",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"If the user discarded it,"
cs-410_5_6_67,cs-410,5,6,"00:05:07,010","00:05:11,640",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,And so such feedback can be a long term
cs-410_5_6_68,cs-410,5,6,"00:05:11,640","00:05:16,660",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,And the system can collect a lot of
cs-410_5_6_69,cs-410,5,6,"00:05:16,660","00:05:19,500",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,this then can then be used
cs-410_5_6_70,cs-410,5,6,"00:05:19,500","00:05:23,780",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,Now what's the criteria for
cs-410_5_6_71,cs-410,5,6,"00:05:24,860","00:05:31,190",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,How do we know this filtering
cs-410_5_6_72,cs-410,5,6,"00:05:31,190","00:05:36,440",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,Now in this case we cannot use the ranking
cs-410_5_6_73,cs-410,5,6,"00:05:36,440","00:05:39,300",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,because we can't afford waiting for
cs-410_5_6_74,cs-410,5,6,"00:05:39,300","00:05:42,960",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,then rank the documents to
cs-410_5_6_75,cs-410,5,6,"00:05:42,960","00:05:47,930",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,And so the system must make
cs-410_5_6_76,cs-410,5,6,"00:05:47,930","00:05:51,830",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,to decide whether the item is
cs-410_5_6_77,cs-410,5,6,"00:05:51,830","00:05:55,520",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,"So in other words, we're trying"
cs-410_5_6_78,cs-410,5,6,"00:05:56,800","00:05:57,899",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"So in this case,"
cs-410_5_6_79,cs-410,5,6,"00:05:57,899","00:06:03,600",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,one common user strategy is to use
cs-410_5_6_80,cs-410,5,6,"00:06:03,600","00:06:06,560",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,"So here, I show linear utility function."
cs-410_5_6_81,cs-410,5,6,"00:06:06,560","00:06:11,550",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,That's defined as for example three
cs-410_5_6_82,cs-410,5,6,"00:06:11,550","00:06:17,490",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,"you delivered, minus two multiplied by the"
cs-410_5_6_83,cs-410,5,6,"00:06:17,490","00:06:20,775",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,"So in other words, we could kind of just"
cs-410_5_6_84,cs-410,5,6,"00:06:22,245","00:06:26,215",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,treat this as almost in a gambling game.
cs-410_5_6_85,cs-410,5,6,"00:06:26,215","00:06:32,767",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"If you delete one good item,"
cs-410_5_6_86,cs-410,5,6,"00:06:32,767","00:06:37,660",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,you gain three dollars but if you deliver
cs-410_5_6_87,cs-410,5,6,"00:06:37,660","00:06:41,120",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,And this utility function
cs-410_5_6_88,cs-410,5,6,"00:06:41,120","00:06:45,375",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,how much money you are get by
cs-410_5_6_89,cs-410,5,6,"00:06:45,375","00:06:52,420",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,And so it's clear that if you want
cs-410_5_6_90,cs-410,5,6,"00:06:52,420","00:06:57,760",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,this strategy should be delivered
cs-410_5_6_91,cs-410,5,6,"00:06:57,760","00:07:01,160",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,and minimize the delivery of bad articles.
cs-410_5_6_92,cs-410,5,6,"00:07:01,160","00:07:02,370",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,"That's obvious, right?"
cs-410_5_6_93,cs-410,5,6,"00:07:03,570","00:07:08,130",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,Now one interesting question here is
cs-410_5_6_94,cs-410,5,6,"00:07:08,130","00:07:14,140",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,I just showed a three and
cs-410_5_6_95,cs-410,5,6,"00:07:14,140","00:07:16,950",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,"But one can ask the question,"
cs-410_5_6_96,cs-410,5,6,"00:07:17,990","00:07:19,030",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,So what do you think?
cs-410_5_6_97,cs-410,5,6,"00:07:21,080","00:07:23,450",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,Do you think that's a reasonable choice?
cs-410_5_6_98,cs-410,5,6,"00:07:23,450","00:07:24,510",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,What about the other choices?
cs-410_5_6_99,cs-410,5,6,"00:07:26,220","00:07:33,058",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"So for example, we can have 10 and"
cs-410_5_6_100,cs-410,5,6,"00:07:33,058","00:07:34,750",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,What's the difference?
cs-410_5_6_101,cs-410,5,6,"00:07:34,750","00:07:35,330",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,What do you think?
cs-410_5_6_102,cs-410,5,6,"00:07:36,920","00:07:41,820",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,How would this utility function affect
cs-410_5_6_103,cs-410,5,6,"00:07:43,600","00:07:45,589",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,"Right, you can think of"
cs-410_5_6_104,cs-410,5,6,"00:07:45,589","00:07:51,284",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"(10, -1) + (1, -10), which one do"
cs-410_5_6_105,cs-410,5,6,"00:07:51,284","00:07:57,760",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,system to over do it and which one would
cs-410_5_6_106,cs-410,5,6,"00:07:57,760","00:08:03,380",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,If you think about it you will see that
cs-410_5_6_107,cs-410,5,6,"00:08:03,380","00:08:08,410",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,our good document you incur only a small
cs-410_5_6_108,cs-410,5,6,"00:08:08,410","00:08:11,740",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"Intuitively, you would be"
cs-410_5_6_109,cs-410,5,6,"00:08:11,740","00:08:16,370",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,And you can try to deliver more in
cs-410_5_6_110,cs-410,5,6,"00:08:16,370","00:08:17,870",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,And then we'll get a big reward.
cs-410_5_6_111,cs-410,5,6,"00:08:19,600","00:08:23,364",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,"So on the other hand,"
cs-410_5_6_112,cs-410,5,6,"00:08:23,364","00:08:28,228",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,you really don't get such a big prize
cs-410_5_6_113,cs-410,5,6,"00:08:28,228","00:08:31,250",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,"On the other hand, you will have"
cs-410_5_6_114,cs-410,5,6,"00:08:31,250","00:08:32,710",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"You can imagine that,"
cs-410_5_6_115,cs-410,5,6,"00:08:32,710","00:08:36,590",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,the system would be very reluctant
cs-410_5_6_116,cs-410,5,6,"00:08:36,590","00:08:41,198",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,It has to be absolutely
cs-410_5_6_117,cs-410,5,6,"00:08:41,198","00:08:45,990",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,So this utility function has to be
cs-410_5_6_118,cs-410,5,6,"00:08:45,990","00:08:49,660",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,The three basic problems in content-based
cs-410_5_6_119,cs-410,5,6,"00:08:49,660","00:08:53,620",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,"first, it has to make"
cs-410_5_6_120,cs-410,5,6,"00:08:53,620","00:08:58,200",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,"So it has to be a binary decision maker,"
cs-410_5_6_121,cs-410,5,6,"00:08:58,200","00:09:03,620",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,Given a text document and
cs-410_5_6_122,cs-410,5,6,"00:09:03,620","00:09:07,040",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,"it has to say yes or no, whether this"
cs-410_5_6_123,cs-410,5,6,"00:09:08,050","00:09:12,375",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"So that's a decision module, and"
cs-410_5_6_124,cs-410,5,6,"00:09:12,375","00:09:17,220",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,module as you have seen earlier and
cs-410_5_6_125,cs-410,5,6,"00:09:17,220","00:09:22,050",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,And we have to initialize the system
cs-410_5_6_126,cs-410,5,6,"00:09:22,050","00:09:25,250",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=562,text exclusion or
cs-410_5_6_127,cs-410,5,6,"00:09:26,710","00:09:30,375",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,And the third model is
cs-410_5_6_128,cs-410,5,6,"00:09:30,375","00:09:35,445",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,has to be able to learn from limited
cs-410_5_6_129,cs-410,5,6,"00:09:35,445","00:09:41,100",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,counted them from the user about their
cs-410_5_6_130,cs-410,5,6,"00:09:41,100","00:09:45,702",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,If we don't deliver document
cs-410_5_6_131,cs-410,5,6,"00:09:45,702","00:09:48,900",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,be able to know whether
cs-410_5_6_132,cs-410,5,6,"00:09:50,460","00:09:55,130",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,And we had accumulate a lot of documents
cs-410_5_6_133,cs-410,5,6,"00:09:56,220","00:10:01,470",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,All these modules will have to be
cs-410_5_6_134,cs-410,5,6,"00:10:01,470","00:10:03,050",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,So how can we deal with such a system?
cs-410_5_6_135,cs-410,5,6,"00:10:03,050","00:10:05,260",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,And there are many different approaches.
cs-410_5_6_136,cs-410,5,6,"00:10:05,260","00:10:09,600",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,Here we're going to talk about
cs-410_5_6_137,cs-410,5,6,"00:10:09,600","00:10:12,120",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,a search engine for information filtering.
cs-410_5_6_138,cs-410,5,6,"00:10:12,120","00:10:15,880",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,"Again, here's why we've spent a lot of"
cs-410_5_6_139,cs-410,5,6,"00:10:15,880","00:10:20,830",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,Because it's actually not very hard
cs-410_5_6_140,cs-410,5,6,"00:10:20,830","00:10:22,320",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,information filtering.
cs-410_5_6_141,cs-410,5,6,"00:10:22,320","00:10:26,410",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,So here's the basic idea for
cs-410_5_6_142,cs-410,5,6,"00:10:26,410","00:10:27,960",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,information filtering.
cs-410_5_6_143,cs-410,5,6,"00:10:27,960","00:10:31,180",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,"First, we can reuse a lot of"
cs-410_5_6_144,cs-410,5,6,"00:10:31,180","00:10:34,950",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,"Right, so we know how to score"
cs-410_5_6_145,cs-410,5,6,"00:10:34,950","00:10:39,620",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,We're going to match the similarity
cs-410_5_6_146,cs-410,5,6,"00:10:39,620","00:10:40,930",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,a document.
cs-410_5_6_147,cs-410,5,6,"00:10:40,930","00:10:44,320",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,And then we can use a score threshold for
cs-410_5_6_148,cs-410,5,6,"00:10:44,320","00:10:49,290",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,We do retrieval and then we kind of find
cs-410_5_6_149,cs-410,5,6,"00:10:49,290","00:10:56,890",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,apply a threshold to see whether the
cs-410_5_6_150,cs-410,5,6,"00:10:56,890","00:10:58,230",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,"And if it's passing the threshold,"
cs-410_5_6_151,cs-410,5,6,"00:10:58,230","00:11:02,900",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,we're going to say it's relevant and
cs-410_5_6_152,cs-410,5,6,"00:11:02,900","00:11:08,310",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,"Another component that we have to add is,"
cs-410_5_6_153,cs-410,5,6,"00:11:08,310","00:11:13,080",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,we had used is the traditional feedback
cs-410_5_6_154,cs-410,5,6,"00:11:13,080","00:11:18,632",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,And we know rock hill can be using for
cs-410_5_6_155,cs-410,5,6,"00:11:18,632","00:11:25,008",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,"And, but we have to develop a new"
cs-410_5_6_156,cs-410,5,6,"00:11:25,008","00:11:27,279",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,And we need to set it initially and
cs-410_5_6_157,cs-410,5,6,"00:11:27,279","00:11:32,170",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,then we have to learn how to
cs-410_5_6_158,cs-410,5,6,"00:11:32,170","00:11:37,276",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,So here's what the system
cs-410_5_6_159,cs-410,5,6,"00:11:37,276","00:11:45,040",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=697,generalize the vector-space model for
cs-410_5_6_160,cs-410,5,6,"00:11:45,040","00:11:49,348",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,So you can see the document vector could
cs-410_5_6_161,cs-410,5,6,"00:11:49,348","00:11:53,820",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,already exists in a search engine
cs-410_5_6_162,cs-410,5,6,"00:11:53,820","00:11:58,630",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,And the profile will be treated
cs-410_5_6_163,cs-410,5,6,"00:11:58,630","00:12:02,100",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,the profile vector can be matched with
cs-410_5_6_164,cs-410,5,6,"00:12:03,130","00:12:06,960",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,And then this score would be fed into a
cs-410_5_6_165,cs-410,5,6,"00:12:06,960","00:12:13,690",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=726,"no, and then the evaluation would be based"
cs-410_5_6_166,cs-410,5,6,"00:12:13,690","00:12:16,870",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=733,If it says yes and then the document
cs-410_5_6_167,cs-410,5,6,"00:12:16,870","00:12:19,660",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,And then user could give some feedback.
cs-410_5_6_168,cs-410,5,6,"00:12:19,660","00:12:25,530",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=739,The feedback information would be
cs-410_5_6_169,cs-410,5,6,"00:12:25,530","00:12:28,500",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,to adjust the vector representation.
cs-410_5_6_170,cs-410,5,6,"00:12:28,500","00:12:33,150",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,So the vector learning is essentially
cs-410_5_6_171,cs-410,5,6,"00:12:33,150","00:12:36,140",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,feedback in the case of search.
cs-410_5_6_172,cs-410,5,6,"00:12:36,140","00:12:39,480",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,The threshold of learning
cs-410_5_6_173,cs-410,5,6,"00:12:39,480","00:12:42,580",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=759,that we need to talk
cs-410_5_6_174,cs-410,5,6,"00:12:42,580","00:12:52,580",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=762,[MUSIC]
cs-410_5_7_1,cs-410,5,7,"00:00:06,440","00:00:11,320",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,This lecture is about the
cs-410_5_7_2,cs-410,5,7,"00:00:12,020","00:00:14,100",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture, we are going"
cs-410_5_7_3,cs-410,5,7,"00:00:14,100","00:00:16,680",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,to discuss textual
cs-410_5_7_4,cs-410,5,7,"00:00:16,680","00:00:20,475",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,and discuss how natural
cs-410_5_7_5,cs-410,5,7,"00:00:20,475","00:00:24,450",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,allow us to represent text
cs-410_5_7_6,cs-410,5,7,"00:00:24,450","00:00:28,590",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,Let's take a look at this
cs-410_5_7_7,cs-410,5,7,"00:00:28,590","00:00:33,900",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,We can represent this sentence
cs-410_5_7_8,cs-410,5,7,"00:00:33,900","00:00:37,680",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"First, we can always"
cs-410_5_7_9,cs-410,5,7,"00:00:37,680","00:00:42,090",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,represent such a sentence
cs-410_5_7_10,cs-410,5,7,"00:00:42,090","00:00:45,135",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,This is true for
cs-410_5_7_11,cs-410,5,7,"00:00:45,135","00:00:49,210",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,when we store them
cs-410_5_7_12,cs-410,5,7,"00:00:49,310","00:00:53,480",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,When we store a natural
cs-410_5_7_13,cs-410,5,7,"00:00:53,480","00:00:55,805",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,"as a string of characters,"
cs-410_5_7_14,cs-410,5,7,"00:00:55,805","00:01:00,350",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,we have perhaps the most general
cs-410_5_7_15,cs-410,5,7,"00:01:00,350","00:01:02,270",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,since we always use
cs-410_5_7_16,cs-410,5,7,"00:01:02,270","00:01:05,360",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,this approach to
cs-410_5_7_17,cs-410,5,7,"00:01:05,360","00:01:10,070",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,"But unfortunately, using"
cs-410_5_7_18,cs-410,5,7,"00:01:10,070","00:01:12,950",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"help us to do semantic analysis,"
cs-410_5_7_19,cs-410,5,7,"00:01:12,950","00:01:14,135",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,which is often needed
cs-410_5_7_20,cs-410,5,7,"00:01:14,135","00:01:17,600",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,for many applications
cs-410_5_7_21,cs-410,5,7,"00:01:17,600","00:01:21,650",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,The reason is because we're
cs-410_5_7_22,cs-410,5,7,"00:01:21,650","00:01:22,880",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,"So as a string,"
cs-410_5_7_23,cs-410,5,7,"00:01:22,880","00:01:25,100",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,we're going to keep
cs-410_5_7_24,cs-410,5,7,"00:01:25,100","00:01:29,005",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,and these ASCII symbols.
cs-410_5_7_25,cs-410,5,7,"00:01:29,005","00:01:32,090",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,We can perhaps count what's
cs-410_5_7_26,cs-410,5,7,"00:01:32,090","00:01:35,225",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,the most frequent character
cs-410_5_7_27,cs-410,5,7,"00:01:35,225","00:01:38,960",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,or the correlation
cs-410_5_7_28,cs-410,5,7,"00:01:38,960","00:01:43,075",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,but we can't really
cs-410_5_7_29,cs-410,5,7,"00:01:43,075","00:01:47,300",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"Yet, this is the most"
cs-410_5_7_30,cs-410,5,7,"00:01:47,300","00:01:49,250",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,text because we can use
cs-410_5_7_31,cs-410,5,7,"00:01:49,250","00:01:53,045",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,this to represent any
cs-410_5_7_32,cs-410,5,7,"00:01:53,045","00:01:55,220",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,If we try to do
cs-410_5_7_33,cs-410,5,7,"00:01:55,220","00:01:57,635",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,a little bit more natural
cs-410_5_7_34,cs-410,5,7,"00:01:57,635","00:02:00,540",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,"by doing word segmentation,"
cs-410_5_7_35,cs-410,5,7,"00:02:00,540","00:02:05,210",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,then we can obtain a
cs-410_5_7_36,cs-410,5,7,"00:02:05,210","00:02:08,315",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,but in the form of a
cs-410_5_7_37,cs-410,5,7,"00:02:08,315","00:02:11,750",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,So here we see that
cs-410_5_7_38,cs-410,5,7,"00:02:11,750","00:02:17,100",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,words like a dog is chasing etc.
cs-410_5_7_39,cs-410,5,7,"00:02:17,230","00:02:20,600",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,Now with this level
cs-410_5_7_40,cs-410,5,7,"00:02:20,600","00:02:23,965",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,we certainly can do
cs-410_5_7_41,cs-410,5,7,"00:02:23,965","00:02:27,065",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,and this is mainly because
cs-410_5_7_42,cs-410,5,7,"00:02:27,065","00:02:30,275",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,of human communication
cs-410_5_7_43,cs-410,5,7,"00:02:30,275","00:02:33,035",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,so they are very powerful.
cs-410_5_7_44,cs-410,5,7,"00:02:33,035","00:02:36,080",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,"By identifying words, we can for"
cs-410_5_7_45,cs-410,5,7,"00:02:36,080","00:02:38,780",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,example easily count what are
cs-410_5_7_46,cs-410,5,7,"00:02:38,780","00:02:40,820",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,the most frequent words in
cs-410_5_7_47,cs-410,5,7,"00:02:40,820","00:02:45,185",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,this document or in
cs-410_5_7_48,cs-410,5,7,"00:02:45,185","00:02:48,200",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,These words can be used to form
cs-410_5_7_49,cs-410,5,7,"00:02:48,200","00:02:51,940",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,topics when we combine
cs-410_5_7_50,cs-410,5,7,"00:02:51,940","00:02:54,060",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,"and some words are positive,"
cs-410_5_7_51,cs-410,5,7,"00:02:54,060","00:02:55,700",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,"some words negative, so we can"
cs-410_5_7_52,cs-410,5,7,"00:02:55,700","00:02:58,410",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,also do sentiment analysis.
cs-410_5_7_53,cs-410,5,7,"00:02:58,660","00:03:02,690",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,So representing text data
cs-410_5_7_54,cs-410,5,7,"00:03:02,690","00:03:06,775",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,opens up a lot of interesting
cs-410_5_7_55,cs-410,5,7,"00:03:06,775","00:03:09,060",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"However, this level of"
cs-410_5_7_56,cs-410,5,7,"00:03:09,060","00:03:12,080",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,representation is slightly
cs-410_5_7_57,cs-410,5,7,"00:03:12,080","00:03:17,300",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,of characters because in
cs-410_5_7_58,cs-410,5,7,"00:03:17,300","00:03:21,350",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,it's actually not
cs-410_5_7_59,cs-410,5,7,"00:03:21,350","00:03:25,010",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,all the word boundaries
cs-410_5_7_60,cs-410,5,7,"00:03:25,010","00:03:27,685",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,you see text as a sequence of
cs-410_5_7_61,cs-410,5,7,"00:03:27,685","00:03:31,345",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,characters with
cs-410_5_7_62,cs-410,5,7,"00:03:31,345","00:03:33,160",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,So you'll have to rely on
cs-410_5_7_63,cs-410,5,7,"00:03:33,160","00:03:36,925",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,some special techniques
cs-410_5_7_64,cs-410,5,7,"00:03:36,925","00:03:39,940",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,"In such a language,"
cs-410_5_7_65,cs-410,5,7,"00:03:39,940","00:03:43,600",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,we might make mistakes
cs-410_5_7_66,cs-410,5,7,"00:03:43,600","00:03:46,480",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,So the sequence of
cs-410_5_7_67,cs-410,5,7,"00:03:46,480","00:03:50,230",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,not as robust as
cs-410_5_7_68,cs-410,5,7,"00:03:50,230","00:03:53,230",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,"But in English, it's very"
cs-410_5_7_69,cs-410,5,7,"00:03:53,230","00:03:56,005",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,easy to obtain this level
cs-410_5_7_70,cs-410,5,7,"00:03:56,005","00:03:59,270",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,so we can do that all the time.
cs-410_5_7_71,cs-410,5,7,"00:04:00,860","00:04:03,295",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,"Now, if we go further"
cs-410_5_7_72,cs-410,5,7,"00:04:03,295","00:04:04,645",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,to do naturally
cs-410_5_7_73,cs-410,5,7,"00:04:04,645","00:04:08,125",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,we can add a part of speech tags.
cs-410_5_7_74,cs-410,5,7,"00:04:08,125","00:04:09,955",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,"Now once we do that,"
cs-410_5_7_75,cs-410,5,7,"00:04:09,955","00:04:11,850",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,"we can count, for example,"
cs-410_5_7_76,cs-410,5,7,"00:04:11,850","00:04:14,680",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,the most frequent
cs-410_5_7_77,cs-410,5,7,"00:04:14,680","00:04:18,220",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,nouns are associated with
cs-410_5_7_78,cs-410,5,7,"00:04:18,220","00:04:19,480",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,So this opens up
cs-410_5_7_79,cs-410,5,7,"00:04:19,480","00:04:23,020",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,a little bit more
cs-410_5_7_80,cs-410,5,7,"00:04:23,020","00:04:24,625",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,for further analysis.
cs-410_5_7_81,cs-410,5,7,"00:04:24,625","00:04:28,270",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,Note that I use a plus sign
cs-410_5_7_82,cs-410,5,7,"00:04:28,270","00:04:32,305",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,representing text as a sequence
cs-410_5_7_83,cs-410,5,7,"00:04:32,305","00:04:35,395",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,we don't necessarily replace
cs-410_5_7_84,cs-410,5,7,"00:04:35,395","00:04:37,840",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,the original word
cs-410_5_7_85,cs-410,5,7,"00:04:37,840","00:04:40,975",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"Instead, we add this as"
cs-410_5_7_86,cs-410,5,7,"00:04:40,975","00:04:44,050",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,an additional way of
cs-410_5_7_87,cs-410,5,7,"00:04:44,050","00:04:47,230",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,so that now the data is
cs-410_5_7_88,cs-410,5,7,"00:04:47,230","00:04:50,965",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,of words and a sequence
cs-410_5_7_89,cs-410,5,7,"00:04:50,965","00:04:54,055",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,This enriches the
cs-410_5_7_90,cs-410,5,7,"00:04:54,055","00:04:59,160",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,and thus also enables
cs-410_5_7_91,cs-410,5,7,"00:05:00,340","00:05:04,040",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,"If we go further, then we'll"
cs-410_5_7_92,cs-410,5,7,"00:05:04,040","00:05:08,020",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,often to obtain
cs-410_5_7_93,cs-410,5,7,"00:05:08,020","00:05:09,700",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"Now this of course,"
cs-410_5_7_94,cs-410,5,7,"00:05:09,700","00:05:12,230",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,further open up
cs-410_5_7_95,cs-410,5,7,"00:05:12,230","00:05:14,840",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,"of, for example,"
cs-410_5_7_96,cs-410,5,7,"00:05:14,840","00:05:22,520",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,the writing styles or
cs-410_5_7_97,cs-410,5,7,"00:05:22,520","00:05:26,440",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,If we go further for
cs-410_5_7_98,cs-410,5,7,"00:05:26,440","00:05:31,684",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,then we might be able to
cs-410_5_7_99,cs-410,5,7,"00:05:31,684","00:05:35,515",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,and we also can recognize
cs-410_5_7_100,cs-410,5,7,"00:05:35,515","00:05:38,055",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,and playground as a location.
cs-410_5_7_101,cs-410,5,7,"00:05:38,055","00:05:41,335",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,We can further analyze
cs-410_5_7_102,cs-410,5,7,"00:05:41,335","00:05:45,830",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,dog is chasing the boy and
cs-410_5_7_103,cs-410,5,7,"00:05:45,830","00:05:48,875",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,Now this will add
cs-410_5_7_104,cs-410,5,7,"00:05:48,875","00:05:52,945",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,relations through
cs-410_5_7_105,cs-410,5,7,"00:05:52,945","00:05:54,790",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,"At this level,"
cs-410_5_7_106,cs-410,5,7,"00:05:54,790","00:05:57,605",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,then we can do even more
cs-410_5_7_107,cs-410,5,7,"00:05:57,605","00:05:59,795",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,"For example, now we"
cs-410_5_7_108,cs-410,5,7,"00:05:59,795","00:06:02,360",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,the most frequent person that's
cs-410_5_7_109,cs-410,5,7,"00:06:02,360","00:06:06,284",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,mentioning this whole collection
cs-410_5_7_110,cs-410,5,7,"00:06:06,284","00:06:09,205",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,or whenever you
cs-410_5_7_111,cs-410,5,7,"00:06:09,205","00:06:13,655",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,you also tend to see mentioning
cs-410_5_7_112,cs-410,5,7,"00:06:13,655","00:06:19,480",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,So this is a very
cs-410_5_7_113,cs-410,5,7,"00:06:19,480","00:06:21,830",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,and it's also related to
cs-410_5_7_114,cs-410,5,7,"00:06:21,830","00:06:24,500",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,the knowledge graph that
cs-410_5_7_115,cs-410,5,7,"00:06:24,500","00:06:27,620",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,of that Google is doing as
cs-410_5_7_116,cs-410,5,7,"00:06:27,620","00:06:31,690",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,a more semantic way of
cs-410_5_7_117,cs-410,5,7,"00:06:31,690","00:06:39,080",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,"However, it's also less robust"
cs-410_5_7_118,cs-410,5,7,"00:06:39,080","00:06:42,410",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,even syntactical analysis
cs-410_5_7_119,cs-410,5,7,"00:06:42,410","00:06:43,985",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,always easy to identify
cs-410_5_7_120,cs-410,5,7,"00:06:43,985","00:06:46,160",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,all the entities with
cs-410_5_7_121,cs-410,5,7,"00:06:46,160","00:06:47,735",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,"and we might make mistakes,"
cs-410_5_7_122,cs-410,5,7,"00:06:47,735","00:06:50,270",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,and relations are
cs-410_5_7_123,cs-410,5,7,"00:06:50,270","00:06:52,685",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,and we might make mistakes.
cs-410_5_7_124,cs-410,5,7,"00:06:52,685","00:06:56,120",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,So this makes this level of
cs-410_5_7_125,cs-410,5,7,"00:06:56,120","00:06:58,190",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,yet it's very useful.
cs-410_5_7_126,cs-410,5,7,"00:06:58,190","00:07:01,700",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,Now if we move further
cs-410_5_7_127,cs-410,5,7,"00:07:01,700","00:07:05,465",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,then we can have predicates
cs-410_5_7_128,cs-410,5,7,"00:07:05,465","00:07:08,630",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"With inference rules, we can"
cs-410_5_7_129,cs-410,5,7,"00:07:08,630","00:07:13,700",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,infer interesting derived
cs-410_5_7_130,cs-410,5,7,"00:07:13,700","00:07:15,020",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,so that's very useful.
cs-410_5_7_131,cs-410,5,7,"00:07:15,020","00:07:17,420",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"But unfortunately,"
cs-410_5_7_132,cs-410,5,7,"00:07:17,420","00:07:19,940",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,representation is even less
cs-410_5_7_133,cs-410,5,7,"00:07:19,940","00:07:22,010",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,robust and we can make
cs-410_5_7_134,cs-410,5,7,"00:07:22,010","00:07:25,120",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,mistakes and we can't do
cs-410_5_7_135,cs-410,5,7,"00:07:25,120","00:07:28,885",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,that all the time for
cs-410_5_7_136,cs-410,5,7,"00:07:28,885","00:07:33,820",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,"Finally, speech acts would"
cs-410_5_7_137,cs-410,5,7,"00:07:33,820","00:07:38,605",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,of repetition of the intent
cs-410_5_7_138,cs-410,5,7,"00:07:38,605","00:07:40,000",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,"So in this case,"
cs-410_5_7_139,cs-410,5,7,"00:07:40,000","00:07:41,485",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,it might be a request.
cs-410_5_7_140,cs-410,5,7,"00:07:41,485","00:07:44,650",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,So knowing that would
cs-410_5_7_141,cs-410,5,7,"00:07:44,650","00:07:47,140",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,even more interesting
cs-410_5_7_142,cs-410,5,7,"00:07:47,140","00:07:51,765",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,this observer or the author
cs-410_5_7_143,cs-410,5,7,"00:07:51,765","00:07:53,895",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,What's the intention
cs-410_5_7_144,cs-410,5,7,"00:07:53,895","00:07:57,210",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,What's scenarios? What kind
cs-410_5_7_145,cs-410,5,7,"00:07:57,210","00:08:02,740",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,So this is another level
cs-410_5_7_146,cs-410,5,7,"00:08:02,740","00:08:05,755",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,of analysis that would
cs-410_5_7_147,cs-410,5,7,"00:08:05,755","00:08:10,250",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,So this picture shows
cs-410_5_7_148,cs-410,5,7,"00:08:10,250","00:08:12,530",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,we generally see
cs-410_5_7_149,cs-410,5,7,"00:08:12,530","00:08:15,535",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=492,natural language processing
cs-410_5_7_150,cs-410,5,7,"00:08:15,535","00:08:18,080",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,"Unfortunately,"
cs-410_5_7_151,cs-410,5,7,"00:08:18,080","00:08:20,330",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,"require more human effort,"
cs-410_5_7_152,cs-410,5,7,"00:08:20,330","00:08:23,060",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,and they are less accurate.
cs-410_5_7_153,cs-410,5,7,"00:08:23,060","00:08:26,570",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,That means there are mistakes.
cs-410_5_7_154,cs-410,5,7,"00:08:26,570","00:08:29,945",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,So if we add an texts that are at
cs-410_5_7_155,cs-410,5,7,"00:08:29,945","00:08:32,240",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,the levels that are
cs-410_5_7_156,cs-410,5,7,"00:08:32,240","00:08:34,970",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,representing deeper
cs-410_5_7_157,cs-410,5,7,"00:08:34,970","00:08:37,790",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,then we have to
cs-410_5_7_158,cs-410,5,7,"00:08:37,790","00:08:42,380",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=517,So that also means it's
cs-410_5_7_159,cs-410,5,7,"00:08:42,380","00:08:46,835",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,such deep analysis with
cs-410_5_7_160,cs-410,5,7,"00:08:46,835","00:08:48,695",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,"for example, sequence of words."
cs-410_5_7_161,cs-410,5,7,"00:08:48,695","00:08:50,675",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"On the right side,"
cs-410_5_7_162,cs-410,5,7,"00:08:50,675","00:08:55,240",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,you'll see the arrow points
cs-410_5_7_163,cs-410,5,7,"00:08:55,240","00:08:56,960",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,"As we go down,"
cs-410_5_7_164,cs-410,5,7,"00:08:56,960","00:08:59,780",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,we are representation
cs-410_5_7_165,cs-410,5,7,"00:08:59,780","00:09:02,600",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,to knowledge representation
cs-410_5_7_166,cs-410,5,7,"00:09:02,600","00:09:08,210",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,and need for solving
cs-410_5_7_167,cs-410,5,7,"00:09:08,210","00:09:11,750",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,Now this is desirable because as
cs-410_5_7_168,cs-410,5,7,"00:09:11,750","00:09:15,110",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,we can represent text at
cs-410_5_7_169,cs-410,5,7,"00:09:15,110","00:09:17,315",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,we can easily extract
cs-410_5_7_170,cs-410,5,7,"00:09:17,315","00:09:19,280",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,That's the purpose
cs-410_5_7_171,cs-410,5,7,"00:09:19,280","00:09:21,965",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=559,So there is a trade-off
cs-410_5_7_172,cs-410,5,7,"00:09:21,965","00:09:24,920",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,here between doing
cs-410_5_7_173,cs-410,5,7,"00:09:24,920","00:09:27,260",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,might have errors but would give
cs-410_5_7_174,cs-410,5,7,"00:09:27,260","00:09:31,225",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,us direct knowledge that
cs-410_5_7_175,cs-410,5,7,"00:09:31,225","00:09:33,910",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"Doing shallow analysis, which"
cs-410_5_7_176,cs-410,5,7,"00:09:33,910","00:09:37,010",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,is more robust but
cs-410_5_7_177,cs-410,5,7,"00:09:37,010","00:09:42,665",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,give us the necessary deeper
cs-410_5_7_178,cs-410,5,7,"00:09:42,665","00:09:45,740",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,I should also say that
cs-410_5_7_179,cs-410,5,7,"00:09:45,740","00:09:49,085",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,humans and are meant to
cs-410_5_7_180,cs-410,5,7,"00:09:49,085","00:09:52,340",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,"So as a result, in"
cs-410_5_7_181,cs-410,5,7,"00:09:52,340","00:09:56,090",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=592,text-mining humans play
cs-410_5_7_182,cs-410,5,7,"00:09:56,090","00:09:58,010",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,they are always in the loop.
cs-410_5_7_183,cs-410,5,7,"00:09:58,010","00:10:00,650",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,Meaning that we should optimize
cs-410_5_7_184,cs-410,5,7,"00:10:00,650","00:10:03,695",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,the collaboration of
cs-410_5_7_185,cs-410,5,7,"00:10:03,695","00:10:05,540",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=603,"So in that sense,"
cs-410_5_7_186,cs-410,5,7,"00:10:05,540","00:10:08,480",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,it's okay that computers
cs-410_5_7_187,cs-410,5,7,"00:10:08,480","00:10:12,920",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,to have compute accurately
cs-410_5_7_188,cs-410,5,7,"00:10:12,920","00:10:15,290",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,and the patterns
cs-410_5_7_189,cs-410,5,7,"00:10:15,290","00:10:18,035",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,from text data can be
cs-410_5_7_190,cs-410,5,7,"00:10:18,035","00:10:20,840",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,and humans can
cs-410_5_7_191,cs-410,5,7,"00:10:20,840","00:10:24,650",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,to do more accurate analysis
cs-410_5_7_192,cs-410,5,7,"00:10:24,650","00:10:28,640",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,by providing features
cs-410_5_7_193,cs-410,5,7,"00:10:28,640","00:10:33,870",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=628,learning programs to make
cs-410_5_8_1,cs-410,5,8,"00:00:00,025","00:00:06,885",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_5_8_2,cs-410,5,8,"00:00:06,885","00:00:11,190",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,lecture is about topic mining and
cs-410_5_8_3,cs-410,5,8,"00:00:11,190","00:00:14,270",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,We're going to talk about its
cs-410_5_8_4,cs-410,5,8,"00:00:17,780","00:00:22,630",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,In this lecture we're going to talk
cs-410_5_8_5,cs-410,5,8,"00:00:23,770","00:00:28,310",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,"As you see on this road map,"
cs-410_5_8_6,cs-410,5,8,"00:00:28,310","00:00:33,190",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,"mining knowledge about language,"
cs-410_5_8_7,cs-410,5,8,"00:00:33,190","00:00:37,987",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,word associations such as paradigmatic and
cs-410_5_8_8,cs-410,5,8,"00:00:39,190","00:00:43,100",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"Now, starting from this lecture, we're"
cs-410_5_8_9,cs-410,5,8,"00:00:43,100","00:00:47,570",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,"knowledge, which is content mining, and"
cs-410_5_8_10,cs-410,5,8,"00:00:47,570","00:00:55,031",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,trying to discover knowledge about
cs-410_5_8_11,cs-410,5,8,"00:00:56,140","00:00:58,810",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,And we call that topic mining and
cs-410_5_8_12,cs-410,5,8,"00:00:59,920","00:01:04,350",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,"In this lecture, we're going to talk about"
cs-410_5_8_13,cs-410,5,8,"00:01:04,350","00:01:08,260",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,"So first of all,"
cs-410_5_8_14,cs-410,5,8,"00:01:08,260","00:01:12,600",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,So topic is something that we
cs-410_5_8_15,cs-410,5,8,"00:01:12,600","00:01:15,840",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,it's actually not that
cs-410_5_8_16,cs-410,5,8,"00:01:15,840","00:01:20,420",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,"Roughly speaking, topic is the main"
cs-410_5_8_17,cs-410,5,8,"00:01:20,420","00:01:25,860",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,And you can think of this as a theme or
cs-410_5_8_18,cs-410,5,8,"00:01:25,860","00:01:28,420",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,It can also have different granularities.
cs-410_5_8_19,cs-410,5,8,"00:01:28,420","00:01:31,240",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"For example,"
cs-410_5_8_20,cs-410,5,8,"00:01:31,240","00:01:34,800",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"A topic of article,"
cs-410_5_8_21,cs-410,5,8,"00:01:34,800","00:01:40,540",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,the topic of all the research articles
cs-410_5_8_22,cs-410,5,8,"00:01:40,540","00:01:45,629",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,so different grand narratives of topics
cs-410_5_8_23,cs-410,5,8,"00:01:46,760","00:01:51,628",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"Indeed, there are many applications that"
cs-410_5_8_24,cs-410,5,8,"00:01:51,628","00:01:52,980",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,they're analyzed then.
cs-410_5_8_25,cs-410,5,8,"00:01:52,980","00:01:54,300",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,Here are some examples.
cs-410_5_8_26,cs-410,5,8,"00:01:54,300","00:01:58,280",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,"For example, we might be interested"
cs-410_5_8_27,cs-410,5,8,"00:01:58,280","00:02:00,470",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,users are talking about today?
cs-410_5_8_28,cs-410,5,8,"00:02:00,470","00:02:03,600",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"Are they talking about NBA sports, or"
cs-410_5_8_29,cs-410,5,8,"00:02:03,600","00:02:08,540",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,are they talking about some
cs-410_5_8_30,cs-410,5,8,"00:02:08,540","00:02:12,970",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,Or we are interested in
cs-410_5_8_31,cs-410,5,8,"00:02:12,970","00:02:17,090",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,"For example, one might be interested in"
cs-410_5_8_32,cs-410,5,8,"00:02:17,090","00:02:21,840",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,"topics in data mining, and how are they"
cs-410_5_8_33,cs-410,5,8,"00:02:21,840","00:02:26,820",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,Now this involves discovery of topics
cs-410_5_8_34,cs-410,5,8,"00:02:26,820","00:02:32,910",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,also we want to discover topics in
cs-410_5_8_35,cs-410,5,8,"00:02:32,910","00:02:34,690",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,And then we can make a comparison.
cs-410_5_8_36,cs-410,5,8,"00:02:34,690","00:02:38,400",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,We might also be also interested in
cs-410_5_8_37,cs-410,5,8,"00:02:38,400","00:02:43,710",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,"some products like the iPhone 6,"
cs-410_5_8_38,cs-410,5,8,"00:02:43,710","00:02:48,360",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,And this involves discovering
cs-410_5_8_39,cs-410,5,8,"00:02:48,360","00:02:52,470",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,iPhone 6 and
cs-410_5_8_40,cs-410,5,8,"00:02:52,470","00:02:56,810",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,Or perhaps we're interested in knowing
cs-410_5_8_41,cs-410,5,8,"00:02:56,810","00:02:58,110",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,presidential election?
cs-410_5_8_42,cs-410,5,8,"00:02:59,780","00:03:04,800",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,And all these have to do with discovering
cs-410_5_8_43,cs-410,5,8,"00:03:04,800","00:03:08,680",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,and we're going to talk about a lot
cs-410_5_8_44,cs-410,5,8,"00:03:08,680","00:03:12,920",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,In general we can view a topic as
cs-410_5_8_45,cs-410,5,8,"00:03:12,920","00:03:17,830",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,So from text data we expect to
cs-410_5_8_46,cs-410,5,8,"00:03:17,830","00:03:22,650",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,then these topics generally provide
cs-410_5_8_47,cs-410,5,8,"00:03:22,650","00:03:25,690",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,And it tells us something about the world.
cs-410_5_8_48,cs-410,5,8,"00:03:25,690","00:03:28,230",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,"About a product, about a person etc."
cs-410_5_8_49,cs-410,5,8,"00:03:29,350","00:03:32,390",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,"Now when we have some non-text data,"
cs-410_5_8_50,cs-410,5,8,"00:03:32,390","00:03:36,420",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,then we can have more context for
cs-410_5_8_51,cs-410,5,8,"00:03:36,420","00:03:41,620",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,"For example, we might know the time"
cs-410_5_8_52,cs-410,5,8,"00:03:41,620","00:03:47,110",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,locations where the text
cs-410_5_8_53,cs-410,5,8,"00:03:47,110","00:03:52,945",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,"or the authors of the text, or"
cs-410_5_8_54,cs-410,5,8,"00:03:52,945","00:03:54,400",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"All such meta data, or"
cs-410_5_8_55,cs-410,5,8,"00:03:54,400","00:03:59,610",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,context variables can be associated
cs-410_5_8_56,cs-410,5,8,"00:03:59,610","00:04:05,340",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,then we can use these context variables
cs-410_5_8_57,cs-410,5,8,"00:04:05,340","00:04:09,320",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"For example, looking at topics over time,"
cs-410_5_8_58,cs-410,5,8,"00:04:09,320","00:04:14,290",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,"whether there's a trending topic, or"
cs-410_5_8_59,cs-410,5,8,"00:04:15,620","00:04:18,950",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,Soon you are looking at topics
cs-410_5_8_60,cs-410,5,8,"00:04:18,950","00:04:24,185",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,We might know some insights about
cs-410_5_8_61,cs-410,5,8,"00:04:26,150","00:04:29,900",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,So that's why mining
cs-410_5_8_62,cs-410,5,8,"00:04:29,900","00:04:34,540",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,"Now, let's look at the tasks"
cs-410_5_8_63,cs-410,5,8,"00:04:34,540","00:04:39,380",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,"In general, it would involve first"
cs-410_5_8_64,cs-410,5,8,"00:04:39,380","00:04:40,810",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,k topics.
cs-410_5_8_65,cs-410,5,8,"00:04:40,810","00:04:45,430",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"And then we also would like to know, which"
cs-410_5_8_66,cs-410,5,8,"00:04:45,430","00:04:46,600",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,to what extent.
cs-410_5_8_67,cs-410,5,8,"00:04:46,600","00:04:52,970",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,"So for example, in document one, we"
cs-410_5_8_68,cs-410,5,8,"00:04:52,970","00:04:57,390",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,Topic 2 and
cs-410_5_8_69,cs-410,5,8,"00:04:58,890","00:05:00,712",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"And other topics,"
cs-410_5_8_70,cs-410,5,8,"00:05:00,712","00:05:06,778",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,"Document two, on the other hand,"
cs-410_5_8_71,cs-410,5,8,"00:05:06,778","00:05:10,553",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,"but it did not cover Topic 1 at all, and"
cs-410_5_8_72,cs-410,5,8,"00:05:10,553","00:05:15,873",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"it also covers Topic k to some extent,"
cs-410_5_8_73,cs-410,5,8,"00:05:15,873","00:05:19,995",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,So now you can see there
cs-410_5_8_74,cs-410,5,8,"00:05:19,995","00:05:25,760",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,"sub-tasks, the first is to discover k"
cs-410_5_8_75,cs-410,5,8,"00:05:25,760","00:05:27,140",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,What are these k topics?
cs-410_5_8_76,cs-410,5,8,"00:05:27,140","00:05:28,920",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,"Okay, major topics in the text they are."
cs-410_5_8_77,cs-410,5,8,"00:05:28,920","00:05:33,180",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,The second task is to figure out
cs-410_5_8_78,cs-410,5,8,"00:05:33,180","00:05:34,430",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,to what extent.
cs-410_5_8_79,cs-410,5,8,"00:05:34,430","00:05:37,810",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,"So more formally,"
cs-410_5_8_80,cs-410,5,8,"00:05:37,810","00:05:42,365",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"First, we have, as input,"
cs-410_5_8_81,cs-410,5,8,"00:05:42,365","00:05:47,050",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,Here we can denote the text
cs-410_5_8_82,cs-410,5,8,"00:05:47,050","00:05:51,740",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,denote text article as d i.
cs-410_5_8_83,cs-410,5,8,"00:05:51,740","00:05:56,700",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,"And, we generally also need to have"
cs-410_5_8_84,cs-410,5,8,"00:05:56,700","00:06:01,730",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,But there may be techniques that can
cs-410_5_8_85,cs-410,5,8,"00:06:01,730","00:06:06,735",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,But in the techniques that we will
cs-410_5_8_86,cs-410,5,8,"00:06:06,735","00:06:12,340",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,"techniques, we often need to"
cs-410_5_8_87,cs-410,5,8,"00:06:14,580","00:06:19,860",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,Now the output would then be the k
cs-410_5_8_88,cs-410,5,8,"00:06:19,860","00:06:23,210",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,in order as theta sub
cs-410_5_8_89,cs-410,5,8,"00:06:24,540","00:06:29,820",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,Also we want to generate the coverage of
cs-410_5_8_90,cs-410,5,8,"00:06:29,820","00:06:32,518",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=389,this is denoted by pi sub i j.
cs-410_5_8_91,cs-410,5,8,"00:06:33,562","00:06:38,073",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,And pi sub ij is the probability
cs-410_5_8_92,cs-410,5,8,"00:06:38,073","00:06:41,290",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,covering topic theta sub j.
cs-410_5_8_93,cs-410,5,8,"00:06:41,290","00:06:45,450",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,"So obviously for each document, we have"
cs-410_5_8_94,cs-410,5,8,"00:06:45,450","00:06:47,830",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,"what extent the document covers,"
cs-410_5_8_95,cs-410,5,8,"00:06:48,930","00:06:53,610",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,And we can assume that these
cs-410_5_8_96,cs-410,5,8,"00:06:53,610","00:06:57,000",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,Because a document won't be able to cover
cs-410_5_8_97,cs-410,5,8,"00:06:57,000","00:07:02,520",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,other topics outside of the topics
cs-410_5_8_98,cs-410,5,8,"00:07:02,520","00:07:08,170",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=422,"So now, the question is, how do we define"
cs-410_5_8_99,cs-410,5,8,"00:07:08,170","00:07:11,500",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,Now this problem has not
cs-410_5_8_100,cs-410,5,8,"00:07:11,500","00:07:15,180",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,until we define what is exactly theta.
cs-410_5_8_101,cs-410,5,8,"00:07:16,970","00:07:19,381",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,"So in the next few lectures,"
cs-410_5_8_102,cs-410,5,8,"00:07:19,381","00:07:24,211",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,we're going to talk about
cs-410_5_8_103,cs-410,5,8,"00:07:24,211","00:07:34,211",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,[MUSIC]
cs-410_6_1_1,cs-410,6,1,"00:00:00,008","00:00:05,424",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,In this lecture we're going to talk
cs-410_6_1_2,cs-410,6,1,"00:00:05,424","00:00:12,465",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=5,about how to instantiate
cs-410_6_1_3,cs-410,6,1,"00:00:12,465","00:00:19,875",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,that we can get very
cs-410_6_1_4,cs-410,6,1,"00:00:22,974","00:00:27,888",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,So this is to continue the discussion
cs-410_6_1_5,cs-410,6,1,"00:00:27,888","00:00:32,810",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,which is one particular approach
cs-410_6_1_6,cs-410,6,1,"00:00:34,420","00:00:38,551",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,And we're going to talk about how
cs-410_6_1_7,cs-410,6,1,"00:00:38,551","00:00:42,810",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,the the vector space
cs-410_6_1_8,cs-410,6,1,"00:00:42,810","00:00:48,270",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,instantiate the framework to derive
cs-410_6_1_9,cs-410,6,1,"00:00:48,270","00:00:53,380",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,And we're going to cover the symbolist
cs-410_6_1_10,cs-410,6,1,"00:00:55,360","00:00:58,390",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,So as we discussed in
cs-410_6_1_11,cs-410,6,1,"00:00:58,390","00:01:00,600",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,the vector space model
cs-410_6_1_12,cs-410,6,1,"00:01:00,600","00:01:02,619",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,And this didn't say.
cs-410_6_1_13,cs-410,6,1,"00:01:05,266","00:01:11,040",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,"As we discussed in the previous lecture,"
cs-410_6_1_14,cs-410,6,1,"00:01:11,040","00:01:13,160",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,It does not say many things.
cs-410_6_1_15,cs-410,6,1,"00:01:14,710","00:01:15,520",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,"So, for example,"
cs-410_6_1_16,cs-410,6,1,"00:01:15,520","00:01:19,470",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,here it shows that it did not say
cs-410_6_1_17,cs-410,6,1,"00:01:20,770","00:01:25,939",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,It also did not say how we place
cs-410_6_1_18,cs-410,6,1,"00:01:27,130","00:01:31,470",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,It did not say how we place a query
cs-410_6_1_19,cs-410,6,1,"00:01:32,500","00:01:37,250",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"And, finally, it did not say how we"
cs-410_6_1_20,cs-410,6,1,"00:01:37,250","00:01:39,020",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,the query vector and the document vector.
cs-410_6_1_21,cs-410,6,1,"00:01:40,570","00:01:44,860",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"So you can imagine,"
cs-410_6_1_22,cs-410,6,1,"00:01:46,040","00:01:52,940",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,we have to say specifically
cs-410_6_1_23,cs-410,6,1,"00:01:52,940","00:01:54,830",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,What is exactly xi?
cs-410_6_1_24,cs-410,6,1,"00:01:54,830","00:01:56,700",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,And what is exactly yi?
cs-410_6_1_25,cs-410,6,1,"00:01:58,460","00:02:02,260",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,This will determine where
cs-410_6_1_26,cs-410,6,1,"00:02:02,260","00:02:04,560",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,where we place a query vector.
cs-410_6_1_27,cs-410,6,1,"00:02:04,560","00:02:05,230",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,"And, of course,"
cs-410_6_1_28,cs-410,6,1,"00:02:05,230","00:02:08,869",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,we also need to say exactly what
cs-410_6_1_29,cs-410,6,1,"00:02:11,120","00:02:16,653",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,So if we can provide a definition
cs-410_6_1_30,cs-410,6,1,"00:02:16,653","00:02:22,590",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,the dimensions and these xi's or
cs-410_6_1_31,cs-410,6,1,"00:02:22,590","00:02:28,725",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"queries and document, then we will be"
cs-410_6_1_32,cs-410,6,1,"00:02:28,725","00:02:33,080",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,query vectors in this well defined space.
cs-410_6_1_33,cs-410,6,1,"00:02:33,080","00:02:36,414",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,"And then,"
cs-410_6_1_34,cs-410,6,1,"00:02:36,414","00:02:39,685",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,then we'll have a well
cs-410_6_1_35,cs-410,6,1,"00:02:41,427","00:02:47,630",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,So let's see how we can do that and
cs-410_6_1_36,cs-410,6,1,"00:02:47,630","00:02:52,460",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,"Actually, I would suggest you to"
cs-410_6_1_37,cs-410,6,1,"00:02:52,460","00:02:54,980",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,spend a couple minutes to think about.
cs-410_6_1_38,cs-410,6,1,"00:02:54,980","00:02:58,280",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,Suppose you are asked
cs-410_6_1_39,cs-410,6,1,"00:02:59,590","00:03:05,810",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,You have come up with the idea of vector
cs-410_6_1_40,cs-410,6,1,"00:03:05,810","00:03:10,310",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,"out how to compute these vectors exactly,"
cs-410_6_1_41,cs-410,6,1,"00:03:10,310","00:03:10,810",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,What would you do?
cs-410_6_1_42,cs-410,6,1,"00:03:12,540","00:03:15,857",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,"So, think for a couple of minutes,"
cs-410_6_1_43,cs-410,6,1,"00:03:20,581","00:03:26,460",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,"So, let's think about some simplest ways"
cs-410_6_1_44,cs-410,6,1,"00:03:26,460","00:03:28,810",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"First, how do we define the dimension?"
cs-410_6_1_45,cs-410,6,1,"00:03:28,810","00:03:31,430",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,"Well, the obvious choice is to use"
cs-410_6_1_46,cs-410,6,1,"00:03:31,430","00:03:34,636",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,each word in our vocabulary
cs-410_6_1_47,cs-410,6,1,"00:03:34,636","00:03:38,775",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,And show that there are N
cs-410_6_1_48,cs-410,6,1,"00:03:38,775","00:03:41,160",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"Therefore, there are N dimensions."
cs-410_6_1_49,cs-410,6,1,"00:03:41,160","00:03:42,818",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,Each word defines one dimension.
cs-410_6_1_50,cs-410,6,1,"00:03:42,818","00:03:46,273",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,And this is basically
cs-410_6_1_51,cs-410,6,1,"00:03:48,965","00:03:52,395",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,Now let's look at how we
cs-410_6_1_52,cs-410,6,1,"00:03:54,395","00:03:57,175",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"Again here, the simplest strategy is to"
cs-410_6_1_53,cs-410,6,1,"00:03:58,700","00:04:03,650",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,use a Bit Vector to represent
cs-410_6_1_54,cs-410,6,1,"00:04:04,720","00:04:07,937",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,"And that means each element, xi and"
cs-410_6_1_55,cs-410,6,1,"00:04:07,937","00:04:12,020",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,yi will be taking a value
cs-410_6_1_56,cs-410,6,1,"00:04:13,270","00:04:14,300",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,"When it's 1,"
cs-410_6_1_57,cs-410,6,1,"00:04:14,300","00:04:20,750",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,it means the corresponding word is
cs-410_6_1_58,cs-410,6,1,"00:04:20,750","00:04:25,180",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,"When it's 0,"
cs-410_6_1_59,cs-410,6,1,"00:04:27,070","00:04:31,210",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,So you can imagine if the user
cs-410_6_1_60,cs-410,6,1,"00:04:31,210","00:04:35,790",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,then the query vector will only
cs-410_6_1_61,cs-410,6,1,"00:04:37,630","00:04:41,450",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"The document vector,"
cs-410_6_1_62,cs-410,6,1,"00:04:41,450","00:04:46,700",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,But it will also have many zeros since
cs-410_6_1_63,cs-410,6,1,"00:04:46,700","00:04:50,720",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,Many words don't really
cs-410_6_1_64,cs-410,6,1,"00:04:52,110","00:04:56,570",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,Many words will only occasionally
cs-410_6_1_65,cs-410,6,1,"00:04:58,680","00:05:01,770",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,A lot of words will be absent
cs-410_6_1_66,cs-410,6,1,"00:05:04,390","00:05:09,770",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,So now we have placed the documents and
cs-410_6_1_67,cs-410,6,1,"00:05:11,450","00:05:14,240",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,Let's look at how we
cs-410_6_1_68,cs-410,6,1,"00:05:15,770","00:05:19,400",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"So, a commonly used similarity"
cs-410_6_1_69,cs-410,6,1,"00:05:20,900","00:05:25,590",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,The Dot Product of two
cs-410_6_1_70,cs-410,6,1,"00:05:25,590","00:05:30,590",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,the sum of the products of the
cs-410_6_1_71,cs-410,6,1,"00:05:30,590","00:05:38,596",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,"So, here we see that it's"
cs-410_6_1_72,cs-410,6,1,"00:05:38,596","00:05:40,228",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,"So, here."
cs-410_6_1_73,cs-410,6,1,"00:05:40,228","00:05:43,420",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,"And then, x2 multiplied by y2."
cs-410_6_1_74,cs-410,6,1,"00:05:43,420","00:05:47,100",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"And then, finally, xn multiplied by yn."
cs-410_6_1_75,cs-410,6,1,"00:05:47,100","00:05:48,810",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,"And then, we take a sum here."
cs-410_6_1_76,cs-410,6,1,"00:05:50,630","00:05:52,610",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,So that's a Dot Product.
cs-410_6_1_77,cs-410,6,1,"00:05:52,610","00:05:57,580",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,"Now, we can represent this in a more"
cs-410_6_1_78,cs-410,6,1,"00:05:58,740","00:06:04,120",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,So this is only one of the many different
cs-410_6_1_79,cs-410,6,1,"00:06:04,120","00:06:10,640",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"So, now we see that we have"
cs-410_6_1_80,cs-410,6,1,"00:06:10,640","00:06:16,050",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,"we have defined the vectors, and we have"
cs-410_6_1_81,cs-410,6,1,"00:06:16,050","00:06:21,495",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,So now we finally have the simplest
cs-410_6_1_82,cs-410,6,1,"00:06:21,495","00:06:26,882",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,on the bit vector [INAUDIBLE] dot product
cs-410_6_1_83,cs-410,6,1,"00:06:26,882","00:06:30,195",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,And the formula looks like this.
cs-410_6_1_84,cs-410,6,1,"00:06:30,195","00:06:32,415",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,So this is our formula.
cs-410_6_1_85,cs-410,6,1,"00:06:32,415","00:06:37,670",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,And that's actually a particular retrieval
cs-410_6_1_86,cs-410,6,1,"00:06:37,670","00:06:42,573",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=397,Now we can finally implement this
cs-410_6_1_87,cs-410,6,1,"00:06:42,573","00:06:45,350",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,and then rank the documents for query.
cs-410_6_1_88,cs-410,6,1,"00:06:45,350","00:06:50,110",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,"Now, at this point you should"
cs-410_6_1_89,cs-410,6,1,"00:06:50,110","00:06:53,400",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,to think about how we can
cs-410_6_1_90,cs-410,6,1,"00:06:53,400","00:06:56,972",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,"So, we have gone through the process"
cs-410_6_1_91,cs-410,6,1,"00:06:56,972","00:07:00,620",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,using a vector space model.
cs-410_6_1_92,cs-410,6,1,"00:07:00,620","00:07:05,185",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"And then,"
cs-410_6_1_93,cs-410,6,1,"00:07:05,185","00:07:09,780",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"vectors in the vector space, and"
cs-410_6_1_94,cs-410,6,1,"00:07:09,780","00:07:14,270",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,"So in the end, we've got a specific"
cs-410_6_1_95,cs-410,6,1,"00:07:15,370","00:07:18,370",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"Now, the next step is to think about"
cs-410_6_1_96,cs-410,6,1,"00:07:18,370","00:07:21,160",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,"actually makes sense, right?"
cs-410_6_1_97,cs-410,6,1,"00:07:21,160","00:07:24,140",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,Can we expect this function
cs-410_6_1_98,cs-410,6,1,"00:07:24,140","00:07:27,400",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,when we used it to rank documents for
cs-410_6_1_99,cs-410,6,1,"00:07:28,790","00:07:35,870",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,So it's worth thinking about what is
cs-410_6_1_100,cs-410,6,1,"00:07:35,870","00:07:38,220",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,"So, in the end, we'll get a number."
cs-410_6_1_101,cs-410,6,1,"00:07:38,220","00:07:40,240",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,But what does this number mean?
cs-410_6_1_102,cs-410,6,1,"00:07:40,240","00:07:40,990",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,Is it meaningful?
cs-410_6_1_103,cs-410,6,1,"00:07:42,200","00:07:44,390",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,"So, spend a couple minutes"
cs-410_6_1_104,cs-410,6,1,"00:07:45,880","00:07:46,540",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,"And, of course,"
cs-410_6_1_105,cs-410,6,1,"00:07:46,540","00:07:52,600",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,the general question here is do you
cs-410_6_1_106,cs-410,6,1,"00:07:52,600","00:07:54,680",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,Would it actually work well?
cs-410_6_1_107,cs-410,6,1,"00:07:54,680","00:07:58,329",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,"So, again,"
cs-410_6_1_108,cs-410,6,1,"00:07:58,329","00:08:00,190",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=478,Is it actually meaningful?
cs-410_6_1_109,cs-410,6,1,"00:08:01,280","00:08:03,190",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,Does it mean something?
cs-410_6_1_110,cs-410,6,1,"00:08:03,190","00:08:06,560",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,This is related to how well
cs-410_6_1_111,cs-410,6,1,"00:08:08,260","00:08:11,530",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"So, in order to assess"
cs-410_6_1_112,cs-410,6,1,"00:08:11,530","00:08:15,520",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,"vector space model actually works well,"
cs-410_6_1_113,cs-410,6,1,"00:08:17,170","00:08:22,570",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,"So, here I show some sample documents and"
cs-410_6_1_114,cs-410,6,1,"00:08:22,570","00:08:26,390",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,The query is news about
cs-410_6_1_115,cs-410,6,1,"00:08:26,390","00:08:28,580",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,And we have five documents here.
cs-410_6_1_116,cs-410,6,1,"00:08:28,580","00:08:32,280",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,They cover different terms in the query.
cs-410_6_1_117,cs-410,6,1,"00:08:34,710","00:08:39,890",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,And if you look at these documents for
cs-410_6_1_118,cs-410,6,1,"00:08:41,880","00:08:47,070",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,"some documents are probably relevant, and"
cs-410_6_1_119,cs-410,6,1,"00:08:48,300","00:08:54,690",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"Now, if I asked you to rank these"
cs-410_6_1_120,cs-410,6,1,"00:08:54,690","00:08:57,270",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=534,This is basically our ideal ranking.
cs-410_6_1_121,cs-410,6,1,"00:08:57,270","00:09:01,180",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,"When humans can examine the documents,"
cs-410_6_1_122,cs-410,6,1,"00:09:03,430","00:09:06,900",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,"Now, so think for a moment,"
cs-410_6_1_123,cs-410,6,1,"00:09:06,900","00:09:10,210",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,And perhaps by pausing the lecture.
cs-410_6_1_124,cs-410,6,1,"00:09:12,510","00:09:18,750",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,So I think most of you would
cs-410_6_1_125,cs-410,6,1,"00:09:18,750","00:09:23,353",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,better than others because they
cs-410_6_1_126,cs-410,6,1,"00:09:23,353","00:09:26,860",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,"They match news,"
cs-410_6_1_127,cs-410,6,1,"00:09:27,900","00:09:33,160",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"So, it looks like these documents"
cs-410_6_1_128,cs-410,6,1,"00:09:33,160","00:09:37,230",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,They should be ranked on top.
cs-410_6_1_129,cs-410,6,1,"00:09:37,230","00:09:41,810",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,"And the other three d2, d1, and"
cs-410_6_1_130,cs-410,6,1,"00:09:41,810","00:09:45,990",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,So we can also say d4 and
cs-410_6_1_131,cs-410,6,1,"00:09:45,990","00:09:50,150",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=585,"d1, d2 and d5 are non-relevant."
cs-410_6_1_132,cs-410,6,1,"00:09:50,150","00:09:55,290",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,So now let's see if our simplest
cs-410_6_1_133,cs-410,6,1,"00:09:55,290","00:09:57,400",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=595,or could do something closer.
cs-410_6_1_134,cs-410,6,1,"00:09:57,400","00:10:01,250",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,"So, let's first think about"
cs-410_6_1_135,cs-410,6,1,"00:10:01,250","00:10:02,272",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,to score documents.
cs-410_6_1_136,cs-410,6,1,"00:10:02,272","00:10:04,000",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,All right.
cs-410_6_1_137,cs-410,6,1,"00:10:04,000","00:10:07,420",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,"Here I show two documents, d1 and d3."
cs-410_6_1_138,cs-410,6,1,"00:10:07,420","00:10:10,390",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,And we have the query also here.
cs-410_6_1_139,cs-410,6,1,"00:10:10,390","00:10:15,130",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,"In the vector space model, of course we"
cs-410_6_1_140,cs-410,6,1,"00:10:15,130","00:10:16,830",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,these documents and the query.
cs-410_6_1_141,cs-410,6,1,"00:10:16,830","00:10:18,860",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,"Now, I showed the vocabulary here as well."
cs-410_6_1_142,cs-410,6,1,"00:10:18,860","00:10:22,850",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,So these are the end dimensions
cs-410_6_1_143,cs-410,6,1,"00:10:22,850","00:10:26,620",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,So what do you think is the vector for
cs-410_6_1_144,cs-410,6,1,"00:10:27,700","00:10:32,870",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,Note that we're assuming
cs-410_6_1_145,cs-410,6,1,"00:10:32,870","00:10:39,230",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=632,to indicate whether a term is absent or
cs-410_6_1_146,cs-410,6,1,"00:10:39,230","00:10:42,380",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=639,"So these are zero,1 bit vectors."
cs-410_6_1_147,cs-410,6,1,"00:10:43,880","00:10:45,790",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,So what do you think is the query vector?
cs-410_6_1_148,cs-410,6,1,"00:10:47,820","00:10:51,200",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=647,"Well, the query has four words here."
cs-410_6_1_149,cs-410,6,1,"00:10:51,200","00:10:54,380",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,"So for these four words,"
cs-410_6_1_150,cs-410,6,1,"00:10:54,380","00:10:55,980",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,"And for the rest, there will be zeros."
cs-410_6_1_151,cs-410,6,1,"00:10:57,680","00:10:59,290",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,"Now, what about the documents?"
cs-410_6_1_152,cs-410,6,1,"00:10:59,290","00:11:00,610",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,It's the same.
cs-410_6_1_153,cs-410,6,1,"00:11:00,610","00:11:03,430",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,"So d1 has two rows, news and about."
cs-410_6_1_154,cs-410,6,1,"00:11:03,430","00:11:07,367",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,"So, there are two 1's here,"
cs-410_6_1_155,cs-410,6,1,"00:11:07,367","00:11:12,220",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,"Similarly, so now that we"
cs-410_6_1_156,cs-410,6,1,"00:11:12,220","00:11:16,380",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,"have the two vectors,"
cs-410_6_1_157,cs-410,6,1,"00:11:17,470","00:11:19,550",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,And we're going to use Do Product.
cs-410_6_1_158,cs-410,6,1,"00:11:19,550","00:11:21,610",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=679,"So you can see when we use Dot Product,"
cs-410_6_1_159,cs-410,6,1,"00:11:21,610","00:11:26,030",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,we just multiply the corresponding
cs-410_6_1_160,cs-410,6,1,"00:11:26,030","00:11:30,894",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=686,"So these two will be formal product,"
cs-410_6_1_161,cs-410,6,1,"00:11:30,894","00:11:33,920",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,and these two will
cs-410_6_1_162,cs-410,6,1,"00:11:33,920","00:11:38,210",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=693,and these two will generate yet
cs-410_6_1_163,cs-410,6,1,"00:11:40,020","00:11:46,320",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,"Now you can easily see if we do that,"
cs-410_6_1_164,cs-410,6,1,"00:11:48,180","00:11:54,170",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,these zeroes because whenever we have
cs-410_6_1_165,cs-410,6,1,"00:11:54,170","00:11:57,538",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,So when we take a sum
cs-410_6_1_166,cs-410,6,1,"00:11:57,538","00:12:02,940",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,then the zero entries will be gone.
cs-410_6_1_167,cs-410,6,1,"00:12:04,400","00:12:08,010",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,"As long as you have one zero,"
cs-410_6_1_168,cs-410,6,1,"00:12:08,010","00:12:14,710",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,"So, in the fact, we're just"
cs-410_6_1_169,cs-410,6,1,"00:12:14,710","00:12:18,220",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,"In this case, we have seen two,"
cs-410_6_1_170,cs-410,6,1,"00:12:18,220","00:12:20,240",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=738,So what does that mean?
cs-410_6_1_171,cs-410,6,1,"00:12:20,240","00:12:25,190",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,"Well, that means this number, or"
cs-410_6_1_172,cs-410,6,1,"00:12:25,190","00:12:33,130",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,is simply the count of how many unique
cs-410_6_1_173,cs-410,6,1,"00:12:33,130","00:12:39,350",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,Because if a term is matched in the
cs-410_6_1_174,cs-410,6,1,"00:12:41,390","00:12:44,740",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,"If it's not, then there will"
cs-410_6_1_175,cs-410,6,1,"00:12:46,310","00:12:50,410",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,"Similarly, if the document has a term but"
cs-410_6_1_176,cs-410,6,1,"00:12:50,410","00:12:53,220",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=770,there will be a zero in the query vector.
cs-410_6_1_177,cs-410,6,1,"00:12:53,220","00:12:55,020",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,So those don't count.
cs-410_6_1_178,cs-410,6,1,"00:12:55,020","00:12:58,760",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,"So, as a result,"
cs-410_6_1_179,cs-410,6,1,"00:12:58,760","00:13:03,820",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,measures how many unique query
cs-410_6_1_180,cs-410,6,1,"00:13:03,820","00:13:05,770",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=783,This is how we interpret this score.
cs-410_6_1_181,cs-410,6,1,"00:13:07,150","00:13:10,520",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=787,"Now, we can also take a look at d3."
cs-410_6_1_182,cs-410,6,1,"00:13:10,520","00:13:18,003",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=790,"In this case, you can see the result"
cs-410_6_1_183,cs-410,6,1,"00:13:18,003","00:13:23,140",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=798,"distinctive query words news, presidential"
cs-410_6_1_184,cs-410,6,1,"00:13:23,140","00:13:28,200",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=803,"Now in this case, this seems"
cs-410_6_1_185,cs-410,6,1,"00:13:29,260","00:13:33,440",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=809,And this simplest vector
cs-410_6_1_186,cs-410,6,1,"00:13:33,440","00:13:35,050",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,So that looks pretty good.
cs-410_6_1_187,cs-410,6,1,"00:13:35,050","00:13:40,030",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,"However, if we examine this model in"
cs-410_6_1_188,cs-410,6,1,"00:13:40,030","00:13:44,891",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=820,"So, here I'm going to show all"
cs-410_6_1_189,cs-410,6,1,"00:13:44,891","00:13:49,977",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=824,And you can easily verify they're
cs-410_6_1_190,cs-410,6,1,"00:13:49,977","00:13:55,070",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=829,counting the number of unique query
cs-410_6_1_191,cs-410,6,1,"00:13:56,470","00:13:59,270",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=836,Now note that this measure
cs-410_6_1_192,cs-410,6,1,"00:13:59,270","00:14:03,740",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=839,It basically means if a document
cs-410_6_1_193,cs-410,6,1,"00:14:03,740","00:14:07,210",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=843,then the document will be
cs-410_6_1_194,cs-410,6,1,"00:14:07,210","00:14:09,190",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=847,And that seems to make sense.
cs-410_6_1_195,cs-410,6,1,"00:14:09,190","00:14:16,870",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=849,The only problem is here we can note that
cs-410_6_1_196,cs-410,6,1,"00:14:16,870","00:14:22,320",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=856,And they tied with a 3 as a score.
cs-410_6_1_197,cs-410,6,1,"00:14:25,050","00:14:31,000",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,"So, that's a problem because if you look"
cs-410_6_1_198,cs-410,6,1,"00:14:31,000","00:14:36,920",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=871,should be ranked above d3 because
cs-410_6_1_199,cs-410,6,1,"00:14:36,920","00:14:42,100",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=876,"d3 only mentions the presidential once,"
cs-410_6_1_200,cs-410,6,1,"00:14:42,100","00:14:47,634",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=882,"In the case of d3,"
cs-410_6_1_201,cs-410,6,1,"00:14:47,634","00:14:51,360",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=887,But d4 is clearly above
cs-410_6_1_202,cs-410,6,1,"00:14:51,360","00:14:58,200",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=891,Another problem is that d2 and
cs-410_6_1_203,cs-410,6,1,"00:14:58,200","00:15:01,880",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=898,But if you look at the three words
cs-410_6_1_204,cs-410,6,1,"00:15:01,880","00:15:07,020",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=901,"it matched the news, about and campaign."
cs-410_6_1_205,cs-410,6,1,"00:15:07,020","00:15:11,500",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=907,"But in the case of d3, it matched news,"
cs-410_6_1_206,cs-410,6,1,"00:15:12,530","00:15:17,960",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=912,So intuitively this reads better
cs-410_6_1_207,cs-410,6,1,"00:15:17,960","00:15:21,920",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=917,"is more important than matching about,"
cs-410_6_1_208,cs-410,6,1,"00:15:21,920","00:15:24,910",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=921,even though about and
cs-410_6_1_209,cs-410,6,1,"00:15:26,170","00:15:30,730",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=926,"So intuitively,"
cs-410_6_1_210,cs-410,6,1,"00:15:30,730","00:15:32,750",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=930,But this model doesn't do that.
cs-410_6_1_211,cs-410,6,1,"00:15:33,860","00:15:37,150",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=933,So that means this model
cs-410_6_1_212,cs-410,6,1,"00:15:37,150","00:15:39,109",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=937,We have to solve these problems.
cs-410_6_1_213,cs-410,6,1,"00:15:41,188","00:15:41,991",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=941,"To summarize,"
cs-410_6_1_214,cs-410,6,1,"00:15:41,991","00:15:45,770",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=941,in this lecture we talked about how
cs-410_6_1_215,cs-410,6,1,"00:15:47,610","00:15:49,540",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=947,We mainly need to do three things.
cs-410_6_1_216,cs-410,6,1,"00:15:49,540","00:15:51,796",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=949,One is to define the dimension.
cs-410_6_1_217,cs-410,6,1,"00:15:51,796","00:15:59,896",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=951,The second is to decide how to place
cs-410_6_1_218,cs-410,6,1,"00:15:59,896","00:16:05,761",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=959,and to also place a query in
cs-410_6_1_219,cs-410,6,1,"00:16:07,862","00:16:11,900",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=967,And third is to define
cs-410_6_1_220,cs-410,6,1,"00:16:11,900","00:16:15,790",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,particularly the query vector and
cs-410_6_1_221,cs-410,6,1,"00:16:17,080","00:16:22,430",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=977,We also talked about various simple way
cs-410_6_1_222,cs-410,6,1,"00:16:22,430","00:16:27,910",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=982,"Indeed, that's probably the simplest"
cs-410_6_1_223,cs-410,6,1,"00:16:27,910","00:16:31,480",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=987,"In this case,"
cs-410_6_1_224,cs-410,6,1,"00:16:31,480","00:16:37,430",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=991,"We use a zero, 1 bit vector to"
cs-410_6_1_225,cs-410,6,1,"00:16:37,430","00:16:42,690",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=997,"In this case, we basically only care"
cs-410_6_1_226,cs-410,6,1,"00:16:42,690","00:16:43,790",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1002,We ignore the frequency.
cs-410_6_1_227,cs-410,6,1,"00:16:45,560","00:16:49,220",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,And we use the Dot Product
cs-410_6_1_228,cs-410,6,1,"00:16:50,360","00:16:53,304",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1010,"And with such a instantiation,"
cs-410_6_1_229,cs-410,6,1,"00:16:53,304","00:16:58,870",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,we showed that the scoring
cs-410_6_1_230,cs-410,6,1,"00:16:58,870","00:17:03,260",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1018,a document based on the number of distinct
cs-410_6_1_231,cs-410,6,1,"00:17:04,650","00:17:09,800",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1024,We also showed that such a simple vector
cs-410_6_1_232,cs-410,6,1,"00:17:09,800","00:17:10,720",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1029,we need to improve it.
cs-410_6_1_233,cs-410,6,1,"00:17:12,540","00:17:18,797",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1032,And this is a topic that we're
cs-410_6_1_234,cs-410,6,1,"00:17:18,797","00:17:28,797",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1038,[MUSIC]
cs-410_6_2_1,cs-410,6,2,"00:00:00,000","00:00:07,148",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_6_2_2,cs-410,6,2,"00:00:07,148","00:00:12,770",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about how to do faster
cs-410_6_2_3,cs-410,6,2,"00:00:14,710","00:00:19,487",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"In this lecture, we're going to continue"
cs-410_6_2_4,cs-410,6,2,"00:00:19,487","00:00:20,583",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,"In particular,"
cs-410_6_2_5,cs-410,6,2,"00:00:20,583","00:00:25,840",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,we're going to talk about how to support
cs-410_6_2_6,cs-410,6,2,"00:00:26,906","00:00:31,360",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,So let's think about what a general
cs-410_6_2_7,cs-410,6,2,"00:00:32,730","00:00:37,260",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,"Now of course, the vector space"
cs-410_6_2_8,cs-410,6,2,"00:00:37,260","00:00:40,750",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,we can imagine many other retrieval
cs-410_6_2_9,cs-410,6,2,"00:00:42,390","00:00:44,970",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,So the form of this
cs-410_6_2_10,cs-410,6,2,"00:00:46,060","00:00:49,870",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,We see this scoring function
cs-410_6_2_11,cs-410,6,2,"00:00:49,870","00:00:55,260",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,a query Q is defined as
cs-410_6_2_12,cs-410,6,2,"00:00:55,260","00:01:00,350",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,that adjustment a function that
cs-410_6_2_13,cs-410,6,2,"00:01:00,350","00:01:05,425",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"That I'll assume here at the end,"
cs-410_6_2_14,cs-410,6,2,"00:01:05,425","00:01:09,100",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,f sub d of d and f sub q of q.
cs-410_6_2_15,cs-410,6,2,"00:01:09,100","00:01:14,467",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,These are adjustment factors
cs-410_6_2_16,cs-410,6,2,"00:01:14,467","00:01:19,387",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,so they are at the level of a document and
cs-410_6_2_17,cs-410,6,2,"00:01:19,387","00:01:22,719",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,"So and then inside of this function,"
cs-410_6_2_18,cs-410,6,2,"00:01:22,719","00:01:27,127",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,we also see there's
cs-410_6_2_19,cs-410,6,2,"00:01:27,127","00:01:33,102",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,So this is the main part
cs-410_6_2_20,cs-410,6,2,"00:01:33,102","00:01:38,365",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,these as I just said of
cs-410_6_2_21,cs-410,6,2,"00:01:38,365","00:01:43,931",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,the level of the whole document and
cs-410_6_2_22,cs-410,6,2,"00:01:43,931","00:01:47,521",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"For example, document [INAUDIBLE] and"
cs-410_6_2_23,cs-410,6,2,"00:01:47,521","00:01:52,805",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,this aggregate punching would
cs-410_6_2_24,cs-410,6,2,"00:01:52,805","00:01:55,847",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,"Now inside this h function,"
cs-410_6_2_25,cs-410,6,2,"00:01:55,847","00:02:01,300",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,there are functions that
cs-410_6_2_26,cs-410,6,2,"00:02:01,300","00:02:06,384",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,of the contribution of
cs-410_6_2_27,cs-410,6,2,"00:02:08,475","00:02:14,305",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,"So this g,"
cs-410_6_2_28,cs-410,6,2,"00:02:14,305","00:02:19,670",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,of a matched query term ti in document d.
cs-410_6_2_29,cs-410,6,2,"00:02:23,710","00:02:28,365",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,And this h function would then
cs-410_6_2_30,cs-410,6,2,"00:02:28,365","00:02:34,390",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,"So for example,"
cs-410_6_2_31,cs-410,6,2,"00:02:36,110","00:02:39,940",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,but it can also be a product or it could
cs-410_6_2_32,cs-410,6,2,"00:02:41,250","00:02:46,207",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,"And then finally, this adjustment"
cs-410_6_2_33,cs-410,6,2,"00:02:46,207","00:02:51,162",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,the document level or query level
cs-410_6_2_34,cs-410,6,2,"00:02:51,162","00:02:53,697",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,"for example, document [INAUDIBLE]."
cs-410_6_2_35,cs-410,6,2,"00:02:53,697","00:02:58,960",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"So, this general form would cover"
cs-410_6_2_36,cs-410,6,2,"00:02:58,960","00:03:06,610",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,Let's look at how we can score documents
cs-410_6_2_37,cs-410,6,2,"00:03:07,610","00:03:10,930",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,"So, here's a general algorithm"
cs-410_6_2_38,cs-410,6,2,"00:03:10,930","00:03:14,670",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,First this query level and
cs-410_6_2_39,cs-410,6,2,"00:03:14,670","00:03:19,540",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,document level factors can be
cs-410_6_2_40,cs-410,6,2,"00:03:19,540","00:03:22,810",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,"Of course, for the query we have to"
cs-410_6_2_41,cs-410,6,2,"00:03:22,810","00:03:28,180",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"document, for example,"
cs-410_6_2_42,cs-410,6,2,"00:03:28,180","00:03:32,850",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,"And then, we maintain a score accumulator"
cs-410_6_2_43,cs-410,6,2,"00:03:34,710","00:03:39,440",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,An h is an aggregation function
cs-410_6_2_44,cs-410,6,2,"00:03:39,440","00:03:40,530",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,So how do we do that?
cs-410_6_2_45,cs-410,6,2,"00:03:40,530","00:03:45,770",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,For each period term we're going to
cs-410_6_2_46,cs-410,6,2,"00:03:45,770","00:03:47,130",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,from the invert index.
cs-410_6_2_47,cs-410,6,2,"00:03:47,130","00:03:51,290",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,This will give us all the documents
cs-410_6_2_48,cs-410,6,2,"00:03:52,850","00:03:57,640",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"and that includes d1, f1 and so dn fn."
cs-410_6_2_49,cs-410,6,2,"00:03:57,640","00:04:03,394",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,So each pair is a document ID and
cs-410_6_2_50,cs-410,6,2,"00:04:03,394","00:04:08,268",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,Then for each entry d sub j and
cs-410_6_2_51,cs-410,6,2,"00:04:08,268","00:04:12,436",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,of the term in this
cs-410_6_2_52,cs-410,6,2,"00:04:12,436","00:04:17,739",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,We'll going to compute the function
cs-410_6_2_53,cs-410,6,2,"00:04:17,739","00:04:19,370",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,"weight of this term, so"
cs-410_6_2_54,cs-410,6,2,"00:04:19,370","00:04:26,170",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,we're computing the weight completion of
cs-410_6_2_55,cs-410,6,2,"00:04:26,170","00:04:31,152",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,"And then, we're going to update"
cs-410_6_2_56,cs-410,6,2,"00:04:31,152","00:04:35,820",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,this document and
cs-410_6_2_57,cs-410,6,2,"00:04:35,820","00:04:41,144",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,accumulator that would
cs-410_6_2_58,cs-410,6,2,"00:04:41,144","00:04:46,640",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,So this is basically a general
cs-410_6_2_59,cs-410,6,2,"00:04:46,640","00:04:51,288",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,functions of this form by
cs-410_6_2_60,cs-410,6,2,"00:04:51,288","00:04:54,621",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,Note that we don't have to
cs-410_6_2_61,cs-410,6,2,"00:04:54,621","00:04:56,906",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,that didn't match any query term.
cs-410_6_2_62,cs-410,6,2,"00:04:56,906","00:04:59,096",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"Well, this is why it's fast,"
cs-410_6_2_63,cs-410,6,2,"00:04:59,096","00:05:04,418",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,we only need to process the documents
cs-410_6_2_64,cs-410,6,2,"00:05:04,418","00:05:09,415",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,"In the end, then we're going to adjust"
cs-410_6_2_65,cs-410,6,2,"00:05:09,415","00:05:11,600",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,sub a and then we can sort.
cs-410_6_2_66,cs-410,6,2,"00:05:11,600","00:05:14,270",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,So let's take a look
cs-410_6_2_67,cs-410,6,2,"00:05:14,270","00:05:17,880",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,"In this case, let's assume the scoring"
cs-410_6_2_68,cs-410,6,2,"00:05:17,880","00:05:24,450",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,"it just takes the sum of t f, the role of"
cs-410_6_2_69,cs-410,6,2,"00:05:25,830","00:05:31,340",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,This simplification would help
cs-410_6_2_70,cs-410,6,2,"00:05:31,340","00:05:36,640",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,It's very easy to extend the computation
cs-410_6_2_71,cs-410,6,2,"00:05:36,640","00:05:43,422",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,"the transformation of tf, or [INAUDIBLE]"
cs-410_6_2_72,cs-410,6,2,"00:05:43,422","00:05:47,890",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,"So let's take a look at specific example,"
cs-410_6_2_73,cs-410,6,2,"00:05:48,980","00:05:54,600",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,and it show some entries of
cs-410_6_2_74,cs-410,6,2,"00:05:54,600","00:05:56,800",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,Information occurred in four documents and
cs-410_6_2_75,cs-410,6,2,"00:05:56,800","00:06:01,260",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,"their frequencies are also there,"
cs-410_6_2_76,cs-410,6,2,"00:06:01,260","00:06:07,210",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,"So let's see how the arrows works, so"
cs-410_6_2_77,cs-410,6,2,"00:06:07,210","00:06:09,580",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,"and we fetch the first query then,"
cs-410_6_2_78,cs-410,6,2,"00:06:09,580","00:06:12,260",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,"That's information, right?"
cs-410_6_2_79,cs-410,6,2,"00:06:12,260","00:06:16,360",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,And imagine we have all these
cs-410_6_2_80,cs-410,6,2,"00:06:17,800","00:06:19,800",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,scores for these documents.
cs-410_6_2_81,cs-410,6,2,"00:06:19,800","00:06:21,740",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,We can imagine there will be other but
cs-410_6_2_82,cs-410,6,2,"00:06:21,740","00:06:24,660",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,then they will only be
cs-410_6_2_83,cs-410,6,2,"00:06:24,660","00:06:28,681",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,"So before we do any waiting of terms,"
cs-410_6_2_84,cs-410,6,2,"00:06:28,681","00:06:32,979",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,we don't even need a score of.
cs-410_6_2_85,cs-410,6,2,"00:06:32,979","00:06:36,859",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,That comes actually we have these score
cs-410_6_2_86,cs-410,6,2,"00:06:38,260","00:06:43,110",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,So lets fetch the interest from
cs-410_6_2_87,cs-410,6,2,"00:06:43,110","00:06:45,080",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,"information, that the first one."
cs-410_6_2_88,cs-410,6,2,"00:06:46,260","00:06:50,809",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,So these four accumulators obviously
cs-410_6_2_89,cs-410,6,2,"00:06:51,830","00:06:54,418",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,"So, the first entry is d1 and 3,"
cs-410_6_2_90,cs-410,6,2,"00:06:54,418","00:06:58,357",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,3 is occurrences of
cs-410_6_2_91,cs-410,6,2,"00:06:58,357","00:07:03,617",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,Since our scoring function assume that the
cs-410_6_2_92,cs-410,6,2,"00:07:03,617","00:07:09,178",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,We just need to add a 3 to the score
cs-410_6_2_93,cs-410,6,2,"00:07:09,178","00:07:16,388",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,the increase of score due to matching
cs-410_6_2_94,cs-410,6,2,"00:07:16,388","00:07:19,679",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,"And then, we go to the next entry,"
cs-410_6_2_95,cs-410,6,2,"00:07:19,679","00:07:22,493",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,then we add a 4 to the score
cs-410_6_2_96,cs-410,6,2,"00:07:22,493","00:07:27,614",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,"Of course, at this point, that we will"
cs-410_6_2_97,cs-410,6,2,"00:07:27,614","00:07:33,427",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,"And so at this point, we allocated"
cs-410_6_2_98,cs-410,6,2,"00:07:33,427","00:07:39,174",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,"and we add one, we allocate another"
cs-410_6_2_99,cs-410,6,2,"00:07:39,174","00:07:44,370",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,"And then finally,"
cs-410_6_2_100,cs-410,6,2,"00:07:44,370","00:07:50,450",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,information occurred five
cs-410_6_2_101,cs-410,6,2,"00:07:50,450","00:07:55,310",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"Okay, so this completes the processing of"
cs-410_6_2_102,cs-410,6,2,"00:07:55,310","00:07:56,500",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,information.
cs-410_6_2_103,cs-410,6,2,"00:07:56,500","00:08:00,080",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,It processed all the contributions
cs-410_6_2_104,cs-410,6,2,"00:08:00,080","00:08:00,820",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,four documents.
cs-410_6_2_105,cs-410,6,2,"00:08:01,830","00:08:06,900",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,"So now, our error will go to"
cs-410_6_2_106,cs-410,6,2,"00:08:06,900","00:08:09,810",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,"So, we're going to fetch all"
cs-410_6_2_107,cs-410,6,2,"00:08:10,830","00:08:11,520",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"So, in this case,"
cs-410_6_2_108,cs-410,6,2,"00:08:11,520","00:08:15,700",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,"there are three entries, and"
cs-410_6_2_109,cs-410,6,2,"00:08:15,700","00:08:18,410",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,The first is d2 and 3 and
cs-410_6_2_110,cs-410,6,2,"00:08:18,410","00:08:22,890",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,that means security occur three
cs-410_6_2_111,cs-410,6,2,"00:08:22,890","00:08:26,300",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,"Well, we do exactly the same,"
cs-410_6_2_112,cs-410,6,2,"00:08:26,300","00:08:31,557",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,"So, this time we're going to change the"
cs-410_6_2_113,cs-410,6,2,"00:08:31,557","00:08:36,390",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,allocated and
cs-410_6_2_114,cs-410,6,2,"00:08:36,390","00:08:40,470",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"value which is a 4, so"
cs-410_6_2_115,cs-410,6,2,"00:08:41,530","00:08:46,333",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,D2 score is increased because the match
cs-410_6_2_116,cs-410,6,2,"00:08:46,333","00:08:47,382",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,the security.
cs-410_6_2_117,cs-410,6,2,"00:08:47,382","00:08:53,721",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"Go to the next entry, that's d4 and"
cs-410_6_2_118,cs-410,6,2,"00:08:53,721","00:08:59,040",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,"d4 and again, we add 1 to d4 so"
cs-410_6_2_119,cs-410,6,2,"00:08:59,040","00:09:02,449",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,"Finally, we process d5 and a 3."
cs-410_6_2_120,cs-410,6,2,"00:09:02,449","00:09:07,679",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,Since we have not yet allocated a score
cs-410_6_2_121,cs-410,6,2,"00:09:07,679","00:09:12,190",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,"we're going to allocate 1 for d5,"
cs-410_6_2_122,cs-410,6,2,"00:09:12,190","00:09:19,480",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,"So, those scores, of the last rule,"
cs-410_6_2_123,cs-410,6,2,"00:09:20,480","00:09:25,810",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,If our scoring function is just
cs-410_6_2_124,cs-410,6,2,"00:09:27,080","00:09:31,600",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"Now, what if we, actually,"
cs-410_6_2_125,cs-410,6,2,"00:09:31,600","00:09:35,130",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"Well, we going to do the [INAUDIBLE]"
cs-410_6_2_126,cs-410,6,2,"00:09:36,490","00:09:40,020",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,"So, to summarize this,"
cs-410_6_2_127,cs-410,6,2,"00:09:40,020","00:09:44,660",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,we first process the information
cs-410_6_2_128,cs-410,6,2,"00:09:44,660","00:09:49,520",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,we processed all the entries
cs-410_6_2_129,cs-410,6,2,"00:09:49,520","00:09:54,775",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,"Then we process the security,"
cs-410_6_2_130,cs-410,6,2,"00:09:54,775","00:10:00,916",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,what should be the order of processing
cs-410_6_2_131,cs-410,6,2,"00:10:00,916","00:10:05,677",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=600,It might make a difference especially
cs-410_6_2_132,cs-410,6,2,"00:10:05,677","00:10:07,670",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,the score accumulators.
cs-410_6_2_133,cs-410,6,2,"00:10:07,670","00:10:12,226",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,"Let's say, we only want to keep"
cs-410_6_2_134,cs-410,6,2,"00:10:12,226","00:10:15,601",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,What do you think would be
cs-410_6_2_135,cs-410,6,2,"00:10:15,601","00:10:22,680",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,Would you process a common term first or
cs-410_6_2_136,cs-410,6,2,"00:10:24,460","00:10:30,597",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,The answers is we just go to who
cs-410_6_2_137,cs-410,6,2,"00:10:30,597","00:10:35,531",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,"A rare term would match a few documents,"
cs-410_6_2_138,cs-410,6,2,"00:10:35,531","00:10:38,910",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,"be higher,"
cs-410_6_2_139,cs-410,6,2,"00:10:38,910","00:10:44,933",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,"And then, it allows us to attach"
cs-410_6_2_140,cs-410,6,2,"00:10:44,933","00:10:48,042",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,"So, it helps pruning"
cs-410_6_2_141,cs-410,6,2,"00:10:48,042","00:10:51,901",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,if we don't need so
cs-410_6_2_142,cs-410,6,2,"00:10:51,901","00:10:55,474",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,So those are all heuristics for
cs-410_6_2_143,cs-410,6,2,"00:10:55,474","00:10:59,850",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,Here you can also see how we can
cs-410_6_2_144,cs-410,6,2,"00:10:59,850","00:11:03,192",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,So they can [INAUDIBLE] when we
cs-410_6_2_145,cs-410,6,2,"00:11:03,192","00:11:04,700",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,each query time.
cs-410_6_2_146,cs-410,6,2,"00:11:04,700","00:11:08,420",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=664,When we fetch the inverted index we
cs-410_6_2_147,cs-410,6,2,"00:11:08,420","00:11:09,990",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,then we can compute IDF.
cs-410_6_2_148,cs-410,6,2,"00:11:09,990","00:11:13,710",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=669,Or maybe perhaps the IDF value
cs-410_6_2_149,cs-410,6,2,"00:11:13,710","00:11:16,890",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,when we indexed the documents.
cs-410_6_2_150,cs-410,6,2,"00:11:16,890","00:11:22,780",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,"At that time, we already computed"
cs-410_6_2_151,cs-410,6,2,"00:11:22,780","00:11:26,570",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=682,so all these can be done at this time.
cs-410_6_2_152,cs-410,6,2,"00:11:26,570","00:11:29,820",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=686,So that would mean when we process
cs-410_6_2_153,cs-410,6,2,"00:11:29,820","00:11:35,300",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,these words would be adjusted by the same
cs-410_6_2_154,cs-410,6,2,"00:11:36,590","00:11:39,580",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,So this is the basic idea of using
cs-410_6_2_155,cs-410,6,2,"00:11:39,580","00:11:44,770",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=699,it works well for all kinds of
cs-410_6_2_156,cs-410,6,2,"00:11:44,770","00:11:49,726",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,"And this generally,"
cs-410_6_2_157,cs-410,6,2,"00:11:49,726","00:11:53,397",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,most state of art retrieval functions.
cs-410_6_2_158,cs-410,6,2,"00:11:53,397","00:11:58,708",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,So there are some tricks to
cs-410_6_2_159,cs-410,6,2,"00:11:58,708","00:12:02,988",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=718,some general techniques
cs-410_6_2_160,cs-410,6,2,"00:12:02,988","00:12:07,756",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,This is we just store some results of
cs-410_6_2_161,cs-410,6,2,"00:12:07,756","00:12:12,291",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,"when you see the same query,"
cs-410_6_2_162,cs-410,6,2,"00:12:12,291","00:12:17,781",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=732,"Similarly, you can also slow the list"
cs-410_6_2_163,cs-410,6,2,"00:12:17,781","00:12:19,041",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,a popular term.
cs-410_6_2_164,cs-410,6,2,"00:12:19,041","00:12:21,268",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=739,"And if the query term is popular likely,"
cs-410_6_2_165,cs-410,6,2,"00:12:21,268","00:12:25,620",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,you will soon need to factor the inverted
cs-410_6_2_166,cs-410,6,2,"00:12:25,620","00:12:30,569",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=745,"So keeping it in the memory would help,"
cs-410_6_2_167,cs-410,6,2,"00:12:30,569","00:12:32,206",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=750,improving efficiency.
cs-410_6_2_168,cs-410,6,2,"00:12:32,206","00:12:36,694",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,We can also keep only the most promising
cs-410_6_2_169,cs-410,6,2,"00:12:36,694","00:12:39,281",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,doesn't want to examine so many documents.
cs-410_6_2_170,cs-410,6,2,"00:12:39,281","00:12:44,092",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=759,We only need to return high
cs-410_6_2_171,cs-410,6,2,"00:12:44,092","00:12:46,410",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,likely are ranked on the top.
cs-410_6_2_172,cs-410,6,2,"00:12:47,900","00:12:51,860",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=767,"For that purpose,"
cs-410_6_2_173,cs-410,6,2,"00:12:51,860","00:12:53,810",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=771,We don't have to store
cs-410_6_2_174,cs-410,6,2,"00:12:53,810","00:12:59,936",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,"At some point, we just keep"
cs-410_6_2_175,cs-410,6,2,"00:12:59,936","00:13:06,257",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=779,Another technique is to do parallel
cs-410_6_2_176,cs-410,6,2,"00:13:06,257","00:13:11,731",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,really process in such a large
cs-410_6_2_177,cs-410,6,2,"00:13:11,731","00:13:15,869",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=791,And you scale up to
cs-410_6_2_178,cs-410,6,2,"00:13:15,869","00:13:20,628",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=795,the special techniques you
cs-410_6_2_179,cs-410,6,2,"00:13:20,628","00:13:25,609",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,to distribute the storage
cs-410_6_2_180,cs-410,6,2,"00:13:25,609","00:13:31,850",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=805,So here is a list of some text retrieval
cs-410_6_2_181,cs-410,6,2,"00:13:31,850","00:13:37,160",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=811,You can find more information
cs-410_6_2_182,cs-410,6,2,"00:13:37,160","00:13:42,510",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,"And here, I listed your four here,"
cs-410_6_2_183,cs-410,6,2,"00:13:42,510","00:13:48,361",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,that can support a lot of applications and
cs-410_6_2_184,cs-410,6,2,"00:13:48,361","00:13:51,900",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,You can use it to build a search
cs-410_6_2_185,cs-410,6,2,"00:13:51,900","00:13:55,555",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=831,The downside is that it's not
cs-410_6_2_186,cs-410,6,2,"00:13:55,555","00:14:01,500",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=835,the algorithms implemented they are also
cs-410_6_2_187,cs-410,6,2,"00:14:01,500","00:14:06,294",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=841,Lemur or Indri is another
cs-410_6_2_188,cs-410,6,2,"00:14:06,294","00:14:10,068",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=846,a nice support web
cs-410_6_2_189,cs-410,6,2,"00:14:10,068","00:14:16,094",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=850,it has many advanced search algorithms and
cs-410_6_2_190,cs-410,6,2,"00:14:16,094","00:14:20,735",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=856,Terrier is yet another toolkit
cs-410_6_2_191,cs-410,6,2,"00:14:20,735","00:14:25,108",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=860,application capability and
cs-410_6_2_192,cs-410,6,2,"00:14:25,108","00:14:30,008",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,So that's maybe in between Lemur or
cs-410_6_2_193,cs-410,6,2,"00:14:30,008","00:14:34,663",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=870,maybe rather combining
cs-410_6_2_194,cs-410,6,2,"00:14:34,663","00:14:38,110",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=874,so that's also useful tool kit.
cs-410_6_2_195,cs-410,6,2,"00:14:38,110","00:14:41,920",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=878,MeTA is a toolkit that we will use for
cs-410_6_2_196,cs-410,6,2,"00:14:41,920","00:14:46,590",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=881,the problem assignment and
cs-410_6_2_197,cs-410,6,2,"00:14:47,690","00:14:54,250",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=887,a combination of both text retrieval
cs-410_6_2_198,cs-410,6,2,"00:14:54,250","00:15:01,390",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,And so talking models are implement they
cs-410_6_2_199,cs-410,6,2,"00:15:01,390","00:15:06,720",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=901,implemented in the toolkit as
cs-410_6_2_200,cs-410,6,2,"00:15:06,720","00:15:10,580",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=906,So to summarize all the discussion
cs-410_6_2_201,cs-410,6,2,"00:15:11,600","00:15:14,700",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=911,here are the major takeaway points.
cs-410_6_2_202,cs-410,6,2,"00:15:14,700","00:15:20,760",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=914,Inverted index is the primary data
cs-410_6_2_203,cs-410,6,2,"00:15:20,760","00:15:25,300",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=920,and that's the key to enable
cs-410_6_2_204,cs-410,6,2,"00:15:26,350","00:15:31,116",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=926,And the basic idea is to preprocess
cs-410_6_2_205,cs-410,6,2,"00:15:31,116","00:15:34,491",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=931,we want to do compression
cs-410_6_2_206,cs-410,6,2,"00:15:34,491","00:15:39,625",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=934,So that we can save disk space and
cs-410_6_2_207,cs-410,6,2,"00:15:39,625","00:15:43,840",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,processing of inverted index in general.
cs-410_6_2_208,cs-410,6,2,"00:15:43,840","00:15:48,400",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=943,We talked about how to construct the
cs-410_6_2_209,cs-410,6,2,"00:15:48,400","00:15:49,179",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,the memory.
cs-410_6_2_210,cs-410,6,2,"00:15:49,179","00:15:54,374",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=949,And then we talk about faster search using
cs-410_6_2_211,cs-410,6,2,"00:15:54,374","00:15:59,960",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=954,the invective index to accumulate a scores
cs-410_6_2_212,cs-410,6,2,"00:15:59,960","00:16:03,760",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=959,And we exploit the Zipf's law to
cs-410_6_2_213,cs-410,6,2,"00:16:03,760","00:16:06,052",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=963,that don't match any query term and
cs-410_6_2_214,cs-410,6,2,"00:16:06,052","00:16:11,540",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=966,this algorithm can actually support
cs-410_6_2_215,cs-410,6,2,"00:16:13,400","00:16:17,630",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=973,So these basic techniques
cs-410_6_2_216,cs-410,6,2,"00:16:17,630","00:16:23,570",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=977,further scaling up using distributed file
cs-410_6_2_217,cs-410,6,2,"00:16:23,570","00:16:28,410",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=983,Here are two additional readings you
cs-410_6_2_218,cs-410,6,2,"00:16:28,410","00:16:31,040",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=988,you are interested in
cs-410_6_2_219,cs-410,6,2,"00:16:31,040","00:16:38,156",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=991,The first one is a classical
cs-410_6_2_220,cs-410,6,2,"00:16:38,156","00:16:41,590",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=998,o inverted index and
cs-410_6_2_221,cs-410,6,2,"00:16:41,590","00:16:46,035",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1001,"And how to,"
cs-410_6_2_222,cs-410,6,2,"00:16:46,035","00:16:49,811",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1006,"any inputs of the space,"
cs-410_6_2_223,cs-410,6,2,"00:16:49,811","00:16:54,802",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1009,The second one is a newer textbook that
cs-410_6_2_224,cs-410,6,2,"00:16:54,802","00:16:56,675",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1014,evaluating search engines.
cs-410_6_2_225,cs-410,6,2,"00:16:58,835","00:17:08,835",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1018,[MUSIC]
cs-410_6_3_1,cs-410,6,3,"00:00:00,004","00:00:06,485",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND].
cs-410_6_3_2,cs-410,6,3,"00:00:06,485","00:00:10,694",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,This lecture is about some practical
cs-410_6_3_3,cs-410,6,3,"00:00:10,694","00:00:12,939",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,evaluation of text retrieval systems.
cs-410_6_3_4,cs-410,6,3,"00:00:14,440","00:00:17,730",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"In this lecture, we will continue"
cs-410_6_3_5,cs-410,6,3,"00:00:17,730","00:00:21,250",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,We'll cover some practical
cs-410_6_3_6,cs-410,6,3,"00:00:21,250","00:00:24,240",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,in actual evaluation of
cs-410_6_3_7,cs-410,6,3,"00:00:25,500","00:00:29,060",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"So, in order to create"
cs-410_6_3_8,cs-410,6,3,"00:00:29,060","00:00:31,540",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,we have to create a set of queries.
cs-410_6_3_9,cs-410,6,3,"00:00:31,540","00:00:34,270",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,A set of documents and
cs-410_6_3_10,cs-410,6,3,"00:00:35,750","00:00:39,680",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,It turns out that each is
cs-410_6_3_11,cs-410,6,3,"00:00:39,680","00:00:43,240",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"First, the documents and"
cs-410_6_3_12,cs-410,6,3,"00:00:43,240","00:00:47,250",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=43,They must represent the real queries and
cs-410_6_3_13,cs-410,6,3,"00:00:48,290","00:00:50,990",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,And we also have to use many queries and
cs-410_6_3_14,cs-410,6,3,"00:00:50,990","00:00:55,010",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,many documents in order to
cs-410_6_3_15,cs-410,6,3,"00:00:56,470","00:01:02,560",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,For the matching of relevant
cs-410_6_3_16,cs-410,6,3,"00:01:02,560","00:01:10,050",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,We also need to ensure that there exists a
cs-410_6_3_17,cs-410,6,3,"00:01:10,050","00:01:13,900",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"If a query has only one, that's"
cs-410_6_3_18,cs-410,6,3,"00:01:13,900","00:01:18,300",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,It's not very informative to
cs-410_6_3_19,cs-410,6,3,"00:01:18,300","00:01:23,120",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,using such a query because there's not
cs-410_6_3_20,cs-410,6,3,"00:01:23,120","00:01:27,390",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,"So ideally, there should be more"
cs-410_6_3_21,cs-410,6,3,"00:01:27,390","00:01:30,469",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,the queries also should represent
cs-410_6_3_22,cs-410,6,3,"00:01:31,470","00:01:35,240",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"In terms of relevance judgments,"
cs-410_6_3_23,cs-410,6,3,"00:01:35,240","00:01:38,970",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,complete judgments of all
cs-410_6_3_24,cs-410,6,3,"00:01:38,970","00:01:40,670",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,"Yet, minimizing human and"
cs-410_6_3_25,cs-410,6,3,"00:01:40,670","00:01:44,980",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"fault, because we have to use human"
cs-410_6_3_26,cs-410,6,3,"00:01:44,980","00:01:47,690",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,It's very labor intensive.
cs-410_6_3_27,cs-410,6,3,"00:01:47,690","00:01:52,550",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"And as a result, it's impossible to"
cs-410_6_3_28,cs-410,6,3,"00:01:52,550","00:01:57,170",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,"all the queries, especially considering"
cs-410_6_3_29,cs-410,6,3,"00:01:58,750","00:02:03,590",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,"So this is actually a major challenge,"
cs-410_6_3_30,cs-410,6,3,"00:02:03,590","00:02:07,160",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,"For measures, it's also challenging,"
cs-410_6_3_31,cs-410,6,3,"00:02:07,160","00:02:11,680",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,accurately reflect
cs-410_6_3_32,cs-410,6,3,"00:02:11,680","00:02:15,430",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,We have to consider carefully
cs-410_6_3_33,cs-410,6,3,"00:02:15,430","00:02:18,530",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,And then design measures to measure that.
cs-410_6_3_34,cs-410,6,3,"00:02:18,530","00:02:21,482",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,If your measure is not
cs-410_6_3_35,cs-410,6,3,"00:02:21,482","00:02:23,820",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,then your conclusion would be misled.
cs-410_6_3_36,cs-410,6,3,"00:02:23,820","00:02:25,040",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,So it's very important.
cs-410_6_3_37,cs-410,6,3,"00:02:26,880","00:02:29,290",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,So we're going to talk about
cs-410_6_3_38,cs-410,6,3,"00:02:29,290","00:02:31,360",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,One is the statistical significance test.
cs-410_6_3_39,cs-410,6,3,"00:02:31,360","00:02:36,350",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,And this also is a reason why
cs-410_6_3_40,cs-410,6,3,"00:02:36,350","00:02:41,060",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,And the question here is how sure can
cs-410_6_3_41,cs-410,6,3,"00:02:41,060","00:02:44,800",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,doesn't simply result from
cs-410_6_3_42,cs-410,6,3,"00:02:44,800","00:02:49,770",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,So here are some sample results of
cs-410_6_3_43,cs-410,6,3,"00:02:49,770","00:02:53,320",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,System B into different experiments.
cs-410_6_3_44,cs-410,6,3,"00:02:53,320","00:02:57,540",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"And you can see in the bottom,"
cs-410_6_3_45,cs-410,6,3,"00:02:57,540","00:03:02,668",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=177,"So the mean, if you look at the mean"
cs-410_6_3_46,cs-410,6,3,"00:03:02,668","00:03:08,300",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,of positions are exactly the same
cs-410_6_3_47,cs-410,6,3,"00:03:08,300","00:03:13,250",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,"So you can see this is 0.20,"
cs-410_6_3_48,cs-410,6,3,"00:03:13,250","00:03:18,520",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,And again here it's also 0.20 and
cs-410_6_3_49,cs-410,6,3,"00:03:18,520","00:03:23,440",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"Yet, if you look at these exact average"
cs-410_6_3_50,cs-410,6,3,"00:03:23,440","00:03:29,810",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,"If you look at these numbers in detail,"
cs-410_6_3_51,cs-410,6,3,"00:03:29,810","00:03:35,090",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,you would feel that you can trust
cs-410_6_3_52,cs-410,6,3,"00:03:36,100","00:03:41,610",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,"In the another case, in the other case,"
cs-410_6_3_53,cs-410,6,3,"00:03:41,610","00:03:48,470",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,"So, why don't you take a look at all these"
cs-410_6_3_54,cs-410,6,3,"00:03:48,470","00:03:52,565",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"So, if you look at the average,"
cs-410_6_3_55,cs-410,6,3,"00:03:52,565","00:03:56,660",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"we can easily, say that well,"
cs-410_6_3_56,cs-410,6,3,"00:03:56,660","00:03:59,630",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"So, after all it's 0.40 and"
cs-410_6_3_57,cs-410,6,3,"00:03:59,630","00:04:05,950",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"this is twice as much as 0.20,"
cs-410_6_3_58,cs-410,6,3,"00:04:05,950","00:04:10,120",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,"But if you look at these two experiments,"
cs-410_6_3_59,cs-410,6,3,"00:04:11,150","00:04:16,170",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,"You will see that, we've been more"
cs-410_6_3_60,cs-410,6,3,"00:04:16,170","00:04:17,040",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,in experiment one.
cs-410_6_3_61,cs-410,6,3,"00:04:17,040","00:04:19,160",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,In this case.
cs-410_6_3_62,cs-410,6,3,"00:04:19,160","00:04:23,360",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,Because these numbers seem to be
cs-410_6_3_63,cs-410,6,3,"00:04:25,110","00:04:32,342",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,"Whereas in Experiment 2, we're not sure"
cs-410_6_3_64,cs-410,6,3,"00:04:32,342","00:04:38,000",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,after System A is better and
cs-410_6_3_65,cs-410,6,3,"00:04:39,335","00:04:43,790",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"But yet if we look at only average,"
cs-410_6_3_66,cs-410,6,3,"00:04:45,750","00:04:47,640",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,"So, what do you think?"
cs-410_6_3_67,cs-410,6,3,"00:04:49,170","00:04:54,150",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,"How reliable is our conclusion,"
cs-410_6_3_68,cs-410,6,3,"00:04:55,940","00:04:59,430",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"Now in this case, intuitively,"
cs-410_6_3_69,cs-410,6,3,"00:05:01,020","00:05:04,630",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,But how can we quantitate
cs-410_6_3_70,cs-410,6,3,"00:05:04,630","00:05:08,430",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,And this is why we need to do
cs-410_6_3_71,cs-410,6,3,"00:05:09,440","00:05:13,910",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"So, the idea of the statistical"
cs-410_6_3_72,cs-410,6,3,"00:05:13,910","00:05:18,330",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,assess the variants across
cs-410_6_3_73,cs-410,6,3,"00:05:18,330","00:05:21,160",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,"If there is a big variance,"
cs-410_6_3_74,cs-410,6,3,"00:05:21,160","00:05:25,880",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,that means the results could fluctuate
cs-410_6_3_75,cs-410,6,3,"00:05:25,880","00:05:30,740",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,"Then we should believe that,"
cs-410_6_3_76,cs-410,6,3,"00:05:30,740","00:05:35,210",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,the results might change if we
cs-410_6_3_77,cs-410,6,3,"00:05:35,210","00:05:39,350",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,"Right, so this is then not so"
cs-410_6_3_78,cs-410,6,3,"00:05:39,350","00:05:42,660",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,if you have c high variance
cs-410_6_3_79,cs-410,6,3,"00:05:43,660","00:05:49,390",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,So let's look at these results
cs-410_6_3_80,cs-410,6,3,"00:05:49,390","00:05:54,200",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,"So, here we show two different"
cs-410_6_3_81,cs-410,6,3,"00:05:54,200","00:05:57,470",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,One is a sign test where
cs-410_6_3_82,cs-410,6,3,"00:05:57,470","00:06:01,260",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,"If System B is better than System A,"
cs-410_6_3_83,cs-410,6,3,"00:06:01,260","00:06:05,400",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,When System A is better we
cs-410_6_3_84,cs-410,6,3,"00:06:05,400","00:06:09,600",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,"Using this case, if you see this,"
cs-410_6_3_85,cs-410,6,3,"00:06:09,600","00:06:12,980",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,We actually have four cases
cs-410_6_3_86,cs-410,6,3,"00:06:12,980","00:06:16,671",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,"But three cases of System A is better,"
cs-410_6_3_87,cs-410,6,3,"00:06:16,671","00:06:19,880",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,"this is almost like a random results,"
cs-410_6_3_88,cs-410,6,3,"00:06:19,880","00:06:25,880",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,So if you just take a random
cs-410_6_3_89,cs-410,6,3,"00:06:25,880","00:06:30,090",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=385,if you use plus to denote the head and
cs-410_6_3_90,cs-410,6,3,"00:06:30,090","00:06:34,920",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,that could easily be the results of just
cs-410_6_3_91,cs-410,6,3,"00:06:34,920","00:06:39,700",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,"So, the fact that the average is"
cs-410_6_3_92,cs-410,6,3,"00:06:39,700","00:06:41,330",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,We can't reliably conclude that.
cs-410_6_3_93,cs-410,6,3,"00:06:41,330","00:06:45,890",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,And this can be quantitatively
cs-410_6_3_94,cs-410,6,3,"00:06:45,890","00:06:48,380",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,And that basically means
cs-410_6_3_95,cs-410,6,3,"00:06:49,660","00:06:54,480",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,the probability that this result is
cs-410_6_3_96,cs-410,6,3,"00:06:54,480","00:06:56,140",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,"In this case, probability is 1.0."
cs-410_6_3_97,cs-410,6,3,"00:06:56,140","00:07:00,050",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,It means it surely is
cs-410_6_3_98,cs-410,6,3,"00:07:01,310","00:07:06,470",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,"Now in Willcoxan test,"
cs-410_6_3_99,cs-410,6,3,"00:07:06,470","00:07:09,430",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,and we would be not only
cs-410_6_3_100,cs-410,6,3,"00:07:09,430","00:07:12,520",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,we'll be also looking at
cs-410_6_3_101,cs-410,6,3,"00:07:12,520","00:07:14,690",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,"But we can draw a similar conclusion,"
cs-410_6_3_102,cs-410,6,3,"00:07:14,690","00:07:18,630",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,where you say it's very
cs-410_6_3_103,cs-410,6,3,"00:07:18,630","00:07:22,395",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,"To illustrate this, let's think"
cs-410_6_3_104,cs-410,6,3,"00:07:22,395","00:07:23,895",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,And this is called a now distribution.
cs-410_6_3_105,cs-410,6,3,"00:07:23,895","00:07:26,085",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,We assume that the mean is zero here.
cs-410_6_3_106,cs-410,6,3,"00:07:26,085","00:07:28,705",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,Lets say we started with
cs-410_6_3_107,cs-410,6,3,"00:07:28,705","00:07:31,405",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,no difference between the two systems.
cs-410_6_3_108,cs-410,6,3,"00:07:31,405","00:07:35,230",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,But we assume that because of random
cs-410_6_3_109,cs-410,6,3,"00:07:35,230","00:07:37,190",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,we might observe a difference.
cs-410_6_3_110,cs-410,6,3,"00:07:37,190","00:07:41,300",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,So the actual difference might
cs-410_6_3_111,cs-410,6,3,"00:07:41,300","00:07:42,830",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"on the right side here, right?"
cs-410_6_3_112,cs-410,6,3,"00:07:43,920","00:07:48,102",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,"So, and this curve kind of shows"
cs-410_6_3_113,cs-410,6,3,"00:07:48,102","00:07:52,290",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,actually observe values that
cs-410_6_3_114,cs-410,6,3,"00:07:53,770","00:07:59,440",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,"Now, so if we look at this picture then,"
cs-410_6_3_115,cs-410,6,3,"00:08:01,070","00:08:05,530",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,"if a difference is observed here, then"
cs-410_6_3_116,cs-410,6,3,"00:08:05,530","00:08:11,180",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,the chance is very high that this is
cs-410_6_3_117,cs-410,6,3,"00:08:11,180","00:08:16,150",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,We can define a region of
cs-410_6_3_118,cs-410,6,3,"00:08:16,150","00:08:21,890",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,random fluctuation and
cs-410_6_3_119,cs-410,6,3,"00:08:21,890","00:08:27,894",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,And in this then the observed may
cs-410_6_3_120,cs-410,6,3,"00:08:28,960","00:08:34,830",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,But if you observe a value in this
cs-410_6_3_121,cs-410,6,3,"00:08:34,830","00:08:39,880",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,then the difference is unlikely
cs-410_6_3_122,cs-410,6,3,"00:08:39,880","00:08:44,460",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"All right, so there's a very small"
cs-410_6_3_123,cs-410,6,3,"00:08:44,460","00:08:47,400",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=524,such a difference just because
cs-410_6_3_124,cs-410,6,3,"00:08:48,400","00:08:52,800",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,"So in that case, we can then conclude"
cs-410_6_3_125,cs-410,6,3,"00:08:52,800","00:08:54,670",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,So System B is indeed better.
cs-410_6_3_126,cs-410,6,3,"00:08:56,120","00:08:59,550",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,So this is the idea of
cs-410_6_3_127,cs-410,6,3,"00:08:59,550","00:09:03,870",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,The takeaway message here is that you
cs-410_6_3_128,cs-410,6,3,"00:09:03,870","00:09:05,770",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,jumping into a conclusion.
cs-410_6_3_129,cs-410,6,3,"00:09:05,770","00:09:08,330",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,"As in this case,"
cs-410_6_3_130,cs-410,6,3,"00:09:09,790","00:09:13,259",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,There are many different ways of doing
cs-410_6_3_131,cs-410,6,3,"00:09:15,260","00:09:20,270",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,"So now, let's talk about the other"
cs-410_6_3_132,cs-410,6,3,"00:09:20,270","00:09:24,590",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,"as we said earlier,"
cs-410_6_3_133,cs-410,6,3,"00:09:24,590","00:09:27,700",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,completely unless it's
cs-410_6_3_134,cs-410,6,3,"00:09:27,700","00:09:31,530",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"So the question is,"
cs-410_6_3_135,cs-410,6,3,"00:09:31,530","00:09:33,880",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"in the collection,"
cs-410_6_3_136,cs-410,6,3,"00:09:35,000","00:09:38,230",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,And the solution here is Pooling.
cs-410_6_3_137,cs-410,6,3,"00:09:38,230","00:09:45,640",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,And this is a strategy that has been used
cs-410_6_3_138,cs-410,6,3,"00:09:46,710","00:09:49,800",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,So the idea of Pooling is the following.
cs-410_6_3_139,cs-410,6,3,"00:09:49,800","00:09:54,410",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=589,We would first choose a diverse
cs-410_6_3_140,cs-410,6,3,"00:09:54,410","00:09:56,010",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,These are Text Retrieval systems.
cs-410_6_3_141,cs-410,6,3,"00:09:57,130","00:10:02,830",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,And we hope these methods can help us
cs-410_6_3_142,cs-410,6,3,"00:10:02,830","00:10:05,400",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,So the goal is to pick out
cs-410_6_3_143,cs-410,6,3,"00:10:05,400","00:10:08,770",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,We want to make judgements on relevant
cs-410_6_3_144,cs-410,6,3,"00:10:08,770","00:10:12,720",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,useful documents from users perspectives.
cs-410_6_3_145,cs-410,6,3,"00:10:12,720","00:10:16,339",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,So then we're going to have
cs-410_6_3_146,cs-410,6,3,"00:10:17,380","00:10:19,720",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,The K can vary from systems.
cs-410_6_3_147,cs-410,6,3,"00:10:19,720","00:10:24,370",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,But the point is to ask them to suggest
cs-410_6_3_148,cs-410,6,3,"00:10:25,530","00:10:29,780",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,And then we simply combine
cs-410_6_3_149,cs-410,6,3,"00:10:29,780","00:10:34,478",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=629,to form a pool of documents for
cs-410_6_3_150,cs-410,6,3,"00:10:34,478","00:10:41,370",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"To judge, so imagine you have many"
cs-410_6_3_151,cs-410,6,3,"00:10:41,370","00:10:44,498",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,"We take the top-K documents,"
cs-410_6_3_152,cs-410,6,3,"00:10:44,498","00:10:48,060",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,"Now, of course, there are many"
cs-410_6_3_153,cs-410,6,3,"00:10:48,060","00:10:51,860",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,many systems might have retrieved
cs-410_6_3_154,cs-410,6,3,"00:10:51,860","00:10:55,250",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,So there will be some duplicate documents.
cs-410_6_3_155,cs-410,6,3,"00:10:56,480","00:11:00,690",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,And there are also unique documents
cs-410_6_3_156,cs-410,6,3,"00:11:00,690","00:11:03,490",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=660,So the idea of having diverse
cs-410_6_3_157,cs-410,6,3,"00:11:03,490","00:11:07,470",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,set of ranking methods is to
cs-410_6_3_158,cs-410,6,3,"00:11:07,470","00:11:11,140",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,And can include as many possible
cs-410_6_3_159,cs-410,6,3,"00:11:12,360","00:11:17,180",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,"And then, the users would,"
cs-410_6_3_160,cs-410,6,3,"00:11:17,180","00:11:21,250",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,"the judgments on this data set, this pool."
cs-410_6_3_161,cs-410,6,3,"00:11:21,250","00:11:26,710",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,And the other unjudged the documents are
cs-410_6_3_162,cs-410,6,3,"00:11:26,710","00:11:30,900",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=686,"Now if the pool is large enough,"
cs-410_6_3_163,cs-410,6,3,"00:11:32,080","00:11:38,600",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,"But if the pool is not very large,"
cs-410_6_3_164,cs-410,6,3,"00:11:38,600","00:11:41,190",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=698,And we might use other
cs-410_6_3_165,cs-410,6,3,"00:11:41,190","00:11:46,100",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,there are indeed other
cs-410_6_3_166,cs-410,6,3,"00:11:46,100","00:11:49,840",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,And such a strategy is generally okay for
cs-410_6_3_167,cs-410,6,3,"00:11:49,840","00:11:54,740",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,comparing systems that
cs-410_6_3_168,cs-410,6,3,"00:11:54,740","00:11:57,740",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,That means if you participate
cs-410_6_3_169,cs-410,6,3,"00:11:57,740","00:12:00,850",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,then it's unlikely that it
cs-410_6_3_170,cs-410,6,3,"00:12:00,850","00:12:03,100",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=720,because the problematic
cs-410_6_3_171,cs-410,6,3,"00:12:04,300","00:12:07,060",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,"However, this is problematic for"
cs-410_6_3_172,cs-410,6,3,"00:12:07,060","00:12:11,880",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,evaluating a new system that may
cs-410_6_3_173,cs-410,6,3,"00:12:11,880","00:12:16,010",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,"In this case, a new system might"
cs-410_6_3_174,cs-410,6,3,"00:12:16,010","00:12:20,850",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,nominated some read only documents
cs-410_6_3_175,cs-410,6,3,"00:12:20,850","00:12:24,370",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,So those documents might be
cs-410_6_3_176,cs-410,6,3,"00:12:24,370","00:12:26,150",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,That's unfair.
cs-410_6_3_177,cs-410,6,3,"00:12:26,150","00:12:32,810",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=746,So to summarize the whole part of textual
cs-410_6_3_178,cs-410,6,3,"00:12:32,810","00:12:37,150",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=752,Because the problem is the empirically
cs-410_6_3_179,cs-410,6,3,"00:12:38,450","00:12:42,470",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=758,"don't rely on users, there's no way to"
cs-410_6_3_180,cs-410,6,3,"00:12:43,580","00:12:46,600",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,If we have in the property
cs-410_6_3_181,cs-410,6,3,"00:12:46,600","00:12:49,710",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,we might misguide our research or
cs-410_6_3_182,cs-410,6,3,"00:12:49,710","00:12:52,470",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,And we might just draw wrong conclusions.
cs-410_6_3_183,cs-410,6,3,"00:12:52,470","00:12:55,250",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=772,And we have seen this is
cs-410_6_3_184,cs-410,6,3,"00:12:55,250","00:12:58,190",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,So make sure to get it right for
cs-410_6_3_185,cs-410,6,3,"00:13:00,150","00:13:03,400",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=780,The main methodology is the Cranfield
cs-410_6_3_186,cs-410,6,3,"00:13:03,400","00:13:08,230",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=783,And they are the main paradigm used in
cs-410_6_3_187,cs-410,6,3,"00:13:08,230","00:13:10,820",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,not just a search engine variation.
cs-410_6_3_188,cs-410,6,3,"00:13:10,820","00:13:16,020",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=790,Map and nDCG are the two main
cs-410_6_3_189,cs-410,6,3,"00:13:16,020","00:13:19,530",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=796,know about and they are appropriate for
cs-410_6_3_190,cs-410,6,3,"00:13:19,530","00:13:22,950",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,You will see them often
cs-410_6_3_191,cs-410,6,3,"00:13:22,950","00:13:27,080",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=802,Precision at 10 documents is easier
cs-410_6_3_192,cs-410,6,3,"00:13:27,080","00:13:28,500",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,So that's also often useful.
cs-410_6_3_193,cs-410,6,3,"00:13:30,580","00:13:37,610",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,What's not covered is some other
cs-410_6_3_194,cs-410,6,3,"00:13:37,610","00:13:43,720",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=817,"Where the system would mix two,"
cs-410_6_3_195,cs-410,6,3,"00:13:43,720","00:13:46,580",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,And then would show
cs-410_6_3_196,cs-410,6,3,"00:13:46,580","00:13:49,780",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=826,"Of course, the users don't see"
cs-410_6_3_197,cs-410,6,3,"00:13:49,780","00:13:52,410",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=829,The users would judge those results or
cs-410_6_3_198,cs-410,6,3,"00:13:52,410","00:13:58,096",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=832,click on those documents in
cs-410_6_3_199,cs-410,6,3,"00:13:58,096","00:14:02,080",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=838,"In this case then, the search engine"
cs-410_6_3_200,cs-410,6,3,"00:14:02,080","00:14:07,250",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=842,see if one method has contributed
cs-410_6_3_201,cs-410,6,3,"00:14:07,250","00:14:11,570",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=847,"If the user tends to click on one,"
cs-410_6_3_202,cs-410,6,3,"00:14:13,050","00:14:17,730",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=853,then it suggests that
cs-410_6_3_203,cs-410,6,3,"00:14:17,730","00:14:21,008",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=857,So this is what leverages the real users
cs-410_6_3_204,cs-410,6,3,"00:14:21,008","00:14:25,640",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=861,It's called A-B Test and
cs-410_6_3_205,cs-410,6,3,"00:14:25,640","00:14:29,370",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,the modern search engines or
cs-410_6_3_206,cs-410,6,3,"00:14:29,370","00:14:32,590",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=869,Another way to evaluate IR or
cs-410_6_3_207,cs-410,6,3,"00:14:32,590","00:14:36,020",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=872,textual retrieval is user studies and
cs-410_6_3_208,cs-410,6,3,"00:14:36,020","00:14:39,390",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=876,I've put some references here
cs-410_6_3_209,cs-410,6,3,"00:14:39,390","00:14:40,260",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=879,to know more about that.
cs-410_6_3_210,cs-410,6,3,"00:14:41,760","00:14:44,180",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=881,"So, there are three"
cs-410_6_3_211,cs-410,6,3,"00:14:44,180","00:14:49,280",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=884,These are three mini books about
cs-410_6_3_212,cs-410,6,3,"00:14:49,280","00:14:54,280",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=889,in covering a broad review of
cs-410_6_3_213,cs-410,6,3,"00:14:54,280","00:14:58,237",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,And it covers some of the things
cs-410_6_3_214,cs-410,6,3,"00:14:58,237","00:15:01,085",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=898,they also have a lot of others to offer.
cs-410_6_3_215,cs-410,6,3,"00:15:02,777","00:15:12,777",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=902,[MUSIC]
cs-410_6_4_1,cs-410,6,4,"00:00:00,008","00:00:03,638",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_6_4_2,cs-410,6,4,"00:00:07,832","00:00:09,846",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about the specific
cs-410_6_4_3,cs-410,6,4,"00:00:09,846","00:00:14,580",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,smoothing methods for language models
cs-410_6_4_4,cs-410,6,4,"00:00:16,560","00:00:21,030",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"In this lecture, we will continue"
cs-410_6_4_5,cs-410,6,4,"00:00:21,030","00:00:26,020",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,"information retrieval, particularly"
cs-410_6_4_6,cs-410,6,4,"00:00:26,020","00:00:29,485",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,And we're going to talk about specifically
cs-410_6_4_7,cs-410,6,4,"00:00:29,485","00:00:30,856",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,such a retrieval function.
cs-410_6_4_8,cs-410,6,4,"00:00:33,591","00:00:39,021",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,So this is a slide from a previous
cs-410_6_4_9,cs-410,6,4,"00:00:39,021","00:00:44,638",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,likelihood ranking and smoothing
cs-410_6_4_10,cs-410,6,4,"00:00:44,638","00:00:50,002",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,we add up having a retrieval function
cs-410_6_4_11,cs-410,6,4,"00:00:50,002","00:00:57,370",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,So this is the retrieval function based on
cs-410_6_4_12,cs-410,6,4,"00:00:57,370","00:01:02,738",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=57,You can see it's a sum of all
cs-410_6_4_13,cs-410,6,4,"00:01:02,738","00:01:07,506",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,And inside its sum is the count
cs-410_6_4_14,cs-410,6,4,"00:01:07,506","00:01:11,070",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,some weight for the term in the document.
cs-410_6_4_15,cs-410,6,4,"00:01:12,240","00:01:18,170",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"We have t of i, the f weight here, and"
cs-410_6_4_16,cs-410,6,4,"00:01:20,300","00:01:24,793",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,So clearly if we want to implement this
cs-410_6_4_17,cs-410,6,4,"00:01:24,793","00:01:27,650",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,we still need to figure
cs-410_6_4_18,cs-410,6,4,"00:01:27,650","00:01:33,730",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,"In particular, we're going to need to"
cs-410_6_4_19,cs-410,6,4,"00:01:33,730","00:01:39,000",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,of a word exactly and how do we set alpha.
cs-410_6_4_20,cs-410,6,4,"00:01:40,270","00:01:44,410",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"So in order to answer this question,"
cs-410_6_4_21,cs-410,6,4,"00:01:44,410","00:01:47,760",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,"smoothing methods, and"
cs-410_6_4_22,cs-410,6,4,"00:01:48,900","00:01:50,512",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,We're going to talk about
cs-410_6_4_23,cs-410,6,4,"00:01:50,512","00:01:55,575",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,The first is simple linear
cs-410_6_4_24,cs-410,6,4,"00:01:55,575","00:01:59,910",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,And this is also called
cs-410_6_4_25,cs-410,6,4,"00:02:01,170","00:02:04,140",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,So the idea is actually very simple.
cs-410_6_4_26,cs-410,6,4,"00:02:04,140","00:02:09,150",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,This picture shows how
cs-410_6_4_27,cs-410,6,4,"00:02:09,150","00:02:12,440",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,language model by using
cs-410_6_4_28,cs-410,6,4,"00:02:12,440","00:02:17,950",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,That gives us word counts normalized by
cs-410_6_4_29,cs-410,6,4,"00:02:17,950","00:02:21,130",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,The idea of using this method
cs-410_6_4_30,cs-410,6,4,"00:02:22,230","00:02:26,480",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,is to maximize the probability
cs-410_6_4_31,cs-410,6,4,"00:02:26,480","00:02:31,460",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,"As a result,"
cs-410_6_4_32,cs-410,6,4,"00:02:31,460","00:02:36,210",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,"in the text, it's going to get"
cs-410_6_4_33,cs-410,6,4,"00:02:37,810","00:02:42,620",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,"So the idea of smoothing, then,"
cs-410_6_4_34,cs-410,6,4,"00:02:42,620","00:02:47,158",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,where this word is not going to have
cs-410_6_4_35,cs-410,6,4,"00:02:47,158","00:02:50,860",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,nonzero probability should
cs-410_6_4_36,cs-410,6,4,"00:02:50,860","00:02:55,560",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,So we can note that network has
cs-410_6_4_37,cs-410,6,4,"00:02:55,560","00:03:01,367",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,So in this approach what we do is we do
cs-410_6_4_38,cs-410,6,4,"00:03:01,367","00:03:06,655",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,likelihood placement here and
cs-410_6_4_39,cs-410,6,4,"00:03:06,655","00:03:13,040",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,is computed by the smoothing parameter
cs-410_6_4_40,cs-410,6,4,"00:03:13,040","00:03:15,817",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,So this is a smoothing parameter.
cs-410_6_4_41,cs-410,6,4,"00:03:15,817","00:03:20,651",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,"The larger lambda is,"
cs-410_6_4_42,cs-410,6,4,"00:03:20,651","00:03:22,828",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,"So by mixing them together,"
cs-410_6_4_43,cs-410,6,4,"00:03:22,828","00:03:29,060",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,we achieve the goal of assigning nonzero
cs-410_6_4_44,cs-410,6,4,"00:03:29,060","00:03:31,400",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,So let's see how it works for
cs-410_6_4_45,cs-410,6,4,"00:03:32,430","00:03:36,790",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,"For example, if we compute"
cs-410_6_4_46,cs-410,6,4,"00:03:37,940","00:03:41,080",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,Now the maximum likelihood
cs-410_6_4_47,cs-410,6,4,"00:03:41,080","00:03:43,150",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,that's going to be here.
cs-410_6_4_48,cs-410,6,4,"00:03:44,320","00:03:47,740",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,But the collection probability is this.
cs-410_6_4_49,cs-410,6,4,"00:03:47,740","00:03:50,960",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,So we'll just combine them
cs-410_6_4_50,cs-410,6,4,"00:03:53,630","00:04:00,085",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,"We can also see the word network,"
cs-410_6_4_51,cs-410,6,4,"00:04:00,085","00:04:05,305",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=240,now is getting a non-zero
cs-410_6_4_52,cs-410,6,4,"00:04:05,305","00:04:11,992",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,And that's because the count is
cs-410_6_4_53,cs-410,6,4,"00:04:11,992","00:04:19,097",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,"But this part is nonzero, and"
cs-410_6_4_54,cs-410,6,4,"00:04:19,097","00:04:24,109",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,Now if you think about this and
cs-410_6_4_55,cs-410,6,4,"00:04:24,109","00:04:29,250",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,sub d in this smoothing
cs-410_6_4_56,cs-410,6,4,"00:04:29,250","00:04:34,830",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,Because that's remember the coefficient
cs-410_6_4_57,cs-410,6,4,"00:04:34,830","00:04:40,256",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,of the word given by the collection
cs-410_6_4_58,cs-410,6,4,"00:04:40,256","00:04:43,340",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"Okay, so"
cs-410_6_4_59,cs-410,6,4,"00:04:43,340","00:04:47,903",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,The second one is similar but
cs-410_6_4_60,cs-410,6,4,"00:04:47,903","00:04:49,565",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,linear interpolation.
cs-410_6_4_61,cs-410,6,4,"00:04:49,565","00:04:52,570",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,"It's often called Dirichlet Prior,"
cs-410_6_4_62,cs-410,6,4,"00:04:54,540","00:04:59,015",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,So again here we face problem
cs-410_6_4_63,cs-410,6,4,"00:04:59,015","00:05:01,565",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,an unseen word like network.
cs-410_6_4_64,cs-410,6,4,"00:05:03,765","00:05:06,957",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,Again we will use the collection
cs-410_6_4_65,cs-410,6,4,"00:05:06,957","00:05:09,707",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,we're going to combine them
cs-410_6_4_66,cs-410,6,4,"00:05:09,707","00:05:14,739",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,The formula first can be seen as
cs-410_6_4_67,cs-410,6,4,"00:05:14,739","00:05:20,258",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,likelihood estimate and
cs-410_6_4_68,cs-410,6,4,"00:05:20,258","00:05:23,580",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,as in the J-M smoothing method.
cs-410_6_4_69,cs-410,6,4,"00:05:23,580","00:05:28,388",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,Only that the coefficient now
cs-410_6_4_70,cs-410,6,4,"00:05:28,388","00:05:31,532",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,"but a dynamic coefficient in this form,"
cs-410_6_4_71,cs-410,6,4,"00:05:31,532","00:05:36,760",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,"where mu is a parameter,"
cs-410_6_4_72,cs-410,6,4,"00:05:36,760","00:05:40,550",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,And you can see if we
cs-410_6_4_73,cs-410,6,4,"00:05:40,550","00:05:44,690",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,the effect is that a long document would
cs-410_6_4_74,cs-410,6,4,"00:05:46,090","00:05:49,200",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,Because a long document
cs-410_6_4_75,cs-410,6,4,"00:05:49,200","00:05:53,140",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,therefore the coefficient
cs-410_6_4_76,cs-410,6,4,"00:05:53,140","00:05:59,949",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,And so a long document would have
cs-410_6_4_77,cs-410,6,4,"00:05:59,949","00:06:05,734",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,So this seems to make more sense
cs-410_6_4_78,cs-410,6,4,"00:06:05,734","00:06:08,979",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,"Of course,"
cs-410_6_4_79,cs-410,6,4,"00:06:08,979","00:06:12,156",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,that the two coefficients would sum to 1.
cs-410_6_4_80,cs-410,6,4,"00:06:12,156","00:06:16,400",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,Now this is one way to
cs-410_6_4_81,cs-410,6,4,"00:06:16,400","00:06:21,080",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,"Basically, it means it's a dynamic"
cs-410_6_4_82,cs-410,6,4,"00:06:22,790","00:06:27,737",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,There is another way to understand
cs-410_6_4_83,cs-410,6,4,"00:06:27,737","00:06:31,620",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,"easier to remember, and"
cs-410_6_4_84,cs-410,6,4,"00:06:33,310","00:06:38,878",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,So it's easier to see how we can rewrite
cs-410_6_4_85,cs-410,6,4,"00:06:38,878","00:06:42,847",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,Now in this form we can easily
cs-410_6_4_86,cs-410,6,4,"00:06:42,847","00:06:47,060",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,"the maximum likelihood estimate,"
cs-410_6_4_87,cs-410,6,4,"00:06:47,060","00:06:53,346",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,So normalize the count
cs-410_6_4_88,cs-410,6,4,"00:06:53,346","00:07:00,750",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,So in this form we can see what we did is
cs-410_6_4_89,cs-410,6,4,"00:07:01,800","00:07:03,230",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,So what does this mean?
cs-410_6_4_90,cs-410,6,4,"00:07:03,230","00:07:08,042",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,"Well, this is basically something related"
cs-410_6_4_91,cs-410,6,4,"00:07:08,042","00:07:09,180",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,the collection.
cs-410_6_4_92,cs-410,6,4,"00:07:10,390","00:07:13,225",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,And we multiply that by the parameter mu.
cs-410_6_4_93,cs-410,6,4,"00:07:14,510","00:07:18,577",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,And when we combine this
cs-410_6_4_94,cs-410,6,4,"00:07:18,577","00:07:24,265",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,essentially we are adding
cs-410_6_4_95,cs-410,6,4,"00:07:24,265","00:07:31,090",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,We pretend every word has
cs-410_6_4_96,cs-410,6,4,"00:07:31,090","00:07:35,290",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,So the total count would be
cs-410_6_4_97,cs-410,6,4,"00:07:35,290","00:07:38,730",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,the actual count of
cs-410_6_4_98,cs-410,6,4,"00:07:39,950","00:07:46,020",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,"As a result, in total we would"
cs-410_6_4_99,cs-410,6,4,"00:07:46,020","00:07:49,640",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,Why?
cs-410_6_4_100,cs-410,6,4,"00:07:50,770","00:07:55,480",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"over all the words, then we'll see the"
cs-410_6_4_101,cs-410,6,4,"00:07:55,480","00:07:57,380",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,and that gives us just mu.
cs-410_6_4_102,cs-410,6,4,"00:07:57,380","00:08:00,190",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,So this is the total number of
cs-410_6_4_103,cs-410,6,4,"00:08:01,550","00:08:05,270",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,And so
cs-410_6_4_104,cs-410,6,4,"00:08:05,270","00:08:12,590",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,"So in this case, we can easily"
cs-410_6_4_105,cs-410,6,4,"00:08:13,920","00:08:18,130",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,add this as a pseudocount to this data.
cs-410_6_4_106,cs-410,6,4,"00:08:18,130","00:08:22,877",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,Pretend we actually augment the data
cs-410_6_4_107,cs-410,6,4,"00:08:22,877","00:08:26,022",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,defined by the collection language model.
cs-410_6_4_108,cs-410,6,4,"00:08:26,022","00:08:30,201",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=506,"As a result, we have more counts is that"
cs-410_6_4_109,cs-410,6,4,"00:08:30,201","00:08:35,710",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,the total counts for
cs-410_6_4_110,cs-410,6,4,"00:08:35,710","00:08:41,499",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,"And as a result, even if a word has zero"
cs-410_6_4_111,cs-410,6,4,"00:08:41,499","00:08:47,115",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,"count here, then it would still have"
cs-410_6_4_112,cs-410,6,4,"00:08:47,115","00:08:49,750",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,So this is how this method works.
cs-410_6_4_113,cs-410,6,4,"00:08:49,750","00:08:52,650",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,Let's also take a look at
cs-410_6_4_114,cs-410,6,4,"00:08:52,650","00:08:58,580",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,So for text again we will
cs-410_6_4_115,cs-410,6,4,"00:08:58,580","00:09:03,000",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,"that we actually observe, but"
cs-410_6_4_116,cs-410,6,4,"00:09:03,000","00:09:05,725",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,And so the probability of
cs-410_6_4_117,cs-410,6,4,"00:09:05,725","00:09:11,051",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,"Naturally, the probability of"
cs-410_6_4_118,cs-410,6,4,"00:09:11,051","00:09:14,410",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,And so here you can also see
cs-410_6_4_119,cs-410,6,4,"00:09:15,600","00:09:16,850",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,Can you see it?
cs-410_6_4_120,cs-410,6,4,"00:09:16,850","00:09:19,020",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,"If you want to think about it,"
cs-410_6_4_121,cs-410,6,4,"00:09:20,590","00:09:25,618",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,But you'll notice that this
cs-410_6_4_122,cs-410,6,4,"00:09:25,618","00:09:29,122",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,"So we can see, in this case,"
cs-410_6_4_123,cs-410,6,4,"00:09:29,122","00:09:34,089",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,"alpha sub d does depend on the document,"
cs-410_6_4_124,cs-410,6,4,"00:09:34,089","00:09:39,787",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,because this length
cs-410_6_4_125,cs-410,6,4,"00:09:39,787","00:09:44,609",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,"whereas in the linear interpolation,"
cs-410_6_4_126,cs-410,6,4,"00:09:44,609","00:09:50,622",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,"the J-M smoothing method,"
cs-410_6_4_127,cs-410,6,4,"00:09:50,622","00:09:54,919",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,[MUSIC]
cs-410_6_5_1,cs-410,6,5,"00:00:00,025","00:00:06,042",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND] This lecture is about
cs-410_6_5_2,cs-410,6,5,"00:00:06,042","00:00:12,849",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,link analysis for web search.
cs-410_6_5_3,cs-410,6,5,"00:00:12,849","00:00:18,236",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,"In this lecture, we're going to talk"
cs-410_6_5_4,cs-410,6,5,"00:00:18,236","00:00:23,310",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,focusing on how to do link analysis and
cs-410_6_5_5,cs-410,6,5,"00:00:23,310","00:00:31,220",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,The main topic of this lecture is to look
cs-410_6_5_6,cs-410,6,5,"00:00:32,420","00:00:35,660",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,In the previous lecture we talked
cs-410_6_5_7,cs-410,6,5,"00:00:35,660","00:00:42,992",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,"Now that we have index, we want to see"
cs-410_6_5_8,cs-410,6,5,"00:00:42,992","00:00:44,900",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,The web.
cs-410_6_5_9,cs-410,6,5,"00:00:44,900","00:00:48,320",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,"Now standard IR models,"
cs-410_6_5_10,cs-410,6,5,"00:00:48,320","00:00:51,410",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,"In fact,"
cs-410_6_5_11,cs-410,6,5,"00:00:51,410","00:00:54,390",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"improve, for supporting web search."
cs-410_6_5_12,cs-410,6,5,"00:00:54,390","00:00:56,380",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,But they aren't sufficient.
cs-410_6_5_13,cs-410,6,5,"00:00:56,380","00:00:58,550",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,And mainly for the following reasons.
cs-410_6_5_14,cs-410,6,5,"00:00:58,550","00:01:02,630",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,"First, on the web, we tend to have"
cs-410_6_5_15,cs-410,6,5,"00:01:02,630","00:01:07,150",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,"example, people might search for"
cs-410_6_5_16,cs-410,6,5,"00:01:07,150","00:01:11,230",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,And this is different from
cs-410_6_5_17,cs-410,6,5,"00:01:11,230","00:01:15,870",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,where people are primarily interested
cs-410_6_5_18,cs-410,6,5,"00:01:15,870","00:01:19,070",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,So this kind of query is often
cs-410_6_5_19,cs-410,6,5,"00:01:19,070","00:01:23,250",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,The purpose is to navigate into
cs-410_6_5_20,cs-410,6,5,"00:01:23,250","00:01:28,255",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,So for such queries we might benefit
cs-410_6_5_21,cs-410,6,5,"00:01:28,255","00:01:33,020",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"Secondly, documents have additional"
cs-410_6_5_22,cs-410,6,5,"00:01:33,020","00:01:37,220",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"are web format,"
cs-410_6_5_23,cs-410,6,5,"00:01:37,220","00:01:40,538",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,"such as the layout, the title,"
cs-410_6_5_24,cs-410,6,5,"00:01:40,538","00:01:45,340",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,So this has provided opportunity to use
cs-410_6_5_25,cs-410,6,5,"00:01:45,340","00:01:49,800",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,extra context information of
cs-410_6_5_26,cs-410,6,5,"00:01:49,800","00:01:52,440",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"And finally,"
cs-410_6_5_27,cs-410,6,5,"00:01:52,440","00:01:56,570",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=112,That means we have to consider
cs-410_6_5_28,cs-410,6,5,"00:01:56,570","00:01:58,400",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,the range in the algorithm.
cs-410_6_5_29,cs-410,6,5,"00:01:58,400","00:02:03,540",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,This would give us a more robust way
cs-410_6_5_30,cs-410,6,5,"00:02:03,540","00:02:09,350",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,any spammer to just manipulate the one
cs-410_6_5_31,cs-410,6,5,"00:02:10,500","00:02:11,370",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,"So as a result,"
cs-410_6_5_32,cs-410,6,5,"00:02:11,370","00:02:15,500",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,people have made a number of major
cs-410_6_5_33,cs-410,6,5,"00:02:16,830","00:02:21,380",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,One line is to exploit
cs-410_6_5_34,cs-410,6,5,"00:02:23,020","00:02:24,790",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,And that's the main topic of this lecture.
cs-410_6_5_35,cs-410,6,5,"00:02:26,380","00:02:31,260",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,People have also proposed algorithms to
cs-410_6_5_36,cs-410,6,5,"00:02:31,260","00:02:35,370",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,Feedback information the form of
cs-410_6_5_37,cs-410,6,5,"00:02:35,370","00:02:40,720",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,in the category of feedback techniques and
cs-410_6_5_38,cs-410,6,5,"00:02:40,720","00:02:45,009",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,In general in web search the ranking
cs-410_6_5_39,cs-410,6,5,"00:02:45,009","00:02:49,750",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,algorithms to combine
cs-410_6_5_40,cs-410,6,5,"00:02:49,750","00:02:55,509",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,Many of them are based on
cs-410_6_5_41,cs-410,6,5,"00:02:55,509","00:03:03,341",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,as BM25 that we talked about [INAUDIBLE]
cs-410_6_5_42,cs-410,6,5,"00:03:03,341","00:03:09,217",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,to provide additional features
cs-410_6_5_43,cs-410,6,5,"00:03:09,217","00:03:13,364",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,but link information
cs-410_6_5_44,cs-410,6,5,"00:03:13,364","00:03:17,660",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,they provide additional scoring signals.
cs-410_6_5_45,cs-410,6,5,"00:03:17,660","00:03:21,080",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,So let's look at links in
cs-410_6_5_46,cs-410,6,5,"00:03:21,080","00:03:26,450",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,So this is a snapshot of some
cs-410_6_5_47,cs-410,6,5,"00:03:26,450","00:03:30,790",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,So we can see there are many links that
cs-410_6_5_48,cs-410,6,5,"00:03:30,790","00:03:35,730",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,"And in this case, you can also"
cs-410_6_5_49,cs-410,6,5,"00:03:35,730","00:03:40,400",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,a description of a link that's pointing
cs-410_6_5_50,cs-410,6,5,"00:03:40,400","00:03:42,850",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,"Now, this description text"
cs-410_6_5_51,cs-410,6,5,"00:03:44,460","00:03:48,920",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,"Now if you think about this text,"
cs-410_6_5_52,cs-410,6,5,"00:03:48,920","00:03:53,865",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,because it provides some extra
cs-410_6_5_53,cs-410,6,5,"00:03:53,865","00:03:59,685",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,"So for example, if someone wants"
cs-410_6_5_54,cs-410,6,5,"00:03:59,685","00:04:04,555",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,the person might say the biggest
cs-410_6_5_55,cs-410,6,5,"00:04:04,555","00:04:07,695",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,"then the link to Amazon, right?"
cs-410_6_5_56,cs-410,6,5,"00:04:07,695","00:04:11,855",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,"So, the description here after is very"
cs-410_6_5_57,cs-410,6,5,"00:04:11,855","00:04:14,350",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,the query box when they are looking for
cs-410_6_5_58,cs-410,6,5,"00:04:14,350","00:04:19,950",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,And that's why it's very useful for
cs-410_6_5_59,cs-410,6,5,"00:04:19,950","00:04:25,058",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,Suppose someone types in
cs-410_6_5_60,cs-410,6,5,"00:04:25,058","00:04:27,517",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,biggest online bookstore.
cs-410_6_5_61,cs-410,6,5,"00:04:27,517","00:04:35,980",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,All right the query would match
cs-410_6_5_62,cs-410,6,5,"00:04:35,980","00:04:39,650",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,And then this actually
cs-410_6_5_63,cs-410,6,5,"00:04:39,650","00:04:44,090",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,matching the page that's being
cs-410_6_5_64,cs-410,6,5,"00:04:44,090","00:04:45,650",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,a entry page.
cs-410_6_5_65,cs-410,6,5,"00:04:45,650","00:04:50,120",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,So if you match anchor text that
cs-410_6_5_66,cs-410,6,5,"00:04:50,120","00:04:58,080",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,actually that provides good evidence for
cs-410_6_5_67,cs-410,6,5,"00:04:58,080","00:05:00,480",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,So anchor text is very useful.
cs-410_6_5_68,cs-410,6,5,"00:05:00,480","00:05:03,970",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,If you look at the bottom part of this
cs-410_6_5_69,cs-410,6,5,"00:05:03,970","00:05:08,380",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,patterns of some links and these links
cs-410_6_5_70,cs-410,6,5,"00:05:08,380","00:05:09,230",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"So for example,"
cs-410_6_5_71,cs-410,6,5,"00:05:09,230","00:05:14,200",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,on the right side you'll see this
cs-410_6_5_72,cs-410,6,5,"00:05:14,200","00:05:17,180",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,Now that means many other pages
cs-410_6_5_73,cs-410,6,5,"00:05:17,180","00:05:20,270",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,This shows that this page is quite useful.
cs-410_6_5_74,cs-410,6,5,"00:05:21,370","00:05:24,710",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,On the left side you can see this
cs-410_6_5_75,cs-410,6,5,"00:05:24,710","00:05:25,920",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,many other pages.
cs-410_6_5_76,cs-410,6,5,"00:05:25,920","00:05:29,040",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,So this is a director page
cs-410_6_5_77,cs-410,6,5,"00:05:29,040","00:05:31,260",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,actually see a lot of other pages.
cs-410_6_5_78,cs-410,6,5,"00:05:32,670","00:05:35,990",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,So we can call the first
cs-410_6_5_79,cs-410,6,5,"00:05:35,990","00:05:41,250",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,"the second case half page, but this means"
cs-410_6_5_80,cs-410,6,5,"00:05:41,250","00:05:44,080",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,One is to provide extra text for matching.
cs-410_6_5_81,cs-410,6,5,"00:05:44,080","00:05:49,750",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,The other is to provide some
cs-410_6_5_82,cs-410,6,5,"00:05:49,750","00:05:53,980",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,to characterize how likely a page is
cs-410_6_5_83,cs-410,6,5,"00:05:55,820","00:06:02,530",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,So people then of course and proposed
cs-410_6_5_84,cs-410,6,5,"00:06:02,530","00:06:08,030",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,Google's PageRank which was the main
cs-410_6_5_85,cs-410,6,5,"00:06:08,030","00:06:13,360",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,is a good example and
cs-410_6_5_86,cs-410,6,5,"00:06:13,360","00:06:17,070",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,"popularity, basically to score authority."
cs-410_6_5_87,cs-410,6,5,"00:06:17,070","00:06:21,640",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,So the intuitions here are links
cs-410_6_5_88,cs-410,6,5,"00:06:21,640","00:06:24,030",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,Now think about one page
cs-410_6_5_89,cs-410,6,5,"00:06:24,030","00:06:27,440",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,this is very similar to one
cs-410_6_5_90,cs-410,6,5,"00:06:27,440","00:06:30,360",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,"So, of course then,"
cs-410_6_5_91,cs-410,6,5,"00:06:30,360","00:06:33,920",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,then we can assume this page
cs-410_6_5_92,cs-410,6,5,"00:06:35,120","00:06:36,570",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,So that's a very good intuition.
cs-410_6_5_93,cs-410,6,5,"00:06:38,060","00:06:42,950",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,Now PageRank is essentially to take
cs-410_6_5_94,cs-410,6,5,"00:06:42,950","00:06:46,650",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,implement with the principal approach.
cs-410_6_5_95,cs-410,6,5,"00:06:46,650","00:06:51,980",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,"Intuitively, it is essentially doing"
cs-410_6_5_96,cs-410,6,5,"00:06:51,980","00:06:56,390",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=411,It just improves the simple
cs-410_6_5_97,cs-410,6,5,"00:06:56,390","00:06:59,420",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,One it will consider indirect citations.
cs-410_6_5_98,cs-410,6,5,"00:06:59,420","00:07:04,010",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=419,So that means you don't just look
cs-410_6_5_99,cs-410,6,5,"00:07:04,010","00:07:08,550",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,You also look at what are those
cs-410_6_5_100,cs-410,6,5,"00:07:08,550","00:07:13,530",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,If those pages themselves have a lot
cs-410_6_5_101,cs-410,6,5,"00:07:13,530","00:07:16,750",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,"In some sense,"
cs-410_6_5_102,cs-410,6,5,"00:07:16,750","00:07:20,080",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,But if those pages that
cs-410_6_5_103,cs-410,6,5,"00:07:20,080","00:07:25,095",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,being pointed to by other pages they
cs-410_6_5_104,cs-410,6,5,"00:07:25,095","00:07:27,360",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,"then well, you don't get that much."
cs-410_6_5_105,cs-410,6,5,"00:07:27,360","00:07:29,830",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,So that's the idea of
cs-410_6_5_106,cs-410,6,5,"00:07:29,830","00:07:31,770",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,"All right, so"
cs-410_6_5_107,cs-410,6,5,"00:07:31,770","00:07:37,060",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,you can also understand this idea by
cs-410_6_5_108,cs-410,6,5,"00:07:37,060","00:07:42,082",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,"If you're cited by let's say ten papers,"
cs-410_6_5_109,cs-410,6,5,"00:07:42,082","00:07:48,450",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,are just workshop papers or some papers
cs-410_6_5_110,cs-410,6,5,"00:07:49,580","00:07:54,340",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,"So although you've got ten in-links,"
cs-410_6_5_111,cs-410,6,5,"00:07:54,340","00:07:59,910",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,are cited by ten papers that themselves
cs-410_6_5_112,cs-410,6,5,"00:08:01,770","00:08:06,563",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,And so in this case where we would
cs-410_6_5_113,cs-410,6,5,"00:08:06,563","00:08:08,530",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,page does that.
cs-410_6_5_114,cs-410,6,5,"00:08:08,530","00:08:12,174",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,The other idea is it's
cs-410_6_5_115,cs-410,6,5,"00:08:13,810","00:08:21,120",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,Assume that basically every page is having
cs-410_6_5_116,cs-410,6,5,"00:08:21,120","00:08:23,950",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,Essentially you are trying to
cs-410_6_5_117,cs-410,6,5,"00:08:23,950","00:08:27,510",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,links that will link all
cs-410_6_5_118,cs-410,6,5,"00:08:27,510","00:08:32,740",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,that you actually get the pseudo
cs-410_6_5_119,cs-410,6,5,"00:08:34,300","00:08:36,760",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,The reason why they want to do that.
cs-410_6_5_120,cs-410,6,5,"00:08:36,760","00:08:41,980",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,Is this will allow them
cs-410_6_5_121,cs-410,6,5,"00:08:41,980","00:08:47,348",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,elegantly with linear algebra technique.
cs-410_6_5_122,cs-410,6,5,"00:08:47,348","00:08:52,354",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"So, I think maybe the best"
cs-410_6_5_123,cs-410,6,5,"00:08:52,354","00:08:58,172",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,the PageRank is to think
cs-410_6_5_124,cs-410,6,5,"00:08:58,172","00:09:04,549",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,probability of random surfer
cs-410_6_5_125,cs-410,6,5,"00:09:04,549","00:09:14,549",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,[MUSIC]
cs-410_6_6_1,cs-410,6,6,"00:00:00,012","00:00:03,574",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_6_6_2,cs-410,6,6,"00:00:09,704","00:00:11,535",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,There are some interesting challenges
cs-410_6_6_3,cs-410,6,6,"00:00:11,535","00:00:15,015",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,in threshold for
cs-410_6_6_4,cs-410,6,6,"00:00:15,015","00:00:19,965",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,So here I show the historical data that
cs-410_6_6_5,cs-410,6,6,"00:00:19,965","00:00:25,195",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,so you can see the scores and
cs-410_6_6_6,cs-410,6,6,"00:00:25,195","00:00:30,682",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,So the first one has a score of 36.5 and
cs-410_6_6_7,cs-410,6,6,"00:00:30,682","00:00:34,852",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,The second one is not relevant and
cs-410_6_6_8,cs-410,6,6,"00:00:34,852","00:00:37,902",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,"Of course, we have a lot of documents for"
cs-410_6_6_9,cs-410,6,6,"00:00:37,902","00:00:40,910",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,because we have never
cs-410_6_6_10,cs-410,6,6,"00:00:40,910","00:00:42,060",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,"So as you can see here,"
cs-410_6_6_11,cs-410,6,6,"00:00:42,060","00:00:47,380",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,we only see the judgements of
cs-410_6_6_12,cs-410,6,6,"00:00:47,380","00:00:52,100",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,"So this is not a random sample,"
cs-410_6_6_13,cs-410,6,6,"00:00:52,100","00:00:56,930",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,"It's kind of biased, so that creates"
cs-410_6_6_14,cs-410,6,6,"00:00:58,366","00:01:04,230",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,"Secondly, there are in general very little"
cs-410_6_6_15,cs-410,6,6,"00:01:04,230","00:01:07,920",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,so it's also challenging for
cs-410_6_6_16,cs-410,6,6,"00:01:07,920","00:01:12,550",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,typically they require more training data.
cs-410_6_6_17,cs-410,6,6,"00:01:13,830","00:01:17,560",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,And in the extreme case at
cs-410_6_6_18,cs-410,6,6,"00:01:17,560","00:01:18,550",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,labeled data as well.
cs-410_6_6_19,cs-410,6,6,"00:01:18,550","00:01:20,940",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"The system there has to make a decision,"
cs-410_6_6_20,cs-410,6,6,"00:01:20,940","00:01:24,440",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,so that's a very difficult
cs-410_6_6_21,cs-410,6,6,"00:01:24,440","00:01:29,348",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,"Finally, there is also this issue of"
cs-410_6_6_22,cs-410,6,6,"00:01:29,348","00:01:34,987",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,"Now, this means we also want"
cs-410_6_6_23,cs-410,6,6,"00:01:34,987","00:01:39,983",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,space a little bit and
cs-410_6_6_24,cs-410,6,6,"00:01:39,983","00:01:45,899",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,interested in documents that
cs-410_6_6_25,cs-410,6,6,"00:01:45,899","00:01:51,330",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,"So in other words, we're going to"
cs-410_6_6_26,cs-410,6,6,"00:01:51,330","00:01:54,890",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,by testing whether the user might be
cs-410_6_6_27,cs-410,6,6,"00:01:56,550","00:02:00,530",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,currently are not matching
cs-410_6_6_28,cs-410,6,6,"00:02:01,660","00:02:02,650",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,So how do we do that?
cs-410_6_6_29,cs-410,6,6,"00:02:02,650","00:02:06,580",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,"Well, we could lower the threshold"
cs-410_6_6_30,cs-410,6,6,"00:02:06,580","00:02:11,260",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=126,deliver some near misses to the user
cs-410_6_6_31,cs-410,6,6,"00:02:13,160","00:02:18,870",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,to see how the user would
cs-410_6_6_32,cs-410,6,6,"00:02:20,540","00:02:24,920",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,"And this is a tradeoff, because on"
cs-410_6_6_33,cs-410,6,6,"00:02:24,920","00:02:28,130",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"on the other hand,"
cs-410_6_6_34,cs-410,6,6,"00:02:28,130","00:02:31,960",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,because then you will over
cs-410_6_6_35,cs-410,6,6,"00:02:31,960","00:02:36,310",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,So exploitation means you would
cs-410_6_6_36,cs-410,6,6,"00:02:36,310","00:02:39,790",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,Let's say you know the user is
cs-410_6_6_37,cs-410,6,6,"00:02:39,790","00:02:42,950",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,"you don't want to deviate that much, but"
cs-410_6_6_38,cs-410,6,6,"00:02:42,950","00:02:47,220",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,if you don't deviate at all then you don't
cs-410_6_6_39,cs-410,6,6,"00:02:47,220","00:02:50,710",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,You might miss opportunity to learn
cs-410_6_6_40,cs-410,6,6,"00:02:51,930","00:02:53,700",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,So this is a dilemma.
cs-410_6_6_41,cs-410,6,6,"00:02:54,790","00:02:57,710",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,And that's also a difficulty
cs-410_6_6_42,cs-410,6,6,"00:02:58,890","00:03:00,320",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"Now, how do we solve these problems?"
cs-410_6_6_43,cs-410,6,6,"00:03:00,320","00:03:04,499",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"In general, I think one can use the"
cs-410_6_6_44,cs-410,6,6,"00:03:04,499","00:03:09,611",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,And this strategy is basically to optimize
cs-410_6_6_45,cs-410,6,6,"00:03:09,611","00:03:12,480",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,just as you have seen
cs-410_6_6_46,cs-410,6,6,"00:03:12,480","00:03:16,610",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,"Right, so you can just compute"
cs-410_6_6_47,cs-410,6,6,"00:03:16,610","00:03:18,950",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,each candidate score threshold.
cs-410_6_6_48,cs-410,6,6,"00:03:18,950","00:03:21,830",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"Pretend that, what if I cut at this point."
cs-410_6_6_49,cs-410,6,6,"00:03:21,830","00:03:27,090",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,What if I cut at the different scoring
cs-410_6_6_50,cs-410,6,6,"00:03:27,090","00:03:28,900",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,What's utility?
cs-410_6_6_51,cs-410,6,6,"00:03:28,900","00:03:34,030",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,"Since these are training data,"
cs-410_6_6_52,cs-410,6,6,"00:03:34,030","00:03:38,440",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,"and we know that relevant status,"
cs-410_6_6_53,cs-410,6,6,"00:03:38,440","00:03:43,220",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,relevant status based on
cs-410_6_6_54,cs-410,6,6,"00:03:43,220","00:03:47,190",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,So then we can just choose the threshold
cs-410_6_6_55,cs-410,6,6,"00:03:47,190","00:03:49,810",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,on the training data.
cs-410_6_6_56,cs-410,6,6,"00:03:49,810","00:03:55,160",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,"But this of course, doesn't account for"
cs-410_6_6_57,cs-410,6,6,"00:03:56,870","00:04:00,400",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,And there is also the difficulty of
cs-410_6_6_58,cs-410,6,6,"00:04:01,530","00:04:07,300",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,"So, in general, we can only get the upper"
cs-410_6_6_59,cs-410,6,6,"00:04:07,300","00:04:13,190",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,because the threshold might
cs-410_6_6_60,cs-410,6,6,"00:04:13,190","00:04:17,115",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,"So, it's possible that this could"
cs-410_6_6_61,cs-410,6,6,"00:04:17,115","00:04:18,610",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,interesting to the user.
cs-410_6_6_62,cs-410,6,6,"00:04:19,790","00:04:21,400",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,So how do we solve this problem?
cs-410_6_6_63,cs-410,6,6,"00:04:21,400","00:04:22,896",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,"Well, we generally, and"
cs-410_6_6_64,cs-410,6,6,"00:04:22,896","00:04:27,190",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,as I said we can low with this
cs-410_6_6_65,cs-410,6,6,"00:04:27,190","00:04:30,760",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,So here's on particular approach
cs-410_6_6_66,cs-410,6,6,"00:04:30,760","00:04:32,680",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,So the idea is falling.
cs-410_6_6_67,cs-410,6,6,"00:04:32,680","00:04:37,400",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,So here I show a ranked list of all the
cs-410_6_6_68,cs-410,6,6,"00:04:37,400","00:04:40,610",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"far, and"
cs-410_6_6_69,cs-410,6,6,"00:04:40,610","00:04:44,680",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"And on the y axis we show the utility,"
cs-410_6_6_70,cs-410,6,6,"00:04:44,680","00:04:48,670",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,how you specify the coefficients
cs-410_6_6_71,cs-410,6,6,"00:04:48,670","00:04:53,160",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,"we can then imagine, that depending on the"
cs-410_6_6_72,cs-410,6,6,"00:04:54,930","00:04:59,828",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,Suppose I cut at this position and
cs-410_6_6_73,cs-410,6,6,"00:04:59,828","00:05:06,690",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,"For example,"
cs-410_6_6_74,cs-410,6,6,"00:05:06,690","00:05:11,640",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,"The optimal point,"
cs-410_6_6_75,cs-410,6,6,"00:05:11,640","00:05:16,355",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,when it will achieve the maximum utility
cs-410_6_6_76,cs-410,6,6,"00:05:17,510","00:05:23,097",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=317,And there is also zero utility threshold.
cs-410_6_6_77,cs-410,6,6,"00:05:23,097","00:05:27,720",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,You can see at this cutoff
cs-410_6_6_78,cs-410,6,6,"00:05:27,720","00:05:28,740",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,What does that mean?
cs-410_6_6_79,cs-410,6,6,"00:05:28,740","00:05:34,250",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,That means if I lower the threshold
cs-410_6_6_80,cs-410,6,6,"00:05:34,250","00:05:41,305",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,The utility would be lower but
cs-410_6_6_81,cs-410,6,6,"00:05:41,305","00:05:45,835",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,So it's not as high as
cs-410_6_6_82,cs-410,6,6,"00:05:45,835","00:05:51,492",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,But it gives us as a safe point
cs-410_6_6_83,cs-410,6,6,"00:05:51,492","00:05:56,052",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,"as I have explained, it's desirable"
cs-410_6_6_84,cs-410,6,6,"00:05:56,052","00:06:00,622",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,So it's desirable to lower the threshold
cs-410_6_6_85,cs-410,6,6,"00:06:00,622","00:06:04,850",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,"So that means, in general, we want to set"
cs-410_6_6_86,cs-410,6,6,"00:06:04,850","00:06:06,730",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,Let's say we can use the alpha to control
cs-410_6_6_87,cs-410,6,6,"00:06:08,310","00:06:13,210",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,the deviation from
cs-410_6_6_88,cs-410,6,6,"00:06:13,210","00:06:16,570",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=373,So you can see the formula of the
cs-410_6_6_89,cs-410,6,6,"00:06:16,570","00:06:21,210",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,of the zero utility threshold and
cs-410_6_6_90,cs-410,6,6,"00:06:22,490","00:06:25,600",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,"Now, the question is,"
cs-410_6_6_91,cs-410,6,6,"00:06:27,420","00:06:31,880",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,And when should we deviate more
cs-410_6_6_92,cs-410,6,6,"00:06:33,720","00:06:38,450",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,"Well, this can depend on multiple factors,"
cs-410_6_6_93,cs-410,6,6,"00:06:38,450","00:06:43,880",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,encourage this threshold
cs-410_6_6_94,cs-410,6,6,"00:06:43,880","00:06:48,630",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,"up to the zero point, and"
cs-410_6_6_95,cs-410,6,6,"00:06:48,630","00:06:52,990",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,we're not going to necessarily reach
cs-410_6_6_96,cs-410,6,6,"00:06:52,990","00:06:57,947",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,"Rather, we're going to use other"
cs-410_6_6_97,cs-410,6,6,"00:06:57,947","00:07:01,030",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,this specifically is as follows.
cs-410_6_6_98,cs-410,6,6,"00:07:01,030","00:07:06,680",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,So there will be a beta parameter to
cs-410_6_6_99,cs-410,6,6,"00:07:06,680","00:07:12,000",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=426,threshold and this can be based on can
cs-410_6_6_100,cs-410,6,6,"00:07:12,000","00:07:17,960",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=432,"to the training data let's say, and so"
cs-410_6_6_101,cs-410,6,6,"00:07:17,960","00:07:20,500",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=437,But what's more interesting
cs-410_6_6_102,cs-410,6,6,"00:07:20,500","00:07:25,510",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,"Here, and you can see in this formula,"
cs-410_6_6_103,cs-410,6,6,"00:07:25,510","00:07:31,134",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,gamma is controlling the inference
cs-410_6_6_104,cs-410,6,6,"00:07:31,134","00:07:36,210",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,of the number of examples
cs-410_6_6_105,cs-410,6,6,"00:07:36,210","00:07:43,340",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=456,So you can see in this formula as N which
cs-410_6_6_106,cs-410,6,6,"00:07:43,340","00:07:50,820",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,"becomes bigger, then it would"
cs-410_6_6_107,cs-410,6,6,"00:07:50,820","00:07:55,140",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"In other words, when these very"
cs-410_6_6_108,cs-410,6,6,"00:07:55,140","00:07:59,630",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,And that just means if we have seen few
cs-410_6_6_109,cs-410,6,6,"00:07:59,630","00:08:04,330",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,examples we're not sure whether we
cs-410_6_6_110,cs-410,6,6,"00:08:04,330","00:08:09,590",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,So we need to explore but as we have
cs-410_6_6_111,cs-410,6,6,"00:08:09,590","00:08:13,510",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,many that have we feel that we
cs-410_6_6_112,cs-410,6,6,"00:08:13,510","00:08:17,950",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,So this gives us a beta gamma for
cs-410_6_6_113,cs-410,6,6,"00:08:17,950","00:08:21,500",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,The more examples we have seen
cs-410_6_6_114,cs-410,6,6,"00:08:21,500","00:08:25,960",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=501,So the threshold would be closer
cs-410_6_6_115,cs-410,6,6,"00:08:25,960","00:08:28,490",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,that's the basic idea of this approach.
cs-410_6_6_116,cs-410,6,6,"00:08:28,490","00:08:34,120",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,This approach actually has been working
cs-410_6_6_117,cs-410,6,6,"00:08:34,120","00:08:36,030",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,particularly effective.
cs-410_6_6_118,cs-410,6,6,"00:08:36,030","00:08:42,300",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,And also can work on arbitrary utility
cs-410_6_6_119,cs-410,6,6,"00:08:43,710","00:08:48,020",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,And explicitly addresses
cs-410_6_6_120,cs-410,6,6,"00:08:48,020","00:08:53,234",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,it kind of uses the zero utility
cs-410_6_6_121,cs-410,6,6,"00:08:53,234","00:08:56,810",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,exploration-exploitation tradeoff.
cs-410_6_6_122,cs-410,6,6,"00:08:56,810","00:09:02,770",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,We're not never going to explore
cs-410_6_6_123,cs-410,6,6,"00:09:02,770","00:09:05,530",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,"So if you take the analogy of gambling,"
cs-410_6_6_124,cs-410,6,6,"00:09:05,530","00:09:08,950",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,you don't want to risk on losing money.
cs-410_6_6_125,cs-410,6,6,"00:09:08,950","00:09:12,140",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"So it's a safe spend, really"
cs-410_6_6_126,cs-410,6,6,"00:09:13,270","00:09:18,250",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,"And the problem is of course,"
cs-410_6_6_127,cs-410,6,6,"00:09:18,250","00:09:23,643",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,the zero utility lower boundary is also
cs-410_6_6_128,cs-410,6,6,"00:09:23,643","00:09:28,855",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,"course, more advance in machine learning"
cs-410_6_6_129,cs-410,6,6,"00:09:28,855","00:09:33,815",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,solving this problems and
cs-410_6_6_130,cs-410,6,6,"00:09:35,225","00:09:41,550",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,"So to summarize, there are two"
cs-410_6_6_131,cs-410,6,6,"00:09:41,550","00:09:47,070",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,"filtering systems, one is content based,"
cs-410_6_6_132,cs-410,6,6,"00:09:47,070","00:09:51,302",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,and the other is collaborative filtering
cs-410_6_6_133,cs-410,6,6,"00:09:51,302","00:09:56,710",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,We've covered content-based
cs-410_6_6_134,cs-410,6,6,"00:09:56,710","00:09:59,566",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,"In the next lecture, we will talk"
cs-410_6_6_135,cs-410,6,6,"00:09:59,566","00:10:07,030",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,"In content-based filtering system,"
cs-410_6_6_136,cs-410,6,6,"00:10:07,030","00:10:11,750",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,several problems relative to
cs-410_6_6_137,cs-410,6,6,"00:10:11,750","00:10:17,130",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,And such a system can actually be
cs-410_6_6_138,cs-410,6,6,"00:10:17,130","00:10:22,978",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,by adding a threshold mechanism and
cs-410_6_6_139,cs-410,6,6,"00:10:22,978","00:10:28,011",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,allow the system to learn from
cs-410_6_6_140,cs-410,6,6,"00:10:30,357","00:10:40,357",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,[MUSIC]
cs-410_6_7_1,cs-410,6,7,"00:00:00,532","00:00:08,683",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND].
cs-410_6_7_2,cs-410,6,7,"00:00:08,683","00:00:11,442",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=8,"So, as we explained the different text"
cs-410_6_7_3,cs-410,6,7,"00:00:11,442","00:00:15,299",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,representation tends to
cs-410_6_7_4,cs-410,6,7,"00:00:16,560","00:00:19,780",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"In particular,"
cs-410_6_7_5,cs-410,6,7,"00:00:19,780","00:00:24,720",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,more deeper analysis results
cs-410_6_7_6,cs-410,6,7,"00:00:24,720","00:00:27,810",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,And that would open up a more
cs-410_6_7_7,cs-410,6,7,"00:00:29,520","00:00:33,780",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,opportunities and
cs-410_6_7_8,cs-410,6,7,"00:00:33,780","00:00:37,470",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"So, this table summarizes"
cs-410_6_7_9,cs-410,6,7,"00:00:37,470","00:00:39,800",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,So the first column shows
cs-410_6_7_10,cs-410,6,7,"00:00:39,800","00:00:44,820",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,The second visualizes the generality
cs-410_6_7_11,cs-410,6,7,"00:00:44,820","00:00:48,430",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,Meaning whether we can do this
cs-410_6_7_12,cs-410,6,7,"00:00:48,430","00:00:51,880",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,all the text data or only some of them.
cs-410_6_7_13,cs-410,6,7,"00:00:51,880","00:00:54,970",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,And the third column shows
cs-410_6_7_14,cs-410,6,7,"00:00:56,040","00:01:00,130",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,And the final column shows some
cs-410_6_7_15,cs-410,6,7,"00:01:00,130","00:01:04,670",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,can be achieved through this
cs-410_6_7_16,cs-410,6,7,"00:01:04,670","00:01:06,310",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,So let's take a look at them.
cs-410_6_7_17,cs-410,6,7,"00:01:06,310","00:01:12,180",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,So as a stream text can only be processed
cs-410_6_7_18,cs-410,6,7,"00:01:12,180","00:01:14,050",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,"It's very robust, it's general."
cs-410_6_7_19,cs-410,6,7,"00:01:15,100","00:01:17,690",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,And there was still some interesting
cs-410_6_7_20,cs-410,6,7,"00:01:17,690","00:01:18,290",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,at this level.
cs-410_6_7_21,cs-410,6,7,"00:01:18,290","00:01:20,380",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,"For example, compression of text."
cs-410_6_7_22,cs-410,6,7,"00:01:20,380","00:01:24,080",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=80,Doesn't necessarily need to
cs-410_6_7_23,cs-410,6,7,"00:01:24,080","00:01:27,270",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,Although knowing word boundaries
cs-410_6_7_24,cs-410,6,7,"00:01:28,540","00:01:32,470",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,Word base repetition is a very
cs-410_6_7_25,cs-410,6,7,"00:01:32,470","00:01:34,630",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,It's quite general and
cs-410_6_7_26,cs-410,6,7,"00:01:34,630","00:01:39,140",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"relatively robust, indicating they"
cs-410_6_7_27,cs-410,6,7,"00:01:39,140","00:01:44,480",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,"Such as word relation analysis,"
cs-410_6_7_28,cs-410,6,7,"00:01:44,480","00:01:48,930",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,And there are many applications that can
cs-410_6_7_29,cs-410,6,7,"00:01:48,930","00:01:54,930",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,"For example, thesaurus discovery has"
cs-410_6_7_30,cs-410,6,7,"00:01:54,930","00:02:00,550",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,And topic and
cs-410_6_7_31,cs-410,6,7,"00:02:00,550","00:02:03,360",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"And there are, for example, people"
cs-410_6_7_32,cs-410,6,7,"00:02:03,360","00:02:08,190",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,might be interesting in knowing the major
cs-410_6_7_33,cs-410,6,7,"00:02:08,190","00:02:12,730",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,And this can be the case
cs-410_6_7_34,cs-410,6,7,"00:02:12,730","00:02:18,500",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,And scientists want to know what are the
cs-410_6_7_35,cs-410,6,7,"00:02:18,500","00:02:22,950",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,Or customer service people might want to
cs-410_6_7_36,cs-410,6,7,"00:02:22,950","00:02:28,480",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,customers by mining their e-mail messages.
cs-410_6_7_37,cs-410,6,7,"00:02:28,480","00:02:33,850",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,And business intelligence
cs-410_6_7_38,cs-410,6,7,"00:02:33,850","00:02:38,090",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,understanding consumers' opinions about
cs-410_6_7_39,cs-410,6,7,"00:02:38,090","00:02:42,060",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,products to figure out what are the
cs-410_6_7_40,cs-410,6,7,"00:02:43,170","00:02:47,140",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,"And, in general, there are many"
cs-410_6_7_41,cs-410,6,7,"00:02:47,140","00:02:51,300",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,applications that can be enabled by
cs-410_6_7_42,cs-410,6,7,"00:02:53,720","00:02:58,550",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,"Now, moving down, we'll see we can"
cs-410_6_7_43,cs-410,6,7,"00:02:58,550","00:03:01,640",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"By adding syntactical structures,"
cs-410_6_7_44,cs-410,6,7,"00:03:01,640","00:03:03,890",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,syntactical graph analysis.
cs-410_6_7_45,cs-410,6,7,"00:03:03,890","00:03:09,490",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,We can use graph mining algorithms
cs-410_6_7_46,cs-410,6,7,"00:03:09,490","00:03:13,550",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,And some applications are related
cs-410_6_7_47,cs-410,6,7,"00:03:13,550","00:03:14,090",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,"For example,"
cs-410_6_7_48,cs-410,6,7,"00:03:14,090","00:03:18,440",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,stylistic analysis generally requires
cs-410_6_7_49,cs-410,6,7,"00:03:22,000","00:03:26,240",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,We can also generate
cs-410_6_7_50,cs-410,6,7,"00:03:26,240","00:03:32,090",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,And those are features that might help us
cs-410_6_7_51,cs-410,6,7,"00:03:32,090","00:03:37,320",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,categories by looking at the structures
cs-410_6_7_52,cs-410,6,7,"00:03:37,320","00:03:39,350",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,It can be more accurate.
cs-410_6_7_53,cs-410,6,7,"00:03:39,350","00:03:43,360",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,"For example,"
cs-410_6_7_54,cs-410,6,7,"00:03:45,120","00:03:49,298",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,different categories corresponding
cs-410_6_7_55,cs-410,6,7,"00:03:49,298","00:03:56,320",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,You want to figure out which of
cs-410_6_7_56,cs-410,6,7,"00:03:56,320","00:04:01,440",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,"this article, then you generally need"
cs-410_6_7_57,cs-410,6,7,"00:04:03,340","00:04:05,400",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,"When we add entities and relations,"
cs-410_6_7_58,cs-410,6,7,"00:04:05,400","00:04:09,690",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,then we can enable other techniques
cs-410_6_7_59,cs-410,6,7,"00:04:09,690","00:04:13,920",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,"answers, or information network and"
cs-410_6_7_60,cs-410,6,7,"00:04:13,920","00:04:20,956",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,And this analysis enable
cs-410_6_7_61,cs-410,6,7,"00:04:22,285","00:04:22,875",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,"For example,"
cs-410_6_7_62,cs-410,6,7,"00:04:22,875","00:04:27,525",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,discovery of all the knowledge and
cs-410_6_7_63,cs-410,6,7,"00:04:28,865","00:04:31,825",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,You can also use this level representation
cs-410_6_7_64,cs-410,6,7,"00:04:31,825","00:04:35,820",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=271,to integrate everything about
cs-410_6_7_65,cs-410,6,7,"00:04:37,520","00:04:40,280",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"Finally, when we add logical predicates,"
cs-410_6_7_66,cs-410,6,7,"00:04:40,280","00:04:44,330",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"that would enable large inference,"
cs-410_6_7_67,cs-410,6,7,"00:04:44,330","00:04:46,190",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,And this can be very useful for
cs-410_6_7_68,cs-410,6,7,"00:04:46,190","00:04:48,780",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,integrating analysis of
cs-410_6_7_69,cs-410,6,7,"00:04:50,190","00:04:53,560",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,"For example,"
cs-410_6_7_70,cs-410,6,7,"00:04:54,920","00:04:58,370",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,"extracted the information from text,"
cs-410_6_7_71,cs-410,6,7,"00:04:59,830","00:05:04,470",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,A good of example of application in this
cs-410_6_7_72,cs-410,6,7,"00:05:04,470","00:05:07,375",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,is a knowledge assistant for biologists.
cs-410_6_7_73,cs-410,6,7,"00:05:07,375","00:05:14,535",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,And this program that can help a biologist
cs-410_6_7_74,cs-410,6,7,"00:05:14,535","00:05:21,040",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,literature about a research problem such
cs-410_6_7_75,cs-410,6,7,"00:05:22,070","00:05:27,143",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,And the computer can make inferences
cs-410_6_7_76,cs-410,6,7,"00:05:27,143","00:05:32,490",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,about some of the hypothesis that
cs-410_6_7_77,cs-410,6,7,"00:05:32,490","00:05:36,110",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,"For example,"
cs-410_6_7_78,cs-410,6,7,"00:05:36,110","00:05:42,135",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,then the intelligent program can read the
cs-410_6_7_79,cs-410,6,7,"00:05:42,135","00:05:45,250",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,doing compiling and
cs-410_6_7_80,cs-410,6,7,"00:05:45,250","00:05:50,891",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,And then using a logic system to
cs-410_6_7_81,cs-410,6,7,"00:05:50,891","00:05:56,060",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,to researchers questioning about what
cs-410_6_7_82,cs-410,6,7,"00:05:57,990","00:06:01,240",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,So in order to support
cs-410_6_7_83,cs-410,6,7,"00:06:01,240","00:06:04,910",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,we need to go as far as
cs-410_6_7_84,cs-410,6,7,"00:06:04,910","00:06:10,585",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,"Now, this course is covering techniques"
cs-410_6_7_85,cs-410,6,7,"00:06:12,090","00:06:14,610",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,And these techniques are general and
cs-410_6_7_86,cs-410,6,7,"00:06:14,610","00:06:19,460",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,robust and that's more widely
cs-410_6_7_87,cs-410,6,7,"00:06:21,120","00:06:26,565",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,"In fact, in virtually all the text mining"
cs-410_6_7_88,cs-410,6,7,"00:06:26,565","00:06:32,368",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,representation and then techniques that
cs-410_6_7_89,cs-410,6,7,"00:06:35,909","00:06:39,652",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,But obviously all these other
cs-410_6_7_90,cs-410,6,7,"00:06:39,652","00:06:45,440",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,should be combined in order to support
cs-410_6_7_91,cs-410,6,7,"00:06:45,440","00:06:48,790",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,"So to summarize,"
cs-410_6_7_92,cs-410,6,7,"00:06:48,790","00:06:53,615",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,Text representation determines what
cs-410_6_7_93,cs-410,6,7,"00:06:53,615","00:06:57,908",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,And there are multiple ways to
cs-410_6_7_94,cs-410,6,7,"00:06:57,908","00:07:03,099",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,"syntactic structures, entity-relation"
cs-410_6_7_95,cs-410,6,7,"00:07:03,099","00:07:08,326",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,And these different
cs-410_6_7_96,cs-410,6,7,"00:07:08,326","00:07:13,540",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,be combined in real applications
cs-410_6_7_97,cs-410,6,7,"00:07:13,540","00:07:20,263",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=433,"For example, even if we cannot"
cs-410_6_7_98,cs-410,6,7,"00:07:20,263","00:07:25,380",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,"of syntactic structures, we can state"
cs-410_6_7_99,cs-410,6,7,"00:07:25,380","00:07:29,610",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=445,"And if we can recognize some entities,"
cs-410_6_7_100,cs-410,6,7,"00:07:29,610","00:07:32,660",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,So in general we want to
cs-410_6_7_101,cs-410,6,7,"00:07:34,570","00:07:37,210",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,And when different levels
cs-410_6_7_102,cs-410,6,7,"00:07:37,210","00:07:41,520",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,"we can enable a richer analysis,"
cs-410_6_7_103,cs-410,6,7,"00:07:42,830","00:07:46,610",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,This course however focuses
cs-410_6_7_104,cs-410,6,7,"00:07:46,610","00:07:52,170",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,Such techniques have also several
cs-410_6_7_105,cs-410,6,7,"00:07:52,170","00:07:55,460",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,"robust, so they are applicable"
cs-410_6_7_106,cs-410,6,7,"00:07:55,460","00:07:59,780",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,That's a big advantage over
cs-410_6_7_107,cs-410,6,7,"00:07:59,780","00:08:03,510",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,more fragile natural language
cs-410_6_7_108,cs-410,6,7,"00:08:03,510","00:08:07,680",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=483,"Secondly, it does not require"
cs-410_6_7_109,cs-410,6,7,"00:08:07,680","00:08:11,520",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,"sometimes, it does not"
cs-410_6_7_110,cs-410,6,7,"00:08:11,520","00:08:14,037",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,"So that's, again, an important benefit,"
cs-410_6_7_111,cs-410,6,7,"00:08:14,037","00:08:17,962",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,because that means that you can apply
cs-410_6_7_112,cs-410,6,7,"00:08:20,910","00:08:25,373",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,"Third, these techniques are actually"
cs-410_6_7_113,cs-410,6,7,"00:08:25,373","00:08:27,690",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,effective form in implications.
cs-410_6_7_114,cs-410,6,7,"00:08:29,210","00:08:32,180",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,Although not all of course
cs-410_6_7_115,cs-410,6,7,"00:08:34,340","00:08:38,460",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,Now they are very effective
cs-410_6_7_116,cs-410,6,7,"00:08:38,460","00:08:44,610",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,are invented by humans as basically
cs-410_6_7_117,cs-410,6,7,"00:08:45,610","00:08:51,120",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=525,So they are actually quite sufficient for
cs-410_6_7_118,cs-410,6,7,"00:08:53,680","00:09:00,310",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,So that makes this kind of word-based
cs-410_6_7_119,cs-410,6,7,"00:09:00,310","00:09:05,010",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,"And finally, such a word-based"
cs-410_6_7_120,cs-410,6,7,"00:09:05,010","00:09:11,690",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,by such a representation can be combined
cs-410_6_7_121,cs-410,6,7,"00:09:14,020","00:09:15,191",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,So they're not competing with each other.
cs-410_6_7_122,cs-410,6,7,"00:09:15,191","00:09:25,191",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,[MUSIC]
cs-410_6_8_1,cs-410,6,8,"00:00:00,000","00:00:02,974",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_6_8_2,cs-410,6,8,"00:00:07,749","00:00:11,320",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about topic mining and
cs-410_6_8_3,cs-410,6,8,"00:00:12,760","00:00:17,130",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,We're going to talk about
cs-410_6_8_4,cs-410,6,8,"00:00:17,130","00:00:20,700",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,This is a slide that you have
cs-410_6_8_5,cs-410,6,8,"00:00:20,700","00:00:25,120",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,where we define the task of
cs-410_6_8_6,cs-410,6,8,"00:00:25,120","00:00:30,700",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"We also raised the question, how do"
cs-410_6_8_7,cs-410,6,8,"00:00:31,780","00:00:36,020",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,"So in this lecture, we're going to"
cs-410_6_8_8,cs-410,6,8,"00:00:36,020","00:00:37,780",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,that's our initial idea.
cs-410_6_8_9,cs-410,6,8,"00:00:37,780","00:00:40,980",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=37,Our idea here is defining
cs-410_6_8_10,cs-410,6,8,"00:00:42,020","00:00:44,200",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,A term can be a word or a phrase.
cs-410_6_8_11,cs-410,6,8,"00:00:45,240","00:00:49,500",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,"And in general,"
cs-410_6_8_12,cs-410,6,8,"00:00:49,500","00:00:54,200",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,So our first thought is just
cs-410_6_8_13,cs-410,6,8,"00:00:54,200","00:00:58,820",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,"For example, we might have terms"
cs-410_6_8_14,cs-410,6,8,"00:00:58,820","00:00:59,440",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,as you see here.
cs-410_6_8_15,cs-410,6,8,"00:00:59,440","00:01:02,603",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=59,"Now if we define a topic in this way,"
cs-410_6_8_16,cs-410,6,8,"00:01:02,603","00:01:09,200",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,we can then analyze the coverage
cs-410_6_8_17,cs-410,6,8,"00:01:09,200","00:01:10,510",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,"Here for example,"
cs-410_6_8_18,cs-410,6,8,"00:01:10,510","00:01:15,560",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,we might want to discover to what
cs-410_6_8_19,cs-410,6,8,"00:01:15,560","00:01:21,260",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,And we found that 30% of the content
cs-410_6_8_20,cs-410,6,8,"00:01:21,260","00:01:23,730",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,"And 12% is about the travel, etc."
cs-410_6_8_21,cs-410,6,8,"00:01:23,730","00:01:28,880",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,We might also discover document
cs-410_6_8_22,cs-410,6,8,"00:01:28,880","00:01:31,240",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"So the coverage is zero, etc."
cs-410_6_8_23,cs-410,6,8,"00:01:32,630","00:01:39,040",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"So now, of course,"
cs-410_6_8_24,cs-410,6,8,"00:01:39,040","00:01:42,900",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,"topic mining and analysis,"
cs-410_6_8_25,cs-410,6,8,"00:01:42,900","00:01:44,960",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,One is to discover the topics.
cs-410_6_8_26,cs-410,6,8,"00:01:44,960","00:01:48,110",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,And the second is to analyze coverage.
cs-410_6_8_27,cs-410,6,8,"00:01:48,110","00:01:51,550",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,So let's first think
cs-410_6_8_28,cs-410,6,8,"00:01:51,550","00:01:55,080",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,topics if we represent
cs-410_6_8_29,cs-410,6,8,"00:01:55,080","00:01:59,390",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,So that means we need to mine k
cs-410_6_8_30,cs-410,6,8,"00:02:01,050","00:02:04,080",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"Now there are, of course,"
cs-410_6_8_31,cs-410,6,8,"00:02:05,670","00:02:08,617",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,And we're going to talk about
cs-410_6_8_32,cs-410,6,8,"00:02:08,617","00:02:10,750",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,which is also likely effective.
cs-410_6_8_33,cs-410,6,8,"00:02:10,750","00:02:11,641",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,"So first of all,"
cs-410_6_8_34,cs-410,6,8,"00:02:11,641","00:02:16,655",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=131,we're going to parse the text data in
cs-410_6_8_35,cs-410,6,8,"00:02:16,655","00:02:20,665",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,Here candidate terms can be words or
cs-410_6_8_36,cs-410,6,8,"00:02:20,665","00:02:25,475",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,Let's say the simplest solution is
cs-410_6_8_37,cs-410,6,8,"00:02:25,475","00:02:29,145",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=145,These words then become candidate topics.
cs-410_6_8_38,cs-410,6,8,"00:02:29,145","00:02:32,790",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=149,Then we're going to design a scoring
cs-410_6_8_39,cs-410,6,8,"00:02:32,790","00:02:33,650",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,is as a topic.
cs-410_6_8_40,cs-410,6,8,"00:02:35,460","00:02:37,150",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,So how can we design such a function?
cs-410_6_8_41,cs-410,6,8,"00:02:37,150","00:02:40,140",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,Well there are many things
cs-410_6_8_42,cs-410,6,8,"00:02:40,140","00:02:44,180",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=160,"For example, we can use pure statistics"
cs-410_6_8_43,cs-410,6,8,"00:02:45,550","00:02:48,820",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,"Intuitively, we would like to"
cs-410_6_8_44,cs-410,6,8,"00:02:48,820","00:02:53,950",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,meaning terms that can represent
cs-410_6_8_45,cs-410,6,8,"00:02:53,950","00:02:58,610",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=173,So that would mean we want
cs-410_6_8_46,cs-410,6,8,"00:02:58,610","00:03:03,990",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,"However, if we simply use the frequency"
cs-410_6_8_47,cs-410,6,8,"00:03:03,990","00:03:07,982",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,then the highest scored terms
cs-410_6_8_48,cs-410,6,8,"00:03:07,982","00:03:10,876",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,"functional terms like the, etc."
cs-410_6_8_49,cs-410,6,8,"00:03:10,876","00:03:13,510",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,Those terms occur very frequently English.
cs-410_6_8_50,cs-410,6,8,"00:03:14,650","00:03:19,340",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,So we also want to avoid having
cs-410_6_8_51,cs-410,6,8,"00:03:19,340","00:03:22,150",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,we want to penalize such words.
cs-410_6_8_52,cs-410,6,8,"00:03:22,150","00:03:26,480",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"But in general, we would like to favor"
cs-410_6_8_53,cs-410,6,8,"00:03:26,480","00:03:28,020",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,not so frequent.
cs-410_6_8_54,cs-410,6,8,"00:03:28,020","00:03:34,030",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=208,So a particular approach could be based
cs-410_6_8_55,cs-410,6,8,"00:03:35,140","00:03:37,230",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=215,And TF stands for term frequency.
cs-410_6_8_56,cs-410,6,8,"00:03:37,230","00:03:40,420",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,IDF stands for inverse document frequency.
cs-410_6_8_57,cs-410,6,8,"00:03:40,420","00:03:43,310",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=220,We talked about some of these
cs-410_6_8_58,cs-410,6,8,"00:03:43,310","00:03:48,090",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,ideas in the lectures about
cs-410_6_8_59,cs-410,6,8,"00:03:48,090","00:03:50,766",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"So these are statistical methods,"
cs-410_6_8_60,cs-410,6,8,"00:03:50,766","00:03:56,280",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,meaning that the function is
cs-410_6_8_61,cs-410,6,8,"00:03:56,280","00:03:59,080",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=236,So the scoring function
cs-410_6_8_62,cs-410,6,8,"00:03:59,080","00:04:02,890",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"It can be applied to any language,"
cs-410_6_8_63,cs-410,6,8,"00:04:02,890","00:04:06,650",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,But when we apply such a approach
cs-410_6_8_64,cs-410,6,8,"00:04:06,650","00:04:12,020",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,we might also be able to leverage
cs-410_6_8_65,cs-410,6,8,"00:04:12,020","00:04:16,815",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=252,"For example, in news we might favor"
cs-410_6_8_66,cs-410,6,8,"00:04:16,815","00:04:21,340",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,We might want to favor title
cs-410_6_8_67,cs-410,6,8,"00:04:21,340","00:04:26,100",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,use the title to describe
cs-410_6_8_68,cs-410,6,8,"00:04:27,750","00:04:32,480",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=267,"If we're dealing with tweets,"
cs-410_6_8_69,cs-410,6,8,"00:04:32,480","00:04:37,430",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=272,which are invented to denote topics.
cs-410_6_8_70,cs-410,6,8,"00:04:37,430","00:04:43,630",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,"So naturally, hashtags can be good"
cs-410_6_8_71,cs-410,6,8,"00:04:44,780","00:04:50,430",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,"Anyway, after we have this design"
cs-410_6_8_72,cs-410,6,8,"00:04:50,430","00:04:55,960",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,the k topical terms by simply picking
cs-410_6_8_73,cs-410,6,8,"00:04:55,960","00:04:57,240",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"Now, of course,"
cs-410_6_8_74,cs-410,6,8,"00:04:57,240","00:05:02,040",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,we might encounter situation where the
cs-410_6_8_75,cs-410,6,8,"00:05:02,040","00:05:06,910",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,"They're semantically similar, or"
cs-410_6_8_76,cs-410,6,8,"00:05:06,910","00:05:08,860",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,So that's not desirable.
cs-410_6_8_77,cs-410,6,8,"00:05:08,860","00:05:12,280",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,So we also want to have coverage over
cs-410_6_8_78,cs-410,6,8,"00:05:12,280","00:05:15,080",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,So we would like to remove redundancy.
cs-410_6_8_79,cs-410,6,8,"00:05:15,080","00:05:19,600",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,And one way to do that is
cs-410_6_8_80,cs-410,6,8,"00:05:19,600","00:05:24,450",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,which is sometimes called a maximal
cs-410_6_8_81,cs-410,6,8,"00:05:24,450","00:05:29,330",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,"Basically, the idea is to go down"
cs-410_6_8_82,cs-410,6,8,"00:05:29,330","00:05:34,380",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,function and gradually take terms
cs-410_6_8_83,cs-410,6,8,"00:05:34,380","00:05:36,840",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,"The first term, of course, will be picked."
cs-410_6_8_84,cs-410,6,8,"00:05:36,840","00:05:40,500",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=336,"When we pick the next term, we're"
cs-410_6_8_85,cs-410,6,8,"00:05:40,500","00:05:45,120",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,been picked and try to avoid
cs-410_6_8_86,cs-410,6,8,"00:05:45,120","00:05:50,610",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,So while we are considering
cs-410_6_8_87,cs-410,6,8,"00:05:50,610","00:05:54,260",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,we are also considering
cs-410_6_8_88,cs-410,6,8,"00:05:54,260","00:05:56,970",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,with respect to the terms
cs-410_6_8_89,cs-410,6,8,"00:05:58,090","00:06:02,933",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=358,"And with some thresholding,"
cs-410_6_8_90,cs-410,6,8,"00:06:02,933","00:06:08,330",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,the redundancy removal and
cs-410_6_8_91,cs-410,6,8,"00:06:08,330","00:06:11,990",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,"Okay, so"
cs-410_6_8_92,cs-410,6,8,"00:06:11,990","00:06:17,550",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,And those can be regarded as the topics
cs-410_6_8_93,cs-410,6,8,"00:06:17,550","00:06:21,980",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,"Next, let's think about how we're going"
cs-410_6_8_94,cs-410,6,8,"00:06:23,430","00:06:26,971",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,"So looking at this picture,"
cs-410_6_8_95,cs-410,6,8,"00:06:26,971","00:06:28,130",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,these topics.
cs-410_6_8_96,cs-410,6,8,"00:06:28,130","00:06:31,190",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,And now suppose you are give a document.
cs-410_6_8_97,cs-410,6,8,"00:06:31,190","00:06:35,040",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,How should we pick out coverage
cs-410_6_8_98,cs-410,6,8,"00:06:36,660","00:06:42,690",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"Well, one approach can be to simply"
cs-410_6_8_99,cs-410,6,8,"00:06:42,690","00:06:46,770",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,"So for example, sports might have occurred"
cs-410_6_8_100,cs-410,6,8,"00:06:46,770","00:06:49,570",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,"travel occurred twice, etc."
cs-410_6_8_101,cs-410,6,8,"00:06:49,570","00:06:54,420",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,And then we can just normalize these
cs-410_6_8_102,cs-410,6,8,"00:06:54,420","00:06:56,570",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,probability for each topic.
cs-410_6_8_103,cs-410,6,8,"00:06:56,570","00:07:01,780",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=416,"So in general, the formula would"
cs-410_6_8_104,cs-410,6,8,"00:07:01,780","00:07:05,240",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,all the terms that represent the topics.
cs-410_6_8_105,cs-410,6,8,"00:07:05,240","00:07:10,220",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,And then simply normalize them so
cs-410_6_8_106,cs-410,6,8,"00:07:10,220","00:07:13,480",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,topic in the document would add to one.
cs-410_6_8_107,cs-410,6,8,"00:07:15,120","00:07:21,200",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,This forms a distribution of the topics
cs-410_6_8_108,cs-410,6,8,"00:07:21,200","00:07:26,560",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,of different topics in the document.
cs-410_6_8_109,cs-410,6,8,"00:07:26,560","00:07:30,100",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=446,"Now, as always,"
cs-410_6_8_110,cs-410,6,8,"00:07:30,100","00:07:34,940",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=450,"solving problem, we have to ask"
cs-410_6_8_111,cs-410,6,8,"00:07:34,940","00:07:37,180",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,Or is this the best way
cs-410_6_8_112,cs-410,6,8,"00:07:38,690","00:07:41,110",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,So now let's examine this approach.
cs-410_6_8_113,cs-410,6,8,"00:07:41,110","00:07:44,940",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=461,"In general,"
cs-410_6_8_114,cs-410,6,8,"00:07:46,010","00:07:50,280",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,by using actual data sets and
cs-410_6_8_115,cs-410,6,8,"00:07:52,360","00:07:57,340",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,"Well, in this case let's take"
cs-410_6_8_116,cs-410,6,8,"00:07:57,340","00:08:03,260",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,And we have a text document that's
cs-410_6_8_117,cs-410,6,8,"00:08:04,800","00:08:07,700",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,"So in terms of the content,"
cs-410_6_8_118,cs-410,6,8,"00:08:08,950","00:08:14,600",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,But if we simply count these
cs-410_6_8_119,cs-410,6,8,"00:08:14,600","00:08:19,070",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,we will find that the word sports
cs-410_6_8_120,cs-410,6,8,"00:08:19,070","00:08:21,420",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,even though the content
cs-410_6_8_121,cs-410,6,8,"00:08:22,520","00:08:25,750",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,So the count of sports is zero.
cs-410_6_8_122,cs-410,6,8,"00:08:25,750","00:08:31,939",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,That means the coverage of sports
cs-410_6_8_123,cs-410,6,8,"00:08:31,939","00:08:36,723",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"Now of course,"
cs-410_6_8_124,cs-410,6,8,"00:08:36,723","00:08:40,980",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,the document and
cs-410_6_8_125,cs-410,6,8,"00:08:40,980","00:08:42,230",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,And that's okay.
cs-410_6_8_126,cs-410,6,8,"00:08:42,230","00:08:47,257",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,But sports certainly is not okay because
cs-410_6_8_127,cs-410,6,8,"00:08:47,257","00:08:49,150",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,So this estimate has problem.
cs-410_6_8_128,cs-410,6,8,"00:08:50,880","00:08:56,050",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,"What's worse, the term travel"
cs-410_6_8_129,cs-410,6,8,"00:08:56,050","00:08:59,940",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,So when we estimate the coverage
cs-410_6_8_130,cs-410,6,8,"00:08:59,940","00:09:02,140",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,we have got a non-zero count.
cs-410_6_8_131,cs-410,6,8,"00:09:02,140","00:09:05,000",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,So its estimated coverage
cs-410_6_8_132,cs-410,6,8,"00:09:05,000","00:09:07,770",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,So this obviously is also not desirable.
cs-410_6_8_133,cs-410,6,8,"00:09:08,800","00:09:13,910",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,So this simple example illustrates
cs-410_6_8_134,cs-410,6,8,"00:09:13,910","00:09:17,704",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=553,"First, when we count what"
cs-410_6_8_135,cs-410,6,8,"00:09:17,704","00:09:20,460",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,we also need to consider related words.
cs-410_6_8_136,cs-410,6,8,"00:09:20,460","00:09:24,285",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,We can't simply just count
cs-410_6_8_137,cs-410,6,8,"00:09:24,285","00:09:26,440",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"In this case, it did not occur at all."
cs-410_6_8_138,cs-410,6,8,"00:09:26,440","00:09:31,340",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,But there are many related words
cs-410_6_8_139,cs-410,6,8,"00:09:31,340","00:09:33,860",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,So we need to count
cs-410_6_8_140,cs-410,6,8,"00:09:33,860","00:09:38,910",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=573,The second problem is that a word
cs-410_6_8_141,cs-410,6,8,"00:09:38,910","00:09:42,900",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=578,So here it probably means
cs-410_6_8_142,cs-410,6,8,"00:09:42,900","00:09:47,600",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,we can imagine it might also
cs-410_6_8_143,cs-410,6,8,"00:09:47,600","00:09:53,120",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,"So in that case, the star might actually"
cs-410_6_8_144,cs-410,6,8,"00:09:54,210","00:09:56,360",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,So we need to deal with that as well.
cs-410_6_8_145,cs-410,6,8,"00:09:56,360","00:10:02,325",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,"Finally, a main restriction of this"
cs-410_6_8_146,cs-410,6,8,"00:10:02,325","00:10:08,520",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,"term to describe the topic, so it cannot"
cs-410_6_8_147,cs-410,6,8,"00:10:08,520","00:10:12,040",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,"For example, a very specialized"
cs-410_6_8_148,cs-410,6,8,"00:10:12,040","00:10:15,210",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,describe by using just a word or
cs-410_6_8_149,cs-410,6,8,"00:10:15,210","00:10:17,150",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=615,We need to use more words.
cs-410_6_8_150,cs-410,6,8,"00:10:17,150","00:10:20,760",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,So this example illustrates
cs-410_6_8_151,cs-410,6,8,"00:10:20,760","00:10:23,310",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,this approach of treating a term as topic.
cs-410_6_8_152,cs-410,6,8,"00:10:23,310","00:10:26,725",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,"First, it lacks expressive power."
cs-410_6_8_153,cs-410,6,8,"00:10:26,725","00:10:30,729",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,Meaning that it can only represent
cs-410_6_8_154,cs-410,6,8,"00:10:30,729","00:10:36,035",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,it cannot represent the complicated topics
cs-410_6_8_155,cs-410,6,8,"00:10:37,055","00:10:40,660",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=637,"Second, it's incomplete"
cs-410_6_8_156,cs-410,6,8,"00:10:40,660","00:10:44,930",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,meaning that the topic itself
cs-410_6_8_157,cs-410,6,8,"00:10:44,930","00:10:48,820",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,It does not suggest what other
cs-410_6_8_158,cs-410,6,8,"00:10:48,820","00:10:52,370",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=648,"Even if we're talking about sports,"
cs-410_6_8_159,cs-410,6,8,"00:10:52,370","00:10:57,060",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,So it does not allow us to easily
cs-410_6_8_160,cs-410,6,8,"00:10:57,060","00:10:59,200",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,conversion to coverage of this topic.
cs-410_6_8_161,cs-410,6,8,"00:10:59,200","00:11:02,410",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,"Finally, there is this problem"
cs-410_6_8_162,cs-410,6,8,"00:11:02,410","00:11:05,862",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,A topical term or
cs-410_6_8_163,cs-410,6,8,"00:11:05,862","00:11:08,540",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=665,"For example,"
cs-410_6_8_164,cs-410,6,8,"00:11:10,570","00:11:14,125",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,"So in the next lecture,"
cs-410_6_8_165,cs-410,6,8,"00:11:14,125","00:11:18,806",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,about how to solve
cs-410_6_8_166,cs-410,6,8,"00:11:18,806","00:11:28,806",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,[MUSIC]
cs-410_7_4_1,cs-410,7,4,"00:00:00,006","00:00:03,253",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_7_4_2,cs-410,7,4,"00:00:13,295","00:00:15,326",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,So let's plug in these model masses
cs-410_7_4_3,cs-410,7,4,"00:00:15,326","00:00:18,610",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,into the ranking function to
cs-410_7_4_4,cs-410,7,4,"00:00:18,610","00:00:20,780",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,This is a general smoothing.
cs-410_7_4_5,cs-410,7,4,"00:00:20,780","00:00:24,570",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=20,So a general ranking function for
cs-410_7_4_6,cs-410,7,4,"00:00:24,570","00:00:26,500",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,you have seen this before.
cs-410_7_4_7,cs-410,7,4,"00:00:28,060","00:00:32,550",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,And now we have a very specific smoothing
cs-410_7_4_8,cs-410,7,4,"00:00:33,690","00:00:39,190",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,So now let's see what what's a value for
cs-410_7_4_9,cs-410,7,4,"00:00:40,450","00:00:42,900",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,And what's the value for p sub c here?
cs-410_7_4_10,cs-410,7,4,"00:00:42,900","00:00:46,930",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,"Right, so we may need to decide this"
cs-410_7_4_11,cs-410,7,4,"00:00:46,930","00:00:50,470",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,in order to figure out the exact
cs-410_7_4_12,cs-410,7,4,"00:00:50,470","00:00:52,598",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,And we also need to figure
cs-410_7_4_13,cs-410,7,4,"00:00:52,598","00:00:55,910",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,So let's see.
cs-410_7_4_14,cs-410,7,4,"00:00:55,910","00:01:00,666",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,"Well this ratio is basically this,"
cs-410_7_4_15,cs-410,7,4,"00:01:00,666","00:01:05,315",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,"here, this is the probability"
cs-410_7_4_16,cs-410,7,4,"00:01:05,315","00:01:09,330",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,and this is the probability
cs-410_7_4_17,cs-410,7,4,"00:01:09,330","00:01:14,935",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,in other words basically 11
cs-410_7_4_18,cs-410,7,4,"00:01:14,935","00:01:18,530",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,"this, so it's easy to see that."
cs-410_7_4_19,cs-410,7,4,"00:01:18,530","00:01:21,681",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,This can be then rewritten as this.
cs-410_7_4_20,cs-410,7,4,"00:01:21,681","00:01:24,500",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,Very simple.
cs-410_7_4_21,cs-410,7,4,"00:01:24,500","00:01:26,810",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,So we can plug this into here.
cs-410_7_4_22,cs-410,7,4,"00:01:28,650","00:01:30,710",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,"And then here, what's the value for alpha?"
cs-410_7_4_23,cs-410,7,4,"00:01:30,710","00:01:31,660",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,What do you think?
cs-410_7_4_24,cs-410,7,4,"00:01:31,660","00:01:35,250",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,"So it would be just lambda, right?"
cs-410_7_4_25,cs-410,7,4,"00:01:38,250","00:01:43,900",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,And what would happen if we plug in
cs-410_7_4_26,cs-410,7,4,"00:01:43,900","00:01:45,350",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,What can we say about this?
cs-410_7_4_27,cs-410,7,4,"00:01:47,940","00:01:49,640",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,Does it depend on the document?
cs-410_7_4_28,cs-410,7,4,"00:01:50,660","00:01:52,170",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,"No, so it can be ignored."
cs-410_7_4_29,cs-410,7,4,"00:01:53,570","00:01:55,040",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,Right?
cs-410_7_4_30,cs-410,7,4,"00:01:55,040","00:01:58,690",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,So we'll end up having this
cs-410_7_4_31,cs-410,7,4,"00:02:00,520","00:02:02,690",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"And in this case you can easy to see,"
cs-410_7_4_32,cs-410,7,4,"00:02:02,690","00:02:07,780",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=122,this a precisely a vector space
cs-410_7_4_33,cs-410,7,4,"00:02:07,780","00:02:13,480",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"a sum over all the matched query terms,"
cs-410_7_4_34,cs-410,7,4,"00:02:13,480","00:02:16,140",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,What do you think is a element
cs-410_7_4_35,cs-410,7,4,"00:02:18,670","00:02:20,200",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,"Well it's this, right."
cs-410_7_4_36,cs-410,7,4,"00:02:20,200","00:02:23,210",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=140,So that's our document left element.
cs-410_7_4_37,cs-410,7,4,"00:02:23,210","00:02:29,210",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,And let's further examine what's
cs-410_7_4_38,cs-410,7,4,"00:02:30,370","00:02:32,440",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,Well one plus this.
cs-410_7_4_39,cs-410,7,4,"00:02:32,440","00:02:36,630",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,"So it's going to be nonnegative,"
cs-410_7_4_40,cs-410,7,4,"00:02:36,630","00:02:37,850",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,"it's going to be at least 1, right?"
cs-410_7_4_41,cs-410,7,4,"00:02:39,450","00:02:42,900",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,"And these, this is a parameter,"
cs-410_7_4_42,cs-410,7,4,"00:02:42,900","00:02:44,340",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,And let's look at this.
cs-410_7_4_43,cs-410,7,4,"00:02:44,340","00:02:45,480",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,Now this is a TF.
cs-410_7_4_44,cs-410,7,4,"00:02:45,480","00:02:48,070",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,Now we see very clearly
cs-410_7_4_45,cs-410,7,4,"00:02:49,250","00:02:54,080",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"And the larger the count is,"
cs-410_7_4_46,cs-410,7,4,"00:02:54,080","00:02:57,080",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,"We also see IDF weighting,"
cs-410_7_4_47,cs-410,7,4,"00:02:58,720","00:03:00,996",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,And we see docking the lan's
cs-410_7_4_48,cs-410,7,4,"00:03:00,996","00:03:03,270",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,So all these heuristics
cs-410_7_4_49,cs-410,7,4,"00:03:04,532","00:03:08,480",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,What's interesting that
cs-410_7_4_50,cs-410,7,4,"00:03:08,480","00:03:12,330",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=188,weighting function automatically
cs-410_7_4_51,cs-410,7,4,"00:03:12,330","00:03:14,270",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,"Whereas in the vector space model,"
cs-410_7_4_52,cs-410,7,4,"00:03:14,270","00:03:19,330",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,we had to go through those heuristic
cs-410_7_4_53,cs-410,7,4,"00:03:19,330","00:03:21,880",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,And in this case note that
cs-410_7_4_54,cs-410,7,4,"00:03:21,880","00:03:25,120",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,And when you see whether this
cs-410_7_4_55,cs-410,7,4,"00:03:26,690","00:03:31,050",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,All right so what do you think
cs-410_7_4_56,cs-410,7,4,"00:03:31,050","00:03:33,320",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,This is a math of document.
cs-410_7_4_57,cs-410,7,4,"00:03:33,320","00:03:37,340",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,"Total number of words,"
cs-410_7_4_58,cs-410,7,4,"00:03:38,400","00:03:42,727",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"given by the collection, right?"
cs-410_7_4_59,cs-410,7,4,"00:03:42,727","00:03:48,090",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,So this actually can be interpreted
cs-410_7_4_60,cs-410,7,4,"00:03:48,090","00:03:53,730",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,"If we're going to draw, a word,"
cs-410_7_4_61,cs-410,7,4,"00:03:53,730","00:03:57,980",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,"And, we're going to draw as many as"
cs-410_7_4_62,cs-410,7,4,"00:03:59,310","00:04:02,940",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"If you do that,"
cs-410_7_4_63,cs-410,7,4,"00:04:02,940","00:04:06,950",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,would be precisely given
cs-410_7_4_64,cs-410,7,4,"00:04:08,240","00:04:14,400",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=248,"So, this ratio basically,"
cs-410_7_4_65,cs-410,7,4,"00:04:15,860","00:04:21,280",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,The actual count of the word in the
cs-410_7_4_66,cs-410,7,4,"00:04:21,280","00:04:29,570",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,product if the word is in fact following
cs-410_7_4_67,cs-410,7,4,"00:04:29,570","00:04:33,250",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,And if this counter is larger than
cs-410_7_4_68,cs-410,7,4,"00:04:33,250","00:04:34,789",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,this ratio would be larger than one.
cs-410_7_4_69,cs-410,7,4,"00:04:37,100","00:04:40,460",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,So that's actually a very
cs-410_7_4_70,cs-410,7,4,"00:04:40,460","00:04:43,930",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,"It's very natural and intuitive,"
cs-410_7_4_71,cs-410,7,4,"00:04:45,240","00:04:49,580",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,And this is one advantage of using
cs-410_7_4_72,cs-410,7,4,"00:04:49,580","00:04:53,240",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=289,where we have made explicit assumptions.
cs-410_7_4_73,cs-410,7,4,"00:04:53,240","00:04:56,490",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"And, we know precisely why"
cs-410_7_4_74,cs-410,7,4,"00:04:56,490","00:04:58,800",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"And, why we have these probabilities here."
cs-410_7_4_75,cs-410,7,4,"00:05:00,280","00:05:04,290",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,"And, we also have a formula that"
cs-410_7_4_76,cs-410,7,4,"00:05:04,290","00:05:07,190",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,does TF-IDF weighting and
cs-410_7_4_77,cs-410,7,4,"00:05:09,010","00:05:11,440",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"Let's look at the,"
cs-410_7_4_78,cs-410,7,4,"00:05:11,440","00:05:16,852",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,It's very similar to
cs-410_7_4_79,cs-410,7,4,"00:05:16,852","00:05:21,540",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,"In this case,"
cs-410_7_4_80,cs-410,7,4,"00:05:21,540","00:05:27,660",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,that's different from
cs-410_7_4_81,cs-410,7,4,"00:05:27,660","00:05:30,660",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,But the format looks very similar.
cs-410_7_4_82,cs-410,7,4,"00:05:30,660","00:05:32,570",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=330,The form of the function
cs-410_7_4_83,cs-410,7,4,"00:05:34,540","00:05:36,730",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,So we still have linear operation here.
cs-410_7_4_84,cs-410,7,4,"00:05:38,090","00:05:40,130",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,"And when we compute this ratio,"
cs-410_7_4_85,cs-410,7,4,"00:05:40,130","00:05:45,460",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,one will find that is that
cs-410_7_4_86,cs-410,7,4,"00:05:46,930","00:05:51,620",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,And what's interesting here is that we
cs-410_7_4_87,cs-410,7,4,"00:05:51,620","00:05:54,440",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,We're comparing the actual count.
cs-410_7_4_88,cs-410,7,4,"00:05:54,440","00:05:59,400",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,Which is the expected account of the world
cs-410_7_4_89,cs-410,7,4,"00:05:59,400","00:06:02,660",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,the collection world probability.
cs-410_7_4_90,cs-410,7,4,"00:06:02,660","00:06:07,266",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,So note that it's interesting we don't
cs-410_7_4_91,cs-410,7,4,"00:06:07,266","00:06:08,910",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,lighter in the JMs model.
cs-410_7_4_92,cs-410,7,4,"00:06:08,910","00:06:13,880",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,All right so this of course
cs-410_7_4_93,cs-410,7,4,"00:06:15,290","00:06:18,200",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,"So you might wonder, so"
cs-410_7_4_94,cs-410,7,4,"00:06:18,200","00:06:23,650",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,Interestingly the docking lens
cs-410_7_4_95,cs-410,7,4,"00:06:23,650","00:06:26,850",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,this would be plugged into this part.
cs-410_7_4_96,cs-410,7,4,"00:06:26,850","00:06:31,860",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,As a result what we get is
cs-410_7_4_97,cs-410,7,4,"00:06:31,860","00:06:35,239",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,this is again a sum over
cs-410_7_4_98,cs-410,7,4,"00:06:36,290","00:06:40,050",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"And we're against the queer,"
cs-410_7_4_99,cs-410,7,4,"00:06:41,410","00:06:45,425",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=401,And you can interpret this as
cs-410_7_4_100,cs-410,7,4,"00:06:45,425","00:06:48,700",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,but this is no longer
cs-410_7_4_101,cs-410,7,4,"00:06:50,100","00:06:55,165",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=410,"Because we have this part,"
cs-410_7_4_102,cs-410,7,4,"00:06:55,165","00:06:57,810",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,right?
cs-410_7_4_103,cs-410,7,4,"00:06:57,810","00:07:01,510",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,So that just means if
cs-410_7_4_104,cs-410,7,4,"00:07:01,510","00:07:05,160",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,we have to take a sum over
cs-410_7_4_105,cs-410,7,4,"00:07:05,160","00:07:09,270",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,then do some adjustment of
cs-410_7_4_106,cs-410,7,4,"00:07:11,510","00:07:15,974",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,"But it's still, it's still clear"
cs-410_7_4_107,cs-410,7,4,"00:07:15,974","00:07:19,765",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,modulation because this lens
cs-410_7_4_108,cs-410,7,4,"00:07:19,765","00:07:23,237",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=439,a longer document will
cs-410_7_4_109,cs-410,7,4,"00:07:23,237","00:07:27,600",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,And we can also see it has tf here and
cs-410_7_4_110,cs-410,7,4,"00:07:27,600","00:07:32,038",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,Only that this time the form of the
cs-410_7_4_111,cs-410,7,4,"00:07:32,038","00:07:34,580",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,in JMs one.
cs-410_7_4_112,cs-410,7,4,"00:07:34,580","00:07:39,780",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,But intuitively it still implements TFIDF
cs-410_7_4_113,cs-410,7,4,"00:07:39,780","00:07:44,340",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,the form of the function is dictated
cs-410_7_4_114,cs-410,7,4,"00:07:44,340","00:07:45,938",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=464,assumptions that we have made.
cs-410_7_4_115,cs-410,7,4,"00:07:45,938","00:07:50,420",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,Now there are also
cs-410_7_4_116,cs-410,7,4,"00:07:50,420","00:07:53,600",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=470,"And that is, there's no guarantee"
cs-410_7_4_117,cs-410,7,4,"00:07:53,600","00:07:55,800",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,of the formula will actually work well.
cs-410_7_4_118,cs-410,7,4,"00:07:55,800","00:08:01,037",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,"So if we look about at this geo function,"
cs-410_7_4_119,cs-410,7,4,"00:08:01,037","00:08:06,860",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,rendition for example it's unclear whether
cs-410_7_4_120,cs-410,7,4,"00:08:06,860","00:08:13,110",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,Unfortunately we can see here there
cs-410_7_4_121,cs-410,7,4,"00:08:13,110","00:08:17,580",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"So we do have also the,"
cs-410_7_4_122,cs-410,7,4,"00:08:17,580","00:08:20,986",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,So we do have the sublinear
cs-410_7_4_123,cs-410,7,4,"00:08:20,986","00:08:23,320",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,we do not intentionally do that.
cs-410_7_4_124,cs-410,7,4,"00:08:23,320","00:08:27,750",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,That means there's no guarantee that
cs-410_7_4_125,cs-410,7,4,"00:08:27,750","00:08:31,800",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,"Suppose we don't have logarithm,"
cs-410_7_4_126,cs-410,7,4,"00:08:31,800","00:08:35,810",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"As we discussed before, perhaps"
cs-410_7_4_127,cs-410,7,4,"00:08:35,810","00:08:40,870",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,So that's an example of the gap
cs-410_7_4_128,cs-410,7,4,"00:08:40,870","00:08:43,080",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=520,"the relevance that we have to model,"
cs-410_7_4_129,cs-410,7,4,"00:08:43,080","00:08:48,720",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,which is really a subject
cs-410_7_4_130,cs-410,7,4,"00:08:50,640","00:08:53,390",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,So it doesn't mean we cannot fix this.
cs-410_7_4_131,cs-410,7,4,"00:08:53,390","00:08:57,390",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,"For example, imagine if we did"
cs-410_7_4_132,cs-410,7,4,"00:08:57,390","00:08:59,250",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=537,So we can take a risk and
cs-410_7_4_133,cs-410,7,4,"00:08:59,250","00:09:01,935",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,or we can even add double logarithm.
cs-410_7_4_134,cs-410,7,4,"00:09:01,935","00:09:06,200",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=541,"But then, it would mean that the function"
cs-410_7_4_135,cs-410,7,4,"00:09:06,200","00:09:10,780",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,So the consequence of
cs-410_7_4_136,cs-410,7,4,"00:09:10,780","00:09:14,670",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,longer as predictable as
cs-410_7_4_137,cs-410,7,4,"00:09:15,810","00:09:21,410",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,"So, that's also why, for example,"
cs-410_7_4_138,cs-410,7,4,"00:09:21,410","00:09:26,720",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,"still, open channel how to use"
cs-410_7_4_139,cs-410,7,4,"00:09:26,720","00:09:28,690",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,better model than the PM25.
cs-410_7_4_140,cs-410,7,4,"00:09:30,420","00:09:34,500",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=570,In particular how do we use query
cs-410_7_4_141,cs-410,7,4,"00:09:34,500","00:09:37,650",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,that would work consistently
cs-410_7_4_142,cs-410,7,4,"00:09:37,650","00:09:39,070",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,Currently we still cannot do that.
cs-410_7_4_143,cs-410,7,4,"00:09:40,240","00:09:41,640",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=580,Still interesting open question.
cs-410_7_4_144,cs-410,7,4,"00:09:43,450","00:09:46,975",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,"So to summarize this part, we've talked"
cs-410_7_4_145,cs-410,7,4,"00:09:46,975","00:09:52,550",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,Jelinek-Mercer which is doing the fixed
cs-410_7_4_146,cs-410,7,4,"00:09:52,550","00:09:58,430",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=592,Dirichlet Prior this is what add a pseudo
cs-410_7_4_147,cs-410,7,4,"00:09:58,430","00:10:04,160",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=598,interpolation in that the coefficient
cs-410_7_4_148,cs-410,7,4,"00:10:05,940","00:10:10,890",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=605,"In most cases we can see, by using these"
cs-410_7_4_149,cs-410,7,4,"00:10:10,890","00:10:16,670",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,reach a retrieval function where
cs-410_7_4_150,cs-410,7,4,"00:10:16,670","00:10:17,790",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,So they are less heuristic.
cs-410_7_4_151,cs-410,7,4,"00:10:19,090","00:10:23,810",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,Explaining the results also show
cs-410_7_4_152,cs-410,7,4,"00:10:23,810","00:10:31,036",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,Also are very effective and they are
cs-410_7_4_153,cs-410,7,4,"00:10:31,036","00:10:36,260",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,So this is a major advantage
cs-410_7_4_154,cs-410,7,4,"00:10:36,260","00:10:39,480",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=636,where we don't have to do
cs-410_7_4_155,cs-410,7,4,"00:10:40,770","00:10:44,240",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,Yet in the end that we naturally
cs-410_7_4_156,cs-410,7,4,"00:10:44,240","00:10:45,239",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=644,doc length normalization.
cs-410_7_4_157,cs-410,7,4,"00:10:46,480","00:10:51,120",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,Each of these functions also has
cs-410_7_4_158,cs-410,7,4,"00:10:51,120","00:10:54,840",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,In this case of course we still need
cs-410_7_4_159,cs-410,7,4,"00:10:54,840","00:10:58,850",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,There are also methods that can be
cs-410_7_4_160,cs-410,7,4,"00:10:59,950","00:11:04,020",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,"So overall,"
cs-410_7_4_161,cs-410,7,4,"00:11:04,020","00:11:08,900",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=664,we follow very different strategies
cs-410_7_4_162,cs-410,7,4,"00:11:08,900","00:11:12,980",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=668,"Yet, in the end, we end up uh,with"
cs-410_7_4_163,cs-410,7,4,"00:11:12,980","00:11:15,540",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,look very similar to
cs-410_7_4_164,cs-410,7,4,"00:11:15,540","00:11:21,160",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,With some advantages in having
cs-410_7_4_165,cs-410,7,4,"00:11:21,160","00:11:24,940",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,"And then, the form dictated"
cs-410_7_4_166,cs-410,7,4,"00:11:24,940","00:11:29,740",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=684,"Now, this also concludes our discussion of"
cs-410_7_4_167,cs-410,7,4,"00:11:29,740","00:11:34,680",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,And let's recall what
cs-410_7_4_168,cs-410,7,4,"00:11:34,680","00:11:39,390",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=694,in order to derive the functions
cs-410_7_4_169,cs-410,7,4,"00:11:39,390","00:11:42,130",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=699,Well we basically have made four
cs-410_7_4_170,cs-410,7,4,"00:11:42,130","00:11:48,399",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,The first assumption is that the relevance
cs-410_7_4_171,cs-410,7,4,"00:11:49,470","00:11:53,450",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,"And the second assumption with med is, are"
cs-410_7_4_172,cs-410,7,4,"00:11:53,450","00:11:57,240",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=713,that allows us to decompose
cs-410_7_4_173,cs-410,7,4,"00:11:57,240","00:12:01,690",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,into a product of probabilities
cs-410_7_4_174,cs-410,7,4,"00:12:03,090","00:12:07,850",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,"And then,"
cs-410_7_4_175,cs-410,7,4,"00:12:07,850","00:12:10,550",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,"if a word is not seen,"
cs-410_7_4_176,cs-410,7,4,"00:12:10,550","00:12:14,870",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,its probability proportional to
cs-410_7_4_177,cs-410,7,4,"00:12:14,870","00:12:17,290",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,That's a smoothing with
cs-410_7_4_178,cs-410,7,4,"00:12:17,290","00:12:20,980",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,"And finally, we made one of these"
cs-410_7_4_179,cs-410,7,4,"00:12:20,980","00:12:24,940",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,So we either used JM smoothing or
cs-410_7_4_180,cs-410,7,4,"00:12:24,940","00:12:28,820",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,If we make these four assumptions
cs-410_7_4_181,cs-410,7,4,"00:12:28,820","00:12:33,430",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,to take the form of the retrieval
cs-410_7_4_182,cs-410,7,4,"00:12:33,430","00:12:37,730",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,Fortunately the function has a nice
cs-410_7_4_183,cs-410,7,4,"00:12:37,730","00:12:44,510",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,weighting and document machine and
cs-410_7_4_184,cs-410,7,4,"00:12:44,510","00:12:45,440",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,"So in that sense,"
cs-410_7_4_185,cs-410,7,4,"00:12:45,440","00:12:48,920",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=765,these functions are less heuristic
cs-410_7_4_186,cs-410,7,4,"00:12:50,460","00:12:54,282",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=770,"And there are many extensions of this,"
cs-410_7_4_187,cs-410,7,4,"00:12:54,282","00:12:59,336",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=774,you can find the discussion of them in
cs-410_7_4_188,cs-410,7,4,"00:13:04,921","00:13:14,921",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=784,[MUSIC]
cs-410_7_5_1,cs-410,7,5,"00:00:00,049","00:00:03,810",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[MUSIC]
cs-410_7_5_2,cs-410,7,5,"00:00:09,183","00:00:12,025",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,So let's take a look at this in detail.
cs-410_7_5_3,cs-410,7,5,"00:00:12,025","00:00:17,269",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,So in this random surfing
cs-410_7_5_4,cs-410,7,5,"00:00:17,269","00:00:22,575",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=17,random surfer would choose
cs-410_7_5_5,cs-410,7,5,"00:00:22,575","00:00:25,225",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,So this is a small graph here.
cs-410_7_5_6,cs-410,7,5,"00:00:25,225","00:00:29,490",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"That's of course, over simplification"
cs-410_7_5_7,cs-410,7,5,"00:00:29,490","00:00:35,207",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,But let's say there are four
cs-410_7_5_8,cs-410,7,5,"00:00:35,207","00:00:41,360",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,And let's assume that a random surfer or
cs-410_7_5_9,cs-410,7,5,"00:00:41,360","00:00:46,373",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,And then the random
cs-410_7_5_10,cs-410,7,5,"00:00:46,373","00:00:50,439",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,just randomly jumping to any page or
cs-410_7_5_11,cs-410,7,5,"00:00:50,439","00:00:55,330",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,follow a link and
cs-410_7_5_12,cs-410,7,5,"00:00:56,440","00:00:59,650",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,"So if the random surfer is at d1,"
cs-410_7_5_13,cs-410,7,5,"00:01:01,100","00:01:06,260",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,then there is some probability that
cs-410_7_5_14,cs-410,7,5,"00:01:06,260","00:01:09,510",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,"Now there are two outlinks here,"
cs-410_7_5_15,cs-410,7,5,"00:01:09,510","00:01:12,740",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,the other is pointing to d4.
cs-410_7_5_16,cs-410,7,5,"00:01:12,740","00:01:19,020",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=72,So the random surfer could pick any
cs-410_7_5_17,cs-410,7,5,"00:01:19,020","00:01:25,800",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,But it also assumes that the random so
cs-410_7_5_18,cs-410,7,5,"00:01:25,800","00:01:30,586",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=85,So the random surfing which decide
cs-410_7_5_19,cs-410,7,5,"00:01:30,586","00:01:34,050",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,simply randomly jump
cs-410_7_5_20,cs-410,7,5,"00:01:34,050","00:01:39,760",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"So if it does that, it would be able"
cs-410_7_5_21,cs-410,7,5,"00:01:39,760","00:01:45,090",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,"though there's no link you actually,"
cs-410_7_5_22,cs-410,7,5,"00:01:46,170","00:01:49,713",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,So this is to assume that
cs-410_7_5_23,cs-410,7,5,"00:01:49,713","00:01:54,852",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,Imagine a random surfer is
cs-410_7_5_24,cs-410,7,5,"00:01:54,852","00:01:59,989",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,then we can ask the question how
cs-410_7_5_25,cs-410,7,5,"00:01:59,989","00:02:05,864",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,would actually reach a particular
cs-410_7_5_26,cs-410,7,5,"00:02:05,864","00:02:09,824",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,That's the average probability of
cs-410_7_5_27,cs-410,7,5,"00:02:09,824","00:02:13,830",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,this probability is precisely
cs-410_7_5_28,cs-410,7,5,"00:02:13,830","00:02:17,558",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,So the page rank score of
cs-410_7_5_29,cs-410,7,5,"00:02:17,558","00:02:21,644",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,probability that the surfer
cs-410_7_5_30,cs-410,7,5,"00:02:21,644","00:02:27,220",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,"Now intuitively, this would basically"
cs-410_7_5_31,cs-410,7,5,"00:02:27,220","00:02:30,970",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"Because if a page has a lot of inlinks,"
cs-410_7_5_32,cs-410,7,5,"00:02:30,970","00:02:34,580",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,then it would have a higher
cs-410_7_5_33,cs-410,7,5,"00:02:34,580","00:02:37,650",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,Because there will be more
cs-410_7_5_34,cs-410,7,5,"00:02:37,650","00:02:39,940",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,follow a link to come to this page.
cs-410_7_5_35,cs-410,7,5,"00:02:41,290","00:02:45,030",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,And this is why the random surfing model
cs-410_7_5_36,cs-410,7,5,"00:02:45,030","00:02:48,510",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,actually captures the ID
cs-410_7_5_37,cs-410,7,5,"00:02:48,510","00:02:52,700",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,Note that it also considers
cs-410_7_5_38,cs-410,7,5,"00:02:52,700","00:02:59,690",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,Because if the page is that point then
cs-410_7_5_39,cs-410,7,5,"00:02:59,690","00:03:04,550",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,That would mean the random surfer would
cs-410_7_5_40,cs-410,7,5,"00:03:04,550","00:03:07,680",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,"therefore, it increase"
cs-410_7_5_41,cs-410,7,5,"00:03:07,680","00:03:13,580",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=187,So this is just a nice way to capture
cs-410_7_5_42,cs-410,7,5,"00:03:13,580","00:03:18,440",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,"So mathematically, how can we compute this"
cs-410_7_5_43,cs-410,7,5,"00:03:18,440","00:03:22,390",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,we need to take a look at how this
cs-410_7_5_44,cs-410,7,5,"00:03:22,390","00:03:25,184",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,So first of all let's take a look
cs-410_7_5_45,cs-410,7,5,"00:03:25,184","00:03:29,437",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,And this is just metrics with
cs-410_7_5_46,cs-410,7,5,"00:03:29,437","00:03:33,273",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,the random surfer would go
cs-410_7_5_47,cs-410,7,5,"00:03:33,273","00:03:37,230",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,So each rule stands for a starting page.
cs-410_7_5_48,cs-410,7,5,"00:03:37,230","00:03:41,754",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,"For example, rule one would"
cs-410_7_5_49,cs-410,7,5,"00:03:41,754","00:03:44,581",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,to any of the other four pages from d1.
cs-410_7_5_50,cs-410,7,5,"00:03:44,581","00:03:53,097",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,And here we see there are only
cs-410_7_5_51,cs-410,7,5,"00:03:53,097","00:04:01,492",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,So this is because if you look at
cs-410_7_5_52,cs-410,7,5,"00:04:01,492","00:04:05,918",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,There is no link from d1 or d2.
cs-410_7_5_53,cs-410,7,5,"00:04:05,918","00:04:10,579",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,So we've got 0s for the first 2
cs-410_7_5_54,cs-410,7,5,"00:04:10,579","00:04:15,762",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,columns and 0.5 for d3 and d4.
cs-410_7_5_55,cs-410,7,5,"00:04:15,762","00:04:19,416",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,"In general, the M in this matrix,"
cs-410_7_5_56,cs-410,7,5,"00:04:19,416","00:04:24,586",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,M sub ij is the probability
cs-410_7_5_57,cs-410,7,5,"00:04:24,586","00:04:29,668",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,"And obviously for each rule,"
cs-410_7_5_58,cs-410,7,5,"00:04:29,668","00:04:36,115",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,because the surfer would have to go to
cs-410_7_5_59,cs-410,7,5,"00:04:36,115","00:04:39,196",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,So this is a transition metric.
cs-410_7_5_60,cs-410,7,5,"00:04:39,196","00:04:43,690",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,Now how can we compute the probability
cs-410_7_5_61,cs-410,7,5,"00:04:44,900","00:04:49,900",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=284,Well if you look at the surf
cs-410_7_5_62,cs-410,7,5,"00:04:50,910","00:04:56,280",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,we can compute the probability
cs-410_7_5_63,cs-410,7,5,"00:04:56,280","00:05:01,140",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,"So here on the left hand side,"
cs-410_7_5_64,cs-410,7,5,"00:05:02,170","00:05:08,540",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,"visiting page dj at time plus 1,"
cs-410_7_5_65,cs-410,7,5,"00:05:08,540","00:05:14,740",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,"On the right hand side, you can see"
cs-410_7_5_66,cs-410,7,5,"00:05:14,740","00:05:20,000",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,of at page di at time t.
cs-410_7_5_67,cs-410,7,5,"00:05:21,408","00:05:26,020",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,So you can see the subscript
cs-410_7_5_68,cs-410,7,5,"00:05:26,020","00:05:34,314",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,that indicates that's the probability that
cs-410_7_5_69,cs-410,7,5,"00:05:34,314","00:05:38,500",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,"So the equation basically,"
cs-410_7_5_70,cs-410,7,5,"00:05:38,500","00:05:43,790",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,possibilities of reaching
cs-410_7_5_71,cs-410,7,5,"00:05:43,790","00:05:45,510",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,What are these two possibilities?
cs-410_7_5_72,cs-410,7,5,"00:05:45,510","00:05:48,200",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=345,Well one is through random surfing and
cs-410_7_5_73,cs-410,7,5,"00:05:48,200","00:05:51,930",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,"one is through following a link,"
cs-410_7_5_74,cs-410,7,5,"00:05:53,500","00:05:56,612",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,So the first part captures the probability
cs-410_7_5_75,cs-410,7,5,"00:05:56,612","00:06:01,373",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,that the random surfer would reach
cs-410_7_5_76,cs-410,7,5,"00:06:01,373","00:06:06,283",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,And you can see the random
cs-410_7_5_77,cs-410,7,5,"00:06:06,283","00:06:10,455",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=366,with probability 1 minus
cs-410_7_5_78,cs-410,7,5,"00:06:10,455","00:06:14,200",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,And so
cs-410_7_5_79,cs-410,7,5,"00:06:14,200","00:06:18,250",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,But the main party is realist
cs-410_7_5_80,cs-410,7,5,"00:06:18,250","00:06:22,060",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=378,that the surfer could have been at time t.
cs-410_7_5_81,cs-410,7,5,"00:06:23,760","00:06:27,890",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,There are n pages so
cs-410_7_5_82,cs-410,7,5,"00:06:27,890","00:06:31,730",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,Inside the sum is a product
cs-410_7_5_83,cs-410,7,5,"00:06:31,730","00:06:36,763",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=391,One is the probability that the surfer
cs-410_7_5_84,cs-410,7,5,"00:06:36,763","00:06:42,115",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=396,"was at di at time t, that's p sub t of di."
cs-410_7_5_85,cs-410,7,5,"00:06:42,115","00:06:47,422",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,The other is the transition
cs-410_7_5_86,cs-410,7,5,"00:06:47,422","00:06:52,217",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,"And so in order to reach this dj page,"
cs-410_7_5_87,cs-410,7,5,"00:06:52,217","00:06:57,880",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,the surfer must first be at di at time t.
cs-410_7_5_88,cs-410,7,5,"00:06:57,880","00:07:03,090",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,"And then also, would also have to"
cs-410_7_5_89,cs-410,7,5,"00:07:03,090","00:07:09,090",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=423,So the probability is the probability
cs-410_7_5_90,cs-410,7,5,"00:07:09,090","00:07:15,950",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,the probability of going from that
cs-410_7_5_91,cs-410,7,5,"00:07:15,950","00:07:20,792",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"The second part is a similar sum, the only"
cs-410_7_5_92,cs-410,7,5,"00:07:20,792","00:07:23,980",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,probability is a uniform
cs-410_7_5_93,cs-410,7,5,"00:07:23,980","00:07:27,708",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=443,1 over n and
cs-410_7_5_94,cs-410,7,5,"00:07:27,708","00:07:31,110",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,of reaching this page
cs-410_7_5_95,cs-410,7,5,"00:07:32,630","00:07:37,520",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=452,So the form is exactly the same and
cs-410_7_5_96,cs-410,7,5,"00:07:37,520","00:07:43,310",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,see on why PageRank is essentially assumed
cs-410_7_5_97,cs-410,7,5,"00:07:43,310","00:07:49,070",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,If you think about this 1 over n as
cs-410_7_5_98,cs-410,7,5,"00:07:49,070","00:07:55,320",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,that has all the elements being
cs-410_7_5_99,cs-410,7,5,"00:07:55,320","00:07:59,621",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,Then you can see very clearly
cs-410_7_5_100,cs-410,7,5,"00:07:59,621","00:08:01,784",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,because they are of the same form.
cs-410_7_5_101,cs-410,7,5,"00:08:01,784","00:08:07,310",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,We can imagine there's a different
cs-410_7_5_102,cs-410,7,5,"00:08:07,310","00:08:11,340",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=487,that uniform metrics where
cs-410_7_5_103,cs-410,7,5,"00:08:11,340","00:08:16,347",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,And in this sense PageRank uses
cs-410_7_5_104,cs-410,7,5,"00:08:16,347","00:08:22,312",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,ensuring that there's no zero entry
cs-410_7_5_105,cs-410,7,5,"00:08:22,312","00:08:28,530",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,Now of course this is the time dependent
cs-410_7_5_106,cs-410,7,5,"00:08:28,530","00:08:32,480",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,"Now we can imagine, if we'll compute"
cs-410_7_5_107,cs-410,7,5,"00:08:32,480","00:08:36,420",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,the average of probabilities probably
cs-410_7_5_108,cs-410,7,5,"00:08:36,420","00:08:38,320",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,without considering the time index.
cs-410_7_5_109,cs-410,7,5,"00:08:38,320","00:08:41,780",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,So let's drop the time index and
cs-410_7_5_110,cs-410,7,5,"00:08:42,910","00:08:47,100",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,"Now this would give us any equations,"
cs-410_7_5_111,cs-410,7,5,"00:08:47,100","00:08:49,520",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,each page we have such equation.
cs-410_7_5_112,cs-410,7,5,"00:08:49,520","00:08:52,800",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,And if you look at the what
cs-410_7_5_113,cs-410,7,5,"00:08:52,800","00:08:55,170",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,there are also precisely n variables.
cs-410_7_5_114,cs-410,7,5,"00:08:58,280","00:09:03,220",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,"So this basically means,"
cs-410_7_5_115,cs-410,7,5,"00:09:04,600","00:09:10,260",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,n equations with n variables and
cs-410_7_5_116,cs-410,7,5,"00:09:10,260","00:09:16,420",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,"So basically, now the problem boils"
cs-410_7_5_117,cs-410,7,5,"00:09:16,420","00:09:20,950",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,"And here, I also show"
cs-410_7_5_118,cs-410,7,5,"00:09:20,950","00:09:26,690",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,It's the vector p here equals a matrix or
cs-410_7_5_119,cs-410,7,5,"00:09:26,690","00:09:31,390",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,the transpose of the matrix here and
cs-410_7_5_120,cs-410,7,5,"00:09:32,580","00:09:36,890",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=572,"Now, if you still remember some knowledge"
cs-410_7_5_121,cs-410,7,5,"00:09:36,890","00:09:42,140",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,"and then you will realize, this is"
cs-410_7_5_122,cs-410,7,5,"00:09:42,140","00:09:47,690",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,"When multiply the metrics by this vector,"
cs-410_7_5_123,cs-410,7,5,"00:09:47,690","00:09:52,280",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,this can be solved by
cs-410_7_5_124,cs-410,7,5,"00:09:54,700","00:09:57,380",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=594,So because the equations here
cs-410_7_5_125,cs-410,7,5,"00:09:57,380","00:10:02,002",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,on the back are basically
cs-410_7_5_126,cs-410,7,5,"00:10:02,002","00:10:09,170",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,So you'll see the relation between the
cs-410_7_5_127,cs-410,7,5,"00:10:09,170","00:10:13,844",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,And this iterative approach or
cs-410_7_5_128,cs-410,7,5,"00:10:13,844","00:10:19,093",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=613,we simply start with s
cs-410_7_5_129,cs-410,7,5,"00:10:19,093","00:10:24,242",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=619,And then we repeatedly
cs-410_7_5_130,cs-410,7,5,"00:10:24,242","00:10:29,970",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=624,multiplying the metrics
cs-410_7_5_131,cs-410,7,5,"00:10:31,360","00:10:37,820",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,I also show a concrete example here.
cs-410_7_5_132,cs-410,7,5,"00:10:37,820","00:10:40,130",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=637,So you can see this now.
cs-410_7_5_133,cs-410,7,5,"00:10:40,130","00:10:43,368",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,"If we assume alpha is 0.2,"
cs-410_7_5_134,cs-410,7,5,"00:10:43,368","00:10:49,066",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,then with the example that
cs-410_7_5_135,cs-410,7,5,"00:10:49,066","00:10:54,393",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,we have the original
cs-410_7_5_136,cs-410,7,5,"00:10:54,393","00:10:59,943",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,"That includes the graph, the actual links"
cs-410_7_5_137,cs-410,7,5,"00:10:59,943","00:11:04,856",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,"metrics, uniform transition metrics"
cs-410_7_5_138,cs-410,7,5,"00:11:04,856","00:11:09,707",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=664,And we can combine them together with
cs-410_7_5_139,cs-410,7,5,"00:11:09,707","00:11:12,260",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=669,metric that would be like this.
cs-410_7_5_140,cs-410,7,5,"00:11:12,260","00:11:13,320",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,"So essentially,"
cs-410_7_5_141,cs-410,7,5,"00:11:13,320","00:11:18,300",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=673,we can imagine now the web looks like
cs-410_7_5_142,cs-410,7,5,"00:11:18,300","00:11:22,300",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=678,They're all virtual links
cs-410_7_5_143,cs-410,7,5,"00:11:22,300","00:11:27,210",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=682,The page we're on now would just
cs-410_7_5_144,cs-410,7,5,"00:11:27,210","00:11:30,270",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,then just computed the updating of this
cs-410_7_5_145,cs-410,7,5,"00:11:30,270","00:11:34,899",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,p vector by using this
cs-410_7_5_146,cs-410,7,5,"00:11:36,600","00:11:40,640",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,Now if you rewrite this
cs-410_7_5_147,cs-410,7,5,"00:11:42,020","00:11:45,080",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,"terms of individual equations,"
cs-410_7_5_148,cs-410,7,5,"00:11:46,530","00:11:51,435",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=706,"And this is basically,"
cs-410_7_5_149,cs-410,7,5,"00:11:51,435","00:11:54,385",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=711,this particular pages and page score.
cs-410_7_5_150,cs-410,7,5,"00:11:54,385","00:11:59,364",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,So you can also see if you want to compute
cs-410_7_5_151,cs-410,7,5,"00:11:59,364","00:12:04,617",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=719,You basically multiply
cs-410_7_5_152,cs-410,7,5,"00:12:04,617","00:12:09,379",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=724,and we'll take the third
cs-410_7_5_153,cs-410,7,5,"00:12:09,379","00:12:13,950",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=729,And that will give us the value for
cs-410_7_5_154,cs-410,7,5,"00:12:16,000","00:12:20,170",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,So this is how we updated the vector
cs-410_7_5_155,cs-410,7,5,"00:12:20,170","00:12:23,270",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=740,these guys for this.
cs-410_7_5_156,cs-410,7,5,"00:12:23,270","00:12:28,080",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,And then we just revise
cs-410_7_5_157,cs-410,7,5,"00:12:28,080","00:12:31,550",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=748,set of scores and
cs-410_7_5_158,cs-410,7,5,"00:12:33,150","00:12:37,590",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,So we just repeatedly apply this and
cs-410_7_5_159,cs-410,7,5,"00:12:37,590","00:12:41,432",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,"And when the matrix is like this,"
cs-410_7_5_160,cs-410,7,5,"00:12:41,432","00:12:43,510",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,it can be guaranteed to converge.
cs-410_7_5_161,cs-410,7,5,"00:12:44,790","00:12:49,765",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,And at that point the we will just have
cs-410_7_5_162,cs-410,7,5,"00:12:49,765","00:12:53,101",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,We typically go to sets of
cs-410_7_5_163,cs-410,7,5,"00:12:55,300","00:12:58,543",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,"So interestingly,"
cs-410_7_5_164,cs-410,7,5,"00:12:58,543","00:13:03,296",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,also interpreted as propagating
cs-410_7_5_165,cs-410,7,5,"00:13:03,296","00:13:08,847",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=783,Or if you look at this formula and
cs-410_7_5_166,cs-410,7,5,"00:13:08,847","00:13:13,440",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,"can you imagine,"
cs-410_7_5_167,cs-410,7,5,"00:13:13,440","00:13:17,479",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=793,essentially propagating
cs-410_7_5_168,cs-410,7,5,"00:13:17,479","00:13:19,801",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=797,"I hope you will see that indeed,"
cs-410_7_5_169,cs-410,7,5,"00:13:19,801","00:13:24,565",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,we can imagine we have values
cs-410_7_5_170,cs-410,7,5,"00:13:24,565","00:13:30,220",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=804,So we can have values here and
cs-410_7_5_171,cs-410,7,5,"00:13:30,220","00:13:35,170",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,And then we're going to use these
cs-410_7_5_172,cs-410,7,5,"00:13:35,170","00:13:42,290",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,And if you look at the equation here
cs-410_7_5_173,cs-410,7,5,"00:13:42,290","00:13:48,890",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,to combine the scores of the pages that
cs-410_7_5_174,cs-410,7,5,"00:13:48,890","00:13:54,067",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=828,So we'll look at all the pages
cs-410_7_5_175,cs-410,7,5,"00:13:54,067","00:14:00,916",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=834,then combine this score and propagate the
cs-410_7_5_176,cs-410,7,5,"00:14:00,916","00:14:06,145",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=840,To look at the scores that we present
cs-410_7_5_177,cs-410,7,5,"00:14:06,145","00:14:11,410",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=846,surfer would be visiting the other
cs-410_7_5_178,cs-410,7,5,"00:14:11,410","00:14:16,275",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,And then just do
cs-410_7_5_179,cs-410,7,5,"00:14:16,275","00:14:21,409",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=856,"the probability of reaching this page, d1."
cs-410_7_5_180,cs-410,7,5,"00:14:21,409","00:14:23,910",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=861,So there are two interpretations here.
cs-410_7_5_181,cs-410,7,5,"00:14:23,910","00:14:26,364",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=863,One is just the matrix multiplication.
cs-410_7_5_182,cs-410,7,5,"00:14:26,364","00:14:31,498",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=866,We repeat the multiplying
cs-410_7_5_183,cs-410,7,5,"00:14:31,498","00:14:35,204",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=871,The other is to just think
cs-410_7_5_184,cs-410,7,5,"00:14:35,204","00:14:38,180",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=875,these scores repeatedly on the web.
cs-410_7_5_185,cs-410,7,5,"00:14:38,180","00:14:43,150",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=878,"So in practice, the combination of"
cs-410_7_5_186,cs-410,7,5,"00:14:43,150","00:14:48,820",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,Because the matrices is fast and there
cs-410_7_5_187,cs-410,7,5,"00:14:48,820","00:14:53,820",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=888,So that you avoid actually
cs-410_7_5_188,cs-410,7,5,"00:14:53,820","00:14:55,260",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=893,all those elements.
cs-410_7_5_189,cs-410,7,5,"00:14:56,670","00:15:00,670",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=896,Sometimes you may also normalize the
cs-410_7_5_190,cs-410,7,5,"00:15:00,670","00:15:05,249",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=900,"different form of the equation, but"
cs-410_7_5_191,cs-410,7,5,"00:15:06,290","00:15:09,650",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=906,The results of this potential
cs-410_7_5_192,cs-410,7,5,"00:15:10,740","00:15:17,540",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=910,"In that case, if a page does not have"
cs-410_7_5_193,cs-410,7,5,"00:15:17,540","00:15:22,250",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=917,these pages would not sum to 1.
cs-410_7_5_194,cs-410,7,5,"00:15:22,250","00:15:26,349",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=922,"Basically, the probability of reaching the"
cs-410_7_5_195,cs-410,7,5,"00:15:26,349","00:15:29,246",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=926,"1, mainly because we have lost"
cs-410_7_5_196,cs-410,7,5,"00:15:29,246","00:15:33,588",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=929,One would assume there's some probability
cs-410_7_5_197,cs-410,7,5,"00:15:33,588","00:15:37,160",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=933,"the links, but"
cs-410_7_5_198,cs-410,7,5,"00:15:37,160","00:15:42,980",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=937,And one possible solution is simply to use
cs-410_7_5_199,cs-410,7,5,"00:15:42,980","00:15:45,270",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=942,and that could easily fix this.
cs-410_7_5_200,cs-410,7,5,"00:15:46,740","00:15:50,750",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=946,"Basically, that's to say alpha would"
cs-410_7_5_201,cs-410,7,5,"00:15:50,750","00:15:54,130",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=950,"In that case,"
cs-410_7_5_202,cs-410,7,5,"00:15:54,130","00:15:57,330",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=954,randomly jump to another page
cs-410_7_5_203,cs-410,7,5,"00:15:59,120","00:16:05,060",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=959,"There are many extensions of PageRank, one"
cs-410_7_5_204,cs-410,7,5,"00:16:05,060","00:16:11,639",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=965,Note that PageRank doesn't merely
cs-410_7_5_205,cs-410,7,5,"00:16:11,639","00:16:15,370",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=971,So we can make PageRank specific however.
cs-410_7_5_206,cs-410,7,5,"00:16:15,370","00:16:19,260",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=975,"So for example,"
cs-410_7_5_207,cs-410,7,5,"00:16:19,260","00:16:22,567",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=979,we can simply assume
cs-410_7_5_208,cs-410,7,5,"00:16:22,567","00:16:25,660",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=982,The surfer is not randomly
cs-410_7_5_209,cs-410,7,5,"00:16:25,660","00:16:32,320",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=985,"Instead, he's going to jump to only those"
cs-410_7_5_210,cs-410,7,5,"00:16:32,320","00:16:36,780",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=992,"For example, if the query is not sports"
cs-410_7_5_211,cs-410,7,5,"00:16:36,780","00:16:40,670",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=996,"doing random jumping, it's going"
cs-410_7_5_212,cs-410,7,5,"00:16:40,670","00:16:45,350",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,"By doing this, then we can buy"
cs-410_7_5_213,cs-410,7,5,"00:16:45,350","00:16:49,054",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,And then if you know the current
cs-410_7_5_214,cs-410,7,5,"00:16:49,054","00:16:53,368",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1009,then you can use this specialized
cs-410_7_5_215,cs-410,7,5,"00:16:53,368","00:16:57,754",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,That would be better than if you
cs-410_7_5_216,cs-410,7,5,"00:16:57,754","00:17:01,877",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1017,PageRank is also a channel that can be
cs-410_7_5_217,cs-410,7,5,"00:17:01,877","00:17:06,100",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1021,network analysis particularly for
cs-410_7_5_218,cs-410,7,5,"00:17:06,100","00:17:09,970",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1026,You can imagine if you compute
cs-410_7_5_219,cs-410,7,5,"00:17:09,970","00:17:14,356",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1029,"social network, where a link"
cs-410_7_5_220,cs-410,7,5,"00:17:14,356","00:17:18,744",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1034,"a relation, you would get some"
cs-410_7_5_221,cs-410,7,5,"00:17:18,744","00:17:28,744",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1038,[MUSIC]
cs-410_7_6_1,cs-410,7,6,"00:00:07,400","00:00:09,600",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,This lecture is about
cs-410_7_6_2,cs-410,7,6,"00:00:11,540","00:00:16,250",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,In this lecture we're going to continue
cs-410_7_6_3,cs-410,7,6,"00:00:16,250","00:00:21,390",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"In particular, we're going to look at"
cs-410_7_6_4,cs-410,7,6,"00:00:21,390","00:00:25,710",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,You have seen this slide before when
cs-410_7_6_5,cs-410,7,6,"00:00:25,710","00:00:30,310",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"answer the basic question,"
cs-410_7_6_6,cs-410,7,6,"00:00:30,310","00:00:31,290",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"In the previous lecture,"
cs-410_7_6_7,cs-410,7,6,"00:00:31,290","00:00:36,180",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=31,"we looked at the item similarity,"
cs-410_7_6_8,cs-410,7,6,"00:00:36,180","00:00:39,580",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,"In this lecture, we're going to"
cs-410_7_6_9,cs-410,7,6,"00:00:39,580","00:00:42,490",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,"This is a different strategy,"
cs-410_7_6_10,cs-410,7,6,"00:00:44,090","00:00:45,630",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,"So first, what is collaborative filtering?"
cs-410_7_6_11,cs-410,7,6,"00:00:47,460","00:00:49,525",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,It is to make filtering decisions for
cs-410_7_6_12,cs-410,7,6,"00:00:49,525","00:00:52,660",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,individual user based on
cs-410_7_6_13,cs-410,7,6,"00:00:54,240","00:00:58,000",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=54,And that is to say we will
cs-410_7_6_14,cs-410,7,6,"00:00:58,000","00:01:02,080",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,preferences from that
cs-410_7_6_15,cs-410,7,6,"00:01:02,080","00:01:04,530",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,So the general idea is the following.
cs-410_7_6_16,cs-410,7,6,"00:01:04,530","00:01:11,693",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=64,"Given a user u, we're going to first"
cs-410_7_6_17,cs-410,7,6,"00:01:11,693","00:01:15,581",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,And then we're going to
cs-410_7_6_18,cs-410,7,6,"00:01:15,581","00:01:20,540",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,based on the preferences of
cs-410_7_6_19,cs-410,7,6,"00:01:22,390","00:01:26,960",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,"Now, the user similarity here can"
cs-410_7_6_20,cs-410,7,6,"00:01:26,960","00:01:29,610",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,the preferences on a common set of items.
cs-410_7_6_21,cs-410,7,6,"00:01:31,070","00:01:36,020",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,Now here you can see the exact
cs-410_7_6_22,cs-410,7,6,"00:01:36,020","00:01:40,430",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,We're going to look at the only the
cs-410_7_6_23,cs-410,7,6,"00:01:41,730","00:01:44,120",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,So this means this
cs-410_7_6_24,cs-410,7,6,"00:01:44,120","00:01:49,450",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,"It can be applied to any items,"
cs-410_7_6_25,cs-410,7,6,"00:01:49,450","00:01:53,700",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,So this approach would work well
cs-410_7_6_26,cs-410,7,6,"00:01:53,700","00:01:59,230",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,"First, users with the same interest"
cs-410_7_6_27,cs-410,7,6,"00:01:59,230","00:02:03,570",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=119,"Second, the users with similar preferences"
cs-410_7_6_28,cs-410,7,6,"00:02:03,570","00:02:08,650",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=123,"So for example, if the interest of"
cs-410_7_6_29,cs-410,7,6,"00:02:08,650","00:02:12,960",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,then we can infer the user
cs-410_7_6_30,cs-410,7,6,"00:02:14,280","00:02:17,270",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,So those who are interested in
cs-410_7_6_31,cs-410,7,6,"00:02:17,270","00:02:19,840",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,probably all favor SIGIR papers.
cs-410_7_6_32,cs-410,7,6,"00:02:19,840","00:02:21,880",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,That's an assumption that we make.
cs-410_7_6_33,cs-410,7,6,"00:02:21,880","00:02:23,440",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,"And if this assumption is true,"
cs-410_7_6_34,cs-410,7,6,"00:02:23,440","00:02:27,490",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,then it would help collaborative
cs-410_7_6_35,cs-410,7,6,"00:02:27,490","00:02:34,055",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,We can also assume that if we see
cs-410_7_6_36,cs-410,7,6,"00:02:34,055","00:02:38,215",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,then we can infer their interest
cs-410_7_6_37,cs-410,7,6,"00:02:38,215","00:02:43,025",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,"So in these simple examples,"
cs-410_7_6_38,cs-410,7,6,"00:02:43,025","00:02:48,492",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,in many cases such assumption
cs-410_7_6_39,cs-410,7,6,"00:02:48,492","00:02:52,896",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=168,So another assumption we have to make
cs-410_7_6_40,cs-410,7,6,"00:02:52,896","00:02:56,012",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,number of user preferences
cs-410_7_6_41,cs-410,7,6,"00:02:56,012","00:03:00,722",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,"So for example, if you see a lot"
cs-410_7_6_42,cs-410,7,6,"00:03:00,722","00:03:03,160",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,those indicate their
cs-410_7_6_43,cs-410,7,6,"00:03:03,160","00:03:06,832",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,"And if you have a lot of such data,"
cs-410_7_6_44,cs-410,7,6,"00:03:06,832","00:03:08,689",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,filtering can be very effective.
cs-410_7_6_45,cs-410,7,6,"00:03:09,960","00:03:14,680",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,"If not, there will be a problem, and"
cs-410_7_6_46,cs-410,7,6,"00:03:14,680","00:03:18,640",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,That means you don't have many
cs-410_7_6_47,cs-410,7,6,"00:03:18,640","00:03:23,722",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,the system could not fully take advantage
cs-410_7_6_48,cs-410,7,6,"00:03:23,722","00:03:28,690",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,So let's look at the filtering
cs-410_7_6_49,cs-410,7,6,"00:03:30,340","00:03:33,791",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,"So this picture shows that we are,"
cs-410_7_6_50,cs-410,7,6,"00:03:33,791","00:03:38,075",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,"in general, considering a lot of users and"
cs-410_7_6_51,cs-410,7,6,"00:03:38,075","00:03:42,956",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"we're showing m users here, so U1 through."
cs-410_7_6_52,cs-410,7,6,"00:03:42,956","00:03:46,040",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,And we're also considering
cs-410_7_6_53,cs-410,7,6,"00:03:46,040","00:03:49,870",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,Let's say n objects in
cs-410_7_6_54,cs-410,7,6,"00:03:49,870","00:03:55,330",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,And then we will assume that
cs-410_7_6_55,cs-410,7,6,"00:03:55,330","00:04:01,510",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,objects and the user could for
cs-410_7_6_56,cs-410,7,6,"00:04:01,510","00:04:06,490",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,"For example, those items could be movies,"
cs-410_7_6_57,cs-410,7,6,"00:04:06,490","00:04:10,500",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,then the users would give
cs-410_7_6_58,cs-410,7,6,"00:04:10,500","00:04:14,829",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,So what you see here is that we have
cs-410_7_6_59,cs-410,7,6,"00:04:14,829","00:04:16,231",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,some combinations.
cs-410_7_6_60,cs-410,7,6,"00:04:16,231","00:04:21,751",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,"So some users have watched some movies,"
cs-410_7_6_61,cs-410,7,6,"00:04:21,751","00:04:26,075",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,they obviously won't be able
cs-410_7_6_62,cs-410,7,6,"00:04:26,075","00:04:30,040",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,some users may actually
cs-410_7_6_63,cs-410,7,6,"00:04:30,040","00:04:34,410",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=270,So this is in general a small symmetrics.
cs-410_7_6_64,cs-410,7,6,"00:04:34,410","00:04:38,030",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,So many items and
cs-410_7_6_65,cs-410,7,6,"00:04:39,160","00:04:46,070",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,And what's interesting here is we
cs-410_7_6_66,cs-410,7,6,"00:04:46,070","00:04:51,780",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,of an element in this matrix
cs-410_7_6_67,cs-410,7,6,"00:04:51,780","00:04:56,610",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=291,And that's after the essential question
cs-410_7_6_68,cs-410,7,6,"00:04:56,610","00:04:59,950",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=296,we assume there's an unknown
cs-410_7_6_69,cs-410,7,6,"00:04:59,950","00:05:04,400",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,That would map a pair of user and
cs-410_7_6_70,cs-410,7,6,"00:05:04,400","00:05:07,610",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=304,And we have observed the sum
cs-410_7_6_71,cs-410,7,6,"00:05:08,960","00:05:14,296",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=308,And we want to infer the value
cs-410_7_6_72,cs-410,7,6,"00:05:14,296","00:05:20,168",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,other pairs that don't have
cs-410_7_6_73,cs-410,7,6,"00:05:20,168","00:05:26,198",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,So this is very similar to other
cs-410_7_6_74,cs-410,7,6,"00:05:26,198","00:05:31,784",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=326,know the values of the function
cs-410_7_6_75,cs-410,7,6,"00:05:31,784","00:05:37,384",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=331,And we hope to predict the values of
cs-410_7_6_76,cs-410,7,6,"00:05:37,384","00:05:40,344",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,this is a function approximation.
cs-410_7_6_77,cs-410,7,6,"00:05:40,344","00:05:47,440",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,And how can we pick out the function
cs-410_7_6_78,cs-410,7,6,"00:05:47,440","00:05:50,230",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,So this is the setup.
cs-410_7_6_79,cs-410,7,6,"00:05:50,230","00:05:54,680",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,Now there are many approaches
cs-410_7_6_80,cs-410,7,6,"00:05:54,680","00:06:00,415",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,"In fact,"
cs-410_7_6_81,cs-410,7,6,"00:06:00,415","00:06:09,095",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=360,reason that there are special
cs-410_7_6_82,cs-410,7,6,"00:06:10,419","00:06:15,730",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,major conference devoted to the problem.
cs-410_7_6_83,cs-410,7,6,"00:06:15,730","00:06:20,199",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,[MUSIC]
cs-410_7_7_1,cs-410,7,7,"00:00:00,025","00:00:04,546",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND] This lecture is
cs-410_7_7_2,cs-410,7,7,"00:00:04,546","00:00:10,323",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=4,about the word association
cs-410_7_7_3,cs-410,7,7,"00:00:10,323","00:00:15,100",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=10,mining and analysis.
cs-410_7_7_4,cs-410,7,7,"00:00:15,100","00:00:19,884",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=15,"In this lecture,"
cs-410_7_7_5,cs-410,7,7,"00:00:19,884","00:00:22,902",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,associations of words from text.
cs-410_7_7_6,cs-410,7,7,"00:00:22,902","00:00:27,900",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=22,Now this is an example of knowledge
cs-410_7_7_7,cs-410,7,7,"00:00:27,900","00:00:29,960",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=27,we can mine from text data.
cs-410_7_7_8,cs-410,7,7,"00:00:33,942","00:00:35,090",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,Here's the outline.
cs-410_7_7_9,cs-410,7,7,"00:00:35,090","00:00:39,828",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,We're going to first talk about
cs-410_7_7_10,cs-410,7,7,"00:00:39,828","00:00:45,100",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,then explain why discovering such
cs-410_7_7_11,cs-410,7,7,"00:00:45,100","00:00:50,070",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,we're going to talk about some general
cs-410_7_7_12,cs-410,7,7,"00:00:50,070","00:00:55,209",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,In general there are two word
cs-410_7_7_13,cs-410,7,7,"00:00:56,680","00:00:58,680",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,One is called a paradigmatic relation.
cs-410_7_7_14,cs-410,7,7,"00:00:58,680","00:01:03,000",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,The other is syntagmatic relation.
cs-410_7_7_15,cs-410,7,7,"00:01:03,000","00:01:07,780",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,A and B have paradigmatic relation
cs-410_7_7_16,cs-410,7,7,"00:01:07,780","00:01:11,700",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,if they can be substituted for each other.
cs-410_7_7_17,cs-410,7,7,"00:01:11,700","00:01:17,910",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,That means the two words that
cs-410_7_7_18,cs-410,7,7,"00:01:17,910","00:01:23,130",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,"would be in the same semantic class,"
cs-410_7_7_19,cs-410,7,7,"00:01:23,130","00:01:26,910",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,And we can in general
cs-410_7_7_20,cs-410,7,7,"00:01:26,910","00:01:30,310",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,without affecting
cs-410_7_7_21,cs-410,7,7,"00:01:30,310","00:01:33,810",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,That means we would still
cs-410_7_7_22,cs-410,7,7,"00:01:33,810","00:01:41,530",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"For example, cat and dog, these two"
cs-410_7_7_23,cs-410,7,7,"00:01:41,530","00:01:47,710",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,because they are in
cs-410_7_7_24,cs-410,7,7,"00:01:47,710","00:01:51,827",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,"And in general,"
cs-410_7_7_25,cs-410,7,7,"00:01:51,827","00:01:56,880",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,the sentence would still be a valid
cs-410_7_7_26,cs-410,7,7,"00:01:58,320","00:02:01,990",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,Similarly Monday and
cs-410_7_7_27,cs-410,7,7,"00:02:04,930","00:02:09,390",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,The second kind of relation is
cs-410_7_7_28,cs-410,7,7,"00:02:10,610","00:02:17,200",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,"In this case, the two words that have this"
cs-410_7_7_29,cs-410,7,7,"00:02:17,200","00:02:22,190",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,So A and B have syntagmatic relation if
cs-410_7_7_30,cs-410,7,7,"00:02:22,190","00:02:29,500",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,"a sentence, that means these two"
cs-410_7_7_31,cs-410,7,7,"00:02:30,720","00:02:36,830",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,"So for example, cat and sit are related"
cs-410_7_7_32,cs-410,7,7,"00:02:38,060","00:02:43,870",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,"Similarly, car and"
cs-410_7_7_33,cs-410,7,7,"00:02:43,870","00:02:47,550",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=163,they can be combined with
cs-410_7_7_34,cs-410,7,7,"00:02:47,550","00:02:54,150",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,"However, in general, we can not"
cs-410_7_7_35,cs-410,7,7,"00:02:54,150","00:02:59,590",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,car with drive in the sentence
cs-410_7_7_36,cs-410,7,7,"00:02:59,590","00:03:03,950",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,"meaning that if we do that, the sentence"
cs-410_7_7_37,cs-410,7,7,"00:03:03,950","00:03:10,135",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,So this is different from
cs-410_7_7_38,cs-410,7,7,"00:03:10,135","00:03:15,875",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,And these two relations are in fact so
cs-410_7_7_39,cs-410,7,7,"00:03:17,365","00:03:24,180",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=197,generalized to capture basic relations
cs-410_7_7_40,cs-410,7,7,"00:03:24,180","00:03:27,880",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,And definitely they can be
cs-410_7_7_41,cs-410,7,7,"00:03:27,880","00:03:31,630",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,relations of any items in a language.
cs-410_7_7_42,cs-410,7,7,"00:03:31,630","00:03:36,620",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,"So, A and B don't have to be words and"
cs-410_7_7_43,cs-410,7,7,"00:03:37,960","00:03:44,710",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,And they can even be more complex
cs-410_7_7_44,cs-410,7,7,"00:03:44,710","00:03:48,820",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,If you think about the general
cs-410_7_7_45,cs-410,7,7,"00:03:48,820","00:03:53,066",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,then we can think about the units
cs-410_7_7_46,cs-410,7,7,"00:03:53,066","00:03:58,980",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=233,Then we think of paradigmatic
cs-410_7_7_47,cs-410,7,7,"00:03:58,980","00:04:05,890",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,are applied to units that tend to occur
cs-410_7_7_48,cs-410,7,7,"00:04:05,890","00:04:11,660",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,or in a sequence of data
cs-410_7_7_49,cs-410,7,7,"00:04:11,660","00:04:20,980",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,So they occur in similar locations
cs-410_7_7_50,cs-410,7,7,"00:04:20,980","00:04:25,415",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=260,Syntagmatical relation on
cs-410_7_7_51,cs-410,7,7,"00:04:25,415","00:04:30,210",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,co-occurrent elements that tend
cs-410_7_7_52,cs-410,7,7,"00:04:33,150","00:04:38,470",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,So these two are complimentary and
cs-410_7_7_53,cs-410,7,7,"00:04:38,470","00:04:42,810",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=278,And we're interested in discovering
cs-410_7_7_54,cs-410,7,7,"00:04:42,810","00:04:46,420",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,Discovering such worded
cs-410_7_7_55,cs-410,7,7,"00:04:47,480","00:04:52,920",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"First, such relations can be directly"
cs-410_7_7_56,cs-410,7,7,"00:04:52,920","00:04:58,880",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,"tasks, and this is because this is part"
cs-410_7_7_57,cs-410,7,7,"00:04:58,880","00:05:02,440",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,So if you know these two words
cs-410_7_7_58,cs-410,7,7,"00:05:02,440","00:05:04,970",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,and then you can help a lot of tasks.
cs-410_7_7_59,cs-410,7,7,"00:05:05,980","00:05:10,970",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,And grammar learning can be also
cs-410_7_7_60,cs-410,7,7,"00:05:10,970","00:05:15,130",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,Because if we can learn
cs-410_7_7_61,cs-410,7,7,"00:05:15,130","00:05:20,000",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=315,"then we form classes of words,"
cs-410_7_7_62,cs-410,7,7,"00:05:20,000","00:05:25,630",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,"And if we learn syntagmatic relations,"
cs-410_7_7_63,cs-410,7,7,"00:05:25,630","00:05:32,400",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,the rules for putting together a larger
cs-410_7_7_64,cs-410,7,7,"00:05:32,400","00:05:37,390",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,So we learn the structure and
cs-410_7_7_65,cs-410,7,7,"00:05:39,855","00:05:43,070",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,Word relations can be also very useful for
cs-410_7_7_66,cs-410,7,7,"00:05:43,070","00:05:46,580",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,many applications in text retrieval and
cs-410_7_7_67,cs-410,7,7,"00:05:46,580","00:05:50,520",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,"For example, in search and"
cs-410_7_7_68,cs-410,7,7,"00:05:50,520","00:05:55,930",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,"associations to modify a query,"
cs-410_7_7_69,cs-410,7,7,"00:05:55,930","00:06:00,480",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,introduce additional related words into
cs-410_7_7_70,cs-410,7,7,"00:06:01,590","00:06:03,390",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,It's often called a query expansion.
cs-410_7_7_71,cs-410,7,7,"00:06:05,290","00:06:10,030",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,Or you can use related words to
cs-410_7_7_72,cs-410,7,7,"00:06:10,030","00:06:11,660",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,to explore the information space.
cs-410_7_7_73,cs-410,7,7,"00:06:12,740","00:06:15,610",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=372,Another application is to
cs-410_7_7_74,cs-410,7,7,"00:06:15,610","00:06:19,790",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,automatically construct the top
cs-410_7_7_75,cs-410,7,7,"00:06:19,790","00:06:24,540",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=379,We can have words as nodes and
cs-410_7_7_76,cs-410,7,7,"00:06:24,540","00:06:27,930",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=384,A user could navigate from
cs-410_7_7_77,cs-410,7,7,"00:06:28,990","00:06:31,180",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,find information in the information space.
cs-410_7_7_78,cs-410,7,7,"00:06:33,620","00:06:40,620",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,"Finally, such word associations can also"
cs-410_7_7_79,cs-410,7,7,"00:06:40,620","00:06:45,680",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,"For example, we might be interested"
cs-410_7_7_80,cs-410,7,7,"00:06:45,680","00:06:48,620",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,negative opinions about the iPhone 6.
cs-410_7_7_81,cs-410,7,7,"00:06:48,620","00:06:55,180",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"In order to do that, we can look at what"
cs-410_7_7_82,cs-410,7,7,"00:06:55,180","00:07:01,630",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,a feature word like battery in
cs-410_7_7_83,cs-410,7,7,"00:07:01,630","00:07:05,147",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,Such a syntagmatical
cs-410_7_7_84,cs-410,7,7,"00:07:05,147","00:07:08,854",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,show the detailed opinions
cs-410_7_7_85,cs-410,7,7,"00:07:16,696","00:07:20,837",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,"So, how can we discover such"
cs-410_7_7_86,cs-410,7,7,"00:07:20,837","00:07:24,450",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,"Now, here are some intuitions"
cs-410_7_7_87,cs-410,7,7,"00:07:24,450","00:07:27,479",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,Now let's first look at
cs-410_7_7_88,cs-410,7,7,"00:07:29,080","00:07:32,940",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,Here we essentially can take
cs-410_7_7_89,cs-410,7,7,"00:07:34,150","00:07:38,440",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,So here you see some simple
cs-410_7_7_90,cs-410,7,7,"00:07:38,440","00:07:43,416",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,You can see they generally
cs-410_7_7_91,cs-410,7,7,"00:07:43,416","00:07:48,390",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,and that after all is the definition
cs-410_7_7_92,cs-410,7,7,"00:07:49,540","00:07:54,510",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,On the right side you can kind
cs-410_7_7_93,cs-410,7,7,"00:07:54,510","00:07:59,090",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,the context of cat and
cs-410_7_7_94,cs-410,7,7,"00:08:00,640","00:08:05,230",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,I've taken away cat and
cs-410_7_7_95,cs-410,7,7,"00:08:05,230","00:08:07,280",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,that you can see just the context.
cs-410_7_7_96,cs-410,7,7,"00:08:08,810","00:08:12,660",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,"Now, of course we can have different"
cs-410_7_7_97,cs-410,7,7,"00:08:13,810","00:08:19,528",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,"For example, we can look at"
cs-410_7_7_98,cs-410,7,7,"00:08:19,528","00:08:24,222",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,part of this context.
cs-410_7_7_99,cs-410,7,7,"00:08:24,222","00:08:28,000",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,So we can call this left context.
cs-410_7_7_100,cs-410,7,7,"00:08:28,000","00:08:34,800",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,What words occur before we see cat or dog?
cs-410_7_7_101,cs-410,7,7,"00:08:34,800","00:08:39,910",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=514,"So, you can see in this case, clearly"
cs-410_7_7_102,cs-410,7,7,"00:08:41,810","00:08:47,860",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,You generally say his cat or my cat and
cs-410_7_7_103,cs-410,7,7,"00:08:47,860","00:08:52,290",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,So that makes them similar
cs-410_7_7_104,cs-410,7,7,"00:08:53,660","00:08:58,880",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=533,"Similarly, if you look at the words"
cs-410_7_7_105,cs-410,7,7,"00:08:58,880","00:09:03,970",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,"which we can call right context,"
cs-410_7_7_106,cs-410,7,7,"00:09:03,970","00:09:07,490",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=543,"Of course, it's an extreme case,"
cs-410_7_7_107,cs-410,7,7,"00:09:08,670","00:09:12,883",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,"And in general,"
cs-410_7_7_108,cs-410,7,7,"00:09:12,883","00:09:15,170",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,that can't follow cat and dog.
cs-410_7_7_109,cs-410,7,7,"00:09:17,830","00:09:21,700",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=557,You can also even look
cs-410_7_7_110,cs-410,7,7,"00:09:21,700","00:09:24,690",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=561,And that might include all
cs-410_7_7_111,cs-410,7,7,"00:09:24,690","00:09:26,640",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,in sentences around this word.
cs-410_7_7_112,cs-410,7,7,"00:09:27,658","00:09:34,300",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"And even in the general context, you also"
cs-410_7_7_113,cs-410,7,7,"00:09:35,400","00:09:41,480",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,So this was just a suggestion
cs-410_7_7_114,cs-410,7,7,"00:09:41,480","00:09:47,000",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,relation by looking at
cs-410_7_7_115,cs-410,7,7,"00:09:47,000","00:09:50,900",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,"So, for example,"
cs-410_7_7_116,cs-410,7,7,"00:09:50,900","00:09:54,760",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=590,How similar are context of cat and
cs-410_7_7_117,cs-410,7,7,"00:09:56,240","00:10:01,630",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,In contrast how similar are context
cs-410_7_7_118,cs-410,7,7,"00:10:02,660","00:10:07,610",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,"Now, intuitively,"
cs-410_7_7_119,cs-410,7,7,"00:10:07,610","00:10:11,030",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,the context of dog would
cs-410_7_7_120,cs-410,7,7,"00:10:11,030","00:10:16,550",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,the context of cat and
cs-410_7_7_121,cs-410,7,7,"00:10:16,550","00:10:20,680",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,"That means, in the first case"
cs-410_7_7_122,cs-410,7,7,"00:10:21,910","00:10:25,940",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,between the context of cat and
cs-410_7_7_123,cs-410,7,7,"00:10:25,940","00:10:30,248",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=625,the similarity between context of cat and
cs-410_7_7_124,cs-410,7,7,"00:10:30,248","00:10:35,750",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,because they all not having a paradigmatic
cs-410_7_7_125,cs-410,7,7,"00:10:35,750","00:10:40,550",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,relationship and imagine what words
cs-410_7_7_126,cs-410,7,7,"00:10:40,550","00:10:44,900",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=640,It would be very different from
cs-410_7_7_127,cs-410,7,7,"00:10:46,620","00:10:50,340",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,So this is the basic idea of what
cs-410_7_7_128,cs-410,7,7,"00:10:52,040","00:10:54,180",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=652,What about the syntagmatic relation?
cs-410_7_7_129,cs-410,7,7,"00:10:54,180","00:10:58,550",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,"Well, here we're going to explore"
cs-410_7_7_130,cs-410,7,7,"00:10:58,550","00:11:02,430",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,again based on the definition
cs-410_7_7_131,cs-410,7,7,"00:11:03,990","00:11:05,600",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=663,Here you see the same sample of text.
cs-410_7_7_132,cs-410,7,7,"00:11:06,640","00:11:10,710",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,But here we're interested in knowing
cs-410_7_7_133,cs-410,7,7,"00:11:10,710","00:11:14,780",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,with the verb eats and
cs-410_7_7_134,cs-410,7,7,"00:11:16,380","00:11:20,880",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,And if you look at the right
cs-410_7_7_135,cs-410,7,7,"00:11:20,880","00:11:25,245",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,"you see,"
cs-410_7_7_136,cs-410,7,7,"00:11:27,110","00:11:30,140",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,I've taken away the word to its left and
cs-410_7_7_137,cs-410,7,7,"00:11:30,140","00:11:33,970",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=690,also the word to its
cs-410_7_7_138,cs-410,7,7,"00:11:35,340","00:11:41,900",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=695,"And then we ask the question, what words"
cs-410_7_7_139,cs-410,7,7,"00:11:43,650","00:11:47,960",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=703,And what words tend to
cs-410_7_7_140,cs-410,7,7,"00:11:49,560","00:11:54,997",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,Now thinking about this question
cs-410_7_7_141,cs-410,7,7,"00:11:54,997","00:12:00,830",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=714,relations because syntagmatic relations
cs-410_7_7_142,cs-410,7,7,"00:12:03,070","00:12:07,290",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=723,So the important question to ask for
cs-410_7_7_143,cs-410,7,7,"00:12:07,290","00:12:14,570",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,"whenever eats occurs,"
cs-410_7_7_144,cs-410,7,7,"00:12:16,180","00:12:19,120",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,So the question here has
cs-410_7_7_145,cs-410,7,7,"00:12:19,120","00:12:23,940",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=739,are some other words that tend
cs-410_7_7_146,cs-410,7,7,"00:12:23,940","00:12:28,240",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=743,Meaning that whenever you see eats
cs-410_7_7_147,cs-410,7,7,"00:12:29,620","00:12:34,660",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,"And if you don't see eats, probably,"
cs-410_7_7_148,cs-410,7,7,"00:12:36,560","00:12:40,210",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,So this intuition can help
cs-410_7_7_149,cs-410,7,7,"00:12:41,530","00:12:43,200",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,"Now again, consider example."
cs-410_7_7_150,cs-410,7,7,"00:12:44,210","00:12:48,170",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,How helpful is occurrence of eats for
cs-410_7_7_151,cs-410,7,7,"00:12:49,870","00:12:53,056",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,Right.
cs-410_7_7_152,cs-410,7,7,"00:12:53,056","00:12:58,930",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,in a sentence would generally help us
cs-410_7_7_153,cs-410,7,7,"00:12:58,930","00:13:01,801",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,"And if we see eats occur in the sentence,"
cs-410_7_7_154,cs-410,7,7,"00:13:01,801","00:13:05,770",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=781,that should increase the chance
cs-410_7_7_155,cs-410,7,7,"00:13:08,490","00:13:12,150",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,"In contrast,"
cs-410_7_7_156,cs-410,7,7,"00:13:12,150","00:13:15,710",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=792,how helpful is the occurrence of eats for
cs-410_7_7_157,cs-410,7,7,"00:13:17,330","00:13:20,270",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=797,Because eats and
cs-410_7_7_158,cs-410,7,7,"00:13:20,270","00:13:24,840",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,knowing whether eats occurred
cs-410_7_7_159,cs-410,7,7,"00:13:24,840","00:13:30,140",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=804,"really help us predict the weather,"
cs-410_7_7_160,cs-410,7,7,"00:13:30,140","00:13:34,100",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=810,So this is in contrast to
cs-410_7_7_161,cs-410,7,7,"00:13:35,550","00:13:38,790",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=815,This also helps explain that intuition
cs-410_7_7_162,cs-410,7,7,"00:13:38,790","00:13:43,100",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=818,behind the methods of what
cs-410_7_7_163,cs-410,7,7,"00:13:43,100","00:13:49,090",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,Mainly we need to capture the correlation
cs-410_7_7_164,cs-410,7,7,"00:13:50,440","00:13:52,860",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=830,So to summarize the general ideas for
cs-410_7_7_165,cs-410,7,7,"00:13:52,860","00:13:55,810",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=832,discovering word associations
cs-410_7_7_166,cs-410,7,7,"00:13:56,880","00:14:02,240",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=836,"For paradigmatic relation,"
cs-410_7_7_167,cs-410,7,7,"00:14:02,240","00:14:04,830",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=842,And then compute its context similarity.
cs-410_7_7_168,cs-410,7,7,"00:14:04,830","00:14:09,030",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,We're going to assume the words
cs-410_7_7_169,cs-410,7,7,"00:14:09,030","00:14:12,260",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=849,to have paradigmatic relation.
cs-410_7_7_170,cs-410,7,7,"00:14:14,640","00:14:19,970",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=854,"For syntagmatic relation, we will count"
cs-410_7_7_171,cs-410,7,7,"00:14:19,970","00:14:25,180",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,"in a context, which can be a sentence,"
cs-410_7_7_172,cs-410,7,7,"00:14:25,180","00:14:28,180",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=865,And we're going to compare
cs-410_7_7_173,cs-410,7,7,"00:14:28,180","00:14:31,660",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=868,their co-occurrences with
cs-410_7_7_174,cs-410,7,7,"00:14:33,280","00:14:36,660",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,We're going to assume words
cs-410_7_7_175,cs-410,7,7,"00:14:36,660","00:14:42,335",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=876,relatively low individual occurrences
cs-410_7_7_176,cs-410,7,7,"00:14:42,335","00:14:46,581",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=882,because they attempt to occur together and
cs-410_7_7_177,cs-410,7,7,"00:14:46,581","00:14:51,635",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,Note that the paradigmatic relation and
cs-410_7_7_178,cs-410,7,7,"00:14:51,635","00:14:57,065",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=891,are actually closely related
cs-410_7_7_179,cs-410,7,7,"00:14:57,065","00:15:02,810",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=897,related words tend to have syntagmatic
cs-410_7_7_180,cs-410,7,7,"00:15:02,810","00:15:05,420",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=902,They tend to be associated
cs-410_7_7_181,cs-410,7,7,"00:15:05,420","00:15:10,870",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=905,that suggests that we can also do join
cs-410_7_7_182,cs-410,7,7,"00:15:10,870","00:15:15,190",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=910,So these general ideas can be
cs-410_7_7_183,cs-410,7,7,"00:15:15,190","00:15:19,129",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=915,"And the course won't cover all of them,"
cs-410_7_7_184,cs-410,7,7,"00:15:19,129","00:15:24,774",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=919,we will cover at least some of
cs-410_7_7_185,cs-410,7,7,"00:15:24,774","00:15:27,669",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=924,discovering these relations.
cs-410_7_7_186,cs-410,7,7,"00:15:27,669","00:15:37,669",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=927,[MUSIC]
cs-410_7_8_1,cs-410,7,8,"00:00:00,025","00:00:05,683",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND] This lecture is a continued
cs-410_7_8_2,cs-410,7,8,"00:00:05,683","00:00:13,370",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=5,discussion of probabilistic topic models.
cs-410_7_8_3,cs-410,7,8,"00:00:13,370","00:00:19,990",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=13,"In this lecture, we're going to continue"
cs-410_7_8_4,cs-410,7,8,"00:00:19,990","00:00:24,970",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,We're going to talk about
cs-410_7_8_5,cs-410,7,8,"00:00:24,970","00:00:28,300",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,are interested in just mining
cs-410_7_8_6,cs-410,7,8,"00:00:30,880","00:00:35,910",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"So in this simple setup,"
cs-410_7_8_7,cs-410,7,8,"00:00:35,910","00:00:41,060",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=35,one document and
cs-410_7_8_8,cs-410,7,8,"00:00:41,060","00:00:44,810",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,So this is the simplest
cs-410_7_8_9,cs-410,7,8,"00:00:44,810","00:00:49,921",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,"The input now no longer has k,"
cs-410_7_8_10,cs-410,7,8,"00:00:49,921","00:00:55,670",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,know there is only one topic and the
cs-410_7_8_11,cs-410,7,8,"00:00:55,670","00:01:00,738",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,"In the output,"
cs-410_7_8_12,cs-410,7,8,"00:01:00,738","00:01:06,150",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,we assumed that the document
cs-410_7_8_13,cs-410,7,8,"00:01:06,150","00:01:10,532",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,So the main goal is just to discover
cs-410_7_8_14,cs-410,7,8,"00:01:10,532","00:01:12,930",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,"this single topic, as shown here."
cs-410_7_8_15,cs-410,7,8,"00:01:14,770","00:01:19,275",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=74,"As always, when we think about using a"
cs-410_7_8_16,cs-410,7,8,"00:01:19,275","00:01:24,280",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,we start with thinking about what
cs-410_7_8_17,cs-410,7,8,"00:01:24,280","00:01:28,880",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,from what perspective we're going to
cs-410_7_8_18,cs-410,7,8,"00:01:28,880","00:01:32,268",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,And then we're going to
cs-410_7_8_19,cs-410,7,8,"00:01:32,268","00:01:36,520",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,"the generating of the data,"
cs-410_7_8_20,cs-410,7,8,"00:01:36,520","00:01:41,310",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,Where our perspective just means we want
cs-410_7_8_21,cs-410,7,8,"00:01:41,310","00:01:45,700",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,"the data, so that the model will"
cs-410_7_8_22,cs-410,7,8,"00:01:45,700","00:01:48,770",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=105,discovering the knowledge that we want.
cs-410_7_8_23,cs-410,7,8,"00:01:48,770","00:01:54,210",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,And then we'll be thinking
cs-410_7_8_24,cs-410,7,8,"00:01:54,210","00:02:00,480",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,write down the microfunction to
cs-410_7_8_25,cs-410,7,8,"00:02:00,480","00:02:04,860",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,a data point will be
cs-410_7_8_26,cs-410,7,8,"00:02:05,900","00:02:10,370",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,And the likelihood function will have
cs-410_7_8_27,cs-410,7,8,"00:02:10,370","00:02:15,780",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,And then we argue our interest in
cs-410_7_8_28,cs-410,7,8,"00:02:15,780","00:02:21,680",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=135,by maximizing the likelihood which will
cs-410_7_8_29,cs-410,7,8,"00:02:21,680","00:02:26,710",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,These estimator parameters
cs-410_7_8_30,cs-410,7,8,"00:02:26,710","00:02:31,640",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,"of the mining hours,"
cs-410_7_8_31,cs-410,7,8,"00:02:31,640","00:02:35,320",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=151,parameters as the knowledge
cs-410_7_8_32,cs-410,7,8,"00:02:35,320","00:02:39,690",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,So let's look at these steps for
cs-410_7_8_33,cs-410,7,8,"00:02:39,690","00:02:45,970",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,Later we'll look at this procedure for
cs-410_7_8_34,cs-410,7,8,"00:02:45,970","00:02:50,170",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=165,"So our data, in this case is, just"
cs-410_7_8_35,cs-410,7,8,"00:02:50,170","00:02:52,520",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,Each word here is denoted by x sub i.
cs-410_7_8_36,cs-410,7,8,"00:02:52,520","00:02:56,800",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,Our model is a Unigram language model.
cs-410_7_8_37,cs-410,7,8,"00:02:56,800","00:03:03,420",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,A word distribution that we hope to
cs-410_7_8_38,cs-410,7,8,"00:03:03,420","00:03:08,950",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,So we will have as many parameters as many
cs-410_7_8_39,cs-410,7,8,"00:03:09,950","00:03:14,580",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,And for convenience we're
cs-410_7_8_40,cs-410,7,8,"00:03:14,580","00:03:18,270",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,denote the probability of word w sub i.
cs-410_7_8_41,cs-410,7,8,"00:03:20,450","00:03:23,384",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,And obviously these theta
cs-410_7_8_42,cs-410,7,8,"00:03:24,480","00:03:27,110",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=204,Now what does a likelihood
cs-410_7_8_43,cs-410,7,8,"00:03:27,110","00:03:30,970",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,"Well, this is just the probability"
cs-410_7_8_44,cs-410,7,8,"00:03:30,970","00:03:31,948",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,that given such a model.
cs-410_7_8_45,cs-410,7,8,"00:03:31,948","00:03:36,920",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,Because we assume the independence in
cs-410_7_8_46,cs-410,7,8,"00:03:36,920","00:03:41,010",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,the document will be just a product
cs-410_7_8_47,cs-410,7,8,"00:03:42,790","00:03:46,900",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,And since some word might
cs-410_7_8_48,cs-410,7,8,"00:03:46,900","00:03:51,070",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,So we can also rewrite this
cs-410_7_8_49,cs-410,7,8,"00:03:52,580","00:03:58,550",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=232,"So in this line, we have rewritten"
cs-410_7_8_50,cs-410,7,8,"00:03:58,550","00:04:05,360",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,over all the unique words in
cs-410_7_8_51,cs-410,7,8,"00:04:05,360","00:04:09,170",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,Now this is different
cs-410_7_8_52,cs-410,7,8,"00:04:09,170","00:04:13,990",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,"Well, the product is over different"
cs-410_7_8_53,cs-410,7,8,"00:04:15,040","00:04:19,694",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,"Now when we do this transformation,"
cs-410_7_8_54,cs-410,7,8,"00:04:19,694","00:04:24,120",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,introduce a counter function here.
cs-410_7_8_55,cs-410,7,8,"00:04:24,120","00:04:29,395",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=264,This denotes the count of
cs-410_7_8_56,cs-410,7,8,"00:04:29,395","00:04:33,390",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,similarly this is the count
cs-410_7_8_57,cs-410,7,8,"00:04:33,390","00:04:37,890",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,because these words might
cs-410_7_8_58,cs-410,7,8,"00:04:37,890","00:04:40,459",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,You can also see if a word did
cs-410_7_8_59,cs-410,7,8,"00:04:41,810","00:04:46,790",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,"It will have a zero count, therefore"
cs-410_7_8_60,cs-410,7,8,"00:04:46,790","00:04:50,410",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,So this is a very useful form of
cs-410_7_8_61,cs-410,7,8,"00:04:50,410","00:04:55,060",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,writing down the likelihood function
cs-410_7_8_62,cs-410,7,8,"00:04:55,060","00:05:01,230",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,"So I want you to pay attention to this,"
cs-410_7_8_63,cs-410,7,8,"00:05:01,230","00:05:07,120",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=301,It's just to change the product over all
cs-410_7_8_64,cs-410,7,8,"00:05:07,120","00:05:12,013",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,"So in the end, of course, we'll use"
cs-410_7_8_65,cs-410,7,8,"00:05:12,013","00:05:14,512",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=312,function and it would look like this.
cs-410_7_8_66,cs-410,7,8,"00:05:14,512","00:05:19,468",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,"Next, we're going to find"
cs-410_7_8_67,cs-410,7,8,"00:05:19,468","00:05:24,530",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,of these words that would maximize
cs-410_7_8_68,cs-410,7,8,"00:05:24,530","00:05:30,539",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,So now lets take a look at the maximum
cs-410_7_8_69,cs-410,7,8,"00:05:32,520","00:05:35,870",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,This line is copied from
cs-410_7_8_70,cs-410,7,8,"00:05:35,870","00:05:37,340",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,It's just our likelihood function.
cs-410_7_8_71,cs-410,7,8,"00:05:38,590","00:05:43,950",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,So our goal is to maximize
cs-410_7_8_72,cs-410,7,8,"00:05:43,950","00:05:46,210",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,We will find it often easy to
cs-410_7_8_73,cs-410,7,8,"00:05:47,310","00:05:51,110",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=347,maximize the local likelihood
cs-410_7_8_74,cs-410,7,8,"00:05:51,110","00:05:56,531",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=351,And this is purely for
cs-410_7_8_75,cs-410,7,8,"00:05:56,531","00:06:03,698",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=356,the logarithm transformation our function
cs-410_7_8_76,cs-410,7,8,"00:06:03,698","00:06:10,704",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=363,And we also have constraints
cs-410_7_8_77,cs-410,7,8,"00:06:10,704","00:06:16,743",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,The sum makes it easier to take
cs-410_7_8_78,cs-410,7,8,"00:06:16,743","00:06:21,022",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,finding the optimal
cs-410_7_8_79,cs-410,7,8,"00:06:21,022","00:06:27,349",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,"So please take a look at this sum again,"
cs-410_7_8_80,cs-410,7,8,"00:06:27,349","00:06:32,434",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,And this is a form of
cs-410_7_8_81,cs-410,7,8,"00:06:32,434","00:06:38,430",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,"see later also,"
cs-410_7_8_82,cs-410,7,8,"00:06:38,430","00:06:42,340",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,So it's a sum over all
cs-410_7_8_83,cs-410,7,8,"00:06:42,340","00:06:48,105",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,And inside the sum there is
cs-410_7_8_84,cs-410,7,8,"00:06:48,105","00:06:54,980",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,And this is macroed by
cs-410_7_8_85,cs-410,7,8,"00:06:55,990","00:06:57,920",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,So let's see how we can
cs-410_7_8_86,cs-410,7,8,"00:06:58,920","00:07:04,030",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,Now at this point the problem is purely a
cs-410_7_8_87,cs-410,7,8,"00:07:04,030","00:07:11,360",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,to just the find the optimal solution
cs-410_7_8_88,cs-410,7,8,"00:07:11,360","00:07:14,694",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,The objective function is
cs-410_7_8_89,cs-410,7,8,"00:07:14,694","00:07:18,621",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,the constraint is that all these
cs-410_7_8_90,cs-410,7,8,"00:07:18,621","00:07:23,234",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=438,"So, one way to solve the problem is"
cs-410_7_8_91,cs-410,7,8,"00:07:24,520","00:07:29,040",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,Now this command is beyond
cs-410_7_8_92,cs-410,7,8,"00:07:29,040","00:07:33,670",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=449,since Lagrange multiplier is a very
cs-410_7_8_93,cs-410,7,8,"00:07:33,670","00:07:37,940",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,"to just give a brief introduction to this,"
cs-410_7_8_94,cs-410,7,8,"00:07:39,720","00:07:43,857",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,So in this approach we will
cs-410_7_8_95,cs-410,7,8,"00:07:43,857","00:07:49,887",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,And this function will combine
cs-410_7_8_96,cs-410,7,8,"00:07:49,887","00:07:55,392",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,with another term that
cs-410_7_8_97,cs-410,7,8,"00:07:55,392","00:07:59,980",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=475,"we introduce Lagrange multiplier here,"
cs-410_7_8_98,cs-410,7,8,"00:07:59,980","00:08:04,978",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=479,"lambda, so it's an additional parameter."
cs-410_7_8_99,cs-410,7,8,"00:08:04,978","00:08:10,432",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=484,"Now, the idea of this approach is just to"
cs-410_7_8_100,cs-410,7,8,"00:08:10,432","00:08:14,800",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"in some sense,"
cs-410_7_8_101,cs-410,7,8,"00:08:14,800","00:08:18,318",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=494,Now we are just interested in
cs-410_7_8_102,cs-410,7,8,"00:08:19,460","00:08:24,022",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=499,"As you may recall from calculus,"
cs-410_7_8_103,cs-410,7,8,"00:08:24,022","00:08:29,910",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,would be achieved when
cs-410_7_8_104,cs-410,7,8,"00:08:29,910","00:08:31,673",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,This is a necessary condition.
cs-410_7_8_105,cs-410,7,8,"00:08:31,673","00:08:33,182",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=511,"It's not sufficient, though."
cs-410_7_8_106,cs-410,7,8,"00:08:33,182","00:08:38,205",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=513,So if we do that you will
cs-410_7_8_107,cs-410,7,8,"00:08:38,205","00:08:42,785",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,with respect to theta i
cs-410_7_8_108,cs-410,7,8,"00:08:42,785","00:08:50,815",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=522,And this part comes from the derivative
cs-410_7_8_109,cs-410,7,8,"00:08:50,815","00:08:55,390",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=530,this lambda is simply taken from here.
cs-410_7_8_110,cs-410,7,8,"00:08:55,390","00:09:00,178",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,And when we set it to zero we can
cs-410_7_8_111,cs-410,7,8,"00:09:00,178","00:09:05,610",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,easily see theta sub i is
cs-410_7_8_112,cs-410,7,8,"00:09:06,820","00:09:09,900",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=546,Since we know all the theta
cs-410_7_8_113,cs-410,7,8,"00:09:09,900","00:09:12,423",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=549,"we can plug this into this constraint,"
cs-410_7_8_114,cs-410,7,8,"00:09:12,423","00:09:15,600",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=552,And this will allow us to solve for
cs-410_7_8_115,cs-410,7,8,"00:09:16,630","00:09:20,840",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,And this is just a net
cs-410_7_8_116,cs-410,7,8,"00:09:20,840","00:09:27,350",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,And this further allows us to then
cs-410_7_8_117,cs-410,7,8,"00:09:27,350","00:09:31,380",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"eventually, to find the optimal"
cs-410_7_8_118,cs-410,7,8,"00:09:31,380","00:09:37,280",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,And if you look at this formula it turns
cs-410_7_8_119,cs-410,7,8,"00:09:37,280","00:09:43,089",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=577,because this is just the normalized
cs-410_7_8_120,cs-410,7,8,"00:09:43,089","00:09:47,751",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=583,which is also a sum of all
cs-410_7_8_121,cs-410,7,8,"00:09:47,751","00:09:52,157",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=587,"So, after all this mess, after all,"
cs-410_7_8_122,cs-410,7,8,"00:09:52,157","00:09:59,044",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=592,we have just obtained something
cs-410_7_8_123,cs-410,7,8,"00:09:59,044","00:10:04,415",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,this will be just our
cs-410_7_8_124,cs-410,7,8,"00:10:04,415","00:10:10,338",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,maximize the data by
cs-410_7_8_125,cs-410,7,8,"00:10:10,338","00:10:16,419",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=610,mass as possible to all
cs-410_7_8_126,cs-410,7,8,"00:10:16,419","00:10:21,408",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=616,And you might also notice that this is
cs-410_7_8_127,cs-410,7,8,"00:10:21,408","00:10:23,450",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=621,raised estimator.
cs-410_7_8_128,cs-410,7,8,"00:10:23,450","00:10:29,333",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=623,"In general, the estimator would be to"
cs-410_7_8_129,cs-410,7,8,"00:10:29,333","00:10:35,050",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=629,the counts have to be done in a particular
cs-410_7_8_130,cs-410,7,8,"00:10:35,050","00:10:41,730",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=635,So this is basically an analytical
cs-410_7_8_131,cs-410,7,8,"00:10:41,730","00:10:46,303",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,"In general though, when the likelihood"
cs-410_7_8_132,cs-410,7,8,"00:10:46,303","00:10:50,919",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,going to be able to solve the optimization
cs-410_7_8_133,cs-410,7,8,"00:10:50,919","00:10:55,134",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,Instead we have to use some
cs-410_7_8_134,cs-410,7,8,"00:10:55,134","00:10:58,787",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=655,"we're going to see such cases later, also."
cs-410_7_8_135,cs-410,7,8,"00:10:58,787","00:11:02,385",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,So if you imagine what would we
cs-410_7_8_136,cs-410,7,8,"00:11:02,385","00:11:07,146",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,likelihood estimator to estimate one
cs-410_7_8_137,cs-410,7,8,"00:11:07,146","00:11:09,903",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=667,Let's imagine this document
cs-410_7_8_138,cs-410,7,8,"00:11:09,903","00:11:16,277",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=669,"Now, what you might see is"
cs-410_7_8_139,cs-410,7,8,"00:11:16,277","00:11:20,555",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=676,"On the top, you will see the high"
cs-410_7_8_140,cs-410,7,8,"00:11:20,555","00:11:23,710",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=680,"common words,"
cs-410_7_8_141,cs-410,7,8,"00:11:23,710","00:11:27,742",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,And this will be followed by
cs-410_7_8_142,cs-410,7,8,"00:11:27,742","00:11:31,622",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,"characterize the topic well like text,"
cs-410_7_8_143,cs-410,7,8,"00:11:31,622","00:11:36,275",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,"And then in the end,"
cs-410_7_8_144,cs-410,7,8,"00:11:36,275","00:11:40,017",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=696,words that are not really
cs-410_7_8_145,cs-410,7,8,"00:11:40,017","00:11:44,320",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,they might be extraneously
cs-410_7_8_146,cs-410,7,8,"00:11:44,320","00:11:49,590",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,"As a topic representation,"
cs-410_7_8_147,cs-410,7,8,"00:11:49,590","00:11:52,452",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,That because the high probability
cs-410_7_8_148,cs-410,7,8,"00:11:52,452","00:11:55,310",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,they are not really
cs-410_7_8_149,cs-410,7,8,"00:11:55,310","00:11:58,280",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=715,So my question is how can we
cs-410_7_8_150,cs-410,7,8,"00:11:59,720","00:12:02,680",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=719,Now this is the topic of the next module.
cs-410_7_8_151,cs-410,7,8,"00:12:02,680","00:12:06,913",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=722,We're going to talk about how to use
cs-410_7_8_152,cs-410,7,8,"00:12:06,913","00:12:08,077",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=726,these common words.
cs-410_7_8_153,cs-410,7,8,"00:12:08,077","00:12:18,077",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=728,[MUSIC]
cs-410_8_5_1,cs-410,8,5,"00:00:00,000","00:00:06,073",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_8_5_2,cs-410,8,5,"00:00:06,073","00:00:13,035",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,we talked about PageRank as
cs-410_8_5_3,cs-410,8,5,"00:00:14,155","00:00:21,245",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,"Now, we also looked at some other examples"
cs-410_8_5_4,cs-410,8,5,"00:00:21,245","00:00:24,425",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,"So there is another algorithm called HITS,"
cs-410_8_5_5,cs-410,8,5,"00:00:24,425","00:00:28,257",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,that going to compute the scores for
cs-410_8_5_6,cs-410,8,5,"00:00:28,257","00:00:33,167",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=28,The intuitions are pages that are widely
cs-410_8_5_7,cs-410,8,5,"00:00:33,167","00:00:38,577",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,whereas pages that cite many
cs-410_8_5_8,cs-410,8,5,"00:00:38,577","00:00:42,650",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,I think that the most interesting
cs-410_8_5_9,cs-410,8,5,"00:00:42,650","00:00:46,930",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,is it's going to use
cs-410_8_5_10,cs-410,8,5,"00:00:46,930","00:00:51,470",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=46,to kind of help improve the scoring for
cs-410_8_5_11,cs-410,8,5,"00:00:51,470","00:00:53,318",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=51,"And so here's the idea,"
cs-410_8_5_12,cs-410,8,5,"00:00:53,318","00:00:57,809",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=53,it was assumed that good
cs-410_8_5_13,cs-410,8,5,"00:00:58,870","00:01:03,847",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,That means if you are cited by many
cs-410_8_5_14,cs-410,8,5,"00:01:03,847","00:01:07,266",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,"that inquiry says, you're an authority."
cs-410_8_5_15,cs-410,8,5,"00:01:07,266","00:01:11,740",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=67,"And similarly, good hubs are those"
cs-410_8_5_16,cs-410,8,5,"00:01:11,740","00:01:15,560",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,So if you pointed to a lot
cs-410_8_5_17,cs-410,8,5,"00:01:15,560","00:01:17,880",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,then your hubs score would be increased.
cs-410_8_5_18,cs-410,8,5,"00:01:17,880","00:01:22,115",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,So then you will have literally reinforced
cs-410_8_5_19,cs-410,8,5,"00:01:22,115","00:01:22,968",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,some good hubs.
cs-410_8_5_20,cs-410,8,5,"00:01:22,968","00:01:27,635",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=82,And so you have pointed to some good
cs-410_8_5_21,cs-410,8,5,"00:01:27,635","00:01:30,544",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,whereas those authority
cs-410_8_5_22,cs-410,8,5,"00:01:30,544","00:01:34,736",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,improved because they
cs-410_8_5_23,cs-410,8,5,"00:01:34,736","00:01:39,380",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,And this is algorithms is also general it
cs-410_8_5_24,cs-410,8,5,"00:01:39,380","00:01:40,730",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,network analysis.
cs-410_8_5_25,cs-410,8,5,"00:01:40,730","00:01:43,170",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=100,"So just briefly, here's how it works."
cs-410_8_5_26,cs-410,8,5,"00:01:43,170","00:01:47,090",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"We first also construct a matrix, but this"
cs-410_8_5_27,cs-410,8,5,"00:01:47,090","00:01:49,750",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=107,matrix and
cs-410_8_5_28,cs-410,8,5,"00:01:49,750","00:01:54,100",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"So if there's a link there's a 1,"
cs-410_8_5_29,cs-410,8,5,"00:01:54,100","00:01:56,185",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,"Again, it's the same graph."
cs-410_8_5_30,cs-410,8,5,"00:01:56,185","00:02:01,335",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,And then we're going to
cs-410_8_5_31,cs-410,8,5,"00:02:01,335","00:02:06,955",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,as the sum of the authority scores of
cs-410_8_5_32,cs-410,8,5,"00:02:08,270","00:02:09,620",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=128,"So whether you are hub,"
cs-410_8_5_33,cs-410,8,5,"00:02:09,620","00:02:14,430",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,really depends on whether you are pointing
cs-410_8_5_34,cs-410,8,5,"00:02:14,430","00:02:17,080",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,That's what it says in the first equation.
cs-410_8_5_35,cs-410,8,5,"00:02:17,080","00:02:22,130",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=137,"In the second equation,"
cs-410_8_5_36,cs-410,8,5,"00:02:22,130","00:02:27,350",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,as a sum of the hub scores of all
cs-410_8_5_37,cs-410,8,5,"00:02:27,350","00:02:30,260",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,So whether you are good authority
cs-410_8_5_38,cs-410,8,5,"00:02:30,260","00:02:33,420",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,pages that are pointing
cs-410_8_5_39,cs-410,8,5,"00:02:33,420","00:02:37,380",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,So you can see this forms
cs-410_8_5_40,cs-410,8,5,"00:02:38,770","00:02:44,586",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=158,"Now, these three questions can be"
cs-410_8_5_41,cs-410,8,5,"00:02:44,586","00:02:50,707",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,So what we get here is then the hub
cs-410_8_5_42,cs-410,8,5,"00:02:50,707","00:02:55,770",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,of the adjacency matrix and
cs-410_8_5_43,cs-410,8,5,"00:02:55,770","00:03:00,026",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,and this is basically the first equation.
cs-410_8_5_44,cs-410,8,5,"00:03:00,026","00:03:05,292",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=180,"And similarly, the second equation"
cs-410_8_5_45,cs-410,8,5,"00:03:05,292","00:03:11,034",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,vector is equal to the product of
cs-410_8_5_46,cs-410,8,5,"00:03:11,034","00:03:15,820",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,"Now, these are just different ways"
cs-410_8_5_47,cs-410,8,5,"00:03:15,820","00:03:19,967",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,But what's interesting is that
cs-410_8_5_48,cs-410,8,5,"00:03:19,967","00:03:26,680",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=199,you can also plug in the authority
cs-410_8_5_49,cs-410,8,5,"00:03:26,680","00:03:31,500",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=206,"So if you do that, you have actually"
cs-410_8_5_50,cs-410,8,5,"00:03:31,500","00:03:33,930",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,and you get the equations
cs-410_8_5_51,cs-410,8,5,"00:03:34,980","00:03:39,032",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,The hubs score vector is
cs-410_8_5_52,cs-410,8,5,"00:03:39,032","00:03:43,522",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,by a transpose multiplied
cs-410_8_5_53,cs-410,8,5,"00:03:43,522","00:03:47,511",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,"Similarly, we can do a transformation"
cs-410_8_5_54,cs-410,8,5,"00:03:47,511","00:03:49,440",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,just the authorities also.
cs-410_8_5_55,cs-410,8,5,"00:03:49,440","00:03:54,370",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,So although we frame the problem
cs-410_8_5_56,cs-410,8,5,"00:03:54,370","00:03:58,410",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,we can actually eliminate one of them to
cs-410_8_5_57,cs-410,8,5,"00:03:59,530","00:04:03,810",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"Now, the difference between this and page"
cs-410_8_5_58,cs-410,8,5,"00:04:03,810","00:04:07,960",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,a multiplication of the adjacency
cs-410_8_5_59,cs-410,8,5,"00:04:07,960","00:04:09,840",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,So this is different from page rank.
cs-410_8_5_60,cs-410,8,5,"00:04:11,250","00:04:15,373",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,"But mathematically, then we will"
cs-410_8_5_61,cs-410,8,5,"00:04:15,373","00:04:19,777",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=255,"So in HITS,"
cs-410_8_5_62,cs-410,8,5,"00:04:19,777","00:04:22,215",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,"Let's say, 1 for all these values, and"
cs-410_8_5_63,cs-410,8,5,"00:04:22,215","00:04:26,580",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,then we would iteratively apply
cs-410_8_5_64,cs-410,8,5,"00:04:26,580","00:04:33,100",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,And this is equivalent to multiply
cs-410_8_5_65,cs-410,8,5,"00:04:34,720","00:04:37,740",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,So the arrows of these is exactly
cs-410_8_5_66,cs-410,8,5,"00:04:37,740","00:04:43,035",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=277,But here because the adjacency
cs-410_8_5_67,cs-410,8,5,"00:04:43,035","00:04:47,463",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=283,So what we have to do is after each
cs-410_8_5_68,cs-410,8,5,"00:04:47,463","00:04:50,980",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,this would allow us to
cs-410_8_5_69,cs-410,8,5,"00:04:50,980","00:04:53,671",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,Otherwise they would grow larger and
cs-410_8_5_70,cs-410,8,5,"00:04:53,671","00:04:57,180",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=293,"And if we do that, and"
cs-410_8_5_71,cs-410,8,5,"00:04:58,360","00:05:03,920",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=298,"That was the computer, the hubs scores,"
cs-410_8_5_72,cs-410,8,5,"00:05:03,920","00:05:08,647",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=303,And these scores can then be used in
cs-410_8_5_73,cs-410,8,5,"00:05:09,860","00:05:14,525",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=309,"So to summarize in this lecture, we have"
cs-410_8_5_74,cs-410,8,5,"00:05:14,525","00:05:19,302",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,"In particular,"
cs-410_8_5_75,cs-410,8,5,"00:05:19,302","00:05:23,737",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=319,increase the text
cs-410_8_5_76,cs-410,8,5,"00:05:23,737","00:05:25,959",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=323,And we also talk about the PageRank and
cs-410_8_5_77,cs-410,8,5,"00:05:25,959","00:05:29,380",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,page anchor as two major
cs-410_8_5_78,cs-410,8,5,"00:05:29,380","00:05:35,930",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=329,Both can generate scores for web pages
cs-410_8_5_79,cs-410,8,5,"00:05:35,930","00:05:39,600",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=335,Note that PageRank and
cs-410_8_5_80,cs-410,8,5,"00:05:39,600","00:05:46,663",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=339,So they have many applications in
cs-410_8_5_81,cs-410,8,5,"00:05:46,663","00:05:56,663",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,[MUSIC]
cs-410_8_6_1,cs-410,8,6,"00:00:00,012","00:00:09,135",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_8_6_2,cs-410,8,6,"00:00:09,135","00:00:12,960",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=9,here we're going to talk
cs-410_8_6_3,cs-410,8,6,"00:00:12,960","00:00:18,430",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,And that would be based on
cs-410_8_6_4,cs-410,8,6,"00:00:18,430","00:00:23,220",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=18,then predicting the rating of and
cs-410_8_6_5,cs-410,8,6,"00:00:23,220","00:00:32,540",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,object by an active user using the ratings
cs-410_8_6_6,cs-410,8,6,"00:00:32,540","00:00:38,600",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=32,This is called a memory based approach
cs-410_8_6_7,cs-410,8,6,"00:00:40,120","00:00:44,460",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=40,storing all the user information and
cs-410_8_6_8,cs-410,8,6,"00:00:44,460","00:00:49,713",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,when we are considering a particular
cs-410_8_6_9,cs-410,8,6,"00:00:49,713","00:00:56,210",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=49,retrieve the rating users or
cs-410_8_6_10,cs-410,8,6,"00:00:56,210","00:01:01,140",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,And then try to use this
cs-410_8_6_11,cs-410,8,6,"00:01:01,140","00:01:05,120",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,to predict the preference of this user.
cs-410_8_6_12,cs-410,8,6,"00:01:05,120","00:01:11,700",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,So here is the general idea and
cs-410_8_6_13,cs-410,8,6,"00:01:11,700","00:01:16,570",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,x sub i j denotes the rating
cs-410_8_6_14,cs-410,8,6,"00:01:17,910","00:01:23,460",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,and n sub i is average rating
cs-410_8_6_15,cs-410,8,6,"00:01:26,100","00:01:31,050",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,So this n i is needed because
cs-410_8_6_16,cs-410,8,6,"00:01:31,050","00:01:35,500",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=91,we would like to normalize
cs-410_8_6_17,cs-410,8,6,"00:01:35,500","00:01:39,190",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=95,So how do you do normalization?
cs-410_8_6_18,cs-410,8,6,"00:01:39,190","00:01:46,440",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,"Well, we're going to just subtract"
cs-410_8_6_19,cs-410,8,6,"00:01:46,440","00:01:49,890",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=106,"Now, this is to normalize these ratings so"
cs-410_8_6_20,cs-410,8,6,"00:01:49,890","00:01:53,510",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,that the ratings from different
cs-410_8_6_21,cs-410,8,6,"00:01:55,590","00:02:00,220",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=115,"Because some users might be more generous,"
cs-410_8_6_22,cs-410,8,6,"00:02:00,220","00:02:05,160",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,ratings but some others might be
cs-410_8_6_23,cs-410,8,6,"00:02:05,160","00:02:10,850",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,cannot be directly compared with each
cs-410_8_6_24,cs-410,8,6,"00:02:10,850","00:02:13,450",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,So we need to do this normalization.
cs-410_8_6_25,cs-410,8,6,"00:02:13,450","00:02:18,420",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,Another prediction of
cs-410_8_6_26,cs-410,8,6,"00:02:18,420","00:02:22,880",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=138,by another user or
cs-410_8_6_27,cs-410,8,6,"00:02:24,460","00:02:29,419",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,can be based on the average
cs-410_8_6_28,cs-410,8,6,"00:02:30,630","00:02:36,960",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,So the user u sub a is the user that we
cs-410_8_6_29,cs-410,8,6,"00:02:36,960","00:02:42,400",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,And we now are interested in
cs-410_8_6_30,cs-410,8,6,"00:02:42,400","00:02:47,910",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=162,So we're interested in knowing how
cs-410_8_6_31,cs-410,8,6,"00:02:47,910","00:02:49,370",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,How do we know that?
cs-410_8_6_32,cs-410,8,6,"00:02:50,370","00:02:55,560",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,Where the idea here is to look at
cs-410_8_6_33,cs-410,8,6,"00:02:55,560","00:02:57,260",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,have liked this object.
cs-410_8_6_34,cs-410,8,6,"00:02:59,530","00:03:04,720",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,So mathematically this is to say
cs-410_8_6_35,cs-410,8,6,"00:03:04,720","00:03:12,130",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=184,"this user on this app object,"
cs-410_8_6_36,cs-410,8,6,"00:03:12,130","00:03:18,640",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,combination of the normalized
cs-410_8_6_37,cs-410,8,6,"00:03:18,640","00:03:23,950",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=198,"and in fact here,"
cs-410_8_6_38,cs-410,8,6,"00:03:23,950","00:03:29,180",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=203,But not all users contribute
cs-410_8_6_39,cs-410,8,6,"00:03:29,180","00:03:31,748",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,and this is conjured by the weights.
cs-410_8_6_40,cs-410,8,6,"00:03:31,748","00:03:37,191",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=211,So this weight controls the inference
cs-410_8_6_41,cs-410,8,6,"00:03:37,191","00:03:41,618",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=217,of the user on the prediction.
cs-410_8_6_42,cs-410,8,6,"00:03:41,618","00:03:46,763",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,"And of course,"
cs-410_8_6_43,cs-410,8,6,"00:03:46,763","00:03:51,917",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=226,the similarity between ua and
cs-410_8_6_44,cs-410,8,6,"00:03:51,917","00:03:57,650",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=231,"The more similar they are,"
cs-410_8_6_45,cs-410,8,6,"00:03:57,650","00:04:02,420",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=237,user ui can make in predicting
cs-410_8_6_46,cs-410,8,6,"00:04:03,950","00:04:06,060",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=243,"So, the formula is extremely simple."
cs-410_8_6_47,cs-410,8,6,"00:04:06,060","00:04:10,140",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,"You can see,"
cs-410_8_6_48,cs-410,8,6,"00:04:10,140","00:04:14,040",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,"And inside the sum we have their ratings,"
cs-410_8_6_49,cs-410,8,6,"00:04:14,040","00:04:17,380",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,their normalized ratings
cs-410_8_6_50,cs-410,8,6,"00:04:17,380","00:04:21,400",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=257,The ratings need to be normalized in
cs-410_8_6_51,cs-410,8,6,"00:04:22,690","00:04:25,739",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=262,And then these ratings
cs-410_8_6_52,cs-410,8,6,"00:04:26,750","00:04:33,310",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=266,So you can imagine w of a and i is just
cs-410_8_6_53,cs-410,8,6,"00:04:34,470","00:04:35,350",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=274,Now what's k here?
cs-410_8_6_54,cs-410,8,6,"00:04:35,350","00:04:39,120",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,Well k is simply a normalizer.
cs-410_8_6_55,cs-410,8,6,"00:04:39,120","00:04:45,670",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,It's just one over the sum of all
cs-410_8_6_56,cs-410,8,6,"00:04:47,860","00:04:54,680",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=287,"So this means, basically, if you consider"
cs-410_8_6_57,cs-410,8,6,"00:04:54,680","00:04:59,259",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,we have coefficients of weight that
cs-410_8_6_58,cs-410,8,6,"00:05:00,430","00:05:05,690",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=300,And it's just a normalization strategy so
cs-410_8_6_59,cs-410,8,6,"00:05:05,690","00:05:12,319",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=305,in the same range as these ratings
cs-410_8_6_60,cs-410,8,6,"00:05:13,650","00:05:14,560",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,Right?
cs-410_8_6_61,cs-410,8,6,"00:05:14,560","00:05:20,320",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,So this is basically the main idea
cs-410_8_6_62,cs-410,8,6,"00:05:20,320","00:05:21,270",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,collaborative filtering.
cs-410_8_6_63,cs-410,8,6,"00:05:22,750","00:05:27,880",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,"Once we make this prediction,"
cs-410_8_6_64,cs-410,8,6,"00:05:27,880","00:05:33,270",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=327,back through the rating that
cs-410_8_6_65,cs-410,8,6,"00:05:33,270","00:05:38,520",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=333,"the user would actually make,"
cs-410_8_6_66,cs-410,8,6,"00:05:38,520","00:05:44,110",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=338,and this is to further
cs-410_8_6_67,cs-410,8,6,"00:05:44,110","00:05:49,980",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=344,average rating of this user u
cs-410_8_6_68,cs-410,8,6,"00:05:49,980","00:05:54,290",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=349,This would recover a meaningful rating for
cs-410_8_6_69,cs-410,8,6,"00:05:54,290","00:05:59,410",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=354,"So if this user is generous, then"
cs-410_8_6_70,cs-410,8,6,"00:05:59,410","00:06:04,580",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,and when we add that the rating will be
cs-410_8_6_71,cs-410,8,6,"00:06:04,580","00:06:10,459",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,Now when you recommend an item to a user
cs-410_8_6_72,cs-410,8,6,"00:06:10,459","00:06:15,093",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,because you are interested in
cs-410_8_6_73,cs-410,8,6,"00:06:15,093","00:06:17,158",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,that's more meaningful.
cs-410_8_6_74,cs-410,8,6,"00:06:17,158","00:06:22,624",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,But when they evaluate these
cs-410_8_6_75,cs-410,8,6,"00:06:22,624","00:06:27,563",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,they typically assume that
cs-410_8_6_76,cs-410,8,6,"00:06:27,563","00:06:32,923",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,these objects to be unknown and
cs-410_8_6_77,cs-410,8,6,"00:06:32,923","00:06:38,938",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=392,then you compare the predicted
cs-410_8_6_78,cs-410,8,6,"00:06:38,938","00:06:42,020",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,"So, you do have access"
cs-410_8_6_79,cs-410,8,6,"00:06:42,020","00:06:44,100",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,"But, then you pretend that you don't know,"
cs-410_8_6_80,cs-410,8,6,"00:06:44,100","00:06:48,420",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,then you compare your systems
cs-410_8_6_81,cs-410,8,6,"00:06:48,420","00:06:54,130",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"In that case, obviously, the systems"
cs-410_8_6_82,cs-410,8,6,"00:06:54,130","00:06:59,570",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=414,the actual ratings of the user and
cs-410_8_6_83,cs-410,8,6,"00:07:01,040","00:07:05,160",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=421,Okay so this is the memory based approach.
cs-410_8_6_84,cs-410,8,6,"00:07:05,160","00:07:07,000",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"Now, of course,"
cs-410_8_6_85,cs-410,8,6,"00:07:07,000","00:07:09,430",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,if you want to write
cs-410_8_6_86,cs-410,8,6,"00:07:09,430","00:07:15,510",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=429,you still face the problem of
cs-410_8_6_87,cs-410,8,6,"00:07:15,510","00:07:20,890",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,"Once you know the w function, then"
cs-410_8_6_88,cs-410,8,6,"00:07:22,740","00:07:28,910",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,"So, indeed, there are many different ways"
cs-410_8_6_89,cs-410,8,6,"00:07:28,910","00:07:33,550",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,"w, and specific approaches generally"
cs-410_8_6_90,cs-410,8,6,"00:07:35,500","00:07:38,220",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,So here are some possibilities and
cs-410_8_6_91,cs-410,8,6,"00:07:38,220","00:07:42,010",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=458,you can imagine there
cs-410_8_6_92,cs-410,8,6,"00:07:42,010","00:07:46,460",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=462,One popular approach is we use
cs-410_8_6_93,cs-410,8,6,"00:07:48,130","00:07:52,380",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=468,This would be a sum over
cs-410_8_6_94,cs-410,8,6,"00:07:52,380","00:07:56,280",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,And the formula is a standard
cs-410_8_6_95,cs-410,8,6,"00:07:56,280","00:07:58,690",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=476,coefficient formula as shown here.
cs-410_8_6_96,cs-410,8,6,"00:08:00,060","00:08:05,300",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,So this basically measures
cs-410_8_6_97,cs-410,8,6,"00:08:05,300","00:08:10,229",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,to all give higher ratings to similar
cs-410_8_6_98,cs-410,8,6,"00:08:11,780","00:08:15,990",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=491,"Another measure is the cosine measure,"
cs-410_8_6_99,cs-410,8,6,"00:08:15,990","00:08:20,820",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=495,vectors as vectors in the vector space.
cs-410_8_6_100,cs-410,8,6,"00:08:20,820","00:08:24,400",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=500,"And then,"
cs-410_8_6_101,cs-410,8,6,"00:08:24,400","00:08:27,880",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=504,compute the cosine of
cs-410_8_6_102,cs-410,8,6,"00:08:27,880","00:08:32,590",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,And this measure has been using the vector
cs-410_8_6_103,cs-410,8,6,"00:08:32,590","00:08:36,400",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=512,So as you can imagine there are just
cs-410_8_6_104,cs-410,8,6,"00:08:36,400","00:08:41,330",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,"In all these cases, note that the user's"
cs-410_8_6_105,cs-410,8,6,"00:08:41,330","00:08:47,135",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,on items and we did not actually use
cs-410_8_6_106,cs-410,8,6,"00:08:47,135","00:08:51,802",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,"It didn't matter these items are,"
cs-410_8_6_107,cs-410,8,6,"00:08:51,802","00:08:55,276",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=531,"they can be books, they can be products,"
cs-410_8_6_108,cs-410,8,6,"00:08:55,276","00:09:00,541",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,they can be text documents which
cs-410_8_6_109,cs-410,8,6,"00:09:00,541","00:09:07,120",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,so this allows such approach to be
cs-410_8_6_110,cs-410,8,6,"00:09:07,120","00:09:08,920",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=547,"Now in some newer approaches of course,"
cs-410_8_6_111,cs-410,8,6,"00:09:08,920","00:09:11,830",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=548,we would like to use more
cs-410_8_6_112,cs-410,8,6,"00:09:11,830","00:09:18,750",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=551,"Clearly, we know more about the user,"
cs-410_8_6_113,cs-410,8,6,"00:09:18,750","00:09:23,659",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=558,"So in the actual filtering system,"
cs-410_8_6_114,cs-410,8,6,"00:09:23,659","00:09:27,820",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=563,we could also combine that
cs-410_8_6_115,cs-410,8,6,"00:09:27,820","00:09:34,040",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=567,"We could use more context information,"
cs-410_8_6_116,cs-410,8,6,"00:09:34,040","00:09:39,140",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,"that people are just starting, and"
cs-410_8_6_117,cs-410,8,6,"00:09:39,140","00:09:44,147",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=579,"But, this memory based approach has"
cs-410_8_6_118,cs-410,8,6,"00:09:44,147","00:09:48,750",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=584,and it's easy to implement in
cs-410_8_6_119,cs-410,8,6,"00:09:48,750","00:09:53,698",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,a starting point to see if the strategy
cs-410_8_6_120,cs-410,8,6,"00:09:56,108","00:10:01,305",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=596,"So, there are some obvious ways"
cs-410_8_6_121,cs-410,8,6,"00:10:01,305","00:10:07,070",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=601,mainly we would like to improve
cs-410_8_6_122,cs-410,8,6,"00:10:07,070","00:10:09,690",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=607,And there are some practical
cs-410_8_6_123,cs-410,8,6,"00:10:09,690","00:10:11,960",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=609,"So for example,"
cs-410_8_6_124,cs-410,8,6,"00:10:11,960","00:10:12,990",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=611,What do you do with them?
cs-410_8_6_125,cs-410,8,6,"00:10:12,990","00:10:18,060",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,"Well, you can set them to default values"
cs-410_8_6_126,cs-410,8,6,"00:10:18,060","00:10:20,310",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,And that would be a simple solution.
cs-410_8_6_127,cs-410,8,6,"00:10:20,310","00:10:26,388",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=620,But there are advanced approaches that
cs-410_8_6_128,cs-410,8,6,"00:10:26,388","00:10:32,878",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,"missing values, and then use predictive"
cs-410_8_6_129,cs-410,8,6,"00:10:32,878","00:10:38,880",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=632,So in fact that the memory based apology
cs-410_8_6_130,cs-410,8,6,"00:10:38,880","00:10:43,128",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,So you get you have iterative approach
cs-410_8_6_131,cs-410,8,6,"00:10:43,128","00:10:43,895",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,prediction and
cs-410_8_6_132,cs-410,8,6,"00:10:43,895","00:10:48,095",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,then you can use the predictive values to
cs-410_8_6_133,cs-410,8,6,"00:10:49,525","00:10:54,840",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=649,So this is a heuristic
cs-410_8_6_134,cs-410,8,6,"00:10:54,840","00:10:59,639",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=654,And the strategy obviously would affect
cs-410_8_6_135,cs-410,8,6,"00:10:59,639","00:11:04,040",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=659,just like any other heuristics would
cs-410_8_6_136,cs-410,8,6,"00:11:06,290","00:11:10,460",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,Another idea which is actually very
cs-410_8_6_137,cs-410,8,6,"00:11:10,460","00:11:15,150",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=670,have seen in text search is called
cs-410_8_6_138,cs-410,8,6,"00:11:15,150","00:11:23,980",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=675,Now here the idea is to look at where
cs-410_8_6_139,cs-410,8,6,"00:11:23,980","00:11:29,092",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,If the item is a popular item that
cs-410_8_6_140,cs-410,8,6,"00:11:29,092","00:11:35,110",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,seen [INAUDIBLE] to people interested
cs-410_8_6_141,cs-410,8,6,"00:11:35,110","00:11:40,620",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=695,"interesting but if it's a rare item,"
cs-410_8_6_142,cs-410,8,6,"00:11:40,620","00:11:44,770",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=700,But these two users deal with this
cs-410_8_6_143,cs-410,8,6,"00:11:44,770","00:11:47,370",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=704,"And, that says more"
cs-410_8_6_144,cs-410,8,6,"00:11:47,370","00:11:52,177",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=707,It's kind of to emphasize
cs-410_8_6_145,cs-410,8,6,"00:11:52,177","00:11:56,738",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,on items that are not
cs-410_8_6_146,cs-410,8,6,"00:11:56,738","00:12:06,738",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=716,[MUSIC]
cs-410_8_7_1,cs-410,8,7,"00:00:00,025","00:00:07,935",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_8_7_2,cs-410,8,7,"00:00:07,935","00:00:14,253",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,lecture is about
cs-410_8_7_3,cs-410,8,7,"00:00:14,253","00:00:19,131",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,In this lecture we are going to talk about
cs-410_8_7_4,cs-410,8,7,"00:00:19,131","00:00:22,160",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,association called
cs-410_8_7_5,cs-410,8,7,"00:00:25,400","00:00:30,307",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=25,"By definition,"
cs-410_8_7_6,cs-410,8,7,"00:00:30,307","00:00:34,503",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,related if they share a similar context.
cs-410_8_7_7,cs-410,8,7,"00:00:34,503","00:00:39,086",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,"Namely, they occur in"
cs-410_8_7_8,cs-410,8,7,"00:00:39,086","00:00:44,280",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,So naturally our idea of discovering such
cs-410_8_7_9,cs-410,8,7,"00:00:44,280","00:00:49,080",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,of each word and then try to compute
cs-410_8_7_10,cs-410,8,7,"00:00:50,160","00:00:54,360",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,So here is an example of
cs-410_8_7_11,cs-410,8,7,"00:00:55,800","00:01:01,690",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,Here I have taken the word
cs-410_8_7_12,cs-410,8,7,"00:01:01,690","00:01:08,080",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,you can see we are seeing some remaining
cs-410_8_7_13,cs-410,8,7,"00:01:09,610","00:01:12,479",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,"Now, we can do the same thing for"
cs-410_8_7_14,cs-410,8,7,"00:01:13,660","00:01:18,370",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,So in general we would like to capture
cs-410_8_7_15,cs-410,8,7,"00:01:18,370","00:01:23,340",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=78,the similarity of the context of cat and
cs-410_8_7_16,cs-410,8,7,"00:01:24,790","00:01:29,970",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=84,So now the question is how can we
cs-410_8_7_17,cs-410,8,7,"00:01:29,970","00:01:31,458",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=89,then define the similarity function.
cs-410_8_7_18,cs-410,8,7,"00:01:33,340","00:01:38,560",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=93,"So first, we note that the context"
cs-410_8_7_19,cs-410,8,7,"00:01:38,560","00:01:43,637",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,"So, they can be regarded as"
cs-410_8_7_20,cs-410,8,7,"00:01:43,637","00:01:49,370",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=103,"document, but there are also different"
cs-410_8_7_21,cs-410,8,7,"00:01:49,370","00:01:57,470",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=109,"For example, we can look at the word"
cs-410_8_7_22,cs-410,8,7,"00:01:57,470","00:02:00,440",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=117,We can call this context Left1 context.
cs-410_8_7_23,cs-410,8,7,"00:02:00,440","00:02:04,980",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,"All right, so in this case you"
cs-410_8_7_24,cs-410,8,7,"00:02:04,980","00:02:07,430",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,"big, a, the, et cetera."
cs-410_8_7_25,cs-410,8,7,"00:02:07,430","00:02:12,690",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,These are the words that can
cs-410_8_7_26,cs-410,8,7,"00:02:12,690","00:02:19,280",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=132,"So we say my cat, his cat,"
cs-410_8_7_27,cs-410,8,7,"00:02:19,280","00:02:24,180",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,"Similarly, we can also collect the words"
cs-410_8_7_28,cs-410,8,7,"00:02:24,180","00:02:28,156",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=144,"We can call this context Right1, and"
cs-410_8_7_29,cs-410,8,7,"00:02:28,156","00:02:34,128",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=148,"here we see words like eats,"
cs-410_8_7_30,cs-410,8,7,"00:02:34,128","00:02:35,907",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,"Or, more generally,"
cs-410_8_7_31,cs-410,8,7,"00:02:35,907","00:02:41,253",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=155,we can look at all the words in
cs-410_8_7_32,cs-410,8,7,"00:02:41,253","00:02:46,960",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,"Here, let's say we can take a window"
cs-410_8_7_33,cs-410,8,7,"00:02:46,960","00:02:48,720",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,We call this context Window8.
cs-410_8_7_34,cs-410,8,7,"00:02:49,850","00:02:54,680",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=169,"Now, of course, you can see all"
cs-410_8_7_35,cs-410,8,7,"00:02:54,680","00:02:58,829",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,so we'll have a bag of words in
cs-410_8_7_36,cs-410,8,7,"00:03:01,270","00:03:06,410",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,"Now, such a word based representation"
cs-410_8_7_37,cs-410,8,7,"00:03:06,410","00:03:12,230",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,an interesting way to define the
cs-410_8_7_38,cs-410,8,7,"00:03:12,230","00:03:15,911",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=192,Because if you look at just
cs-410_8_7_39,cs-410,8,7,"00:03:15,911","00:03:21,750",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=195,then we'll see words that share
cs-410_8_7_40,cs-410,8,7,"00:03:21,750","00:03:27,650",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=201,and we kind of ignored the other words
cs-410_8_7_41,cs-410,8,7,"00:03:27,650","00:03:32,380",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,So that gives us one perspective to
cs-410_8_7_42,cs-410,8,7,"00:03:32,380","00:03:34,244",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,"if we only use the Right1 context,"
cs-410_8_7_43,cs-410,8,7,"00:03:34,244","00:03:38,420",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=214,we will capture this narrative
cs-410_8_7_44,cs-410,8,7,"00:03:38,420","00:03:43,040",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,Using both the Left1 and
cs-410_8_7_45,cs-410,8,7,"00:03:43,040","00:03:47,720",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=223,the similarity with even
cs-410_8_7_46,cs-410,8,7,"00:03:49,910","00:03:54,744",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=229,"So in general, context may contain"
cs-410_8_7_47,cs-410,8,7,"00:03:54,744","00:03:59,575",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,"my, that you see here, or"
cs-410_8_7_48,cs-410,8,7,"00:03:59,575","00:04:02,961",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,"Tuesday, or"
cs-410_8_7_49,cs-410,8,7,"00:04:05,461","00:04:10,174",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=245,And this flexibility also allows us
cs-410_8_7_50,cs-410,8,7,"00:04:10,174","00:04:11,660",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,different ways.
cs-410_8_7_51,cs-410,8,7,"00:04:11,660","00:04:13,500",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=251,"Sometimes this is useful,"
cs-410_8_7_52,cs-410,8,7,"00:04:13,500","00:04:19,130",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,as we might want to capture
cs-410_8_7_53,cs-410,8,7,"00:04:19,130","00:04:25,270",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,That would give us loosely
cs-410_8_7_54,cs-410,8,7,"00:04:25,270","00:04:29,340",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,Whereas if you use only the words
cs-410_8_7_55,cs-410,8,7,"00:04:29,340","00:04:35,520",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,"to the right of the word, then you"
cs-410_8_7_56,cs-410,8,7,"00:04:35,520","00:04:39,950",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=275,much related by their syntactical
cs-410_8_7_57,cs-410,8,7,"00:04:41,170","00:04:46,304",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=281,So the general idea of discovering
cs-410_8_7_58,cs-410,8,7,"00:04:46,304","00:04:50,754",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=286,is to compute the similarity
cs-410_8_7_59,cs-410,8,7,"00:04:50,754","00:04:55,264",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,"So here, for example,"
cs-410_8_7_60,cs-410,8,7,"00:04:55,264","00:04:59,110",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,dog based on the similarity
cs-410_8_7_61,cs-410,8,7,"00:04:59,110","00:05:02,890",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,"In general, we can combine all"
cs-410_8_7_62,cs-410,8,7,"00:05:02,890","00:05:06,395",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,"And so the similarity function is,"
cs-410_8_7_63,cs-410,8,7,"00:05:06,395","00:05:10,336",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=306,a combination of similarities
cs-410_8_7_64,cs-410,8,7,"00:05:10,336","00:05:14,849",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=310,"And of course, we can also assign"
cs-410_8_7_65,cs-410,8,7,"00:05:14,849","00:05:20,170",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=314,similarities to allow us to focus
cs-410_8_7_66,cs-410,8,7,"00:05:20,170","00:05:24,395",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=320,And this would be naturally
cs-410_8_7_67,cs-410,8,7,"00:05:24,395","00:05:28,935",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=324,here the main idea for discovering
cs-410_8_7_68,cs-410,8,7,"00:05:28,935","00:05:32,470",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,to computer the similarity
cs-410_8_7_69,cs-410,8,7,"00:05:32,470","00:05:37,670",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,So next let's see how we exactly
cs-410_8_7_70,cs-410,8,7,"00:05:37,670","00:05:42,235",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,"Now to answer this question,"
cs-410_8_7_71,cs-410,8,7,"00:05:42,235","00:05:46,520",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=342,representation as vectors
cs-410_8_7_72,cs-410,8,7,"00:05:48,340","00:05:53,016",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,Now those of you who have been
cs-410_8_7_73,cs-410,8,7,"00:05:53,016","00:05:57,936",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,textual retrieval techniques would
cs-410_8_7_74,cs-410,8,7,"00:05:57,936","00:06:02,711",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,been used frequently for
cs-410_8_7_75,cs-410,8,7,"00:06:02,711","00:06:08,115",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,But here we also find it convenient
cs-410_8_7_76,cs-410,8,7,"00:06:08,115","00:06:11,130",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=368,paradigmatic relation discovery.
cs-410_8_7_77,cs-410,8,7,"00:06:11,130","00:06:15,440",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=371,So the idea of this
cs-410_8_7_78,cs-410,8,7,"00:06:15,440","00:06:20,140",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=375,word in our vocabulary as defining one
cs-410_8_7_79,cs-410,8,7,"00:06:20,140","00:06:23,615",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,So we have N words in
cs-410_8_7_80,cs-410,8,7,"00:06:23,615","00:06:27,462",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=383,"then we have N dimensions,"
cs-410_8_7_81,cs-410,8,7,"00:06:27,462","00:06:34,311",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=387,"And on the bottom, you can see a frequency"
cs-410_8_7_82,cs-410,8,7,"00:06:34,311","00:06:39,855",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=394,and here we see where eats
cs-410_8_7_83,cs-410,8,7,"00:06:39,855","00:06:43,140",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=399,"ate occurred 3 times, et cetera."
cs-410_8_7_84,cs-410,8,7,"00:06:43,140","00:06:48,003",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=403,So this vector can then be placed
cs-410_8_7_85,cs-410,8,7,"00:06:48,003","00:06:53,347",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=408,"So in general,"
cs-410_8_7_86,cs-410,8,7,"00:06:53,347","00:06:58,933",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=413,"context of cat as one vector,"
cs-410_8_7_87,cs-410,8,7,"00:06:58,933","00:07:04,045",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=418,"dog, might give us a different context,"
cs-410_8_7_88,cs-410,8,7,"00:07:04,045","00:07:07,880",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=424,And then we can measure
cs-410_8_7_89,cs-410,8,7,"00:07:07,880","00:07:10,980",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=427,So by viewing context in
cs-410_8_7_90,cs-410,8,7,"00:07:10,980","00:07:15,100",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,we convert the problem of
cs-410_8_7_91,cs-410,8,7,"00:07:15,100","00:07:18,820",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=435,into the problem of computing
cs-410_8_7_92,cs-410,8,7,"00:07:20,300","00:07:24,170",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=440,So the two questions that we
cs-410_8_7_93,cs-410,8,7,"00:07:24,170","00:07:28,750",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,"how to compute each vector, and"
cs-410_8_7_94,cs-410,8,7,"00:07:31,050","00:07:33,579",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,And the other question is how
cs-410_8_7_95,cs-410,8,7,"00:07:35,580","00:07:40,515",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=455,"Now in general, there are many approaches"
cs-410_8_7_96,cs-410,8,7,"00:07:40,515","00:07:43,795",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,most of them are developed for
cs-410_8_7_97,cs-410,8,7,"00:07:43,795","00:07:47,821",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=463,And they have been shown to work well for
cs-410_8_7_98,cs-410,8,7,"00:07:47,821","00:07:52,712",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=467,matching a query vector and
cs-410_8_7_99,cs-410,8,7,"00:07:52,712","00:07:57,555",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=472,But we can adapt many of
cs-410_8_7_100,cs-410,8,7,"00:07:57,555","00:08:01,378",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,of context documents for our purpose here.
cs-410_8_7_101,cs-410,8,7,"00:08:01,378","00:08:05,829",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=481,So let's first look at
cs-410_8_7_102,cs-410,8,7,"00:08:05,829","00:08:10,481",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,where we try to match
cs-410_8_7_103,cs-410,8,7,"00:08:10,481","00:08:15,150",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=490,"the expected overlap of words,"
cs-410_8_7_104,cs-410,8,7,"00:08:17,020","00:08:22,495",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,So the idea here is to represent
cs-410_8_7_105,cs-410,8,7,"00:08:22,495","00:08:28,438",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,where each word has a weight
cs-410_8_7_106,cs-410,8,7,"00:08:28,438","00:08:35,336",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=508,that a randomly picked word from
cs-410_8_7_107,cs-410,8,7,"00:08:35,336","00:08:39,956",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,"So in other words,"
cs-410_8_7_108,cs-410,8,7,"00:08:39,956","00:08:43,476",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=519,"account of word wi in the context, and"
cs-410_8_7_109,cs-410,8,7,"00:08:43,476","00:08:48,756",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,this can be interpreted as
cs-410_8_7_110,cs-410,8,7,"00:08:48,756","00:08:54,600",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=528,actually pick this word from d1
cs-410_8_7_111,cs-410,8,7,"00:08:56,760","00:09:01,620",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,"Now, of course these xi's would sum to one"
cs-410_8_7_112,cs-410,8,7,"00:09:02,930","00:09:05,750",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,and this means the vector is
cs-410_8_7_113,cs-410,8,7,"00:09:05,750","00:09:08,193",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,actually probability of
cs-410_8_7_114,cs-410,8,7,"00:09:10,500","00:09:15,883",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,"So, the vector d2 can be also"
cs-410_8_7_115,cs-410,8,7,"00:09:15,883","00:09:23,540",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=555,this would give us then two probability
cs-410_8_7_116,cs-410,8,7,"00:09:24,840","00:09:28,220",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,"So, that addresses the problem"
cs-410_8_7_117,cs-410,8,7,"00:09:28,220","00:09:31,760",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=568,next let's see how we can define
cs-410_8_7_118,cs-410,8,7,"00:09:31,760","00:09:35,668",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=571,"Well, here, we simply define"
cs-410_8_7_119,cs-410,8,7,"00:09:35,668","00:09:39,890",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=575,"vectors, and"
cs-410_8_7_120,cs-410,8,7,"00:09:41,410","00:09:43,960",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=581,of the corresponding
cs-410_8_7_121,cs-410,8,7,"00:09:46,630","00:09:51,847",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=586,"Now, it's interesting to see"
cs-410_8_7_122,cs-410,8,7,"00:09:51,847","00:09:57,360",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,"actually has a nice interpretation,"
cs-410_8_7_123,cs-410,8,7,"00:09:57,360","00:10:02,548",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,"Dot product, in fact that gives"
cs-410_8_7_124,cs-410,8,7,"00:10:02,548","00:10:08,570",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=602,randomly picked words from
cs-410_8_7_125,cs-410,8,7,"00:10:08,570","00:10:12,630",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=608,That means if we try to pick a word
cs-410_8_7_126,cs-410,8,7,"00:10:12,630","00:10:17,860",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=612,"word from another context, we can then"
cs-410_8_7_127,cs-410,8,7,"00:10:17,860","00:10:22,650",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=617,"If the two contexts are very similar,"
cs-410_8_7_128,cs-410,8,7,"00:10:22,650","00:10:27,390",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,see the two words picked from
cs-410_8_7_129,cs-410,8,7,"00:10:27,390","00:10:30,900",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=627,"If they are very different,"
cs-410_8_7_130,cs-410,8,7,"00:10:30,900","00:10:34,890",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=630,identical words being picked from
cs-410_8_7_131,cs-410,8,7,"00:10:34,890","00:10:39,865",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=634,"So this intuitively makes sense, right,"
cs-410_8_7_132,cs-410,8,7,"00:10:41,490","00:10:46,819",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=641,Now you might want to also take
cs-410_8_7_133,cs-410,8,7,"00:10:46,819","00:10:51,627",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,see why this can be interpreted
cs-410_8_7_134,cs-410,8,7,"00:10:51,627","00:10:55,410",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=651,two randomly picked words are identical.
cs-410_8_7_135,cs-410,8,7,"00:10:57,440","00:11:04,550",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=657,So if you just stare at the formula
cs-410_8_7_136,cs-410,8,7,"00:11:04,550","00:11:12,034",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=664,then you will see basically in each
cs-410_8_7_137,cs-410,8,7,"00:11:12,034","00:11:17,170",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=672,we will see an overlap on
cs-410_8_7_138,cs-410,8,7,"00:11:17,170","00:11:23,661",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,And where xi gives us a probability that
cs-410_8_7_139,cs-410,8,7,"00:11:23,661","00:11:28,503",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=683,and yi gives us the probability
cs-410_8_7_140,cs-410,8,7,"00:11:28,503","00:11:32,024",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=688,And when we pick the same
cs-410_8_7_141,cs-410,8,7,"00:11:32,024","00:11:34,920",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=692,"then we have an identical pick, right so."
cs-410_8_7_142,cs-410,8,7,"00:11:34,920","00:11:42,380",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=694,"That's one possible approach, EOWC,"
cs-410_8_7_143,cs-410,8,7,"00:11:42,380","00:11:49,440",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=702,"Now as always, we would like to assess"
cs-410_8_7_144,cs-410,8,7,"00:11:49,440","00:11:52,880",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=709,"Now of course, ultimately we have to"
cs-410_8_7_145,cs-410,8,7,"00:11:52,880","00:11:56,259",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,see if it gives us really
cs-410_8_7_146,cs-410,8,7,"00:11:57,730","00:12:01,010",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=717,"Really give us paradigmatical relations,"
cs-410_8_7_147,cs-410,8,7,"00:12:01,010","00:12:05,380",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=721,analytically we can also analyze
cs-410_8_7_148,cs-410,8,7,"00:12:05,380","00:12:11,020",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=725,"So first, as I said,"
cs-410_8_7_149,cs-410,8,7,"00:12:11,020","00:12:15,802",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=731,formula will give a higher score if there
cs-410_8_7_150,cs-410,8,7,"00:12:15,802","00:12:17,988",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=735,So that's exactly what we want.
cs-410_8_7_151,cs-410,8,7,"00:12:17,988","00:12:21,170",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=737,But if you analyze
cs-410_8_7_152,cs-410,8,7,"00:12:21,170","00:12:24,286",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,then you also see there might
cs-410_8_7_153,cs-410,8,7,"00:12:24,286","00:12:27,735",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=744,and specifically there
cs-410_8_7_154,cs-410,8,7,"00:12:27,735","00:12:33,935",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=747,"First, it might favor matching"
cs-410_8_7_155,cs-410,8,7,"00:12:33,935","00:12:35,795",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,over matching more distinct terms.
cs-410_8_7_156,cs-410,8,7,"00:12:36,825","00:12:44,300",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=756,"And that is because in the dot product,"
cs-410_8_7_157,cs-410,8,7,"00:12:44,300","00:12:50,190",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,element is shared by both contexts and
cs-410_8_7_158,cs-410,8,7,"00:12:51,250","00:12:55,710",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=771,it might indeed make the score
cs-410_8_7_159,cs-410,8,7,"00:12:55,710","00:13:01,150",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=775,where the two vectors actually have
cs-410_8_7_160,cs-410,8,7,"00:13:01,150","00:13:06,878",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=781,But each term has a relatively low
cs-410_8_7_161,cs-410,8,7,"00:13:06,878","00:13:09,586",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=786,"Of course, this might be"
cs-410_8_7_162,cs-410,8,7,"00:13:09,586","00:13:14,527",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=789,"But in our case, we should intuitively"
cs-410_8_7_163,cs-410,8,7,"00:13:14,527","00:13:19,645",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=794,"more different terms in the context,"
cs-410_8_7_164,cs-410,8,7,"00:13:19,645","00:13:24,253",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=799,in saying that the two words
cs-410_8_7_165,cs-410,8,7,"00:13:24,253","00:13:27,020",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=804,If you only rely on one term and
cs-410_8_7_166,cs-410,8,7,"00:13:27,020","00:13:32,465",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=807,"that's a little bit questionable,"
cs-410_8_7_167,cs-410,8,7,"00:13:34,675","00:13:38,795",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=814,Now the second problem is that it
cs-410_8_7_168,cs-410,8,7,"00:13:38,795","00:13:42,131",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=818,So if you match a word like the and
cs-410_8_7_169,cs-410,8,7,"00:13:42,131","00:13:47,443",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=822,it will be the same as
cs-410_8_7_170,cs-410,8,7,"00:13:47,443","00:13:52,388",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=827,intuitively we know
cs-410_8_7_171,cs-410,8,7,"00:13:52,388","00:13:57,816",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=832,surprising because the occurs everywhere.
cs-410_8_7_172,cs-410,8,7,"00:13:57,816","00:14:02,787",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,So matching the is not as such
cs-410_8_7_173,cs-410,8,7,"00:14:02,787","00:14:07,956",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=842,"a word like eats,"
cs-410_8_7_174,cs-410,8,7,"00:14:07,956","00:14:11,216",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=847,So this is another
cs-410_8_7_175,cs-410,8,7,"00:14:13,426","00:14:19,003",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=853,In the next chapter we are going to talk
cs-410_8_7_176,cs-410,8,7,"00:14:19,003","00:14:29,003",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=859,[MUSIC]
cs-410_9_6_1,cs-410,9,6,"00:00:00,012","00:00:07,878",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[SOUND]
cs-410_9_6_2,cs-410,9,6,"00:00:07,878","00:00:12,848",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=7,to summarize our discussion of
cs-410_9_6_3,cs-410,9,6,"00:00:12,848","00:00:16,640",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,the filtering task for
cs-410_9_6_4,cs-410,9,6,"00:00:16,640","00:00:21,020",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"in some other sense,"
cs-410_9_6_5,cs-410,9,6,"00:00:21,020","00:00:24,230",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,So it's easy because
cs-410_9_6_6,cs-410,9,6,"00:00:24,230","00:00:30,300",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,In this case the system takes initiative
cs-410_9_6_7,cs-410,9,6,"00:00:30,300","00:00:33,230",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,"The user doesn't really make any effort,"
cs-410_9_6_8,cs-410,9,6,"00:00:33,230","00:00:36,100",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,any recommendation is better than nothing.
cs-410_9_6_9,cs-410,9,6,"00:00:36,100","00:00:41,710",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,All right.
cs-410_9_6_10,cs-410,9,6,"00:00:41,710","00:00:44,180",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=41,items or useless documents.
cs-410_9_6_11,cs-410,9,6,"00:00:44,180","00:00:47,220",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=44,If you can recommend
cs-410_9_6_12,cs-410,9,6,"00:00:47,220","00:00:52,390",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,"users generally will appreciate it,"
cs-410_9_6_13,cs-410,9,6,"00:00:52,390","00:00:56,810",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,"However, filtering is actually much harder"
cs-410_9_6_14,cs-410,9,6,"00:00:56,810","00:01:01,860",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=56,make a binary decision and you can't
cs-410_9_6_15,cs-410,9,6,"00:01:01,860","00:01:06,520",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=61,then you're going to see whether
cs-410_9_6_16,cs-410,9,6,"00:01:06,520","00:01:10,040",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=66,You have to make a decision
cs-410_9_6_17,cs-410,9,6,"00:01:10,040","00:01:11,260",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=70,Think about news filtering.
cs-410_9_6_18,cs-410,9,6,"00:01:11,260","00:01:15,060",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,As soon as you see the news enough
cs-410_9_6_19,cs-410,9,6,"00:01:15,060","00:01:16,780",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,interesting to the user.
cs-410_9_6_20,cs-410,9,6,"00:01:16,780","00:01:21,190",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=76,"If you wait for a few days, well, even if"
cs-410_9_6_21,cs-410,9,6,"00:01:21,190","00:01:26,690",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=81,"the most relevant news, the utility is"
cs-410_9_6_22,cs-410,9,6,"00:01:28,160","00:01:32,140",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=88,Another reason why it's hard
cs-410_9_6_23,cs-410,9,6,"00:01:32,140","00:01:34,620",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,if you think of this
cs-410_9_6_24,cs-410,9,6,"00:01:34,620","00:01:36,010",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,"Collaborative filtering, for"
cs-410_9_6_25,cs-410,9,6,"00:01:36,010","00:01:41,030",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,"example, is purely based on"
cs-410_9_6_26,cs-410,9,6,"00:01:41,030","00:01:48,120",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=101,So if you don't have many ratings there's
cs-410_9_6_27,cs-410,9,6,"00:01:48,120","00:01:51,470",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,And yeah I just mentioned
cs-410_9_6_28,cs-410,9,6,"00:01:51,470","00:01:54,450",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=111,"This is actually a very serious,"
cs-410_9_6_29,cs-410,9,6,"00:01:54,450","00:01:59,180",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=114,But of course there are strategies that
cs-410_9_6_30,cs-410,9,6,"00:02:00,680","00:02:04,930",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=120,and there are different strategies that
cs-410_9_6_31,cs-410,9,6,"00:02:04,930","00:02:09,620",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=124,"You can use, for example, more user"
cs-410_9_6_32,cs-410,9,6,"00:02:09,620","00:02:14,470",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=129,instead of using the preferences
cs-410_9_6_33,cs-410,9,6,"00:02:14,470","00:02:19,000",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=134,items give me additional information
cs-410_9_6_34,cs-410,9,6,"00:02:21,110","00:02:26,840",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=141,And we also talk about two strategies for
cs-410_9_6_35,cs-410,9,6,"00:02:26,840","00:02:30,140",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=146,One is content-based where
cs-410_9_6_36,cs-410,9,6,"00:02:30,140","00:02:34,670",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=150,is collaborative filtering where
cs-410_9_6_37,cs-410,9,6,"00:02:34,670","00:02:37,990",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=154,And they obviously can be
cs-410_9_6_38,cs-410,9,6,"00:02:37,990","00:02:41,480",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=157,You can imagine they generally
cs-410_9_6_39,cs-410,9,6,"00:02:41,480","00:02:46,166",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=161,So that would give us a hybrid
cs-410_9_6_40,cs-410,9,6,"00:02:46,166","00:02:52,620",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,And we also could recall that we talked
cs-410_9_6_41,cs-410,9,6,"00:02:52,620","00:02:58,470",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=172,about push versus pull as two strategies
cs-410_9_6_42,cs-410,9,6,"00:02:58,470","00:03:03,110",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,And recommender system easy to
cs-410_9_6_43,cs-410,9,6,"00:03:03,110","00:03:06,650",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=183,search engines are serving
cs-410_9_6_44,cs-410,9,6,"00:03:06,650","00:03:09,740",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=186,"Obviously the two should be combined,"
cs-410_9_6_45,cs-410,9,6,"00:03:09,740","00:03:13,400",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=189,The two have a system
cs-410_9_6_46,cs-410,9,6,"00:03:13,400","00:03:16,600",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=193,with multiple mode information access.
cs-410_9_6_47,cs-410,9,6,"00:03:16,600","00:03:22,870",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,So in the future we could anticipate such
cs-410_9_6_48,cs-410,9,6,"00:03:22,870","00:03:27,570",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,"And either,"
cs-410_9_6_49,cs-410,9,6,"00:03:27,570","00:03:33,740",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,there are a lot of new algorithms
cs-410_9_6_50,cs-410,9,6,"00:03:33,740","00:03:39,070",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=213,In particular those new algorithms tend
cs-410_9_6_51,cs-410,9,6,"00:03:39,070","00:03:42,850",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=219,Now the context here could be
cs-410_9_6_52,cs-410,9,6,"00:03:42,850","00:03:44,920",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,could also be the context of the user.
cs-410_9_6_53,cs-410,9,6,"00:03:44,920","00:03:45,750",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=224,Items.
cs-410_9_6_54,cs-410,9,6,"00:03:45,750","00:03:47,570",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,The items are not the isolated.
cs-410_9_6_55,cs-410,9,6,"00:03:47,570","00:03:50,290",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=227,They're connected in many ways.
cs-410_9_6_56,cs-410,9,6,"00:03:50,290","00:03:54,590",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,The users might form
cs-410_9_6_57,cs-410,9,6,"00:03:54,590","00:03:58,980",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,so there's a rich context there
cs-410_9_6_58,cs-410,9,6,"00:03:59,980","00:04:04,100",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,really solve the problem well and
cs-410_9_6_59,cs-410,9,6,"00:04:04,100","00:04:09,650",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,research area where also machine
cs-410_9_6_60,cs-410,9,6,"00:04:09,650","00:04:13,624",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,Here are some additional readings in
cs-410_9_6_61,cs-410,9,6,"00:04:13,624","00:04:18,494",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,the handbook called
cs-410_9_6_62,cs-410,9,6,"00:04:18,494","00:04:23,364",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=258,has a collection of a lot
cs-410_9_6_63,cs-410,9,6,"00:04:23,364","00:04:28,362",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,can give you an overview
cs-410_9_6_64,cs-410,9,6,"00:04:28,362","00:04:33,122",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,approaches through recommender systems.
cs-410_9_6_65,cs-410,9,6,"00:04:33,122","00:04:43,122",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,[MUSIC]
cs-410_9_7_1,cs-410,9,7,"00:00:05,960","00:00:08,625",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=5,"In this lecture, we continue"
cs-410_9_7_2,cs-410,9,7,"00:00:08,625","00:00:11,565",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=8,discussing Paradigmatical
cs-410_9_7_3,cs-410,9,7,"00:00:11,565","00:00:14,175",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=11,Earlier we introduced
cs-410_9_7_4,cs-410,9,7,"00:00:14,175","00:00:16,935",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=14,Expected Overlap of
cs-410_9_7_5,cs-410,9,7,"00:00:16,935","00:00:21,090",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=16,"In this method, we"
cs-410_9_7_6,cs-410,9,7,"00:00:21,090","00:00:23,040",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=21,a word vector that represents
cs-410_9_7_7,cs-410,9,7,"00:00:23,040","00:00:26,490",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=23,the probability of a
cs-410_9_7_8,cs-410,9,7,"00:00:26,490","00:00:30,345",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=26,We measure the similarity
cs-410_9_7_9,cs-410,9,7,"00:00:30,345","00:00:34,320",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=30,which can be interpreted as
cs-410_9_7_10,cs-410,9,7,"00:00:34,320","00:00:36,240",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=34,randomly picked words from
cs-410_9_7_11,cs-410,9,7,"00:00:36,240","00:00:38,585",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=36,the two contexts are identical.
cs-410_9_7_12,cs-410,9,7,"00:00:38,585","00:00:42,515",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=38,We also discussed
cs-410_9_7_13,cs-410,9,7,"00:00:42,515","00:00:45,920",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=42,The first is that
cs-410_9_7_14,cs-410,9,7,"00:00:45,920","00:00:47,900",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=45,one frequent term very well over
cs-410_9_7_15,cs-410,9,7,"00:00:47,900","00:00:50,390",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=47,matching more distinct terms.
cs-410_9_7_16,cs-410,9,7,"00:00:50,390","00:00:55,235",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=50,It put too much emphasis on
cs-410_9_7_17,cs-410,9,7,"00:00:55,235","00:01:00,350",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=55,The second is that it
cs-410_9_7_18,cs-410,9,7,"00:01:00,350","00:01:03,995",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=60,Even a common word like
cs-410_9_7_19,cs-410,9,7,"00:01:03,995","00:01:08,885",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=63,equally as content
cs-410_9_7_20,cs-410,9,7,"00:01:08,885","00:01:11,270",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=68,So now we are
cs-410_9_7_21,cs-410,9,7,"00:01:11,270","00:01:13,715",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=71,going to talk about how
cs-410_9_7_22,cs-410,9,7,"00:01:13,715","00:01:15,965",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"More specifically, we're"
cs-410_9_7_23,cs-410,9,7,"00:01:15,965","00:01:19,790",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=75,some retrieval heuristics
cs-410_9_7_24,cs-410,9,7,"00:01:19,790","00:01:23,900",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=79,These heuristics can effectively
cs-410_9_7_25,cs-410,9,7,"00:01:23,900","00:01:26,975",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,as these problems also
cs-410_9_7_26,cs-410,9,7,"00:01:26,975","00:01:30,680",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=86,when we match a query that
cs-410_9_7_27,cs-410,9,7,"00:01:30,680","00:01:32,920",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,"So to address the first problem,"
cs-410_9_7_28,cs-410,9,7,"00:01:32,920","00:01:36,385",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=92,we can use a sublinear
cs-410_9_7_29,cs-410,9,7,"00:01:36,385","00:01:37,970",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=96,"That is, we don't have to use"
cs-410_9_7_30,cs-410,9,7,"00:01:37,970","00:01:39,650",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=97,the raw frequency count of
cs-410_9_7_31,cs-410,9,7,"00:01:39,650","00:01:42,140",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=99,a term to represent the context.
cs-410_9_7_32,cs-410,9,7,"00:01:42,140","00:01:44,780",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,We can transform
cs-410_9_7_33,cs-410,9,7,"00:01:44,780","00:01:48,025",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=104,that wouldn't emphasize so
cs-410_9_7_34,cs-410,9,7,"00:01:48,025","00:01:50,130",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,To address the
cs-410_9_7_35,cs-410,9,7,"00:01:50,130","00:01:53,195",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=110,we can put more weight
cs-410_9_7_36,cs-410,9,7,"00:01:53,195","00:01:56,330",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,That is we can reward
cs-410_9_7_37,cs-410,9,7,"00:01:56,330","00:01:58,760",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,This heuristic is called the IDF
cs-410_9_7_38,cs-410,9,7,"00:01:58,760","00:02:01,135",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=118,term weighting in text retrieval.
cs-410_9_7_39,cs-410,9,7,"00:02:01,135","00:02:05,085",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,IDF stands for
cs-410_9_7_40,cs-410,9,7,"00:02:05,085","00:02:07,010",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=125,"So now, we're going to talk about"
cs-410_9_7_41,cs-410,9,7,"00:02:07,010","00:02:10,130",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,the two heuristics
cs-410_9_7_42,cs-410,9,7,"00:02:10,130","00:02:13,930",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=130,First let's talk about
cs-410_9_7_43,cs-410,9,7,"00:02:13,930","00:02:16,400",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,That is to convert
cs-410_9_7_44,cs-410,9,7,"00:02:16,400","00:02:19,565",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=136,a word in the document
cs-410_9_7_45,cs-410,9,7,"00:02:19,565","00:02:23,195",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=139,that reflects our belief
cs-410_9_7_46,cs-410,9,7,"00:02:23,195","00:02:27,200",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=143,about how important
cs-410_9_7_47,cs-410,9,7,"00:02:27,200","00:02:32,370",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,So that will be
cs-410_9_7_48,cs-410,9,7,"00:02:32,370","00:02:36,415",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=152,That's shown in the y-axis.
cs-410_9_7_49,cs-410,9,7,"00:02:36,415","00:02:39,920",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=156,"Now, in general, there are"
cs-410_9_7_50,cs-410,9,7,"00:02:39,920","00:02:44,250",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,Let's first look at
cs-410_9_7_51,cs-410,9,7,"00:02:44,250","00:02:47,920",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=164,"In this case, we're"
cs-410_9_7_52,cs-410,9,7,"00:02:47,920","00:02:51,510",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=167,any non-zero counts
cs-410_9_7_53,cs-410,9,7,"00:02:51,510","00:02:55,450",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=171,one and the zero count
cs-410_9_7_54,cs-410,9,7,"00:02:55,450","00:02:56,990",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=175,So with this mapping
cs-410_9_7_55,cs-410,9,7,"00:02:56,990","00:02:59,240",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=176,all the frequencies will be
cs-410_9_7_56,cs-410,9,7,"00:02:59,240","00:03:02,605",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=179,mapped to only two
cs-410_9_7_57,cs-410,9,7,"00:03:02,605","00:03:11,015",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=182,The mapping function is shown
cs-410_9_7_58,cs-410,9,7,"00:03:11,015","00:03:14,030",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=191,"Now, this is naive"
cs-410_9_7_59,cs-410,9,7,"00:03:14,030","00:03:16,660",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=194,because it's not
cs-410_9_7_60,cs-410,9,7,"00:03:16,660","00:03:20,195",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,"However, this actually"
cs-410_9_7_61,cs-410,9,7,"00:03:20,195","00:03:25,700",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=200,emphasizing matching all
cs-410_9_7_62,cs-410,9,7,"00:03:25,700","00:03:27,725",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,So it does not allow
cs-410_9_7_63,cs-410,9,7,"00:03:27,725","00:03:30,505",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=207,a frequency of word to
cs-410_9_7_64,cs-410,9,7,"00:03:30,505","00:03:32,930",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=210,"Now, the approach"
cs-410_9_7_65,cs-410,9,7,"00:03:32,930","00:03:36,650",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,earlier in the expected
cs-410_9_7_66,cs-410,9,7,"00:03:36,650","00:03:38,225",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,is a linear transformation.
cs-410_9_7_67,cs-410,9,7,"00:03:38,225","00:03:41,870",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=218,"We basically, take"
cs-410_9_7_68,cs-410,9,7,"00:03:41,870","00:03:45,445",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=221,So we use the raw count
cs-410_9_7_69,cs-410,9,7,"00:03:45,445","00:03:48,140",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,That created the problem
cs-410_9_7_70,cs-410,9,7,"00:03:48,140","00:03:50,360",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=228,that we just talked about namely;
cs-410_9_7_71,cs-410,9,7,"00:03:50,360","00:03:54,935",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,it emphasize too much on just
cs-410_9_7_72,cs-410,9,7,"00:03:54,935","00:03:58,520",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=234,Matching one frequent term
cs-410_9_7_73,cs-410,9,7,"00:03:58,520","00:04:02,750",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=238,So we can have a lot
cs-410_9_7_74,cs-410,9,7,"00:04:02,750","00:04:04,475",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=242,of other interesting
cs-410_9_7_75,cs-410,9,7,"00:04:04,475","00:04:06,875",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=244,"in between the two extremes,"
cs-410_9_7_76,cs-410,9,7,"00:04:06,875","00:04:10,640",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=246,and they generally form
cs-410_9_7_77,cs-410,9,7,"00:04:10,640","00:04:13,340",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=250,"So for example,"
cs-410_9_7_78,cs-410,9,7,"00:04:13,340","00:04:16,080",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=253,"logarithm of the raw count,"
cs-410_9_7_79,cs-410,9,7,"00:04:16,080","00:04:19,400",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,and this will give us curve
cs-410_9_7_80,cs-410,9,7,"00:04:19,400","00:04:21,260",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=259,that you are seeing here.
cs-410_9_7_81,cs-410,9,7,"00:04:21,260","00:04:25,295",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,"In this case, you can see"
cs-410_9_7_82,cs-410,9,7,"00:04:25,295","00:04:29,330",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=265,The high counts are
cs-410_9_7_83,cs-410,9,7,"00:04:29,330","00:04:33,470",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=269,so the curve is a sublinear
cs-410_9_7_84,cs-410,9,7,"00:04:33,470","00:04:39,240",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=273,the weight of
cs-410_9_7_85,cs-410,9,7,"00:04:39,240","00:04:42,875",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=279,"This is what we want,"
cs-410_9_7_86,cs-410,9,7,"00:04:42,875","00:04:47,340",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=282,terms from dominating
cs-410_9_7_87,cs-410,9,7,"00:04:48,620","00:04:50,870",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=288,"Now, there is also"
cs-410_9_7_88,cs-410,9,7,"00:04:50,870","00:04:52,760",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,another interesting
cs-410_9_7_89,cs-410,9,7,"00:04:52,760","00:04:55,430",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=292,a BM25 transformation which
cs-410_9_7_90,cs-410,9,7,"00:04:55,430","00:04:59,945",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=295,has been shown to be very
cs-410_9_7_91,cs-410,9,7,"00:04:59,945","00:05:02,735",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=299,"In this transformation, we have"
cs-410_9_7_92,cs-410,9,7,"00:05:02,735","00:05:07,225",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,a form that looks like this.
cs-410_9_7_93,cs-410,9,7,"00:05:07,225","00:05:11,640",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,So it's k plus one multiplied
cs-410_9_7_94,cs-410,9,7,"00:05:11,640","00:05:13,800",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,"where k is a parameter,"
cs-410_9_7_95,cs-410,9,7,"00:05:13,800","00:05:16,485",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=313,"x is the count,"
cs-410_9_7_96,cs-410,9,7,"00:05:16,485","00:05:18,690",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,the raw count of a word.
cs-410_9_7_97,cs-410,9,7,"00:05:18,690","00:05:22,190",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=318,"Now, the transformation"
cs-410_9_7_98,cs-410,9,7,"00:05:22,190","00:05:25,430",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=322,that it can actually go from
cs-410_9_7_99,cs-410,9,7,"00:05:25,430","00:05:28,910",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,one extreme to the other
cs-410_9_7_100,cs-410,9,7,"00:05:28,910","00:05:34,725",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=328,k. It also interesting
cs-410_9_7_101,cs-410,9,7,"00:05:34,725","00:05:37,135",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,k plus one in this case.
cs-410_9_7_102,cs-410,9,7,"00:05:37,135","00:05:41,435",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=337,So this puts
cs-410_9_7_103,cs-410,9,7,"00:05:41,435","00:05:43,040",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=341,"on high frequency terms,"
cs-410_9_7_104,cs-410,9,7,"00:05:43,040","00:05:46,460",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=343,because their weight would
cs-410_9_7_105,cs-410,9,7,"00:05:46,460","00:05:50,900",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=346,"As we vary k, if we can"
cs-410_9_7_106,cs-410,9,7,"00:05:50,900","00:05:52,590",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=350,"So when k is set to zero,"
cs-410_9_7_107,cs-410,9,7,"00:05:52,590","00:05:55,680",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=352,"we roughly have the 0,1 vector."
cs-410_9_7_108,cs-410,9,7,"00:05:55,680","00:05:59,090",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=355,Whereas when we set k
cs-410_9_7_109,cs-410,9,7,"00:05:59,090","00:06:02,075",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=359,it will behave more like
cs-410_9_7_110,cs-410,9,7,"00:06:02,075","00:06:05,270",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=362,So this transformation
cs-410_9_7_111,cs-410,9,7,"00:06:05,270","00:06:07,880",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=365,far the most effective
cs-410_9_7_112,cs-410,9,7,"00:06:07,880","00:06:10,880",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=367,text retrieval and it also makes
cs-410_9_7_113,cs-410,9,7,"00:06:10,880","00:06:14,285",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=370,sense for our problem setup.
cs-410_9_7_114,cs-410,9,7,"00:06:14,285","00:06:17,195",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=374,So we just talked about how
cs-410_9_7_115,cs-410,9,7,"00:06:17,195","00:06:20,660",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=377,overemphasizing a frequency term
cs-410_9_7_116,cs-410,9,7,"00:06:20,660","00:06:22,850",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=380,Now let's look at
cs-410_9_7_117,cs-410,9,7,"00:06:22,850","00:06:26,585",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=382,and that is how we can
cs-410_9_7_118,cs-410,9,7,"00:06:26,585","00:06:28,935",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,"Matching ""the"" is not surprising,"
cs-410_9_7_119,cs-410,9,7,"00:06:28,935","00:06:30,645",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=388,"because ""the"" occurs everywhere."
cs-410_9_7_120,cs-410,9,7,"00:06:30,645","00:06:33,020",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,"But matching ""eats"""
cs-410_9_7_121,cs-410,9,7,"00:06:33,020","00:06:35,105",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=393,So how can we address
cs-410_9_7_122,cs-410,9,7,"00:06:35,105","00:06:38,965",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,"Now in this case, we can"
cs-410_9_7_123,cs-410,9,7,"00:06:38,965","00:06:42,095",123,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=398,That's commonly
cs-410_9_7_124,cs-410,9,7,"00:06:42,095","00:06:45,065",124,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=402,IDF stands for
cs-410_9_7_125,cs-410,9,7,"00:06:45,065","00:06:47,675",125,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=405,Document frequency
cs-410_9_7_126,cs-410,9,7,"00:06:47,675","00:06:49,370",126,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=407,of the total number of
cs-410_9_7_127,cs-410,9,7,"00:06:49,370","00:06:52,235",127,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,documents that contain
cs-410_9_7_128,cs-410,9,7,"00:06:52,235","00:06:57,200",128,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=412,So here we show that the IDF
cs-410_9_7_129,cs-410,9,7,"00:06:57,200","00:07:00,230",129,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=417,a logarithm function
cs-410_9_7_130,cs-410,9,7,"00:07:00,230","00:07:05,065",130,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,of documents that match a
cs-410_9_7_131,cs-410,9,7,"00:07:05,065","00:07:08,870",131,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,So K is the number of
cs-410_9_7_132,cs-410,9,7,"00:07:08,870","00:07:11,630",132,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=428,document frequency and M
cs-410_9_7_133,cs-410,9,7,"00:07:11,630","00:07:14,615",133,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=431,here is the total number of
cs-410_9_7_134,cs-410,9,7,"00:07:14,615","00:07:21,200",134,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=434,The IDF function is giving
cs-410_9_7_135,cs-410,9,7,"00:07:21,200","00:07:24,815",135,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=441,meaning that it
cs-410_9_7_136,cs-410,9,7,"00:07:24,815","00:07:28,805",136,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=444,The maximum value is
cs-410_9_7_137,cs-410,9,7,"00:07:28,805","00:07:33,650",137,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=448,That's when the word occurred
cs-410_9_7_138,cs-410,9,7,"00:07:33,650","00:07:37,235",138,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=453,"So that's a very rare term,"
cs-410_9_7_139,cs-410,9,7,"00:07:37,235","00:07:40,745",139,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=457,the rare is term in
cs-410_9_7_140,cs-410,9,7,"00:07:40,745","00:07:46,700",140,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=460,The lowest value you can
cs-410_9_7_141,cs-410,9,7,"00:07:46,700","00:07:49,115",141,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=466,its maximum which would be M.
cs-410_9_7_142,cs-410,9,7,"00:07:49,115","00:07:53,880",142,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=469,So that would be
cs-410_9_7_143,cs-410,9,7,"00:07:53,990","00:07:57,340",143,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=473,close to zero in fact.
cs-410_9_7_144,cs-410,9,7,"00:07:57,470","00:08:02,360",144,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=477,So this of course measure
cs-410_9_7_145,cs-410,9,7,"00:08:02,360","00:08:06,740",145,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=482,is used in search where we
cs-410_9_7_146,cs-410,9,7,"00:08:06,740","00:08:09,960",146,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=486,"In our case, what would"
cs-410_9_7_147,cs-410,9,7,"00:08:09,960","00:08:13,040",147,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=489,"Well, we can also"
cs-410_9_7_148,cs-410,9,7,"00:08:13,040","00:08:16,610",148,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,we can collect all the words
cs-410_9_7_149,cs-410,9,7,"00:08:16,610","00:08:18,590",149,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=496,"That is to say,"
cs-410_9_7_150,cs-410,9,7,"00:08:18,590","00:08:22,225",150,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=498,a word that's popular in
cs-410_9_7_151,cs-410,9,7,"00:08:22,225","00:08:25,650",151,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=502,would also have a low IDF.
cs-410_9_7_152,cs-410,9,7,"00:08:25,650","00:08:29,445",152,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=505,"Because depending on the dataset,"
cs-410_9_7_153,cs-410,9,7,"00:08:29,445","00:08:35,105",153,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=509,we can construct the context
cs-410_9_7_154,cs-410,9,7,"00:08:35,105","00:08:38,010",154,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=515,But in the end if a term is
cs-410_9_7_155,cs-410,9,7,"00:08:38,010","00:08:41,024",155,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=518,very frequent in
cs-410_9_7_156,cs-410,9,7,"00:08:41,024","00:08:43,210",156,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,then it will still be frequent
cs-410_9_7_157,cs-410,9,7,"00:08:43,210","00:08:47,220",157,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=523,in the collective
cs-410_9_7_158,cs-410,9,7,"00:08:47,620","00:08:52,355",158,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=527,So how can we add
cs-410_9_7_159,cs-410,9,7,"00:08:52,355","00:08:56,910",159,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=532,improve our similarity function?
cs-410_9_7_160,cs-410,9,7,"00:08:56,910","00:08:58,565",160,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=536,"Well, here's one way"
cs-410_9_7_161,cs-410,9,7,"00:08:58,565","00:09:00,920",161,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=538,many other ways
cs-410_9_7_162,cs-410,9,7,"00:09:00,920","00:09:02,520",162,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=540,"But this is a reasonable way,"
cs-410_9_7_163,cs-410,9,7,"00:09:02,520","00:09:05,825",163,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=542,where we can adapt
cs-410_9_7_164,cs-410,9,7,"00:09:05,825","00:09:09,520",164,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=545,for paradigmatical
cs-410_9_7_165,cs-410,9,7,"00:09:14,120","00:09:20,555",165,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=554,"In this case, we define the"
cs-410_9_7_166,cs-410,9,7,"00:09:20,555","00:09:26,825",166,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=560,elements representing
cs-410_9_7_167,cs-410,9,7,"00:09:26,825","00:09:29,810",167,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=566,So in this
cs-410_9_7_168,cs-410,9,7,"00:09:29,810","00:09:36,985",168,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,we take sum over all
cs-410_9_7_169,cs-410,9,7,"00:09:36,985","00:09:42,155",169,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=576,normalize the weight of
cs-410_9_7_170,cs-410,9,7,"00:09:42,155","00:09:48,210",170,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=582,of the weights of all the words.
cs-410_9_7_171,cs-410,9,7,"00:09:48,210","00:09:51,030",171,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=588,This is to again ensure all the
cs-410_9_7_172,cs-410,9,7,"00:09:51,030","00:09:53,975",172,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=591,xi's will sum to
cs-410_9_7_173,cs-410,9,7,"00:09:53,975","00:09:57,800",173,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=593,So this would be very similar
cs-410_9_7_174,cs-410,9,7,"00:09:57,800","00:09:59,420",174,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=597,in that this vector is
cs-410_9_7_175,cs-410,9,7,"00:09:59,420","00:10:04,015",175,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=599,actually something similar
cs-410_9_7_176,cs-410,9,7,"00:10:04,015","00:10:06,685",176,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=604,all the xi's will sum to one.
cs-410_9_7_177,cs-410,9,7,"00:10:06,685","00:10:13,560",177,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=606,"Now, the weight of BM25 for"
cs-410_9_7_178,cs-410,9,7,"00:10:14,460","00:10:18,940",178,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=614,If you compare this with
cs-410_9_7_179,cs-410,9,7,"00:10:18,940","00:10:22,930",179,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=618,have a normalized count
cs-410_9_7_180,cs-410,9,7,"00:10:22,930","00:10:26,320",180,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=622,So we only have this one
cs-410_9_7_181,cs-410,9,7,"00:10:26,320","00:10:31,090",181,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=626,the total counts of words in
cs-410_9_7_182,cs-410,9,7,"00:10:31,090","00:10:33,430",182,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=631,and that's what we had before.
cs-410_9_7_183,cs-410,9,7,"00:10:33,430","00:10:36,039",183,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=633,But now with the BM25
cs-410_9_7_184,cs-410,9,7,"00:10:36,039","00:10:38,335",184,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=636,we introduced something else.
cs-410_9_7_185,cs-410,9,7,"00:10:38,335","00:10:42,040",185,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=638,"First, of course,"
cs-410_9_7_186,cs-410,9,7,"00:10:42,040","00:10:43,420",186,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=642,this count is just to
cs-410_9_7_187,cs-410,9,7,"00:10:43,420","00:10:46,075",187,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=643,achieve the sub-linear
cs-410_9_7_188,cs-410,9,7,"00:10:46,075","00:10:50,155",188,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=646,But we also see we introduced
cs-410_9_7_189,cs-410,9,7,"00:10:50,155","00:10:56,110",189,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=650,and this parameter is
cs-410_9_7_190,cs-410,9,7,"00:10:56,110","00:10:58,810",190,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=656,although zero is also possible.
cs-410_9_7_191,cs-410,9,7,"00:10:58,810","00:11:02,950",191,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=658,But this controls
cs-410_9_7_192,cs-410,9,7,"00:11:02,950","00:11:06,535",192,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=662,and also controls to what extent
cs-410_9_7_193,cs-410,9,7,"00:11:06,535","00:11:11,240",193,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=666,it simulates the
cs-410_9_7_194,cs-410,9,7,"00:11:11,250","00:11:14,830",194,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=671,"So this is one parameter,"
cs-410_9_7_195,cs-410,9,7,"00:11:14,830","00:11:17,140",195,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=674,but we also see there is
cs-410_9_7_196,cs-410,9,7,"00:11:17,140","00:11:21,115",196,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=677,"b, and this would be"
cs-410_9_7_197,cs-410,9,7,"00:11:21,115","00:11:25,405",197,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=681,This is a parameter to
cs-410_9_7_198,cs-410,9,7,"00:11:25,405","00:11:27,294",198,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=685,"In this case,"
cs-410_9_7_199,cs-410,9,7,"00:11:27,294","00:11:29,200",199,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=687,the normalization formula has
cs-410_9_7_200,cs-410,9,7,"00:11:29,200","00:11:31,885",200,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=689,a average document lens here.
cs-410_9_7_201,cs-410,9,7,"00:11:31,885","00:11:35,770",201,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=691,This is computed up
cs-410_9_7_202,cs-410,9,7,"00:11:35,770","00:11:39,880",202,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=695,of the lenses of all the
cs-410_9_7_203,cs-410,9,7,"00:11:39,880","00:11:41,605",203,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=699,"In this case, all the lenses of"
cs-410_9_7_204,cs-410,9,7,"00:11:41,605","00:11:45,340",204,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=701,all the context of documents
cs-410_9_7_205,cs-410,9,7,"00:11:45,340","00:11:48,175",205,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=705,So this average documents
cs-410_9_7_206,cs-410,9,7,"00:11:48,175","00:11:50,425",206,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=708,will be a constant for
cs-410_9_7_207,cs-410,9,7,"00:11:50,425","00:11:52,795",207,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=710,So it actually is only
cs-410_9_7_208,cs-410,9,7,"00:11:52,795","00:11:56,530",208,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=712,affecting the effect
cs-410_9_7_209,cs-410,9,7,"00:11:56,530","00:12:01,180",209,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=716,"b, here because"
cs-410_9_7_210,cs-410,9,7,"00:12:01,180","00:12:07,780",210,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=721,But I kept it here because
cs-410_9_7_211,cs-410,9,7,"00:12:07,780","00:12:10,840",211,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=727,for in retrieval where it would
cs-410_9_7_212,cs-410,9,7,"00:12:10,840","00:12:14,770",212,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=730,give us a stabilized
cs-410_9_7_213,cs-410,9,7,"00:12:14,770","00:12:16,570",213,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=734,"But for our purpose,"
cs-410_9_7_214,cs-410,9,7,"00:12:16,570","00:12:21,430",214,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=736,this will be a constant so
cs-410_9_7_215,cs-410,9,7,"00:12:21,430","00:12:28,550",215,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=741,the lens normalization
cs-410_9_7_216,cs-410,9,7,"00:12:29,400","00:12:33,295",216,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=749,"Now, with this definition then,"
cs-410_9_7_217,cs-410,9,7,"00:12:33,295","00:12:37,810",217,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=753,we have a new way to define
cs-410_9_7_218,cs-410,9,7,"00:12:37,810","00:12:41,785",218,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=757,and we can compute
cs-410_9_7_219,cs-410,9,7,"00:12:41,785","00:12:43,255",219,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=761,The difference is that
cs-410_9_7_220,cs-410,9,7,"00:12:43,255","00:12:44,950",220,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=763,the high-frequency terms will now
cs-410_9_7_221,cs-410,9,7,"00:12:44,950","00:12:46,930",221,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=764,have a somewhat lower weights.
cs-410_9_7_222,cs-410,9,7,"00:12:46,930","00:12:49,690",222,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=766,This would help us control
cs-410_9_7_223,cs-410,9,7,"00:12:49,690","00:12:53,575",223,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=769,the inference of
cs-410_9_7_224,cs-410,9,7,"00:12:53,575","00:12:58,000",224,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=773,"Now, the idea can be added"
cs-410_9_7_225,cs-410,9,7,"00:12:58,000","00:12:59,905",225,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=778,That means we'll
cs-410_9_7_226,cs-410,9,7,"00:12:59,905","00:13:01,990",226,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=779,for matching each term.
cs-410_9_7_227,cs-410,9,7,"00:13:01,990","00:13:05,650",227,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=781,So you may recall
cs-410_9_7_228,cs-410,9,7,"00:13:05,650","00:13:08,305",228,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=785,all the possible words
cs-410_9_7_229,cs-410,9,7,"00:13:08,305","00:13:11,365",229,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=788,overlap between the two contexts.
cs-410_9_7_230,cs-410,9,7,"00:13:11,365","00:13:15,790",230,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=791,The x_i and the y_i
cs-410_9_7_231,cs-410,9,7,"00:13:15,790","00:13:20,245",231,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=795,of picking the word
cs-410_9_7_232,cs-410,9,7,"00:13:20,245","00:13:22,330",232,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=800,"Therefore, it"
cs-410_9_7_233,cs-410,9,7,"00:13:22,330","00:13:24,805",233,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=802,we'll see a match on this word.
cs-410_9_7_234,cs-410,9,7,"00:13:24,805","00:13:26,695",234,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=804,"Now, IDF would give us"
cs-410_9_7_235,cs-410,9,7,"00:13:26,695","00:13:29,200",235,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=806,the importance of
cs-410_9_7_236,cs-410,9,7,"00:13:29,200","00:13:33,700",236,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=809,A common word will be worth
cs-410_9_7_237,cs-410,9,7,"00:13:33,700","00:13:36,715",237,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=813,So we emphasize more on
cs-410_9_7_238,cs-410,9,7,"00:13:36,715","00:13:38,785",238,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=816,"So with this modification,"
cs-410_9_7_239,cs-410,9,7,"00:13:38,785","00:13:40,660",239,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=818,then the new function will
cs-410_9_7_240,cs-410,9,7,"00:13:40,660","00:13:43,270",240,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=820,likely address
cs-410_9_7_241,cs-410,9,7,"00:13:43,270","00:13:45,310",241,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=823,"Now, interestingly"
cs-410_9_7_242,cs-410,9,7,"00:13:45,310","00:13:49,825",242,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=825,this approach to discover
cs-410_9_7_243,cs-410,9,7,"00:13:49,825","00:13:57,430",243,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=829,"In general, when we re-brand"
cs-410_9_7_244,cs-410,9,7,"00:13:57,430","00:13:59,365",244,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=837,"a context with a term vector,"
cs-410_9_7_245,cs-410,9,7,"00:13:59,365","00:14:01,900",245,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=839,we would likely see
cs-410_9_7_246,cs-410,9,7,"00:14:01,900","00:14:04,135",246,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=841,some terms have high weights
cs-410_9_7_247,cs-410,9,7,"00:14:04,135","00:14:06,040",247,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=844,and other terms have low weights.
cs-410_9_7_248,cs-410,9,7,"00:14:06,040","00:14:09,490",248,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=846,Depending on how we assign
cs-410_9_7_249,cs-410,9,7,"00:14:09,490","00:14:11,650",249,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=849,we might be able to
cs-410_9_7_250,cs-410,9,7,"00:14:11,650","00:14:13,720",250,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=851,discover the words that
cs-410_9_7_251,cs-410,9,7,"00:14:13,720","00:14:15,700",251,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=853,are strongly associated with
cs-410_9_7_252,cs-410,9,7,"00:14:15,700","00:14:18,400",252,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=855,the candidate word
cs-410_9_7_253,cs-410,9,7,"00:14:18,400","00:14:20,560",253,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=858,So let's take a look at
cs-410_9_7_254,cs-410,9,7,"00:14:20,560","00:14:23,815",254,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=860,the term vector in
cs-410_9_7_255,cs-410,9,7,"00:14:23,815","00:14:29,885",255,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=863,We have each x_i
cs-410_9_7_256,cs-410,9,7,"00:14:29,885","00:14:33,610",256,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=869,defined as the normalized
cs-410_9_7_257,cs-410,9,7,"00:14:33,610","00:14:37,420",257,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=873,"Now, this weight alone only"
cs-410_9_7_258,cs-410,9,7,"00:14:37,420","00:14:41,110",258,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=877,reflects how frequent the word
cs-410_9_7_259,cs-410,9,7,"00:14:41,110","00:14:43,345",259,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=881,But we can't just say
cs-410_9_7_260,cs-410,9,7,"00:14:43,345","00:14:44,500",260,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=883,any frequent term in
cs-410_9_7_261,cs-410,9,7,"00:14:44,500","00:14:46,560",261,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=884,the context that would
cs-410_9_7_262,cs-410,9,7,"00:14:46,560","00:14:50,235",262,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=886,the candidate word because
cs-410_9_7_263,cs-410,9,7,"00:14:50,235","00:14:51,990",263,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=890,many common words like 'the' will
cs-410_9_7_264,cs-410,9,7,"00:14:51,990","00:14:54,540",264,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=891,occur frequently in
cs-410_9_7_265,cs-410,9,7,"00:14:54,540","00:14:59,645",265,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=894,But if we apply IDF
cs-410_9_7_266,cs-410,9,7,"00:14:59,645","00:15:07,090",266,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=899,we can then re-weight
cs-410_9_7_267,cs-410,9,7,"00:15:07,090","00:15:09,220",267,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=907,That means the words that are
cs-410_9_7_268,cs-410,9,7,"00:15:09,220","00:15:11,920",268,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=909,common like 'the'
cs-410_9_7_269,cs-410,9,7,"00:15:11,920","00:15:14,920",269,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=911,So now the highest
cs-410_9_7_270,cs-410,9,7,"00:15:14,920","00:15:18,220",270,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=914,those common terms because
cs-410_9_7_271,cs-410,9,7,"00:15:18,220","00:15:20,980",271,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=918,"Instead, those terms would"
cs-410_9_7_272,cs-410,9,7,"00:15:20,980","00:15:23,920",272,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=920,be the terms that are
cs-410_9_7_273,cs-410,9,7,"00:15:23,920","00:15:26,080",273,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=923,but not frequent
cs-410_9_7_274,cs-410,9,7,"00:15:26,080","00:15:29,590",274,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=926,So those are clearly the words
cs-410_9_7_275,cs-410,9,7,"00:15:29,590","00:15:33,820",275,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=929,the context of the candidate
cs-410_9_7_276,cs-410,9,7,"00:15:33,820","00:15:35,365",276,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=933,"So for this reason,"
cs-410_9_7_277,cs-410,9,7,"00:15:35,365","00:15:39,865",277,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=935,the highly weighted terms in
cs-410_9_7_278,cs-410,9,7,"00:15:39,865","00:15:42,310",278,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=939,can also be assumed to
cs-410_9_7_279,cs-410,9,7,"00:15:42,310","00:15:45,940",279,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=942,be candidates for
cs-410_9_7_280,cs-410,9,7,"00:15:45,940","00:15:48,895",280,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=945,"Now, of course, this is"
cs-410_9_7_281,cs-410,9,7,"00:15:48,895","00:15:53,560",281,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=948,our approach for discovering
cs-410_9_7_282,cs-410,9,7,"00:15:53,560","00:15:57,025",282,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=953,"In the next lecture, we're"
cs-410_9_7_283,cs-410,9,7,"00:15:57,025","00:16:01,850",283,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=957,how to discover
cs-410_9_7_284,cs-410,9,7,"00:16:02,280","00:16:05,305",284,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=962,But it clearly shows the relation
cs-410_9_7_285,cs-410,9,7,"00:16:05,305","00:16:08,995",285,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=965,between discovering
cs-410_9_7_286,cs-410,9,7,"00:16:08,995","00:16:12,670",286,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=968,Indeed they can be discovered in
cs-410_9_7_287,cs-410,9,7,"00:16:12,670","00:16:18,340",287,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=972,a joint manner by leveraging
cs-410_9_7_288,cs-410,9,7,"00:16:18,340","00:16:22,600",288,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=978,"So to summarize,"
cs-410_9_7_289,cs-410,9,7,"00:16:22,600","00:16:26,050",289,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=982,paradigmatic relations is to
cs-410_9_7_290,cs-410,9,7,"00:16:26,050","00:16:27,610",290,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=986,collect the context of
cs-410_9_7_291,cs-410,9,7,"00:16:27,610","00:16:30,460",291,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=987,a candidate word to
cs-410_9_7_292,cs-410,9,7,"00:16:30,460","00:16:33,685",292,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=990,This is typically represented
cs-410_9_7_293,cs-410,9,7,"00:16:33,685","00:16:35,890",293,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=993,Then compute the similarity of
cs-410_9_7_294,cs-410,9,7,"00:16:35,890","00:16:38,005",294,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=995,the corresponding
cs-410_9_7_295,cs-410,9,7,"00:16:38,005","00:16:40,540",295,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=998,of two candidate words.
cs-410_9_7_296,cs-410,9,7,"00:16:40,540","00:16:45,910",296,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1000,Then we can take
cs-410_9_7_297,cs-410,9,7,"00:16:45,910","00:16:50,305",297,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1005,and treat them as having
cs-410_9_7_298,cs-410,9,7,"00:16:50,305","00:16:53,395",298,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1010,These are the words that
cs-410_9_7_299,cs-410,9,7,"00:16:53,395","00:16:55,540",299,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1013,There are many different ways to
cs-410_9_7_300,cs-410,9,7,"00:16:55,540","00:16:58,090",300,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1015,implement this general idea.
cs-410_9_7_301,cs-410,9,7,"00:16:58,090","00:17:01,435",301,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1018,We just talked about
cs-410_9_7_302,cs-410,9,7,"00:17:01,435","00:17:04,510",302,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1021,"More specifically, we"
cs-410_9_7_303,cs-410,9,7,"00:17:04,510","00:17:07,765",303,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1024,text retrieval models to help us
cs-410_9_7_304,cs-410,9,7,"00:17:07,765","00:17:10,690",304,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1027,design effective
cs-410_9_7_305,cs-410,9,7,"00:17:10,690","00:17:15,170",305,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1030,compute the
cs-410_9_7_306,cs-410,9,7,"00:17:15,960","00:17:19,330",306,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1035,"More specifically, we have used"
cs-410_9_7_307,cs-410,9,7,"00:17:19,330","00:17:23,020",307,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1039,the BM25 and IDF weighting
cs-410_9_7_308,cs-410,9,7,"00:17:23,020","00:17:27,250",308,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1043,to discover
cs-410_9_7_309,cs-410,9,7,"00:17:27,250","00:17:30,100",309,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1047,These approaches also represent
cs-410_9_7_310,cs-410,9,7,"00:17:30,100","00:17:33,310",310,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1050,the state of the art in
cs-410_9_7_311,cs-410,9,7,"00:17:33,310","00:17:37,165",311,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1053,"Finally, syntagmatic relations"
cs-410_9_7_312,cs-410,9,7,"00:17:37,165","00:17:42,140",312,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=1057,as a by-product when we discover
cs-410_10_6_1,cs-410,10,6,"00:00:00,012","00:00:03,145",1,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=0,[NOISE]
cs-410_10_6_2,cs-410,10,6,"00:00:06,848","00:00:10,210",2,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=6,This lecture is a summary of this course.
cs-410_10_6_3,cs-410,10,6,"00:00:12,890","00:00:17,110",3,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=12,This map shows the major topics
cs-410_10_6_4,cs-410,10,6,"00:00:19,170","00:00:24,230",4,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=19,And here are some key
cs-410_10_6_5,cs-410,10,6,"00:00:24,230","00:00:28,020",5,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=24,"First, we talked about natural"
cs-410_10_6_6,cs-410,10,6,"00:00:29,170","00:00:33,120",6,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=29,Here the main take-away messages
cs-410_10_6_7,cs-410,10,6,"00:00:33,120","00:00:39,210",7,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=33,"a foundation for text retrieval, but"
cs-410_10_6_8,cs-410,10,6,"00:00:39,210","00:00:48,540",8,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=39,the battle of wars is generally the main
cs-410_10_6_9,cs-410,10,6,"00:00:48,540","00:00:52,730",9,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=48,And it's often sufficient before
cs-410_10_6_10,cs-410,10,6,"00:00:52,730","00:00:58,290",10,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=52,obviously for
cs-410_10_6_11,cs-410,10,6,"00:00:58,290","00:01:02,640",11,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=58,a deeper natural language
cs-410_10_6_12,cs-410,10,6,"00:01:02,640","00:01:05,070",12,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=62,We then talked about the high
cs-410_10_6_13,cs-410,10,6,"00:01:05,070","00:01:09,170",13,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=65,text access and
cs-410_10_6_14,cs-410,10,6,"00:01:09,170","00:01:12,200",14,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=69,In pull we talked about
cs-410_10_6_15,cs-410,10,6,"00:01:13,250","00:01:17,800",15,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=73,"Now in general in future search engines,"
cs-410_10_6_16,cs-410,10,6,"00:01:17,800","00:01:20,466",16,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=77,to provide a math involved
cs-410_10_6_17,cs-410,10,6,"00:01:23,022","00:01:27,680",17,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=83,And now we'll talk about a number of
cs-410_10_6_18,cs-410,10,6,"00:01:27,680","00:01:30,350",18,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=87,We talked about the search problem.
cs-410_10_6_19,cs-410,10,6,"00:01:30,350","00:01:32,280",19,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=90,And we framed that as a ranking problem.
cs-410_10_6_20,cs-410,10,6,"00:01:34,830","00:01:38,680",20,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=94,And we talked about a number
cs-410_10_6_21,cs-410,10,6,"00:01:38,680","00:01:42,170",21,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=98,We start with the overview
cs-410_10_6_22,cs-410,10,6,"00:01:42,170","00:01:46,710",22,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=102,the probabilistic model and then we talked
cs-410_10_6_23,cs-410,10,6,"00:01:48,400","00:01:53,730",23,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=108,We also later talked about
cs-410_10_6_24,cs-410,10,6,"00:01:53,730","00:01:56,280",24,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=113,that's probabilistic model.
cs-410_10_6_25,cs-410,10,6,"00:01:56,280","00:02:01,910",25,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=116,"And here, many take-away message is that"
cs-410_10_6_26,cs-410,10,6,"00:02:01,910","00:02:07,510",26,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=121,"look similar, and"
cs-410_10_6_27,cs-410,10,6,"00:02:07,510","00:02:13,730",27,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=127,"Most important ones are TF-IDF weighting,"
cs-410_10_6_28,cs-410,10,6,"00:02:13,730","00:02:20,460",28,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=133,And the TF is often transformed through
cs-410_10_6_29,cs-410,10,6,"00:02:22,070","00:02:27,730",29,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=142,And then we talked about how to
cs-410_10_6_30,cs-410,10,6,"00:02:27,730","00:02:33,940",30,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=147,"the main techniques that we talked about,"
cs-410_10_6_31,cs-410,10,6,"00:02:33,940","00:02:39,590",31,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=153,that we can prepare the system
cs-410_10_6_32,cs-410,10,6,"00:02:39,590","00:02:45,100",32,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=159,And we talked about how to do a faster
cs-410_10_6_33,cs-410,10,6,"00:02:46,180","00:02:50,800",33,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=166,And we then talked about how to
cs-410_10_6_34,cs-410,10,6,"00:02:50,800","00:02:54,860",34,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=170,mainly introduced to
cs-410_10_6_35,cs-410,10,6,"00:02:54,860","00:02:58,770",35,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=174,This was a very important
cs-410_10_6_36,cs-410,10,6,"00:02:58,770","00:03:00,490",36,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=178,applied to many tasks.
cs-410_10_6_37,cs-410,10,6,"00:03:01,980","00:03:05,450",37,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=181,We talked about the major
cs-410_10_6_38,cs-410,10,6,"00:03:05,450","00:03:10,800",38,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=185,"So, the most important measures for"
cs-410_10_6_39,cs-410,10,6,"00:03:10,800","00:03:16,400",39,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=190,"are MAP, mean average precision,"
cs-410_10_6_40,cs-410,10,6,"00:03:16,400","00:03:20,880",40,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=196,accumulative gain and also precision and
cs-410_10_6_41,cs-410,10,6,"00:03:22,580","00:03:25,540",41,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=202,And we then talked about
cs-410_10_6_42,cs-410,10,6,"00:03:25,540","00:03:29,180",42,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=205,And we talked about the Rocchio
cs-410_10_6_43,cs-410,10,6,"00:03:29,180","00:03:32,200",43,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=209,the mixture model and
cs-410_10_6_44,cs-410,10,6,"00:03:32,200","00:03:36,630",44,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=212,Feedback is a very important
cs-410_10_6_45,cs-410,10,6,"00:03:36,630","00:03:41,430",45,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=216,the opportunity of learning from
cs-410_10_6_46,cs-410,10,6,"00:03:42,960","00:03:45,800",46,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=222,We then talked about Web search.
cs-410_10_6_47,cs-410,10,6,"00:03:45,800","00:03:50,150",47,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=225,And here we talked about how
cs-410_10_6_48,cs-410,10,6,"00:03:50,150","00:03:55,330",48,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=230,to solve the scalability issue in that
cs-410_10_6_49,cs-410,10,6,"00:03:55,330","00:03:59,130",49,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=235,Then we talked about how to use linking
cs-410_10_6_50,cs-410,10,6,"00:03:59,130","00:04:01,490",50,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=239,We talked about page rank and
cs-410_10_6_51,cs-410,10,6,"00:04:01,490","00:04:06,010",51,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=241,hits as the major hours is to
cs-410_10_6_52,cs-410,10,6,"00:04:07,320","00:04:09,562",52,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=247,We then talked about
cs-410_10_6_53,cs-410,10,6,"00:04:09,562","00:04:14,810",53,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=249,This is the use of machine learning
cs-410_10_6_54,cs-410,10,6,"00:04:14,810","00:04:16,640",54,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=254,improvement scoring.
cs-410_10_6_55,cs-410,10,6,"00:04:16,640","00:04:21,460",55,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=256,Not only that the effectiveness can be
cs-410_10_6_56,cs-410,10,6,"00:04:21,460","00:04:23,620",56,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=261,we can also improve the robustness of the.
cs-410_10_6_57,cs-410,10,6,"00:04:23,620","00:04:28,560",57,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=263,The ranking function so that it's
cs-410_10_6_58,cs-410,10,6,"00:04:28,560","00:04:34,540",58,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=268,It just some features to promote the page.
cs-410_10_6_59,cs-410,10,6,"00:04:36,270","00:04:39,279",59,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=276,And finally we talked about
cs-410_10_6_60,cs-410,10,6,"00:04:40,460","00:04:45,730",60,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=280,About the some major reactions
cs-410_10_6_61,cs-410,10,6,"00:04:45,730","00:04:49,390",61,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=285,in the future in improving the count
cs-410_10_6_62,cs-410,10,6,"00:04:50,610","00:04:54,030",62,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=290,And then finally we talked about
cs-410_10_6_63,cs-410,10,6,"00:04:54,030","00:04:57,890",63,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=294,these are systems to
cs-410_10_6_64,cs-410,10,6,"00:04:57,890","00:05:02,120",64,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=297,"And we'll talk about the two approaches,"
cs-410_10_6_65,cs-410,10,6,"00:05:02,120","00:05:06,240",65,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=302,one is collaborative filtering and
cs-410_10_6_66,cs-410,10,6,"00:05:07,330","00:05:11,930",66,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=307,"Now, an obvious missing piece"
cs-410_10_6_67,cs-410,10,6,"00:05:11,930","00:05:16,884",67,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=311,"in this picture is the user,"
cs-410_10_6_68,cs-410,10,6,"00:05:16,884","00:05:21,620",68,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=316,so user interface is also an important
cs-410_10_6_69,cs-410,10,6,"00:05:21,620","00:05:25,850",69,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=321,Even though the current search interface
cs-410_10_6_70,cs-410,10,6,"00:05:25,850","00:05:32,020",70,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=325,done a lot of studies of user interfaces
cs-410_10_6_71,cs-410,10,6,"00:05:32,020","00:05:34,680",71,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=332,"And this is the topic to that,"
cs-410_10_6_72,cs-410,10,6,"00:05:34,680","00:05:40,350",72,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=334,you can learn more by reading this book.
cs-410_10_6_73,cs-410,10,6,"00:05:40,350","00:05:47,430",73,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=340,It's an excellent book about all kinds
cs-410_10_6_74,cs-410,10,6,"00:05:48,800","00:05:53,360",74,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=348,If you want to know more about
cs-410_10_6_75,cs-410,10,6,"00:05:53,360","00:05:57,590",75,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=353,you can also read some additional
cs-410_10_6_76,cs-410,10,6,"00:05:57,590","00:06:01,110",76,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=357,In this short course we only
cs-410_10_6_77,cs-410,10,6,"00:06:01,110","00:06:03,610",77,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=361,topics in text retrievals and
cs-410_10_6_78,cs-410,10,6,"00:06:04,770","00:06:09,930",78,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=364,And these resources provide additional
cs-410_10_6_79,cs-410,10,6,"00:06:09,930","00:06:16,220",79,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=369,they give a more thorough treatment of
cs-410_10_6_80,cs-410,10,6,"00:06:16,220","00:06:19,410",80,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=376,And a main source is
cs-410_10_6_81,cs-410,10,6,"00:06:21,570","00:06:26,916",81,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=381,that you can see a lot of short
cs-410_10_6_82,cs-410,10,6,"00:06:26,916","00:06:30,290",82,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=386,or long tutorials.
cs-410_10_6_83,cs-410,10,6,"00:06:30,290","00:06:35,260",83,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=390,They tend to provide a lot of
cs-410_10_6_84,cs-410,10,6,"00:06:35,260","00:06:40,830",84,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=395,And there a lot of series that
cs-410_10_6_85,cs-410,10,6,"00:06:40,830","00:06:44,660",85,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=400,"One is information concepts,"
cs-410_10_6_86,cs-410,10,6,"00:06:44,660","00:06:46,310",86,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=404,One is human langauge technology.
cs-410_10_6_87,cs-410,10,6,"00:06:46,310","00:06:49,452",87,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=406,And yet another is artificial
cs-410_10_6_88,cs-410,10,6,"00:06:49,452","00:06:55,535",88,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=409,There are also some major journals and
cs-410_10_6_89,cs-410,10,6,"00:06:55,535","00:07:00,485",89,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=415,tend to have a lot of research papers
cs-410_10_6_90,cs-410,10,6,"00:07:00,485","00:07:05,370",90,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=420,"And finally, for more information"
cs-410_10_6_91,cs-410,10,6,"00:07:05,370","00:07:08,930",91,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=425,"tool kits, etc you can check out his URL."
cs-410_10_6_92,cs-410,10,6,"00:07:10,010","00:07:16,320",92,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=430,"So, if you have not taken the text"
cs-410_10_6_93,cs-410,10,6,"00:07:16,320","00:07:22,630",93,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=436,specialization series then naturally
cs-410_10_6_94,cs-410,10,6,"00:07:22,630","00:07:27,900",94,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=442,"As this picture shows,"
cs-410_10_6_95,cs-410,10,6,"00:07:27,900","00:07:31,800",95,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=447,we generally need two kinds of techniques.
cs-410_10_6_96,cs-410,10,6,"00:07:31,800","00:07:34,710",96,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=451,"One is text retrieval,"
cs-410_10_6_97,cs-410,10,6,"00:07:34,710","00:07:39,490",97,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=454,And these techniques will help us
cs-410_10_6_98,cs-410,10,6,"00:07:39,490","00:07:45,550",98,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=459,"relevant text data, which are actually"
cs-410_10_6_99,cs-410,10,6,"00:07:45,550","00:07:51,190",99,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=465,Now human plays important role in mining
cs-410_10_6_100,cs-410,10,6,"00:07:51,190","00:07:54,630",100,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=471,written for humans to consume.
cs-410_10_6_101,cs-410,10,6,"00:07:54,630","00:08:00,580",101,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=474,So involving humans in the process
cs-410_10_6_102,cs-410,10,6,"00:08:00,580","00:08:05,050",102,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=480,in this course we have covered
cs-410_10_6_103,cs-410,10,6,"00:08:05,050","00:08:08,300",103,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=485,access to the most relevant data.
cs-410_10_6_104,cs-410,10,6,"00:08:08,300","00:08:13,210",104,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=488,These techniques are always so
cs-410_10_6_105,cs-410,10,6,"00:08:13,210","00:08:17,770",105,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=493,to help provide prominence and
cs-410_10_6_106,cs-410,10,6,"00:08:17,770","00:08:23,990",106,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=497,patterns that the user will
cs-410_10_6_107,cs-410,10,6,"00:08:23,990","00:08:27,870",107,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=503,"So, in general, the user would have"
cs-410_10_6_108,cs-410,10,6,"00:08:27,870","00:08:29,359",108,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=507,better understand the patterns.
cs-410_10_6_109,cs-410,10,6,"00:08:30,360","00:08:36,200",109,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=510,"So the text mining cause, or rather,"
cs-410_10_6_110,cs-410,10,6,"00:08:36,200","00:08:41,660",110,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=516,will be dealing with what to do once
cs-410_10_6_111,cs-410,10,6,"00:08:41,660","00:08:46,010",111,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=521,So this is a second step in this
cs-410_10_6_112,cs-410,10,6,"00:08:46,010","00:08:48,790",112,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=526,the text data into actionable knowledge.
cs-410_10_6_113,cs-410,10,6,"00:08:49,830","00:08:55,900",113,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=529,And this has to do with helping users to
cs-410_10_6_114,cs-410,10,6,"00:08:55,900","00:08:59,750",114,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=535,to find the patterns and
cs-410_10_6_115,cs-410,10,6,"00:08:59,750","00:09:04,640",115,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=539,In text and such knowledge can
cs-410_10_6_116,cs-410,10,6,"00:09:04,640","00:09:10,500",116,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=544,systems to help decision making or
cs-410_10_6_117,cs-410,10,6,"00:09:10,500","00:09:16,624",117,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=550,"So, if you have not taken that course,"
cs-410_10_6_118,cs-410,10,6,"00:09:16,624","00:09:22,030",118,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=556,that natural next step would
cs-410_10_6_119,cs-410,10,6,"00:09:24,000","00:09:25,770",119,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=564,Thank you for taking this course.
cs-410_10_6_120,cs-410,10,6,"00:09:25,770","00:09:29,570",120,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=565,I hope you had fun and
cs-410_10_6_121,cs-410,10,6,"00:09:29,570","00:09:34,236",121,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=569,And I look forward to interacting
cs-410_10_6_122,cs-410,10,6,"00:09:34,236","00:09:44,236",122,https://www.coursera.org/learn/cs-410/lecture/rLpwp?t=574,[MUSIC]
